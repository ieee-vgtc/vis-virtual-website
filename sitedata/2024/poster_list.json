{
    "v-vis-posters-1030": {
        "event": "VIS Posters",
        "event_prefix": "v-vis-posters",
        "title": "Towards Understanding the Impact of Guidance in Data Visualization Systems for Domain Experts",
        "uid": "v-vis-posters-1030",
        "discord_channel": "https://discord.com/channels/1286338967674818684/1288497673212788798",
        "authors": [
            {
                "name": "Sherry Qiu",
                "email": "sherry.qiu@yale.edu",
                "affiliations": [
                    "Yale University, New Haven, United States"
                ],
                "is_corresponding": true
            },
            {
                "name": "HOLLY RUSHMEIER",
                "email": "",
                "affiliations": [
                    "Yale, New Haven, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Kim RM Blenman",
                "email": "",
                "affiliations": [
                    "Yale University, New Haven, United States",
                    "Yale University, New Haven, United States"
                ],
                "is_corresponding": false
            }
        ],
        "abstract": "Guided data visualization systems are highly useful for domain experts to highlight important trends in their large-scale and complex datasets. However, more work is needed to understand the impact of guidance on interpreting data visualizations as well as on the resulting use of visualizations when communicating insights. We conducted two user studies with domain experts and found that experts benefit from a guided coarse-to-fine structure when using data visualization systems, as this is the same structure in which they communicate findings.",
        "has_summary_pdf": true,
        "has_poster_pdf": true,
        "has_image": true
    },
    "v-vis-posters-1032": {
        "event": "VIS Posters",
        "event_prefix": "v-vis-posters",
        "title": "Mapping Inconsistencies: Applying an Interdisciplinary Framework to Evaluate Gender-based Violence Data Collection and Visualization",
        "uid": "v-vis-posters-1032",
        "discord_channel": "https://discord.com/channels/1286338967674818684/1288497673212788798",
        "authors": [
            {
                "name": "Yifan Zhang",
                "email": "yifan_zhang4@brown.edu",
                "affiliations": [
                    "Brown University, Providence, United States"
                ],
                "is_corresponding": true
            },
            {
                "name": "Helis Sikk",
                "email": "",
                "affiliations": [
                    "Brown University , Providence, RI, United States"
                ],
                "is_corresponding": false
            }
        ],
        "abstract": "Gender-based violence (GBV) is a critical human rights issue where data visualizations play an important role in educating the public about its scale and impact, and in shaping the decisions of policymakers. However, this area often suffers from missing and inconsistent data due to underreporting, lack of infrastructural support, and varying definitions of GBV across societal and cultural contexts, posing challenges for accurate visualizations. This study proposes an interdisciplinary framework for evaluating the reliability of GBV visualizations, focusing on the intersection between data quality and visual representation. The framework was applied to a representative sample of 15 countries from the World Health Organization Global Database on the Prevalence Violence against Women (VAW), the current largest visualization project on GBV. A summary dataset was produced based on the evaluation results and used to create analysis and \"meta-visualizations\" that reveal shortcomings of the visualization design and underlying data collection. The study reveals numerous limitations of the VAW project, including misleading color schemes, insufficient representation of data context and quality, and lack of diverse voices in data collection. The study calls for more culturally responsive data collection and nuanced visualization approaches to accurately portray and address GBV. The next step involves applying the framework to the remaining countries in the database and developing guidelines for more effective GBV data collection and visualization, thereby contributing to addressing GBV as a multidimensional, situated issue.",
        "has_summary_pdf": true,
        "has_poster_pdf": true,
        "has_image": true
    },
    "v-vis-posters-1034": {
        "event": "VIS Posters",
        "event_prefix": "v-vis-posters",
        "title": "MetaMood: An AI-based Shared Emotion Visualisation in Immersive Healing Spaces",
        "uid": "v-vis-posters-1034",
        "discord_channel": "https://discord.com/channels/1286338967674818684/1288497673212788798",
        "authors": [
            {
                "name": "Fengyi Yan",
                "email": "756573064@qq.com",
                "affiliations": [
                    "Beihang University, Beiiing, China",
                    "Beihang University, Beiiing, China"
                ],
                "is_corresponding": true
            },
            {
                "name": "Siyu Luo",
                "email": "",
                "affiliations": [
                    "Academy of Arts and Design, Beijing, China",
                    "Academy of Arts and Design, Beijing, China"
                ],
                "is_corresponding": false
            },
            {
                "name": "Shuo Yan",
                "email": "",
                "affiliations": [
                    "Beihang University, Beijing, China",
                    "Beihang University, Beijing, China"
                ],
                "is_corresponding": false
            },
            {
                "name": "Xukun Shen",
                "email": "",
                "affiliations": [
                    "Beihang University, Beijing, China",
                    "Beihang University, Beijing, China"
                ],
                "is_corresponding": false
            }
        ],
        "abstract": "In group art therapy, visualizing emotions through artistic creation is a challenge. In this work, we present an artificial intelligence (AI) visualization system named Metamood, which focuses on helping people automatically generate visual expressions of emotions in group art therapy.To create the exploration method, we conducted the following procedure:1) Using the Russell emotion model and Long Short-Term Memory (LSTM) for emotion classification.2) Creating the relationship between emotions and virtual environment design.3) Proposing individual and Explicit Shared Emotion visualization methods and deploying them in both Virtual Reality(VR) and Mixed Reality (MR) environments.",
        "has_summary_pdf": true,
        "has_poster_pdf": true,
        "has_image": true
    },
    "v-vis-posters-1037": {
        "event": "VIS Posters",
        "event_prefix": "v-vis-posters",
        "title": "Dynamic Vector Graphics: Enabling Data-Driven Illustrations",
        "uid": "v-vis-posters-1037",
        "discord_channel": "https://discord.com/channels/1286338967674818684/1288497673212788798",
        "authors": [
            {
                "name": "Jordan Riley Benson",
                "email": "jordan.riley.benson@gmail.com",
                "affiliations": [
                    "SAS, Cary, United States"
                ],
                "is_corresponding": true
            },
            {
                "name": "Karl Prewo",
                "email": "",
                "affiliations": [
                    "SAS Institute, Cary, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Rajiv Ramarajan",
                "email": "",
                "affiliations": [
                    "SAS Institute, Cary, United States"
                ],
                "is_corresponding": false
            }
        ],
        "abstract": "A bespoke illustration is often used to grab attention and increase engagement in data communication. However, if the illustration is driven by data and needs to update as that data changes it now requires proficiency with both programming and illustration tools. The resultant gulf of execution has been identified in previous work and addressed with custom vector editing tools and integrations between existing vector editors and libraries like D3. We propose a technique for annotating SVG files and data files to enable dynamic illustrations using only existing no-code tools. We also examine the data mappings needed by such a system and the terms that illustrators use to describe those mappings.",
        "has_summary_pdf": true,
        "has_poster_pdf": true,
        "has_image": true
    },
    "v-vis-posters-1038": {
        "event": "VIS Posters",
        "event_prefix": "v-vis-posters",
        "title": "Designing an Interactive Web-based Rainfall Analysis System",
        "uid": "v-vis-posters-1038",
        "discord_channel": "https://discord.com/channels/1286338967674818684/1288497673212788798",
        "authors": [
            {
                "name": "Dong Hyun Jeong",
                "email": "djeong@udc.edu",
                "affiliations": [
                    "Univ. of the District of Columbia, Washington, United States"
                ],
                "is_corresponding": true
            },
            {
                "name": "Pradeep Behera",
                "email": "",
                "affiliations": [
                    "University of the District of Columbia, Washington, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Brian Higgs",
                "email": "",
                "affiliations": [
                    "University of the District of Columbia, Washington, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Soo-Yeon Ji",
                "email": "",
                "affiliations": [
                    "Bowie State University, Bowie, United States"
                ],
                "is_corresponding": false
            }
        ],
        "abstract": "Analyzing hourly precipitation is crucial for identifying patterns, trends, and anomalies in rainfall, which is essential for designing urban drainage systems, flood forecasting, hydrological modeling, and climate studies. Although conducting an analysis by identifying statistical characteristics of storm events with hourly precipitation data is critical, no effective system is available to provide a precise understanding of storm events by evaluating historical data. This study presents an interactive, web-based statistical rainfall analysis system that compiles all available hourly precipitation data across all U.S. states. It is designed as a web-based visual analytics system, utilizing multiple visualizations with supporting multiple user interaction techniques to facilitate comprehensive and interactive data analysis.",
        "has_summary_pdf": true,
        "has_poster_pdf": true,
        "has_image": true
    },
    "v-vis-posters-1039": {
        "event": "VIS Posters",
        "event_prefix": "v-vis-posters",
        "title": "QuanText: Text Data Visualization in Quantum Computing",
        "uid": "v-vis-posters-1039",
        "discord_channel": "https://discord.com/channels/1286338967674818684/1288497673212788798",
        "authors": [
            {
                "name": "Abu Kaisar Mohammad Masum",
                "email": "abu.cse@diu.edu.bd",
                "affiliations": [
                    "Florida Institute of Technology, Melbourne, United States"
                ],
                "is_corresponding": true
            },
            {
                "name": "Naveed Mahmud",
                "email": "",
                "affiliations": [
                    "Florida Institute of Technology, Melbourne, United States"
                ],
                "is_corresponding": false
            }
        ],
        "abstract": "In the current field of quantum computing, data visualization stands out as a potential game-changer for the upcoming quantum revolution. The relevance and importance of data visualization are highlighted in various scientific domains; there is an existing in-text data visualization within quantum computing. In this work, we propose an approach to address this void, bridging classical and quantum methodologies for effective text data visualization and presenting a promising path for future developments in emerging fields such as quantum machine learning and quantum natural language processing.",
        "has_summary_pdf": true,
        "has_poster_pdf": true,
        "has_image": true
    },
    "v-vis-posters-1042": {
        "event": "VIS Posters",
        "event_prefix": "v-vis-posters",
        "title": "Visual Analytics System for Monitoring Mobile and Wearable Sensing Data Collection Campaigns",
        "uid": "v-vis-posters-1042",
        "discord_channel": "https://discord.com/channels/1286338967674818684/1288497673212788798",
        "authors": [
            {
                "name": "Yugyeong Jung",
                "email": "yugyeong.jung@kaist.ac.kr",
                "affiliations": [
                    "KAIST, Daejeon, Korea, Republic of"
                ],
                "is_corresponding": true
            },
            {
                "name": "Uichin Lee",
                "email": "",
                "affiliations": [
                    "KAIST, Daejeon, Korea, Republic of"
                ],
                "is_corresponding": false
            }
        ],
        "abstract": "The proliferation of mobile and wearable sensing research has amplified the need for in-the-wild data collection to capture and analyze human behaviors. However, data quality can be compromised by factors such as participants turning off devices, failing to respond to surveys, or not wearing sensors. We developed a visualization dashboard to monitor missing data in mobile and wearable data collection campaigns so that researchers can find and handle the problems. The tool utilizes a simple quality metric of item count, along with statistical quality control mechanisms, to help researchers quickly identify and prevent significant missing data issues. The dashboard's feasibility was validated through a real-world field study, demonstrating its utility in highlighting missing data and facilitating researcher intervention.",
        "has_summary_pdf": true,
        "has_poster_pdf": true,
        "has_image": true
    },
    "v-vis-posters-1043": {
        "event": "VIS Posters",
        "event_prefix": "v-vis-posters",
        "title": "Iterative Quantification of Categorical Criteria for Enhanced Job Seeking",
        "uid": "v-vis-posters-1043",
        "discord_channel": "https://discord.com/channels/1286338967674818684/1288497673212788798",
        "authors": [
            {
                "name": "Ba\u015fak Oral",
                "email": "e.oral@uu.nl",
                "affiliations": [
                    "Utrecht University, Utrecht, Netherlands"
                ],
                "is_corresponding": true
            },
            {
                "name": "Robert V\u00f5eras",
                "email": "",
                "affiliations": [
                    "-, Utrecht, Netherlands"
                ],
                "is_corresponding": false
            },
            {
                "name": "Evanthia Dimara",
                "email": "",
                "affiliations": [
                    "Utrecht University, Utrecht, Netherlands"
                ],
                "is_corresponding": false
            }
        ],
        "abstract": "In personal and unstructured multi-criteria decision making (MCDM) contexts such as job seeking, qualitative factors like work culture and team dynamics can be as important as quantitative factors like salary and commute distance. However, most MCDM visualization tools, such as LineUp and ValueCharts, focus on quantitative data, often overlooking qualitative criteria. This gap appears to stem from a lack of studies on real-world decision making tasks. To address this, we conducted in-depth interviews with job seekers, emphasizing the integration and prioritization of qualitative data. After investigating the role of qualitative data in decision making, we introduced and evaluated a tool that extends LineUp\u201a\u00c4\u00f4s features to support qualitative criteria. Our insights underscore the vital role of qualitative data in job seeking, illustrating how visualization design can better accommodate the nuanced preferences inherent in these decision making processes.",
        "has_summary_pdf": true,
        "has_poster_pdf": true,
        "has_image": true
    },
    "v-vis-posters-1044": {
        "event": "VIS Posters",
        "event_prefix": "v-vis-posters",
        "title": "In space, no one (but AI) can hear you scream",
        "uid": "v-vis-posters-1044",
        "discord_channel": "https://discord.com/channels/1286338967674818684/1288497673212788798",
        "authors": [
            {
                "name": "Mathis Brossier",
                "email": "mathis.brossier@liu.se",
                "affiliations": [
                    "LiU Link\u00f6ping Universitet, Norrk\u00f6ping, Sweden"
                ],
                "is_corresponding": true
            },
            {
                "name": "Alexander Bock",
                "email": "",
                "affiliations": [
                    "Link\u00f6ping University, Norrk\u00f6ping, Sweden"
                ],
                "is_corresponding": false
            },
            {
                "name": "Konrad J Sch\u00f6nborn",
                "email": "",
                "affiliations": [
                    "Link\u00f6ping University, Norrk\u00f6ping, Sweden"
                ],
                "is_corresponding": false
            },
            {
                "name": "Tobias Isenberg",
                "email": "",
                "affiliations": [
                    "Inria, Saclay, France"
                ],
                "is_corresponding": false
            },
            {
                "name": "Anders Ynnerman",
                "email": "",
                "affiliations": [
                    "Link\u00f6ping University, Norrk\u00f6ping, Sweden"
                ],
                "is_corresponding": false
            },
            {
                "name": "Lonni Besan\u00e7on",
                "email": "",
                "affiliations": [
                    "Link\u00f6ping University, Norrk\u00f6ping, Sweden"
                ],
                "is_corresponding": false
            }
        ],
        "abstract": "We present our initial work on integrating a conversational agent using a large-language model (LLM) in OpenSpace, to provide conversational-based navigation for astrophysics visualization software. We focus on applications of visualization for education and outreach, where the versatility and intuitiveness of conversational agents can be leveraged to provide engaging and meaningful learning experiences. Visualization benefits from the development of LLMs by leveraging its capability to understand requests in natural language, allowing users to express complex tasks efficiently. Natural Language Interfaces can be combined with more traditional visualization interaction techniques, streamlining real-time interaction and facilitating free data exploration. We thus instructed a voice-controlled GPT-4o LLM to send commands to an OpenSpace instance, effectively providing the LLM with the the ability to steer the visualization software as a museum facilitator would for educational shows. We present our implementation and discuss future possibilities.",
        "has_summary_pdf": true,
        "has_poster_pdf": true,
        "has_image": true
    },
    "v-vis-posters-1045": {
        "event": "VIS Posters",
        "event_prefix": "v-vis-posters",
        "title": "Exploring Large-scale Trajectory Data through 2D Time-space View",
        "uid": "v-vis-posters-1045",
        "discord_channel": "https://discord.com/channels/1286338967674818684/1288497673212788798",
        "authors": [
            {
                "name": "Yumeng Xue",
                "email": "yumeng.xue@uni-konstanz.de",
                "affiliations": [
                    "University of Konstanz, Konstanz, Germany"
                ],
                "is_corresponding": true
            },
            {
                "name": "Patrick Paetzold",
                "email": "",
                "affiliations": [
                    "University of Konstanz, Konstanz, Germany"
                ],
                "is_corresponding": false
            },
            {
                "name": "Bin Chen",
                "email": "",
                "affiliations": [
                    "University of Konstanz, Konstanz, Germany"
                ],
                "is_corresponding": false
            },
            {
                "name": "Rebecca Kehlbeck",
                "email": "",
                "affiliations": [
                    "University of Konstanz, Konstanz, Germany"
                ],
                "is_corresponding": false
            },
            {
                "name": "Yunhai Wang",
                "email": "",
                "affiliations": [
                    "Renmin University of China, Beijing, China"
                ],
                "is_corresponding": false
            },
            {
                "name": "Oliver Deussen",
                "email": "",
                "affiliations": [
                    "University of Konstanz, Konstanz, Germany"
                ],
                "is_corresponding": false
            }
        ],
        "abstract": "Visualizing both temporal and spatial aspects of trajectories on a 2D plane presents challenges due to the need to sacrifice one dimension in a single view. In this work, we try to address these challenges by proposing a 2D multi-view approach that enhances user interaction and comprehension compared to traditional 3D methods like the space-time cube. Our approach emphasizes spatial visualization while enabling interactive selection of regions of interest (ROI) for detailed temporal analysis. Preliminary tests demonstrate the feasibility of our method for large-scale spatio-temporal data, with future work focusing on refining temporal interaction flexibility and direction selection within the spatial view.",
        "has_summary_pdf": true,
        "has_poster_pdf": true,
        "has_image": true
    },
    "v-vis-posters-1048": {
        "event": "VIS Posters",
        "event_prefix": "v-vis-posters",
        "title": "Skeleton: Facilitating collaborative design and development scaffolding of accessible data navigation experiences",
        "uid": "v-vis-posters-1048",
        "discord_channel": "https://discord.com/channels/1286338967674818684/1288497673212788798",
        "authors": [
            {
                "name": "Chieri J Nnadozie",
                "email": "cnnadozi@andrew.cmu.edu",
                "affiliations": [
                    "Carnegie Mellon University, Pittsburgh, United States"
                ],
                "is_corresponding": true
            },
            {
                "name": "Frank Elavsky",
                "email": "",
                "affiliations": [
                    "Carnegie Mellon University, Pittsburgh, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Dominik Moritz",
                "email": "",
                "affiliations": [
                    "Carnegie Mellon University, Pittsburgh, United States"
                ],
                "is_corresponding": false
            }
        ],
        "abstract": "Currently, designing and building accessible data navigation using a tool such as Data Navigator is difficult due to the complexity of knowledge required to specify interaction possibilities using code. Exploring and expressing ideal experiences for end-users becomes time consuming. Our solution, Skeleton, provides a visual user interface canvas (with affordances similar to popular tools such as Figma or tldraw) and automatically generates code based on specified visual design constraints. Our approach enables creators to visually design data navigation structures without having to worry about code while easily adjusting their designs as they see fit. We hypothesize this interface will provide a flexible design experience that facilitates more active collaboration, faster iterations, and clearer communication of prototype ideas through demonstration.",
        "has_summary_pdf": true,
        "has_poster_pdf": true,
        "has_image": true
    },
    "v-vis-posters-1050": {
        "event": "VIS Posters",
        "event_prefix": "v-vis-posters",
        "title": "Game-Based Evaluation of Uncertainty Visualization",
        "uid": "v-vis-posters-1050",
        "discord_channel": "https://discord.com/channels/1286338967674818684/1288497673212788798",
        "authors": [
            {
                "name": "Mahsa Geshvadi",
                "email": "mahsa.geshvadi001@umb.edu",
                "affiliations": [
                    "University of Massachusetts Boston, Boston, United States"
                ],
                "is_corresponding": true
            },
            {
                "name": "Reuben Dorent",
                "email": "",
                "affiliations": [
                    "Harvard Medical School, Boston, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Colin Galvin",
                "email": "",
                "affiliations": [
                    "Brigham and Women's Hospital, Boston, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Nazim Haouchine",
                "email": "",
                "affiliations": [
                    "Inria, Strasbourg, France"
                ],
                "is_corresponding": false
            },
            {
                "name": "Tina Kapur",
                "email": "",
                "affiliations": [
                    "Brigham and Women's Hospital, Boston, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Steve Pieper PhD",
                "email": "",
                "affiliations": [
                    "Isomics, Inc., Cambridge, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "William Wells",
                "email": "",
                "affiliations": [
                    "Brigham and Women's Hospital, Boston, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Alexandra J. Golby",
                "email": "",
                "affiliations": [
                    "Brigham and Women's Hospital, Boston, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Daniel Haehn",
                "email": "",
                "affiliations": [
                    "University of Massachusetts Boston, Boston, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Sarah Frisken",
                "email": "",
                "affiliations": [
                    "Brigham and Women's Hospital, Boston, United States"
                ],
                "is_corresponding": false
            }
        ],
        "abstract": "Uncertainty visualization has recently raised significant attention within visualization communities, as it could allow for more informed decision-making. While various uncertainty visualization methods have been proposed, their evaluation is challenging and often does not capture the practical effect on decision-making. This study introduces a novel game-based evaluation approach to assess the effectiveness of uncertainty visualization on decision-making. In this paper, we describe a game to benchmark various uncertainty visualization techniques developed for brain tumor surgery. The results of our study with domain experts show that the simulation enhances the perception and comprehension of uncertainty. Participants' decisions, free from harmful consequences for patients, allow them to explore and understand the outcomes of different choices. Our findings indicate that participants performed better and scored higher with uncertainty visualization than without.",
        "has_summary_pdf": true,
        "has_poster_pdf": true,
        "has_image": true
    },
    "v-vis-posters-1053": {
        "event": "VIS Posters",
        "event_prefix": "v-vis-posters",
        "title": "scFlowVis: Streamlining scRNA-seq Analysis through Visual Design",
        "uid": "v-vis-posters-1053",
        "discord_channel": "https://discord.com/channels/1286338967674818684/1288497673212788798",
        "authors": [
            {
                "name": "Yiwen Xing",
                "email": "yiwen.xing@kcl.ac.uk",
                "affiliations": [
                    "King's College London, London, United Kingdom"
                ],
                "is_corresponding": true
            },
            {
                "name": "stanley Odezi owomero",
                "email": "",
                "affiliations": [
                    "Kings College London, London, United Kingdom"
                ],
                "is_corresponding": false
            },
            {
                "name": "Sophia Tsoka",
                "email": "",
                "affiliations": [
                    "King;s College London, London, United Kingdom"
                ],
                "is_corresponding": false
            },
            {
                "name": "Rita Borgo",
                "email": "",
                "affiliations": [
                    "Kings College London, London, United Kingdom"
                ],
                "is_corresponding": false
            },
            {
                "name": "Alfie Abdul-Rahman",
                "email": "",
                "affiliations": [
                    "King's College London, London, United Kingdom"
                ],
                "is_corresponding": false
            }
        ],
        "abstract": "The rise of single-cell RNA sequencing (scRNA-seq) technologies has increased the need for visual analytical tools. Current visualization tools often cater to specific analysis stages rather than the entire pipeline, and many analyses rely on computational notebooks requiring significant programming skills. To address these challenges, we developed scFlowVis, a web-based visualization application designed in collaboration with bioinformatics and biological experts. scFlowVis supports the entire analysis pipeline for scRNA-seq data, providing visualization and interactive assistance throughout the process without programming. It dynamically links panel functionalities, allowing users to explore data attributes, compare parameters, and document workflows visually. The tool can also recommend appropriate visualizations based on data characteristics. Our experiences highlight the challenges and lessons in developing visualization tools for similar research contexts. scFlowVis has been released on https://github.com/EavanXing0416/scFlowVis.",
        "has_summary_pdf": true,
        "has_poster_pdf": true,
        "has_image": true
    },
    "v-vis-posters-1055": {
        "event": "VIS Posters",
        "event_prefix": "v-vis-posters",
        "title": "Visualization Guardrails: Designing Interventions Against Cherry-Picking in Interactive Data Explorers",
        "uid": "v-vis-posters-1055",
        "discord_channel": "https://discord.com/channels/1286338967674818684/1288497673212788798",
        "authors": [
            {
                "name": "Maxim Lisnic",
                "email": "maxim.lisnic@utah.edu",
                "affiliations": [
                    "University of Utah, Salt Lake City, United States"
                ],
                "is_corresponding": true
            },
            {
                "name": "Zach Cutler",
                "email": "",
                "affiliations": [
                    "University of Utah, Salt Lake City, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Marina Kogan",
                "email": "",
                "affiliations": [
                    "University of Utah, Salt Lake City, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Alexander Lex",
                "email": "",
                "affiliations": [
                    "University of Utah, Salt Lake City, United States"
                ],
                "is_corresponding": false
            }
        ],
        "abstract": "The growing popularity of interactive time series exploration platforms has made visualizing data of public interest more accessible to general audiences. At the same time, the democratized access to professional-looking explorers with preloaded data enables the creation of convincing visualizations with carefully cherry-picked items. Prior research shows that people use data explorers to create and share charts that support their potentially biased or misleading views on public health or economic policy and that such charts have, for example, contributed to the spread of COVID-19 misinformation. Interventions against misinformation have focused on post hoc approaches such as fact-checking or removing misleading content, which are known to be challenging to execute. In this work, we explore whether we can use visualization design to impede cherry-picking---one of the most common methods employed by deceptive charts created on data exploration platforms. We describe a design space of guardrails---interventions against cherry-picking in time series explorers. Using our design space, we create a prototype data explorer with four types of guardrails and conduct two crowd-sourced experiments to evaluate them. Based on our findings, we propose recommendations for developing effective guardrails for visualizations.",
        "has_summary_pdf": true,
        "has_poster_pdf": true,
        "has_image": true
    },
    "v-vis-posters-1056": {
        "event": "VIS Posters",
        "event_prefix": "v-vis-posters",
        "title": "Scholarly Exploration via Conversations with Scholars-Papers Embedding",
        "uid": "v-vis-posters-1056",
        "discord_channel": "https://discord.com/channels/1286338967674818684/1288497673212788798",
        "authors": [
            {
                "name": "Ryan Yen",
                "email": "r4yen@uwaterloo.ca",
                "affiliations": [
                    "University of Waterloo, Waterloo, Canada"
                ],
                "is_corresponding": true
            },
            {
                "name": "Yelizaveta Brus",
                "email": "",
                "affiliations": [
                    "University of Waterloo, Waterloo, Canada"
                ],
                "is_corresponding": false
            },
            {
                "name": "Leyi Yan",
                "email": "",
                "affiliations": [
                    "University of Waterloo, Waterloo, Canada"
                ],
                "is_corresponding": false
            },
            {
                "name": "Jimmy Lin",
                "email": "",
                "affiliations": [
                    "University of Waterloo, Waterloo, Canada"
                ],
                "is_corresponding": false
            },
            {
                "name": "Jian Zhao",
                "email": "",
                "affiliations": [
                    "University of Waterloo, Waterloo, Canada"
                ],
                "is_corresponding": false
            }
        ],
        "abstract": "We propose a system that supports contextually aware, controllable, and interactive exploration of academic publications and scholars. By enabling bidirectional interaction between question-answering components and Scholets, the 2D projections of scholarly works' embeddings, our system enables users to textually and visually interact with large amounts of publications. We report the system design and demonstrate its utility through an exploratory study with graduate researchers.",
        "has_summary_pdf": true,
        "has_poster_pdf": true,
        "has_image": true
    },
    "v-vis-posters-1059": {
        "event": "VIS Posters",
        "event_prefix": "v-vis-posters",
        "title": "Navigating Multi-Attribute Spatial Data Through Layer Toggling and Visibility-Preserving Lenses",
        "uid": "v-vis-posters-1059",
        "discord_channel": "https://discord.com/channels/1286338967674818684/1288497673212788798",
        "authors": [
            {
                "name": "Karelia Alexandra Vilca Salinas",
                "email": "karelia@usp.br",
                "affiliations": [
                    "Universidade de Sao Paulo, S\u00e3o Paulo, Brazil"
                ],
                "is_corresponding": true
            },
            {
                "name": "Jean-Daniel Fekete",
                "email": "",
                "affiliations": [
                    "Inria, Saclay, France",
                    "Universit\u00e9 Paris-Saclay, CNRS, Orsay, France"
                ],
                "is_corresponding": false
            },
            {
                "name": "Luis Gustavo Nonato",
                "email": "",
                "affiliations": [
                    "University of Sao Paulo, Sao Carlos, Brazil"
                ],
                "is_corresponding": false
            }
        ],
        "abstract": "This work introduces two interaction techniques for examining multi-attribute spatial data for visual analytics applications: Layer Toggling and Visibility-Preserving Lenses. With the rise of open city data and services, gathering information at the city scale, like transportation, modeling, and predicting city events such as crime and traffic, has become feasible. Visually assessing the quality of these models requires correlating features, predictions, and ground truth both globally and locally, which is challenging with multi-attribute data due to occlusion. To address this, we introduce Layer Toggling for instantaneously changing layer visibility using a physical button box, allowing comparison of spatially coherent views using retinal persistence. Visibility-preserving lenses dynamically adjust to the density of revealed features, facilitating the exploration of spatial (2D) and quantitative data (1D), such as temporal attributes. We validate our approach on a use case visualizing urban data from S\u221a\u00a3o Paulo city across multiple data layers. Our methods support user exploration and expert analysis tasks, especially in validating and interpreting prediction algorithm outcomes at both global and local scales.",
        "has_summary_pdf": true,
        "has_poster_pdf": true,
        "has_image": true
    },
    "v-vis-posters-1061": {
        "event": "VIS Posters",
        "event_prefix": "v-vis-posters",
        "title": "A Taxonomy for Analyzing Dashboard Design in XR based Content and Data Visualization Tool",
        "uid": "v-vis-posters-1061",
        "discord_channel": "https://discord.com/channels/1286338967674818684/1288497673212788798",
        "authors": [
            {
                "name": "Hyoji Ha",
                "email": "hjha0508@u.sogang.ac.kr",
                "affiliations": [
                    "Sogang University, Seoul, Korea, Republic of"
                ],
                "is_corresponding": true
            },
            {
                "name": "Hyerim Joung",
                "email": "",
                "affiliations": [
                    "Ajou University, Suwon, Korea, Republic of"
                ],
                "is_corresponding": false
            },
            {
                "name": "Sanghun Park",
                "email": "",
                "affiliations": [
                    "Sogang University, Seoul, Korea, Republic of"
                ],
                "is_corresponding": false
            }
        ],
        "abstract": "This study presents a taxonomy that can analyze the design methods and components of XR based dashboards according to categories. The dashboard cases utilized in the analysis are design outputs that assist effective interaction for XR content and XR data visualization tool. For the study, after preliminary data investigation and composition of dashboard design categories with domain experts, we constructed a taxonomy by creating detailed sub-elements. Using the constructed taxonomy, we analyzed a total of 30 dashboard cases and derived dashboard design patterns. In the future, this study plans to create an exploration system that can examine the characteristics of dashboard cases and review the design process by utilizing the constructed taxonomy.",
        "has_summary_pdf": true,
        "has_poster_pdf": true,
        "has_image": true
    },
    "v-vis-posters-1062": {
        "event": "VIS Posters",
        "event_prefix": "v-vis-posters",
        "title": "Neighborhood-Preserving Voronoi Treemaps",
        "uid": "v-vis-posters-1062",
        "discord_channel": "https://discord.com/channels/1286338967674818684/1288497673212788798",
        "authors": [
            {
                "name": "Rebecca Kehlbeck",
                "email": "patrick.paetzold@uni-konstanz.de",
                "affiliations": [
                    "University of Konstanz, Konstanz, Germany"
                ],
                "is_corresponding": false
            },
            {
                "name": "Patrick Paetzold",
                "email": "",
                "affiliations": [
                    "University of Konstanz, Konstanz, Germany"
                ],
                "is_corresponding": true
            },
            {
                "name": "Yumeng Xue",
                "email": "",
                "affiliations": [
                    "University of Konstanz, Konstanz, Germany"
                ],
                "is_corresponding": false
            },
            {
                "name": "Bin Chen",
                "email": "",
                "affiliations": [
                    "University of Konstanz, Konstanz, Germany"
                ],
                "is_corresponding": false
            },
            {
                "name": "Yunhai Wang",
                "email": "",
                "affiliations": [
                    "Renmin University of China, Beijing, China"
                ],
                "is_corresponding": false
            },
            {
                "name": "Oliver Deussen",
                "email": "",
                "affiliations": [
                    "University of Konstanz, Konstanz, Germany"
                ],
                "is_corresponding": false
            }
        ],
        "abstract": "Voronoi treemaps are a well-established tool for simultaneously depicting data points and their hierarchical relationships. In addition to the hierarchical structure, information that indicates neighborhood and similarities between data points frequently exists. Examples include shared attributes like borders between countries or contextualized semantic information such as embedding vectors derived from large language models. In this work, we introduce a novel adaptation of the traditional Voronoi treemap algorithm that leverages similarity information to optimize the proximities or neighborhoods of Voronoi cells during initialization and optimization to more accurately reflect their similarities. We demonstrate the practicality of our approach through multiple real-world examples drawn from the domains of infographics and linguistics.",
        "has_summary_pdf": true,
        "has_poster_pdf": true,
        "has_image": true
    },
    "v-vis-posters-1063": {
        "event": "VIS Posters",
        "event_prefix": "v-vis-posters",
        "title": "EpiVECS: Exploring Spatiotemporal Data Using Low-Dimensional Cluster Representations",
        "uid": "v-vis-posters-1063",
        "discord_channel": "https://discord.com/channels/1286338967674818684/1288497673212788798",
        "authors": [
            {
                "name": "Lee Mason",
                "email": "masonlk@nih.gov",
                "affiliations": [
                    "Queen's University, Belfast, United Kingdom",
                    "NIH, Rockville, United States"
                ],
                "is_corresponding": true
            },
            {
                "name": "Bl\u00e1naid Hicks",
                "email": "",
                "affiliations": [
                    "Queen's University Belfast , Belfast , United Kingdom"
                ],
                "is_corresponding": false
            },
            {
                "name": "Jonas S Almeida",
                "email": "",
                "affiliations": [
                    "National Institutes of Health, Rockville, United States"
                ],
                "is_corresponding": false
            }
        ],
        "abstract": "Spatiotemporal data analysis is a critical yet complex task. Methods that simplify the exploration of such data are essential for various purposes, including monitoring and hypothesis generation. In a recent study, we utilized a combination of clustering and dimensionality reduction techniques to spatially visualize patterns in time-series data. We evaluated several such methods to determine their effectiveness based on various validation metrics. Our findings revealed that methods based on k-means clustering generally outperform self-organizing maps on real-world datasets. Additionally, we introduced EpiVECS, an open-source web application that facilitates cluster embedding and exploration of results through interactive visualization. EpiVECS is accessible at https://episphere.github.io/epivecs.",
        "has_summary_pdf": true,
        "has_poster_pdf": true,
        "has_image": true
    },
    "v-vis-posters-1066": {
        "event": "VIS Posters",
        "event_prefix": "v-vis-posters",
        "title": "Towards Glanceable On-Demand AR Conversation Visualization",
        "uid": "v-vis-posters-1066",
        "discord_channel": "https://discord.com/channels/1286338967674818684/1288497673212788798",
        "authors": [
            {
                "name": "Shanna Li Ching Hollingworth",
                "email": "shanna.hollingwor1@ucalgary.ca",
                "affiliations": [
                    "University of Calgary, Calgary, Canada"
                ],
                "is_corresponding": true
            },
            {
                "name": "Wesley Willett",
                "email": "",
                "affiliations": [
                    "University of Calgary, Calgary, Canada"
                ],
                "is_corresponding": false
            }
        ],
        "abstract": "We explore the potential for glanceable real-time conversation timelines in the Augmented Reality (AR) space, with a focus on lightweight and non-intrusive design. In recent years, AR has become an increasingly popular option for conversational support, leading to many contributions in the field. However, there has been less focus on developing lightweight, non-intrusive, on-demand conversation visualizations in AR. It is important that visualizations don't distract from the conversation partner or the conversation itself. Furthermore, the information displayed may not always be relevant to the user at every moment. Therefore, we explore methodologies to dissect and visualize conversation timelines in real time, providing glanceable summarizations and breakdowns of conversations which are easily summoned and dismissed. In our prototyping, we use a combination of Large Language Models (LLMs) and Natural Language Processing (NLP) techniques to recognize and classify changes in topic. To display the visualization, we use the XReal sunglasses as a lightweight unobtrusive headset which could reasonably be worn in everyday life. We identify a unique set of visualization challenges and opportunities related to minimalist conversation visualizations. Following these explorations, we discuss our findings, and provide our initial reflections on various visualization techniques.",
        "has_summary_pdf": true,
        "has_poster_pdf": true,
        "has_image": true
    },
    "v-vis-posters-1067": {
        "event": "VIS Posters",
        "event_prefix": "v-vis-posters",
        "title": "How Do Professionals Use Annotations in Visualizations?",
        "uid": "v-vis-posters-1067",
        "discord_channel": "https://discord.com/channels/1286338967674818684/1288497673212788798",
        "authors": [
            {
                "name": "Md Dilshadur Rahman",
                "email": "dilshadur@sci.utah.edu",
                "affiliations": [
                    "University of Utah, Salt Lake City, United States",
                    "University of Utah, Salt Lake City, United States"
                ],
                "is_corresponding": true
            },
            {
                "name": "Ghulam Jilani Quadri",
                "email": "",
                "affiliations": [
                    "University of Oklahoma, Norman, United States",
                    "University of Oklahoma, Norman, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Paul Rosen",
                "email": "",
                "affiliations": [
                    "University of Utah, Salt Lake City, United States",
                    "University of Utah, Salt Lake City, United States"
                ],
                "is_corresponding": false
            }
        ],
        "abstract": "Annotations are crucial in visualizations, communicating insights and directing attention to key visual elements. Understanding and practicing professional annotation techniques can significantly enhance visualization clarity and effectiveness. Our study analyzes 72 professionally designed static charts with annotations from major US news outlets, using a qualitative approach to identify annotation types, assess their frequency, explore annotation combinations, categorize text quantity, and examine the relationship between chart captions and annotations. The analysis reveals professionals' common strategies: alignment with captions, targeted highlighting, descriptive text, strategic use of multiple annotations, and emphasis on article-related numbers. These findings offer guidance for improving annotation practices, tools, and methodologies, enhancing data comprehension and communication in visualizations.",
        "has_summary_pdf": true,
        "has_poster_pdf": true,
        "has_image": true
    },
    "v-vis-posters-1068": {
        "event": "VIS Posters",
        "event_prefix": "v-vis-posters",
        "title": "Transformer Explainer: Interactive Learning of Text-Generative Models",
        "uid": "v-vis-posters-1068",
        "discord_channel": "https://discord.com/channels/1286338967674818684/1288497673212788798",
        "authors": [
            {
                "name": "Aeree Cho",
                "email": "aeree@gatech.edu",
                "affiliations": [
                    "Georgia Institute of Technology, Atlanta, United States"
                ],
                "is_corresponding": true
            },
            {
                "name": "Grace C. Kim",
                "email": "",
                "affiliations": [
                    "Georgia Institute of Technology, Atlanta, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Alexander Karpekov",
                "email": "",
                "affiliations": [
                    "Georgia Tech, Atlanta, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Alec Helbling",
                "email": "",
                "affiliations": [
                    "Georgia Institute of Technology, Atlanta, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Zijie J. Wang",
                "email": "",
                "affiliations": [
                    "Georgia Tech, Atlanta, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Seongmin Lee",
                "email": "",
                "affiliations": [
                    "Georgia Tech, Atlanta, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Benjamin Hoover",
                "email": "",
                "affiliations": [
                    "IBM Research AI, Cambridge, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Duen Horng (Polo) Chau",
                "email": "",
                "affiliations": [
                    "Georgia Tech, Atlanta, United States"
                ],
                "is_corresponding": false
            }
        ],
        "abstract": "Transformers have revolutionized machine learning, yet their inner workings remain opaque to many. We present Transformer Explainer, an interactive visualization tool designed for non-experts to learn about Transformers through the GPT-2 model. Our tool helps users understand complex Transformer concepts by integrating a model overview and smooth transitions across abstraction levels of math operations and model structures. It runs a live GPT-2 model locally in the user\u201a\u00c4\u00f4s browser, empowering users to experiment with their own input and observe in real-time how the internal components and parameters of the Transformer work together to predict the next tokens. Our tool requires no installation or special hardware, broadening the public\u201a\u00c4\u00f4s access to AI education. Within the first ten days of release, it drew over 60,000 users. Our open-sourced tool is available at https://poloclub.github.io/transformer-explainer/. A video demo is available at https://youtu.be/ECR4oAwocjs.",
        "has_summary_pdf": true,
        "has_poster_pdf": true,
        "has_image": true
    },
    "v-vis-posters-1069": {
        "event": "VIS Posters",
        "event_prefix": "v-vis-posters",
        "title": "Leveraging LLMs to Infer Causality from Visualized Data: Alignments and Deviations from Human Judgments",
        "uid": "v-vis-posters-1069",
        "discord_channel": "https://discord.com/channels/1286338967674818684/1288497673212788798",
        "authors": [
            {
                "name": "Arran Zeyu Wang",
                "email": "zeyuwang@cs.unc.edu",
                "affiliations": [
                    "University of North Carolina-Chapel Hill, Chapel Hill, United States"
                ],
                "is_corresponding": true
            },
            {
                "name": "David Borland",
                "email": "",
                "affiliations": [
                    "UNC-Chapel Hill, Chapel Hill, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "David Gotz",
                "email": "",
                "affiliations": [
                    "University of North Carolina, Chapel Hill, United States"
                ],
                "is_corresponding": false
            }
        ],
        "abstract": "Data visualizations are commonly employed to convey relationships between variables from complex datasets in exploratory data analysis. Recent advancements in Large Language Models (LLMs) have shown surprising performance in assisting data analysis and visualization. In this poster, we investigate the capabilities of LLMs for reasoning about causality between concept pairs in visualized data using line charts, bar charts, and scatterplots. By using LLMs to replicate two human-subject empirical studies about causality judgments, we how their inferences about causality between concept pairs compare to those of humans, both with and without accompanying visualizations showing varying association levels. Our findings indicate that LLMs' causality inferences are more likely to align with human results without visualizations at very high or very low causal ratings, but LLMs are more influenced by low visualized associations and relatively unaffected by high visualized associations.",
        "has_summary_pdf": true,
        "has_poster_pdf": true,
        "has_image": true
    },
    "v-vis-posters-1070": {
        "event": "VIS Posters",
        "event_prefix": "v-vis-posters",
        "title": "A Versatile Collage Visualization Technique",
        "uid": "v-vis-posters-1070",
        "discord_channel": "https://discord.com/channels/1286338967674818684/1288497673212788798",
        "authors": [
            {
                "name": "Zhenyu Wang",
                "email": "lumin.vis@gmail.com",
                "affiliations": [
                    "Shenzhen University, Shenzhen, China",
                    "Shenzhen University, Shenzhen, China"
                ],
                "is_corresponding": false
            },
            {
                "name": "Daniel CohenOr",
                "email": "",
                "affiliations": [
                    "Tel Aviv University, Tel Aviv, Israel"
                ],
                "is_corresponding": false
            },
            {
                "name": "Min Lu",
                "email": "",
                "affiliations": [
                    "Shenzhen University, Shenzhen, China"
                ],
                "is_corresponding": true
            }
        ],
        "abstract": "Collage techniques are commonly used in visualization to organize a collection of geometric shapes, facilitating the representation of visual features holistically, as seen in word clouds or circular packing diagrams. Typically, packing methods rely on object-space optimization techniques, which often necessitate customizing the optimization process to suit the complexity of geometric primitives and the specific application requirements. In this paper, we introduce a versatile image-space collage technique designed to pack geometric elements into a given shape. Leveraging a differential renderer and image-space losses, our optimization process is highly efficient and can easily accommodate various loss functions. We demonstrate the diverse visual expressiveness of our approach across various visualization applications. The project page is https://szuviz.github.io/pixel-space-collage-technique/.",
        "has_summary_pdf": true,
        "has_poster_pdf": true,
        "has_image": true
    },
    "v-vis-posters-1071": {
        "event": "VIS Posters",
        "event_prefix": "v-vis-posters",
        "title": "Exploring the Hierarchical Nature of Visual Comprehension Through the Lens of Individual Differences",
        "uid": "v-vis-posters-1071",
        "discord_channel": "https://discord.com/channels/1286338967674818684/1288497673212788798",
        "authors": [
            {
                "name": "Faraz Naeinian",
                "email": "quadri@ou.edu",
                "affiliations": [
                    "University of Oklahoma, Norman, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Arran Zeyu Wang",
                "email": "",
                "affiliations": [
                    "University of North Carolina-Chapel Hill, Chapel Hill, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Danielle Albers Szafir",
                "email": "",
                "affiliations": [
                    "University of North Carolina-Chapel Hill, Chapel Hill, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Ghulam Jilani Quadri",
                "email": "",
                "affiliations": [
                    "University of Oklahoma, Norman, United States"
                ],
                "is_corresponding": true
            }
        ],
        "abstract": "Visualization facilitates visual comprehension of salient patterns in complex datasets. However, high-level visual comprehension varies significantly based on individuals' backgrounds, for example, education and professional experience, indicating the need to understand the hierarchical nature of visual comprehension. This work explores how visualization comprehension can be hierarchical based on the individual's background. We interviewed ten participants, showing them eight stimuli scatterplots to investigate variation among participants' high-level comprehension. Participants described each of the tested graphs using natural language. The descriptions were coded using axial coding to identify their alignments with the designer's intentions. The results revealed that high-level comprehension could be of four levels: basic reading of the graph, conceptual understanding, common graph knowledge, and statistics and patterns. Our findings show that by understanding individual's differences in comprehension levels, visualization designers can enhance the accessibility and effectiveness of their visualizations, ensuring that they communicate information more effectively to people with diverse backgrounds and expertise levels.",
        "has_summary_pdf": true,
        "has_poster_pdf": true,
        "has_image": true
    },
    "v-vis-posters-1072": {
        "event": "VIS Posters",
        "event_prefix": "v-vis-posters",
        "title": "Audience Reach of Scientific Data Visualizations in Planetarium-Screened Films",
        "uid": "v-vis-posters-1072",
        "discord_channel": "https://discord.com/channels/1286338967674818684/1288497673212788798",
        "authors": [
            {
                "name": "Kalina Borkiewicz",
                "email": "kalina@sci.utah.edu",
                "affiliations": [
                    "Scientific Computing and Imaging Institute, Salt Lake City, United States",
                    "National Center for Supercomputing Applications, Urbana, United States"
                ],
                "is_corresponding": true
            },
            {
                "name": "Eric Jensen",
                "email": "",
                "affiliations": [
                    "University of Illinois at Urbana-Champaign, Urbana, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Yiwen Miao",
                "email": "",
                "affiliations": [
                    "University of Illinois at Urbana-Champaign, Urbana, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Stuart Levy",
                "email": "",
                "affiliations": [
                    "National Center for Supercomputing Applications, Urbana, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "J.P. Naiman",
                "email": "",
                "affiliations": [
                    "University of Illinois at Urbana-Champaign, Urbana, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Jeffrey D Carpenter",
                "email": "",
                "affiliations": [
                    "National Center for Supercomputing Applications, Urbana, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Katherine E. Isaacs",
                "email": "",
                "affiliations": [
                    "The University of Utah, Salt Lake City, United States"
                ],
                "is_corresponding": false
            }
        ],
        "abstract": "Planetarium dome show films combine cinematic scientific visualizations with animations and recorded videos to provide immersive educational experiences worldwide. However, quantifying their global reach presents significant challenges due to the lack of standardized viewership tracking mechanisms across diverse planetarium venues. We present an analysis of the global impact of these shows in planetariums, presenting data regarding four documentary films from a single visualization lab. Specifically, we designed and administered a viewership survey of four long-running planetarium shows that contained cinematic scientific visualizations. Reported survey data shows that between 1.2 - 2.6 million people have viewed these four films across the 68 responding planetariums. When we include estimates and extrapolate for the 315 planetariums that licensed these shows, we arrive at an estimate of 16.5 - 24.1 million people having seen these films.",
        "has_summary_pdf": true,
        "has_poster_pdf": true,
        "has_image": true
    },
    "v-vis-posters-1073": {
        "event": "VIS Posters",
        "event_prefix": "v-vis-posters",
        "title": "LLM Attributor: Interactive Visual Attribution for LLM Generation",
        "uid": "v-vis-posters-1073",
        "discord_channel": "https://discord.com/channels/1286338967674818684/1288497673212788798",
        "authors": [
            {
                "name": "Seongmin Lee",
                "email": "seongmin@gatech.edu",
                "affiliations": [
                    "Georgia Tech, Atlanta, United States"
                ],
                "is_corresponding": true
            },
            {
                "name": "Zijie J. Wang",
                "email": "",
                "affiliations": [
                    "Georgia Tech, Atlanta, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Aishwarya Chakravarthy",
                "email": "",
                "affiliations": [
                    "Georgia Institute of Technology, Atlanta, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Alec Helbling",
                "email": "",
                "affiliations": [
                    "Georgia Institute of Technology, Atlanta, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "ShengYun Peng",
                "email": "",
                "affiliations": [
                    "Georgia Institute of Technology, Atlanta, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Mansi Phute",
                "email": "",
                "affiliations": [
                    "Georgia Institute of Technology, Atlanta, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Duen Horng (Polo) Chau",
                "email": "",
                "affiliations": [
                    "Georgia Tech, Atlanta, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Minsuk Kahng",
                "email": "",
                "affiliations": [
                    "Google, Atlanta, United States"
                ],
                "is_corresponding": false
            }
        ],
        "abstract": "While large language models (LLMs) have shown remarkable capability to generate convincing text across diverse domains, concerns around its potential risks have highlighted the importance of understanding the rationale behind text generation. We present LLM Attributor, a Python library that provides interactive visualizations for training data attribution of an LLM\u201a\u00c4\u00f4s text generation. Our library offers a new way to quickly attribute an LLM\u201a\u00c4\u00f4s text generation to training data points to inspect model behaviors, enhance its trustworthiness, and compare model-generated text with user-provided text. Thanks to LLM Attributor's broad support for computational notebooks, users can easily integrate it into their workflow to interactively visualize attributions of their models. For easier access and extensibility, we open-source LLM Attributor at https://github.com/poloclub/LLM-Attribution.",
        "has_summary_pdf": true,
        "has_poster_pdf": true,
        "has_image": true
    },
    "v-vis-posters-1074": {
        "event": "VIS Posters",
        "event_prefix": "v-vis-posters",
        "title": "LLM Assisted Analysis of Text-Embedding Visualizations",
        "uid": "v-vis-posters-1074",
        "discord_channel": "https://discord.com/channels/1286338967674818684/1288497673212788798",
        "authors": [
            {
                "name": "Allen Detmer",
                "email": "allen.detmer@gmail.com",
                "affiliations": [
                    "University of Cincinnati, Cincinnati, United States"
                ],
                "is_corresponding": true
            },
            {
                "name": "Raj K Bhatnagar",
                "email": "",
                "affiliations": [
                    "University of Cincinnati, Cincinnati, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Jillian Aurisano",
                "email": "",
                "affiliations": [
                    "University of Cincinnati, Cincinnati, United States"
                ],
                "is_corresponding": false
            }
        ],
        "abstract": "Dimensionality reduction is a widely adopted tool in Natural Language Processing (NLP). Techniques such as Uniform Manifold Approximation and Projection (UMAP) transform high-dimensional embeddings of text data into a lower-dimensional space for visualization. Two-dimensional plots of these embeddings aid in developing insights into model performance. To make sense of these plots, users need to inspect the underlying text represented by the points which can be time-consuming and cognitively intensive. To address this challenge, we developed a novel approach for summarizing and analyzing data behind user selections in text embedding plots. Our interactive approach involves allowing the user to make selections on the text embedding and then utilizing a large-language model (LLM) for: getting a quick overview of the selection, identifying instances of miss-classification, understanding text data within a mixed-class selection, and suggesting additional labels that better fit the underlying text. We implemented our approach in a prototype application, Text-Embedding Selection Sidekick (TESS), and present our initial results.",
        "has_summary_pdf": true,
        "has_poster_pdf": true,
        "has_image": true
    },
    "v-vis-posters-1075": {
        "event": "VIS Posters",
        "event_prefix": "v-vis-posters",
        "title": "CrowdAloud: A Platform for Crowd-Sourced Think-Aloud Studies",
        "uid": "v-vis-posters-1075",
        "discord_channel": "https://discord.com/channels/1286338967674818684/1288497673212788798",
        "authors": [
            {
                "name": "Zach Cutler",
                "email": "zcutler@sci.utah.edu",
                "affiliations": [
                    "University of Utah, Salt Lake City, United States"
                ],
                "is_corresponding": true
            },
            {
                "name": "Lane Harrison",
                "email": "",
                "affiliations": [
                    "Worcester Polytechnic Institute, Worcester, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Carolina Nobre",
                "email": "",
                "affiliations": [
                    "University of Toronto, Toronto, Canada"
                ],
                "is_corresponding": false
            },
            {
                "name": "Alexander Lex",
                "email": "",
                "affiliations": [
                    "University of Utah, Salt Lake City, United States"
                ],
                "is_corresponding": false
            }
        ],
        "abstract": "The think-aloud protocol has long been recognized as a useful tool for conducting user studies. Crowd-sourced studies likely would also benefit from think-aloud, but the increased technical overhead, as well as the uncertainty of how participants may react to being asked to speak without a researcher present, have restricted think-aloud to in-person studies. To address this problem, we introduce CrowdAloud, a user study platform for creating and analyzing crowd-sourced think-aloud studies. Utilizing audio and task-level provenance data, CrowdAloud provides the necessary conditions for think-aloud studies to be run as well as a qualitative analysis conducted on the results, without the technical overhead involved with an ad-hoc solution. To evaluate the comparability of in-person and crowd-sourced studies, we conduct two identical think aloud studies, one in-person and one online with crowd-sourced participants. Based on our results, we discuss implications of crowd-sourced think-aloud studies.",
        "has_summary_pdf": true,
        "has_poster_pdf": true,
        "has_image": true
    },
    "v-vis-posters-1076": {
        "event": "VIS Posters",
        "event_prefix": "v-vis-posters",
        "title": "Enhancing Accessibility of UpSet Plots with Text Descriptions",
        "uid": "v-vis-posters-1076",
        "discord_channel": "https://discord.com/channels/1286338967674818684/1288497673212788798",
        "authors": [
            {
                "name": "Ishrat Jahan Eliza",
                "email": "ishratjahan.eliza@utah.edu",
                "affiliations": [
                    "University of Utah, Salt Lake City, United States"
                ],
                "is_corresponding": true
            },
            {
                "name": "Jake Wagoner",
                "email": "",
                "affiliations": [
                    "University of Utah, Salt Lake City, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Jack Wilburn",
                "email": "",
                "affiliations": [
                    "University of Utah, Salt Lake City, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Nate Lanza",
                "email": "",
                "affiliations": [
                    "Scientific Computing and Imaging Institute, Salt Lake City, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Daniel Hajas",
                "email": "",
                "affiliations": [
                    "University College London, London, United Kingdom"
                ],
                "is_corresponding": false
            },
            {
                "name": "Alexander Lex",
                "email": "",
                "affiliations": [
                    "University of Utah, Salt Lake City, United States"
                ],
                "is_corresponding": false
            }
        ],
        "abstract": "Data visualizations are typically not accessible to blind and low-vision users. The most widely used remedy for making data visualizations accessible is text descriptions. Yet, manually creating useful text descriptions is often omitted by visualization authors, either because of a lack of awareness or a perceived burden. Automatically generated text descriptions are a potential partial remedy. However, with current methods it is unfeasible to create text descriptions for complex scientific charts. In this poster, we describe our methods for generating text descriptions for one complex scientific visualization: the UpSet plot. UpSet is a widely used technique for the visualization and analysis of sets and their intersections. At the same time, UpSet is arguably unfamiliar to novices and used mostly in scientific contexts. Generating text descriptions for UpSet plots is challenging because the patterns observed in UpSet plots have not been studied. We first analyze patterns present in dozens of published UpSet plots. We then introduce software that generates text descriptions for UpSet plots based on the patterns present in the chart. Finally, we introduce a web service that generates text descriptions based on a specification of an UpSet plot, and demonstrate its use in both an interactive web-based implementation and a static Python implementation of UpSet.",
        "has_summary_pdf": true,
        "has_poster_pdf": true,
        "has_image": true
    },
    "v-vis-posters-1077": {
        "event": "VIS Posters",
        "event_prefix": "v-vis-posters",
        "title": "GASP: A Gradient-Aware Shortest Path Algorithm for Boundary-Confined Visualization of 3D Reeb Graphs",
        "uid": "v-vis-posters-1077",
        "discord_channel": "https://discord.com/channels/1286338967674818684/1288497673212788798",
        "authors": [
            {
                "name": "Sefat E Rahman",
                "email": "sefat.rahman@utah.edu",
                "affiliations": [
                    "University of Utah, Salt Lake City, United States",
                    "University of Utah, Salt Lake City, United States"
                ],
                "is_corresponding": true
            },
            {
                "name": "Tushar M. Athawale",
                "email": "",
                "affiliations": [
                    "Oak Ridge National Laboratory, Oak Ridge, United States",
                    "Oak Ridge National Laboratory, Oak Ridge, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Paul Rosen",
                "email": "",
                "affiliations": [
                    "University of Utah, Salt Lake City, United States",
                    "University of Utah, Salt Lake City, United States"
                ],
                "is_corresponding": false
            }
        ],
        "abstract": "Reeb graphs are a topological analysis tool useful for abstracting data across various fields, e.g., medical imaging and geography. The Topology ToolKit (TTK) is a popular library for topological data analysis and visualization, including Reeb graphs. However, TTK\u201a\u00c4\u00f4s visualization method struggles to accurately represent Reeb graph arcs. This paper presents a new algorithm that improves Reeb graph visualizations by ensuring arcs stay within model boundaries, follow the shortest path between critical points, and align with the gradient of the elevation function. Qualitative and quantitative eval- uations demonstrate significant improvements.",
        "has_summary_pdf": true,
        "has_poster_pdf": true,
        "has_image": true
    },
    "v-vis-posters-1078": {
        "event": "VIS Posters",
        "event_prefix": "v-vis-posters",
        "title": "Exploring Global Ecosystem Variation through GEDI waveforms",
        "uid": "v-vis-posters-1078",
        "discord_channel": "https://discord.com/channels/1286338967674818684/1288497673212788798",
        "authors": [
            {
                "name": "Ziang Liu",
                "email": "ziang_liu@brown.edu",
                "affiliations": [
                    "Brown University, Providence, United States"
                ],
                "is_corresponding": true
            },
            {
                "name": "James Tompkin",
                "email": "",
                "affiliations": [
                    "Brown University, Providence, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Matthew Harrison",
                "email": "",
                "affiliations": [
                    "Brown University, Providence, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "James R. Kellner",
                "email": "",
                "affiliations": [
                    "Brown University, Providence, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "David H. Laidlaw",
                "email": "",
                "affiliations": [
                    "Brown University, Providence, United States"
                ],
                "is_corresponding": false
            }
        ],
        "abstract": "Ecosystems are important for maintaining the planet's ecological balance, supporting biodiversity, and providing resources. The Global Ecosystem Dynamics Investigation (GEDI) mission uses space lidar to measure the 3D structure of global vegetation. The observations from GEDI, in the form of waveforms, hold the potential to reveal the spatial patterns and variation of ecosystem structures. However, it is difficult to understand this data due to its multivariate nature and random spatial sampling. Here, we use machine learning to develop a binning-based technique to visualize the spatial variation of large quantities of GEDI waveforms. This method can highlight the transition of different ecosystems through sharp changes in waveform distribution. In addition, it can reveal the internal heterogeneity of defined ecoregions. We expect this method to be helpful in studies of global ecosystems and generalize to other remote sensing data.",
        "has_summary_pdf": true,
        "has_poster_pdf": true,
        "has_image": true
    },
    "v-vis-posters-1079": {
        "event": "VIS Posters",
        "event_prefix": "v-vis-posters",
        "title": "AdaptLIL: A Gaze-Adaptive Visualization for Ontology Mapping",
        "uid": "v-vis-posters-1079",
        "discord_channel": "https://discord.com/channels/1286338967674818684/1288497673212788798",
        "authors": [
            {
                "name": "Nicholas Chow",
                "email": "nickrjchow@gmail.com",
                "affiliations": [
                    "California State University, Long Beach, Long Beach, United States"
                ],
                "is_corresponding": true
            },
            {
                "name": "Bo Fu",
                "email": "",
                "affiliations": [
                    "California State University, Long Beach, Long Beach, United States"
                ],
                "is_corresponding": false
            }
        ],
        "abstract": "This poster showcases AdaptLIL, a real-time adaptive link-indented list ontology mapping visualization that uses eye gaze as the primary input source. Through a multimodal combination of real-time systems, deep learning, and web development applications, this system uniquely curtails graphical overlays (adaptations) to pairwise mappings of link-indented list ontology visualizations for individual users based solely on their eye gaze.",
        "has_summary_pdf": true,
        "has_poster_pdf": true,
        "has_image": true
    },
    "v-vis-posters-1082": {
        "event": "VIS Posters",
        "event_prefix": "v-vis-posters",
        "title": "Comixplain: Comics on Visualization Foundations in Higher Education",
        "uid": "v-vis-posters-1082",
        "discord_channel": "https://discord.com/channels/1286338967674818684/1288497673212788798",
        "authors": [
            {
                "name": "Magdalena Boucher",
                "email": "magdalena.boucher@fhstp.ac.at",
                "affiliations": [
                    "St. P\u00f6lten University of Applied Sciences, St. P\u00f6lten, Austria"
                ],
                "is_corresponding": true
            },
            {
                "name": "Christina Stoiber",
                "email": "",
                "affiliations": [
                    "St. Poelten University of Applied Sciences, St. Poelten, Austria"
                ],
                "is_corresponding": false
            },
            {
                "name": "Alena Boucher",
                "email": "",
                "affiliations": [
                    "Institute of CreativeMedia/Technologies, St. P\u00f6lten, Austria",
                    "Austrian Computer Society, Vienna, Austria"
                ],
                "is_corresponding": false
            },
            {
                "name": "Hsiang-Yun Wu",
                "email": "",
                "affiliations": [
                    "St. P\u00f6lten University of Applied Sciences, St. P\u00f6lten, Austria"
                ],
                "is_corresponding": false
            },
            {
                "name": "Wolfgang Aigner",
                "email": "",
                "affiliations": [
                    "St. Poelten University of Applied Sciences, St. Poelten, Austria"
                ],
                "is_corresponding": false
            },
            {
                "name": "Victor Adriel de Jesus Oliveira",
                "email": "",
                "affiliations": [
                    "St. Poelten University of Applied Sciences, St. Poelten, Austria"
                ],
                "is_corresponding": false
            }
        ],
        "abstract": "Since visual assets are powerful tools for facilitating comprehension, educational resources, such as slides, videos, and textbooks, employ charts and visual metaphors to convey complex concepts. However, highly visual media like comics remain underused in higher education. To address this gap, we introduce Comixplain, a toolkit designed to enhance the learning of academic subjects through comics, specifically applied in visualization education. Through surveys, workshops, and focus groups, we identified key requirements that informed the creation of all assets. Two comics were subsequently integrated into two data visualization courses and evaluated with students and lecturers.",
        "has_summary_pdf": true,
        "has_poster_pdf": true,
        "has_image": true
    },
    "v-vis-posters-1083": {
        "event": "VIS Posters",
        "event_prefix": "v-vis-posters",
        "title": "Towards Metrics for Evaluating Creativity in Visualisation Design",
        "uid": "v-vis-posters-1083",
        "discord_channel": "https://discord.com/channels/1286338967674818684/1288497673212788798",
        "authors": [
            {
                "name": "Aron E. Owen",
                "email": "aron.e.owen@bangor.ac.uk",
                "affiliations": [
                    "Bangor University, Bangor, United Kingdom"
                ],
                "is_corresponding": true
            },
            {
                "name": "Jonathan C Roberts",
                "email": "",
                "affiliations": [
                    "Bangor University, Bangor, United Kingdom"
                ],
                "is_corresponding": false
            }
        ],
        "abstract": "Creativity in visualisation design is essential for designers and data scientists who need to present data in innovative ways. It is often achieved through sketching or drafting low-fidelity prototypes. However, judging this innovation is often difficult. A creative visualisation test would offer a structured approach to enhancing visual thinking and design skills, which are vital across many fields. Such a test can facilitate objective evaluation, skill identification, benchmarking, fostering innovation, and improving learning outcomes. In developing such a test, we propose focusing on four criteria: Quantity, Correctness, Novelty, and Feasibility. These criteria integrate into a test that is easy to administer. We name it the Rowen Test of Creativity in Visualisation Design; We introduce the test, scoring system and results from using eight visualisation experts.",
        "has_summary_pdf": true,
        "has_poster_pdf": true,
        "has_image": true
    },
    "v-vis-posters-1084": {
        "event": "VIS Posters",
        "event_prefix": "v-vis-posters",
        "title": "UniDistriVis: Univariate Distribution All in One",
        "uid": "v-vis-posters-1084",
        "discord_channel": "https://discord.com/channels/1286338967674818684/1288497673212788798",
        "authors": [
            {
                "name": "Yichong Wang",
                "email": "pinkr1veroops@gmail.com",
                "affiliations": [
                    "Zhejiang University, Hangzhou, China"
                ],
                "is_corresponding": true
            },
            {
                "name": "Tan Zhou",
                "email": "",
                "affiliations": [
                    "Shanghai Jiao Tong University, Shanghai, China"
                ],
                "is_corresponding": false
            },
            {
                "name": "Yanhao Zhu",
                "email": "",
                "affiliations": [
                    "Fudan University, Shanghai, China"
                ],
                "is_corresponding": false
            }
        ],
        "abstract": "We present UniDistriVis, an innovative in-browser visualization tool designed to aid users in understanding and analyzing the probability density function (PDF) and cumulative distribution function (CDF) of discrete and continuous random variables and querying the shape, formula, and parameter meaning of all univariate distributions. While many existing tools such as Matlab and Python can plot an univariate distribution figure, UniDistriVis can realize it by dragging sliders without coding, which can be applicable to a variety of education and data analysis scenarios. Meanwhile, UniDistriVis is capable of anchoring parameters with a fixed random variable, visually demonstrating the impact of these parameter changes within a single graph. Furthermore, Unidistrivis uses its rich univariate relationship formulas to fit the user's own data to find the most suitable data distribution type. UniDistriVis is open-sourced at https://github.com/PinkR1ver/univariate-distribution-relationships and runs in all modern web browsers. A demo of the tool in action is available at: https://unidistrivis.streamlit.app/",
        "has_summary_pdf": true,
        "has_poster_pdf": true,
        "has_image": true
    },
    "v-vis-posters-1087": {
        "event": "VIS Posters",
        "event_prefix": "v-vis-posters",
        "title": "Aiding Humans in Financial Fraud Decision Making: Toward an XAI-Visualization Framework",
        "uid": "v-vis-posters-1087",
        "discord_channel": "https://discord.com/channels/1286338967674818684/1288497673212788798",
        "authors": [
            {
                "name": "Angelos Chatzimparmpas",
                "email": "a.chatzimparmpas@uu.nl",
                "affiliations": [
                    "Utrecht University, Utrecht, Netherlands"
                ],
                "is_corresponding": true
            },
            {
                "name": "Evanthia Dimara",
                "email": "",
                "affiliations": [
                    "Utrecht University, Utrecht, Netherlands"
                ],
                "is_corresponding": false
            }
        ],
        "abstract": "AI prevails in financial fraud detection and decision making. Yet, due to concerns about biased automated decision making or profiling, regulations mandate that final decisions are made by humans. Financial fraud investigators face the challenge of manually synthesizing vast amounts of unstructured information, including AI alerts, transaction histories, social media insights, and governmental laws. Current Visual Analytics (VA) systems primarily support isolated aspects of this process, such as explaining binary AI alerts and visualizing transaction patterns, thus adding yet another layer of information to the overall complexity. In this work, we propose a framework where the VA system supports decision makers throughout all stages of financial fraud investigation, including data collection, information synthesis, and human criteria iteration. We illustrate how VA can claim a central role in AI-aided decision making, ensuring that human judgment remains in control while minimizing potential biases and labor-intensive tasks.",
        "has_summary_pdf": true,
        "has_poster_pdf": true,
        "has_image": true
    },
    "v-vis-posters-1088": {
        "event": "VIS Posters",
        "event_prefix": "v-vis-posters",
        "title": "Examining the Capabilities of LLMs in Interpreting Categorical Encodings from Data Visualizations",
        "uid": "v-vis-posters-1088",
        "discord_channel": "https://discord.com/channels/1286338967674818684/1288497673212788798",
        "authors": [
            {
                "name": "Arran Zeyu Wang",
                "email": "zeyuwang@cs.unc.edu",
                "affiliations": [
                    "University of North Carolina-Chapel Hill, Chapel Hill, United States"
                ],
                "is_corresponding": true
            },
            {
                "name": "Matt-Heun Hong",
                "email": "",
                "affiliations": [
                    "University of North Carolina at Chapel Hill, Chapel Hill, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Danielle Albers Szafir",
                "email": "",
                "affiliations": [
                    "University of North Carolina-Chapel Hill, Chapel Hill, United States"
                ],
                "is_corresponding": false
            }
        ],
        "abstract": "Efficiently communicating information from categorical datasets to users is a crucial goal for data visualization. Recently, Large Language Models (LLMs) have been increasingly used to assist users in performing data analysis. In this poster, we investigated the capabilities of LLMs in interpreting categorical information in visualizations. We conducted two studies to analyze how effectively LLMs can estimate class means in multiclass scatterplots using both color and shape encodings. We found that LLMs struggled to comprehend categorical information with higher numbers of categories, especially compared to robust human performance found in past work. We argue that reasoning about data visualizations with categorical visual encodings remains a challenging task for current language models, highlighting the need for further research in this area.",
        "has_summary_pdf": true,
        "has_poster_pdf": true,
        "has_image": true
    },
    "v-vis-posters-1090": {
        "event": "VIS Posters",
        "event_prefix": "v-vis-posters",
        "title": "ChannelExplorer: Visual Analytics at Activation Channel\u201a\u00c4\u00f4s Granularity",
        "uid": "v-vis-posters-1090",
        "discord_channel": "https://discord.com/channels/1286338967674818684/1288497673212788798",
        "authors": [
            {
                "name": "Md Rahat-uz- Zaman",
                "email": "rahatzamancse@gmail.com",
                "affiliations": [
                    "University of Utah, Salt Lake City, United States"
                ],
                "is_corresponding": true
            },
            {
                "name": "Bei Wang",
                "email": "",
                "affiliations": [
                    "University of Utah, Salt Lake City, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Paul Rosen",
                "email": "",
                "affiliations": [
                    "University of Utah, Salt Lake City, United States"
                ],
                "is_corresponding": false
            }
        ],
        "abstract": "With the widespread use of convolutional layers in various neural networks for vision tasks, understanding their complex structures has become crucial for their optimization. Using our developed open-source visualization tool, ChannelExplorer, this study employs well-known visualization techniques like scatterplots, heatmaps, and similarity matrices to examine activation patterns in model layers. These visualizations help users create class hierarchy in any classification dataset, identify weak activation channels in the model for pruning, and find confusion in the model to classify certain images. We demonstrate this by showing better class labeling of a popular dataset, ImageNet. Additionally, we demonstrate the model speed improvement opportunity by finding weak filters in a popular model, InceptionV3. Finally, we evaluated the effectiveness of the tool with expert users.",
        "has_summary_pdf": true,
        "has_poster_pdf": true,
        "has_image": true
    },
    "v-vis-posters-1091": {
        "event": "VIS Posters",
        "event_prefix": "v-vis-posters",
        "title": "Vispubs.com: A Visualization Publications Repository",
        "uid": "v-vis-posters-1091",
        "discord_channel": "https://discord.com/channels/1286338967674818684/1288497673212788798",
        "authors": [
            {
                "name": "Devin Lange",
                "email": "devin@sci.utah.edu",
                "affiliations": [
                    "University of Utah, Salt Lake City, United States"
                ],
                "is_corresponding": true
            }
        ],
        "abstract": "Data visualization researchers publish in various conferences and journals. This paper will focus on a few of the main venues, VIS, EuroVis, and CHI. In addition to the published manuscript, researchers often create other artifacts related to the work, such as videos, blog posts, and deployed versions of the tools created. Some of these artifacts can be found through search engines, but others are not readily accessible. Additionally, performing a keyword search across visualization publication venues is not currently possible. This project aims to collect publications along with their auxiliary artifacts across venues into a single repository with a user-friendly interface.",
        "has_summary_pdf": true,
        "has_poster_pdf": true,
        "has_image": true
    },
    "v-vis-posters-1092": {
        "event": "VIS Posters",
        "event_prefix": "v-vis-posters",
        "title": "Design Contradictions: Help or Hindrance?",
        "uid": "v-vis-posters-1092",
        "discord_channel": "https://discord.com/channels/1286338967674818684/1288497673212788798",
        "authors": [
            {
                "name": "Aron E. Owen",
                "email": "aron.e.owen@bangor.ac.uk",
                "affiliations": [
                    "Bangor University, Bangor, United Kingdom"
                ],
                "is_corresponding": true
            },
            {
                "name": "Jonathan C Roberts",
                "email": "",
                "affiliations": [
                    "Bangor University, Bangor, United Kingdom"
                ],
                "is_corresponding": false
            }
        ],
        "abstract": "The need for innovative ideas in data visualisation drives us to explore new creative approaches. Combining two or more creative words, particularly those that contradict each other, can positively impact the creative process, sparking novel ideas and designs. As we move towards AI-driven design, an open question arises: do these design contradictions work positively with AI tools? Currently, the answer is no. AI systems, like large language models (LLMs), rely on algorithms that engender similarity, whereas creativity often requires divergence and novelty. This poster initiates a conversation on how to drive AI systems to be more creative and generate new ideas. This research invites us to reconsider traditional design methods and explore new approaches in an AI-driven world. Can we apply the same techniques used in traditional design, like the double diamond model, or do we need new methods for design engineering? How can we quickly design visualisations and craft new ideas with generative AI? This paper seeks to start this critical conversation and offers practical insights into the potential of AI in driving creativity in data visualisation.",
        "has_summary_pdf": true,
        "has_poster_pdf": true,
        "has_image": true
    },
    "v-vis-posters-1093": {
        "event": "VIS Posters",
        "event_prefix": "v-vis-posters",
        "title": "Fostering Creative Visualisation Skills Through Data-Art Exhibitions",
        "uid": "v-vis-posters-1093",
        "discord_channel": "https://discord.com/channels/1286338967674818684/1288497673212788798",
        "authors": [
            {
                "name": "Jonathan C Roberts",
                "email": "j.c.roberts@bangor.ac.uk",
                "affiliations": [
                    "Bangor University, Bangor, United Kingdom"
                ],
                "is_corresponding": true
            }
        ],
        "abstract": "Data-art exhibitions offer a unique and real-world setting to foster creative visualisation skills among students. They serve as a real-world platform for students to display their work, bridging the gap between classroom learning and professional practice. Students must develop a technical solution, grasp the context, and produce work that is appropriate for public presentation. This scenario helps to encourage innovative thinking, engagement with the topic, and helps to enhance technical proficiency. We present our implementation of a data-art exhibition within a computing curriculum, for third-year degree-level students. Students create art-based visualisations from selected datasets and present their work in a public exhibition. We have used this initiative over the course of two academic years with different cohorts, and reflect on its impact on student learning and creativity.",
        "has_summary_pdf": true,
        "has_poster_pdf": true,
        "has_image": true
    },
    "v-vis-posters-1094": {
        "event": "VIS Posters",
        "event_prefix": "v-vis-posters",
        "title": "Exploring Fairness across Many Rankings",
        "uid": "v-vis-posters-1094",
        "discord_channel": "https://discord.com/channels/1286338967674818684/1288497673212788798",
        "authors": [
            {
                "name": "Hilson Shrestha",
                "email": "hilsonshrestha@gmail.com",
                "affiliations": [
                    "Worcester Polytechnic Institute, Worcester, United States"
                ],
                "is_corresponding": true
            },
            {
                "name": "Kathleen Cachel",
                "email": "",
                "affiliations": [
                    "Worcester Polytechnic Institute, Worcester, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Mallak Alkhathlan",
                "email": "",
                "affiliations": [
                    "Worcester Polytechnic Institute, Worcester, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Elke A. Rundensteiner",
                "email": "",
                "affiliations": [
                    "Worcester Polytechnic Institute, Worcester, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Lane Harrison",
                "email": "",
                "affiliations": [
                    "Worcester Polytechnic Institute, Worcester, United States"
                ],
                "is_corresponding": false
            }
        ],
        "abstract": "Analyzing large sets of rankings brings many unique challenges, with existing methods often obscuring key insights and overlooking outlier perspectives. This work explores these challenges through a visualization system, FairSpace, designed to effectively aggregate and visualize consensus patterns within large-scale ranking data. Addressing limitations of existing methods, FairSpace incorporates techniques for handling large numbers of individual rankings, revealing and highlighting similarities and differences across and within rankings, uncovering levels of fairness at multiple levels of aggregation, and identifying potential biases or outliers. Through interactive visualizations and data exploration tools, the system empowers users to understand the reasons behind the fair consensus, fostering transparent and informed decision-making.",
        "has_summary_pdf": true,
        "has_poster_pdf": true,
        "has_image": true
    },
    "v-vis-posters-1095": {
        "event": "VIS Posters",
        "event_prefix": "v-vis-posters",
        "title": "Contrasting Diverse, Probabilistic, and Visualization-Based Data Selection Methods for Visual Analytics",
        "uid": "v-vis-posters-1095",
        "discord_channel": "https://discord.com/channels/1286338967674818684/1288497673212788798",
        "authors": [
            {
                "name": "Hamza Elhamdadi",
                "email": "helhamdadi@umass.edu",
                "affiliations": [
                    "University of South Florida, Tampa, United States"
                ],
                "is_corresponding": true
            },
            {
                "name": "Maliha Tashfia Islam",
                "email": "",
                "affiliations": [
                    "University of Massachusetts Amherst, Amherst, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Subrata Mitra",
                "email": "",
                "affiliations": [
                    "Adobe, Bangalore, India"
                ],
                "is_corresponding": false
            },
            {
                "name": "Iftikhar Ahamath Burhanuddin",
                "email": "",
                "affiliations": [
                    "Adobe Research, Bengaluru, India"
                ],
                "is_corresponding": false
            },
            {
                "name": "Tong Yu",
                "email": "",
                "affiliations": [
                    "Adobe Research, San Jose, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Alexandra Meliou",
                "email": "",
                "affiliations": [
                    "University of Massachusetts Amherst, Amherst, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Cindy Xiong Bearfield",
                "email": "",
                "affiliations": [
                    "Georgia Tech, Atlanta, United States"
                ],
                "is_corresponding": false
            }
        ],
        "abstract": "As datasets increase in size and complexity, query latency and visualization rendering times can increase prohibitively. Analysts often pragmatically resort to performing exploratory analysis over a sampled subset of the data. However, a sampled subset may not accurately reflect key patterns, may miss important values, and may distort trends which are important for accurately completing common visual analytics tasks. Our work provides insights on the tradeoffs of various sampling methods. We conduct two experiments on five large-scale datasets using five sampling methods (two diverse, two probablistic, one visualization-based). The first experiment investigates the subjective ability of these five sampling methods to capture key features of the data. In our second experiment, we empirically measure user performance accuracy in common, low-level analytic tasks using visualizations sampled via the five methods. We evaluate the performance of these methods via three metrics: the accuracy of (1) human response at reading the sampled data values, (2) human perception in using samples to make inferences about the original data, and (3) human perception in using samples to correctly estimate the ground truth.",
        "has_summary_pdf": true,
        "has_poster_pdf": true,
        "has_image": true
    },
    "v-vis-posters-1096": {
        "event": "VIS Posters",
        "event_prefix": "v-vis-posters",
        "title": "Exploring AI-Driven Interactive Chart Transformation and Visualization Creation",
        "uid": "v-vis-posters-1096",
        "discord_channel": "https://discord.com/channels/1286338967674818684/1288497673212788798",
        "authors": [
            {
                "name": "Bijesh Shrestha",
                "email": "bshrestha@wpi.edu",
                "affiliations": [
                    "Worcester Polytechnic Institute, Worcester, United States"
                ],
                "is_corresponding": true
            },
            {
                "name": "Roee Shraga",
                "email": "",
                "affiliations": [
                    "Worcester Polytechnic Institute, Worcester, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Lane Harrison",
                "email": "",
                "affiliations": [
                    "Worcester Polytechnic Institute, Worcester, United States"
                ],
                "is_corresponding": false
            }
        ],
        "abstract": "Charts often hold crucial details not explicitly mentioned in the text of the document. While existing chart question-answering (CQA) systems can generate real-time text and visualizations based on data, they often fail to capture the contextual relationship between text and charts along with the lack of dynamic feedback mechanisms for iterative refinement to create tailored report. In this paper, we introduce and evaluate a prototype workflow for a CQA tool through the use of LLMs to query text and chart data, transform existing charts, and create new visualizations to support tailored report generation. CQA is designed to facilitate the interactive querying of the text and chart data within a document, chart transformation, and generation of new tailored report containing text and new visualizations that are directly derived from the original source documents. An initial testing indicated that, with the prototype, users can derive new visualizations and query for detailed insights and explanations. Testing also identified areas such as function invocation errors and subsequent LLM hallucinations, highlighting areas for improving robustness and reliability.",
        "has_summary_pdf": true,
        "has_poster_pdf": true,
        "has_image": true
    },
    "v-vis-posters-1097": {
        "event": "VIS Posters",
        "event_prefix": "v-vis-posters",
        "title": "SurpriseSync: Visual Exploration for De-biased Choropleth Maps",
        "uid": "v-vis-posters-1097",
        "discord_channel": "https://discord.com/channels/1286338967674818684/1288497673212788798",
        "authors": [
            {
                "name": "Akim Ndlovu",
                "email": "andlovu@wpi.edu",
                "affiliations": [
                    "Worcester Polytechnic Institute, Worcester, United States"
                ],
                "is_corresponding": true
            },
            {
                "name": "Hilson Shrestha",
                "email": "",
                "affiliations": [
                    "Worcester Polytechnic Institute, Worcester, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Evan Peck",
                "email": "",
                "affiliations": [
                    "University of Colorado Boulder, Boulder, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Lane Harrison",
                "email": "",
                "affiliations": [
                    "Worcester Polytechnic Institute, Worcester, United States"
                ],
                "is_corresponding": false
            }
        ],
        "abstract": "Geospatial visualizations such as choropleth maps suffer from many well-documented biases. One recently developed technique, surprise maps, uses Bayesian weighting to reveal map areas that deviate from an expected model. While useful, such techniques can require significant time and expertise on the part of the user for adjusting parameter settings, often requiring code-based approaches. In this paper, we explore the use of coordinated multiple views address these challenges. We describe SurpriseSync, which 1) directly visualizes surprise models and parameters using coordinated multiple views, 2) enables model tuning and encoding adjustment via direct manipulation and interaction with visualizations, and 3) supports the creation and comparison of linked surprise models at multiple scales. We discuss how SurpriseSync aims to empower users to make discoveries in spatial data that are difficult or impossible to identify with choropleth maps or static Surprise Maps alone.",
        "has_summary_pdf": true,
        "has_poster_pdf": true,
        "has_image": true
    },
    "v-vis-posters-1098": {
        "event": "VIS Posters",
        "event_prefix": "v-vis-posters",
        "title": "I Do Not (Completely) Trust Your Data: Towards Visualization Lexicons for Ambiguous and Incomplete Data",
        "uid": "v-vis-posters-1098",
        "discord_channel": "https://discord.com/channels/1286338967674818684/1288497673212788798",
        "authors": [
            {
                "name": "Karly Ross",
                "email": "karly.ross@ucalgary.ca",
                "affiliations": [
                    "University of Calgary, Calgary, Canada"
                ],
                "is_corresponding": true
            },
            {
                "name": "Wesley Willett",
                "email": "",
                "affiliations": [
                    "University of Calgary, Calgary, Canada"
                ],
                "is_corresponding": false
            }
        ],
        "abstract": "We present two challenges associated with our current work on a visual lexicon that expresses data absences and ambiguities. First, most visualization approaches fail to express the potential ambiguity and incompleteness in the data they represent. This can pose a fundamental sense-making and communication challenge in contexts (like community organizing), where official data sources and local knowledge may have little overlap or even disagree. Second, indicating expectations, ambiguity, or contradiction between community and administrative data is likely to increase visualization complexity. This increased complexity poses challenges for accessibility and engagement. We outline our work to create a visual lexicon and address the interactions between these challenges.",
        "has_summary_pdf": true,
        "has_poster_pdf": true,
        "has_image": true
    },
    "v-vis-posters-1099": {
        "event": "VIS Posters",
        "event_prefix": "v-vis-posters",
        "title": "What Makes a Visualization Visually Complex? Exploring Design Features Related to Visual Complexity",
        "uid": "v-vis-posters-1099",
        "discord_channel": "https://discord.com/channels/1286338967674818684/1288497673212788798",
        "authors": [
            {
                "name": "Kylie R. Lin",
                "email": "klin368@gatech.edu",
                "affiliations": [
                    "Georgia Institute of Technology, Atlanta, United States"
                ],
                "is_corresponding": true
            },
            {
                "name": "Sean Sheng-tse Ru",
                "email": "",
                "affiliations": [
                    "Georgia Institute of Technology, Atlanta, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "David N. Rapp",
                "email": "",
                "affiliations": [
                    "Northwestern University, Evanston, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Hui Guan",
                "email": "",
                "affiliations": [
                    "University of Massachusetts, Amherst , Amherst, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Cindy Xiong Bearfield",
                "email": "",
                "affiliations": [
                    "Georgia Tech, Atlanta, United States"
                ],
                "is_corresponding": false
            }
        ],
        "abstract": "Overly complex visualizations have the potential to overwhelm audiences and occlude key takeaways during visualization experiences. While a long line of research in the visualization community has sought to identify 'best design practices' to make visualizations more interpretable, the community has yet to articulate how the presence of specific design features contributes to whether a visualization is deemed overwhelming or overly complex. As an initial exploration into this gap, we augmented the MASSVIS dataset, one of the most comprehensive static visualization datasets to-date, with visual complexity ratings and also novel metadata capturing design features related to text, color, underlying data, and chart types. We examined distributions of visual design features across visualizations in the MASSVIS dataset and compared their effects on complexity. Intuitively, we find that higher quantities of visual elements are associated with higher ratings of complexity. Moreover, we determine that visualizations with certain designs (e.g., diagrams) are associated with higher ratings of complexity as opposed to visualizations without, while other design features are associated with lower ratings (e.g., captions).",
        "has_summary_pdf": true,
        "has_poster_pdf": true,
        "has_image": true
    },
    "v-vis-posters-1100": {
        "event": "VIS Posters",
        "event_prefix": "v-vis-posters",
        "title": "Visual Stenography: Feature Recreation and Preservation in Sketches of Line Charts",
        "uid": "v-vis-posters-1100",
        "discord_channel": "https://discord.com/channels/1286338967674818684/1288497673212788798",
        "authors": [
            {
                "name": "Rifat Ara Proma",
                "email": "u1450373@umail.utah.edu",
                "affiliations": [
                    "University of Utah, Salt Lake City, United States"
                ],
                "is_corresponding": true
            },
            {
                "name": "Michael Correll",
                "email": "",
                "affiliations": [
                    "Northeastern University, Portland, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Ghulam Jilani Quadri",
                "email": "",
                "affiliations": [
                    "University of Oklahoma, Norman, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Paul Rosen",
                "email": "",
                "affiliations": [
                    "University of Utah, Salt Lake City, United States"
                ],
                "is_corresponding": false
            }
        ],
        "abstract": "Line charts can surface many relevant features in time series data, from trends to periodicity to peaks and valleys. However, not every potentially important feature in the data may correspond to a visual feature which readers can detect, attend to, or value. In this work, we perform a mixed-methods study, where participants engage in a visual stenography task in which they re-draw line charts, to solicit information about the visual features that participants believe to be important in line charts and how faithfully and accurately they recreate them. We identified three predominant strategies, whose use correlated with the noise present in the stimuli: the replicators attempted to retain all major features of the line chart; the trend keepers faithfully retained trends but no other features; and the overwhelmed only represented the noise. Further, we found that participants tended to faithfully retain trends and peaks and valleys when these features were present, while periodicity and noise were represented in more qualitative or gestural ways: semantically rather than accurately. These results suggest a need to consider more flexible and human-centric ways of presenting, summarizing, pre-processing, or clustering time series data.",
        "has_summary_pdf": true,
        "has_poster_pdf": true,
        "has_image": true
    },
    "v-vis-posters-1101": {
        "event": "VIS Posters",
        "event_prefix": "v-vis-posters",
        "title": "Generalized Transformation of Earth Science Datasets for 3D Narrative Visualization",
        "uid": "v-vis-posters-1101",
        "discord_channel": "https://discord.com/channels/1286338967674818684/1288497673212788798",
        "authors": [
            {
                "name": "Connor Bleisch",
                "email": "hc0021@uah.edu",
                "affiliations": [
                    "The University of Alabama in Huntsville, Huntsville, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Manil Maskey",
                "email": "",
                "affiliations": [
                    "NASA, Huntsville, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Haeyong Chung",
                "email": "",
                "affiliations": [
                    "University of Alabama in Huntsville, Huntsville, United States"
                ],
                "is_corresponding": true
            }
        ],
        "abstract": "We present a novel transformation method that converts extensive Earth science datasets into various 3D representations for narrative data visualizations. Our method comprises two main components: First, we classify Earth science data by analyzing the spatial and temporal structures of the datasets. This involves determining if the intervals within these dimensions are regular or irregular and integrating domain knowledge about the physical climate phenomena depicted in the data. Second, the transformation functions aim to preserve the accuracy and integrity of the original data representation, conveying both qualitative and quantitative attributes. We applied this method to Earth science datasets and developed a narrative visualization that helps people understand a wildfire scenario.",
        "has_summary_pdf": true,
        "has_poster_pdf": true,
        "has_image": true
    },
    "v-vis-posters-1103": {
        "event": "VIS Posters",
        "event_prefix": "v-vis-posters",
        "title": "Visualizing Large Multiplex Geographic Network Data using a Regionalization Approach",
        "uid": "v-vis-posters-1103",
        "discord_channel": "https://discord.com/channels/1286338967674818684/1288497673212788798",
        "authors": [
            {
                "name": "Clio Andris",
                "email": "clio@gatech.edu",
                "affiliations": [
                    "Georgia Tech, Atlanta, United States"
                ],
                "is_corresponding": true
            },
            {
                "name": "Caglar Koylu",
                "email": "",
                "affiliations": [
                    "University of Iowa, Iowa City, United States Minor Outlying Islands"
                ],
                "is_corresponding": false
            },
            {
                "name": "Mason A Porter",
                "email": "",
                "affiliations": [
                    "University of California, Los Angeles, Los Angeles, United States"
                ],
                "is_corresponding": false
            }
        ],
        "abstract": "We outline a geovisual analytic method that is suited for large multiplex spatial networks (i.e., networks with different edge types). We use a county-to-county network of migrants, commuters, travelers, Facebook friends, and Twitter ties in the contiguous U.S. totaling over 1 billion weighted flows. Our goal is to visualize strong and weak connections in the U.S. To do so, we apply a network community detection algorithm to create communities (i.e., subgraphs) from each of the five input data sets. Then, we use an adjacency matrix of counties to visualize the number of times each pair of adjacent counties is assigned to the same subgraph. The resulting map shows tight-knit regions and large divides in the country and provides an alternative to mapping large origin-destination flows without losing key information.",
        "has_summary_pdf": true,
        "has_poster_pdf": true,
        "has_image": true
    },
    "v-vis-posters-1104": {
        "event": "VIS Posters",
        "event_prefix": "v-vis-posters",
        "title": "CausalSynth: An Interactive Web Application for Synthetic Dataset Generation and Visualization with User-Defined Causal Relationships",
        "uid": "v-vis-posters-1104",
        "discord_channel": "https://discord.com/channels/1286338967674818684/1288497673212788798",
        "authors": [
            {
                "name": "Zhehao Wang",
                "email": "zhehaow24@gmail.com",
                "affiliations": [
                    "University of North Carolina - Chapel Hill, Chapel Hill, United States"
                ],
                "is_corresponding": true
            },
            {
                "name": "Arran Zeyu Wang",
                "email": "",
                "affiliations": [
                    "University of North Carolina-Chapel Hill, Chapel Hill, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "David Borland",
                "email": "",
                "affiliations": [
                    "UNC-Chapel Hill, Chapel Hill, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "David Gotz",
                "email": "",
                "affiliations": [
                    "University of North Carolina, Chapel Hill, United States"
                ],
                "is_corresponding": false
            }
        ],
        "abstract": "Understanding and inferring causal relationships between variables is a fundamental task in visualization and visual analysis. However, it can be challenging to verify inferences of causal relationships from traditional observational data because they often lack a ground truth causal model, complicating the evaluation of visual causal inference tools. To address this challenge, we introduce CausalSynth, an interactive web application designed to generate synthetic datasets from user-defined causal relationships. CausalSynth enables users to define acyclic causal graphs via a user-friendly graphical interface, establish interrelationships between variables, and produce datasets that reflect these desired causal interactions. The application also includes built-in tools for visualizing the generated datasets, facilitating deeper insights into the user-defined causal structure and aiding the validation of the generated data. By providing a user-friendly interface for synthetic data generation and visualization based on ground truth causal models, CausalSynth helps support more meaningful evaluations of visual causal inference technologies.",
        "has_summary_pdf": true,
        "has_poster_pdf": true,
        "has_image": true
    },
    "v-vis-posters-1105": {
        "event": "VIS Posters",
        "event_prefix": "v-vis-posters",
        "title": "Efficiently Crowdsourcing Visual Importance with Punch-Hole Annotation",
        "uid": "v-vis-posters-1105",
        "discord_channel": "https://discord.com/channels/1286338967674818684/1288497673212788798",
        "authors": [
            {
                "name": "Minsuk Chang",
                "email": "jangsus1@snu.ac.kr",
                "affiliations": [
                    "Seoul National University, Seoul, Korea, Republic of"
                ],
                "is_corresponding": true
            },
            {
                "name": "Soohyun Lee",
                "email": "",
                "affiliations": [
                    "Seoul National University, Seoul, Korea, Republic of"
                ],
                "is_corresponding": false
            },
            {
                "name": "Aeri Cho",
                "email": "",
                "affiliations": [
                    "Seoul National University, Seoul, Korea, Republic of"
                ],
                "is_corresponding": false
            },
            {
                "name": "Hyeon Jeon",
                "email": "",
                "affiliations": [
                    "Seoul National University, Seoul, Korea, Republic of"
                ],
                "is_corresponding": false
            },
            {
                "name": "Seokhyeon Park",
                "email": "",
                "affiliations": [
                    "Seoul National University, Seoul, Korea, Republic of"
                ],
                "is_corresponding": false
            },
            {
                "name": "Cindy Xiong Bearfield",
                "email": "",
                "affiliations": [
                    "Georgia Tech, Atlanta, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Jinwook Seo",
                "email": "",
                "affiliations": [
                    "Seoul National University, Seoul, Korea, Republic of"
                ],
                "is_corresponding": false
            }
        ],
        "abstract": "We introduce a novel crowdsourcing method for identifying important areas in graphical images through punch-hole labeling. Traditional methods, such as gaze trackers and mouse-based annotations, which generate continuous data, can be impractical in crowdsourcing scenarios. They require many participants, and the outcome data can be noisy. In contrast, our method first segments the graphical image with a grid and drops a portion of the patches (punch holes). Then, we iteratively ask the labeler to validate each annotation with holes, narrowing down the annotation only having the most important area. This approach aims to reduce annotation noise in crowdsourcing by standardizing the annotations while enhancing labeling efficiency and reliability. Preliminary findings from fundamental charts demonstrate that punch-hole labeling can effectively pinpoint critical regions. This also highlights its potential for broader application in visualization research, particularly in studying large-scale users\u201a\u00c4\u00f4 graphical perception. Our future work aims to enhance the algorithm to achieve faster labeling speed and prove its utility through large-scale experiments.",
        "has_summary_pdf": true,
        "has_poster_pdf": true,
        "has_image": true
    },
    "v-vis-posters-1106": {
        "event": "VIS Posters",
        "event_prefix": "v-vis-posters",
        "title": "Balancing Code Order and Loop Structure in a Control Flow Layout",
        "uid": "v-vis-posters-1106",
        "discord_channel": "https://discord.com/channels/1286338967674818684/1288497673212788798",
        "authors": [
            {
                "name": "Shadmaan Hye",
                "email": "praptishadmaan@gmail.com",
                "affiliations": [
                    "University of Utah, Salt Lake City, United States"
                ],
                "is_corresponding": true
            },
            {
                "name": "Matthew Legendre",
                "email": "",
                "affiliations": [
                    "Lawrence Livermore National Laboratory, Livermore, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Katherine E. Isaacs",
                "email": "",
                "affiliations": [
                    "The University of Utah, Salt Lake City, United States"
                ],
                "is_corresponding": false
            }
        ],
        "abstract": "Investigating program optimizations requires navigation of the thousands of binary code lines and comparing them to their generating source code. Existing tools either focus on the understanding of assembly instructions order or the structure of the control flow. They usually focus on data from a single source file, which is insufficient to analyze large complex binaries. We have designed a new layout to balance the instruction order control flow of the binary code, visualize the loop structure, and handle large complex programs. This new layout is part of the tool DisViz, which correlates source code and assembly instructions. It is also designed for large and more complex multi-source binaries, making the navigation process easier. Our poster presents the system design and layout of DisViz and demonstrates its advantage for large-scale binary program analysis.",
        "has_summary_pdf": true,
        "has_poster_pdf": true,
        "has_image": true
    },
    "v-vis-posters-1107": {
        "event": "VIS Posters",
        "event_prefix": "v-vis-posters",
        "title": "Meet Them Where They Are: An Analysis of Visualization Use in Machine Learning Tutorials and Software Libraries",
        "uid": "v-vis-posters-1107",
        "discord_channel": "https://discord.com/channels/1286338967674818684/1288497673212788798",
        "authors": [
            {
                "name": "Ge Gao",
                "email": "dylancashman@brandeis.edu",
                "affiliations": [
                    "Brandeis University, Waltham, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Yuxuan Xiong",
                "email": "",
                "affiliations": [
                    "Brandeis University, Waltham, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Dylan Cashman",
                "email": "",
                "affiliations": [
                    "Brandeis University, Waltham, United States"
                ],
                "is_corresponding": true
            }
        ],
        "abstract": "Visualization research aims to contribute to the development and use of machine learning by developing novel visual encodings that target needs identified through interviews with expert users. While interviews with experts can help identify specific needs, they may not fully recognize more fundamental interest from broader but less expert communities. More importantly, targeted design studies have limited reach, typically only being deployed to a single target user group. We suggest that there is potential for broader impact in visualization research by reaching out to broader audiences, identifying gaps, and advocating for existing visualization research artifacts. In this work, we analyze publicly available data from machine learning tutorials and public software packages to understand how visualization is being used to explain and interpret machine learning concepts to beginners and learners. We present guidelines for visualization researchers to incorporate their research artifacts into tutorials and open source software packages to improve their impact.",
        "has_summary_pdf": true,
        "has_poster_pdf": true,
        "has_image": true
    },
    "v-vis-posters-1108": {
        "event": "VIS Posters",
        "event_prefix": "v-vis-posters",
        "title": "A replication of visual perception studies with tactile representations of data for visually impaired users",
        "uid": "v-vis-posters-1108",
        "discord_channel": "https://discord.com/channels/1286338967674818684/1288497673212788798",
        "authors": [
            {
                "name": "Areen Khalaila",
                "email": "areenkh@brandeis.edu",
                "affiliations": [
                    "Brandeis University, Waltham, United States"
                ],
                "is_corresponding": true
            },
            {
                "name": "Lane Harrison",
                "email": "",
                "affiliations": [
                    "Worcester Polytechnic Institute, Worcester, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Nam Wook Kim",
                "email": "",
                "affiliations": [
                    "Boston College, Chestnut Hill, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Dylan Cashman",
                "email": "",
                "affiliations": [
                    "Brandeis University, Waltham, United States"
                ],
                "is_corresponding": false
            }
        ],
        "abstract": "New consumer devices with refreshable tactile displays promise to allow visually impaired people to analyze data through tactile representations of graphical visualizations. To understand whether results based on visual perception translate to tactile perception, we present a study replicating the formative study by Cleveland and McGill (1984) on graphical perception to tactile representations suitable for visually impaired users. To assess how tactile graphics can convey complex graphical information, we investigate the effectiveness of tactile data visualizations compared to reported results on visual graphical primitives, examining the accuracy and inference times of visually impaired versus sighted users. We find that visually impaired users interpret simpler tactile formats such as bar charts with significantly greater accuracy and speed than more complex formats like bubble charts.",
        "has_summary_pdf": true,
        "has_poster_pdf": true,
        "has_image": true
    },
    "v-vis-posters-1111": {
        "event": "VIS Posters",
        "event_prefix": "v-vis-posters",
        "title": "Charting Complexity: How Chart Types Relate to Visual Complexity",
        "uid": "v-vis-posters-1111",
        "discord_channel": "https://discord.com/channels/1286338967674818684/1288497673212788798",
        "authors": [
            {
                "name": "Sean Sheng-tse Ru",
                "email": "sru3@gatech.edu",
                "affiliations": [
                    "Georgia Institute of Technology, Atlanta, United States"
                ],
                "is_corresponding": true
            },
            {
                "name": "Kylie R. Lin",
                "email": "",
                "affiliations": [
                    "Georgia Institute of Technology, Atlanta, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "David N. Rapp",
                "email": "",
                "affiliations": [
                    "Northwestern University, Evanston, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Hui Guan",
                "email": "",
                "affiliations": [
                    "University of Massachusetts, Amherst , Amherst, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Cindy Xiong Bearfield",
                "email": "",
                "affiliations": [
                    "Georgia Tech, Atlanta, United States"
                ],
                "is_corresponding": false
            }
        ],
        "abstract": "When studying human perceptions of visualizations, it is important to capture the extent to which a visualization is deemed complex or difficult to parse. To do this, researchers can measure a visualization\u201a\u00c4\u00f4s visual complexity. However, the extent to which ratings of complexity vary depending on the design of a visualization has not yet been studied. We offer an initial exploration into this space by investigating how the chart types used in static visualizations are associated with variations in human ratings of visual complexity. We utilized the MASSVIS dataset of static visualizations, labeling each visualization by the chart types it contains and eliciting visual complexity ratings for each visualization. We find that the spread of visual complexity ratings for a given visualization looks different across chart types \u201a\u00c4\u00ec for example, Grid & Matrix, Line, and Bar charts were most represented in visualizations with less varied ratings, and Text and Circle charts were most represented in visualizations with more varied ratings. These findings suggest that some chart types may elicit a variety of reactions from different audiences when it comes to perceiving visual complexity while others may elicit more consistent perceptions.",
        "has_summary_pdf": true,
        "has_poster_pdf": true,
        "has_image": true
    },
    "v-vis-posters-1112": {
        "event": "VIS Posters",
        "event_prefix": "v-vis-posters",
        "title": "Extracting Visualization Workflows from Versioned Notebooks",
        "uid": "v-vis-posters-1112",
        "discord_channel": "https://discord.com/channels/1286338967674818684/1288497673212788798",
        "authors": [
            {
                "name": "Colin Brown",
                "email": "colinjbrown@niu.edu",
                "affiliations": [
                    "Northern Illinois University, DeKalb, United States"
                ],
                "is_corresponding": true
            },
            {
                "name": "Hamed Alhoori",
                "email": "",
                "affiliations": [
                    "Northern Illinois University , Dekalb , United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Maoyuan Sun",
                "email": "",
                "affiliations": [
                    "University of Massachusetts Dartmouth, Dartmouth, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "David Koop",
                "email": "",
                "affiliations": [
                    "Northern Illinois University, DeKalb, United States"
                ],
                "is_corresponding": false
            }
        ],
        "abstract": "Designing visualizations is very often an iterative process that involves exploration of different visual encodings. While there have been many studies of the design process in different contexts, the lower-level details of code-heavy visualization workflows have been harder to capture. Using exploratory notebooks and higher-level frameworks that facilitate rapid iteration, users can quickly test ideas and examine results. We use publicly-available notebook version histories to study how users work in these environments, observing both how they build new visualizations from existing templates or previous work, and how they refine and enhance visualizations over time. In addition, we study differences between the Observable Plot and Vega Lite visualization frameworks, and compare those code-oriented frameworks with the Observable Chart Cell wizard. Our analysis provides insights into how users build off of existing work including templates, and provides some clues about the use of different visualization properties and encodings, including those that may require more exploration.",
        "has_summary_pdf": true,
        "has_poster_pdf": true,
        "has_image": true
    },
    "v-vis-posters-1113": {
        "event": "VIS Posters",
        "event_prefix": "v-vis-posters",
        "title": "Visual Analysis of Motion for Camouflaged Object Detection",
        "uid": "v-vis-posters-1113",
        "discord_channel": "https://discord.com/channels/1286338967674818684/1288497673212788798",
        "authors": [
            {
                "name": "Debra L Hogue",
                "email": "debra.hogue1981@gmail.com",
                "affiliations": [
                    "University of Oklahoma, Norman, United States"
                ],
                "is_corresponding": true
            },
            {
                "name": "David Shane Elliott",
                "email": "",
                "affiliations": [
                    "University of Oklahoma, Norman, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Chris Weaver",
                "email": "",
                "affiliations": [
                    "University of Oklahoma, Norman, United States"
                ],
                "is_corresponding": false
            }
        ],
        "abstract": "In this poster, we present a work-in-progress application that integrates image processing, composite visual representation, and coordinated interactions into a visualization pipeline to support motion analysis for camouflage detection in captured videos. The application uses the MAE and SSIM image metrics to calculate differences in frames that are visually encoded as foreground and background pixels and/or marks in the composite view. We applied this approach to videos featuring camouflaged wildlife to assess its potential for camouflage object detection (COD), as a step towards developing effective visual motion analysis for camouflage detection. The source code and application are available on GitHub at https://github.com/enjelika/TemporalMotionExtractionAnalysis.",
        "has_summary_pdf": true,
        "has_poster_pdf": true,
        "has_image": true
    },
    "v-vis-posters-1116": {
        "event": "VIS Posters",
        "event_prefix": "v-vis-posters",
        "title": "Knowledge Graph Based Visual Search Application",
        "uid": "v-vis-posters-1116",
        "discord_channel": "https://discord.com/channels/1286338967674818684/1288497673212788798",
        "authors": [
            {
                "name": "Pawandeep Kaur Betz",
                "email": "pawandeep.kaur-betz@dlr.de",
                "affiliations": [
                    "Institute for Software Technologies, German Aerospace Center (DLR), Braunschweig, Germany"
                ],
                "is_corresponding": true
            },
            {
                "name": "Tobias Hecking",
                "email": "",
                "affiliations": [
                    "German Aerospace Center (DLR), Cologne, Germany"
                ],
                "is_corresponding": false
            },
            {
                "name": "Andreas Schreiber",
                "email": "",
                "affiliations": [
                    "German Aerospace Center (DLR), Cologne, Germany"
                ],
                "is_corresponding": false
            },
            {
                "name": "Andreas Gerndt",
                "email": "",
                "affiliations": [
                    "German Aerospace Center (DLR), Braunschweig, Germany",
                    "University of Bremen, Bremen, Germany"
                ],
                "is_corresponding": false
            }
        ],
        "abstract": "The FAIR data principles advocate for making scientific and research datasets findable. Yet, the sheer volume and diversity of these datasets present significant challenges. Despite advancements in data search technologies, techniques for representing search results are still traditional and inadequate, often returning extraneous results. To address these issues, we developed a graph-based visual search application designed to enhance data search for Earth System Scientists. This application utilizes various chart widgets and a knowledge graph at the backend, connecting two disparate data repositories.",
        "has_summary_pdf": true,
        "has_poster_pdf": true,
        "has_image": true
    },
    "v-vis-posters-1118": {
        "event": "VIS Posters",
        "event_prefix": "v-vis-posters",
        "title": "VIRUS: Visualization of Irregular Research Under Scrutiny",
        "uid": "v-vis-posters-1118",
        "discord_channel": "https://discord.com/channels/1286338967674818684/1288497673212788798",
        "authors": [
            {
                "name": "Fabrice FRANK",
                "email": "lonni.besancon@gmail.com",
                "affiliations": [
                    "None, Essaouira, Morocco"
                ],
                "is_corresponding": false
            },
            {
                "name": "Lonni Besan\u00e7on",
                "email": "",
                "affiliations": [
                    "Link\u00f6ping University, Norrk\u00f6ping, Sweden"
                ],
                "is_corresponding": true
            }
        ],
        "abstract": "We present VIRUS, a visualization system to keep records of questionable papers being investigated and their impact on several scholarly measures. This effort, based on an identified set of potentially problematic papers, is meant to help researchers explore what interactive visualization systems may be needed in the future. As the number of retracted papers continues to soar, we believe that such systems will be essential for sleuthing efforts. We thus describe a prototype that we aim to iteratively evaluate with academic sleuths for improvement. All materials (poster, code, follow-up) for this submission are available at https://osf.io/83v2a/.",
        "has_summary_pdf": true,
        "has_poster_pdf": true,
        "has_image": true
    },
    "a-ldav-posters-1702": {
        "event": "LDAV Posters",
        "event_prefix": "a-ldav-posters",
        "title": "High-quality Approximation of Scientific Data using 3D Gaussian Splatting",
        "uid": "a-ldav-posters-1702",
        "discord_channel": "https://discord.com/channels/1286338967674818684/1288497673212788798",
        "authors": [
            {
                "name": "Andres Role Sewell",
                "email": "sewellandres@gmail.com",
                "affiliations": [
                    "Utah State University, Logan, United States",
                    "Argonne National Laboratory, Lemont, United States"
                ],
                "is_corresponding": true
            },
            {
                "name": "Landon Dyken",
                "email": "",
                "affiliations": [
                    "University of Illinois Chicago, Chicago, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Victor A. Mateevitsi",
                "email": "",
                "affiliations": [
                    "Argonne National Laboratory, Lemont, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Will Usher",
                "email": "",
                "affiliations": [
                    "Luminary Cloud, San Mateo, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Jefferson Amstutz",
                "email": "",
                "affiliations": [
                    "NVIDIA, Austin, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Thomas Marrinan",
                "email": "",
                "affiliations": [
                    "University of St. Thomas, St. Paul, United States",
                    "Argonne National Laboratory, Lemont, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Khairi Reda",
                "email": "",
                "affiliations": [
                    "Indiana University, Indianapolis, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Silvio Rizzi",
                "email": "",
                "affiliations": [
                    "Argonne National Laboratory, Chicago, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Joseph Insley",
                "email": "",
                "affiliations": [
                    "Argonne National Laboratory, Lemont, United States",
                    "Northern Illinois University, DeKalb, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Michael E. Papka",
                "email": "",
                "affiliations": [
                    "Argonne National Laboratory, Lemont, United States",
                    "University of Illinois Chicago, Chicago, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "sidharth kumar",
                "email": "",
                "affiliations": [
                    "University of Illinois at Chicago, Chicago, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Steve Petruzza",
                "email": "",
                "affiliations": [
                    "Utah State Unversity, Logan, United States"
                ],
                "is_corresponding": false
            }
        ],
        "abstract": "This work explores the application of recent advancements in radiance field rendering, specifically 3D Gaussian splatting, to generate high-quality approximations of scientific data. In this technique, a 3D Gaussian splatting model is built from a 3D point cloud generated using structure-from-motion, or a randomly initialized one when using a NeRF as input. This point cloud serves as the basis for initializing a set of Gaussian primitives, which are then refined through machine learning to minimize differences between ground truth and rendered images. We modified this pipeline to train Gaussian models directly from scientific data, eliminating the need for structure-from-motion. We test exporting an isosurface as a point cloud, which is then used to train a Gaussian model representing the dataset's isosurface. We also experimented with using a cinema database to produce a 3D Gaussian model; however, this approach yielded less promising results due to sub-optimal point cloud initialization. Our findings highlight the potential of this technique for scientific datasets, suggesting it could enable efficient post-hoc visualization with reduced computational resources.",
        "has_summary_pdf": false,
        "has_poster_pdf": true,
        "has_image": true
    },
    "a-ldav-posters-3766": {
        "event": "LDAV Posters",
        "event_prefix": "a-ldav-posters",
        "title": "Graphical Representation through a User Interface for In Situ Scientific Visualization with Ascent",
        "uid": "a-ldav-posters-3766",
        "discord_channel": "https://discord.com/channels/1286338967674818684/1288497673212788798",
        "authors": [
            {
                "name": "Colleen Heinemann",
                "email": "heinmnn2@illinois.edu",
                "affiliations": [
                    "University of Illinois at Urbana Champaign, Urbana, United States",
                    "Argonne National Laboratory, Lemont, United States"
                ],
                "is_corresponding": true
            },
            {
                "name": "Jefferson Amstutz",
                "email": "",
                "affiliations": [
                    "NVIDIA, Austin, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Joseph Insley",
                "email": "",
                "affiliations": [
                    "Argonne National Laboratory, Lemont, United States",
                    "Northern Illinois University, DeKalb, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Victor A. Mateevitsi",
                "email": "",
                "affiliations": [
                    "Argonne National Laboratory, Lemont, United States",
                    "University of Illinois Chicago, Chicago, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Michael E. Papka",
                "email": "",
                "affiliations": [
                    "Argonne National Laboratory, Lemont, United States",
                    "University of Illinois Chicago, Chicago, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Silvio Rizzi",
                "email": "",
                "affiliations": [
                    "Argonne National Laboratory, Chicago, United States"
                ],
                "is_corresponding": false
            }
        ],
        "abstract": "This work presents a graphical user interface (GUI) designed to al- low users to interactively create scientific visualizations and work- flows with the in situ visualization software, Ascent. Tradition- ally, in situ pipelines are configured through a YAML file, which can be cumbersome. Our GUI offers an alternative by automating the conversion of user inputs into a format that Ascent can process, eliminating the need for manual file editing. This interactive capa- bility increases the potential for a more diverse group of users to leverage Ascent\u2019s powerful visualization tools.",
        "has_summary_pdf": false,
        "has_poster_pdf": true,
        "has_image": true
    },
    "a-ldav-posters-5078": {
        "event": "LDAV Posters",
        "event_prefix": "a-ldav-posters",
        "title": "Identifying Locally Turbulent Vortices within Instabilities",
        "uid": "a-ldav-posters-5078",
        "discord_channel": "https://discord.com/channels/1286338967674818684/1288497673212788798",
        "authors": [
            {
                "name": "Fabien Vivodtzev",
                "email": "vivodtzev@gmail.com",
                "affiliations": [
                    "CEA, Le Barp, France"
                ],
                "is_corresponding": true
            },
            {
                "name": "Florent Nauleau",
                "email": "",
                "affiliations": [
                    "CEA/CESTA, Le Barp, France"
                ],
                "is_corresponding": false
            },
            {
                "name": "Jean-Philippe Braeunig",
                "email": "",
                "affiliations": [
                    "CEA/CESTA, Le Barp, France"
                ],
                "is_corresponding": false
            },
            {
                "name": "Julien Tierny",
                "email": "",
                "affiliations": [
                    "CNRS, Paris, France",
                    "Sorbonne Universit\u00e9, Paris, France"
                ],
                "is_corresponding": false
            }
        ],
        "abstract": "This work presents an approach for the automatic detection of locally turbulent vortices within turbulent 2D flows such as instabilites. First, given a time step of the flow, methods from Topological Data Analysis (TDA) are leveraged to extract the geometry of the vortices. Specifically, the enstrophy of the flow is simplified by topological persistence, and the vortices are extracted by collecting the basins of the simplified enstrophy\u2019s Morse complex. Next, the local kinetic energy power spectrum is computed for each vortex. We introduce a set of indicators based on the kinetic energy power spectrum to estimate the correlation between the vortex\u2019s behavior and that of an idealized turbulent vortex. Our preliminary experiments show the relevance of these indicators for distinguishing vortices which are turbulent from those which have not yet reached a turbulent state and thus known as laminar.",
        "has_summary_pdf": true,
        "has_poster_pdf": false,
        "has_image": true
    },
    "a-ldav-posters-7573": {
        "event": "LDAV Posters",
        "event_prefix": "a-ldav-posters",
        "title": "Exploring Large-Scale Scientific Data in Virtual Reality",
        "uid": "a-ldav-posters-7573",
        "discord_channel": "https://discord.com/channels/1286338967674818684/1288497673212788798",
        "authors": [
            {
                "name": "Idunnuoluwa Adekemi Adeniji",
                "email": "iadeniji@kean.edu",
                "affiliations": [
                    "Kean University , Union , United States",
                    "Argonne National Laboratory , Lemont , United States"
                ],
                "is_corresponding": true
            },
            {
                "name": "Joseph Insley",
                "email": "",
                "affiliations": [
                    "Argonne National Laboratory, Lemont, United States",
                    "Northern Illinois University, DeKalb, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Silvio Rizzi",
                "email": "",
                "affiliations": [
                    "Argonne National Laboratory, Chicago, United States",
                    "Argonne National Laboratory, Chicago, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Michael E. Papka",
                "email": "",
                "affiliations": [
                    "Argonne National Laboratory, Lemont, United States",
                    "University of Illinois Chicago, Chicago, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Victor A. Mateevitsi",
                "email": "",
                "affiliations": [
                    "Argonne National Laboratory, Lemont, United States",
                    "Argonne National Laboratory, Lemont, United States"
                ],
                "is_corresponding": false
            }
        ],
        "abstract": "The conventional method of data exploration primarily relies on 2D and 3D visualization tools. However, with the advent of lower-cost virtual reality (VR) hardware, a transformation is underway. This study presents a novel data pipeline developed from ParaView (Open source Scientific visualization software) to the Unity Game Engine (Cross platform used to develop interactive contents such as games, animations and in this case VR) to investigate this transition. Specifically, we apply VR technology to the exploration of particle-based scientific datasets, focusing on data generated by a Hardware/Hybrid Accelerated Cosmology Code (HACC) simulation.This project applies VR to the identification of patterns and clusters within HACC particle-based datasets.We enable effective user interaction by integrating VR into the broader field of large data exploration, which includes features like data interaction, manipulation, and in-depth analysis. We implement custom interactions to enable interrogation of underlying data streams to provide deeper insight.",
        "has_summary_pdf": false,
        "has_poster_pdf": true,
        "has_image": true
    },
    "a-ldav-posters-7949": {
        "event": "LDAV Posters",
        "event_prefix": "a-ldav-posters",
        "title": "A Customized Validator Recommender System for PoS Networks Using Similarity-Based Circular Visualization",
        "uid": "a-ldav-posters-7949",
        "discord_channel": "https://discord.com/channels/1286338967674818684/1288497673212788798",
        "authors": [
            {
                "name": "Jaeuk Lee",
                "email": "woukl22@ajou.ac.kr",
                "affiliations": [
                    "Ajou University, Suwon, Korea, Republic of"
                ],
                "is_corresponding": true
            },
            {
                "name": "Jisu Kim",
                "email": "",
                "affiliations": [
                    "Ajou University, Suwon, Korea, Republic of"
                ],
                "is_corresponding": false
            },
            {
                "name": "Hyunwoo Han",
                "email": "",
                "affiliations": [
                    "Stamper Co.,Ltd., Suwon, Korea, Republic of"
                ],
                "is_corresponding": false
            },
            {
                "name": "Kyungwon Lee",
                "email": "",
                "affiliations": [
                    "Ajou university, Suwon, Korea, Republic of"
                ],
                "is_corresponding": false
            }
        ],
        "abstract": "This study analyzes the impact of validator behavior on investor rewards in proof-of-stake (PoS) based blockchain networks and proposes a visualization system to assist investors in selecting appropriate validators. This system enables personalized evaluations through five adjustable indicators tailored to the investor's preferences. By utilizing similarity-based circular visualizations and radar charts, it facilitates the selection and comparison of validators. Additionally, it provides time-series data-based line graphs and raw data-based table views to support detailed comparative analysis among validators. The introduction of such a multifaceted evaluation methodology in staking investments is expected to contribute to the formation of user-customized portfolios and the establishment of optimized investment strategies.",
        "has_summary_pdf": true,
        "has_poster_pdf": false,
        "has_image": true
    },
    "a-ldav-posters-8040": {
        "event": "LDAV Posters",
        "event_prefix": "a-ldav-posters",
        "title": "Visuals on the House: Optimizing HPC Workflows with No-Cost CPU Visualization",
        "uid": "a-ldav-posters-8040",
        "discord_channel": "https://discord.com/channels/1286338967674818684/1288497673212788798",
        "authors": [
            {
                "name": "Victor A. Mateevitsi",
                "email": "vmateevitsi@anl.gov",
                "affiliations": [
                    "Argonne National Laboratory, Lemont, United States",
                    "University of Illinois Chicago, Chicago, United States"
                ],
                "is_corresponding": true
            },
            {
                "name": "Andres Role Sewell",
                "email": "",
                "affiliations": [
                    "Utah State University, Logan, United States",
                    "Argonne National Laboratory, Lemont, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Mathis Bode",
                "email": "",
                "affiliations": [
                    "Forschungszentrum J\u00fclich GmbH, J\u00fclich, Germany"
                ],
                "is_corresponding": false
            },
            {
                "name": "Paul Fischer",
                "email": "",
                "affiliations": [
                    "University of Illinois Urbana-Champaign, Urbana-Champaign, United States",
                    "Argonne National Laboratory, Lemont, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Jens Henrik G\u00f6bbert",
                "email": "",
                "affiliations": [
                    "Juelich Supercomputing Centre, Juelich, Germany"
                ],
                "is_corresponding": false
            },
            {
                "name": "Joseph Insley",
                "email": "",
                "affiliations": [
                    "Argonne National Laboratory, Lemont, United States",
                    "Northern Illinois University, DeKalb, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Ioannis Kavroulakis",
                "email": "",
                "affiliations": [
                    "Aristotle University of Thessaloniki, Thessaloniki, Greece"
                ],
                "is_corresponding": false
            },
            {
                "name": "Yu-Hsiang Lan",
                "email": "",
                "affiliations": [
                    "University of Illinois Urbana-Champaign, Urbana-Champaign, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Misun Min",
                "email": "",
                "affiliations": [
                    "Argonne National Laboratory, Lemont, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Michael E. Papka",
                "email": "",
                "affiliations": [
                    "Argonne National Laboratory, Lemont, United States",
                    "University of Illinois Chicago, Chicago, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Steve Petruzza",
                "email": "",
                "affiliations": [
                    "Utah State Unversity, Logan, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Silvio Rizzi",
                "email": "",
                "affiliations": [
                    "Argonne National Laboratory, Chicago, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Ananias Tomboulides",
                "email": "",
                "affiliations": [
                    "Aristotle University of Thessaloniki, Thessaloniki, Greece"
                ],
                "is_corresponding": false
            },
            {
                "name": "Damaskinos Konioris",
                "email": "",
                "affiliations": [
                    "Aristotle University of Thessaloniki, Thessaloniki, Greece"
                ],
                "is_corresponding": false
            },
            {
                "name": "Dimitrios Papageorgiou",
                "email": "",
                "affiliations": [
                    "Aristotle University of Thessaloniki, Thessaloniki, Greece"
                ],
                "is_corresponding": false
            }
        ],
        "abstract": "The rise of heterogeneous resources in modern High Performance Computing (HPC) systems has propelled the scientific community beyond the exascale threshold. To maximize simulation performance on HPCs, applications increasingly rely on device resources, such as GPUs, leading to under-utilization of host resources, particularly CPUs. In situ analysis and visualization techniques minimize data movement by operating on data in-memory, but this still involves blocking operations that incur a small penalty on simulation performance. We explore a novel instrumentation approach where GPU-based timestep data is copied from device memory to host memory, enabling CPUs to concurrently perform visualization and analysis tasks. This strategy allows simulations to continue uninterrupted by an in situ library's analysis and visualization processes.",
        "has_summary_pdf": true,
        "has_poster_pdf": false,
        "has_image": true
    }
}