{
    "v-short-1040": {
        "slot_id": "v-short-1040",
        "session_id": "short0",
        "title": "Data Guards: Challenges and Solutions for Fostering Trust in Data",
        "contributors": [
            "Nicole Sultanum"
        ],
        "authors": [
            {
                "name": "Nicole Sultanum",
                "email": "nicole.sultanum@gmail.com",
                "affiliations": [
                    "Tableau Research, Seattle, United States"
                ],
                "is_corresponding": true
            },
            {
                "name": "Dennis Bromley",
                "email": "bromley.denny@gmail.com",
                "affiliations": [
                    "Tableau Research, Seattle, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Michael Correll",
                "email": "m.correll@northeastern.edu",
                "affiliations": [
                    "Northeastern University, Portland, United States"
                ],
                "is_corresponding": false
            }
        ],
        "abstract": "From dirty data to intentional deception, there are many threats to the validity of data-driven decisions. Making use of data, especially new or unfamiliar data, therefore requires a degree of trust or verification. How is this trust established? In this paper, we present the results of a series of interviews with both producers and consumers of data artifacts (outputs of data ecosystems like spreadsheets, charts, and dashboards) aimed at understanding strategies and obstacles to building trust in data. We find a recurring need, but lack of existing standards, for data validation and verification, especially among data consumers. We therefore propose a set of data guards: methods and tools for fostering trust in data artifacts.",
        "uid": "v-short-1040",
        "time_stamp": "",
        "time_start": "",
        "time_end": "",
        "paper_type": "short",
        "keywords": [],
        "doi": "",
        "fno": "",
        "presentation_mode": "",
        "open_access": false,
        "accessible_pdf": false,
        "has_image": false,
        "has_pdf": false,
        "paper_award": "",
        "image_caption": "",
        "external_paper_link": "",
        "youtube_ff_link": "",
        "youtube_ff_id": "",
        "bunny_ff_link": "",
        "bunny_ff_subtitles": "",
        "youtube_prerecorded_link": "",
        "youtube_prerecorded_id": "",
        "bunny_prerecorded_link": "",
        "bunny_prerecorded_subtitles": ""
    },
    "v-short-1047": {
        "slot_id": "v-short-1047",
        "session_id": "short0",
        "title": "Intuitive Design of Deep Learning Models through Visual Feedback",
        "contributors": [
            "JunYoung Choi"
        ],
        "authors": [
            {
                "name": "JunYoung Choi",
                "email": "juny0603@gmail.com",
                "affiliations": [
                    "VIENCE Inc., Seoul, Korea, Republic of",
                    "Korea University, Seoul, Korea, Republic of"
                ],
                "is_corresponding": true
            },
            {
                "name": "Sohee Park",
                "email": "wings159@vience.co.kr",
                "affiliations": [
                    "VIENCE Inc., Seoul, Korea, Republic of"
                ],
                "is_corresponding": false
            },
            {
                "name": "GaYeon Koh",
                "email": "hellenkoh@gmail.com",
                "affiliations": [
                    "Korea University, Seoul, Korea, Republic of"
                ],
                "is_corresponding": false
            },
            {
                "name": "Youngseo Kim",
                "email": "k0seo0330@vience.co.kr",
                "affiliations": [
                    "VIENCE Inc., Seoul, Korea, Republic of"
                ],
                "is_corresponding": false
            },
            {
                "name": "Won-Ki Jeong",
                "email": "wkjeong@korea.ac.kr",
                "affiliations": [
                    "VIENCE Inc., Seoul, Korea, Republic of",
                    "Korea University, Seoul, Korea, Republic of"
                ],
                "is_corresponding": false
            }
        ],
        "abstract": "In the rapidly evolving field of deep learning, the traditional methodologies for designing deep learning models predominantly rely on code-based frameworks. While these approaches provide flexibility, they also create a significant barrier to entry for non-experts and obscure the immediate impact of architectural decisions on model performance. In response to this challenge, recent no-code approaches have been developed with the aim of enabling easy model development through graphical interfaces. However, both traditional and no-code methodologies share a common limitation that the inability to predict model outcomes or identify issues without executing the model. To address this limitation, we introduce an intuitive visual feedback-based no-code approach to visualize and analyze deep learning models during the design phase. This approach utilizes dataflow-based visual programming with dynamic visual encoding of model architecture. A user study was conducted with deep learning developers to demonstrate the effectiveness of our approach in enhancing the model design process, improving model understanding, and facilitating a more intuitive development experience. The findings of this study suggest that real-time architectural visualization significantly contributes to more efficient model development and a deeper understanding of model behaviors.",
        "uid": "v-short-1047",
        "time_stamp": "",
        "time_start": "",
        "time_end": "",
        "paper_type": "short",
        "keywords": [],
        "doi": "",
        "fno": "",
        "presentation_mode": "",
        "open_access": false,
        "accessible_pdf": false,
        "has_image": false,
        "has_pdf": false,
        "paper_award": "",
        "image_caption": "",
        "external_paper_link": "",
        "youtube_ff_link": "",
        "youtube_ff_id": "",
        "bunny_ff_link": "",
        "bunny_ff_subtitles": "",
        "youtube_prerecorded_link": "",
        "youtube_prerecorded_id": "",
        "bunny_prerecorded_link": "",
        "bunny_prerecorded_subtitles": ""
    },
    "v-short-1049": {
        "slot_id": "v-short-1049",
        "session_id": "short0",
        "title": "A Comparative Study of Neural Surface Reconstruction for Scientific Visualization",
        "contributors": [
            "Siyuan Yao"
        ],
        "authors": [
            {
                "name": "Siyuan Yao",
                "email": "syao2@nd.edu",
                "affiliations": [
                    "University of Notre Dame, Notre Dame, United States"
                ],
                "is_corresponding": true
            },
            {
                "name": "Weixi Song",
                "email": "song.wx@whu.edu.cn",
                "affiliations": [
                    "Wuhan University, Wuhan, China"
                ],
                "is_corresponding": false
            },
            {
                "name": "Chaoli Wang",
                "email": "chaoli.wang@nd.edu",
                "affiliations": [
                    "University of Notre Dame, Notre Dame, United States"
                ],
                "is_corresponding": false
            }
        ],
        "abstract": "This comparative study evaluates various neural surface reconstruction methods, particularly focusing on their implications for scientific visualization through reconstructing 3D surfaces via multi-view rendering images. We categorize ten methods into neural radiance fields and neural implicit surfaces, uncovering the benefits of leveraging distance functions (i.e., SDFs and UDFs) to enhance the accuracy and smoothness of the reconstructed surfaces. Our findings highlight the efficiency and quality of NeuS2 for reconstructing closed surfaces and identify NeUDF as a promising candidate for reconstructing open surfaces despite some limitations. We further pinpoint directions for future research, including improving detail capture, optimizing UDF computations, and refining surface extraction methods. By sharing our benchmark dataset, we invite researchers to test the performance of their methods, contributing to the advancement of surface reconstruction solutions for scientific visualization.",
        "uid": "v-short-1049",
        "time_stamp": "",
        "time_start": "",
        "time_end": "",
        "paper_type": "short",
        "keywords": [],
        "doi": "",
        "fno": "",
        "presentation_mode": "",
        "open_access": false,
        "accessible_pdf": false,
        "has_image": false,
        "has_pdf": false,
        "paper_award": "",
        "image_caption": "",
        "external_paper_link": "",
        "youtube_ff_link": "",
        "youtube_ff_id": "",
        "bunny_ff_link": "",
        "bunny_ff_subtitles": "",
        "youtube_prerecorded_link": "",
        "youtube_prerecorded_id": "",
        "bunny_prerecorded_link": "",
        "bunny_prerecorded_subtitles": ""
    },
    "v-short-1054": {
        "slot_id": "v-short-1054",
        "session_id": "short0",
        "title": "Accelerating Transfer Function Update for Distance Map based Volume Rendering",
        "contributors": [
            "Michael Rauter"
        ],
        "authors": [
            {
                "name": "Michael Rauter",
                "email": "michael.rauter@fhwn.ac.at",
                "affiliations": [
                    "University of Applied Sciences Wiener Neustadt, Wiener Neustadt, Austria"
                ],
                "is_corresponding": true
            },
            {
                "name": "Lukas Zimmermann PhD",
                "email": "lukas.a.zimmermann@meduniwien.ac.at",
                "affiliations": [
                    "Medical University of Vienna, Vienna, Austria"
                ],
                "is_corresponding": false
            },
            {
                "name": "Markus Zeilinger PhD",
                "email": "markus.zeilinger@fhwn.ac.at",
                "affiliations": [
                    "University of Applied Sciences Wiener Neustadt, Wiener Neustadt, Austria"
                ],
                "is_corresponding": false
            }
        ],
        "abstract": "Direct volume rendering using ray-casting is widely used in practice. By using GPUs and applying acceleration techniques as empty space skipping, high frame rates are possible on modern hardware. This enables performance-critical use-cases such as virtual reality volume rendering. The currently fastest known technique uses volumetric distance maps to skip empty sections of the volume during ray-casting but requires the distance map to be updated per transfer function change. In this paper, we demonstrate a technique for subdividing the volume intensity range into partitions and deriving what we call partitioned distance maps. These can be used to accelerate the distance map computation for a newly changed transfer function by a factor up to 30. This allows the currently fastest known empty space skipping approach to be used while maintaining high frame rates even when the transfer function is changed frequently.",
        "uid": "v-short-1054",
        "time_stamp": "",
        "time_start": "",
        "time_end": "",
        "paper_type": "short",
        "keywords": [],
        "doi": "",
        "fno": "",
        "presentation_mode": "",
        "open_access": false,
        "accessible_pdf": false,
        "has_image": false,
        "has_pdf": false,
        "paper_award": "",
        "image_caption": "",
        "external_paper_link": "",
        "youtube_ff_link": "",
        "youtube_ff_id": "",
        "bunny_ff_link": "",
        "bunny_ff_subtitles": "",
        "youtube_prerecorded_link": "",
        "youtube_prerecorded_id": "",
        "bunny_prerecorded_link": "",
        "bunny_prerecorded_subtitles": ""
    },
    "v-short-1056": {
        "slot_id": "v-short-1056",
        "session_id": "short0",
        "title": "FCNR: Fast Compressive Neural Representation of Visualization Images",
        "contributors": [
            "Yunfei Lu"
        ],
        "authors": [
            {
                "name": "Yunfei Lu",
                "email": "ylu25@nd.edu",
                "affiliations": [
                    "University of Notre Dame, Notre Dame, United States"
                ],
                "is_corresponding": true
            },
            {
                "name": "Pengfei Gu",
                "email": "pgu@nd.edu",
                "affiliations": [
                    "University of Notre Dame, Notre Dame, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Chaoli Wang",
                "email": "chaoli.wang@nd.edu",
                "affiliations": [
                    "University of Notre Dame, Notre Dame, United States"
                ],
                "is_corresponding": false
            }
        ],
        "abstract": "We present FCNR, a fast compressive neural representation for tens of thousands of visualization images under varying viewpoints and timesteps. The existing NeRVI solution, albeit enjoying a high compression rate, incurs slow speeds in encoding and decoding. Built on the recent advances in stereo image compression, FCNR assimilates stereo context modules and joint context transfer modules to compress image pairs. Our solution significantly improves encoding and decoding speed while maintaining high reconstruction quality and satisfying compression rate. To demonstrate its effectiveness, we compare FCNR with state-of-the-art neural compression methods, including E-NeRV, HNeRV, NeRVI, and ECSIC.",
        "uid": "v-short-1056",
        "time_stamp": "",
        "time_start": "",
        "time_end": "",
        "paper_type": "short",
        "keywords": [],
        "doi": "",
        "fno": "",
        "presentation_mode": "",
        "open_access": false,
        "accessible_pdf": false,
        "has_image": false,
        "has_pdf": false,
        "paper_award": "",
        "image_caption": "",
        "external_paper_link": "",
        "youtube_ff_link": "",
        "youtube_ff_id": "",
        "bunny_ff_link": "",
        "bunny_ff_subtitles": "",
        "youtube_prerecorded_link": "",
        "youtube_prerecorded_id": "",
        "bunny_prerecorded_link": "",
        "bunny_prerecorded_subtitles": ""
    },
    "v-short-1057": {
        "slot_id": "v-short-1057",
        "session_id": "short0",
        "title": "On Combined Visual Cluster and Set Analysis",
        "contributors": [
            "Nikolaus Piccolotto"
        ],
        "authors": [
            {
                "name": "Nikolaus Piccolotto",
                "email": "nikolaus.piccolotto@tuwien.ac.at",
                "affiliations": [
                    "TU Wien, Vienna, Austria"
                ],
                "is_corresponding": true
            },
            {
                "name": "Markus Wallinger",
                "email": "mwallinger@ac.tuwien.ac.at",
                "affiliations": [
                    "TU Wien, Vienna, Austria"
                ],
                "is_corresponding": false
            },
            {
                "name": "Silvia Miksch",
                "email": "miksch@ifs.tuwien.ac.at",
                "affiliations": [
                    "Institute of Visual Computing and Human-Centered Technology, Vienna, Austria"
                ],
                "is_corresponding": false
            },
            {
                "name": "Markus B\u221a\u2202gl",
                "email": "markus.boegl@tuwien.ac.at",
                "affiliations": [
                    "TU Wien, Vienna, Austria"
                ],
                "is_corresponding": false
            }
        ],
        "abstract": "Real-world datasets often consist of quantitative and categorical variables. The analyst needs to focus on either kind separately or both jointly. We proposed a visualization technique tackling these challenges that supports visual cluster and set analysis. In this paper, we investigate how its visualization parameters affect the accuracy and speed of cluster and set analysis tasks in a controlled experiment. Our findings show that, with the proper settings, our visualization can support both task types well. However, we did not find settings suitable for the joint task, which provides opportunities for future research.",
        "uid": "v-short-1057",
        "time_stamp": "",
        "time_start": "",
        "time_end": "",
        "paper_type": "short",
        "keywords": [],
        "doi": "",
        "fno": "",
        "presentation_mode": "",
        "open_access": false,
        "accessible_pdf": false,
        "has_image": false,
        "has_pdf": false,
        "paper_award": "",
        "image_caption": "",
        "external_paper_link": "",
        "youtube_ff_link": "",
        "youtube_ff_id": "",
        "bunny_ff_link": "",
        "bunny_ff_subtitles": "",
        "youtube_prerecorded_link": "",
        "youtube_prerecorded_id": "",
        "bunny_prerecorded_link": "",
        "bunny_prerecorded_subtitles": ""
    },
    "v-short-1058": {
        "slot_id": "v-short-1058",
        "session_id": "short0",
        "title": "ImageSI: Semantic Interaction for Deep Learning Image Projections",
        "contributors": [
            "Rebecca Faust"
        ],
        "authors": [
            {
                "name": "Jiayue Lin",
                "email": "jiayuelin@vt.edu",
                "affiliations": [
                    "Vriginia Tech, Blacksburg, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Rebecca Faust",
                "email": "rfaust1@tulane.edu",
                "affiliations": [
                    "Tulane University, New Orleans, United States"
                ],
                "is_corresponding": true
            },
            {
                "name": "Chris North",
                "email": "north@vt.edu",
                "affiliations": [
                    "Virginia Tech, Blacksburg, United States"
                ],
                "is_corresponding": false
            }
        ],
        "abstract": "Semantic interaction (SI) in Dimension Reduction (DR) of images allows users to incorporate feedback through direct manipulation of the 2D positions of images. Through interaction, users specify a set of pairwise relationships that the DR should aim to capture. Existing methods for images incorporate feedback into the DR through feature weights on abstract embedding features. However, if the original embedding features do not suitably capture the users task then the DR cannot either. We propose, ImageSI, an SI method for image DR that incorporates user feedback directly into the image model to update the underlying embeddings, rather than weighting them. In doing so, ImageSI ensures that the embeddings suitably capture the features necessary for the task so that the DR can subsequently organize images using those features. We present two variations of ImageSI using different loss functions - ImageSI_MDS-Inverse , which prioritizes the explicit pairwise relationships from the interaction and ImageSI_Triplet, which prioritizes clustering, using the interaction to define groups of images. Finally, we present a usage scenario and a simulation-based evaluation to demonstrate the utility of ImageSI and compare it to current methods.",
        "uid": "v-short-1058",
        "time_stamp": "",
        "time_start": "",
        "time_end": "",
        "paper_type": "short",
        "keywords": [],
        "doi": "",
        "fno": "",
        "presentation_mode": "",
        "open_access": false,
        "accessible_pdf": false,
        "has_image": false,
        "has_pdf": false,
        "paper_award": "",
        "image_caption": "",
        "external_paper_link": "",
        "youtube_ff_link": "",
        "youtube_ff_id": "",
        "bunny_ff_link": "",
        "bunny_ff_subtitles": "",
        "youtube_prerecorded_link": "",
        "youtube_prerecorded_id": "",
        "bunny_prerecorded_link": "",
        "bunny_prerecorded_subtitles": ""
    },
    "v-short-1059": {
        "slot_id": "v-short-1059",
        "session_id": "short0",
        "title": "A Literature-based Visualization Task Taxonomy for Gantt charts",
        "contributors": [
            "Sayef Azad Sakin"
        ],
        "authors": [
            {
                "name": "Sayef Azad Sakin",
                "email": "sayefsakin@sci.utah.edu",
                "affiliations": [
                    "University of Utah, Salt Lake City, United States"
                ],
                "is_corresponding": true
            },
            {
                "name": "Katherine E. Isaacs",
                "email": "kisaacs@sci.utah.edu",
                "affiliations": [
                    "The University of Utah, Salt Lake City, United States"
                ],
                "is_corresponding": false
            }
        ],
        "abstract": "Gantt charts are a widely-used idiom for visualizing temporal discrete event sequence data where dependencies exist between events. They are popular in domains such as manufacturing and computing for their intuitive layout of such data. However, these domains frequently generate data at scales which tax both the visual representation and the ability to render it at interactive speeds. To aid visualization developers who use Gantt charts in these situations, we develop a task taxonomy of low level visualization tasks supported by Gantt charts and connect them to the data queries needed to support them. Our taxonomy is derived through a systematic literature survey of visualizations using Gantt charts over the past 30 years.",
        "uid": "v-short-1059",
        "time_stamp": "",
        "time_start": "",
        "time_end": "",
        "paper_type": "short",
        "keywords": [],
        "doi": "",
        "fno": "",
        "presentation_mode": "",
        "open_access": false,
        "accessible_pdf": false,
        "has_image": false,
        "has_pdf": false,
        "paper_award": "",
        "image_caption": "",
        "external_paper_link": "",
        "youtube_ff_link": "",
        "youtube_ff_id": "",
        "bunny_ff_link": "",
        "bunny_ff_subtitles": "",
        "youtube_prerecorded_link": "",
        "youtube_prerecorded_id": "",
        "bunny_prerecorded_link": "",
        "bunny_prerecorded_subtitles": ""
    },
    "v-short-1062": {
        "slot_id": "v-short-1062",
        "session_id": "short0",
        "title": "Integrating Annotations into the Design Process for Sonifications and Physicalizations",
        "contributors": [
            "S. Sandra Bae"
        ],
        "authors": [
            {
                "name": "Rhys Sorenson-Graff",
                "email": "sorensor@whitman.edu",
                "affiliations": [
                    "Whitman College, Walla Walla, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "S. Sandra Bae",
                "email": "sandra.bae@colorado.edu",
                "affiliations": [
                    "University of Colorado Boulder, Boulder, United States"
                ],
                "is_corresponding": true
            },
            {
                "name": "Jordan Wirfs-Brock",
                "email": "wirfsbro@colorado.edu",
                "affiliations": [
                    "Whitman College, Walla Walla, United States"
                ],
                "is_corresponding": false
            }
        ],
        "abstract": "Annotations are a critical component of visualizations, helping viewers interpret the visual representation and highlighting critical data insights. Despite its significant role, we lack an understanding of how annotations can be incorporated into other data representations, such as physicalizations and sonifications. Given the emergent nature of these representations, sonifications and physicalizations lack formalized conventions (e.g., design space, vocabulary) that can introduce challenges for audiences to interpret the intended data encoding. To address this challenge, this work focuses on how annotations can be more tightly integrated into the design process of creating sonifications and physicalization. In an exploratory study with 13 designers, we explore how visualization annotation techniques can be adapted to sonic and physical modalities. Our work highlights how annotations for sonification and physicalizations are inseparable from their data encodings",
        "uid": "v-short-1062",
        "time_stamp": "",
        "time_start": "",
        "time_end": "",
        "paper_type": "short",
        "keywords": [],
        "doi": "",
        "fno": "",
        "presentation_mode": "",
        "open_access": false,
        "accessible_pdf": false,
        "has_image": false,
        "has_pdf": false,
        "paper_award": "",
        "image_caption": "",
        "external_paper_link": "",
        "youtube_ff_link": "",
        "youtube_ff_id": "",
        "bunny_ff_link": "",
        "bunny_ff_subtitles": "",
        "youtube_prerecorded_link": "",
        "youtube_prerecorded_id": "",
        "bunny_prerecorded_link": "",
        "bunny_prerecorded_subtitles": ""
    },
    "v-short-1064": {
        "slot_id": "v-short-1064",
        "session_id": "short0",
        "title": "Bavisitter: Integrating Design Guidelines into Large Language Models for Visualization Authoring",
        "contributors": [
            "Jiwon Choi"
        ],
        "authors": [
            {
                "name": "Jiwon Choi",
                "email": "jiwnchoi@skku.edu",
                "affiliations": [
                    "Sungkyunkwan University, Suwon, Korea, Republic of"
                ],
                "is_corresponding": true
            },
            {
                "name": "Jaeung Lee",
                "email": "dlwodnd00@skku.edu",
                "affiliations": [
                    "Sungkyunkwan University, Suwon, Korea, Republic of"
                ],
                "is_corresponding": false
            },
            {
                "name": "Jaemin Jo",
                "email": "jmjo@skku.edu",
                "affiliations": [
                    "Sungkyunkwan University, Suwon, Korea, Republic of"
                ],
                "is_corresponding": false
            }
        ],
        "abstract": "Large Language Models (LLMs) have demonstrated remarkable versatility in visualization authoring, but often generate suboptimal designs that are invalid or fail to adhere to design guidelines for effective visualization. We present Bavisitter, a natural language interface that integrates established visualization design guidelines into LLMs. Based on our survey on the design issues in LLM-generated visualizations, Bavisitter monitors the generated visualizations during a visualization authoring dialogue to detect an issue. When an issue is detected, it intervenes in the dialogue, suggesting possible solutions to the issue by modifying the prompts. We also demonstrate two use cases where Bavisitter detects and resolves design issues from the actual LLM-generated visualizations.",
        "uid": "v-short-1064",
        "time_stamp": "",
        "time_start": "",
        "time_end": "",
        "paper_type": "short",
        "keywords": [],
        "doi": "",
        "fno": "",
        "presentation_mode": "",
        "open_access": false,
        "accessible_pdf": false,
        "has_image": false,
        "has_pdf": false,
        "paper_award": "",
        "image_caption": "",
        "external_paper_link": "",
        "youtube_ff_link": "",
        "youtube_ff_id": "",
        "bunny_ff_link": "",
        "bunny_ff_subtitles": "",
        "youtube_prerecorded_link": "",
        "youtube_prerecorded_id": "",
        "bunny_prerecorded_link": "",
        "bunny_prerecorded_subtitles": ""
    },
    "v-short-1065": {
        "slot_id": "v-short-1065",
        "session_id": "short0",
        "title": "GhostUMAP: Measuring Pointwise Instability in Dimensionality Reduction",
        "contributors": [
            "Myeongwon Jung"
        ],
        "authors": [
            {
                "name": "Myeongwon Jung",
                "email": "mw.jung@skku.edu",
                "affiliations": [
                    "Sungkyunkwan University, Suwon, Korea, Republic of"
                ],
                "is_corresponding": true
            },
            {
                "name": "Takanori Fujiwara",
                "email": "takanori.fujiwara@liu.se",
                "affiliations": [
                    "Link\u221a\u2202ping University, Norrk\u221a\u2202ping, Sweden"
                ],
                "is_corresponding": false
            },
            {
                "name": "Jaemin Jo",
                "email": "jmjo@skku.edu",
                "affiliations": [
                    "Sungkyunkwan University, Suwon, Korea, Republic of"
                ],
                "is_corresponding": false
            }
        ],
        "abstract": "Although many dimensionality reduction (DR) techniques employ stochastic methods for computational efficiency, such as negative sampling or stochastic gradient descent, their impact on the projection has been underexplored. In this work, we investigate how such stochasticity affects the stability of projections and present a novel DR technique, GhostUMAP, to measure the pointwise instability of projections. Our idea is to introduce clones of data points, \"ghosts\", into UMAP's layout optimization process. Ghosts are designed to be completely passive: they do not affect any others but are influenced by attractive and repulsive forces from the original data points. After a single optimization run, GhostUMAP can capture the projection instability of data points by measuring the variance with the projected positions of their ghosts. We also present a successive halving technique to reduce the computation of GhostUMAP. Our results suggest that GhostUMAP can reveal unstable data points with a reasonable computational overhead.",
        "uid": "v-short-1065",
        "time_stamp": "",
        "time_start": "",
        "time_end": "",
        "paper_type": "short",
        "keywords": [],
        "doi": "",
        "fno": "",
        "presentation_mode": "",
        "open_access": false,
        "accessible_pdf": false,
        "has_image": false,
        "has_pdf": false,
        "paper_award": "",
        "image_caption": "",
        "external_paper_link": "",
        "youtube_ff_link": "",
        "youtube_ff_id": "",
        "bunny_ff_link": "",
        "bunny_ff_subtitles": "",
        "youtube_prerecorded_link": "",
        "youtube_prerecorded_id": "",
        "bunny_prerecorded_link": "",
        "bunny_prerecorded_subtitles": ""
    },
    "v-short-1068": {
        "slot_id": "v-short-1068",
        "session_id": "short0",
        "title": "DASH: A Bimodal Data Exploration Tool for Interactive Text and Visualizations",
        "contributors": [
            "Dennis Bromley"
        ],
        "authors": [
            {
                "name": "Dennis Bromley",
                "email": "bromley.denny@gmail.com",
                "affiliations": [
                    "Tableau Research, Seattle, United States"
                ],
                "is_corresponding": true
            },
            {
                "name": "Vidya Setlur",
                "email": "vsetlur@tableau.com",
                "affiliations": [
                    "Tableau Research, Palo Alto, United States"
                ],
                "is_corresponding": false
            }
        ],
        "abstract": "Integrating textual content, such as titles, annotations, and captions, with visualizations facilitates comprehension and takeaways during data exploration. Yet current tools often lack mechanisms for integrating meaningful text with visual data. This paper introduces DASH, a bimodal data exploration tool that supports integrating semantic levels into the interactive process of visualization and text-based analysis. DASH operationalizes a modified version of Lundgard et al.'s semantic hierarchy model that categorizes data descriptions into four levels ranging from basic encodings to high-level insights. By leveraging this structured semantic level framework and a large language model's text generation capabilities, DASH enables the creation of data-driven narratives via drag-and-drop user interaction. Through a preliminary user evaluation, we discuss the utility of DASH's text and chart integration capabilities when participants perform data exploration with the tool. Based on the study's feedback and observations, we discuss implications for designing unified text and chart authoring tools.",
        "uid": "v-short-1068",
        "time_stamp": "",
        "time_start": "",
        "time_end": "",
        "paper_type": "short",
        "keywords": [],
        "doi": "",
        "fno": "",
        "presentation_mode": "",
        "open_access": false,
        "accessible_pdf": false,
        "has_image": false,
        "has_pdf": false,
        "paper_award": "",
        "image_caption": "",
        "external_paper_link": "",
        "youtube_ff_link": "",
        "youtube_ff_id": "",
        "bunny_ff_link": "",
        "bunny_ff_subtitles": "",
        "youtube_prerecorded_link": "",
        "youtube_prerecorded_id": "",
        "bunny_prerecorded_link": "",
        "bunny_prerecorded_subtitles": ""
    },
    "v-short-1072": {
        "slot_id": "v-short-1072",
        "session_id": "short0",
        "title": "Assessing Graphical Perception of Image Embedding Models using Channel Effectiveness",
        "contributors": [
            "Soohyun Lee"
        ],
        "authors": [
            {
                "name": "Soohyun Lee",
                "email": "dtngus0111@gmail.com",
                "affiliations": [
                    "Seoul National University, Seoul, Korea, Republic of"
                ],
                "is_corresponding": true
            },
            {
                "name": "Minsuk Chang",
                "email": "jangsus1@snu.ac.kr",
                "affiliations": [
                    "Seoul National University, Seoul, Korea, Republic of"
                ],
                "is_corresponding": false
            },
            {
                "name": "Seokhyeon Park",
                "email": "shpark@hcil.snu.ac.kr",
                "affiliations": [
                    "Seoul National University, Seoul, Korea, Republic of"
                ],
                "is_corresponding": false
            },
            {
                "name": "Jinwook Seo",
                "email": "jseo@snu.ac.kr",
                "affiliations": [
                    "Seoul National University, Seoul, Korea, Republic of"
                ],
                "is_corresponding": false
            }
        ],
        "abstract": "Recent advancements in vision models have significantly enhanced their ability to perform complex chart understanding tasks, such as chart captioning and chart question answering. However, assessing how these models process charts remains challenging. Existing benchmarks only coarsely evaluate how well the model performs the given task without thoroughly evaluating the underlying mechanisms that drive performance, such as how models extract image embeddings. This gap limits our understanding of the model's perceptual capabilities regarding fundamental graphical components. Therefore, we introduce a novel evaluation framework designed to assess the graphical perception of image embedding models. In the context of chart comprehension, we examine two main aspects of channel effectiveness: accuracy and discriminability of various visual channels. We first assess channel accuracy through the linearity of embeddings, which is the degree to which the perceived magnitude is proportional to the size of the stimulus. % based on the assumption that perceived magnitude should be proportional to the size of Conversely, distances between embeddings serve as a measure of discriminability; embeddings that are far apart can be considered discriminable. Our experiments on a general image embedding model, CLIP, provided that it perceives channel accuracy differently from humans and demonstrated distinct discriminability in specific channels such as length, tilt, and curvature. We aim to extend our work as a more general benchmark for reliable visual encoders and enhance a model for two distinctive goals for future applications: precise chart comprehension and mimicking human perception.",
        "uid": "v-short-1072",
        "time_stamp": "",
        "time_start": "",
        "time_end": "",
        "paper_type": "short",
        "keywords": [],
        "doi": "",
        "fno": "",
        "presentation_mode": "",
        "open_access": false,
        "accessible_pdf": false,
        "has_image": false,
        "has_pdf": false,
        "paper_award": "",
        "image_caption": "",
        "external_paper_link": "",
        "youtube_ff_link": "",
        "youtube_ff_id": "",
        "bunny_ff_link": "",
        "bunny_ff_subtitles": "",
        "youtube_prerecorded_link": "",
        "youtube_prerecorded_id": "",
        "bunny_prerecorded_link": "",
        "bunny_prerecorded_subtitles": ""
    },
    "v-short-1078": {
        "slot_id": "v-short-1078",
        "session_id": "short0",
        "title": "Design Patterns in Right-to-Left Visualizations: The Case of Arabic Content",
        "contributors": [
            "Muna Alebri"
        ],
        "authors": [
            {
                "name": "Muna Alebri",
                "email": "muna.alebri.19@ucl.ac.uk",
                "affiliations": [
                    "University College London, London, United Kingdom",
                    "UAE University , Al Ain, United Arab Emirates"
                ],
                "is_corresponding": true
            },
            {
                "name": "No\u221a\u00b4lle Rakotondravony",
                "email": "ntrakotondravony@wpi.edu",
                "affiliations": [
                    "Worcester Polytechnic Institute, Worcester, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Lane Harrison",
                "email": "ltharrison@wpi.edu",
                "affiliations": [
                    "Worcester Polytechnic Institute, Worcester, United States"
                ],
                "is_corresponding": false
            }
        ],
        "abstract": "Data visualizations are reaching global audiences. As people who use Right-to-left (RTL) scripts constitute over a billion potential data visualization users, a need emerges to investigate how visualizations are communicated to them. Web design guidelines exist to assist designers in adapting different reading directions, yet we lack a similar standard for visualization design. This paper investigates the design patterns of visualizations with RTL scripts. We collected 128 visualizations from data-driven articles published in Arabic news outlets and analyzed their chart composition, textual elements, and sources. Our analysis suggests that designers tend to apply RTL approaches more frequently for categorical data. In other situations, we observed a mix of Left-to-right (LTR) and RTL approaches for chart directions and structures, sometimes inconsistently utilized within the same article. We reflect on this lack of clear guidelines for RTL data visualizations and derive implications for visualization authoring tools and future research directions.",
        "uid": "v-short-1078",
        "time_stamp": "",
        "time_start": "",
        "time_end": "",
        "paper_type": "short",
        "keywords": [],
        "doi": "",
        "fno": "",
        "presentation_mode": "",
        "open_access": false,
        "accessible_pdf": false,
        "has_image": false,
        "has_pdf": false,
        "paper_award": "",
        "image_caption": "",
        "external_paper_link": "",
        "youtube_ff_link": "",
        "youtube_ff_id": "",
        "bunny_ff_link": "",
        "bunny_ff_subtitles": "",
        "youtube_prerecorded_link": "",
        "youtube_prerecorded_id": "",
        "bunny_prerecorded_link": "",
        "bunny_prerecorded_subtitles": ""
    },
    "v-short-1079": {
        "slot_id": "v-short-1079",
        "session_id": "short0",
        "title": "AEye: A Visualization Tool for Image Datasets",
        "contributors": [
            "Florian Gr\u221a\u2202tschla"
        ],
        "authors": [
            {
                "name": "Florian Gr\u221a\u2202tschla",
                "email": "fgroetschla@ethz.ch",
                "affiliations": [
                    "ETH Zurich, Zurich, Switzerland"
                ],
                "is_corresponding": true
            },
            {
                "name": "Luca A Lanzend\u221a\u2202rfer",
                "email": "lanzendoerfer@ethz.ch",
                "affiliations": [
                    "ETH Zurich, Zurich, Switzerland"
                ],
                "is_corresponding": false
            },
            {
                "name": "Marco Calzavara",
                "email": "mcalzavara@student.ethz.ch",
                "affiliations": [
                    "ETH Zurich, Zurich, Switzerland"
                ],
                "is_corresponding": false
            },
            {
                "name": "Roger Wattenhofer",
                "email": "wattenhofer@ethz.ch",
                "affiliations": [
                    "ETH Zurich, Zurich, Switzerland"
                ],
                "is_corresponding": false
            }
        ],
        "abstract": "Image datasets serve as the foundation for machine learning models in computer vision, significantly influencing model capabilities, performance, and biases alongside architectural considerations. Therefore, understanding the composition and distribution of these datasets has become increasingly crucial. To address the need for intuitive exploration of these datasets, we propose AEye, an extensible and scalable visualization tool tailored to image datasets. AEye utilizes a contrastively trained model to embed images into semantically meaningful high-dimensional representations, facilitating data clustering and organization. To visualize the high-dimensional representations, we project them onto a two-dimensional plane and arrange images in layers so users can seamlessly navigate and explore them interactively. Furthermore, AEye facilitates semantic search functionalities for both text and image queries, enabling users to search for content. We open-source the codebase for AEye, and provide a simple configuration to add additional datasets.",
        "uid": "v-short-1079",
        "time_stamp": "",
        "time_start": "",
        "time_end": "",
        "paper_type": "short",
        "keywords": [],
        "doi": "",
        "fno": "",
        "presentation_mode": "",
        "open_access": false,
        "accessible_pdf": false,
        "has_image": false,
        "has_pdf": false,
        "paper_award": "",
        "image_caption": "",
        "external_paper_link": "",
        "youtube_ff_link": "",
        "youtube_ff_id": "",
        "bunny_ff_link": "",
        "bunny_ff_subtitles": "",
        "youtube_prerecorded_link": "",
        "youtube_prerecorded_id": "",
        "bunny_prerecorded_link": "",
        "bunny_prerecorded_subtitles": ""
    },
    "v-short-1081": {
        "slot_id": "v-short-1081",
        "session_id": "short0",
        "title": "Gridlines Mitigate Sine Illusion in Line Charts",
        "contributors": [
            "Cindy Xiong Bearfield"
        ],
        "authors": [
            {
                "name": "Clayton J Knittel",
                "email": "cknit1999@gmail.com",
                "affiliations": [
                    "Google LLC, San Francisco, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Jane Awuah",
                "email": "jawuah3@gatech.edu",
                "affiliations": [
                    "Georgia Institute of Technology, Atlanta, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Steven L Franconeri",
                "email": "franconeri@northwestern.edu",
                "affiliations": [
                    "Northwestern University, Evanston, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Cindy Xiong Bearfield",
                "email": "cxiong@gatech.edu",
                "affiliations": [
                    "Georgia Tech, Atlanta, United States"
                ],
                "is_corresponding": true
            }
        ],
        "abstract": "Sine illusion happens when the more quickly changing pairs of lines lead to bigger underestimates of the delta between them. We evaluate three visual manipulations on mitigating sine illusions: dotted lines, aligned gridlines, and offset gridlines via a user study. We asked participants to compare the deltas between two lines at two time points and found aligned gridlines to be the most effective in mitigating sine illusions. Using data from the user study, we produced a model that predicts the impact of the sine illusion in line charts by accounting for the ratio of the vertical distance between the two points of comparison. When the ratio is less than 50\\%, participants begin to be influenced by the sine illusion. This effect can be significantly exacerbated when the difference between the two deltas falls under 30\\%. We compared two explanations for the sine illusion based on our data: either participants were mistakenly using the perpendicular distance between the two lines to make their comparison (the perpendicular explanation), or they incorrectly relied on the length of the line segment perpendicular to the angle bisector of the bottom and top lines (the equal triangle explanation). We found the equal triangle explanation to be the more predictive model explaining participant behaviors.",
        "uid": "v-short-1081",
        "time_stamp": "",
        "time_start": "",
        "time_end": "",
        "paper_type": "short",
        "keywords": [],
        "doi": "",
        "fno": "",
        "presentation_mode": "",
        "open_access": false,
        "accessible_pdf": false,
        "has_image": false,
        "has_pdf": false,
        "paper_award": "",
        "image_caption": "",
        "external_paper_link": "",
        "youtube_ff_link": "",
        "youtube_ff_id": "",
        "bunny_ff_link": "",
        "bunny_ff_subtitles": "",
        "youtube_prerecorded_link": "",
        "youtube_prerecorded_id": "",
        "bunny_prerecorded_link": "",
        "bunny_prerecorded_subtitles": ""
    },
    "v-short-1089": {
        "slot_id": "v-short-1089",
        "session_id": "short0",
        "title": "A Two-Phase Visualization System for Continuous Human-AI Collaboration in Sequelae Analysis and Modeling",
        "contributors": [
            "Yang Ouyang"
        ],
        "authors": [
            {
                "name": "Yang Ouyang",
                "email": "ouyy@shanghaitech.edu.cn",
                "affiliations": [
                    "ShanghaiTech University, Shanghai, China",
                    "ShanghaiTech University, Shanghai, China"
                ],
                "is_corresponding": true
            },
            {
                "name": "Chenyang Zhang",
                "email": "zhang414@illinois.edu",
                "affiliations": [
                    "University of Illinois at Urbana-Champaign, Champaign, United States",
                    "University of Illinois at Urbana-Champaign, Champaign, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "He Wang",
                "email": "wanghe1@shanghaitech.edu.cn",
                "affiliations": [
                    "ShanghaiTech University, Shanghai, China",
                    "ShanghaiTech University, Shanghai, China"
                ],
                "is_corresponding": false
            },
            {
                "name": "Tianle Ma",
                "email": "15301050137@fudan.edu.cn",
                "affiliations": [
                    "Zhongshan Hospital Fudan University, Shanghai, China",
                    "Zhongshan Hospital Fudan University, Shanghai, China"
                ],
                "is_corresponding": false
            },
            {
                "name": "Chang Jiang",
                "email": "cjiang_fdu@yeah.net",
                "affiliations": [
                    "Zhongshan Hospital Fudan University, Shanghai, China",
                    "Zhongshan Hospital Fudan University, Shanghai, China"
                ],
                "is_corresponding": false
            },
            {
                "name": "Yuheng Yan",
                "email": "522649732@qq.com",
                "affiliations": [
                    "Zhongshan Hospital Fudan University, Shanghai, China",
                    "Zhongshan Hospital Fudan University, Shanghai, China"
                ],
                "is_corresponding": false
            },
            {
                "name": "Zuoqin Yan",
                "email": "yan.zuoqin@zs-hospital.sh.cn",
                "affiliations": [
                    "Zhongshan Hospital Fudan University, Shanghai, China",
                    "Zhongshan Hospital Fudan University, Shanghai, China"
                ],
                "is_corresponding": false
            },
            {
                "name": "Xiaojuan Ma",
                "email": "mxj@cse.ust.hk",
                "affiliations": [
                    "Hong Kong University of Science and Technology, Hong Kong, Hong Kong",
                    "Hong Kong University of Science and Technology, Hong Kong, Hong Kong"
                ],
                "is_corresponding": false
            },
            {
                "name": "Chuhan Shi",
                "email": "cshiag@connect.ust.hk",
                "affiliations": [
                    "Southeast University, Nanjing, China",
                    "Southeast University, Nanjing, China"
                ],
                "is_corresponding": false
            },
            {
                "name": "Quan Li",
                "email": "liquan@shanghaitech.edu.cn",
                "affiliations": [
                    "ShanghaiTech University, Shanghai, China",
                    "ShanghaiTech University, Shanghai, China"
                ],
                "is_corresponding": false
            }
        ],
        "abstract": "In healthcare, AI techniques are widely used for tasks like risk assessment and anomaly detection. Despite AI's potential as a valuable assistant, its role in complex medical data analysis often oversimplifies human-AI collaboration dynamics. To address this, we collaborated with a local hospital, engaging six physicians and one data scientist in a formative study. From this collaboration, we propose a framework integrating two-phase interactive visualization systems: one for Human-Led, AI-Assisted Retrospective Analysis and another for AI-Mediated, Human-Reviewed Iterative Modeling. This framework aims to enhance understanding and discussion around effective human-AI collaboration in healthcare.",
        "uid": "v-short-1089",
        "time_stamp": "",
        "time_start": "",
        "time_end": "",
        "paper_type": "short",
        "keywords": [],
        "doi": "",
        "fno": "",
        "presentation_mode": "",
        "open_access": false,
        "accessible_pdf": false,
        "has_image": false,
        "has_pdf": false,
        "paper_award": "",
        "image_caption": "",
        "external_paper_link": "",
        "youtube_ff_link": "",
        "youtube_ff_id": "",
        "bunny_ff_link": "",
        "bunny_ff_subtitles": "",
        "youtube_prerecorded_link": "",
        "youtube_prerecorded_id": "",
        "bunny_prerecorded_link": "",
        "bunny_prerecorded_subtitles": ""
    },
    "v-short-1090": {
        "slot_id": "v-short-1090",
        "session_id": "short0",
        "title": "Hypertrix: An indicatrix for high-dimensional visualizations",
        "contributors": [
            "Shivam Raval"
        ],
        "authors": [
            {
                "name": "Shivam Raval",
                "email": "sraval@g.harvard.edu",
                "affiliations": [
                    "Harvard University, Boston, United States"
                ],
                "is_corresponding": true
            },
            {
                "name": "Fernanda Viegas",
                "email": "viegas@google.com",
                "affiliations": [
                    "Harvard University, Cambridge, United States",
                    "Google Research, Cambridge, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Martin Wattenberg",
                "email": "wattenberg@gmail.com",
                "affiliations": [
                    "Harvard University, Cambridge, United States",
                    "Google Research, Cambridge, United States"
                ],
                "is_corresponding": false
            }
        ],
        "abstract": "Visualizing high dimensional data is challenging, since any dimensionality reduction technique will distort distances. A classic method in cartography\u201a\u00c4\u00ecTissot\u201a\u00c4\u00f4s Indicatrix, specific to sphere-to-plane maps\u201a\u00c4\u00ecvisualizes distortion using ellipses. Inspired by this idea, we describe the hypertrix: a method for representing distortions that occur when data is projected from arbitrarily high dimensions onto a 2D plane. We demonstrate our technique through synthetic and real-world datasets, and describe how this indicatrix can guide interpretations of nonlinear dimensionality reduction",
        "uid": "v-short-1090",
        "time_stamp": "",
        "time_start": "",
        "time_end": "",
        "paper_type": "short",
        "keywords": [],
        "doi": "",
        "fno": "",
        "presentation_mode": "",
        "open_access": false,
        "accessible_pdf": false,
        "has_image": false,
        "has_pdf": false,
        "paper_award": "",
        "image_caption": "",
        "external_paper_link": "",
        "youtube_ff_link": "",
        "youtube_ff_id": "",
        "bunny_ff_link": "",
        "bunny_ff_subtitles": "",
        "youtube_prerecorded_link": "",
        "youtube_prerecorded_id": "",
        "bunny_prerecorded_link": "",
        "bunny_prerecorded_subtitles": ""
    },
    "v-short-1096": {
        "slot_id": "v-short-1096",
        "session_id": "short0",
        "title": "Use-Coordination: Model, Grammar, and Library for Implementation of Coordinated Multiple Views",
        "contributors": [
            "Mark S Keller"
        ],
        "authors": [
            {
                "name": "Mark S Keller",
                "email": "mark_keller@hms.harvard.edu",
                "affiliations": [
                    "Harvard Medical School, Boston, United States"
                ],
                "is_corresponding": true
            },
            {
                "name": "Trevor Manz",
                "email": "trevor_manz@g.harvard.edu",
                "affiliations": [
                    "Harvard Medical School, Boston, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Nils Gehlenborg",
                "email": "nils@hms.harvard.edu",
                "affiliations": [
                    "Harvard Medical School, Boston, United States"
                ],
                "is_corresponding": false
            }
        ],
        "abstract": "Coordinated multiple views (CMV) in a visual analytics system can help users explore multiple data representations simultaneously with linked interactions. However, the implementation of coordinated multiple views can be challenging. Without standard software libraries, visualization designers need to re-implement CMV during the development of each system. We introduce use-coordination, a grammar and software library that supports the efficient implementation of CMV. The grammar defines a JSON-based representation for an abstract coordination model from the information visualization literature. We contribute an optional extension to the model and grammar that allows for hierarchical coordination. Through three use cases, we show that use-coordination enables implementation of CMV in systems containing not only basic statistical charts but also more complex visualizations such as medical imaging volumes. We describe six software extensions, including a graphical editor for manipulation of coordination, which showcase the potential to build upon our coordination-focused declarative approach.",
        "uid": "v-short-1096",
        "time_stamp": "",
        "time_start": "",
        "time_end": "",
        "paper_type": "short",
        "keywords": [],
        "doi": "",
        "fno": "",
        "presentation_mode": "",
        "open_access": false,
        "accessible_pdf": false,
        "has_image": false,
        "has_pdf": false,
        "paper_award": "",
        "image_caption": "",
        "external_paper_link": "",
        "youtube_ff_link": "",
        "youtube_ff_id": "",
        "bunny_ff_link": "",
        "bunny_ff_subtitles": "",
        "youtube_prerecorded_link": "",
        "youtube_prerecorded_id": "",
        "bunny_prerecorded_link": "",
        "bunny_prerecorded_subtitles": ""
    },
    "v-short-1097": {
        "slot_id": "v-short-1097",
        "session_id": "short0",
        "title": "Groot: An Interface for Editing and Configuring Automated Data Insights",
        "contributors": [
            "Sneha Gathani"
        ],
        "authors": [
            {
                "name": "Sneha Gathani",
                "email": "sgathani@cs.umd.edu",
                "affiliations": [
                    "University of Maryland, College Park, College Park, United States",
                    "Tableau Research, Seattle, United States"
                ],
                "is_corresponding": true
            },
            {
                "name": "Anamaria Crisan",
                "email": "amcrisan@uwaterloo.ca",
                "affiliations": [
                    "Tableau Research, Seattle, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Vidya Setlur",
                "email": "vsetlur@tableau.com",
                "affiliations": [
                    "Tableau Research, Palo Alto, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Arjun Srinivasan",
                "email": "arjun.srinivasan.10@gmail.com",
                "affiliations": [
                    "Tableau Research, Seattle, United States"
                ],
                "is_corresponding": false
            }
        ],
        "abstract": "Visualization tools now commonly present automated insights highlighting salient data patterns, including correlations, distributions, outliers, and differences, among others. While these insights are valuable for data exploration and chart interpretation, users currently only have a binary choice of accepting or rejecting them, lacking the flexibility to refine the system logic or customize the insight generation process. To address this limitation, we present GROOT, a prototype system that allows users to proactively specify and refine automated data insights. The system allows users to directly manipulate chart elements to receive insight recommendations based on their selections. Additionally, GROOT provides users with a manual editing interface to customize, reconfigure, or add new insights to individual charts and propagate them to future explorations. We describe a usage scenario to illustrate how these features collectively support insight editing and configuration, and discuss opportunities for future work including incorporating LLMs, improving semantic data and visualization search, and supporting insight management.",
        "uid": "v-short-1097",
        "time_stamp": "",
        "time_start": "",
        "time_end": "",
        "paper_type": "short",
        "keywords": [],
        "doi": "",
        "fno": "",
        "presentation_mode": "",
        "open_access": false,
        "accessible_pdf": false,
        "has_image": false,
        "has_pdf": false,
        "paper_award": "",
        "image_caption": "",
        "external_paper_link": "",
        "youtube_ff_link": "",
        "youtube_ff_id": "",
        "bunny_ff_link": "",
        "bunny_ff_subtitles": "",
        "youtube_prerecorded_link": "",
        "youtube_prerecorded_id": "",
        "bunny_prerecorded_link": "",
        "bunny_prerecorded_subtitles": ""
    },
    "v-short-1100": {
        "slot_id": "v-short-1100",
        "session_id": "short0",
        "title": "ConFides: A Visual Analytics Solution for Automated Speech Recognition Analysis and Exploration",
        "contributors": [
            "Sunwoo Ha"
        ],
        "authors": [
            {
                "name": "Sunwoo Ha",
                "email": "sha@wustl.edu",
                "affiliations": [
                    "Washington University in St. Louis, St. Louis, United States"
                ],
                "is_corresponding": true
            },
            {
                "name": "Chaehun Lim",
                "email": "chaelim@wustl.edu",
                "affiliations": [
                    "Washington University in St. Louis, St. Louis, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "R. Jordan Crouser",
                "email": "jcrouser@smith.edu",
                "affiliations": [
                    "Smith College, Northampton, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Alvitta Ottley",
                "email": "alvitta@wustl.edu",
                "affiliations": [
                    "Washington University in St. Louis, St. Louis, United States"
                ],
                "is_corresponding": false
            }
        ],
        "abstract": "Confidence scores of automatic speech recognition (ASR) outputs are often inadequately communicated, preventing its seamless integration into analytical workflows. In this paper, we introduce ConFides, a visual analytic system developed in collaboration with intelligence analysts to address this issue. ConFides aims to aid exploration and post-AI-transcription editing by visually representing the confidence associated with the transcription. We demonstrate how our tool can assist intelligence analysts who use ASR outputs in their analytical and exploratory tasks and how it can help mitigate misinterpretation of crucial information. We also discuss opportunities for improving textual data cleaning and model transparency for human-machine collaboration.",
        "uid": "v-short-1100",
        "time_stamp": "",
        "time_start": "",
        "time_end": "",
        "paper_type": "short",
        "keywords": [],
        "doi": "",
        "fno": "",
        "presentation_mode": "",
        "open_access": false,
        "accessible_pdf": false,
        "has_image": false,
        "has_pdf": false,
        "paper_award": "",
        "image_caption": "",
        "external_paper_link": "",
        "youtube_ff_link": "",
        "youtube_ff_id": "",
        "bunny_ff_link": "",
        "bunny_ff_subtitles": "",
        "youtube_prerecorded_link": "",
        "youtube_prerecorded_id": "",
        "bunny_prerecorded_link": "",
        "bunny_prerecorded_subtitles": ""
    },
    "v-short-1101": {
        "slot_id": "v-short-1101",
        "session_id": "short0",
        "title": "What Color Scheme is More Effective in Assisting Readers to Locate Information in a Color-Coded Article?",
        "contributors": [
            "Ho Yin Ng"
        ],
        "authors": [
            {
                "name": "Ho Yin Ng",
                "email": "samnghoyin@gmail.com",
                "affiliations": [
                    "Pennsylvania State University, University Park, United States"
                ],
                "is_corresponding": true
            },
            {
                "name": "Zeyu He",
                "email": "zmh5268@psu.edu",
                "affiliations": [
                    "Pennsylvania State University, University Park, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Ting-Hao Kenneth Huang",
                "email": "txh710@psu.edu",
                "affiliations": [
                    "Pennsylvania State University, University Park , United States"
                ],
                "is_corresponding": false
            }
        ],
        "abstract": "Color coding, a technique assigning specific colors to different information types, has proven advantages in aiding human cognitive activities, especially reading and comprehension. The rise of Large Language Models (LLMs) has streamlined document coding, enabling simple automatic text labeling with various schemes. This has the potential to make color-coding more accessible and benefit more users. However, the importance of color choice, particularly in aiding textual information seeking through various color schemes, is not well studied. This paper presents a user study assessing the effectiveness of various color schemes generated by different base colors for readers' information-seeking performance in text documents color-coded by LLMs. Participants performed information-seeking tasks within scholarly papers' abstracts, each coded with a different scheme under time constraints. Results showed that non-analogous color schemes lead to better information-seeking performance, in both accuracy and response time. Yellow-inclusive color schemes lead to shorter response times and are also preferred by most participants. These could inform the better choice of color scheme for annotating text documents. As LLMs advance document coding, we advocate for more research focusing on the \"color\" aspect of color-coding techniques.",
        "uid": "v-short-1101",
        "time_stamp": "",
        "time_start": "",
        "time_end": "",
        "paper_type": "short",
        "keywords": [],
        "doi": "",
        "fno": "",
        "presentation_mode": "",
        "open_access": false,
        "accessible_pdf": false,
        "has_image": false,
        "has_pdf": false,
        "paper_award": "",
        "image_caption": "",
        "external_paper_link": "",
        "youtube_ff_link": "",
        "youtube_ff_id": "",
        "bunny_ff_link": "",
        "bunny_ff_subtitles": "",
        "youtube_prerecorded_link": "",
        "youtube_prerecorded_id": "",
        "bunny_prerecorded_link": "",
        "bunny_prerecorded_subtitles": ""
    },
    "v-short-1109": {
        "slot_id": "v-short-1109",
        "session_id": "short0",
        "title": "Connections Beyond Data: Exploring Homophily With Visualizations",
        "contributors": [
            "Poorna Talkad Sukumar"
        ],
        "authors": [
            {
                "name": "Poorna Talkad Sukumar",
                "email": "pt2393@nyu.edu",
                "affiliations": [
                    "New York University, Brooklyn, United States"
                ],
                "is_corresponding": true
            },
            {
                "name": "Maurizio Porfiri",
                "email": "mporfiri@nyu.edu",
                "affiliations": [
                    "New York University, Brooklyn, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Oded Nov",
                "email": "onov@nyu.edu",
                "affiliations": [
                    "New York University, New York, United States"
                ],
                "is_corresponding": false
            }
        ],
        "abstract": "Homophily refers to the tendency of individuals to associate with others who are similar to them in characteristics, such as, race, ethnicity, age, gender, or interests. In this paper, we investigate if individuals exhibit racial homophily when viewing visualizations, using mass shooting data in the United States as the example topic. We conducted a crowdsourced experiment (N=450) where each participant was shown a visualization displaying the counts of mass shooting victims, highlighting the counts for one of three racial groups (White, Black, or Hispanic). Participants were assigned to view visualizations highlighting their own race or a different race to assess the influence of racial concordance on changes in affect (emotion) and attitude towards gun control. While we did not find evidence of homophily, the results showed a significant negative shift in affect across all visualization conditions. Notably, political ideology significantly impacted changes in affect, with more liberal views correlating with a more negative affect change. Our findings underscore the complexity of reactions to mass shooting visualizations and highlight the need for additional measures for understanding homophily in visualizations.",
        "uid": "v-short-1109",
        "time_stamp": "",
        "time_start": "",
        "time_end": "",
        "paper_type": "short",
        "keywords": [],
        "doi": "",
        "fno": "",
        "presentation_mode": "",
        "open_access": false,
        "accessible_pdf": false,
        "has_image": false,
        "has_pdf": false,
        "paper_award": "",
        "image_caption": "",
        "external_paper_link": "",
        "youtube_ff_link": "",
        "youtube_ff_id": "",
        "bunny_ff_link": "",
        "bunny_ff_subtitles": "",
        "youtube_prerecorded_link": "",
        "youtube_prerecorded_id": "",
        "bunny_prerecorded_link": "",
        "bunny_prerecorded_subtitles": ""
    },
    "v-short-1114": {
        "slot_id": "v-short-1114",
        "session_id": "short0",
        "title": "The Comic Construction Kit: An Activity for Students to Learn and Explain Data Visualizations",
        "contributors": [
            "Magdalena Boucher"
        ],
        "authors": [
            {
                "name": "Magdalena Boucher",
                "email": "magdalena.boucher@fhstp.ac.at",
                "affiliations": [
                    "St. P\u221a\u2202lten University of Applied Sciences, St. P\u221a\u2202lten, Austria"
                ],
                "is_corresponding": true
            },
            {
                "name": "Christina Stoiber",
                "email": "christina.stoiber@fhstp.ac.at",
                "affiliations": [
                    "St. Poelten University of Applied Sciences, St. Poelten, Austria"
                ],
                "is_corresponding": false
            },
            {
                "name": "Mandy Keck",
                "email": "mandy.keck@fh-hagenberg.at",
                "affiliations": [
                    "School of Informatics, Communications and Media, Hagenberg im M\u221a\u00bahlkreis, Austria"
                ],
                "is_corresponding": false
            },
            {
                "name": "Victor Adriel de Jesus Oliveira",
                "email": "victor.oliveira@fhstp.ac.at",
                "affiliations": [
                    "St. Poelten University of Applied Sciences, St. Poelten, Austria"
                ],
                "is_corresponding": false
            },
            {
                "name": "Wolfgang Aigner",
                "email": "wolfgang.aigner@fhstp.ac.at",
                "affiliations": [
                    "St. Poelten University of Applied Sciences, St. Poelten, Austria"
                ],
                "is_corresponding": false
            }
        ],
        "abstract": "As visualization literacy and its implications gain prominence, we need effective methods to teach and prepare students for the variety of visualizations they might encounter in an increasingly data-driven world. Recently, the potential of comics has been recognized in various data visualization contexts, including educational settings. In this paper, we describe the development of a workshop in which we use our \u201a\u00c4\u00facomic construction kit\u201a\u00c4\u00f9 as a tool for students to understand various data visualization techniques through an interactive creative approach of creating explanatory comics. We report on our insights and learnings from holding eight workshops with high school students, high school teachers, university students, and university lecturers, aiming to enhance the landscape of hands-on visualization activities that can enrich the visualization classroom. The comic construction kit and all supplemental materials are open source under a CC-BY license and available at https://fhstp.github.io/comixplain/vis4schools.html.",
        "uid": "v-short-1114",
        "time_stamp": "",
        "time_start": "",
        "time_end": "",
        "paper_type": "short",
        "keywords": [],
        "doi": "",
        "fno": "",
        "presentation_mode": "",
        "open_access": false,
        "accessible_pdf": false,
        "has_image": false,
        "has_pdf": false,
        "paper_award": "",
        "image_caption": "",
        "external_paper_link": "",
        "youtube_ff_link": "",
        "youtube_ff_id": "",
        "bunny_ff_link": "",
        "bunny_ff_subtitles": "",
        "youtube_prerecorded_link": "",
        "youtube_prerecorded_id": "",
        "bunny_prerecorded_link": "",
        "bunny_prerecorded_subtitles": ""
    },
    "v-short-1116": {
        "slot_id": "v-short-1116",
        "session_id": "short0",
        "title": "Science in a Blink: Supporting Ensemble Perception in Scalar Fields",
        "contributors": [
            "Khairi Reda"
        ],
        "authors": [
            {
                "name": "Victor A. Mateevitsi",
                "email": "vmateevitsi@anl.gov",
                "affiliations": [
                    "Argonne National Laboratory, Lemont, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Michael E. Papka",
                "email": "papka@anl.gov",
                "affiliations": [
                    "Argonne National Laboratory, Lemont, United States",
                    "University of Illinois Chicago, Chicago, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Khairi Reda",
                "email": "redak@iu.edu",
                "affiliations": [
                    "Indiana University, Indianapolis, United States"
                ],
                "is_corresponding": true
            }
        ],
        "abstract": "Visualizations support rapid analysis of scientific datasets, allowing viewers to glean aggregate information (e.g., the mean) within split-seconds. While prior research has explored this ability in conventional charts, it is unclear if spatial visualizations used by computational scientists afford a similar ensemble perception capacity. We investigate people's ability to estimate two summary statistics, mean and variance, from pseudocolor scalar fields. In a crowdsourced experiment, we find that participants can reliably characterize both statistics, although variance discrimination requires a much stronger signal. Multi-hue and diverging colormaps outperformed monochromatic, luminance ramps in aiding this extraction. Analysis of qualitative responses suggests that participants often estimate the distribution of hotspots and valleys as visual proxies for data statistics. These findings suggest that people's summary interpretation of spatial datasets is likely driven by the appearance of discrete color segments, rather than assessments of overall luminance. Implicit color segmentation in quantitative displays could thus prove more useful than previously assumed by facilitating quick, gist-level judgments about color-coded visualizations.",
        "uid": "v-short-1116",
        "time_stamp": "",
        "time_start": "",
        "time_end": "",
        "paper_type": "short",
        "keywords": [],
        "doi": "",
        "fno": "",
        "presentation_mode": "",
        "open_access": false,
        "accessible_pdf": false,
        "has_image": false,
        "has_pdf": false,
        "paper_award": "",
        "image_caption": "",
        "external_paper_link": "",
        "youtube_ff_link": "",
        "youtube_ff_id": "",
        "bunny_ff_link": "",
        "bunny_ff_subtitles": "",
        "youtube_prerecorded_link": "",
        "youtube_prerecorded_id": "",
        "bunny_prerecorded_link": "",
        "bunny_prerecorded_subtitles": ""
    },
    "v-short-1117": {
        "slot_id": "v-short-1117",
        "session_id": "short0",
        "title": "AltGeoViz: Facilitating Accessible Geovisualization",
        "contributors": [
            "Chu Li"
        ],
        "authors": [
            {
                "name": "Chu Li",
                "email": "chuchuli@cs.washington.edu",
                "affiliations": [
                    "University of Washington, Seattle, United States"
                ],
                "is_corresponding": true
            },
            {
                "name": "Rock Yuren Pang",
                "email": "ypang2@cs.washington.edu",
                "affiliations": [
                    "University of Washington, Seattle, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Ather Sharif",
                "email": "asharif@cs.washington.edu",
                "affiliations": [
                    "University of Washington, Seattle, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Arnavi Chheda-Kothary",
                "email": "chheda@cs.washington.edu",
                "affiliations": [
                    "University of Washington, Seattle, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Jeffrey Heer",
                "email": "jheer@uw.edu",
                "affiliations": [
                    "University of Washington, Seattle, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Jon E. Froehlich",
                "email": "jonf@cs.uw.edu",
                "affiliations": [
                    "University of Washington, Seattle, United States"
                ],
                "is_corresponding": false
            }
        ],
        "abstract": "Geovisualizations are powerful tools for exploratory spatial analysis, enabling sighted users to discern patterns, trends, and relationships within geographic data. However, these visual tools have remained largely inaccessible to screen-reader users. We present AltGeoViz, a new system we designed to facilitate geovisualization exploration for these users. AltGeoViz dynamically generates alt-text descriptions based on the user's current map view, providing summaries of spatial patterns and descriptive statistics. In a study of five screen-reader users, we found that AltGeoViz enabled them to interact with geovisualizations in previously infeasible ways. Participants demonstrated a clear understanding of data summaries and their location context, and they could synthesize spatial understandings of their explorations. Moreover, we identified key areas for improvement, such as the addition of intuitive spatial navigation controls and comparative analysis features.",
        "uid": "v-short-1117",
        "time_stamp": "",
        "time_start": "",
        "time_end": "",
        "paper_type": "short",
        "keywords": [],
        "doi": "",
        "fno": "",
        "presentation_mode": "",
        "open_access": false,
        "accessible_pdf": false,
        "has_image": false,
        "has_pdf": false,
        "paper_award": "",
        "image_caption": "",
        "external_paper_link": "",
        "youtube_ff_link": "",
        "youtube_ff_id": "",
        "bunny_ff_link": "",
        "bunny_ff_subtitles": "",
        "youtube_prerecorded_link": "",
        "youtube_prerecorded_id": "",
        "bunny_prerecorded_link": "",
        "bunny_prerecorded_subtitles": ""
    },
    "v-short-1119": {
        "slot_id": "v-short-1119",
        "session_id": "short0",
        "title": "Visualization of 2D Scalar Field Ensembles Using Volume Visualization of the Empirical Distribution Function",
        "contributors": [
            "Tomas Rodolfo Daetz Chacon"
        ],
        "authors": [
            {
                "name": "Tomas Rodolfo Daetz Chacon",
                "email": "daetz@informatik.uni-leipzig.de",
                "affiliations": [
                    "Institute of Computer Science, Leipzig University, Leipzig, Germany"
                ],
                "is_corresponding": true
            },
            {
                "name": "Michael B\u221a\u2202ttinger",
                "email": "boettinger@dkrz.de",
                "affiliations": [
                    "German Climate Computing Center (DKRZ), Hamburg, Germany"
                ],
                "is_corresponding": false
            },
            {
                "name": "Gerik Scheuermann",
                "email": "scheuermann@informatik.uni-leipzig.de",
                "affiliations": [
                    "Leipzig University, Leipzig, Germany"
                ],
                "is_corresponding": false
            },
            {
                "name": "Christian Heine",
                "email": "heine@informatik.uni-leipzig.de",
                "affiliations": [
                    "Leipzig University, Leipzig, Germany"
                ],
                "is_corresponding": false
            }
        ],
        "abstract": "Analyzing uncertainty in spatial data is a vital task in many domains, as for example with climate and weather simulation ensembles. Although there are many methods to support the analysis of the uncertainty, such as uncertain isocontours or calculation of statistical values, it is still a challenge to get an overview of the uncertainty and then decide a further method or parameter to analyze the data, or investigate further some region or point of interest. We present cumulative height fields, a visualization method for 2D scalar field ensembles using the marginal empirical distribution function and show preliminary results using volume rendering and slicing for the Max Planck Institute Grand Ensemble.",
        "uid": "v-short-1119",
        "time_stamp": "",
        "time_start": "",
        "time_end": "",
        "paper_type": "short",
        "keywords": [],
        "doi": "",
        "fno": "",
        "presentation_mode": "",
        "open_access": false,
        "accessible_pdf": false,
        "has_image": false,
        "has_pdf": false,
        "paper_award": "",
        "image_caption": "",
        "external_paper_link": "",
        "youtube_ff_link": "",
        "youtube_ff_id": "",
        "bunny_ff_link": "",
        "bunny_ff_subtitles": "",
        "youtube_prerecorded_link": "",
        "youtube_prerecorded_id": "",
        "bunny_prerecorded_link": "",
        "bunny_prerecorded_subtitles": ""
    },
    "v-short-1121": {
        "slot_id": "v-short-1121",
        "session_id": "short0",
        "title": "Improving Property Graph Layouts by Leveraging Attribute Similarity for Structurally Equivalent Nodes",
        "contributors": [
            "Patrick Mackey"
        ],
        "authors": [
            {
                "name": "Patrick Mackey",
                "email": "patrick.mackey@pnnl.gov",
                "affiliations": [
                    "Pacific Northwest National Lab, Richland, United States"
                ],
                "is_corresponding": true
            },
            {
                "name": "Jacob Miller",
                "email": "jacobmiller1@arizona.edu",
                "affiliations": [
                    "University of Arizona, Tucson, United States",
                    "Pacific Northwest National Laboratory, Richland, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Liz Faultersack",
                "email": "liz.f@pnnl.gov",
                "affiliations": [
                    "Pacific Northwest National Laboratory, Richland, United States"
                ],
                "is_corresponding": false
            }
        ],
        "abstract": "Many real-world networks contain structurally-equivalent nodes. These are defined as vertices that share the same set of neighboring nodes, making them interchangeable with a traditional graph layout approach. However, many real-world graphs also have properties associated with nodes, adding additional meaning to them. We present an approach for swapping locations of structurally-equivalent nodes in graph layout so that those with more similar properties have closer proximity to each other. This improves the usefulness of the visualization from an attribute perspective without negatively impacting the visualization from a structural perspective. We include an algorithm for finding these sets of nodes in linear time, as well as methodologies for ordering nodes based on their attribute similarity, which works for scalar, ordinal, multidimensional, and categorical data.",
        "uid": "v-short-1121",
        "time_stamp": "",
        "time_start": "",
        "time_end": "",
        "paper_type": "short",
        "keywords": [],
        "doi": "",
        "fno": "",
        "presentation_mode": "",
        "open_access": false,
        "accessible_pdf": false,
        "has_image": false,
        "has_pdf": false,
        "paper_award": "",
        "image_caption": "",
        "external_paper_link": "",
        "youtube_ff_link": "",
        "youtube_ff_id": "",
        "bunny_ff_link": "",
        "bunny_ff_subtitles": "",
        "youtube_prerecorded_link": "",
        "youtube_prerecorded_id": "",
        "bunny_prerecorded_link": "",
        "bunny_prerecorded_subtitles": ""
    },
    "v-short-1126": {
        "slot_id": "v-short-1126",
        "session_id": "short0",
        "title": "FAVis: Visual Analytics of Factor Analysis for Psychological Research",
        "contributors": [
            "Yikai Lu"
        ],
        "authors": [
            {
                "name": "Yikai Lu",
                "email": "ylu22@nd.edu",
                "affiliations": [
                    "University of Notre Dame, Notre Dame, United States",
                    "University of Notre Dame, Notre Dame, United States"
                ],
                "is_corresponding": true
            },
            {
                "name": "Chaoli Wang",
                "email": "chaoli.wang@nd.edu",
                "affiliations": [
                    "University of Notre Dame, Notre Dame, United States"
                ],
                "is_corresponding": false
            }
        ],
        "abstract": "Psychological research often involves understanding psychological constructs through conducting factor analysis on data collected by a questionnaire, which can comprise hundreds of questions. Without interactive systems for interpreting factor models, researchers are frequently exposed to subjectivity, potentially leading to misinterpretations or overlooked crucial information. This paper introduces FAVis, a novel interactive visualization tool designed to aid researchers in interpreting and evaluating factor analysis results. FAVis enhances the understanding of relationships between variables and factors by supporting multiple views for visualizing factor loadings and correlations, allowing users to analyze information from various perspectives. The primary feature of FAVis is to enable users to set optimal thresholds for factor loadings to balance clarity and information retention. FAVis also allows users to assign tags to variables, enhancing the understanding of factors by linking them to their associated psychological constructs. We conduct a case study on a dataset from the Motivational State Questionnaire, utilizing a three-factor common factor model. Our user study demonstrates the utility of FAVis in various tasks.",
        "uid": "v-short-1126",
        "time_stamp": "",
        "time_start": "",
        "time_end": "",
        "paper_type": "short",
        "keywords": [],
        "doi": "",
        "fno": "",
        "presentation_mode": "",
        "open_access": false,
        "accessible_pdf": false,
        "has_image": false,
        "has_pdf": false,
        "paper_award": "",
        "image_caption": "",
        "external_paper_link": "",
        "youtube_ff_link": "",
        "youtube_ff_id": "",
        "bunny_ff_link": "",
        "bunny_ff_subtitles": "",
        "youtube_prerecorded_link": "",
        "youtube_prerecorded_id": "",
        "bunny_prerecorded_link": "",
        "bunny_prerecorded_subtitles": ""
    },
    "v-short-1127": {
        "slot_id": "v-short-1127",
        "session_id": "short0",
        "title": "Investigating the Apple Vision Pro Spatial Computing Platform for GPU-Based Volume Visualization",
        "contributors": [
            "Camilla Hrycak"
        ],
        "authors": [
            {
                "name": "Camilla Hrycak",
                "email": "camilla.hrycak@uni-due.de",
                "affiliations": [
                    "University of Duisburg-Essen, Duisburg, Germany"
                ],
                "is_corresponding": true
            },
            {
                "name": "David Lewakis",
                "email": "david.lewakis@stud.uni-due.de",
                "affiliations": [
                    "University of Duisburg-Essen, Duisburg, Germany"
                ],
                "is_corresponding": false
            },
            {
                "name": "Jens Harald Krueger",
                "email": "jens.krueger@uni-due.de",
                "affiliations": [
                    "University of Duisburg-Essen, Duisburg, Germany"
                ],
                "is_corresponding": false
            }
        ],
        "abstract": "In this paper, we analyze the Apple Vision Pro hardware and the visionOS software platform, assessing their capabilities for volume rendering of structured grids, a prevalent technique across various applications. The Apple Vision Pro supports multiple display modes, from classical augmented reality (AR) using video see-through technology to immersive virtual reality (VR) environments that exclusively render virtual objects. These modes utilize different APIs and exhibit distinct capabilities. Our focus is on direct volume rendering, selected for its implementation challenges due to the native graphics APIs being predominantly oriented towards surface shading. Volume rendering is particularly vital in fields where AR and VR visualizations offer substantial benefits, such as in medicine and manufacturing. Despite its initial high cost, we anticipate that the Vision Pro will become more accessible and affordable over time, following Apple's track record of market expansion. As these devices become more prevalent, understanding how to effectively program and utilize them becomes increasingly important, offering significant opportunities for innovation and practical applications in various sectors.",
        "uid": "v-short-1127",
        "time_stamp": "",
        "time_start": "",
        "time_end": "",
        "paper_type": "short",
        "keywords": [],
        "doi": "",
        "fno": "",
        "presentation_mode": "",
        "open_access": false,
        "accessible_pdf": false,
        "has_image": false,
        "has_pdf": false,
        "paper_award": "",
        "image_caption": "",
        "external_paper_link": "",
        "youtube_ff_link": "",
        "youtube_ff_id": "",
        "bunny_ff_link": "",
        "bunny_ff_subtitles": "",
        "youtube_prerecorded_link": "",
        "youtube_prerecorded_id": "",
        "bunny_prerecorded_link": "",
        "bunny_prerecorded_subtitles": ""
    },
    "v-short-1130": {
        "slot_id": "v-short-1130",
        "session_id": "short0",
        "title": "DaVE - A Curated Database of Visualization Examples",
        "contributors": [
            "Jens Koenen"
        ],
        "authors": [
            {
                "name": "Jens Koenen",
                "email": "koenen@informatik.rwth-aachen.de",
                "affiliations": [
                    "RWTH Aachen University, Aachen, Germany"
                ],
                "is_corresponding": true
            },
            {
                "name": "Marvin Petersen",
                "email": "m.petersen@rptu.de",
                "affiliations": [
                    "RPTU Kaiserslautern-Landau, Kaiserslautern, Germany"
                ],
                "is_corresponding": false
            },
            {
                "name": "Christoph Garth",
                "email": "garth@rptu.de",
                "affiliations": [
                    "RPTU Kaiserslautern-Landau, Kaiserslautern, Germany"
                ],
                "is_corresponding": false
            },
            {
                "name": "Tim Gerrits",
                "email": "gerrits@vis.rwth-aachen.de",
                "affiliations": [
                    "RWTH Aachen University, Aachen, Germany"
                ],
                "is_corresponding": false
            }
        ],
        "abstract": "Visualization, from simple line plots to complex high-dimensional visual analysis systems, has established itself throughout numerous domains to explore, analyze, and evaluate data. Applying such visualizations in the context of simulation science where High-Performance Computing (HPC) produces ever-growing amounts of data that is more complex, potentially multidimensional, and multi-modal, takes up resources and a high level of technological experience often not available to domain experts. In this work, we present DaVE - a curated database of visualization examples, which aims to provide state-of-the-art and advanced visualization methods that arise in the context of HPC applications. Based on domain- or data-specific descriptors entered by the user, DaVE provides a list of appropriate visualization techniques, each accompanied by descriptions, examples, references, and resources. Sample code, adaptable container templates, and recipes for easy integration in HPC applications can be downloaded for easy access to high-fidelity visualizations. While the database is currently filled with a limited number of entries based on a broad evaluation of needs and challenges of current HPC users, DaVE is designed to be easily extended by experts from both the visualization and HPC communities.",
        "uid": "v-short-1130",
        "time_stamp": "",
        "time_start": "",
        "time_end": "",
        "paper_type": "short",
        "keywords": [],
        "doi": "",
        "fno": "",
        "presentation_mode": "",
        "open_access": false,
        "accessible_pdf": false,
        "has_image": false,
        "has_pdf": false,
        "paper_award": "",
        "image_caption": "",
        "external_paper_link": "",
        "youtube_ff_link": "",
        "youtube_ff_id": "",
        "bunny_ff_link": "",
        "bunny_ff_subtitles": "",
        "youtube_prerecorded_link": "",
        "youtube_prerecorded_id": "",
        "bunny_prerecorded_link": "",
        "bunny_prerecorded_subtitles": ""
    },
    "v-short-1135": {
        "slot_id": "v-short-1135",
        "session_id": "short0",
        "title": "Feature Clock: High-Dimensional Effects in Two-Dimensional Plots",
        "contributors": [
            "Olga Ovcharenko"
        ],
        "authors": [
            {
                "name": "Olga Ovcharenko",
                "email": "ovcharenko.folga@gmail.com",
                "affiliations": [
                    "ETH Z\u221a\u00barich, Z\u221a\u00barich, Switzerland"
                ],
                "is_corresponding": true
            },
            {
                "name": "Rita Sevastjanova",
                "email": "rita.sevastjanova@uni-konstanz.de",
                "affiliations": [
                    "ETH Z\u221a\u00barich, Z\u221a\u00barich, Switzerland"
                ],
                "is_corresponding": false
            },
            {
                "name": "Valentina Boeva",
                "email": "valentina.boeva@inf.ethz.ch",
                "affiliations": [
                    "ETH Zurich, Z\u221a\u00barich, Switzerland"
                ],
                "is_corresponding": false
            }
        ],
        "abstract": "Humans struggle to perceive and interpret high-dimensional data. Therefore, high-dimensional data are often projected into two dimensions for visualization. Many applications benefit from complex nonlinear dimensionality reduction techniques, but the effects of individual high-dimensional features are hard to explain in the two-dimensional space. Most visualization solutions use multiple two-dimensional plots, each showing the effect of one high-dimensional feature in two dimensions; this approach creates a need for a visual inspection of k plots for a k-dimensional input space. Our solution, Feature Clock, provides a novel approach that eliminates the need to inspect these k plots to grasp the influence of original features on the data structure depicted in two dimensions. Feature Clock enhances the explainability and compactness of visualizations of embedded data and is available in an open-source Python library.",
        "uid": "v-short-1135",
        "time_stamp": "",
        "time_start": "",
        "time_end": "",
        "paper_type": "short",
        "keywords": [],
        "doi": "",
        "fno": "",
        "presentation_mode": "",
        "open_access": false,
        "accessible_pdf": false,
        "has_image": false,
        "has_pdf": false,
        "paper_award": "",
        "image_caption": "",
        "external_paper_link": "",
        "youtube_ff_link": "",
        "youtube_ff_id": "",
        "bunny_ff_link": "",
        "bunny_ff_subtitles": "",
        "youtube_prerecorded_link": "",
        "youtube_prerecorded_id": "",
        "bunny_prerecorded_link": "",
        "bunny_prerecorded_subtitles": ""
    },
    "v-short-1144": {
        "slot_id": "v-short-1144",
        "session_id": "short0",
        "title": "Opening the black box of 3D reconstruction error analysis with VECTOR",
        "contributors": [
            "Mauricio Hess-Flores"
        ],
        "authors": [
            {
                "name": "Racquel Fygenson",
                "email": "racquel.fygenson@gmail.com",
                "affiliations": [
                    "Northeastern University, Boston, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Kazi Jawad",
                "email": "kjawad@andrew.cmu.edu",
                "affiliations": [
                    "Weta FX, Auckland, New Zealand"
                ],
                "is_corresponding": false
            },
            {
                "name": "Zongzhan Li",
                "email": "zongzhanisabelli@gmail.com",
                "affiliations": [
                    "Art Center, Pasadena, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Francois Ayoub",
                "email": "francois.ayoub@jpl.nasa.gov",
                "affiliations": [
                    "California Institute of Technology, Pasadena, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Robert G Deen",
                "email": "bob.deen@jpl.nasa.gov",
                "affiliations": [
                    "California Institute of Technology, Pasadena, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Scott Davidoff",
                "email": "sd@scottdavidoff.com",
                "affiliations": [
                    "California Institute of Technology, Pasadena, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Dominik Moritz",
                "email": "domoritz@cmu.edu",
                "affiliations": [
                    "Carnegie Mellon University, Pittsburgh, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Mauricio Hess-Flores",
                "email": "mauricio.a.hess.flores@jpl.nasa.gov",
                "affiliations": [
                    "NASA-JPL, Pasadena, United States"
                ],
                "is_corresponding": true
            }
        ],
        "abstract": "Reconstruction of 3D scenes from 2D images is a technical challenge that impacts domains from Earth and planetary sciences and space exploration to augmented and virtual reality. Typically, reconstruction algorithms first identify common features across images and then minimize reconstruction errors after estimating the shape of the terrain. This bundle adjustment (BA) step optimizes around a single, simplifying scalar value that obfuscates many possible causes of reconstruction errors (e.g., initial estimate of the position and orientation of the camera, lighting conditions, ease of feature detection in the terrain). Reconstruction errors can lead to inaccurate scientific inferences or endanger a spacecraft exploring a remote environment. To address this challenge, we present VECTOR, a visual analysis tool that improves error inspection for stereo reconstruction BA. VECTOR provides analysts with previously unavailable visibility into feature locations, camera pose, and computed 3D points. VECTOR was developed in partnership with the Perseverance Mars Rover and Ingenuity Mars Helicopter terrain reconstruction team at the NASA Jet Propulsion Laboratory. We report on how this tool was used to debug and improve terrain reconstruction for the Mars 2020 mission.",
        "uid": "v-short-1144",
        "time_stamp": "",
        "time_start": "",
        "time_end": "",
        "paper_type": "short",
        "keywords": [],
        "doi": "",
        "fno": "",
        "presentation_mode": "",
        "open_access": false,
        "accessible_pdf": false,
        "has_image": false,
        "has_pdf": false,
        "paper_award": "",
        "image_caption": "",
        "external_paper_link": "",
        "youtube_ff_link": "",
        "youtube_ff_id": "",
        "bunny_ff_link": "",
        "bunny_ff_subtitles": "",
        "youtube_prerecorded_link": "",
        "youtube_prerecorded_id": "",
        "bunny_prerecorded_link": "",
        "bunny_prerecorded_subtitles": ""
    },
    "v-short-1146": {
        "slot_id": "v-short-1146",
        "session_id": "short0",
        "title": "Visualizations on Smart Watches while Running: It Actually Helps!",
        "contributors": [
            "Charles Perin"
        ],
        "authors": [
            {
                "name": "Sarina Kashanj",
                "email": "sarinaksj@uvic.ca",
                "affiliations": [
                    "University of Victoria, Victoria, Canada"
                ],
                "is_corresponding": false
            },
            {
                "name": "Xiyao Wang",
                "email": "xiyao.wang23@gmail.com",
                "affiliations": [
                    "University of Victoria, Victoira, Canada",
                    "Delft University of Technology, Delft, Netherlands"
                ],
                "is_corresponding": false
            },
            {
                "name": "Charles Perin",
                "email": "cperin@uvic.ca",
                "affiliations": [
                    "University of Victoria, Victoria, Canada"
                ],
                "is_corresponding": true
            }
        ],
        "abstract": "Millions of runners rely on smart watches that display running-related metrics such as pace, heart rate and distance for training and racing -- mostly with text and numbers. Although research tells us that visualizations are a good alternative to text on smart watches, we know little about how visualizations can help in realistic running scenarios. We conducted a study in which 20 runners completed running-related tasks on an outdoor track using both text and visualizations. Our results show that runners are 1.5 to 8 times faster in completing those tasks with visualizations than with text, prefer visualizations to text, and would use such visualizations while running -- were they available on their smart watch.",
        "uid": "v-short-1146",
        "time_stamp": "",
        "time_start": "",
        "time_end": "",
        "paper_type": "short",
        "keywords": [],
        "doi": "",
        "fno": "",
        "presentation_mode": "",
        "open_access": false,
        "accessible_pdf": false,
        "has_image": false,
        "has_pdf": false,
        "paper_award": "",
        "image_caption": "",
        "external_paper_link": "",
        "youtube_ff_link": "",
        "youtube_ff_id": "",
        "bunny_ff_link": "",
        "bunny_ff_subtitles": "",
        "youtube_prerecorded_link": "",
        "youtube_prerecorded_id": "",
        "bunny_prerecorded_link": "",
        "bunny_prerecorded_subtitles": ""
    },
    "v-short-1150": {
        "slot_id": "v-short-1150",
        "session_id": "short0",
        "title": "PyGWalker: On-the-fly Assistant for Exploratory Visual Data Analysis",
        "contributors": [
            "Yue Yu"
        ],
        "authors": [
            {
                "name": "Yue Yu",
                "email": "yue.yu@connect.ust.hk",
                "affiliations": [
                    "The Hong Kong University of Science and Technology, Hong Kong, China",
                    "Kanaries Data Inc., Hangzhou, China"
                ],
                "is_corresponding": true
            },
            {
                "name": "Leixian Shen",
                "email": "lshenaj@connect.ust.hk",
                "affiliations": [
                    "The Hong Kong University of Science and Technology, Hong Kong, China"
                ],
                "is_corresponding": false
            },
            {
                "name": "Fei Long",
                "email": "feilong@kanaries.net",
                "affiliations": [
                    "Kanaries Data Inc., Hangzhou, China"
                ],
                "is_corresponding": false
            },
            {
                "name": "Huamin Qu",
                "email": "huamin@cse.ust.hk",
                "affiliations": [
                    "The Hong Kong University of Science and Technology, Hong Kong, China"
                ],
                "is_corresponding": false
            },
            {
                "name": "Hao Chen",
                "email": "haochen@kanaries.net",
                "affiliations": [
                    "Kanaries Data Inc., Hangzhou, China"
                ],
                "is_corresponding": false
            }
        ],
        "abstract": "Exploratory visual data analysis tools empower data analysts to efficiently and intuitively explore data insights throughout the entire analysis cycle. However, the gap between common programmatic analysis (e.g., within computational notebooks) and exploratory visual analysis leads to a disjointed and inefficient data analysis experience. To bridge this gap, we developed PyGWalker, a Python library that offers on-the-fly assistance for exploratory visual data analysis. It features a lightweight and intuitive GUI with a shelf builder modality. Its loosely coupled architecture supports multiple computational environments to accommodate varying data sizes. Since its release in February 2023, PyGWalker has gained much attention, with 468k downloads on PyPI and over 9.8k stars on GitHub as of April 2024. This demonstrates its value to the data science and visualization community, with researchers and developers integrating it into their own applications and studies.",
        "uid": "v-short-1150",
        "time_stamp": "",
        "time_start": "",
        "time_end": "",
        "paper_type": "short",
        "keywords": [],
        "doi": "",
        "fno": "",
        "presentation_mode": "",
        "open_access": false,
        "accessible_pdf": false,
        "has_image": false,
        "has_pdf": false,
        "paper_award": "",
        "image_caption": "",
        "external_paper_link": "",
        "youtube_ff_link": "",
        "youtube_ff_id": "",
        "bunny_ff_link": "",
        "bunny_ff_subtitles": "",
        "youtube_prerecorded_link": "",
        "youtube_prerecorded_id": "",
        "bunny_prerecorded_link": "",
        "bunny_prerecorded_subtitles": ""
    },
    "v-short-1155": {
        "slot_id": "v-short-1155",
        "session_id": "short0",
        "title": "Active Appearance and Spatial Variation Can Improve Visibility in Area Labels for Augmented Reality",
        "contributors": [
            "James Tompkin"
        ],
        "authors": [
            {
                "name": "Hojung Kwon",
                "email": "hojung_kwon@brown.edu",
                "affiliations": [
                    "Brown University, Providence, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Yuanbo Li",
                "email": "yuanbo_li@brown.edu",
                "affiliations": [
                    "Brown University, Providence, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Xiaohan Ye",
                "email": "chloe_ye2019@hotmail.com",
                "affiliations": [
                    "Brown University, Providence, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Praccho Muna-McQuay",
                "email": "praccho_muna-mcquay@brown.edu",
                "affiliations": [
                    "Brown University, Providence, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Liuren Yin",
                "email": "liuren.yin@duke.edu",
                "affiliations": [
                    "Duke University, Durham, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "James Tompkin",
                "email": "james_tompkin@brown.edu",
                "affiliations": [
                    "Brown University, Providence, United States"
                ],
                "is_corresponding": true
            }
        ],
        "abstract": "Augmented reality (AR) area labels can highlight real-life objects, visualize real world regions with arbitrary boundaries, and show invisible objects or features. Environment conditions such as lighting and clutter can decrease fixed or passive label visibility, and labels that have high opacity levels can occlude crucial details in the environment. We design and evaluate active AR area label visualization modes to enhance visibility across real-life environments, while still retaining environment details within the label. For this, we define a distant characteristic color from the environment in perceptual CIELAB space, then introduce spatial variations among label pixel colors based on the underlying environment variation. In a user study with 18 participants, we discovered that our active label visualization modes can be comparable in visibility to a fixed green baseline by Gabbard et al., and can outperform it with added spatial variation in cluttered environments, across varying levels of lighting (e.g., nighttime), and in environments with colors similar to the fixed baseline color.",
        "uid": "v-short-1155",
        "time_stamp": "",
        "time_start": "",
        "time_end": "",
        "paper_type": "short",
        "keywords": [],
        "doi": "",
        "fno": "",
        "presentation_mode": "",
        "open_access": false,
        "accessible_pdf": false,
        "has_image": false,
        "has_pdf": false,
        "paper_award": "",
        "image_caption": "",
        "external_paper_link": "",
        "youtube_ff_link": "",
        "youtube_ff_id": "",
        "bunny_ff_link": "",
        "bunny_ff_subtitles": "",
        "youtube_prerecorded_link": "",
        "youtube_prerecorded_id": "",
        "bunny_prerecorded_link": "",
        "bunny_prerecorded_subtitles": ""
    },
    "v-short-1156": {
        "slot_id": "v-short-1156",
        "session_id": "short0",
        "title": "An Overview + Detail Layout for Visualizing Compound Graphs",
        "contributors": [
            "Chang Han"
        ],
        "authors": [
            {
                "name": "Chang Han",
                "email": "hatch.on27@gmail.com",
                "affiliations": [
                    "University of Utah, Salt Lake City, United States"
                ],
                "is_corresponding": true
            },
            {
                "name": "Justin Lieffers",
                "email": "lieffers@arizona.edu",
                "affiliations": [
                    "University of Arizona, Tucson, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Clayton Morrison",
                "email": "claytonm@arizona.edu",
                "affiliations": [
                    "University of Arizona, Tucson, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Katherine E. Isaacs",
                "email": "kisaacs@sci.utah.edu",
                "affiliations": [
                    "The University of Utah, Salt Lake City, United States"
                ],
                "is_corresponding": false
            }
        ],
        "abstract": "Compound graphs are networks in which vertices can be grouped into larger subsets, with these subsets capable of further grouping, resulting in a nesting that can be many levels deep. Such graphs arise in several applications including biological workflows, chemical equations, and computational data flow analysis. Common layouts prioritize the lowest level of the grouping, down to the individual ungrouped vertices, which can make the higher level grouped structures more difficult to discern, especially in deeply nested networks. We contribute an overview+detail layout that preserves the saliency of the higher level network structure when groups are expanded to show internal nested structure. Our layout draws inner structures adjacent to their parents, using a modified tree layout to place substructures. We describe our algorithm and then present case studies demonstrating the layout's utility to a domain expert working on data flow analysis. Finally, we discuss network parameters and analysis situations in which our layout is well suited.",
        "uid": "v-short-1156",
        "time_stamp": "",
        "time_start": "",
        "time_end": "",
        "paper_type": "short",
        "keywords": [],
        "doi": "",
        "fno": "",
        "presentation_mode": "",
        "open_access": false,
        "accessible_pdf": false,
        "has_image": false,
        "has_pdf": false,
        "paper_award": "",
        "image_caption": "",
        "external_paper_link": "",
        "youtube_ff_link": "",
        "youtube_ff_id": "",
        "bunny_ff_link": "",
        "bunny_ff_subtitles": "",
        "youtube_prerecorded_link": "",
        "youtube_prerecorded_id": "",
        "bunny_prerecorded_link": "",
        "bunny_prerecorded_subtitles": ""
    },
    "v-short-1159": {
        "slot_id": "v-short-1159",
        "session_id": "short0",
        "title": "Micro Visualizations on a Smartwatch: Assessing Reading Performance While Walking",
        "contributors": [
            "Fairouz Grioui"
        ],
        "authors": [
            {
                "name": "Fairouz Grioui",
                "email": "fairouz.grioui@vis.uni-stuttgart.de",
                "affiliations": [
                    "University of Stuttgart, Stuttgart, Germany"
                ],
                "is_corresponding": true
            },
            {
                "name": "Tanja Blascheck",
                "email": "research@blascheck.eu",
                "affiliations": [
                    "University of Stuttgart, Stuttgart, Germany"
                ],
                "is_corresponding": false
            },
            {
                "name": "Lijie Yao",
                "email": "yaolijie0219@gmail.com",
                "affiliations": [
                    "Universit\u221a\u00a9 Paris-Saclay, CNRS, Orsay, France",
                    "Inria, Saclay, France"
                ],
                "is_corresponding": false
            },
            {
                "name": "Petra Isenberg",
                "email": "petra.isenberg@inria.fr",
                "affiliations": [
                    "Universit\u221a\u00a9 Paris-Saclay, CNRS, Orsay, France",
                    "Inria, Saclay, France"
                ],
                "is_corresponding": false
            }
        ],
        "abstract": "With two studies, we assess how different walking trajectories (straight line, circular, and infinity) and speeds (2 km/h, 4 km/h, and 6 km/h) influence the accuracy and response time of participants reading micro visualizations on a smartwatch. We showed our participants common watch face micro visualizations including date, time, weather information, and four complications showing progress charts of fitness data. Our findings suggest that while walking trajectories did not significantly affect reading performance, overall walking activity, especially at high speeds, hurt reading accuracy and, to some extent, response time.",
        "uid": "v-short-1159",
        "time_stamp": "",
        "time_start": "",
        "time_end": "",
        "paper_type": "short",
        "keywords": [],
        "doi": "",
        "fno": "",
        "presentation_mode": "",
        "open_access": false,
        "accessible_pdf": false,
        "has_image": false,
        "has_pdf": false,
        "paper_award": "",
        "image_caption": "",
        "external_paper_link": "",
        "youtube_ff_link": "",
        "youtube_ff_id": "",
        "bunny_ff_link": "",
        "bunny_ff_subtitles": "",
        "youtube_prerecorded_link": "",
        "youtube_prerecorded_id": "",
        "bunny_prerecorded_link": "",
        "bunny_prerecorded_subtitles": ""
    },
    "v-short-1161": {
        "slot_id": "v-short-1161",
        "session_id": "short0",
        "title": "Visualizing an Exascale Data Center Digital Twin: Considerations, Challenges and Opportunities",
        "contributors": [
            "Matthias Maiterth"
        ],
        "authors": [
            {
                "name": "Matthias Maiterth",
                "email": "maiterthm@ornl.gov",
                "affiliations": [
                    "Oak Ridge National Laboratory, Oak Ridge, United States"
                ],
                "is_corresponding": true
            },
            {
                "name": "Wes Brewer",
                "email": "brewerwh@ornl.gov",
                "affiliations": [
                    "Oak Ridge National Laboratory, Oak Ridge, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Dane De Wet",
                "email": "dewetd@ornl.gov",
                "affiliations": [
                    "Oak Ridge National Laboratory, Oak Ridge, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Scott Greenwood",
                "email": "greenwoodms@ornl.gov",
                "affiliations": [
                    "Oak Ridge National Laboratory, Oak Ridge, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Vineet Kumar",
                "email": "kumarv@ornl.gov",
                "affiliations": [
                    "Oak Ridge National Laboratory, Oak Ridge, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Jesse Hines",
                "email": "hinesjr@ornl.gov",
                "affiliations": [
                    "Oak Ridge National Laboratory, Oak Ridge, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Sedrick L Bouknight",
                "email": "bouknightsl@ornl.gov",
                "affiliations": [
                    "Oak Ridge National Laboratory, Oak Ridge, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Zhe Wang",
                "email": "wangz@ornl.gov",
                "affiliations": [
                    "Oak Ridge National Laboratory, Oak Ridge, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Tim Dykes",
                "email": "tim.dykes@hpe.com",
                "affiliations": [
                    "Hewlett Packard Enterprise, Berkshire, United Kingdom"
                ],
                "is_corresponding": false
            },
            {
                "name": "Feiyi Wang",
                "email": "fwang2@ornl.gov",
                "affiliations": [
                    "Oak Ridge National Laboratory, Oak Ridge, United States"
                ],
                "is_corresponding": false
            }
        ],
        "abstract": "Digital twins are an excellent tool to model, visualize, and simulate complex systems, to understand and optimize their operation. In this work, we present the technical challenges of real-time visualization of a digital twin of the Frontier supercomputer. We show the initial prototype and current state of the twin and highlight technical design challenges of visualizing such a large High Performance Computing (HPC) system. The goal is to understand the use of augmented reality as a primary way to extract information and collaborate on digital twins of complex systems. This leverages the spatio-temporal aspect of a 3D representation of a digital twin, with the ability to view historical and real-time telemetry, triggering simulations of a system state and viewing the results, which can be augmented via dashboards for details. Finally, we discuss considerations and opportunities for augmented reality of digital twins of large-scale, parallel computers.",
        "uid": "v-short-1161",
        "time_stamp": "",
        "time_start": "",
        "time_end": "",
        "paper_type": "short",
        "keywords": [],
        "doi": "",
        "fno": "",
        "presentation_mode": "",
        "open_access": false,
        "accessible_pdf": false,
        "has_image": false,
        "has_pdf": false,
        "paper_award": "",
        "image_caption": "",
        "external_paper_link": "",
        "youtube_ff_link": "",
        "youtube_ff_id": "",
        "bunny_ff_link": "",
        "bunny_ff_subtitles": "",
        "youtube_prerecorded_link": "",
        "youtube_prerecorded_id": "",
        "bunny_prerecorded_link": "",
        "bunny_prerecorded_subtitles": ""
    },
    "v-short-1163": {
        "slot_id": "v-short-1163",
        "session_id": "short0",
        "title": "Curve Segment Neighborhood-based Vector Field Exploration",
        "contributors": [
            "Nguyen K Phan"
        ],
        "authors": [
            {
                "name": "Nguyen K Phan",
                "email": "nguyenpkk95@gmail.com",
                "affiliations": [
                    "University of Houston, Houston, United States"
                ],
                "is_corresponding": true
            },
            {
                "name": "Guoning Chen",
                "email": "chengu@cs.uh.edu",
                "affiliations": [
                    "University of Houston, Houston, United States"
                ],
                "is_corresponding": false
            }
        ],
        "abstract": "Integral curves have been widely used to represent and analyze various vector fields. Curve-based clustering and pattern search approaches are usually applied to aid the identification of meaningful patterns from large numbers of integral curves. However, they need not support an interactive, level-of-detail exploration of these patterns. To address this, we propose a Curve Segment Neighborhood Graph (CSNG) to capture the relationships between neighboring curve segments. This graph representation enables us to adapt the fast community detection algorithm, i.e., the Louvain algorithm, to identify individual graph communities from CSNG. Our results show that these communities often correspond to the features of the flow. To achieve a multi-level interactive exploration of the detected communities, we adapt a force-directed layout that allows users to refine and re-group communities based on their domain knowledge. We incorporate the proposed techniques into an interactive system to enable effective analysis and interpretation of complex patterns in large-scale integral curve datasets.",
        "uid": "v-short-1163",
        "time_stamp": "",
        "time_start": "",
        "time_end": "",
        "paper_type": "short",
        "keywords": [],
        "doi": "",
        "fno": "",
        "presentation_mode": "",
        "open_access": false,
        "accessible_pdf": false,
        "has_image": false,
        "has_pdf": false,
        "paper_award": "",
        "image_caption": "",
        "external_paper_link": "",
        "youtube_ff_link": "",
        "youtube_ff_id": "",
        "bunny_ff_link": "",
        "bunny_ff_subtitles": "",
        "youtube_prerecorded_link": "",
        "youtube_prerecorded_id": "",
        "bunny_prerecorded_link": "",
        "bunny_prerecorded_subtitles": ""
    },
    "v-short-1166": {
        "slot_id": "v-short-1166",
        "session_id": "short0",
        "title": "Counterpoint: Orchestrating Large-Scale Custom Animated Visualizations",
        "contributors": [
            "Venkatesh Sivaraman"
        ],
        "authors": [
            {
                "name": "Venkatesh Sivaraman",
                "email": "vsivaram@andrew.cmu.edu",
                "affiliations": [
                    "Carnegie Mellon University, Pittsburgh, United States"
                ],
                "is_corresponding": true
            },
            {
                "name": "Frank Elavsky",
                "email": "fje@cmu.edu",
                "affiliations": [
                    "Carnegie Mellon University, Pittsburgh, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Dominik Moritz",
                "email": "domoritz@cmu.edu",
                "affiliations": [
                    "Carnegie Mellon University, Pittsburgh, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Adam Perer",
                "email": "adamperer@cmu.edu",
                "affiliations": [
                    "Carnegie Mellon University, Pittsburgh, United States"
                ],
                "is_corresponding": false
            }
        ],
        "abstract": "Custom animated visualizations of large, complex datasets are helpful across many domains, but they are hard to develop. Much of the difficulty arises from maintaining visualization state across a large set of animated graphical elements that may change in number over time. We contribute Counterpoint, a framework for state management designed to help implement such visualizations in JavaScript. Using Counterpoint, developers can manipulate large collections of marks with reactive attributes that are easy to render in scalable APIs such as Canvas and WebGL. Counterpoint also helps orchestrate the entry and exit of graphical elements using the concept of a rendering \"stage.\" Through a performance evaluation, we show that Counterpoint adds minimal overhead over current high-performance rendering techniques while simplifying implementation. We also provide two examples of visualizations created using Counterpoint that illustrate its flexibility and compatibility with other visualization toolkits as well as considerations for users with disabilities. Counterpoint is open-source and available at https://github.com/cmudig/counterpoint.",
        "uid": "v-short-1166",
        "time_stamp": "",
        "time_start": "",
        "time_end": "",
        "paper_type": "short",
        "keywords": [],
        "doi": "",
        "fno": "",
        "presentation_mode": "",
        "open_access": false,
        "accessible_pdf": false,
        "has_image": false,
        "has_pdf": false,
        "paper_award": "",
        "image_caption": "",
        "external_paper_link": "",
        "youtube_ff_link": "",
        "youtube_ff_id": "",
        "bunny_ff_link": "",
        "bunny_ff_subtitles": "",
        "youtube_prerecorded_link": "",
        "youtube_prerecorded_id": "",
        "bunny_prerecorded_link": "",
        "bunny_prerecorded_subtitles": ""
    },
    "v-short-1173": {
        "slot_id": "v-short-1173",
        "session_id": "short0",
        "title": "Fields, Bridges, and Foundations: How Researchers Browse Citation Network Visualizations",
        "contributors": [
            "Kiroong Choe"
        ],
        "authors": [
            {
                "name": "Kiroong Choe",
                "email": "krchoe@hcil.snu.ac.kr",
                "affiliations": [
                    "Seoul National University, Seoul, Korea, Republic of"
                ],
                "is_corresponding": true
            },
            {
                "name": "Eunhye Kim",
                "email": "gracekim027@snu.ac.kr",
                "affiliations": [
                    "Seoul National University, Seoul, Korea, Republic of"
                ],
                "is_corresponding": false
            },
            {
                "name": "Sangwon Park",
                "email": "paulmoguri@snu.ac.kr",
                "affiliations": [
                    "Dept. of Electrical and Computer Engineering, SNU, Seoul, Korea, Republic of"
                ],
                "is_corresponding": false
            },
            {
                "name": "Jinwook Seo",
                "email": "jseo@snu.ac.kr",
                "affiliations": [
                    "Seoul National University, Seoul, Korea, Republic of"
                ],
                "is_corresponding": false
            }
        ],
        "abstract": "Visualizing citation relations with network structures is widely used, but the visual complexity can make it challenging for individual researchers to navigate through them. We collected data from 18 researchers using an interface that we designed using network simplification methods and analyzed how users browsed and identified important papers. Our analysis reveals six major patterns used for identifying papers of interest, which can be categorized into three key components: Fields, Bridges, and Foundations, each viewed from two distinct perspectives: layout-oriented and connection-oriented. The connection-oriented approach was found to be more effective for selecting relevant papers, but the layout-oriented method was adopted more often, even though it led to unexpected results and user frustration. Our findings emphasize the importance of integrating these components and the necessity to balance visual layouts with meaningful connections to enhance the effectiveness of citation networks in academic browsing systems.",
        "uid": "v-short-1173",
        "time_stamp": "",
        "time_start": "",
        "time_end": "",
        "paper_type": "short",
        "keywords": [],
        "doi": "",
        "fno": "",
        "presentation_mode": "",
        "open_access": false,
        "accessible_pdf": false,
        "has_image": false,
        "has_pdf": false,
        "paper_award": "",
        "image_caption": "",
        "external_paper_link": "",
        "youtube_ff_link": "",
        "youtube_ff_id": "",
        "bunny_ff_link": "",
        "bunny_ff_subtitles": "",
        "youtube_prerecorded_link": "",
        "youtube_prerecorded_id": "",
        "bunny_prerecorded_link": "",
        "bunny_prerecorded_subtitles": ""
    },
    "v-short-1177": {
        "slot_id": "v-short-1177",
        "session_id": "short0",
        "title": "Can GPT-4V Detect Misleading Visualizations?",
        "contributors": [
            "Ali Sarvghad"
        ],
        "authors": [
            {
                "name": "Jason Huang Alexander",
                "email": "jhalexander@umass.edu",
                "affiliations": [
                    "University of Massachusetts Amherst, Amherst, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Priyal H Nanda",
                "email": "phnanda@umass.edu",
                "affiliations": [
                    "University of Masssachusetts Amherst, Amherst, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Kai-Cheng Yang",
                "email": "yangkc@iu.edu",
                "affiliations": [
                    "Northeastern University, Boston, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Ali Sarvghad",
                "email": "asarv@cs.umass.edu",
                "affiliations": [
                    "University of Massachusetts Amherst, Amherst, United States"
                ],
                "is_corresponding": true
            }
        ],
        "abstract": "The proliferation of misleading visualizations online, particularly during critical events like public health crises and elections, poses a significant risk of misinformation. This work investigates the capability of GPT-4V to detect misleading visualizations. Utilizing a dataset of tweet-visualization pairs with various visual misleaders, we tested GPT-4V under four experimental conditions: naive zero-shot, naive few-shot, guided zero-shot, and guided few-shot. Our results demonstrate that GPT-4V can detect misleading visualizations with moderate accuracy without prior training (naive zero-shot) and that performance considerably improves by providing the model with the definitions of misleaders (guided zero-shot). However, combining definitions with examples of misleaders (guided few-shot) did not yield further improvements. This study underscores the feasibility of using large vision-language models such as GTP-4V to combat misinformation and emphasizes the importance of optimizing prompt engineering to enhance detection accuracy.",
        "uid": "v-short-1177",
        "time_stamp": "",
        "time_start": "",
        "time_end": "",
        "paper_type": "short",
        "keywords": [],
        "doi": "",
        "fno": "",
        "presentation_mode": "",
        "open_access": false,
        "accessible_pdf": false,
        "has_image": false,
        "has_pdf": false,
        "paper_award": "",
        "image_caption": "",
        "external_paper_link": "",
        "youtube_ff_link": "",
        "youtube_ff_id": "",
        "bunny_ff_link": "",
        "bunny_ff_subtitles": "",
        "youtube_prerecorded_link": "",
        "youtube_prerecorded_id": "",
        "bunny_prerecorded_link": "",
        "bunny_prerecorded_subtitles": ""
    },
    "v-short-1183": {
        "slot_id": "v-short-1183",
        "session_id": "short0",
        "title": "A Ridge-based Approach for Extraction and Visualization of 3D Atmospheric Fronts",
        "contributors": [
            "Anne Gossing"
        ],
        "authors": [
            {
                "name": "Anne Gossing",
                "email": "anne.gossing@fu-berlin.de",
                "affiliations": [
                    "Zuse Institute Berlin, Berlin, Germany"
                ],
                "is_corresponding": true
            },
            {
                "name": "Andreas Beckert",
                "email": "andreas.beckert@uni-hamburg.de",
                "affiliations": [
                    "Universit\u221a\u00a7t Hamburg, Hamburg, Germany"
                ],
                "is_corresponding": false
            },
            {
                "name": "Christoph Fischer",
                "email": "christoph.fischer-1@uni-hamburg.de",
                "affiliations": [
                    "Universit\u221a\u00a7t Hamburg, Hamburg, Germany"
                ],
                "is_corresponding": false
            },
            {
                "name": "Nicolas Klenert",
                "email": "klenert@zib.de",
                "affiliations": [
                    "Zuse Institute Berlin, Berlin, Germany"
                ],
                "is_corresponding": false
            },
            {
                "name": "Vijay Natarajan",
                "email": "vijayn@iisc.ac.in",
                "affiliations": [
                    "Indian Institute of Science, Bangalore, India"
                ],
                "is_corresponding": false
            },
            {
                "name": "George Pacey",
                "email": "george.pacey@fu-berlin.de",
                "affiliations": [
                    "Freie Universit\u221a\u00a7t Berlin, Berlin, Germany"
                ],
                "is_corresponding": false
            },
            {
                "name": "Thorwin Vogt",
                "email": "thorwin.vogt@uni-hamburg.de",
                "affiliations": [
                    "Universit\u221a\u00a7t Hamburg, Hamburg, Germany"
                ],
                "is_corresponding": false
            },
            {
                "name": "Marc Rautenhaus",
                "email": "marc.rautenhaus@uni-hamburg.de",
                "affiliations": [
                    "Universit\u221a\u00a7t Hamburg, Hamburg, Germany"
                ],
                "is_corresponding": false
            },
            {
                "name": "Daniel Baum",
                "email": "baum@zib.de",
                "affiliations": [
                    "Zuse Institute Berlin, Berlin, Germany"
                ],
                "is_corresponding": false
            }
        ],
        "abstract": "An atmospheric front is an imaginary surface that separates two distinct air masses and is commonly defined as the warm-air side of a frontal zone with high gradients of atmospheric temperature and humidity. These fronts are a widely used conceptual model in meteorology, which are often encountered in the literature as two-dimensional (2D) front lines on surface analysis charts. This paper presents a method for computing three-dimensional (3D) atmospheric fronts as surfaces that is capable of extracting continuous and well-confined features suitable for 3D visual analysis, spatio-temporal tracking, and statistical analyses. Recently developed contour-based methods for 3D front extraction rely on computing the third derivative of a moist potential temperature field. Additionally, they require the field to be smoothed to obtain continuous large-scale structures. This paper demonstrates the feasibility of an alternative method to front extraction using ridge surface computation. The proposed method requires only the sec- ond derivative of the input field and produces accurate structures even from unsmoothed data. An application of the ridge-based method to a data set corresponding to Cyclone Friederike demonstrates its benefits and utility towards visual analysis of the full 3D structure of fronts.",
        "uid": "v-short-1183",
        "time_stamp": "",
        "time_start": "",
        "time_end": "",
        "paper_type": "short",
        "keywords": [],
        "doi": "",
        "fno": "",
        "presentation_mode": "",
        "open_access": false,
        "accessible_pdf": false,
        "has_image": false,
        "has_pdf": false,
        "paper_award": "",
        "image_caption": "",
        "external_paper_link": "",
        "youtube_ff_link": "",
        "youtube_ff_id": "",
        "bunny_ff_link": "",
        "bunny_ff_subtitles": "",
        "youtube_prerecorded_link": "",
        "youtube_prerecorded_id": "",
        "bunny_prerecorded_link": "",
        "bunny_prerecorded_subtitles": ""
    },
    "v-short-1184": {
        "slot_id": "v-short-1184",
        "session_id": "short0",
        "title": "Towards a Quality Approach to Hierarchical Color Maps",
        "contributors": [
            "Tobias Mertz"
        ],
        "authors": [
            {
                "name": "Tobias Mertz",
                "email": "tobias.mertz@igd.fraunhofer.de",
                "affiliations": [
                    "Fraunhofer IGD, Darmstadt, Germany"
                ],
                "is_corresponding": true
            },
            {
                "name": "J\u221a\u2202rn Kohlhammer",
                "email": "joern.kohlhammer@igd.fraunhofer.de",
                "affiliations": [
                    "Fraunhofer IGD, Darmstadt, Germany",
                    "TU Darmstadt, Darmstadt, Germany"
                ],
                "is_corresponding": false
            }
        ],
        "abstract": "To improve the perception of hierarchical structures in data sets, several color map generation algorithms have been proposed to take this structure into account. But the design of hierarchical color maps elicits different requirements to those of color maps for tabular data. Within this paper, we make an initial effort to put design rules from the color map literature into the context of hierarchical color maps. We investigate the impact of several design decisions and provide recommendations for various analysis scenarios. Thus, we lay the foundation for objective quality criteria to evaluate hierarchical color maps.",
        "uid": "v-short-1184",
        "time_stamp": "",
        "time_start": "",
        "time_end": "",
        "paper_type": "short",
        "keywords": [],
        "doi": "",
        "fno": "",
        "presentation_mode": "",
        "open_access": false,
        "accessible_pdf": false,
        "has_image": false,
        "has_pdf": false,
        "paper_award": "",
        "image_caption": "",
        "external_paper_link": "",
        "youtube_ff_link": "",
        "youtube_ff_id": "",
        "bunny_ff_link": "",
        "bunny_ff_subtitles": "",
        "youtube_prerecorded_link": "",
        "youtube_prerecorded_id": "",
        "bunny_prerecorded_link": "",
        "bunny_prerecorded_subtitles": ""
    },
    "v-short-1185": {
        "slot_id": "v-short-1185",
        "session_id": "short0",
        "title": "Two-point Equidistant Projection and Degree-of-interest Filtering for Smooth Exploration of Geo-referenced Networks",
        "contributors": [
            "Max Franke"
        ],
        "authors": [
            {
                "name": "Max Franke",
                "email": "max@mumintroll.org",
                "affiliations": [
                    "University of Stuttgart, Stuttgart, Germany"
                ],
                "is_corresponding": true
            },
            {
                "name": "Samuel Beck",
                "email": "samuel.beck@vis.uni-stuttgart.de",
                "affiliations": [
                    "University of Stuttgart, Stuttgart, Germany"
                ],
                "is_corresponding": false
            },
            {
                "name": "Steffen Koch",
                "email": "steffen.koch@vis.uni-stuttgart.de",
                "affiliations": [
                    "University of Stuttgart, Stuttgart, Germany"
                ],
                "is_corresponding": false
            }
        ],
        "abstract": "The visualization and interactive exploration of geo-referenced networks poses challenges if the network's nodes are not evenly distributed. Our approach proposes new ways of realizing animated transitions for exploring such networks from an ego-perspective. We aim to reduce the required screen estate while maintaining the viewers' mental map of distances and directions. A preliminary study provides first insights of the comprehensiveness of animated geographic transitions regarding directional relationships between start and end point in different projections. Two use cases showcase how ego-perspective graph exploration can be supported using less screen space than previous approaches.",
        "uid": "v-short-1185",
        "time_stamp": "",
        "time_start": "",
        "time_end": "",
        "paper_type": "short",
        "keywords": [],
        "doi": "",
        "fno": "",
        "presentation_mode": "",
        "open_access": false,
        "accessible_pdf": false,
        "has_image": false,
        "has_pdf": false,
        "paper_award": "",
        "image_caption": "",
        "external_paper_link": "",
        "youtube_ff_link": "",
        "youtube_ff_id": "",
        "bunny_ff_link": "",
        "bunny_ff_subtitles": "",
        "youtube_prerecorded_link": "",
        "youtube_prerecorded_id": "",
        "bunny_prerecorded_link": "",
        "bunny_prerecorded_subtitles": ""
    },
    "v-short-1186": {
        "slot_id": "v-short-1186",
        "session_id": "short0",
        "title": "Exploring the Capability of LLMs in Performing Low-Level Visual Analytic Tasks on SVG Data Visualizations",
        "contributors": [
            "Zhongzheng Xu"
        ],
        "authors": [
            {
                "name": "Zhongzheng Xu",
                "email": "leooooxzz@gmail.com",
                "affiliations": [
                    "Brown University, Providence, United States"
                ],
                "is_corresponding": true
            },
            {
                "name": "Emily Wall",
                "email": "emily.wall@emory.edu",
                "affiliations": [
                    "Emory University, Atlanta, United States"
                ],
                "is_corresponding": false
            }
        ],
        "abstract": "Data visualizations help extract insights from datasets, but reaching these insights requires decomposing high level goals into low-level analytic tasks that can be complex due to varying degrees of data literacy and visualization experience. Recent advancements in large language models (LLMs) have shown promise for lowering barriers for users to achieve tasks such as writing code and may likewise facilitate visualization insight. Scalable Vector Graphics (SVG), a text-based image format common in data visualizations, matches well with the text sequence processing of transformer-based LLMs. In this paper, we explore the capability of LLMs to perform 10 low-level visual analytic tasks defined by Amar, Eagan, and Stasko directly on SVG-based visualizations. Using zero-shot prompts, we instruct the models to provide responses or modify the SVG code based on given visualizations. Our findings demonstrate that LLMs can effectively modify existing SVG visualizations for some tasks like Cluster but perform poorly on tasks requiring mathematical operations like Compute Derived Value. We also discovered that LLM performance can vary based on factors such as the number of data points, the presence of value labels, and the chart type. Our findings contribute to gauging the general capabilities of LLMs and highlight the need for further exploration and development to fully harness their potential in supporting visual analytic tasks.",
        "uid": "v-short-1186",
        "time_stamp": "",
        "time_start": "",
        "time_end": "",
        "paper_type": "short",
        "keywords": [],
        "doi": "",
        "fno": "",
        "presentation_mode": "",
        "open_access": false,
        "accessible_pdf": false,
        "has_image": false,
        "has_pdf": false,
        "paper_award": "",
        "image_caption": "",
        "external_paper_link": "",
        "youtube_ff_link": "",
        "youtube_ff_id": "",
        "bunny_ff_link": "",
        "bunny_ff_subtitles": "",
        "youtube_prerecorded_link": "",
        "youtube_prerecorded_id": "",
        "bunny_prerecorded_link": "",
        "bunny_prerecorded_subtitles": ""
    },
    "v-short-1188": {
        "slot_id": "v-short-1188",
        "session_id": "short0",
        "title": "Topological Separation of Vortices",
        "contributors": [
            "Adeel Zafar"
        ],
        "authors": [
            {
                "name": "Adeel Zafar",
                "email": "adeelz92@gmail.com",
                "affiliations": [
                    "University of Houston, Houston, United States"
                ],
                "is_corresponding": true
            },
            {
                "name": "Zahra Poorshayegh",
                "email": "zpoorsha@cougarnet.uh.edu",
                "affiliations": [
                    "University of Houston, Houston, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Di Yang",
                "email": "diyang@uh.edu",
                "affiliations": [
                    "University of Houston, Houston, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Guoning Chen",
                "email": "chengu@cs.uh.edu",
                "affiliations": [
                    "University of Houston, Houston, United States"
                ],
                "is_corresponding": false
            }
        ],
        "abstract": "Vortices and their analysis play a critical role in the understanding of complex phenomena in turbulent flow. Traditional vortex extraction methods, notably region-based techniques, often overlook the entanglement phenomenon, resulting in the inclusion of multiple vortices within a single extracted region. Their separation is necessary for quantifying different types of vortices and their statistics. In this study, we propose a novel vortex separation method that extends the conventional contour tree-based segmentation approach with an additional step termed \u201a\u00c4\u00falayering\u201a\u00c4\u00f9. Upon extracting a vortical region using specified vortex criteria (e.g., \u0152\u00aa2), we initially establish topological segmentation based on the contour tree, followed by the layering process to allocate appropriate segmentation IDs to unsegmented cells, thus separating individual vortices within the region. However, these regions may still suffer from inaccurate splits, which we address statistically by leveraging the continuity of vorticity lines across the split boundaries. Our findings demonstrate a significant improvement in both the separation of vortices and the mitigation of inaccurate splits compared to prior methods.",
        "uid": "v-short-1188",
        "time_stamp": "",
        "time_start": "",
        "time_end": "",
        "paper_type": "short",
        "keywords": [],
        "doi": "",
        "fno": "",
        "presentation_mode": "",
        "open_access": false,
        "accessible_pdf": false,
        "has_image": false,
        "has_pdf": false,
        "paper_award": "",
        "image_caption": "",
        "external_paper_link": "",
        "youtube_ff_link": "",
        "youtube_ff_id": "",
        "bunny_ff_link": "",
        "bunny_ff_subtitles": "",
        "youtube_prerecorded_link": "",
        "youtube_prerecorded_id": "",
        "bunny_prerecorded_link": "",
        "bunny_prerecorded_subtitles": ""
    },
    "v-short-1189": {
        "slot_id": "v-short-1189",
        "session_id": "short0",
        "title": "Towards Reusable and Reactive Widgets for Information Visualization Research and Dissemination",
        "contributors": [
            "John Alexis Guerra-Gomez"
        ],
        "authors": [
            {
                "name": "John Alexis Guerra-Gomez",
                "email": "john.guerra@gmail.com",
                "affiliations": [
                    "Northeastern University, San Francisco, United States"
                ],
                "is_corresponding": true
            }
        ],
        "abstract": "The information visualization research community commonly produces supporting software to demonstrate technical contributions to the field. However, developing this software tends to be an overwhelming task, the final product tends to be a research prototype without much thought for modularization and re-usability which makes it harder to replicate and adopt. This paper presents a design pattern for facilitating the creation, dissemination, and re-utilization of visualization techniques using reactive widgets. The design pattern features basic concepts that leverage modern front-end development best practices and standards, which ease development and replication. The paper presents several usage examples of the pattern, templates for implementation, and even a wrapper for facilitating the conversion of any Vega specification into a reactive widget.",
        "uid": "v-short-1189",
        "time_stamp": "",
        "time_start": "",
        "time_end": "",
        "paper_type": "short",
        "keywords": [],
        "doi": "",
        "fno": "",
        "presentation_mode": "",
        "open_access": false,
        "accessible_pdf": false,
        "has_image": false,
        "has_pdf": false,
        "paper_award": "",
        "image_caption": "",
        "external_paper_link": "",
        "youtube_ff_link": "",
        "youtube_ff_id": "",
        "bunny_ff_link": "",
        "bunny_ff_subtitles": "",
        "youtube_prerecorded_link": "",
        "youtube_prerecorded_id": "",
        "bunny_prerecorded_link": "",
        "bunny_prerecorded_subtitles": ""
    },
    "v-short-1191": {
        "slot_id": "v-short-1191",
        "session_id": "short0",
        "title": "Bringing Data into the Conversation: Adapting Content from Business Intelligence Dashboards for Threaded Collaboration Platforms",
        "contributors": [
            "Hyeok Kim"
        ],
        "authors": [
            {
                "name": "Hyeok Kim",
                "email": "hyeokkim2024@u.northwestern.edu",
                "affiliations": [
                    "Northwestern University, Evanston, United States"
                ],
                "is_corresponding": true
            },
            {
                "name": "Arjun Srinivasan",
                "email": "arjun.srinivasan.10@gmail.com",
                "affiliations": [
                    "Tableau Research, Seattle, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Matthew Brehmer",
                "email": "mbrehmer@uwaterloo.ca",
                "affiliations": [
                    "Tableau Research, Seattle, United States"
                ],
                "is_corresponding": false
            }
        ],
        "abstract": "To enable data-driven decision-making across organizations, data professionals need to share insights with their colleagues in context-appropriate communication channels. Many of their colleagues rely on data but are not themselves analysts; furthermore, their colleagues are reluctant or unable to use dedicated analytical applications or dashboards, and they expect communication to take place within threaded collaboration platforms such as Slack or Microsoft Teams. In this paper, we introduce a set of six strategies for adapting content from business intelligence (BI) dashboards into appropriate formats for sharing on collaboration platforms, formats that we refer to as dashboard snapshots. Informed by prior studies of enterprise communication around data, these strategies go beyond redesigning or restyling by considering varying levels of data literacy across an organization, introducing affordances for self-service question-answering, and anticipating the post-sharing lifecycle of data artifacts. These strategies involve the use of templates that are matched to common communicative intents, serving to reduce the workload of data professionals. We contribute a formal representation of these strategies and demonstrate their applicability in a comprehensive enterprise communication scenario featuring multiple stakeholders that unfolds over the span of months.",
        "uid": "v-short-1191",
        "time_stamp": "",
        "time_start": "",
        "time_end": "",
        "paper_type": "short",
        "keywords": [],
        "doi": "",
        "fno": "",
        "presentation_mode": "",
        "open_access": false,
        "accessible_pdf": false,
        "has_image": false,
        "has_pdf": false,
        "paper_award": "",
        "image_caption": "",
        "external_paper_link": "",
        "youtube_ff_link": "",
        "youtube_ff_id": "",
        "bunny_ff_link": "",
        "bunny_ff_subtitles": "",
        "youtube_prerecorded_link": "",
        "youtube_prerecorded_id": "",
        "bunny_prerecorded_link": "",
        "bunny_prerecorded_subtitles": ""
    },
    "v-short-1192": {
        "slot_id": "v-short-1192",
        "session_id": "short0",
        "title": "Animating the Narrative: A Review of Animation Styles in Narrative Visualization",
        "contributors": [
            "Vyri Junhan Yang"
        ],
        "authors": [
            {
                "name": "Vyri Junhan Yang",
                "email": "jyang44@lsu.edu",
                "affiliations": [
                    "Louisiana State University, Baton Rouge, United States"
                ],
                "is_corresponding": true
            },
            {
                "name": "Mahmood Jasim",
                "email": "mjasim@lsu.edu",
                "affiliations": [
                    "Louisiana State University, Baton Rouge, United States"
                ],
                "is_corresponding": false
            }
        ],
        "abstract": "Narrative visualization has become a crucial tool in data presentation, merging storytelling with data visualization to convey complex information in an engaging and accessible manner. In this study, we review the design space for narrative visualizations, focusing on animation style, through a comprehensive analysis of 71 papers from key visualization venues. We categorize these papers into six broad themes: Animation Style, Interactivity, Technology Usage, Methodology Development, Evaluation Type, and Application Domain. Our findings reveal a significant evolution in the field, marked by a growing preference for animated and non-interactive techniques. This trend reflects a shift towards minimizing user interaction while enhancing the clarity and impact of data presentation. We also identified key trends and technologies that have shaped the field, highlighting the role of technologies, such as machine learning in driving these changes. We offer insights into the dynamic interrelations within the narrative visualization domain, suggesting a future research trajectory that balances interactivity with automated tools to foster increased engagement. Our work lays the groundwork for future approaches for effective and innovative narrative visualization in diverse applications.",
        "uid": "v-short-1192",
        "time_stamp": "",
        "time_start": "",
        "time_end": "",
        "paper_type": "short",
        "keywords": [],
        "doi": "",
        "fno": "",
        "presentation_mode": "",
        "open_access": false,
        "accessible_pdf": false,
        "has_image": false,
        "has_pdf": false,
        "paper_award": "",
        "image_caption": "",
        "external_paper_link": "",
        "youtube_ff_link": "",
        "youtube_ff_id": "",
        "bunny_ff_link": "",
        "bunny_ff_subtitles": "",
        "youtube_prerecorded_link": "",
        "youtube_prerecorded_id": "",
        "bunny_prerecorded_link": "",
        "bunny_prerecorded_subtitles": ""
    },
    "v-short-1193": {
        "slot_id": "v-short-1193",
        "session_id": "short0",
        "title": "LinkQ: An LLM-Assisted Visual Interface for Knowledge Graph Question-Answering",
        "contributors": [
            "Harry Li"
        ],
        "authors": [
            {
                "name": "Harry Li",
                "email": "harry.li@ll.mit.edu",
                "affiliations": [
                    "MIT Lincoln Laboratory, Lexington, United States"
                ],
                "is_corresponding": true
            },
            {
                "name": "Gabriel Appleby",
                "email": "gabriel.appleby@tufts.edu",
                "affiliations": [
                    "Tufts University, Medford, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Ashley Suh",
                "email": "ashley.suh@ll.mit.edu",
                "affiliations": [
                    "MIT Lincoln Laboratory, Lexington, United States"
                ],
                "is_corresponding": false
            }
        ],
        "abstract": "We present LinkQ, a system that leverages a large language model (LLM) to facilitate knowledge graph (KG) query construction through natural language question-answering. Traditional approaches often require detailed knowledge of complex graph querying languages, limiting the ability for users -- even experts -- to acquire valuable insights from KG data. LinkQ simplifies this process by first interpreting a user's question, then converting it into a well-formed KG query. By using the LLM to construct a query instead of directly answering the user's question, LinkQ guards against the LLM hallucinating or generating false, erroneous information. By integrating an LLM into LinkQ, users are able to conduct both exploratory and confirmatory data analysis, with the LLM helping to iteratively refine open-ended questions into precise ones. To demonstrate the efficacy of LinkQ, we conducted a qualitative study with five KG practitioners and distill their feedback. Our results indicate that practitioners find LinkQ effective for KG question-answering, and desire future LLM-assisted systems for the exploratory analysis of graph databases.",
        "uid": "v-short-1193",
        "time_stamp": "",
        "time_start": "",
        "time_end": "",
        "paper_type": "short",
        "keywords": [],
        "doi": "",
        "fno": "",
        "presentation_mode": "",
        "open_access": false,
        "accessible_pdf": false,
        "has_image": false,
        "has_pdf": false,
        "paper_award": "",
        "image_caption": "",
        "external_paper_link": "",
        "youtube_ff_link": "",
        "youtube_ff_id": "",
        "bunny_ff_link": "",
        "bunny_ff_subtitles": "",
        "youtube_prerecorded_link": "",
        "youtube_prerecorded_id": "",
        "bunny_prerecorded_link": "",
        "bunny_prerecorded_subtitles": ""
    },
    "v-short-1199": {
        "slot_id": "v-short-1199",
        "session_id": "short0",
        "title": "From Graphs to Words: A Computer-Assisted Framework for the Production of Accessible Text Descriptions",
        "contributors": [
            "Qiang Xu"
        ],
        "authors": [
            {
                "name": "Qiang Xu",
                "email": "qiangxu1204@gmail.com",
                "affiliations": [
                    "Polytechnique Montr\u221a\u00a9al, Montr\u221a\u00a9al, Canada"
                ],
                "is_corresponding": true
            },
            {
                "name": "Thomas Hurtut",
                "email": "thomas.hurtut@polymtl.ca",
                "affiliations": [
                    "Polytechnique Montreal, Montreal, Canada"
                ],
                "is_corresponding": false
            }
        ],
        "abstract": "In the digital landscape, the ubiquity of data visualizations in media underscores the necessity for accessibility to ensure inclusivity for all users, including those with visual impairments. Current visual content often fails to cater to the needs of screen reader users due to the absence of comprehensive textual descriptions. To address this gap, we propose in this paper a framework designed to empower media content creators to transform charts into descriptive narratives. This tool not only facilitates the understanding of complex visual data through text but also fosters a broader awareness of accessibility in digital content creation. Through the application of this framework, users can interpret and convey the insights of data visualizations more effectively, accommodating a diverse audience. Our evaluations reveal that this tool not only enhances the comprehension of data visualizations but also promotes new perspectives on the represented data, thereby broadening the interpretative possibilities for all users.",
        "uid": "v-short-1199",
        "time_stamp": "",
        "time_start": "",
        "time_end": "",
        "paper_type": "short",
        "keywords": [],
        "doi": "",
        "fno": "",
        "presentation_mode": "",
        "open_access": false,
        "accessible_pdf": false,
        "has_image": false,
        "has_pdf": false,
        "paper_award": "",
        "image_caption": "",
        "external_paper_link": "",
        "youtube_ff_link": "",
        "youtube_ff_id": "",
        "bunny_ff_link": "",
        "bunny_ff_subtitles": "",
        "youtube_prerecorded_link": "",
        "youtube_prerecorded_id": "",
        "bunny_prerecorded_link": "",
        "bunny_prerecorded_subtitles": ""
    },
    "v-short-1207": {
        "slot_id": "v-short-1207",
        "session_id": "short0",
        "title": "Design of a Real-Time Visual Analytics Decision Support Interface to Manage Air Traffic Complexity",
        "contributors": [
            "Elmira Zohrevandi"
        ],
        "authors": [
            {
                "name": "Elmira Zohrevandi",
                "email": "elmira.zohrevandi@liu.se",
                "affiliations": [
                    "Link\u221a\u2202ping University, Norrk\u221a\u2202ping, Sweden",
                    "Link\u221a\u2202ping University, Norrk\u221a\u2202ping, Sweden"
                ],
                "is_corresponding": true
            },
            {
                "name": "Katerina Vrotsou",
                "email": "katerina.vrotsou@liu.se",
                "affiliations": [
                    "Link\u221a\u2202ping University, Norrk\u221a\u2202ping, Sweden",
                    "Link\u221a\u2202ping University, Norrk\u221a\u2202ping, Sweden"
                ],
                "is_corresponding": false
            },
            {
                "name": "Carl A. L. Westin",
                "email": "carl.westin@liu.se",
                "affiliations": [
                    "Institute of Science and Technology, Norrk\u221a\u2202ping, Sweden",
                    "Institute of Science and Technology, Norrk\u221a\u2202ping, Sweden"
                ],
                "is_corresponding": false
            },
            {
                "name": "Jonas Lundberg",
                "email": "jonas.lundberg@liu.se",
                "affiliations": [
                    "Link\u221a\u2202ping University, Norrk\u221a\u2202ping, Sweden",
                    "Link\u221a\u2202ping University, Norrk\u221a\u2202ping, Sweden"
                ],
                "is_corresponding": false
            },
            {
                "name": "Anders Ynnerman",
                "email": "anders.ynnerman@liu.se",
                "affiliations": [
                    "Link\u221a\u2202ping University, Norrk\u221a\u2202ping, Sweden",
                    "Link\u221a\u2202ping University, Norrk\u221a\u2202ping, Sweden"
                ],
                "is_corresponding": false
            }
        ],
        "abstract": "An essential task of an air traffic controller is to manage the traffic flow by predicting future trajectories. Complex traffic patterns are difficult to predict and manage and impose cognitive load on the air traffic controllers. In this work we present an interactive visual analytics interface which facilitates detection and resolution of complex traffic patterns for air traffic controllers. The interface supports the users in detecting complex clusters of aircraft and uses visual representations to communicate to the controllers how and propose re-routing. The interface further enables the ATCos to visualize and simultaneously compare how different re-routing strategies for each individual aircraft yield reduction of complexity in the entire sector for the next hour. The development of the concepts was supported by the domain-specific feedback we received from six fully licensed and operational air traffic controllers in an iterative design process over a period of 14 months.",
        "uid": "v-short-1207",
        "time_stamp": "",
        "time_start": "",
        "time_end": "",
        "paper_type": "short",
        "keywords": [],
        "doi": "",
        "fno": "",
        "presentation_mode": "",
        "open_access": false,
        "accessible_pdf": false,
        "has_image": false,
        "has_pdf": false,
        "paper_award": "",
        "image_caption": "",
        "external_paper_link": "",
        "youtube_ff_link": "",
        "youtube_ff_id": "",
        "bunny_ff_link": "",
        "bunny_ff_subtitles": "",
        "youtube_prerecorded_link": "",
        "youtube_prerecorded_id": "",
        "bunny_prerecorded_link": "",
        "bunny_prerecorded_subtitles": ""
    },
    "v-short-1211": {
        "slot_id": "v-short-1211",
        "session_id": "short0",
        "title": "Text-based transfer function design for semantic volume rendering",
        "contributors": [
            "Sangwon Jeong"
        ],
        "authors": [
            {
                "name": "Sangwon Jeong",
                "email": "sangwon.jeong@vanderbilt.edu",
                "affiliations": [
                    "Vanderbilt University, Nashville, United States"
                ],
                "is_corresponding": true
            },
            {
                "name": "Jixian Li",
                "email": "jixianli@sci.utah.edu",
                "affiliations": [
                    "University of Utah, Salt Lake City, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Shusen Liu",
                "email": "shusenl@sci.utah.edu",
                "affiliations": [
                    "Lawrence Livermore National Laboratory , Livermore, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Chris R. Johnson",
                "email": "crj@sci.utah.edu",
                "affiliations": [
                    "University of Utah, Salt Lake City, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Matthew Berger",
                "email": "matthew.berger@vanderbilt.edu",
                "affiliations": [
                    "Vanderbilt University, Nashville, United States"
                ],
                "is_corresponding": false
            }
        ],
        "abstract": "Transfer function design is crucial in volume rendering, as it directly influences the visual representation and interpretation of volumetric data. However, creating effective transfer functions that align with users\u201a\u00c4\u00f4 visual objectives is often challenging due to the complex parameter space and the semantic gap between transfer function values and features of interest within the volume. In this work, we propose a novel approach that leverages recent advancements in language-vision models to bridge this semantic gap. By employing a fully differentiable rendering pipeline and an image-based loss function guided by language descriptions, our method generates transfer functions that yield volume-rendered images closely matching the user\u201a\u00c4\u00f4s intent. We demonstrate the effectiveness of our approach in creating meaningful transfer functions from simple descriptions, empowering users to intuitively express their desired visual outcomes with minimal effort. This advancement streamlines the transfer function design process and makes volume rendering more accessible to a broader range of users.",
        "uid": "v-short-1211",
        "time_stamp": "",
        "time_start": "",
        "time_end": "",
        "paper_type": "short",
        "keywords": [],
        "doi": "",
        "fno": "",
        "presentation_mode": "",
        "open_access": false,
        "accessible_pdf": false,
        "has_image": false,
        "has_pdf": false,
        "paper_award": "",
        "image_caption": "",
        "external_paper_link": "",
        "youtube_ff_link": "",
        "youtube_ff_id": "",
        "bunny_ff_link": "",
        "bunny_ff_subtitles": "",
        "youtube_prerecorded_link": "",
        "youtube_prerecorded_id": "",
        "bunny_prerecorded_link": "",
        "bunny_prerecorded_subtitles": ""
    },
    "v-short-1224": {
        "slot_id": "v-short-1224",
        "session_id": "short0",
        "title": "Diffusion Explainer: Visual Explanation for Text-to-image Stable Diffusion",
        "contributors": [
            "Seongmin Lee"
        ],
        "authors": [
            {
                "name": "Seongmin Lee",
                "email": "seongmin@gatech.edu",
                "affiliations": [
                    "Georgia Tech, Atlanta, United States"
                ],
                "is_corresponding": true
            },
            {
                "name": "Benjamin Hoover",
                "email": "benjamin.hoover@ibm.com",
                "affiliations": [
                    "GA Tech, Atlanta, United States",
                    "IBM Research AI, Cambridge, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Hendrik Strobelt",
                "email": "hendrik@strobelt.com",
                "affiliations": [
                    "IBM Research AI, Cambridge, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Zijie J. Wang",
                "email": "jayw@gatech.edu",
                "affiliations": [
                    "Georgia Tech, Atlanta, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "ShengYun Peng",
                "email": "speng65@gatech.edu",
                "affiliations": [
                    "Georgia Institute of Technology, Atlanta, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Austin P Wright",
                "email": "apwright@gatech.edu",
                "affiliations": [
                    "Georgia Institute of Technology , Atlanta , United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Kevin Li",
                "email": "kevin.li@gatech.edu",
                "affiliations": [
                    "Georgia Institute of Technology, Atlanta, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Haekyu Park",
                "email": "haekyu@gatech.edu",
                "affiliations": [
                    "Georgia Institute of Technology, Atlanta, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Haoyang Yang",
                "email": "alexanderyang@gatech.edu",
                "affiliations": [
                    "Georgia Institute of Technology, Atlanta, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Duen Horng (Polo) Chau",
                "email": "polo@gatech.edu",
                "affiliations": [
                    "Georgia Tech, Atlanta, United States"
                ],
                "is_corresponding": false
            }
        ],
        "abstract": "Diffusion-based generative models\u201a\u00c4\u00f4 impressive ability to create convincing images has garnered global attention. However, their complex structures and operations often pose challenges for non-experts to grasp. We present Diffusion Explainer, the first interactive visualization tool that explains how Stable Diffusion transforms text prompts into images. Diffusion Explainer tightly integrates a visual overview of Stable Diffusion\u201a\u00c4\u00f4s complex structure with explanations of the underlying operations. By comparing image generation of prompt variants, users can discover the impact of keyword changes on image generation. A 56-participant user study demonstrates that Diffusion Explainer offers substantial learning benefits to non-experts. Our tool has been used by over 10,300 users from 124 countries at https://poloclub.github.io/diffusion-explainer/.",
        "uid": "v-short-1224",
        "time_stamp": "",
        "time_start": "",
        "time_end": "",
        "paper_type": "short",
        "keywords": [],
        "doi": "",
        "fno": "",
        "presentation_mode": "",
        "open_access": false,
        "accessible_pdf": false,
        "has_image": false,
        "has_pdf": false,
        "paper_award": "",
        "image_caption": "",
        "external_paper_link": "",
        "youtube_ff_link": "",
        "youtube_ff_id": "",
        "bunny_ff_link": "",
        "bunny_ff_subtitles": "",
        "youtube_prerecorded_link": "",
        "youtube_prerecorded_id": "",
        "bunny_prerecorded_link": "",
        "bunny_prerecorded_subtitles": ""
    },
    "v-short-1235": {
        "slot_id": "v-short-1235",
        "session_id": "short0",
        "title": "Uniform Sample Distribution in Scatterplots via Sector-based Transformation",
        "contributors": [
            "Hennes Rave"
        ],
        "authors": [
            {
                "name": "Hennes Rave",
                "email": "hennes.rave@uni-muenster.de",
                "affiliations": [
                    "University of M\u221a\u00banster, M\u221a\u00banster, Germany"
                ],
                "is_corresponding": true
            },
            {
                "name": "Vladimir Molchanov",
                "email": "molchano@uni-muenster.de",
                "affiliations": [
                    "University of M\u221a\u00banster, M\u221a\u00banster, Germany"
                ],
                "is_corresponding": false
            },
            {
                "name": "Lars Linsen",
                "email": "linsen@uni-muenster.de",
                "affiliations": [
                    "University of M\u221a\u00banster, M\u221a\u00banster, Germany"
                ],
                "is_corresponding": false
            }
        ],
        "abstract": "A high number of samples often leads to occlusion in scatterplots, which hinders data perception and analysis. De-cluttering approaches based on spatial transformation reduce visual clutter by remapping samples using the entire available scatterplot domain. Such regularized scatterplots may still be used for data analysis tasks, if the spatial transformation is smooth and preserves the original neighborhood relations of samples. Recently, Rave et al. proposed an efficient regularization method based on integral images. We propose a generalization of their regularization scheme using sector-based transformations with the aim of increasing sample uniformity of the resulting scatterplot. We document the improvement of our approach using various uniformity measures.",
        "uid": "v-short-1235",
        "time_stamp": "",
        "time_start": "",
        "time_end": "",
        "paper_type": "short",
        "keywords": [],
        "doi": "",
        "fno": "",
        "presentation_mode": "",
        "open_access": false,
        "accessible_pdf": false,
        "has_image": false,
        "has_pdf": false,
        "paper_award": "",
        "image_caption": "",
        "external_paper_link": "",
        "youtube_ff_link": "",
        "youtube_ff_id": "",
        "bunny_ff_link": "",
        "bunny_ff_subtitles": "",
        "youtube_prerecorded_link": "",
        "youtube_prerecorded_id": "",
        "bunny_prerecorded_link": "",
        "bunny_prerecorded_subtitles": ""
    },
    "v-short-1236": {
        "slot_id": "v-short-1236",
        "session_id": "short0",
        "title": "Evaluating the Semantic Profiling Abilities of LLMs for Natural Language Utterances in Data Visualization",
        "contributors": [
            "Hannah K. Bako"
        ],
        "authors": [
            {
                "name": "Hannah K. Bako",
                "email": "hbako@umd.edu",
                "affiliations": [
                    "University of Maryland, College Park, United States"
                ],
                "is_corresponding": true
            },
            {
                "name": "Arshnoor Bhutani",
                "email": "arshnoorbhutani8@gmail.com",
                "affiliations": [
                    "University of Maryland, College Park, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Xinyi Liu",
                "email": "xinyi.liu@utexas.edu",
                "affiliations": [
                    "The University of Texas at Austin, Austin, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Kwesi Adu Cobbina",
                "email": "kcobbina@cs.umd.edu",
                "affiliations": [
                    "University of Maryland, College Park, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Zhicheng Liu",
                "email": "leozcliu@umd.edu",
                "affiliations": [
                    "University of Maryland, College Park, United States"
                ],
                "is_corresponding": false
            }
        ],
        "abstract": "Automatically generating data visualizations in response to human utterances on datasets necessitates a deep semantic understanding of the data utterance, including implicit and explicit references to data attributes, visualization tasks, and necessary data preparation steps. Natural Language Interfaces (NLIs) for data visualization have explored ways to infer such information, yet challenges persist due to inherent uncertainty in human speech. Recent advances in Large Language Models (LLMs) provide an avenue to address these challenges, but their ability to extract the relevant semantic information remains unexplored. In this study, we evaluate four publicly available LLMs (GPT-4, Gemini-Pro, Llama3, and Mixtral), investigating their ability to comprehend utterances even in the presence of uncertainty and identify the relevant data context and visual tasks. Our findings reveal that LLMs are sensitive to uncertainties in utterance. Despite this sensitivity, they are able to extract the relevant data context. However, LLMs struggle with inferring visualization tasks. Based on these results, we highlight future research directions on using LLMs for visualization generation. Our supplementary materials have been shared on OSF: https://osf.io/j342a/wiki/home/?view_only=b4051ffc6253496d9bce818e4a89b9f9",
        "uid": "v-short-1236",
        "time_stamp": "",
        "time_start": "",
        "time_end": "",
        "paper_type": "short",
        "keywords": [],
        "doi": "",
        "fno": "",
        "presentation_mode": "",
        "open_access": false,
        "accessible_pdf": false,
        "has_image": false,
        "has_pdf": false,
        "paper_award": "",
        "image_caption": "",
        "external_paper_link": "",
        "youtube_ff_link": "",
        "youtube_ff_id": "",
        "bunny_ff_link": "",
        "bunny_ff_subtitles": "",
        "youtube_prerecorded_link": "",
        "youtube_prerecorded_id": "",
        "bunny_prerecorded_link": "",
        "bunny_prerecorded_subtitles": ""
    },
    "v-short-1248": {
        "slot_id": "v-short-1248",
        "session_id": "short0",
        "title": "Guided Statistical Workflows with Interactive Explanations and Assumption Checking",
        "contributors": [
            "Yuqi Zhang"
        ],
        "authors": [
            {
                "name": "Yuqi Zhang",
                "email": "yz9381@nyu.edu",
                "affiliations": [
                    "New York University, New York, United States"
                ],
                "is_corresponding": true
            },
            {
                "name": "Adam Perer",
                "email": "adamperer@cmu.edu",
                "affiliations": [
                    "Carnegie Mellon University, Pittsburgh, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Will Epperson",
                "email": "willepp@cmu.edu",
                "affiliations": [
                    "Carnegie Mellon University, Pittsburgh, United States"
                ],
                "is_corresponding": false
            }
        ],
        "abstract": "Statistical practices such as building regression models or running hypothesis tests rely on following rigorous procedures of steps and verifying assumptions on data to produce valid results. However, common statistical tools do not verify users\u201a\u00c4\u00f4 decision choices and provide low-level statistical functions without instructions on the whole analysis practice. Users can easily misuse analysis methods, potentially decreasing the validity of results. To address this problem, we introduce GuidedStats, an interactive interface within computational notebooks that encapsulates guidance, models, visualization, and exportable results into interactive workflows. It breaks down typical analysis processes, such as linear regression and two-sample T-tests, into interactive steps supplemented with automatic visualizations and explanations for step-wise evaluation. Users can iterate on input choices to refine their models, while recommended actions and exports allow the user to continue their analysis in code. Case studies show how GuidedStats offers valuable instructions for conducting fluid statistical analyses while finding possible assumption violations in the underlying data, supporting flexible and accurate statistical analyses.",
        "uid": "v-short-1248",
        "time_stamp": "",
        "time_start": "",
        "time_end": "",
        "paper_type": "short",
        "keywords": [],
        "doi": "",
        "fno": "",
        "presentation_mode": "",
        "open_access": false,
        "accessible_pdf": false,
        "has_image": false,
        "has_pdf": false,
        "paper_award": "",
        "image_caption": "",
        "external_paper_link": "",
        "youtube_ff_link": "",
        "youtube_ff_id": "",
        "bunny_ff_link": "",
        "bunny_ff_subtitles": "",
        "youtube_prerecorded_link": "",
        "youtube_prerecorded_id": "",
        "bunny_prerecorded_link": "",
        "bunny_prerecorded_subtitles": ""
    },
    "v-short-1264": {
        "slot_id": "v-short-1264",
        "session_id": "short0",
        "title": "Demystifying Spatial Dependence: Interactive Visualizations for Interpreting Local Spatial Autocorrelation",
        "contributors": [
            "Lee Mason"
        ],
        "authors": [
            {
                "name": "Lee Mason",
                "email": "masonlk@nih.gov",
                "affiliations": [
                    "NIH, Rockville, United States",
                    "Queen's University, Belfast, United Kingdom"
                ],
                "is_corresponding": true
            },
            {
                "name": "Bl\u221a\u00b0naid Hicks",
                "email": "b.hicks@qub.ac.uk",
                "affiliations": [
                    "Queen's University Belfast , Belfast , United Kingdom"
                ],
                "is_corresponding": false
            },
            {
                "name": "Jonas S Almeida",
                "email": "jonas.dealmeida@nih.gov",
                "affiliations": [
                    "National Institutes of Health, Rockville, United States"
                ],
                "is_corresponding": false
            }
        ],
        "abstract": "The Local Moran's I statistic is a valuable tool for identifying localized patterns of spatial autocorrelation. Understanding these patterns is crucial in spatial analysis, but interpreting the statistic can be difficult. To simplify this process, we introduce three novel visualizations that enhance the interpretation of Local Moran's I results. These visualizations can be interactively linked to one another, and to established visualizations, to offer a more holistic exploration of the results. We provide a JavaScript library with implementations of these new visual elements, along with a web dashboard that demonstrates their integrated use.",
        "uid": "v-short-1264",
        "time_stamp": "",
        "time_start": "",
        "time_end": "",
        "paper_type": "short",
        "keywords": [],
        "doi": "",
        "fno": "",
        "presentation_mode": "",
        "open_access": false,
        "accessible_pdf": false,
        "has_image": false,
        "has_pdf": false,
        "paper_award": "",
        "image_caption": "",
        "external_paper_link": "",
        "youtube_ff_link": "",
        "youtube_ff_id": "",
        "bunny_ff_link": "",
        "bunny_ff_subtitles": "",
        "youtube_prerecorded_link": "",
        "youtube_prerecorded_id": "",
        "bunny_prerecorded_link": "",
        "bunny_prerecorded_subtitles": ""
    },
    "v-short-1274": {
        "slot_id": "v-short-1274",
        "session_id": "short0",
        "title": "Dark Mode or Light Mode? Exploring the Impact of Contrast Polarity on Visualization Performance Between Age Groups",
        "contributors": [
            "Zack While"
        ],
        "authors": [
            {
                "name": "Zack While",
                "email": "zwhile@cs.umass.edu",
                "affiliations": [
                    "University of Massachusetts Amherst, Amherst, United States"
                ],
                "is_corresponding": true
            },
            {
                "name": "Ali Sarvghad",
                "email": "asarv@cs.umass.edu",
                "affiliations": [
                    "University of Massachusetts Amherst, Amherst, United States"
                ],
                "is_corresponding": false
            }
        ],
        "abstract": "This study examines the impact of positive and negative contrast polarities (i.e., light and dark modes) on the performance of younger adults and people in their late adulthood (PLA). In a crowdsourced study with 134 participants (69 below age 60, 66 aged 60 and above), we assessed their accuracy and time performing analysis tasks across three common visualization types (Bar, Line, Scatterplot) and two contrast polarities (positive and negative). We observed that, across both age groups, the polarity that led to better performance and the resulting amount of improvement varied on an individual basis, with each polarity benefiting comparable proportions of participants. Additionally, we observed that the choice of contrast polarity can have an impact on time similar to that of the choice of visualization type, resulting in an average percent difference of around 36%. These findings indicate that, overall, the effects of contrast polarity on visual analysis performance do not noticeably change with age. Furthermore, they underscore the importance of making visualizations available in both contrast polarities to better-support a broad audience with differing needs.",
        "uid": "v-short-1274",
        "time_stamp": "",
        "time_start": "",
        "time_end": "",
        "paper_type": "short",
        "keywords": [],
        "doi": "",
        "fno": "",
        "presentation_mode": "",
        "open_access": false,
        "accessible_pdf": false,
        "has_image": false,
        "has_pdf": false,
        "paper_award": "",
        "image_caption": "",
        "external_paper_link": "",
        "youtube_ff_link": "",
        "youtube_ff_id": "",
        "bunny_ff_link": "",
        "bunny_ff_subtitles": "",
        "youtube_prerecorded_link": "",
        "youtube_prerecorded_id": "",
        "bunny_prerecorded_link": "",
        "bunny_prerecorded_subtitles": ""
    },
    "v-short-1276": {
        "slot_id": "v-short-1276",
        "session_id": "short0",
        "title": "Representing Charts as Text for Language Models: An In-Depth Study of Question Answering for Bar Charts",
        "contributors": [
            "Victor S. Bursztyn"
        ],
        "authors": [
            {
                "name": "Victor S. Bursztyn",
                "email": "victorbursztyn2022@u.northwestern.edu",
                "affiliations": [
                    "Adobe Research, San Jose, United States"
                ],
                "is_corresponding": true
            },
            {
                "name": "Jane Hoffswell",
                "email": "jhoffs@adobe.com",
                "affiliations": [
                    "Adobe Research, Seattle, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Shunan Guo",
                "email": "sguo@adobe.com",
                "affiliations": [
                    "Adobe Research, San Jose, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Eunyee Koh",
                "email": "eunyee@adobe.com",
                "affiliations": [
                    "Adobe Research, San Jose, United States"
                ],
                "is_corresponding": false
            }
        ],
        "abstract": "Machine Learning models for chart-grounded Q&A (CQA) often treat charts as images, but performing CQA on pixel values has proven challenging. We thus investigate a resource overlooked by current ML-based approaches: the declarative documents describing how charts should visually encode data (i.e., chart specifications). In this work, we use chart specifications to enhance language models (LMs) for chart-reading tasks, such that the resulting system can robustly understand language for CQA. Through a case study with 359 bar charts, we test novel fine tuning schemes on both GPT-3 and T5 using a new dataset curated for two CQA tasks: question-answering and visual explanation generation. Our text-only approaches strongly outperform vision-based GPT-4 on explanation generation (99% vs. 63% accuracy), and show promising results for question-answering (57-67% accuracy). Through in-depth experiments, we also show that our text-only approaches are mostly robust to natural language variation.",
        "uid": "v-short-1276",
        "time_stamp": "",
        "time_start": "",
        "time_end": "",
        "paper_type": "short",
        "keywords": [],
        "doi": "",
        "fno": "",
        "presentation_mode": "",
        "open_access": false,
        "accessible_pdf": false,
        "has_image": false,
        "has_pdf": false,
        "paper_award": "",
        "image_caption": "",
        "external_paper_link": "",
        "youtube_ff_link": "",
        "youtube_ff_id": "",
        "bunny_ff_link": "",
        "bunny_ff_subtitles": "",
        "youtube_prerecorded_link": "",
        "youtube_prerecorded_id": "",
        "bunny_prerecorded_link": "",
        "bunny_prerecorded_subtitles": ""
    },
    "v-short-1277": {
        "slot_id": "v-short-1277",
        "session_id": "short0",
        "title": "Building and Eroding: Exogenous and Endogenous Factors that Influence Subjective Trust in Visualization",
        "contributors": [
            "R. Jordan Crouser"
        ],
        "authors": [
            {
                "name": "R. Jordan Crouser",
                "email": "jcrouser@smith.edu",
                "affiliations": [
                    "Smith College, Northampton, United States"
                ],
                "is_corresponding": true
            },
            {
                "name": "Syrine Matoussi",
                "email": "cmatoussi@smith.edu",
                "affiliations": [
                    "Smith College, Northampton, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Lan Kung",
                "email": "ekung@smith.edu",
                "affiliations": [
                    "Smith College, Northampton, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Saugat Pandey",
                "email": "p.saugat@wustl.edu",
                "affiliations": [
                    "Washington University in St. Louis, St. Louis, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Oen G McKinley",
                "email": "m.oen@wustl.edu",
                "affiliations": [
                    "Washington University in St. Louis, St. Louis, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Alvitta Ottley",
                "email": "alvitta@wustl.edu",
                "affiliations": [
                    "Washington University in St. Louis, St. Louis, United States"
                ],
                "is_corresponding": false
            }
        ],
        "abstract": "Trust is a subjective yet fundamental component of human-computer interaction, and is a determining factor in shaping the efficacy of data visualizations. Prior research has identified five dimensions of trust assessment in visualizations (credibility, clarity, reliability, familiarity, and confidence), and observed that these dimensions tend to vary predictably along with certain features of the visualization being evaluated. This raises a further question: how do the design features driving viewers' trust assessment vary with the characteristics of the viewers themselves? By reanalyzing data from these studies through the lens of individual differences, we build a more detailed map of the relationships between design features, individual characteristics, and trust behaviors. In particular, we model the distinct contributions of endogenous design features (such as visualization type, or the use of color) and exogenous user characteristics (such as visualization literacy), as well as the interactions between them. We then use these findings to make recommendations for individualized and adaptive visualization design.",
        "uid": "v-short-1277",
        "time_stamp": "",
        "time_start": "",
        "time_end": "",
        "paper_type": "short",
        "keywords": [],
        "doi": "",
        "fno": "",
        "presentation_mode": "",
        "open_access": false,
        "accessible_pdf": false,
        "has_image": false,
        "has_pdf": false,
        "paper_award": "",
        "image_caption": "",
        "external_paper_link": "",
        "youtube_ff_link": "",
        "youtube_ff_id": "",
        "bunny_ff_link": "",
        "bunny_ff_subtitles": "",
        "youtube_prerecorded_link": "",
        "youtube_prerecorded_id": "",
        "bunny_prerecorded_link": "",
        "bunny_prerecorded_subtitles": ""
    },
    "v-short-1285": {
        "slot_id": "v-short-1285",
        "session_id": "short0",
        "title": "\"Must Be a Tuesday\": Affect, Attribution, and Geographic Variability in Equity-Oriented Visualizations of Population Health Disparities",
        "contributors": [
            "Lace M. Padilla"
        ],
        "authors": [
            {
                "name": "Eli Holder",
                "email": "eli@3iap.com",
                "affiliations": [
                    "3iap, Raleigh, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Lace M. Padilla",
                "email": "l.padilla@northeastern.edu",
                "affiliations": [
                    "Northeastern University, Boston, United States",
                    "University of California Merced, Merced, United States"
                ],
                "is_corresponding": true
            }
        ],
        "abstract": "This study examines the impact of social-comparison risk visualizations on public health communication, comparing the effects of traditional bar charts against alternative jitter plots emphasizing geographic variability (geo jitter). The research highlights that whereas both visualization types increased perceived vulnerability, behavioral intent, and policy support, the geo jitter plots were significantly more effective in reducing unjustified personal attributions. Importantly, the findings also underscore the emotional challenges faced by visualization viewers from marginalized communities, indicating a need for designs that are sensitive to the potential for reinforcing stereotypes or eliciting negative emotions. This work suggests a strategic reevaluation of visual communication tools in public health to enhance understanding and engagement without contributing to negative attributions or emotional distress.",
        "uid": "v-short-1285",
        "time_stamp": "",
        "time_start": "",
        "time_end": "",
        "paper_type": "short",
        "keywords": [],
        "doi": "",
        "fno": "",
        "presentation_mode": "",
        "open_access": false,
        "accessible_pdf": false,
        "has_image": false,
        "has_pdf": false,
        "paper_award": "",
        "image_caption": "",
        "external_paper_link": "",
        "youtube_ff_link": "",
        "youtube_ff_id": "",
        "bunny_ff_link": "",
        "bunny_ff_subtitles": "",
        "youtube_prerecorded_link": "",
        "youtube_prerecorded_id": "",
        "bunny_prerecorded_link": "",
        "bunny_prerecorded_subtitles": ""
    },
    "v-short-1292": {
        "slot_id": "v-short-1292",
        "session_id": "short0",
        "title": "Multi-User Mobile Augmented Reality for Cardiovascular Surgical Planning",
        "contributors": [
            "Pratham Darrpan Mehta"
        ],
        "authors": [
            {
                "name": "Pratham Darrpan Mehta",
                "email": "pratham.mehta001@gmail.com",
                "affiliations": [
                    "Georgia Tech, Atlanta, United States"
                ],
                "is_corresponding": true
            },
            {
                "name": "Rahul Ozhur Narayanan",
                "email": "rnarayanan39@gatech.edu",
                "affiliations": [
                    "Georgia Tech, Atlanta, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Harsha Karanth",
                "email": "harsha5431@gmail.com",
                "affiliations": [
                    "Georgia Tech, Atlanta, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Haoyang Yang",
                "email": "alexanderyang@gatech.edu",
                "affiliations": [
                    "Georgia Institute of Technology, Atlanta, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Timothy C Slesnick",
                "email": "slesnickt@kidsheart.com",
                "affiliations": [
                    "Emory University, Atlanta, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Fawwaz Shaw",
                "email": "fawwaz.shaw@choa.org",
                "affiliations": [
                    "Emory University/Children's Healthcare of Atlanta, Atlanta, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Duen Horng (Polo) Chau",
                "email": "polo@gatech.edu",
                "affiliations": [
                    "Georgia Tech, Atlanta, United States"
                ],
                "is_corresponding": false
            }
        ],
        "abstract": "Collaborative planning for congenital heart diseases typically involves creating physical heart models through 3D printing, which are then examined by both surgeons and cardiologists. Recent developments in mobile augmented reality (AR) technologies have presented a viable alternative, known for their ease of use and portability. However, there is still a lack of research examining the utilization of multi-user mobile AR environments to support collaborative planning for cardiovascular surgeries. We created ARCollab, an iOS AR app designed for enabling multiple surgeons and cardiologists to interact with a patient's 3D heart model in a shared environment. ARCollab enables surgeons and cardiologists to import heart models, manipulate them through gestures and collaborate with other users, eliminating the need for fabricating physical heart models. Our evaluation of ARCollab's usability and usefulness in enhancing collaboration, conducted with three cardiothoracic surgeons and two cardiologists, marks the first human evaluation of a multi-user mobile AR tool for surgical planning. ARCollab is open-source, available at https://github.com/poloclub/arcollab.",
        "uid": "v-short-1292",
        "time_stamp": "",
        "time_start": "",
        "time_end": "",
        "paper_type": "short",
        "keywords": [],
        "doi": "",
        "fno": "",
        "presentation_mode": "",
        "open_access": false,
        "accessible_pdf": false,
        "has_image": false,
        "has_pdf": false,
        "paper_award": "",
        "image_caption": "",
        "external_paper_link": "",
        "youtube_ff_link": "",
        "youtube_ff_id": "",
        "bunny_ff_link": "",
        "bunny_ff_subtitles": "",
        "youtube_prerecorded_link": "",
        "youtube_prerecorded_id": "",
        "bunny_prerecorded_link": "",
        "bunny_prerecorded_subtitles": ""
    },
    "v-short-1301": {
        "slot_id": "v-short-1301",
        "session_id": "short0",
        "title": "Zoomable Glyph Tables for Interpreting Probabilistic Model Outputs for Reactionary Train Delays",
        "contributors": [
            "Aidan Slingsby"
        ],
        "authors": [
            {
                "name": "Aidan Slingsby",
                "email": "a.slingsby@city.ac.uk",
                "affiliations": [
                    "City, University of London, London, United Kingdom"
                ],
                "is_corresponding": true
            },
            {
                "name": "Jonathan Hyde",
                "email": "jonathan.hyde@risksol.co.uk",
                "affiliations": [
                    "Risk Solutions, Warrington, United Kingdom"
                ],
                "is_corresponding": false
            }
        ],
        "abstract": "Reactionary delay'' is a result of the accumulated cascading effects of knock-on train delays. It is becoming an increasing problem as shared railway infrastructure becomes more crowded. The chaotic nature of its effects is notoriously hard to predict. We use a stochastic Monte-Carto-style simulation of reactionary delay that produces whole distributions of likely reactionary delay. Our contribution is the demonstrating how Zoomable GlyphTables -- case-by-variable tables in which cases are rows, variables are columns, variables are complex composite metrics that incorporate distributions, and cells contain mini-charts that depict these as different level of detail through zoom interaction -- help interpret these results for helping understanding the causes and effects of reactionary delay and how they have been informing timetable robustness testing and tweaking. We describe our design principles, demonstrate how this supported our analytical tasks and we reflect on wider potential for Zoomable GlyphTables to be used more widely.",
        "uid": "v-short-1301",
        "time_stamp": "",
        "time_start": "",
        "time_end": "",
        "paper_type": "short",
        "keywords": [],
        "doi": "",
        "fno": "",
        "presentation_mode": "",
        "open_access": false,
        "accessible_pdf": false,
        "has_image": false,
        "has_pdf": false,
        "paper_award": "",
        "image_caption": "",
        "external_paper_link": "",
        "youtube_ff_link": "",
        "youtube_ff_id": "",
        "bunny_ff_link": "",
        "bunny_ff_subtitles": "",
        "youtube_prerecorded_link": "",
        "youtube_prerecorded_id": "",
        "bunny_prerecorded_link": "",
        "bunny_prerecorded_subtitles": ""
    }
}