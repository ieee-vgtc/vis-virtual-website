{
    "v-short-1040": {
        "slot_id": "v-short-1040",
        "session_id": "short0",
        "title": "Data Guards: Challenges and Solutions for Fostering Trust in Data",
        "contributors": [
            "Nicole Sultanum"
        ],
        "authors": [
            {
                "name": "Nicole Sultanum",
                "email": "nicole.sultanum@gmail.com",
                "affiliations": [
                    "Tableau Research, Seattle, United States"
                ],
                "is_corresponding": true
            },
            {
                "name": "Dennis Bromley",
                "email": "bromley.denny@gmail.com",
                "affiliations": [
                    "Tableau Research, Seattle, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Michael Correll",
                "email": "m.correll@northeastern.edu",
                "affiliations": [
                    "Northeastern University, Portland, United States"
                ],
                "is_corresponding": false
            }
        ],
        "abstract": "From dirty data to intentional deception, there are many threats to the validity of data-driven decisions. Making use of data, especially new or unfamiliar data, therefore requires a degree of trust or verification. How is this trust established? In this paper, we present the results of a series of interviews with both producers and consumers of data artifacts (outputs of data ecosystems like spreadsheets, charts, and dashboards) aimed at understanding strategies and obstacles to building trust in data. We find a recurring need, but lack of existing standards, for data validation and verification, especially among data consumers. We therefore propose a set of data guards: methods and tools for fostering trust in data artifacts.",
        "uid": "v-short-1040",
        "time_stamp": "",
        "time_start": "",
        "time_end": "",
        "paper_type": "short",
        "keywords": [],
        "doi": "",
        "fno": "",
        "presentation_mode": "",
        "open_access": false,
        "accessible_pdf": false,
        "has_image": false,
        "has_pdf": false,
        "paper_award": "",
        "image_caption": "",
        "external_paper_link": "",
        "youtube_ff_link": "",
        "youtube_ff_id": "",
        "bunny_ff_link": "",
        "bunny_ff_subtitles": "",
        "youtube_prerecorded_link": "",
        "youtube_prerecorded_id": "",
        "bunny_prerecorded_link": "",
        "bunny_prerecorded_subtitles": ""
    },
    "v-short-1047": {
        "slot_id": "v-short-1047",
        "session_id": "short0",
        "title": "Intuitive Design of Deep Learning Models through Visual Feedback",
        "contributors": [
            "JunYoung Choi"
        ],
        "authors": [
            {
                "name": "JunYoung Choi",
                "email": "juny0603@gmail.com",
                "affiliations": [
                    "VIENCE Inc., Seoul, Korea, Republic of",
                    "Korea University, Seoul, Korea, Republic of"
                ],
                "is_corresponding": true
            },
            {
                "name": "Sohee Park",
                "email": "wings159@vience.co.kr",
                "affiliations": [
                    "VIENCE Inc., Seoul, Korea, Republic of"
                ],
                "is_corresponding": false
            },
            {
                "name": "GaYeon Koh",
                "email": "hellenkoh@gmail.com",
                "affiliations": [
                    "Korea University, Seoul, Korea, Republic of"
                ],
                "is_corresponding": false
            },
            {
                "name": "Youngseo Kim",
                "email": "k0seo0330@vience.co.kr",
                "affiliations": [
                    "VIENCE Inc., Seoul, Korea, Republic of"
                ],
                "is_corresponding": false
            },
            {
                "name": "Won-Ki Jeong",
                "email": "wkjeong@korea.ac.kr",
                "affiliations": [
                    "VIENCE Inc., Seoul, Korea, Republic of",
                    "Korea University, Seoul, Korea, Republic of"
                ],
                "is_corresponding": false
            }
        ],
        "abstract": "In the rapidly evolving field of deep learning, the traditional methodologies for designing deep learning models predominantly rely on code-based frameworks. While these approaches provide flexibility, they also create a significant barrier to entry for non-experts and obscure the immediate impact of architectural decisions on model performance. In response to this challenge, recent no-code approaches have been developed with the aim of enabling easy model development through graphical interfaces. However, both traditional and no-code methodologies share a common limitation that the inability to predict model outcomes or identify issues without executing the model. To address this limitation, we introduce an intuitive visual feedback-based no-code approach to visualize and analyze deep learning models during the design phase. This approach utilizes dataflow-based visual programming with dynamic visual encoding of model architecture. A user study was conducted with deep learning developers to demonstrate the effectiveness of our approach in enhancing the model design process, improving model understanding, and facilitating a more intuitive development experience. The findings of this study suggest that real-time architectural visualization significantly contributes to more efficient model development and a deeper understanding of model behaviors.",
        "uid": "v-short-1047",
        "time_stamp": "",
        "time_start": "",
        "time_end": "",
        "paper_type": "short",
        "keywords": [],
        "doi": "",
        "fno": "",
        "presentation_mode": "",
        "open_access": false,
        "accessible_pdf": false,
        "has_image": false,
        "has_pdf": false,
        "paper_award": "",
        "image_caption": "",
        "external_paper_link": "",
        "youtube_ff_link": "",
        "youtube_ff_id": "",
        "bunny_ff_link": "",
        "bunny_ff_subtitles": "",
        "youtube_prerecorded_link": "",
        "youtube_prerecorded_id": "",
        "bunny_prerecorded_link": "",
        "bunny_prerecorded_subtitles": ""
    },
    "v-short-1049": {
        "slot_id": "v-short-1049",
        "session_id": "short0",
        "title": "A Comparative Study of Neural Surface Reconstruction for Scientific Visualization",
        "contributors": [
            "Siyuan Yao"
        ],
        "authors": [
            {
                "name": "Siyuan Yao",
                "email": "syao2@nd.edu",
                "affiliations": [
                    "University of Notre Dame, Notre Dame, United States"
                ],
                "is_corresponding": true
            },
            {
                "name": "Weixi Song",
                "email": "song.wx@whu.edu.cn",
                "affiliations": [
                    "Wuhan University, Wuhan, China"
                ],
                "is_corresponding": false
            },
            {
                "name": "Chaoli Wang",
                "email": "chaoli.wang@nd.edu",
                "affiliations": [
                    "University of Notre Dame, Notre Dame, United States"
                ],
                "is_corresponding": false
            }
        ],
        "abstract": "This comparative study evaluates various neural surface reconstruction methods, particularly focusing on their implications for scientific visualization through reconstructing 3D surfaces via multi-view rendering images. We categorize ten methods into neural radiance fields and neural implicit surfaces, uncovering the benefits of leveraging distance functions (i.e., SDFs and UDFs) to enhance the accuracy and smoothness of the reconstructed surfaces. Our findings highlight the efficiency and quality of NeuS2 for reconstructing closed surfaces and identify NeUDF as a promising candidate for reconstructing open surfaces despite some limitations. We further pinpoint directions for future research, including improving detail capture, optimizing UDF computations, and refining surface extraction methods. By sharing our benchmark dataset, we invite researchers to test the performance of their methods, contributing to the advancement of surface reconstruction solutions for scientific visualization.",
        "uid": "v-short-1049",
        "time_stamp": "",
        "time_start": "",
        "time_end": "",
        "paper_type": "short",
        "keywords": [],
        "doi": "",
        "fno": "",
        "presentation_mode": "",
        "open_access": false,
        "accessible_pdf": false,
        "has_image": false,
        "has_pdf": false,
        "paper_award": "",
        "image_caption": "",
        "external_paper_link": "",
        "youtube_ff_link": "",
        "youtube_ff_id": "",
        "bunny_ff_link": "",
        "bunny_ff_subtitles": "",
        "youtube_prerecorded_link": "",
        "youtube_prerecorded_id": "",
        "bunny_prerecorded_link": "",
        "bunny_prerecorded_subtitles": ""
    },
    "v-short-1054": {
        "slot_id": "v-short-1054",
        "session_id": "short0",
        "title": "Accelerating Transfer Function Update for Distance Map based Volume Rendering",
        "contributors": [
            "Michael Rauter"
        ],
        "authors": [
            {
                "name": "Michael Rauter",
                "email": "michael.rauter@fhwn.ac.at",
                "affiliations": [
                    "University of Applied Sciences Wiener Neustadt, Wiener Neustadt, Austria"
                ],
                "is_corresponding": true
            },
            {
                "name": "Lukas Zimmermann PhD",
                "email": "lukas.a.zimmermann@meduniwien.ac.at",
                "affiliations": [
                    "Medical University of Vienna, Vienna, Austria"
                ],
                "is_corresponding": false
            },
            {
                "name": "Markus Zeilinger PhD",
                "email": "markus.zeilinger@fhwn.ac.at",
                "affiliations": [
                    "University of Applied Sciences Wiener Neustadt, Wiener Neustadt, Austria"
                ],
                "is_corresponding": false
            }
        ],
        "abstract": "Direct volume rendering using ray-casting is widely used in practice. By using GPUs and applying acceleration techniques as empty space skipping, high frame rates are possible on modern hardware. This enables performance-critical use-cases such as virtual reality volume rendering. The currently fastest known technique uses volumetric distance maps to skip empty sections of the volume during ray-casting but requires the distance map to be updated per transfer function change. In this paper, we demonstrate a technique for subdividing the volume intensity range into partitions and deriving what we call partitioned distance maps. These can be used to accelerate the distance map computation for a newly changed transfer function by a factor up to 30. This allows the currently fastest known empty space skipping approach to be used while maintaining high frame rates even when the transfer function is changed frequently.",
        "uid": "v-short-1054",
        "time_stamp": "",
        "time_start": "",
        "time_end": "",
        "paper_type": "short",
        "keywords": [],
        "doi": "",
        "fno": "",
        "presentation_mode": "",
        "open_access": false,
        "accessible_pdf": false,
        "has_image": false,
        "has_pdf": false,
        "paper_award": "",
        "image_caption": "",
        "external_paper_link": "",
        "youtube_ff_link": "",
        "youtube_ff_id": "",
        "bunny_ff_link": "",
        "bunny_ff_subtitles": "",
        "youtube_prerecorded_link": "",
        "youtube_prerecorded_id": "",
        "bunny_prerecorded_link": "",
        "bunny_prerecorded_subtitles": ""
    },
    "v-short-1056": {
        "slot_id": "v-short-1056",
        "session_id": "short0",
        "title": "FCNR: Fast Compressive Neural Representation of Visualization Images",
        "contributors": [
            "Yunfei Lu"
        ],
        "authors": [
            {
                "name": "Yunfei Lu",
                "email": "ylu25@nd.edu",
                "affiliations": [
                    "University of Notre Dame, Notre Dame, United States"
                ],
                "is_corresponding": true
            },
            {
                "name": "Pengfei Gu",
                "email": "pgu@nd.edu",
                "affiliations": [
                    "University of Notre Dame, Notre Dame, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Chaoli Wang",
                "email": "chaoli.wang@nd.edu",
                "affiliations": [
                    "University of Notre Dame, Notre Dame, United States"
                ],
                "is_corresponding": false
            }
        ],
        "abstract": "We present FCNR, a fast compressive neural representation for tens of thousands of visualization images under varying viewpoints and timesteps. The existing NeRVI solution, albeit enjoying a high compression rate, incurs slow speeds in encoding and decoding. Built on the recent advances in stereo image compression, FCNR assimilates stereo context modules and joint context transfer modules to compress image pairs. Our solution significantly improves encoding and decoding speed while maintaining high reconstruction quality and satisfying compression rate. To demonstrate its effectiveness, we compare FCNR with state-of-the-art neural compression methods, including E-NeRV, HNeRV, NeRVI, and ECSIC.",
        "uid": "v-short-1056",
        "time_stamp": "",
        "time_start": "",
        "time_end": "",
        "paper_type": "short",
        "keywords": [],
        "doi": "",
        "fno": "",
        "presentation_mode": "",
        "open_access": false,
        "accessible_pdf": false,
        "has_image": false,
        "has_pdf": false,
        "paper_award": "",
        "image_caption": "",
        "external_paper_link": "",
        "youtube_ff_link": "",
        "youtube_ff_id": "",
        "bunny_ff_link": "",
        "bunny_ff_subtitles": "",
        "youtube_prerecorded_link": "",
        "youtube_prerecorded_id": "",
        "bunny_prerecorded_link": "",
        "bunny_prerecorded_subtitles": ""
    },
    "v-short-1057": {
        "slot_id": "v-short-1057",
        "session_id": "short0",
        "title": "On Combined Visual Cluster and Set Analysis",
        "contributors": [
            "Nikolaus Piccolotto"
        ],
        "authors": [
            {
                "name": "Nikolaus Piccolotto",
                "email": "nikolaus.piccolotto@tuwien.ac.at",
                "affiliations": [
                    "TU Wien, Vienna, Austria"
                ],
                "is_corresponding": true
            },
            {
                "name": "Markus Wallinger",
                "email": "mwallinger@ac.tuwien.ac.at",
                "affiliations": [
                    "TU Wien, Vienna, Austria"
                ],
                "is_corresponding": false
            },
            {
                "name": "Silvia Miksch",
                "email": "miksch@ifs.tuwien.ac.at",
                "affiliations": [
                    "Institute of Visual Computing and Human-Centered Technology, Vienna, Austria"
                ],
                "is_corresponding": false
            },
            {
                "name": "Markus B\u00f6gl",
                "email": "markus.boegl@tuwien.ac.at",
                "affiliations": [
                    "TU Wien, Vienna, Austria"
                ],
                "is_corresponding": false
            }
        ],
        "abstract": "Real-world datasets often consist of quantitative and categorical variables. The analyst needs to focus on either kind separately or both jointly. We proposed a visualization technique tackling these challenges that supports visual cluster and set analysis. In this paper, we investigate how its visualization parameters affect the accuracy and speed of cluster and set analysis tasks in a controlled experiment. Our findings show that, with the proper settings, our visualization can support both task types well. However, we did not find settings suitable for the joint task, which provides opportunities for future research.",
        "uid": "v-short-1057",
        "time_stamp": "",
        "time_start": "",
        "time_end": "",
        "paper_type": "short",
        "keywords": [],
        "doi": "",
        "fno": "",
        "presentation_mode": "",
        "open_access": false,
        "accessible_pdf": false,
        "has_image": false,
        "has_pdf": false,
        "paper_award": "",
        "image_caption": "",
        "external_paper_link": "",
        "youtube_ff_link": "",
        "youtube_ff_id": "",
        "bunny_ff_link": "",
        "bunny_ff_subtitles": "",
        "youtube_prerecorded_link": "",
        "youtube_prerecorded_id": "",
        "bunny_prerecorded_link": "",
        "bunny_prerecorded_subtitles": ""
    },
    "v-short-1058": {
        "slot_id": "v-short-1058",
        "session_id": "short0",
        "title": "ImageSI: Semantic Interaction for Deep Learning Image Projections",
        "contributors": [
            "Rebecca Faust"
        ],
        "authors": [
            {
                "name": "Jiayue Lin",
                "email": "jiayuelin@vt.edu",
                "affiliations": [
                    "Vriginia Tech, Blacksburg, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Rebecca Faust",
                "email": "rfaust1@tulane.edu",
                "affiliations": [
                    "Tulane University, New Orleans, United States"
                ],
                "is_corresponding": true
            },
            {
                "name": "Chris North",
                "email": "north@vt.edu",
                "affiliations": [
                    "Virginia Tech, Blacksburg, United States"
                ],
                "is_corresponding": false
            }
        ],
        "abstract": "Semantic interaction (SI) in Dimension Reduction (DR) of images allows users to incorporate feedback through direct manipulation of the 2D positions of images. Through interaction, users specify a set of pairwise relationships that the DR should aim to capture. Existing methods for images incorporate feedback into the DR through feature weights on abstract embedding features. However, if the original embedding features do not suitably capture the users task then the DR cannot either. We propose, ImageSI, an SI method for image DR that incorporates user feedback directly into the image model to update the underlying embeddings, rather than weighting them. In doing so, ImageSI ensures that the embeddings suitably capture the features necessary for the task so that the DR can subsequently organize images using those features. We present two variations of ImageSI using different loss functions - ImageSI_MDS-Inverse , which prioritizes the explicit pairwise relationships from the interaction and ImageSI_Triplet, which prioritizes clustering, using the interaction to define groups of images. Finally, we present a usage scenario and a simulation-based evaluation to demonstrate the utility of ImageSI and compare it to current methods.",
        "uid": "v-short-1058",
        "time_stamp": "",
        "time_start": "",
        "time_end": "",
        "paper_type": "short",
        "keywords": [],
        "doi": "",
        "fno": "",
        "presentation_mode": "",
        "open_access": false,
        "accessible_pdf": false,
        "has_image": false,
        "has_pdf": false,
        "paper_award": "",
        "image_caption": "",
        "external_paper_link": "",
        "youtube_ff_link": "",
        "youtube_ff_id": "",
        "bunny_ff_link": "",
        "bunny_ff_subtitles": "",
        "youtube_prerecorded_link": "",
        "youtube_prerecorded_id": "",
        "bunny_prerecorded_link": "",
        "bunny_prerecorded_subtitles": ""
    },
    "v-short-1059": {
        "slot_id": "v-short-1059",
        "session_id": "short0",
        "title": "A Literature-based Visualization Task Taxonomy for Gantt charts",
        "contributors": [
            "Sayef Azad Sakin"
        ],
        "authors": [
            {
                "name": "Sayef Azad Sakin",
                "email": "sayefsakin@sci.utah.edu",
                "affiliations": [
                    "University of Utah, Salt Lake City, United States"
                ],
                "is_corresponding": true
            },
            {
                "name": "Katherine E. Isaacs",
                "email": "kisaacs@sci.utah.edu",
                "affiliations": [
                    "The University of Utah, Salt Lake City, United States"
                ],
                "is_corresponding": false
            }
        ],
        "abstract": "Gantt charts are a widely-used idiom for visualizing temporal discrete event sequence data where dependencies exist between events. They are popular in domains such as manufacturing and computing for their intuitive layout of such data. However, these domains frequently generate data at scales which tax both the visual representation and the ability to render it at interactive speeds. To aid visualization developers who use Gantt charts in these situations, we develop a task taxonomy of low level visualization tasks supported by Gantt charts and connect them to the data queries needed to support them. Our taxonomy is derived through a systematic literature survey of visualizations using Gantt charts over the past 30 years.",
        "uid": "v-short-1059",
        "time_stamp": "",
        "time_start": "",
        "time_end": "",
        "paper_type": "short",
        "keywords": [],
        "doi": "",
        "fno": "",
        "presentation_mode": "",
        "open_access": false,
        "accessible_pdf": false,
        "has_image": false,
        "has_pdf": false,
        "paper_award": "",
        "image_caption": "",
        "external_paper_link": "",
        "youtube_ff_link": "",
        "youtube_ff_id": "",
        "bunny_ff_link": "",
        "bunny_ff_subtitles": "",
        "youtube_prerecorded_link": "",
        "youtube_prerecorded_id": "",
        "bunny_prerecorded_link": "",
        "bunny_prerecorded_subtitles": ""
    },
    "v-short-1062": {
        "slot_id": "v-short-1062",
        "session_id": "short0",
        "title": "Integrating Annotations into the Design Process for Sonifications and Physicalizations",
        "contributors": [
            "S. Sandra Bae"
        ],
        "authors": [
            {
                "name": "Rhys Sorenson-Graff",
                "email": "sorensor@whitman.edu",
                "affiliations": [
                    "Whitman College, Walla Walla, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "S. Sandra Bae",
                "email": "sandra.bae@colorado.edu",
                "affiliations": [
                    "University of Colorado Boulder, Boulder, United States"
                ],
                "is_corresponding": true
            },
            {
                "name": "Jordan Wirfs-Brock",
                "email": "wirfsbro@colorado.edu",
                "affiliations": [
                    "Whitman College, Walla Walla, United States"
                ],
                "is_corresponding": false
            }
        ],
        "abstract": "Annotations are a critical component of visualizations, helping viewers interpret the visual representation and highlighting critical data insights. Despite its significant role, we lack an understanding of how annotations can be incorporated into other data representations, such as physicalizations and sonifications. Given the emergent nature of these representations, sonifications and physicalizations lack formalized conventions (e.g., design space, vocabulary) that can introduce challenges for audiences to interpret the intended data encoding. To address this challenge, this work focuses on how annotations can be more tightly integrated into the design process of creating sonifications and physicalization. In an exploratory study with 13 designers, we explore how visualization annotation techniques can be adapted to sonic and physical modalities. Our work highlights how annotations for sonification and physicalizations are inseparable from their data encodings",
        "uid": "v-short-1062",
        "time_stamp": "",
        "time_start": "",
        "time_end": "",
        "paper_type": "short",
        "keywords": [],
        "doi": "",
        "fno": "",
        "presentation_mode": "",
        "open_access": false,
        "accessible_pdf": false,
        "has_image": false,
        "has_pdf": false,
        "paper_award": "",
        "image_caption": "",
        "external_paper_link": "",
        "youtube_ff_link": "",
        "youtube_ff_id": "",
        "bunny_ff_link": "",
        "bunny_ff_subtitles": "",
        "youtube_prerecorded_link": "",
        "youtube_prerecorded_id": "",
        "bunny_prerecorded_link": "",
        "bunny_prerecorded_subtitles": ""
    },
    "v-short-1064": {
        "slot_id": "v-short-1064",
        "session_id": "short0",
        "title": "Bavisitter: Integrating Design Guidelines into Large Language Models for Visualization Authoring",
        "contributors": [
            "Jiwon Choi"
        ],
        "authors": [
            {
                "name": "Jiwon Choi",
                "email": "jiwnchoi@skku.edu",
                "affiliations": [
                    "Sungkyunkwan University, Suwon, Korea, Republic of"
                ],
                "is_corresponding": true
            },
            {
                "name": "Jaeung Lee",
                "email": "dlwodnd00@skku.edu",
                "affiliations": [
                    "Sungkyunkwan University, Suwon, Korea, Republic of"
                ],
                "is_corresponding": false
            },
            {
                "name": "Jaemin Jo",
                "email": "jmjo@skku.edu",
                "affiliations": [
                    "Sungkyunkwan University, Suwon, Korea, Republic of"
                ],
                "is_corresponding": false
            }
        ],
        "abstract": "Large Language Models (LLMs) have demonstrated remarkable versatility in visualization authoring, but often generate suboptimal designs that are invalid or fail to adhere to design guidelines for effective visualization. We present Bavisitter, a natural language interface that integrates established visualization design guidelines into LLMs. Based on our survey on the design issues in LLM-generated visualizations, Bavisitter monitors the generated visualizations during a visualization authoring dialogue to detect an issue. When an issue is detected, it intervenes in the dialogue, suggesting possible solutions to the issue by modifying the prompts. We also demonstrate two use cases where Bavisitter detects and resolves design issues from the actual LLM-generated visualizations.",
        "uid": "v-short-1064",
        "time_stamp": "",
        "time_start": "",
        "time_end": "",
        "paper_type": "short",
        "keywords": [],
        "doi": "",
        "fno": "",
        "presentation_mode": "",
        "open_access": false,
        "accessible_pdf": false,
        "has_image": false,
        "has_pdf": false,
        "paper_award": "",
        "image_caption": "",
        "external_paper_link": "",
        "youtube_ff_link": "",
        "youtube_ff_id": "",
        "bunny_ff_link": "",
        "bunny_ff_subtitles": "",
        "youtube_prerecorded_link": "",
        "youtube_prerecorded_id": "",
        "bunny_prerecorded_link": "",
        "bunny_prerecorded_subtitles": ""
    },
    "v-short-1065": {
        "slot_id": "v-short-1065",
        "session_id": "short0",
        "title": "GhostUMAP: Measuring Pointwise Instability in Dimensionality Reduction",
        "contributors": [
            "Myeongwon Jung"
        ],
        "authors": [
            {
                "name": "Myeongwon Jung",
                "email": "mw.jung@skku.edu",
                "affiliations": [
                    "Sungkyunkwan University, Suwon, Korea, Republic of"
                ],
                "is_corresponding": true
            },
            {
                "name": "Takanori Fujiwara",
                "email": "takanori.fujiwara@liu.se",
                "affiliations": [
                    "Link\u00f6ping University, Norrk\u00f6ping, Sweden"
                ],
                "is_corresponding": false
            },
            {
                "name": "Jaemin Jo",
                "email": "jmjo@skku.edu",
                "affiliations": [
                    "Sungkyunkwan University, Suwon, Korea, Republic of"
                ],
                "is_corresponding": false
            }
        ],
        "abstract": "Although many dimensionality reduction (DR) techniques employ stochastic methods for computational efficiency, such as negative sampling or stochastic gradient descent, their impact on the projection has been underexplored. In this work, we investigate how such stochasticity affects the stability of projections and present a novel DR technique, GhostUMAP, to measure the pointwise instability of projections. Our idea is to introduce clones of data points, \"ghosts\", into UMAP's layout optimization process. Ghosts are designed to be completely passive: they do not affect any others but are influenced by attractive and repulsive forces from the original data points. After a single optimization run, GhostUMAP can capture the projection instability of data points by measuring the variance with the projected positions of their ghosts. We also present a successive halving technique to reduce the computation of GhostUMAP. Our results suggest that GhostUMAP can reveal unstable data points with a reasonable computational overhead.",
        "uid": "v-short-1065",
        "time_stamp": "",
        "time_start": "",
        "time_end": "",
        "paper_type": "short",
        "keywords": [],
        "doi": "",
        "fno": "",
        "presentation_mode": "",
        "open_access": false,
        "accessible_pdf": false,
        "has_image": false,
        "has_pdf": false,
        "paper_award": "",
        "image_caption": "",
        "external_paper_link": "",
        "youtube_ff_link": "",
        "youtube_ff_id": "",
        "bunny_ff_link": "",
        "bunny_ff_subtitles": "",
        "youtube_prerecorded_link": "",
        "youtube_prerecorded_id": "",
        "bunny_prerecorded_link": "",
        "bunny_prerecorded_subtitles": ""
    },
    "v-short-1068": {
        "slot_id": "v-short-1068",
        "session_id": "short0",
        "title": "DASH: A Bimodal Data Exploration Tool for Interactive Text and Visualizations",
        "contributors": [
            "Dennis Bromley"
        ],
        "authors": [
            {
                "name": "Dennis Bromley",
                "email": "bromley.denny@gmail.com",
                "affiliations": [
                    "Tableau Research, Seattle, United States"
                ],
                "is_corresponding": true
            },
            {
                "name": "Vidya Setlur",
                "email": "vsetlur@tableau.com",
                "affiliations": [
                    "Tableau Research, Palo Alto, United States"
                ],
                "is_corresponding": false
            }
        ],
        "abstract": "Integrating textual content, such as titles, annotations, and captions, with visualizations facilitates comprehension and takeaways during data exploration. Yet current tools often lack mechanisms for integrating meaningful text with visual data. This paper introduces DASH, a bimodal data exploration tool that supports integrating semantic levels into the interactive process of visualization and text-based analysis. DASH operationalizes a modified version of Lundgard et al.'s semantic hierarchy model that categorizes data descriptions into four levels ranging from basic encodings to high-level insights. By leveraging this structured semantic level framework and a large language model's text generation capabilities, DASH enables the creation of data-driven narratives via drag-and-drop user interaction. Through a preliminary user evaluation, we discuss the utility of DASH's text and chart integration capabilities when participants perform data exploration with the tool. Based on the study's feedback and observations, we discuss implications for designing unified text and chart authoring tools.",
        "uid": "v-short-1068",
        "time_stamp": "",
        "time_start": "",
        "time_end": "",
        "paper_type": "short",
        "keywords": [],
        "doi": "",
        "fno": "",
        "presentation_mode": "",
        "open_access": false,
        "accessible_pdf": false,
        "has_image": false,
        "has_pdf": false,
        "paper_award": "",
        "image_caption": "",
        "external_paper_link": "",
        "youtube_ff_link": "",
        "youtube_ff_id": "",
        "bunny_ff_link": "",
        "bunny_ff_subtitles": "",
        "youtube_prerecorded_link": "",
        "youtube_prerecorded_id": "",
        "bunny_prerecorded_link": "",
        "bunny_prerecorded_subtitles": ""
    },
    "v-short-1072": {
        "slot_id": "v-short-1072",
        "session_id": "short0",
        "title": "Assessing Graphical Perception of Image Embedding Models using Channel Effectiveness",
        "contributors": [
            "Soohyun Lee"
        ],
        "authors": [
            {
                "name": "Soohyun Lee",
                "email": "dtngus0111@gmail.com",
                "affiliations": [
                    "Seoul National University, Seoul, Korea, Republic of"
                ],
                "is_corresponding": true
            },
            {
                "name": "Minsuk Chang",
                "email": "jangsus1@snu.ac.kr",
                "affiliations": [
                    "Seoul National University, Seoul, Korea, Republic of"
                ],
                "is_corresponding": false
            },
            {
                "name": "Seokhyeon Park",
                "email": "shpark@hcil.snu.ac.kr",
                "affiliations": [
                    "Seoul National University, Seoul, Korea, Republic of"
                ],
                "is_corresponding": false
            },
            {
                "name": "Jinwook Seo",
                "email": "jseo@snu.ac.kr",
                "affiliations": [
                    "Seoul National University, Seoul, Korea, Republic of"
                ],
                "is_corresponding": false
            }
        ],
        "abstract": "Recent advancements in vision models have significantly enhanced their ability to perform complex chart understanding tasks, such as chart captioning and chart question answering. However, assessing how these models process charts remains challenging. Existing benchmarks only coarsely evaluate how well the model performs the given task without thoroughly evaluating the underlying mechanisms that drive performance, such as how models extract image embeddings. This gap limits our understanding of the model's perceptual capabilities regarding fundamental graphical components. Therefore, we introduce a novel evaluation framework designed to assess the graphical perception of image embedding models. In the context of chart comprehension, we examine two main aspects of channel effectiveness: accuracy and discriminability of various visual channels. We first assess channel accuracy through the linearity of embeddings, which is the degree to which the perceived magnitude is proportional to the size of the stimulus. % based on the assumption that perceived magnitude should be proportional to the size of Conversely, distances between embeddings serve as a measure of discriminability; embeddings that are far apart can be considered discriminable. Our experiments on a general image embedding model, CLIP, provided that it perceives channel accuracy differently from humans and demonstrated distinct discriminability in specific channels such as length, tilt, and curvature. We aim to extend our work as a more general benchmark for reliable visual encoders and enhance a model for two distinctive goals for future applications: precise chart comprehension and mimicking human perception.",
        "uid": "v-short-1072",
        "time_stamp": "",
        "time_start": "",
        "time_end": "",
        "paper_type": "short",
        "keywords": [],
        "doi": "",
        "fno": "",
        "presentation_mode": "",
        "open_access": false,
        "accessible_pdf": false,
        "has_image": false,
        "has_pdf": false,
        "paper_award": "",
        "image_caption": "",
        "external_paper_link": "",
        "youtube_ff_link": "",
        "youtube_ff_id": "",
        "bunny_ff_link": "",
        "bunny_ff_subtitles": "",
        "youtube_prerecorded_link": "",
        "youtube_prerecorded_id": "",
        "bunny_prerecorded_link": "",
        "bunny_prerecorded_subtitles": ""
    },
    "v-short-1078": {
        "slot_id": "v-short-1078",
        "session_id": "short0",
        "title": "Design Patterns in Right-to-Left Visualizations: The Case of Arabic Content",
        "contributors": [
            "Muna Alebri"
        ],
        "authors": [
            {
                "name": "Muna Alebri",
                "email": "muna.alebri.19@ucl.ac.uk",
                "affiliations": [
                    "University College London, London, United Kingdom",
                    "UAE University , Al Ain, United Arab Emirates"
                ],
                "is_corresponding": true
            },
            {
                "name": "No\u00eblle Rakotondravony",
                "email": "ntrakotondravony@wpi.edu",
                "affiliations": [
                    "Worcester Polytechnic Institute, Worcester, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Lane Harrison",
                "email": "ltharrison@wpi.edu",
                "affiliations": [
                    "Worcester Polytechnic Institute, Worcester, United States"
                ],
                "is_corresponding": false
            }
        ],
        "abstract": "Data visualizations are reaching global audiences. As people who use Right-to-left (RTL) scripts constitute over a billion potential data visualization users, a need emerges to investigate how visualizations are communicated to them. Web design guidelines exist to assist designers in adapting different reading directions, yet we lack a similar standard for visualization design. This paper investigates the design patterns of visualizations with RTL scripts. We collected 128 visualizations from data-driven articles published in Arabic news outlets and analyzed their chart composition, textual elements, and sources. Our analysis suggests that designers tend to apply RTL approaches more frequently for categorical data. In other situations, we observed a mix of Left-to-right (LTR) and RTL approaches for chart directions and structures, sometimes inconsistently utilized within the same article. We reflect on this lack of clear guidelines for RTL data visualizations and derive implications for visualization authoring tools and future research directions.",
        "uid": "v-short-1078",
        "time_stamp": "",
        "time_start": "",
        "time_end": "",
        "paper_type": "short",
        "keywords": [],
        "doi": "",
        "fno": "",
        "presentation_mode": "",
        "open_access": false,
        "accessible_pdf": false,
        "has_image": false,
        "has_pdf": false,
        "paper_award": "",
        "image_caption": "",
        "external_paper_link": "",
        "youtube_ff_link": "",
        "youtube_ff_id": "",
        "bunny_ff_link": "",
        "bunny_ff_subtitles": "",
        "youtube_prerecorded_link": "",
        "youtube_prerecorded_id": "",
        "bunny_prerecorded_link": "",
        "bunny_prerecorded_subtitles": ""
    },
    "v-short-1079": {
        "slot_id": "v-short-1079",
        "session_id": "short0",
        "title": "AEye: A Visualization Tool for Image Datasets",
        "contributors": [
            "Florian Gr\u221a\u2202tschla"
        ],
        "authors": [
            {
                "name": "Florian Gr\u00f6tschla",
                "email": "fgroetschla@ethz.ch",
                "affiliations": [
                    "ETH Zurich, Zurich, Switzerland"
                ],
                "is_corresponding": false
            },
            {
                "name": "Luca A Lanzend\u00f6rfer",
                "email": "lanzendoerfer@ethz.ch",
                "affiliations": [
                    "ETH Zurich, Zurich, Switzerland"
                ],
                "is_corresponding": false
            },
            {
                "name": "Marco Calzavara",
                "email": "mcalzavara@student.ethz.ch",
                "affiliations": [
                    "ETH Zurich, Zurich, Switzerland"
                ],
                "is_corresponding": false
            },
            {
                "name": "Roger Wattenhofer",
                "email": "wattenhofer@ethz.ch",
                "affiliations": [
                    "ETH Zurich, Zurich, Switzerland"
                ],
                "is_corresponding": false
            }
        ],
        "abstract": "Image datasets serve as the foundation for machine learning models in computer vision, significantly influencing model capabilities, performance, and biases alongside architectural considerations. Therefore, understanding the composition and distribution of these datasets has become increasingly crucial. To address the need for intuitive exploration of these datasets, we propose AEye, an extensible and scalable visualization tool tailored to image datasets. AEye utilizes a contrastively trained model to embed images into semantically meaningful high-dimensional representations, facilitating data clustering and organization. To visualize the high-dimensional representations, we project them onto a two-dimensional plane and arrange images in layers so users can seamlessly navigate and explore them interactively. Furthermore, AEye facilitates semantic search functionalities for both text and image queries, enabling users to search for content. We open-source the codebase for AEye, and provide a simple configuration to add additional datasets.",
        "uid": "v-short-1079",
        "time_stamp": "",
        "time_start": "",
        "time_end": "",
        "paper_type": "short",
        "keywords": [],
        "doi": "",
        "fno": "",
        "presentation_mode": "",
        "open_access": false,
        "accessible_pdf": false,
        "has_image": false,
        "has_pdf": false,
        "paper_award": "",
        "image_caption": "",
        "external_paper_link": "",
        "youtube_ff_link": "",
        "youtube_ff_id": "",
        "bunny_ff_link": "",
        "bunny_ff_subtitles": "",
        "youtube_prerecorded_link": "",
        "youtube_prerecorded_id": "",
        "bunny_prerecorded_link": "",
        "bunny_prerecorded_subtitles": ""
    },
    "v-short-1081": {
        "slot_id": "v-short-1081",
        "session_id": "short0",
        "title": "Gridlines Mitigate Sine Illusion in Line Charts",
        "contributors": [
            "Cindy Xiong Bearfield"
        ],
        "authors": [
            {
                "name": "Clayton J Knittel",
                "email": "cknit1999@gmail.com",
                "affiliations": [
                    "Google LLC, San Francisco, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Jane Awuah",
                "email": "jawuah3@gatech.edu",
                "affiliations": [
                    "Georgia Institute of Technology, Atlanta, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Steven L Franconeri",
                "email": "franconeri@northwestern.edu",
                "affiliations": [
                    "Northwestern University, Evanston, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Cindy Xiong Bearfield",
                "email": "cxiong@gatech.edu",
                "affiliations": [
                    "Georgia Tech, Atlanta, United States"
                ],
                "is_corresponding": true
            }
        ],
        "abstract": "Sine illusion happens when the more quickly changing pairs of lines lead to bigger underestimates of the delta between them. We evaluate three visual manipulations on mitigating sine illusions: dotted lines, aligned gridlines, and offset gridlines via a user study. We asked participants to compare the deltas between two lines at two time points and found aligned gridlines to be the most effective in mitigating sine illusions. Using data from the user study, we produced a model that predicts the impact of the sine illusion in line charts by accounting for the ratio of the vertical distance between the two points of comparison. When the ratio is less than 50\\%, participants begin to be influenced by the sine illusion. This effect can be significantly exacerbated when the difference between the two deltas falls under 30\\%. We compared two explanations for the sine illusion based on our data: either participants were mistakenly using the perpendicular distance between the two lines to make their comparison (the perpendicular explanation), or they incorrectly relied on the length of the line segment perpendicular to the angle bisector of the bottom and top lines (the equal triangle explanation). We found the equal triangle explanation to be the more predictive model explaining participant behaviors.",
        "uid": "v-short-1081",
        "time_stamp": "",
        "time_start": "",
        "time_end": "",
        "paper_type": "short",
        "keywords": [],
        "doi": "",
        "fno": "",
        "presentation_mode": "",
        "open_access": false,
        "accessible_pdf": false,
        "has_image": false,
        "has_pdf": false,
        "paper_award": "",
        "image_caption": "",
        "external_paper_link": "",
        "youtube_ff_link": "",
        "youtube_ff_id": "",
        "bunny_ff_link": "",
        "bunny_ff_subtitles": "",
        "youtube_prerecorded_link": "",
        "youtube_prerecorded_id": "",
        "bunny_prerecorded_link": "",
        "bunny_prerecorded_subtitles": ""
    },
    "v-short-1089": {
        "slot_id": "v-short-1089",
        "session_id": "short0",
        "title": "A Two-Phase Visualization System for Continuous Human-AI Collaboration in Sequelae Analysis and Modeling",
        "contributors": [
            "Yang Ouyang"
        ],
        "authors": [
            {
                "name": "Yang Ouyang",
                "email": "ouyy@shanghaitech.edu.cn",
                "affiliations": [
                    "ShanghaiTech University, Shanghai, China",
                    "ShanghaiTech University, Shanghai, China"
                ],
                "is_corresponding": true
            },
            {
                "name": "Chenyang Zhang",
                "email": "zhang414@illinois.edu",
                "affiliations": [
                    "University of Illinois at Urbana-Champaign, Champaign, United States",
                    "University of Illinois at Urbana-Champaign, Champaign, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "He Wang",
                "email": "wanghe1@shanghaitech.edu.cn",
                "affiliations": [
                    "ShanghaiTech University, Shanghai, China",
                    "ShanghaiTech University, Shanghai, China"
                ],
                "is_corresponding": false
            },
            {
                "name": "Tianle Ma",
                "email": "15301050137@fudan.edu.cn",
                "affiliations": [
                    "Zhongshan Hospital Fudan University, Shanghai, China",
                    "Zhongshan Hospital Fudan University, Shanghai, China"
                ],
                "is_corresponding": false
            },
            {
                "name": "Chang Jiang",
                "email": "cjiang_fdu@yeah.net",
                "affiliations": [
                    "Zhongshan Hospital Fudan University, Shanghai, China",
                    "Zhongshan Hospital Fudan University, Shanghai, China"
                ],
                "is_corresponding": false
            },
            {
                "name": "Yuheng Yan",
                "email": "522649732@qq.com",
                "affiliations": [
                    "Zhongshan Hospital Fudan University, Shanghai, China",
                    "Zhongshan Hospital Fudan University, Shanghai, China"
                ],
                "is_corresponding": false
            },
            {
                "name": "Zuoqin Yan",
                "email": "yan.zuoqin@zs-hospital.sh.cn",
                "affiliations": [
                    "Zhongshan Hospital Fudan University, Shanghai, China",
                    "Zhongshan Hospital Fudan University, Shanghai, China"
                ],
                "is_corresponding": false
            },
            {
                "name": "Xiaojuan Ma",
                "email": "mxj@cse.ust.hk",
                "affiliations": [
                    "Hong Kong University of Science and Technology, Hong Kong, Hong Kong",
                    "Hong Kong University of Science and Technology, Hong Kong, Hong Kong"
                ],
                "is_corresponding": false
            },
            {
                "name": "Chuhan Shi",
                "email": "cshiag@connect.ust.hk",
                "affiliations": [
                    "Southeast University, Nanjing, China",
                    "Southeast University, Nanjing, China"
                ],
                "is_corresponding": false
            },
            {
                "name": "Quan Li",
                "email": "liquan@shanghaitech.edu.cn",
                "affiliations": [
                    "ShanghaiTech University, Shanghai, China",
                    "ShanghaiTech University, Shanghai, China"
                ],
                "is_corresponding": false
            }
        ],
        "abstract": "In healthcare, AI techniques are widely used for tasks like risk assessment and anomaly detection. Despite AI's potential as a valuable assistant, its role in complex medical data analysis often oversimplifies human-AI collaboration dynamics. To address this, we collaborated with a local hospital, engaging six physicians and one data scientist in a formative study. From this collaboration, we propose a framework integrating two-phase interactive visualization systems: one for Human-Led, AI-Assisted Retrospective Analysis and another for AI-Mediated, Human-Reviewed Iterative Modeling. This framework aims to enhance understanding and discussion around effective human-AI collaboration in healthcare.",
        "uid": "v-short-1089",
        "time_stamp": "",
        "time_start": "",
        "time_end": "",
        "paper_type": "short",
        "keywords": [],
        "doi": "",
        "fno": "",
        "presentation_mode": "",
        "open_access": false,
        "accessible_pdf": false,
        "has_image": false,
        "has_pdf": false,
        "paper_award": "",
        "image_caption": "",
        "external_paper_link": "",
        "youtube_ff_link": "",
        "youtube_ff_id": "",
        "bunny_ff_link": "",
        "bunny_ff_subtitles": "",
        "youtube_prerecorded_link": "",
        "youtube_prerecorded_id": "",
        "bunny_prerecorded_link": "",
        "bunny_prerecorded_subtitles": ""
    },
    "v-short-1090": {
        "slot_id": "v-short-1090",
        "session_id": "short0",
        "title": "Hypertrix: An indicatrix for high-dimensional visualizations",
        "contributors": [
            "Shivam Raval"
        ],
        "authors": [
            {
                "name": "Shivam Raval",
                "email": "sraval@g.harvard.edu",
                "affiliations": [
                    "Harvard University, Boston, United States"
                ],
                "is_corresponding": true
            },
            {
                "name": "Fernanda Viegas",
                "email": "viegas@google.com",
                "affiliations": [
                    "Harvard University, Cambridge, United States",
                    "Google Research, Cambridge, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Martin Wattenberg",
                "email": "wattenberg@gmail.com",
                "affiliations": [
                    "Harvard University, Cambridge, United States",
                    "Google Research, Cambridge, United States"
                ],
                "is_corresponding": false
            }
        ],
        "abstract": "Visualizing high dimensional data is challenging, since any dimensionality reduction technique will distort distances. A classic method in cartography\u2013Tissot\u2019s Indicatrix, specific to sphere-to-plane maps\u2013visualizes distortion using ellipses. Inspired by this idea, we describe the hypertrix: a method for representing distortions that occur when data is projected from arbitrarily high dimensions onto a 2D plane. We demonstrate our technique through synthetic and real-world datasets, and describe how this indicatrix can guide interpretations of nonlinear dimensionality reduction",
        "uid": "v-short-1090",
        "time_stamp": "",
        "time_start": "",
        "time_end": "",
        "paper_type": "short",
        "keywords": [],
        "doi": "",
        "fno": "",
        "presentation_mode": "",
        "open_access": false,
        "accessible_pdf": false,
        "has_image": false,
        "has_pdf": false,
        "paper_award": "",
        "image_caption": "",
        "external_paper_link": "",
        "youtube_ff_link": "",
        "youtube_ff_id": "",
        "bunny_ff_link": "",
        "bunny_ff_subtitles": "",
        "youtube_prerecorded_link": "",
        "youtube_prerecorded_id": "",
        "bunny_prerecorded_link": "",
        "bunny_prerecorded_subtitles": ""
    },
    "v-short-1096": {
        "slot_id": "v-short-1096",
        "session_id": "short0",
        "title": "Use-Coordination: Model, Grammar, and Library for Implementation of Coordinated Multiple Views",
        "contributors": [
            "Mark S Keller"
        ],
        "authors": [
            {
                "name": "Mark S Keller",
                "email": "mark_keller@hms.harvard.edu",
                "affiliations": [
                    "Harvard Medical School, Boston, United States"
                ],
                "is_corresponding": true
            },
            {
                "name": "Trevor Manz",
                "email": "trevor_manz@g.harvard.edu",
                "affiliations": [
                    "Harvard Medical School, Boston, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Nils Gehlenborg",
                "email": "nils@hms.harvard.edu",
                "affiliations": [
                    "Harvard Medical School, Boston, United States"
                ],
                "is_corresponding": false
            }
        ],
        "abstract": "Coordinated multiple views (CMV) in a visual analytics system can help users explore multiple data representations simultaneously with linked interactions. However, the implementation of coordinated multiple views can be challenging. Without standard software libraries, visualization designers need to re-implement CMV during the development of each system. We introduce use-coordination, a grammar and software library that supports the efficient implementation of CMV. The grammar defines a JSON-based representation for an abstract coordination model from the information visualization literature. We contribute an optional extension to the model and grammar that allows for hierarchical coordination. Through three use cases, we show that use-coordination enables implementation of CMV in systems containing not only basic statistical charts but also more complex visualizations such as medical imaging volumes. We describe six software extensions, including a graphical editor for manipulation of coordination, which showcase the potential to build upon our coordination-focused declarative approach.",
        "uid": "v-short-1096",
        "time_stamp": "",
        "time_start": "",
        "time_end": "",
        "paper_type": "short",
        "keywords": [],
        "doi": "",
        "fno": "",
        "presentation_mode": "",
        "open_access": false,
        "accessible_pdf": false,
        "has_image": false,
        "has_pdf": false,
        "paper_award": "",
        "image_caption": "",
        "external_paper_link": "",
        "youtube_ff_link": "",
        "youtube_ff_id": "",
        "bunny_ff_link": "",
        "bunny_ff_subtitles": "",
        "youtube_prerecorded_link": "",
        "youtube_prerecorded_id": "",
        "bunny_prerecorded_link": "",
        "bunny_prerecorded_subtitles": ""
    },
    "v-short-1097": {
        "slot_id": "v-short-1097",
        "session_id": "short0",
        "title": "Groot: An Interface for Editing and Configuring Automated Data Insights",
        "contributors": [
            "Sneha Gathani"
        ],
        "authors": [
            {
                "name": "Sneha Gathani",
                "email": "sgathani@cs.umd.edu",
                "affiliations": [
                    "University of Maryland, College Park, College Park, United States",
                    "Tableau Research, Seattle, United States"
                ],
                "is_corresponding": true
            },
            {
                "name": "Anamaria Crisan",
                "email": "amcrisan@uwaterloo.ca",
                "affiliations": [
                    "Tableau Research, Seattle, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Vidya Setlur",
                "email": "vsetlur@tableau.com",
                "affiliations": [
                    "Tableau Research, Palo Alto, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Arjun Srinivasan",
                "email": "arjun.srinivasan.10@gmail.com",
                "affiliations": [
                    "Tableau Research, Seattle, United States"
                ],
                "is_corresponding": false
            }
        ],
        "abstract": "Visualization tools now commonly present automated insights highlighting salient data patterns, including correlations, distributions, outliers, and differences, among others. While these insights are valuable for data exploration and chart interpretation, users currently only have a binary choice of accepting or rejecting them, lacking the flexibility to refine the system logic or customize the insight generation process. To address this limitation, we present GROOT, a prototype system that allows users to proactively specify and refine automated data insights. The system allows users to directly manipulate chart elements to receive insight recommendations based on their selections. Additionally, GROOT provides users with a manual editing interface to customize, reconfigure, or add new insights to individual charts and propagate them to future explorations. We describe a usage scenario to illustrate how these features collectively support insight editing and configuration, and discuss opportunities for future work including incorporating LLMs, improving semantic data and visualization search, and supporting insight management.",
        "uid": "v-short-1097",
        "time_stamp": "",
        "time_start": "",
        "time_end": "",
        "paper_type": "short",
        "keywords": [],
        "doi": "",
        "fno": "",
        "presentation_mode": "",
        "open_access": false,
        "accessible_pdf": false,
        "has_image": false,
        "has_pdf": false,
        "paper_award": "",
        "image_caption": "",
        "external_paper_link": "",
        "youtube_ff_link": "",
        "youtube_ff_id": "",
        "bunny_ff_link": "",
        "bunny_ff_subtitles": "",
        "youtube_prerecorded_link": "",
        "youtube_prerecorded_id": "",
        "bunny_prerecorded_link": "",
        "bunny_prerecorded_subtitles": ""
    },
    "v-short-1100": {
        "slot_id": "v-short-1100",
        "session_id": "short0",
        "title": "ConFides: A Visual Analytics Solution for Automated Speech Recognition Analysis and Exploration",
        "contributors": [
            "Sunwoo Ha"
        ],
        "authors": [
            {
                "name": "Sunwoo Ha",
                "email": "sha@wustl.edu",
                "affiliations": [
                    "Washington University in St. Louis, St. Louis, United States"
                ],
                "is_corresponding": true
            },
            {
                "name": "Chaehun Lim",
                "email": "chaelim@wustl.edu",
                "affiliations": [
                    "Washington University in St. Louis, St. Louis, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "R. Jordan Crouser",
                "email": "jcrouser@smith.edu",
                "affiliations": [
                    "Smith College, Northampton, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Alvitta Ottley",
                "email": "alvitta@wustl.edu",
                "affiliations": [
                    "Washington University in St. Louis, St. Louis, United States"
                ],
                "is_corresponding": false
            }
        ],
        "abstract": "Confidence scores of automatic speech recognition (ASR) outputs are often inadequately communicated, preventing its seamless integration into analytical workflows. In this paper, we introduce ConFides, a visual analytic system developed in collaboration with intelligence analysts to address this issue. ConFides aims to aid exploration and post-AI-transcription editing by visually representing the confidence associated with the transcription.  We demonstrate how our tool can assist intelligence analysts who use ASR outputs in their analytical and exploratory tasks and how it can help mitigate misinterpretation of crucial information. We also discuss opportunities for improving textual data cleaning and model transparency for human-machine collaboration.",
        "uid": "v-short-1100",
        "time_stamp": "",
        "time_start": "",
        "time_end": "",
        "paper_type": "short",
        "keywords": [],
        "doi": "",
        "fno": "",
        "presentation_mode": "",
        "open_access": false,
        "accessible_pdf": false,
        "has_image": false,
        "has_pdf": false,
        "paper_award": "",
        "image_caption": "",
        "external_paper_link": "",
        "youtube_ff_link": "",
        "youtube_ff_id": "",
        "bunny_ff_link": "",
        "bunny_ff_subtitles": "",
        "youtube_prerecorded_link": "",
        "youtube_prerecorded_id": "",
        "bunny_prerecorded_link": "",
        "bunny_prerecorded_subtitles": ""
    },
    "v-short-1101": {
        "slot_id": "v-short-1101",
        "session_id": "short0",
        "title": "What Color Scheme is More Effective in Assisting Readers to Locate Information in a Color-Coded Article?",
        "contributors": [
            "Ho Yin Ng"
        ],
        "authors": [
            {
                "name": "Ho Yin Ng",
                "email": "samnghoyin@gmail.com",
                "affiliations": [
                    "Pennsylvania State University, University Park, United States"
                ],
                "is_corresponding": true
            },
            {
                "name": "Zeyu He",
                "email": "zmh5268@psu.edu",
                "affiliations": [
                    "Pennsylvania State University, University Park, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Ting-Hao Kenneth Huang",
                "email": "txh710@psu.edu",
                "affiliations": [
                    "Pennsylvania State University, University Park , United States"
                ],
                "is_corresponding": false
            }
        ],
        "abstract": "Color coding, a technique assigning specific colors to different information types, has proven advantages in aiding human cognitive activities, especially reading and comprehension. The rise of Large Language Models (LLMs) has streamlined document coding, enabling simple automatic text labeling with various schemes. This has the potential to make color-coding more accessible and benefit more users. However, the importance of color choice, particularly in aiding textual information seeking through various color schemes, is not well studied. This paper presents a user study assessing the effectiveness of various color schemes generated by different base colors for readers' information-seeking performance in text documents color-coded by LLMs. Participants performed information-seeking tasks within scholarly papers' abstracts, each coded with a different scheme under time constraints. Results showed that non-analogous color schemes lead to better information-seeking performance, in both accuracy and response time. Yellow-inclusive color schemes lead to shorter response times and are also preferred by most participants. These could inform the better choice of color scheme for annotating text documents. As LLMs advance document coding, we advocate for more research focusing on the \"color\" aspect of color-coding techniques.",
        "uid": "v-short-1101",
        "time_stamp": "",
        "time_start": "",
        "time_end": "",
        "paper_type": "short",
        "keywords": [],
        "doi": "",
        "fno": "",
        "presentation_mode": "",
        "open_access": false,
        "accessible_pdf": false,
        "has_image": false,
        "has_pdf": false,
        "paper_award": "",
        "image_caption": "",
        "external_paper_link": "",
        "youtube_ff_link": "",
        "youtube_ff_id": "",
        "bunny_ff_link": "",
        "bunny_ff_subtitles": "",
        "youtube_prerecorded_link": "",
        "youtube_prerecorded_id": "",
        "bunny_prerecorded_link": "",
        "bunny_prerecorded_subtitles": ""
    },
    "v-short-1109": {
        "slot_id": "v-short-1109",
        "session_id": "short0",
        "title": "Connections Beyond Data: Exploring Homophily With Visualizations",
        "contributors": [
            "Poorna Talkad Sukumar"
        ],
        "authors": [
            {
                "name": "Poorna Talkad Sukumar",
                "email": "pt2393@nyu.edu",
                "affiliations": [
                    "New York University, Brooklyn, United States"
                ],
                "is_corresponding": true
            },
            {
                "name": "Maurizio Porfiri",
                "email": "mporfiri@nyu.edu",
                "affiliations": [
                    "New York University, Brooklyn, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Oded Nov",
                "email": "onov@nyu.edu",
                "affiliations": [
                    "New York University, New York, United States"
                ],
                "is_corresponding": false
            }
        ],
        "abstract": "Homophily refers to the tendency of individuals to associate with others who are similar to them in characteristics, such as, race, ethnicity, age, gender, or interests. In this paper, we investigate if individuals exhibit racial homophily when viewing visualizations, using mass shooting data in the United States as the example topic. We conducted a crowdsourced experiment (N=450) where each participant was shown a visualization displaying the counts of mass shooting victims, highlighting the counts for one of three racial groups (White, Black, or Hispanic). Participants were assigned to view visualizations highlighting their own race or a different race to assess the influence of racial concordance on changes in affect (emotion) and attitude towards gun control. While we did not find evidence of homophily, the results showed a significant negative shift in affect across all visualization conditions. Notably, political ideology significantly impacted changes in affect, with more liberal views correlating with a more negative affect change. Our findings underscore the complexity of reactions to mass shooting visualizations and highlight the need for additional measures for understanding homophily in visualizations.",
        "uid": "v-short-1109",
        "time_stamp": "",
        "time_start": "",
        "time_end": "",
        "paper_type": "short",
        "keywords": [],
        "doi": "",
        "fno": "",
        "presentation_mode": "",
        "open_access": false,
        "accessible_pdf": false,
        "has_image": false,
        "has_pdf": false,
        "paper_award": "",
        "image_caption": "",
        "external_paper_link": "",
        "youtube_ff_link": "",
        "youtube_ff_id": "",
        "bunny_ff_link": "",
        "bunny_ff_subtitles": "",
        "youtube_prerecorded_link": "",
        "youtube_prerecorded_id": "",
        "bunny_prerecorded_link": "",
        "bunny_prerecorded_subtitles": ""
    },
    "v-short-1114": {
        "slot_id": "v-short-1114",
        "session_id": "short0",
        "title": "The Comic Construction Kit: An Activity for Students to Learn and Explain Data Visualizations",
        "contributors": [
            "Magdalena Boucher"
        ],
        "authors": [
            {
                "name": "Magdalena Boucher",
                "email": "magdalena.boucher@fhstp.ac.at",
                "affiliations": [
                    "St. P\u00f6lten University of Applied Sciences, St. P\u00f6lten, Austria"
                ],
                "is_corresponding": true
            },
            {
                "name": "Christina Stoiber",
                "email": "christina.stoiber@fhstp.ac.at",
                "affiliations": [
                    "St. Poelten University of Applied Sciences, St. Poelten, Austria"
                ],
                "is_corresponding": false
            },
            {
                "name": "Mandy Keck",
                "email": "mandy.keck@fh-hagenberg.at",
                "affiliations": [
                    "School of Informatics, Communications and Media, Hagenberg im M\u00fchlkreis, Austria"
                ],
                "is_corresponding": false
            },
            {
                "name": "Victor Adriel de Jesus Oliveira",
                "email": "victor.oliveira@fhstp.ac.at",
                "affiliations": [
                    "St. Poelten University of Applied Sciences, St. Poelten, Austria"
                ],
                "is_corresponding": false
            },
            {
                "name": "Wolfgang Aigner",
                "email": "wolfgang.aigner@fhstp.ac.at",
                "affiliations": [
                    "St. Poelten University of Applied Sciences, St. Poelten, Austria"
                ],
                "is_corresponding": false
            }
        ],
        "abstract": "As visualization literacy and its implications gain prominence, we need effective methods to teach and prepare students for the variety of visualizations they might encounter in an increasingly data-driven world. Recently, the potential of comics has been recognized in various data visualization contexts, including educational settings. In this paper, we describe the development of a workshop in which we use our \u201ccomic construction kit\u201d as a tool for students to understand various data visualization techniques through an interactive creative approach of creating explanatory comics. We report on our insights and learnings from holding eight workshops with high school students, high school teachers, university students, and university lecturers, aiming to enhance the landscape of hands-on visualization activities that can enrich the visualization classroom. The comic construction kit and all supplemental materials are open source under a CC-BY license and available at https://fhstp.github.io/comixplain/vis4schools.html.",
        "uid": "v-short-1114",
        "time_stamp": "",
        "time_start": "",
        "time_end": "",
        "paper_type": "short",
        "keywords": [],
        "doi": "",
        "fno": "",
        "presentation_mode": "",
        "open_access": false,
        "accessible_pdf": false,
        "has_image": false,
        "has_pdf": false,
        "paper_award": "",
        "image_caption": "",
        "external_paper_link": "",
        "youtube_ff_link": "",
        "youtube_ff_id": "",
        "bunny_ff_link": "",
        "bunny_ff_subtitles": "",
        "youtube_prerecorded_link": "",
        "youtube_prerecorded_id": "",
        "bunny_prerecorded_link": "",
        "bunny_prerecorded_subtitles": ""
    },
    "v-short-1116": {
        "slot_id": "v-short-1116",
        "session_id": "short0",
        "title": "Science in a Blink: Supporting Ensemble Perception in Scalar Fields",
        "contributors": [
            "Khairi Reda"
        ],
        "authors": [
            {
                "name": "Victor A. Mateevitsi",
                "email": "vmateevitsi@anl.gov",
                "affiliations": [
                    "Argonne National Laboratory, Lemont, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Michael E. Papka",
                "email": "papka@anl.gov",
                "affiliations": [
                    "Argonne National Laboratory, Lemont, United States",
                    "University of Illinois Chicago, Chicago, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Khairi Reda",
                "email": "redak@iu.edu",
                "affiliations": [
                    "Indiana University, Indianapolis, United States"
                ],
                "is_corresponding": true
            }
        ],
        "abstract": "Visualizations support rapid analysis of scientific datasets, allowing viewers to glean aggregate information (e.g., the mean) within split-seconds. While prior research has explored this ability in conventional charts, it is unclear if spatial visualizations used by computational scientists afford a similar ensemble perception capacity. We investigate people's ability to estimate two summary statistics, mean and variance, from pseudocolor scalar fields. In a crowdsourced experiment, we find that participants can reliably characterize both statistics, although variance discrimination requires a much stronger signal. Multi-hue and diverging colormaps outperformed monochromatic, luminance ramps in aiding this extraction. Analysis of qualitative responses suggests that participants often estimate the distribution of hotspots and valleys as visual proxies for data statistics. These findings suggest that people's summary interpretation of spatial datasets is likely driven by the appearance of discrete color segments, rather than assessments of overall luminance. Implicit color segmentation in quantitative displays could thus prove more useful than previously assumed by facilitating quick, gist-level judgments about color-coded visualizations.",
        "uid": "v-short-1116",
        "time_stamp": "",
        "time_start": "",
        "time_end": "",
        "paper_type": "short",
        "keywords": [],
        "doi": "",
        "fno": "",
        "presentation_mode": "",
        "open_access": false,
        "accessible_pdf": false,
        "has_image": false,
        "has_pdf": false,
        "paper_award": "",
        "image_caption": "",
        "external_paper_link": "",
        "youtube_ff_link": "",
        "youtube_ff_id": "",
        "bunny_ff_link": "",
        "bunny_ff_subtitles": "",
        "youtube_prerecorded_link": "",
        "youtube_prerecorded_id": "",
        "bunny_prerecorded_link": "",
        "bunny_prerecorded_subtitles": ""
    },
    "v-short-1117": {
        "slot_id": "v-short-1117",
        "session_id": "short0",
        "title": "AltGeoViz: Facilitating Accessible Geovisualization",
        "contributors": [
            "Chu Li"
        ],
        "authors": [
            {
                "name": "Chu Li",
                "email": "chuchuli@cs.washington.edu",
                "affiliations": [
                    "University of Washington, Seattle, United States"
                ],
                "is_corresponding": true
            },
            {
                "name": "Rock Yuren Pang",
                "email": "ypang2@cs.washington.edu",
                "affiliations": [
                    "University of Washington, Seattle, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Ather Sharif",
                "email": "asharif@cs.washington.edu",
                "affiliations": [
                    "University of Washington, Seattle, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Arnavi Chheda-Kothary",
                "email": "chheda@cs.washington.edu",
                "affiliations": [
                    "University of Washington, Seattle, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Jeffrey Heer",
                "email": "jheer@uw.edu",
                "affiliations": [
                    "University of Washington, Seattle, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Jon E. Froehlich",
                "email": "jonf@cs.uw.edu",
                "affiliations": [
                    "University of Washington, Seattle, United States"
                ],
                "is_corresponding": false
            }
        ],
        "abstract": "Geovisualizations are powerful tools for exploratory spatial analysis, enabling sighted users to discern patterns, trends, and relationships within geographic data. However, these visual tools have remained largely inaccessible to screen-reader users. We present AltGeoViz, a new system we designed to facilitate geovisualization exploration for these users. AltGeoViz dynamically generates alt-text descriptions based on the user's current map view, providing summaries of spatial patterns and descriptive statistics.  In a study of five screen-reader users, we found that AltGeoViz enabled them to interact with geovisualizations in previously infeasible ways. Participants demonstrated a clear understanding of data summaries and their location context, and they could synthesize spatial understandings of their explorations. Moreover, we identified key areas for improvement, such as the addition of intuitive spatial navigation controls and comparative analysis features.",
        "uid": "v-short-1117",
        "time_stamp": "",
        "time_start": "",
        "time_end": "",
        "paper_type": "short",
        "keywords": [],
        "doi": "",
        "fno": "",
        "presentation_mode": "",
        "open_access": false,
        "accessible_pdf": false,
        "has_image": false,
        "has_pdf": false,
        "paper_award": "",
        "image_caption": "",
        "external_paper_link": "",
        "youtube_ff_link": "",
        "youtube_ff_id": "",
        "bunny_ff_link": "",
        "bunny_ff_subtitles": "",
        "youtube_prerecorded_link": "",
        "youtube_prerecorded_id": "",
        "bunny_prerecorded_link": "",
        "bunny_prerecorded_subtitles": ""
    },
    "v-short-1119": {
        "slot_id": "v-short-1119",
        "session_id": "short0",
        "title": "Visualization of 2D Scalar Field Ensembles Using Volume Visualization of the Empirical Distribution Function",
        "contributors": [
            "Tomas Rodolfo Daetz Chacon"
        ],
        "authors": [
            {
                "name": "Tomas Rodolfo Daetz Chacon",
                "email": "daetz@informatik.uni-leipzig.de",
                "affiliations": [
                    "Institute of Computer Science, Leipzig University, Leipzig, Germany"
                ],
                "is_corresponding": true
            },
            {
                "name": "Michael B\u00f6ttinger",
                "email": "boettinger@dkrz.de",
                "affiliations": [
                    "German Climate Computing Center (DKRZ), Hamburg, Germany"
                ],
                "is_corresponding": false
            },
            {
                "name": "Gerik Scheuermann",
                "email": "scheuermann@informatik.uni-leipzig.de",
                "affiliations": [
                    "Leipzig University, Leipzig, Germany"
                ],
                "is_corresponding": false
            },
            {
                "name": "Christian Heine",
                "email": "heine@informatik.uni-leipzig.de",
                "affiliations": [
                    "Leipzig University, Leipzig, Germany"
                ],
                "is_corresponding": false
            }
        ],
        "abstract": "Analyzing uncertainty in spatial data is a vital task in many domains, as for example with climate and weather simulation ensembles. Although there are many methods to support the analysis of the uncertainty, such as uncertain isocontours or calculation of statistical values, it is still a challenge to get an overview of the uncertainty and then decide a further method or parameter to analyze the data, or investigate further some region or point of interest. We present cumulative height fields, a visualization method for 2D scalar field ensembles using the marginal empirical distribution function and show preliminary results using volume rendering and slicing for the Max Planck Institute Grand Ensemble.",
        "uid": "v-short-1119",
        "time_stamp": "",
        "time_start": "",
        "time_end": "",
        "paper_type": "short",
        "keywords": [],
        "doi": "",
        "fno": "",
        "presentation_mode": "",
        "open_access": false,
        "accessible_pdf": false,
        "has_image": false,
        "has_pdf": false,
        "paper_award": "",
        "image_caption": "",
        "external_paper_link": "",
        "youtube_ff_link": "",
        "youtube_ff_id": "",
        "bunny_ff_link": "",
        "bunny_ff_subtitles": "",
        "youtube_prerecorded_link": "",
        "youtube_prerecorded_id": "",
        "bunny_prerecorded_link": "",
        "bunny_prerecorded_subtitles": ""
    },
    "v-short-1121": {
        "slot_id": "v-short-1121",
        "session_id": "short0",
        "title": "Improving Property Graph Layouts by Leveraging Attribute Similarity for Structurally Equivalent Nodes",
        "contributors": [
            "Patrick Mackey"
        ],
        "authors": [
            {
                "name": "Patrick Mackey",
                "email": "patrick.mackey@pnnl.gov",
                "affiliations": [
                    "Pacific Northwest National Lab, Richland, United States"
                ],
                "is_corresponding": true
            },
            {
                "name": "Jacob Miller",
                "email": "jacobmiller1@arizona.edu",
                "affiliations": [
                    "University of Arizona, Tucson, United States",
                    "Pacific Northwest National Laboratory, Richland, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Liz Faultersack",
                "email": "liz.f@pnnl.gov",
                "affiliations": [
                    "Pacific Northwest National Laboratory, Richland, United States"
                ],
                "is_corresponding": false
            }
        ],
        "abstract": "Many real-world networks contain structurally-equivalent nodes. These are defined as vertices that share the same set of neighboring nodes, making them interchangeable with a traditional graph layout approach. However, many real-world graphs also have properties associated with nodes, adding additional meaning to them. We present an approach for swapping locations of structurally-equivalent nodes in graph layout so that those with more similar properties have closer proximity to each other. This improves the usefulness of the visualization from an attribute perspective without negatively impacting the visualization from a structural perspective. We include an algorithm for finding these sets of nodes in linear time, as well as methodologies for ordering nodes based on their attribute similarity, which works for scalar, ordinal, multidimensional, and categorical data.",
        "uid": "v-short-1121",
        "time_stamp": "",
        "time_start": "",
        "time_end": "",
        "paper_type": "short",
        "keywords": [],
        "doi": "",
        "fno": "",
        "presentation_mode": "",
        "open_access": false,
        "accessible_pdf": false,
        "has_image": false,
        "has_pdf": false,
        "paper_award": "",
        "image_caption": "",
        "external_paper_link": "",
        "youtube_ff_link": "",
        "youtube_ff_id": "",
        "bunny_ff_link": "",
        "bunny_ff_subtitles": "",
        "youtube_prerecorded_link": "",
        "youtube_prerecorded_id": "",
        "bunny_prerecorded_link": "",
        "bunny_prerecorded_subtitles": ""
    },
    "v-short-1126": {
        "slot_id": "v-short-1126",
        "session_id": "short0",
        "title": "FAVis: Visual Analytics of Factor Analysis for Psychological Research",
        "contributors": [
            "Yikai Lu"
        ],
        "authors": [
            {
                "name": "Yikai Lu",
                "email": "ylu22@nd.edu",
                "affiliations": [
                    "University of Notre Dame, Notre Dame, United States",
                    "University of Notre Dame, Notre Dame, United States"
                ],
                "is_corresponding": true
            },
            {
                "name": "Chaoli Wang",
                "email": "chaoli.wang@nd.edu",
                "affiliations": [
                    "University of Notre Dame, Notre Dame, United States"
                ],
                "is_corresponding": false
            }
        ],
        "abstract": "Psychological research often involves understanding psychological constructs through conducting factor analysis on data collected by a questionnaire, which can comprise hundreds of questions. Without interactive systems for interpreting factor models, researchers are frequently exposed to subjectivity, potentially leading to misinterpretations or overlooked crucial information. This paper introduces FAVis, a novel interactive visualization tool designed to aid researchers in interpreting and evaluating factor analysis results. FAVis enhances the understanding of relationships between variables and factors by supporting multiple views for visualizing factor loadings and correlations, allowing users to analyze information from various perspectives. The primary feature of FAVis is to enable users to set optimal thresholds for factor loadings to balance clarity and information retention. FAVis also allows users to assign tags to variables, enhancing the understanding of factors by linking them to their associated psychological constructs. We conduct a case study on a dataset from the Motivational State Questionnaire, utilizing a three-factor common factor model. Our user study demonstrates the utility of FAVis in various tasks.",
        "uid": "v-short-1126",
        "time_stamp": "",
        "time_start": "",
        "time_end": "",
        "paper_type": "short",
        "keywords": [],
        "doi": "",
        "fno": "",
        "presentation_mode": "",
        "open_access": false,
        "accessible_pdf": false,
        "has_image": false,
        "has_pdf": false,
        "paper_award": "",
        "image_caption": "",
        "external_paper_link": "",
        "youtube_ff_link": "",
        "youtube_ff_id": "",
        "bunny_ff_link": "",
        "bunny_ff_subtitles": "",
        "youtube_prerecorded_link": "",
        "youtube_prerecorded_id": "",
        "bunny_prerecorded_link": "",
        "bunny_prerecorded_subtitles": ""
    },
    "v-short-1127": {
        "slot_id": "v-short-1127",
        "session_id": "short0",
        "title": "Investigating the Apple Vision Pro Spatial Computing Platform for GPU-Based Volume Visualization",
        "contributors": [
            "Camilla Hrycak"
        ],
        "authors": [
            {
                "name": "Camilla Hrycak",
                "email": "camilla.hrycak@uni-due.de",
                "affiliations": [
                    "University of Duisburg-Essen, Duisburg, Germany"
                ],
                "is_corresponding": true
            },
            {
                "name": "David Lewakis",
                "email": "david.lewakis@stud.uni-due.de",
                "affiliations": [
                    "University of Duisburg-Essen, Duisburg, Germany"
                ],
                "is_corresponding": false
            },
            {
                "name": "Jens Harald Krueger",
                "email": "jens.krueger@uni-due.de",
                "affiliations": [
                    "University of Duisburg-Essen, Duisburg, Germany"
                ],
                "is_corresponding": false
            }
        ],
        "abstract": "In this paper, we analyze the Apple Vision Pro hardware and the visionOS software platform, assessing their capabilities for volume rendering of structured grids, a prevalent technique across various applications. The Apple Vision Pro supports multiple display modes, from classical augmented reality (AR) using video see-through technology to immersive virtual reality (VR) environments that exclusively render virtual objects. These modes utilize different APIs and exhibit distinct capabilities. Our focus is on direct volume rendering, selected for its implementation challenges due to the native graphics APIs being predominantly oriented towards surface shading. Volume rendering is particularly vital in fields where AR and VR visualizations offer substantial benefits, such as in medicine and manufacturing. Despite its initial high cost, we anticipate that the Vision Pro will become more accessible and affordable over time, following Apple's track record of market expansion. As these devices become more prevalent, understanding how to effectively program and utilize them becomes increasingly important, offering significant opportunities for innovation and practical applications in various sectors.",
        "uid": "v-short-1127",
        "time_stamp": "",
        "time_start": "",
        "time_end": "",
        "paper_type": "short",
        "keywords": [],
        "doi": "",
        "fno": "",
        "presentation_mode": "",
        "open_access": false,
        "accessible_pdf": false,
        "has_image": false,
        "has_pdf": false,
        "paper_award": "",
        "image_caption": "",
        "external_paper_link": "",
        "youtube_ff_link": "",
        "youtube_ff_id": "",
        "bunny_ff_link": "",
        "bunny_ff_subtitles": "",
        "youtube_prerecorded_link": "",
        "youtube_prerecorded_id": "",
        "bunny_prerecorded_link": "",
        "bunny_prerecorded_subtitles": ""
    },
    "v-short-1130": {
        "slot_id": "v-short-1130",
        "session_id": "short0",
        "title": "DaVE - A Curated Database of Visualization Examples",
        "contributors": [
            "Jens Koenen"
        ],
        "authors": [
            {
                "name": "Jens Koenen",
                "email": "koenen@informatik.rwth-aachen.de",
                "affiliations": [
                    "RWTH Aachen University, Aachen, Germany"
                ],
                "is_corresponding": true
            },
            {
                "name": "Marvin Petersen",
                "email": "m.petersen@rptu.de",
                "affiliations": [
                    "RPTU Kaiserslautern-Landau, Kaiserslautern, Germany"
                ],
                "is_corresponding": false
            },
            {
                "name": "Christoph Garth",
                "email": "garth@rptu.de",
                "affiliations": [
                    "RPTU Kaiserslautern-Landau, Kaiserslautern, Germany"
                ],
                "is_corresponding": false
            },
            {
                "name": "Tim Gerrits",
                "email": "gerrits@vis.rwth-aachen.de",
                "affiliations": [
                    "RWTH Aachen University, Aachen, Germany"
                ],
                "is_corresponding": false
            }
        ],
        "abstract": "Visualization, from simple line plots to complex high-dimensional visual analysis systems, has established itself throughout numerous domains to explore, analyze, and evaluate data. Applying such visualizations in the context of simulation science where High-Performance Computing (HPC) produces ever-growing amounts of data that is more complex, potentially multidimensional, and multi-modal, takes up resources and a high level of technological experience often not available to domain experts. In this work, we present DaVE - a curated database of visualization examples, which aims to provide state-of-the-art and advanced visualization methods that arise in the context of HPC applications. Based on domain- or data-specific descriptors entered by the user, DaVE provides a list of appropriate visualization techniques, each accompanied by descriptions, examples, references, and resources. Sample code, adaptable container templates, and recipes for easy integration in HPC applications can be downloaded for easy access to high-fidelity visualizations. While the database is currently filled with a limited number of entries based on a broad evaluation of needs and challenges of current HPC users, DaVE is designed to be easily extended by experts from both the visualization and HPC communities.",
        "uid": "v-short-1130",
        "time_stamp": "",
        "time_start": "",
        "time_end": "",
        "paper_type": "short",
        "keywords": [],
        "doi": "",
        "fno": "",
        "presentation_mode": "",
        "open_access": false,
        "accessible_pdf": false,
        "has_image": false,
        "has_pdf": false,
        "paper_award": "",
        "image_caption": "",
        "external_paper_link": "",
        "youtube_ff_link": "",
        "youtube_ff_id": "",
        "bunny_ff_link": "",
        "bunny_ff_subtitles": "",
        "youtube_prerecorded_link": "",
        "youtube_prerecorded_id": "",
        "bunny_prerecorded_link": "",
        "bunny_prerecorded_subtitles": ""
    },
    "v-short-1135": {
        "slot_id": "v-short-1135",
        "session_id": "short0",
        "title": "Feature Clock: High-Dimensional Effects in Two-Dimensional Plots",
        "contributors": [
            "Olga Ovcharenko"
        ],
        "authors": [
            {
                "name": "Olga Ovcharenko",
                "email": "ovcharenko.folga@gmail.com",
                "affiliations": [
                    "ETH Z\u00fcrich, Z\u00fcrich, Switzerland"
                ],
                "is_corresponding": true
            },
            {
                "name": "Rita Sevastjanova",
                "email": "rita.sevastjanova@uni-konstanz.de",
                "affiliations": [
                    "ETH Z\u00fcrich, Z\u00fcrich, Switzerland"
                ],
                "is_corresponding": false
            },
            {
                "name": "Valentina Boeva",
                "email": "valentina.boeva@inf.ethz.ch",
                "affiliations": [
                    "ETH Zurich, Z\u00fcrich, Switzerland"
                ],
                "is_corresponding": false
            }
        ],
        "abstract": "Humans struggle to perceive and interpret high-dimensional data. Therefore, high-dimensional data are often projected into two dimensions for visualization. Many applications benefit from complex nonlinear dimensionality reduction techniques, but the effects of individual high-dimensional features are hard to explain in the two-dimensional space. Most visualization solutions use multiple two-dimensional plots, each showing the effect of one high-dimensional feature in two dimensions; this approach creates a need for a visual inspection of k plots for a k-dimensional input space. Our solution, Feature Clock, provides a novel approach that eliminates the need to inspect these k plots to grasp the influence of original features on the data structure depicted in two dimensions. Feature Clock enhances the explainability and compactness of visualizations of embedded data and is available in an open-source Python library.",
        "uid": "v-short-1135",
        "time_stamp": "",
        "time_start": "",
        "time_end": "",
        "paper_type": "short",
        "keywords": [],
        "doi": "",
        "fno": "",
        "presentation_mode": "",
        "open_access": false,
        "accessible_pdf": false,
        "has_image": false,
        "has_pdf": false,
        "paper_award": "",
        "image_caption": "",
        "external_paper_link": "",
        "youtube_ff_link": "",
        "youtube_ff_id": "",
        "bunny_ff_link": "",
        "bunny_ff_subtitles": "",
        "youtube_prerecorded_link": "",
        "youtube_prerecorded_id": "",
        "bunny_prerecorded_link": "",
        "bunny_prerecorded_subtitles": ""
    },
    "v-short-1144": {
        "slot_id": "v-short-1144",
        "session_id": "short0",
        "title": "Opening the black box of 3D reconstruction error analysis with VECTOR",
        "contributors": [
            "Mauricio Hess-Flores"
        ],
        "authors": [
            {
                "name": "Racquel Fygenson",
                "email": "racquel.fygenson@gmail.com",
                "affiliations": [
                    "Northeastern University, Boston, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Kazi Jawad",
                "email": "kjawad@andrew.cmu.edu",
                "affiliations": [
                    "Weta FX, Auckland, New Zealand"
                ],
                "is_corresponding": false
            },
            {
                "name": "Zongzhan Li",
                "email": "zongzhanisabelli@gmail.com",
                "affiliations": [
                    "Art Center, Pasadena, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Francois Ayoub",
                "email": "francois.ayoub@jpl.nasa.gov",
                "affiliations": [
                    "California Institute of Technology, Pasadena, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Robert G Deen",
                "email": "bob.deen@jpl.nasa.gov",
                "affiliations": [
                    "California Institute of Technology, Pasadena, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Scott Davidoff",
                "email": "sd@scottdavidoff.com",
                "affiliations": [
                    "California Institute of Technology, Pasadena, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Dominik Moritz",
                "email": "domoritz@cmu.edu",
                "affiliations": [
                    "Carnegie Mellon University, Pittsburgh, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Mauricio Hess-Flores",
                "email": "mauricio.a.hess.flores@jpl.nasa.gov",
                "affiliations": [
                    "NASA-JPL, Pasadena, United States"
                ],
                "is_corresponding": true
            }
        ],
        "abstract": "Reconstruction of 3D scenes from 2D images is a technical challenge that impacts domains from Earth and planetary sciences and space exploration to augmented and virtual reality. Typically, reconstruction algorithms first identify common features across images and then minimize reconstruction errors after estimating the shape of the terrain. This bundle adjustment (BA) step optimizes around a single, simplifying scalar value that obfuscates many possible causes of reconstruction errors (e.g., initial estimate of the position and orientation of the camera, lighting conditions, ease of feature detection in the terrain). Reconstruction errors can lead to inaccurate scientific inferences or endanger a spacecraft exploring a remote environment. To address this challenge, we present VECTOR, a visual analysis tool that improves error inspection for stereo reconstruction BA. VECTOR provides analysts with previously unavailable visibility into feature locations, camera pose, and computed 3D points. VECTOR was developed in partnership with the Perseverance Mars Rover and Ingenuity Mars Helicopter terrain reconstruction team at the NASA Jet Propulsion Laboratory. We report on how this tool was used to debug and improve terrain reconstruction for the Mars 2020 mission.",
        "uid": "v-short-1144",
        "time_stamp": "",
        "time_start": "",
        "time_end": "",
        "paper_type": "short",
        "keywords": [],
        "doi": "",
        "fno": "",
        "presentation_mode": "",
        "open_access": false,
        "accessible_pdf": false,
        "has_image": false,
        "has_pdf": false,
        "paper_award": "",
        "image_caption": "",
        "external_paper_link": "",
        "youtube_ff_link": "",
        "youtube_ff_id": "",
        "bunny_ff_link": "",
        "bunny_ff_subtitles": "",
        "youtube_prerecorded_link": "",
        "youtube_prerecorded_id": "",
        "bunny_prerecorded_link": "",
        "bunny_prerecorded_subtitles": ""
    },
    "v-short-1146": {
        "slot_id": "v-short-1146",
        "session_id": "short0",
        "title": "Visualizations on Smart Watches while Running: It Actually Helps!",
        "contributors": [
            "Charles Perin"
        ],
        "authors": [
            {
                "name": "Sarina Kashanj",
                "email": "sarinaksj@uvic.ca",
                "affiliations": [
                    "University of Victoria, Victoria, Canada"
                ],
                "is_corresponding": false
            },
            {
                "name": "Xiyao Wang",
                "email": "xiyao.wang23@gmail.com",
                "affiliations": [
                    "University of Victoria, Victoira, Canada",
                    "Delft University of Technology, Delft, Netherlands"
                ],
                "is_corresponding": false
            },
            {
                "name": "Charles Perin",
                "email": "cperin@uvic.ca",
                "affiliations": [
                    "University of Victoria, Victoria, Canada"
                ],
                "is_corresponding": true
            }
        ],
        "abstract": "Millions of runners rely on smart watches that display running-related metrics such as pace, heart rate and distance for training and racing -- mostly with text and numbers. Although research tells us that visualizations are a good alternative to text on smart watches, we know little about how visualizations can help in realistic running scenarios. We conducted a study in which 20 runners completed running-related tasks on an outdoor track using both text and visualizations. Our results show that runners are 1.5 to 8 times faster in completing those tasks with visualizations than with text, prefer visualizations to text, and would use such visualizations while running -- were they available on their smart watch.",
        "uid": "v-short-1146",
        "time_stamp": "",
        "time_start": "",
        "time_end": "",
        "paper_type": "short",
        "keywords": [],
        "doi": "",
        "fno": "",
        "presentation_mode": "",
        "open_access": false,
        "accessible_pdf": false,
        "has_image": false,
        "has_pdf": false,
        "paper_award": "",
        "image_caption": "",
        "external_paper_link": "",
        "youtube_ff_link": "",
        "youtube_ff_id": "",
        "bunny_ff_link": "",
        "bunny_ff_subtitles": "",
        "youtube_prerecorded_link": "",
        "youtube_prerecorded_id": "",
        "bunny_prerecorded_link": "",
        "bunny_prerecorded_subtitles": ""
    },
    "v-short-1150": {
        "slot_id": "v-short-1150",
        "session_id": "short0",
        "title": "PyGWalker: On-the-fly Assistant for Exploratory Visual Data Analysis",
        "contributors": [
            "Yue Yu"
        ],
        "authors": [
            {
                "name": "Yue Yu",
                "email": "yue.yu@connect.ust.hk",
                "affiliations": [
                    "The Hong Kong University of Science and Technology, Hong Kong, China",
                    "Kanaries Data Inc., Hangzhou, China"
                ],
                "is_corresponding": true
            },
            {
                "name": "Leixian Shen",
                "email": "lshenaj@connect.ust.hk",
                "affiliations": [
                    "The Hong Kong University of Science and Technology, Hong Kong, China"
                ],
                "is_corresponding": false
            },
            {
                "name": "Fei Long",
                "email": "feilong@kanaries.net",
                "affiliations": [
                    "Kanaries Data Inc., Hangzhou, China"
                ],
                "is_corresponding": false
            },
            {
                "name": "Huamin Qu",
                "email": "huamin@cse.ust.hk",
                "affiliations": [
                    "The Hong Kong University of Science and Technology, Hong Kong, China"
                ],
                "is_corresponding": false
            },
            {
                "name": "Hao Chen",
                "email": "haochen@kanaries.net",
                "affiliations": [
                    "Kanaries Data Inc., Hangzhou, China"
                ],
                "is_corresponding": false
            }
        ],
        "abstract": "Exploratory visual data analysis tools empower data analysts to efficiently and intuitively explore data insights throughout the entire analysis cycle. However, the gap between common programmatic analysis (e.g., within computational notebooks) and exploratory visual analysis leads to a disjointed and inefficient data analysis experience. To bridge this gap, we developed PyGWalker, a Python library that offers on-the-fly assistance for exploratory visual data analysis. It features a lightweight and intuitive GUI with a shelf builder modality. Its loosely coupled architecture supports multiple computational environments to accommodate varying data sizes. Since its release in February 2023, PyGWalker has gained much attention, with 468k downloads on PyPI and over 9.8k stars on GitHub as of April 2024. This demonstrates its value to the data science and visualization community, with researchers and developers integrating it into their own applications and studies.",
        "uid": "v-short-1150",
        "time_stamp": "",
        "time_start": "",
        "time_end": "",
        "paper_type": "short",
        "keywords": [],
        "doi": "",
        "fno": "",
        "presentation_mode": "",
        "open_access": false,
        "accessible_pdf": false,
        "has_image": false,
        "has_pdf": false,
        "paper_award": "",
        "image_caption": "",
        "external_paper_link": "",
        "youtube_ff_link": "",
        "youtube_ff_id": "",
        "bunny_ff_link": "",
        "bunny_ff_subtitles": "",
        "youtube_prerecorded_link": "",
        "youtube_prerecorded_id": "",
        "bunny_prerecorded_link": "",
        "bunny_prerecorded_subtitles": ""
    },
    "v-short-1155": {
        "slot_id": "v-short-1155",
        "session_id": "short0",
        "title": "Active Appearance and Spatial Variation Can Improve Visibility in Area Labels for Augmented Reality",
        "contributors": [
            "James Tompkin"
        ],
        "authors": [
            {
                "name": "Hojung Kwon",
                "email": "hojung_kwon@brown.edu",
                "affiliations": [
                    "Brown University, Providence, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Yuanbo Li",
                "email": "yuanbo_li@brown.edu",
                "affiliations": [
                    "Brown University, Providence, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Xiaohan Ye",
                "email": "chloe_ye2019@hotmail.com",
                "affiliations": [
                    "Brown University, Providence, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Praccho Muna-McQuay",
                "email": "praccho_muna-mcquay@brown.edu",
                "affiliations": [
                    "Brown University, Providence, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Liuren Yin",
                "email": "liuren.yin@duke.edu",
                "affiliations": [
                    "Duke University, Durham, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "James Tompkin",
                "email": "james_tompkin@brown.edu",
                "affiliations": [
                    "Brown University, Providence, United States"
                ],
                "is_corresponding": true
            }
        ],
        "abstract": "Augmented reality (AR) area labels can highlight real-life objects, visualize real world regions with arbitrary boundaries, and show invisible objects or features. Environment conditions such as lighting and clutter can decrease fixed or passive label visibility, and labels that have high opacity levels can occlude crucial details in the environment. We design and evaluate active AR area label visualization modes to enhance visibility across real-life environments, while still retaining environment details within the label. For this, we define a distant characteristic color from the environment in perceptual CIELAB space, then introduce spatial variations among label pixel colors based on the underlying environment variation. In a user study with 18 participants, we discovered that our active label visualization modes can be comparable in visibility to a fixed green baseline by Gabbard et al., and can outperform it with added spatial variation in cluttered environments, across varying levels of lighting (e.g., nighttime), and in environments with colors similar to the fixed baseline color.",
        "uid": "v-short-1155",
        "time_stamp": "",
        "time_start": "",
        "time_end": "",
        "paper_type": "short",
        "keywords": [],
        "doi": "",
        "fno": "",
        "presentation_mode": "",
        "open_access": false,
        "accessible_pdf": false,
        "has_image": false,
        "has_pdf": false,
        "paper_award": "",
        "image_caption": "",
        "external_paper_link": "",
        "youtube_ff_link": "",
        "youtube_ff_id": "",
        "bunny_ff_link": "",
        "bunny_ff_subtitles": "",
        "youtube_prerecorded_link": "",
        "youtube_prerecorded_id": "",
        "bunny_prerecorded_link": "",
        "bunny_prerecorded_subtitles": ""
    },
    "v-short-1156": {
        "slot_id": "v-short-1156",
        "session_id": "short0",
        "title": "An Overview + Detail Layout for Visualizing Compound Graphs",
        "contributors": [
            "Chang Han"
        ],
        "authors": [
            {
                "name": "Chang Han",
                "email": "hatch.on27@gmail.com",
                "affiliations": [
                    "University of Utah, Salt Lake City, United States"
                ],
                "is_corresponding": true
            },
            {
                "name": "Justin Lieffers",
                "email": "lieffers@arizona.edu",
                "affiliations": [
                    "University of Arizona, Tucson, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Clayton Morrison",
                "email": "claytonm@arizona.edu",
                "affiliations": [
                    "University of Arizona, Tucson, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Katherine E. Isaacs",
                "email": "kisaacs@sci.utah.edu",
                "affiliations": [
                    "The University of Utah, Salt Lake City, United States"
                ],
                "is_corresponding": false
            }
        ],
        "abstract": "Compound graphs are networks in which vertices can be grouped into larger subsets, with these subsets capable of further grouping, resulting in a nesting that can be many levels deep. Such graphs arise in several applications including biological workflows, chemical equations, and computational data flow analysis. Common layouts prioritize the lowest level of the grouping, down to the individual ungrouped vertices, which can make the higher level grouped structures more difficult to discern, especially in deeply nested networks. We contribute an overview+detail layout that preserves the saliency of the higher level network structure when groups are expanded to show internal nested structure. Our layout draws inner structures adjacent to their parents, using a modified tree layout to place substructures. We describe our algorithm and then present case studies demonstrating the layout's utility to a domain expert working on data flow analysis. Finally, we discuss network parameters and analysis situations in which our layout is well suited.",
        "uid": "v-short-1156",
        "time_stamp": "",
        "time_start": "",
        "time_end": "",
        "paper_type": "short",
        "keywords": [],
        "doi": "",
        "fno": "",
        "presentation_mode": "",
        "open_access": false,
        "accessible_pdf": false,
        "has_image": false,
        "has_pdf": false,
        "paper_award": "",
        "image_caption": "",
        "external_paper_link": "",
        "youtube_ff_link": "",
        "youtube_ff_id": "",
        "bunny_ff_link": "",
        "bunny_ff_subtitles": "",
        "youtube_prerecorded_link": "",
        "youtube_prerecorded_id": "",
        "bunny_prerecorded_link": "",
        "bunny_prerecorded_subtitles": ""
    },
    "v-short-1159": {
        "slot_id": "v-short-1159",
        "session_id": "short0",
        "title": "Micro Visualizations on a Smartwatch: Assessing Reading Performance While Walking",
        "contributors": [
            "Fairouz Grioui"
        ],
        "authors": [
            {
                "name": "Fairouz Grioui",
                "email": "fairouz.grioui@vis.uni-stuttgart.de",
                "affiliations": [
                    "University of Stuttgart, Stuttgart, Germany"
                ],
                "is_corresponding": true
            },
            {
                "name": "Tanja Blascheck",
                "email": "research@blascheck.eu",
                "affiliations": [
                    "University of Stuttgart, Stuttgart, Germany"
                ],
                "is_corresponding": false
            },
            {
                "name": "Lijie Yao",
                "email": "yaolijie0219@gmail.com",
                "affiliations": [
                    "Universit\u00e9 Paris-Saclay, CNRS, Orsay, France",
                    "Inria, Saclay, France"
                ],
                "is_corresponding": false
            },
            {
                "name": "Petra Isenberg",
                "email": "petra.isenberg@inria.fr",
                "affiliations": [
                    "Universit\u00e9 Paris-Saclay, CNRS, Orsay, France",
                    "Inria, Saclay, France"
                ],
                "is_corresponding": false
            }
        ],
        "abstract": "With two studies, we assess how different walking trajectories (straight line, circular, and infinity) and speeds (2 km/h, 4 km/h, and 6 km/h) influence the accuracy and response time of participants reading micro visualizations on a smartwatch. We showed our participants common watch face micro visualizations including date, time, weather information, and four complications showing progress charts of fitness data. Our findings suggest that while walking trajectories did not significantly affect reading performance, overall walking activity, especially at high speeds, hurt reading accuracy and, to some extent, response time.",
        "uid": "v-short-1159",
        "time_stamp": "",
        "time_start": "",
        "time_end": "",
        "paper_type": "short",
        "keywords": [],
        "doi": "",
        "fno": "",
        "presentation_mode": "",
        "open_access": false,
        "accessible_pdf": false,
        "has_image": false,
        "has_pdf": false,
        "paper_award": "",
        "image_caption": "",
        "external_paper_link": "",
        "youtube_ff_link": "",
        "youtube_ff_id": "",
        "bunny_ff_link": "",
        "bunny_ff_subtitles": "",
        "youtube_prerecorded_link": "",
        "youtube_prerecorded_id": "",
        "bunny_prerecorded_link": "",
        "bunny_prerecorded_subtitles": ""
    },
    "v-short-1161": {
        "slot_id": "v-short-1161",
        "session_id": "short0",
        "title": "Visualizing an Exascale Data Center Digital Twin: Considerations, Challenges and Opportunities",
        "contributors": [
            "Matthias Maiterth"
        ],
        "authors": [
            {
                "name": "Matthias Maiterth",
                "email": "maiterthm@ornl.gov",
                "affiliations": [
                    "Oak Ridge National Laboratory, Oak Ridge, United States"
                ],
                "is_corresponding": true
            },
            {
                "name": "Wes Brewer",
                "email": "brewerwh@ornl.gov",
                "affiliations": [
                    "Oak Ridge National Laboratory, Oak Ridge, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Dane De Wet",
                "email": "dewetd@ornl.gov",
                "affiliations": [
                    "Oak Ridge National Laboratory, Oak Ridge, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Scott Greenwood",
                "email": "greenwoodms@ornl.gov",
                "affiliations": [
                    "Oak Ridge National Laboratory, Oak Ridge, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Vineet Kumar",
                "email": "kumarv@ornl.gov",
                "affiliations": [
                    "Oak Ridge National Laboratory, Oak Ridge, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Jesse Hines",
                "email": "hinesjr@ornl.gov",
                "affiliations": [
                    "Oak Ridge National Laboratory, Oak Ridge, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Sedrick L Bouknight",
                "email": "bouknightsl@ornl.gov",
                "affiliations": [
                    "Oak Ridge National Laboratory, Oak Ridge, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Zhe Wang",
                "email": "wangz@ornl.gov",
                "affiliations": [
                    "Oak Ridge National Laboratory, Oak Ridge, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Tim Dykes",
                "email": "tim.dykes@hpe.com",
                "affiliations": [
                    "Hewlett Packard Enterprise, Berkshire, United Kingdom"
                ],
                "is_corresponding": false
            },
            {
                "name": "Feiyi Wang",
                "email": "fwang2@ornl.gov",
                "affiliations": [
                    "Oak Ridge National Laboratory, Oak Ridge, United States"
                ],
                "is_corresponding": false
            }
        ],
        "abstract": "Digital twins are an excellent tool to model, visualize, and simulate complex systems, to understand and optimize their operation. In this work, we present the technical challenges of real-time visualization of a digital twin of the Frontier supercomputer. We show the initial prototype and current state of the twin and highlight technical design challenges of visualizing such a large High Performance Computing (HPC) system. The goal is to understand the use of augmented reality as a primary way to extract information and collaborate on digital twins of complex systems. This leverages the spatio-temporal aspect of a 3D representation of a digital twin, with the ability to view historical and real-time telemetry, triggering simulations of a system state and viewing the results, which can be augmented via dashboards for details. Finally, we discuss considerations and opportunities for augmented reality of digital twins of large-scale, parallel computers.",
        "uid": "v-short-1161",
        "time_stamp": "",
        "time_start": "",
        "time_end": "",
        "paper_type": "short",
        "keywords": [],
        "doi": "",
        "fno": "",
        "presentation_mode": "",
        "open_access": false,
        "accessible_pdf": false,
        "has_image": false,
        "has_pdf": false,
        "paper_award": "",
        "image_caption": "",
        "external_paper_link": "",
        "youtube_ff_link": "",
        "youtube_ff_id": "",
        "bunny_ff_link": "",
        "bunny_ff_subtitles": "",
        "youtube_prerecorded_link": "",
        "youtube_prerecorded_id": "",
        "bunny_prerecorded_link": "",
        "bunny_prerecorded_subtitles": ""
    },
    "v-short-1163": {
        "slot_id": "v-short-1163",
        "session_id": "short0",
        "title": "Curve Segment Neighborhood-based Vector Field Exploration",
        "contributors": [
            "Nguyen K Phan"
        ],
        "authors": [
            {
                "name": "Nguyen K Phan",
                "email": "nguyenpkk95@gmail.com",
                "affiliations": [
                    "University of Houston, Houston, United States"
                ],
                "is_corresponding": true
            },
            {
                "name": "Guoning Chen",
                "email": "chengu@cs.uh.edu",
                "affiliations": [
                    "University of Houston, Houston, United States"
                ],
                "is_corresponding": false
            }
        ],
        "abstract": "Integral curves have been widely used to represent and analyze various vector fields. Curve-based clustering and pattern search approaches are usually applied to aid the identification of meaningful patterns from large numbers of integral curves.  However, they need not support an interactive, level-of-detail exploration of these patterns. To address this, we propose a Curve Segment Neighborhood Graph (CSNG) to capture the relationships between neighboring curve segments. This graph representation enables us to adapt the fast community detection algorithm, i.e., the Louvain algorithm, to identify individual graph communities from CSNG. Our results show that these communities often correspond to the features of the flow. To achieve a multi-level interactive exploration of the detected communities, we adapt a force-directed layout that allows users to refine and re-group communities based on their domain knowledge. We incorporate the proposed techniques into an interactive system to enable effective analysis and interpretation of complex patterns in large-scale integral curve datasets.",
        "uid": "v-short-1163",
        "time_stamp": "",
        "time_start": "",
        "time_end": "",
        "paper_type": "short",
        "keywords": [],
        "doi": "",
        "fno": "",
        "presentation_mode": "",
        "open_access": false,
        "accessible_pdf": false,
        "has_image": false,
        "has_pdf": false,
        "paper_award": "",
        "image_caption": "",
        "external_paper_link": "",
        "youtube_ff_link": "",
        "youtube_ff_id": "",
        "bunny_ff_link": "",
        "bunny_ff_subtitles": "",
        "youtube_prerecorded_link": "",
        "youtube_prerecorded_id": "",
        "bunny_prerecorded_link": "",
        "bunny_prerecorded_subtitles": ""
    },
    "v-short-1166": {
        "slot_id": "v-short-1166",
        "session_id": "short0",
        "title": "Counterpoint: Orchestrating Large-Scale Custom Animated Visualizations",
        "contributors": [
            "Venkatesh Sivaraman"
        ],
        "authors": [
            {
                "name": "Venkatesh Sivaraman",
                "email": "vsivaram@andrew.cmu.edu",
                "affiliations": [
                    "Carnegie Mellon University, Pittsburgh, United States"
                ],
                "is_corresponding": true
            },
            {
                "name": "Frank Elavsky",
                "email": "fje@cmu.edu",
                "affiliations": [
                    "Carnegie Mellon University, Pittsburgh, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Dominik Moritz",
                "email": "domoritz@cmu.edu",
                "affiliations": [
                    "Carnegie Mellon University, Pittsburgh, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Adam Perer",
                "email": "adamperer@cmu.edu",
                "affiliations": [
                    "Carnegie Mellon University, Pittsburgh, United States"
                ],
                "is_corresponding": false
            }
        ],
        "abstract": "Custom animated visualizations of large, complex datasets are helpful across many domains, but they are hard to develop. Much of the difficulty arises from maintaining visualization state across a large set of animated graphical elements that may change in number over time. We contribute Counterpoint, a framework for state management designed to help implement such visualizations in JavaScript. Using Counterpoint, developers can manipulate large collections of marks with reactive attributes that are easy to render in scalable APIs such as Canvas and WebGL. Counterpoint also helps orchestrate the entry and exit of graphical elements using the concept of a rendering \"stage.\" Through a performance evaluation, we show that Counterpoint adds minimal overhead over current high-performance rendering techniques while simplifying implementation. We also provide two examples of visualizations created using Counterpoint that illustrate its flexibility and compatibility with other visualization toolkits as well as considerations for users with disabilities. Counterpoint is open-source and available at https://github.com/cmudig/counterpoint.",
        "uid": "v-short-1166",
        "time_stamp": "",
        "time_start": "",
        "time_end": "",
        "paper_type": "short",
        "keywords": [],
        "doi": "",
        "fno": "",
        "presentation_mode": "",
        "open_access": false,
        "accessible_pdf": false,
        "has_image": false,
        "has_pdf": false,
        "paper_award": "",
        "image_caption": "",
        "external_paper_link": "",
        "youtube_ff_link": "",
        "youtube_ff_id": "",
        "bunny_ff_link": "",
        "bunny_ff_subtitles": "",
        "youtube_prerecorded_link": "",
        "youtube_prerecorded_id": "",
        "bunny_prerecorded_link": "",
        "bunny_prerecorded_subtitles": ""
    },
    "v-short-1173": {
        "slot_id": "v-short-1173",
        "session_id": "short0",
        "title": "Fields, Bridges, and Foundations: How Researchers Browse Citation Network Visualizations",
        "contributors": [
            "Kiroong Choe"
        ],
        "authors": [
            {
                "name": "Kiroong Choe",
                "email": "krchoe@hcil.snu.ac.kr",
                "affiliations": [
                    "Seoul National University, Seoul, Korea, Republic of"
                ],
                "is_corresponding": true
            },
            {
                "name": "Eunhye Kim",
                "email": "gracekim027@snu.ac.kr",
                "affiliations": [
                    "Seoul National University, Seoul, Korea, Republic of"
                ],
                "is_corresponding": false
            },
            {
                "name": "Sangwon Park",
                "email": "paulmoguri@snu.ac.kr",
                "affiliations": [
                    "Dept. of Electrical and Computer Engineering, SNU, Seoul, Korea, Republic of"
                ],
                "is_corresponding": false
            },
            {
                "name": "Jinwook Seo",
                "email": "jseo@snu.ac.kr",
                "affiliations": [
                    "Seoul National University, Seoul, Korea, Republic of"
                ],
                "is_corresponding": false
            }
        ],
        "abstract": "Visualizing citation relations with network structures is widely used, but the visual complexity can make it challenging for individual researchers to navigate through them. We collected data from 18 researchers using an interface that we designed using network simplification methods and analyzed how users browsed and identified important papers. Our analysis reveals six major patterns used for identifying papers of interest, which can be categorized into three key components: Fields, Bridges, and Foundations, each viewed from two distinct perspectives: layout-oriented and connection-oriented. The connection-oriented approach was found to be more effective for selecting relevant papers, but the layout-oriented method was adopted more often, even though it led to unexpected results and user frustration. Our findings emphasize the importance of integrating these components and the necessity to  balance visual layouts with meaningful connections to enhance the effectiveness of citation networks in academic browsing systems.",
        "uid": "v-short-1173",
        "time_stamp": "",
        "time_start": "",
        "time_end": "",
        "paper_type": "short",
        "keywords": [],
        "doi": "",
        "fno": "",
        "presentation_mode": "",
        "open_access": false,
        "accessible_pdf": false,
        "has_image": false,
        "has_pdf": false,
        "paper_award": "",
        "image_caption": "",
        "external_paper_link": "",
        "youtube_ff_link": "",
        "youtube_ff_id": "",
        "bunny_ff_link": "",
        "bunny_ff_subtitles": "",
        "youtube_prerecorded_link": "",
        "youtube_prerecorded_id": "",
        "bunny_prerecorded_link": "",
        "bunny_prerecorded_subtitles": ""
    },
    "v-short-1177": {
        "slot_id": "v-short-1177",
        "session_id": "short0",
        "title": "Can GPT-4V Detect Misleading Visualizations?",
        "contributors": [
            "Ali Sarvghad"
        ],
        "authors": [
            {
                "name": "Jason Huang Alexander",
                "email": "jhalexander@umass.edu",
                "affiliations": [
                    "University of Massachusetts Amherst, Amherst, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Priyal H Nanda",
                "email": "phnanda@umass.edu",
                "affiliations": [
                    "University of Masssachusetts Amherst, Amherst, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Kai-Cheng Yang",
                "email": "yangkc@iu.edu",
                "affiliations": [
                    "Northeastern University, Boston, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Ali Sarvghad",
                "email": "asarv@cs.umass.edu",
                "affiliations": [
                    "University of Massachusetts Amherst, Amherst, United States"
                ],
                "is_corresponding": true
            }
        ],
        "abstract": "The proliferation of misleading visualizations online, particularly during critical events like public health crises and elections, poses a significant risk of misinformation. This work investigates the capability of GPT-4V to detect misleading visualizations. Utilizing a dataset of tweet-visualization pairs with various visual misleaders, we tested GPT-4V under four experimental conditions: naive zero-shot, naive few-shot, guided zero-shot, and guided few-shot. Our results demonstrate that GPT-4V can detect misleading visualizations with moderate accuracy without prior training (naive zero-shot) and that performance considerably improves by providing the model with the definitions of misleaders (guided zero-shot). However, combining definitions with examples of misleaders (guided few-shot) did not yield further improvements. This study underscores the feasibility of using large vision-language models such as GTP-4V to combat misinformation and emphasizes the importance of optimizing prompt engineering to enhance detection accuracy.",
        "uid": "v-short-1177",
        "time_stamp": "",
        "time_start": "",
        "time_end": "",
        "paper_type": "short",
        "keywords": [],
        "doi": "",
        "fno": "",
        "presentation_mode": "",
        "open_access": false,
        "accessible_pdf": false,
        "has_image": false,
        "has_pdf": false,
        "paper_award": "",
        "image_caption": "",
        "external_paper_link": "",
        "youtube_ff_link": "",
        "youtube_ff_id": "",
        "bunny_ff_link": "",
        "bunny_ff_subtitles": "",
        "youtube_prerecorded_link": "",
        "youtube_prerecorded_id": "",
        "bunny_prerecorded_link": "",
        "bunny_prerecorded_subtitles": ""
    },
    "v-short-1183": {
        "slot_id": "v-short-1183",
        "session_id": "short0",
        "title": "A Ridge-based Approach for Extraction and Visualization of 3D Atmospheric Fronts",
        "contributors": [
            "Anne Gossing"
        ],
        "authors": [
            {
                "name": "Anne Gossing",
                "email": "anne.gossing@fu-berlin.de",
                "affiliations": [
                    "Zuse Institute Berlin, Berlin, Germany"
                ],
                "is_corresponding": true
            },
            {
                "name": "Andreas Beckert",
                "email": "andreas.beckert@uni-hamburg.de",
                "affiliations": [
                    "Universit\u00e4t Hamburg, Hamburg, Germany"
                ],
                "is_corresponding": false
            },
            {
                "name": "Christoph Fischer",
                "email": "christoph.fischer-1@uni-hamburg.de",
                "affiliations": [
                    "Universit\u00e4t Hamburg, Hamburg, Germany"
                ],
                "is_corresponding": false
            },
            {
                "name": "Nicolas Klenert",
                "email": "klenert@zib.de",
                "affiliations": [
                    "Zuse Institute Berlin, Berlin, Germany"
                ],
                "is_corresponding": false
            },
            {
                "name": "Vijay Natarajan",
                "email": "vijayn@iisc.ac.in",
                "affiliations": [
                    "Indian Institute of Science, Bangalore, India"
                ],
                "is_corresponding": false
            },
            {
                "name": "George Pacey",
                "email": "george.pacey@fu-berlin.de",
                "affiliations": [
                    "Freie Universit\u00e4t Berlin, Berlin, Germany"
                ],
                "is_corresponding": false
            },
            {
                "name": "Thorwin Vogt",
                "email": "thorwin.vogt@uni-hamburg.de",
                "affiliations": [
                    "Universit\u00e4t Hamburg, Hamburg, Germany"
                ],
                "is_corresponding": false
            },
            {
                "name": "Marc Rautenhaus",
                "email": "marc.rautenhaus@uni-hamburg.de",
                "affiliations": [
                    "Universit\u00e4t Hamburg, Hamburg, Germany"
                ],
                "is_corresponding": false
            },
            {
                "name": "Daniel Baum",
                "email": "baum@zib.de",
                "affiliations": [
                    "Zuse Institute Berlin, Berlin, Germany"
                ],
                "is_corresponding": false
            }
        ],
        "abstract": "An atmospheric front is an imaginary surface that separates two distinct air masses and is commonly defined as the warm-air side of a frontal zone with high gradients of atmospheric temperature and humidity. These fronts are a widely used conceptual model in meteorology, which are often encountered in the literature as two-dimensional (2D) front lines on surface analysis charts. This paper presents a method for computing three-dimensional (3D) atmospheric fronts as surfaces that is capable of extracting continuous and well-confined features suitable for 3D visual analysis, spatio-temporal tracking, and statistical analyses. Recently developed contour-based methods for 3D front extraction rely on computing the third derivative of a moist potential temperature field. Additionally, they require the field to be smoothed to obtain continuous large-scale structures. This paper demonstrates the feasibility of an alternative method to front extraction using ridge surface computation. The proposed method requires only the sec- ond derivative of the input field and produces accurate structures even from unsmoothed data. An application of the ridge-based method to a data set corresponding to Cyclone Friederike demonstrates its benefits and utility towards visual analysis of the full 3D structure of fronts.",
        "uid": "v-short-1183",
        "time_stamp": "",
        "time_start": "",
        "time_end": "",
        "paper_type": "short",
        "keywords": [],
        "doi": "",
        "fno": "",
        "presentation_mode": "",
        "open_access": false,
        "accessible_pdf": false,
        "has_image": false,
        "has_pdf": false,
        "paper_award": "",
        "image_caption": "",
        "external_paper_link": "",
        "youtube_ff_link": "",
        "youtube_ff_id": "",
        "bunny_ff_link": "",
        "bunny_ff_subtitles": "",
        "youtube_prerecorded_link": "",
        "youtube_prerecorded_id": "",
        "bunny_prerecorded_link": "",
        "bunny_prerecorded_subtitles": ""
    },
    "v-short-1184": {
        "slot_id": "v-short-1184",
        "session_id": "short0",
        "title": "Towards a Quality Approach to Hierarchical Color Maps",
        "contributors": [
            "Tobias Mertz"
        ],
        "authors": [
            {
                "name": "Tobias Mertz",
                "email": "tobias.mertz@igd.fraunhofer.de",
                "affiliations": [
                    "Fraunhofer IGD, Darmstadt, Germany"
                ],
                "is_corresponding": true
            },
            {
                "name": "J\u00f6rn Kohlhammer",
                "email": "joern.kohlhammer@igd.fraunhofer.de",
                "affiliations": [
                    "Fraunhofer IGD, Darmstadt, Germany",
                    "TU Darmstadt, Darmstadt, Germany"
                ],
                "is_corresponding": false
            }
        ],
        "abstract": "To improve the perception of hierarchical structures in data sets, several color map generation algorithms have been proposed to take this structure into account. But the design of hierarchical color maps elicits different requirements to those of color maps for tabular data. Within this paper, we make an initial effort to put design rules from the color map literature into the context of hierarchical color maps. We investigate the impact of several design decisions and provide recommendations for various analysis scenarios. Thus, we lay the foundation for objective quality criteria to evaluate hierarchical color maps.",
        "uid": "v-short-1184",
        "time_stamp": "",
        "time_start": "",
        "time_end": "",
        "paper_type": "short",
        "keywords": [],
        "doi": "",
        "fno": "",
        "presentation_mode": "",
        "open_access": false,
        "accessible_pdf": false,
        "has_image": false,
        "has_pdf": false,
        "paper_award": "",
        "image_caption": "",
        "external_paper_link": "",
        "youtube_ff_link": "",
        "youtube_ff_id": "",
        "bunny_ff_link": "",
        "bunny_ff_subtitles": "",
        "youtube_prerecorded_link": "",
        "youtube_prerecorded_id": "",
        "bunny_prerecorded_link": "",
        "bunny_prerecorded_subtitles": ""
    },
    "v-short-1185": {
        "slot_id": "v-short-1185",
        "session_id": "short0",
        "title": "Two-point Equidistant Projection and Degree-of-interest Filtering for Smooth Exploration of Geo-referenced Networks",
        "contributors": [
            "Max Franke"
        ],
        "authors": [
            {
                "name": "Max Franke",
                "email": "max@mumintroll.org",
                "affiliations": [
                    "University of Stuttgart, Stuttgart, Germany"
                ],
                "is_corresponding": true
            },
            {
                "name": "Samuel Beck",
                "email": "samuel.beck@vis.uni-stuttgart.de",
                "affiliations": [
                    "University of Stuttgart, Stuttgart, Germany"
                ],
                "is_corresponding": false
            },
            {
                "name": "Steffen Koch",
                "email": "steffen.koch@vis.uni-stuttgart.de",
                "affiliations": [
                    "University of Stuttgart, Stuttgart, Germany"
                ],
                "is_corresponding": false
            }
        ],
        "abstract": "The visualization and interactive exploration of geo-referenced networks poses challenges if the network's nodes are not evenly distributed. Our approach proposes new ways of realizing animated transitions for exploring such networks from an ego-perspective. We aim to reduce the required screen estate while maintaining the viewers' mental map of distances and directions. A preliminary study provides first insights of the comprehensiveness of animated geographic transitions regarding directional relationships between start and end point in different projections. Two use cases showcase how ego-perspective graph exploration can be supported using less screen space than previous approaches.",
        "uid": "v-short-1185",
        "time_stamp": "",
        "time_start": "",
        "time_end": "",
        "paper_type": "short",
        "keywords": [],
        "doi": "",
        "fno": "",
        "presentation_mode": "",
        "open_access": false,
        "accessible_pdf": false,
        "has_image": false,
        "has_pdf": false,
        "paper_award": "",
        "image_caption": "",
        "external_paper_link": "",
        "youtube_ff_link": "",
        "youtube_ff_id": "",
        "bunny_ff_link": "",
        "bunny_ff_subtitles": "",
        "youtube_prerecorded_link": "",
        "youtube_prerecorded_id": "",
        "bunny_prerecorded_link": "",
        "bunny_prerecorded_subtitles": ""
    },
    "v-short-1186": {
        "slot_id": "v-short-1186",
        "session_id": "short0",
        "title": "Exploring the Capability of LLMs in Performing Low-Level Visual Analytic Tasks on SVG Data Visualizations",
        "contributors": [
            "Zhongzheng Xu"
        ],
        "authors": [
            {
                "name": "Zhongzheng Xu",
                "email": "leooooxzz@gmail.com",
                "affiliations": [
                    "Brown University, Providence, United States"
                ],
                "is_corresponding": true
            },
            {
                "name": "Emily Wall",
                "email": "emily.wall@emory.edu",
                "affiliations": [
                    "Emory University, Atlanta, United States"
                ],
                "is_corresponding": false
            }
        ],
        "abstract": "Data visualizations help extract insights from datasets, but reaching these insights requires decomposing high level goals into low-level analytic tasks that can be complex due to varying degrees of data literacy and visualization experience. Recent advancements in large language models (LLMs) have shown promise for lowering barriers for users to achieve tasks such as writing code and may likewise facilitate visualization insight. Scalable Vector Graphics (SVG), a text-based image format common in data visualizations, matches well with the text sequence processing of transformer-based LLMs. In this paper, we explore the capability of LLMs to perform 10 low-level visual analytic tasks defined by Amar, Eagan, and Stasko directly on SVG-based visualizations. Using zero-shot prompts, we instruct the models to provide responses or modify the SVG code based on given visualizations. Our findings demonstrate that LLMs can effectively modify existing SVG visualizations for some tasks like Cluster but perform poorly on tasks requiring mathematical operations like Compute Derived Value. We also discovered that LLM performance can vary based on factors such as the number of data points, the presence of value labels, and the chart type. Our findings contribute to gauging the general capabilities of LLMs and highlight the need for further exploration and development to fully harness their potential in supporting visual analytic tasks.",
        "uid": "v-short-1186",
        "time_stamp": "",
        "time_start": "",
        "time_end": "",
        "paper_type": "short",
        "keywords": [],
        "doi": "",
        "fno": "",
        "presentation_mode": "",
        "open_access": false,
        "accessible_pdf": false,
        "has_image": false,
        "has_pdf": false,
        "paper_award": "",
        "image_caption": "",
        "external_paper_link": "",
        "youtube_ff_link": "",
        "youtube_ff_id": "",
        "bunny_ff_link": "",
        "bunny_ff_subtitles": "",
        "youtube_prerecorded_link": "",
        "youtube_prerecorded_id": "",
        "bunny_prerecorded_link": "",
        "bunny_prerecorded_subtitles": ""
    },
    "v-short-1188": {
        "slot_id": "v-short-1188",
        "session_id": "short0",
        "title": "Topological Separation of Vortices",
        "contributors": [
            "Adeel Zafar"
        ],
        "authors": [
            {
                "name": "Adeel Zafar",
                "email": "adeelz92@gmail.com",
                "affiliations": [
                    "University of Houston, Houston, United States"
                ],
                "is_corresponding": true
            },
            {
                "name": "Zahra Poorshayegh",
                "email": "zpoorsha@cougarnet.uh.edu",
                "affiliations": [
                    "University of Houston, Houston, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Di Yang",
                "email": "diyang@uh.edu",
                "affiliations": [
                    "University of Houston, Houston, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Guoning Chen",
                "email": "chengu@cs.uh.edu",
                "affiliations": [
                    "University of Houston, Houston, United States"
                ],
                "is_corresponding": false
            }
        ],
        "abstract": "Vortices and their analysis play a critical role in the understanding of complex phenomena in turbulent flow. Traditional vortex extraction methods, notably region-based techniques, often overlook the entanglement phenomenon, resulting in the inclusion of multiple vortices within a single extracted region. Their separation is necessary for quantifying different types of vortices and their statistics. In this study, we propose a novel vortex separation method that extends the conventional contour tree-based segmentation approach with an additional step termed \u201clayering\u201d. Upon extracting a vortical region using specified vortex criteria (e.g., \u03bb2), we initially establish topological segmentation based on the contour tree, followed by the layering process to allocate appropriate segmentation IDs to unsegmented cells, thus separating individual vortices within the region. However, these regions may still suffer from inaccurate splits, which we address statistically by leveraging the continuity of vorticity lines across the split boundaries. Our findings demonstrate a significant improvement in both the separation of vortices and the mitigation of inaccurate splits compared to prior methods.",
        "uid": "v-short-1188",
        "time_stamp": "",
        "time_start": "",
        "time_end": "",
        "paper_type": "short",
        "keywords": [],
        "doi": "",
        "fno": "",
        "presentation_mode": "",
        "open_access": false,
        "accessible_pdf": false,
        "has_image": false,
        "has_pdf": false,
        "paper_award": "",
        "image_caption": "",
        "external_paper_link": "",
        "youtube_ff_link": "",
        "youtube_ff_id": "",
        "bunny_ff_link": "",
        "bunny_ff_subtitles": "",
        "youtube_prerecorded_link": "",
        "youtube_prerecorded_id": "",
        "bunny_prerecorded_link": "",
        "bunny_prerecorded_subtitles": ""
    },
    "v-short-1189": {
        "slot_id": "v-short-1189",
        "session_id": "short0",
        "title": "Towards Reusable and Reactive Widgets for Information Visualization Research and Dissemination",
        "contributors": [
            "John Alexis Guerra-Gomez"
        ],
        "authors": [
            {
                "name": "John Alexis Guerra-Gomez",
                "email": "john.guerra@gmail.com",
                "affiliations": [
                    "Northeastern University, San Francisco, United States"
                ],
                "is_corresponding": true
            }
        ],
        "abstract": "The information visualization research community commonly produces supporting software to demonstrate technical contributions to the field. However, developing this software tends to be an overwhelming task, the final product tends to be a research prototype without much thought for modularization and re-usability which makes it harder to replicate and adopt. This paper presents a design pattern for facilitating the creation, dissemination, and re-utilization of visualization techniques using reactive widgets. The design pattern features basic concepts that leverage modern front-end development best practices and standards, which ease development and replication. The paper presents several usage examples of the pattern, templates for implementation, and even a wrapper for facilitating the conversion of any Vega specification into a reactive widget.",
        "uid": "v-short-1189",
        "time_stamp": "",
        "time_start": "",
        "time_end": "",
        "paper_type": "short",
        "keywords": [],
        "doi": "",
        "fno": "",
        "presentation_mode": "",
        "open_access": false,
        "accessible_pdf": false,
        "has_image": false,
        "has_pdf": false,
        "paper_award": "",
        "image_caption": "",
        "external_paper_link": "",
        "youtube_ff_link": "",
        "youtube_ff_id": "",
        "bunny_ff_link": "",
        "bunny_ff_subtitles": "",
        "youtube_prerecorded_link": "",
        "youtube_prerecorded_id": "",
        "bunny_prerecorded_link": "",
        "bunny_prerecorded_subtitles": ""
    },
    "v-short-1191": {
        "slot_id": "v-short-1191",
        "session_id": "short0",
        "title": "Bringing Data into the Conversation: Adapting Content from Business Intelligence Dashboards for Threaded Collaboration Platforms",
        "contributors": [
            "Hyeok Kim"
        ],
        "authors": [
            {
                "name": "Hyeok Kim",
                "email": "hyeokkim2024@u.northwestern.edu",
                "affiliations": [
                    "Northwestern University, Evanston, United States"
                ],
                "is_corresponding": true
            },
            {
                "name": "Arjun Srinivasan",
                "email": "arjun.srinivasan.10@gmail.com",
                "affiliations": [
                    "Tableau Research, Seattle, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Matthew Brehmer",
                "email": "mbrehmer@uwaterloo.ca",
                "affiliations": [
                    "Tableau Research, Seattle, United States"
                ],
                "is_corresponding": false
            }
        ],
        "abstract": "To enable data-driven decision-making across organizations, data professionals need to share insights with their colleagues in context-appropriate communication channels. Many of their colleagues rely on data but are not themselves analysts; furthermore, their colleagues are reluctant or unable to use dedicated analytical applications or dashboards, and they expect communication to take place within threaded collaboration platforms such as Slack or Microsoft Teams. In this paper, we introduce a set of six strategies for adapting content from business intelligence (BI) dashboards into appropriate formats for sharing on collaboration platforms, formats that we refer to as dashboard snapshots.  Informed by prior studies of enterprise communication around data, these strategies go beyond redesigning or restyling by considering varying levels of data literacy across an organization, introducing affordances for self-service question-answering, and anticipating the post-sharing lifecycle of data artifacts. These strategies involve the use of templates that are matched to common communicative intents, serving to reduce the workload of data professionals. We contribute a formal representation of these strategies and demonstrate their applicability in a comprehensive enterprise communication scenario featuring multiple stakeholders that unfolds over the span of months.",
        "uid": "v-short-1191",
        "time_stamp": "",
        "time_start": "",
        "time_end": "",
        "paper_type": "short",
        "keywords": [],
        "doi": "",
        "fno": "",
        "presentation_mode": "",
        "open_access": false,
        "accessible_pdf": false,
        "has_image": false,
        "has_pdf": false,
        "paper_award": "",
        "image_caption": "",
        "external_paper_link": "",
        "youtube_ff_link": "",
        "youtube_ff_id": "",
        "bunny_ff_link": "",
        "bunny_ff_subtitles": "",
        "youtube_prerecorded_link": "",
        "youtube_prerecorded_id": "",
        "bunny_prerecorded_link": "",
        "bunny_prerecorded_subtitles": ""
    },
    "v-short-1192": {
        "slot_id": "v-short-1192",
        "session_id": "short0",
        "title": "Animating the Narrative: A Review of Animation Styles in Narrative Visualization",
        "contributors": [
            "Vyri Junhan Yang"
        ],
        "authors": [
            {
                "name": "Vyri Junhan Yang",
                "email": "jyang44@lsu.edu",
                "affiliations": [
                    "Louisiana State University, Baton Rouge, United States"
                ],
                "is_corresponding": true
            },
            {
                "name": "Mahmood Jasim",
                "email": "mjasim@lsu.edu",
                "affiliations": [
                    "Louisiana State University, Baton Rouge, United States"
                ],
                "is_corresponding": false
            }
        ],
        "abstract": "Narrative visualization has become a crucial tool in data presentation, merging storytelling with data visualization to convey complex information in an engaging and accessible manner. In this study, we review the design space for narrative visualizations, focusing on animation style, through a comprehensive analysis of 71 papers from key visualization venues. We categorize these papers into six broad themes: Animation Style, Interactivity, Technology Usage, Methodology Development, Evaluation Type, and Application Domain. Our findings reveal a significant evolution in the field, marked by a growing preference for animated and non-interactive techniques. This trend reflects a shift towards minimizing user interaction while enhancing the clarity and impact of data presentation. We also identified key trends and technologies that have shaped the field, highlighting the role of technologies, such as machine learning in driving these changes. We offer insights into the dynamic interrelations within the narrative visualization domain, suggesting a future research trajectory that balances interactivity with automated tools to foster increased engagement. Our work lays the groundwork for future approaches for effective and innovative narrative visualization in diverse applications.",
        "uid": "v-short-1192",
        "time_stamp": "",
        "time_start": "",
        "time_end": "",
        "paper_type": "short",
        "keywords": [],
        "doi": "",
        "fno": "",
        "presentation_mode": "",
        "open_access": false,
        "accessible_pdf": false,
        "has_image": false,
        "has_pdf": false,
        "paper_award": "",
        "image_caption": "",
        "external_paper_link": "",
        "youtube_ff_link": "",
        "youtube_ff_id": "",
        "bunny_ff_link": "",
        "bunny_ff_subtitles": "",
        "youtube_prerecorded_link": "",
        "youtube_prerecorded_id": "",
        "bunny_prerecorded_link": "",
        "bunny_prerecorded_subtitles": ""
    },
    "v-short-1193": {
        "slot_id": "v-short-1193",
        "session_id": "short0",
        "title": "LinkQ: An LLM-Assisted Visual Interface for Knowledge Graph Question-Answering",
        "contributors": [
            "Harry Li"
        ],
        "authors": [
            {
                "name": "Harry Li",
                "email": "harry.li@ll.mit.edu",
                "affiliations": [
                    "MIT Lincoln Laboratory, Lexington, United States"
                ],
                "is_corresponding": true
            },
            {
                "name": "Gabriel Appleby",
                "email": "gabriel.appleby@tufts.edu",
                "affiliations": [
                    "Tufts University, Medford, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Ashley Suh",
                "email": "ashley.suh@ll.mit.edu",
                "affiliations": [
                    "MIT Lincoln Laboratory, Lexington, United States"
                ],
                "is_corresponding": false
            }
        ],
        "abstract": "We present LinkQ, a system that leverages a large language model (LLM) to facilitate knowledge graph (KG) query construction through natural language question-answering. Traditional approaches often require detailed knowledge of complex graph querying languages, limiting the ability for users -- even experts -- to acquire valuable insights from KG data. LinkQ simplifies this process by first interpreting a user's question, then converting it into a well-formed KG query. By using the LLM to construct a query instead of directly answering the user's question, LinkQ guards against the LLM hallucinating or generating false, erroneous information. By integrating an LLM into LinkQ, users are able to conduct both exploratory and confirmatory data analysis, with the LLM helping to iteratively refine open-ended questions into precise ones. To demonstrate the efficacy of LinkQ, we conducted a qualitative study with five KG practitioners and distill their feedback. Our results indicate that practitioners find LinkQ effective for KG question-answering, and desire future LLM-assisted systems for the exploratory analysis of graph databases.",
        "uid": "v-short-1193",
        "time_stamp": "",
        "time_start": "",
        "time_end": "",
        "paper_type": "short",
        "keywords": [],
        "doi": "",
        "fno": "",
        "presentation_mode": "",
        "open_access": false,
        "accessible_pdf": false,
        "has_image": false,
        "has_pdf": false,
        "paper_award": "",
        "image_caption": "",
        "external_paper_link": "",
        "youtube_ff_link": "",
        "youtube_ff_id": "",
        "bunny_ff_link": "",
        "bunny_ff_subtitles": "",
        "youtube_prerecorded_link": "",
        "youtube_prerecorded_id": "",
        "bunny_prerecorded_link": "",
        "bunny_prerecorded_subtitles": ""
    },
    "v-short-1199": {
        "slot_id": "v-short-1199",
        "session_id": "short0",
        "title": "From Graphs to Words: A Computer-Assisted Framework for the Production of Accessible Text Descriptions",
        "contributors": [
            "Qiang Xu"
        ],
        "authors": [
            {
                "name": "Qiang Xu",
                "email": "qiangxu1204@gmail.com",
                "affiliations": [
                    "Polytechnique Montr\u00e9al, Montr\u00e9al, Canada"
                ],
                "is_corresponding": true
            },
            {
                "name": "Thomas Hurtut",
                "email": "thomas.hurtut@polymtl.ca",
                "affiliations": [
                    "Polytechnique Montreal, Montreal, Canada"
                ],
                "is_corresponding": false
            }
        ],
        "abstract": "In the digital landscape, the ubiquity of data visualizations in media underscores the necessity for accessibility to ensure inclusivity for all users, including those with visual impairments. Current visual content often fails to cater to the needs of screen reader users due to the absence of comprehensive textual descriptions. To address this gap, we propose in this paper a framework designed to empower media content creators to transform charts into descriptive narratives. This tool not only facilitates the understanding of complex visual data through text but also fosters a broader awareness of accessibility in digital content creation. Through the application of this framework, users can interpret and convey the insights of data visualizations more effectively, accommodating a diverse audience. Our evaluations reveal that this tool not only enhances the comprehension of data visualizations but also promotes new perspectives on the represented data, thereby broadening the interpretative possibilities for all users.",
        "uid": "v-short-1199",
        "time_stamp": "",
        "time_start": "",
        "time_end": "",
        "paper_type": "short",
        "keywords": [],
        "doi": "",
        "fno": "",
        "presentation_mode": "",
        "open_access": false,
        "accessible_pdf": false,
        "has_image": false,
        "has_pdf": false,
        "paper_award": "",
        "image_caption": "",
        "external_paper_link": "",
        "youtube_ff_link": "",
        "youtube_ff_id": "",
        "bunny_ff_link": "",
        "bunny_ff_subtitles": "",
        "youtube_prerecorded_link": "",
        "youtube_prerecorded_id": "",
        "bunny_prerecorded_link": "",
        "bunny_prerecorded_subtitles": ""
    },
    "v-short-1207": {
        "slot_id": "v-short-1207",
        "session_id": "short0",
        "title": "Design of a Real-Time Visual Analytics Decision Support Interface to Manage Air Traffic Complexity",
        "contributors": [
            "Elmira Zohrevandi"
        ],
        "authors": [
            {
                "name": "Elmira Zohrevandi",
                "email": "elmira.zohrevandi@liu.se",
                "affiliations": [
                    "Link\u00f6ping University, Norrk\u00f6ping, Sweden",
                    "Link\u00f6ping University, Norrk\u00f6ping, Sweden"
                ],
                "is_corresponding": true
            },
            {
                "name": "Katerina Vrotsou",
                "email": "katerina.vrotsou@liu.se",
                "affiliations": [
                    "Link\u00f6ping University, Norrk\u00f6ping, Sweden",
                    "Link\u00f6ping University, Norrk\u00f6ping, Sweden"
                ],
                "is_corresponding": false
            },
            {
                "name": "Carl A. L. Westin",
                "email": "carl.westin@liu.se",
                "affiliations": [
                    "Institute of Science and Technology, Norrk\u00f6ping, Sweden",
                    "Institute of Science and Technology, Norrk\u00f6ping, Sweden"
                ],
                "is_corresponding": false
            },
            {
                "name": "Jonas Lundberg",
                "email": "jonas.lundberg@liu.se",
                "affiliations": [
                    "Link\u00f6ping University, Norrk\u00f6ping, Sweden",
                    "Link\u00f6ping University, Norrk\u00f6ping, Sweden"
                ],
                "is_corresponding": false
            },
            {
                "name": "Anders Ynnerman",
                "email": "anders.ynnerman@liu.se",
                "affiliations": [
                    "Link\u00f6ping University, Norrk\u00f6ping, Sweden",
                    "Link\u00f6ping University, Norrk\u00f6ping, Sweden"
                ],
                "is_corresponding": false
            }
        ],
        "abstract": "An essential task of an air traffic controller is to manage the traffic flow by predicting future trajectories. Complex traffic patterns are difficult to predict and manage and impose cognitive load on the air traffic controllers. In this work we present an interactive visual analytics interface which facilitates detection and resolution of complex traffic patterns for air traffic controllers. The interface supports the users in detecting complex clusters of aircraft and uses visual representations to communicate to the controllers how and propose re-routing. The interface further enables the ATCos to visualize and simultaneously compare how different re-routing strategies for each individual aircraft yield reduction of complexity in the entire sector for the next hour. The development of the concepts was supported by the domain-specific feedback we received from six fully licensed and operational air traffic controllers in an iterative design process over a period of 14 months.",
        "uid": "v-short-1207",
        "time_stamp": "",
        "time_start": "",
        "time_end": "",
        "paper_type": "short",
        "keywords": [],
        "doi": "",
        "fno": "",
        "presentation_mode": "",
        "open_access": false,
        "accessible_pdf": false,
        "has_image": false,
        "has_pdf": false,
        "paper_award": "",
        "image_caption": "",
        "external_paper_link": "",
        "youtube_ff_link": "",
        "youtube_ff_id": "",
        "bunny_ff_link": "",
        "bunny_ff_subtitles": "",
        "youtube_prerecorded_link": "",
        "youtube_prerecorded_id": "",
        "bunny_prerecorded_link": "",
        "bunny_prerecorded_subtitles": ""
    },
    "v-short-1211": {
        "slot_id": "v-short-1211",
        "session_id": "short0",
        "title": "Text-based transfer function design for semantic volume rendering",
        "contributors": [
            "Sangwon Jeong"
        ],
        "authors": [
            {
                "name": "Sangwon Jeong",
                "email": "sangwon.jeong@vanderbilt.edu",
                "affiliations": [
                    "Vanderbilt University, Nashville, United States"
                ],
                "is_corresponding": true
            },
            {
                "name": "Jixian Li",
                "email": "jixianli@sci.utah.edu",
                "affiliations": [
                    "University of Utah, Salt Lake City, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Shusen Liu",
                "email": "shusenl@sci.utah.edu",
                "affiliations": [
                    "Lawrence Livermore National Laboratory , Livermore, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Chris R. Johnson",
                "email": "crj@sci.utah.edu",
                "affiliations": [
                    "University of Utah, Salt Lake City, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Matthew Berger",
                "email": "matthew.berger@vanderbilt.edu",
                "affiliations": [
                    "Vanderbilt University, Nashville, United States"
                ],
                "is_corresponding": false
            }
        ],
        "abstract": "Transfer function design is crucial in volume rendering, as it directly influences the visual representation and interpretation of volumetric data. However, creating effective transfer functions that align with users\u2019 visual objectives is often challenging due to the complex parameter space and the semantic gap between transfer function values and features of interest within the volume. In this work, we propose a novel approach that leverages recent advancements in language-vision models to bridge this semantic gap. By employing a fully differentiable rendering pipeline and an image-based loss function guided by language descriptions, our method generates transfer functions that yield volume-rendered images closely matching the user\u2019s intent. We demonstrate the effectiveness of our approach in creating meaningful transfer functions from simple descriptions, empowering users to intuitively express their desired visual outcomes with minimal effort. This advancement streamlines the transfer function design process and makes volume rendering more accessible to a broader range of users.",
        "uid": "v-short-1211",
        "time_stamp": "",
        "time_start": "",
        "time_end": "",
        "paper_type": "short",
        "keywords": [],
        "doi": "",
        "fno": "",
        "presentation_mode": "",
        "open_access": false,
        "accessible_pdf": false,
        "has_image": false,
        "has_pdf": false,
        "paper_award": "",
        "image_caption": "",
        "external_paper_link": "",
        "youtube_ff_link": "",
        "youtube_ff_id": "",
        "bunny_ff_link": "",
        "bunny_ff_subtitles": "",
        "youtube_prerecorded_link": "",
        "youtube_prerecorded_id": "",
        "bunny_prerecorded_link": "",
        "bunny_prerecorded_subtitles": ""
    },
    "v-short-1224": {
        "slot_id": "v-short-1224",
        "session_id": "short0",
        "title": "Diffusion Explainer: Visual Explanation for Text-to-image Stable Diffusion",
        "contributors": [
            "Seongmin Lee"
        ],
        "authors": [
            {
                "name": "Seongmin Lee",
                "email": "seongmin@gatech.edu",
                "affiliations": [
                    "Georgia Tech, Atlanta, United States"
                ],
                "is_corresponding": true
            },
            {
                "name": "Benjamin Hoover",
                "email": "benjamin.hoover@ibm.com",
                "affiliations": [
                    "GA Tech, Atlanta, United States",
                    "IBM Research AI, Cambridge, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Hendrik Strobelt",
                "email": "hendrik@strobelt.com",
                "affiliations": [
                    "IBM Research AI, Cambridge, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Zijie J. Wang",
                "email": "jayw@gatech.edu",
                "affiliations": [
                    "Georgia Tech, Atlanta, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "ShengYun Peng",
                "email": "speng65@gatech.edu",
                "affiliations": [
                    "Georgia Institute of Technology, Atlanta, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Austin P Wright",
                "email": "apwright@gatech.edu",
                "affiliations": [
                    "Georgia Institute of Technology , Atlanta , United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Kevin Li",
                "email": "kevin.li@gatech.edu",
                "affiliations": [
                    "Georgia Institute of Technology, Atlanta, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Haekyu Park",
                "email": "haekyu@gatech.edu",
                "affiliations": [
                    "Georgia Institute of Technology, Atlanta, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Haoyang Yang",
                "email": "alexanderyang@gatech.edu",
                "affiliations": [
                    "Georgia Institute of Technology, Atlanta, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Duen Horng (Polo) Chau",
                "email": "polo@gatech.edu",
                "affiliations": [
                    "Georgia Tech, Atlanta, United States"
                ],
                "is_corresponding": false
            }
        ],
        "abstract": "Diffusion-based generative models\u2019 impressive ability to create convincing images has garnered global attention. However, their complex structures and operations often pose challenges for non-experts to grasp. We present Diffusion Explainer, the first interactive visualization tool that explains how Stable Diffusion transforms text prompts into images. Diffusion Explainer tightly integrates a visual overview of Stable Diffusion\u2019s complex structure with explanations of the underlying operations. By comparing image generation of prompt variants, users can discover the impact of keyword changes on image generation. A 56-participant user study demonstrates that Diffusion Explainer offers substantial learning benefits to non-experts. Our tool has been used by over 10,300 users from 124 countries at https://poloclub.github.io/diffusion-explainer/.",
        "uid": "v-short-1224",
        "time_stamp": "",
        "time_start": "",
        "time_end": "",
        "paper_type": "short",
        "keywords": [],
        "doi": "",
        "fno": "",
        "presentation_mode": "",
        "open_access": false,
        "accessible_pdf": false,
        "has_image": false,
        "has_pdf": false,
        "paper_award": "",
        "image_caption": "",
        "external_paper_link": "",
        "youtube_ff_link": "",
        "youtube_ff_id": "",
        "bunny_ff_link": "",
        "bunny_ff_subtitles": "",
        "youtube_prerecorded_link": "",
        "youtube_prerecorded_id": "",
        "bunny_prerecorded_link": "",
        "bunny_prerecorded_subtitles": ""
    },
    "v-short-1235": {
        "slot_id": "v-short-1235",
        "session_id": "short0",
        "title": "Uniform Sample Distribution in Scatterplots via Sector-based Transformation",
        "contributors": [
            "Hennes Rave"
        ],
        "authors": [
            {
                "name": "Hennes Rave",
                "email": "hennes.rave@uni-muenster.de",
                "affiliations": [
                    "University of M\u00fcnster, M\u00fcnster, Germany"
                ],
                "is_corresponding": true
            },
            {
                "name": "Vladimir Molchanov",
                "email": "molchano@uni-muenster.de",
                "affiliations": [
                    "University of M\u00fcnster, M\u00fcnster, Germany"
                ],
                "is_corresponding": false
            },
            {
                "name": "Lars Linsen",
                "email": "linsen@uni-muenster.de",
                "affiliations": [
                    "University of M\u00fcnster, M\u00fcnster, Germany"
                ],
                "is_corresponding": false
            }
        ],
        "abstract": "A high number of samples often leads to occlusion in scatterplots, which hinders data perception and analysis. De-cluttering approaches based on spatial transformation reduce visual clutter by remapping samples using the entire available scatterplot domain. Such regularized scatterplots may still be used for data analysis tasks, if the spatial transformation is smooth and preserves the original neighborhood relations of samples. Recently, Rave et al. proposed an efficient regularization method based on integral images. We propose a generalization of their regularization scheme using sector-based transformations with the aim of increasing sample uniformity of the resulting scatterplot. We document the improvement of our approach using various uniformity measures.",
        "uid": "v-short-1235",
        "time_stamp": "",
        "time_start": "",
        "time_end": "",
        "paper_type": "short",
        "keywords": [],
        "doi": "",
        "fno": "",
        "presentation_mode": "",
        "open_access": false,
        "accessible_pdf": false,
        "has_image": false,
        "has_pdf": false,
        "paper_award": "",
        "image_caption": "",
        "external_paper_link": "",
        "youtube_ff_link": "",
        "youtube_ff_id": "",
        "bunny_ff_link": "",
        "bunny_ff_subtitles": "",
        "youtube_prerecorded_link": "",
        "youtube_prerecorded_id": "",
        "bunny_prerecorded_link": "",
        "bunny_prerecorded_subtitles": ""
    },
    "v-short-1236": {
        "slot_id": "v-short-1236",
        "session_id": "short0",
        "title": "Evaluating the Semantic Profiling Abilities of LLMs for Natural Language Utterances in Data Visualization",
        "contributors": [
            "Hannah K. Bako"
        ],
        "authors": [
            {
                "name": "Hannah K. Bako",
                "email": "hbako@umd.edu",
                "affiliations": [
                    "University of Maryland, College Park, United States"
                ],
                "is_corresponding": true
            },
            {
                "name": "Arshnoor Bhutani",
                "email": "arshnoorbhutani8@gmail.com",
                "affiliations": [
                    "University of Maryland, College Park, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Xinyi Liu",
                "email": "xinyi.liu@utexas.edu",
                "affiliations": [
                    "The University of Texas at Austin, Austin, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Kwesi Adu Cobbina",
                "email": "kcobbina@cs.umd.edu",
                "affiliations": [
                    "University of Maryland, College Park, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Zhicheng Liu",
                "email": "leozcliu@umd.edu",
                "affiliations": [
                    "University of Maryland, College Park, United States"
                ],
                "is_corresponding": false
            }
        ],
        "abstract": "Automatically generating data visualizations in response to human utterances on datasets necessitates a deep semantic understanding of the data utterance, including implicit and explicit references to data attributes, visualization tasks, and necessary data preparation steps. Natural Language Interfaces (NLIs) for data visualization have explored ways to infer such information, yet challenges persist due to inherent uncertainty in human speech. Recent advances in Large Language Models (LLMs) provide an avenue to address these challenges, but their ability to extract the relevant semantic information remains unexplored. In this study, we evaluate four publicly available LLMs (GPT-4, Gemini-Pro, Llama3, and Mixtral), investigating their ability to comprehend utterances even in the presence of uncertainty and identify the relevant data context and visual tasks. Our findings reveal that LLMs are sensitive to uncertainties in utterance. Despite this sensitivity, they are able to extract the relevant data context. However, LLMs struggle with inferring visualization tasks. Based on these results, we highlight future research directions on using LLMs for visualization generation. Our supplementary materials have been shared on OSF: https://osf.io/j342a/wiki/home/?view_only=b4051ffc6253496d9bce818e4a89b9f9",
        "uid": "v-short-1236",
        "time_stamp": "",
        "time_start": "",
        "time_end": "",
        "paper_type": "short",
        "keywords": [],
        "doi": "",
        "fno": "",
        "presentation_mode": "",
        "open_access": false,
        "accessible_pdf": false,
        "has_image": false,
        "has_pdf": false,
        "paper_award": "",
        "image_caption": "",
        "external_paper_link": "",
        "youtube_ff_link": "",
        "youtube_ff_id": "",
        "bunny_ff_link": "",
        "bunny_ff_subtitles": "",
        "youtube_prerecorded_link": "",
        "youtube_prerecorded_id": "",
        "bunny_prerecorded_link": "",
        "bunny_prerecorded_subtitles": ""
    },
    "v-short-1248": {
        "slot_id": "v-short-1248",
        "session_id": "short0",
        "title": "Guided Statistical Workflows with Interactive Explanations and Assumption Checking",
        "contributors": [
            "Yuqi Zhang"
        ],
        "authors": [
            {
                "name": "Yuqi Zhang",
                "email": "yz9381@nyu.edu",
                "affiliations": [
                    "New York University, New York, United States"
                ],
                "is_corresponding": true
            },
            {
                "name": "Adam Perer",
                "email": "adamperer@cmu.edu",
                "affiliations": [
                    "Carnegie Mellon University, Pittsburgh, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Will Epperson",
                "email": "willepp@cmu.edu",
                "affiliations": [
                    "Carnegie Mellon University, Pittsburgh, United States"
                ],
                "is_corresponding": false
            }
        ],
        "abstract": "Statistical practices such as building regression models or running hypothesis tests rely on following rigorous procedures of steps and verifying assumptions on data to produce valid results. However, common statistical tools do not verify users\u2019 decision choices and provide low-level statistical functions without instructions on the whole analysis practice. Users can easily misuse analysis methods, potentially decreasing the validity of results. To address this problem, we introduce GuidedStats, an interactive interface within computational notebooks that encapsulates guidance, models, visualization, and exportable results into interactive workflows. It breaks down typical analysis processes, such as linear regression and two-sample T-tests, into interactive steps supplemented with automatic visualizations and explanations for step-wise evaluation. Users can iterate on input choices to refine their models, while recommended actions and exports allow the user to continue their analysis in code. Case studies show how GuidedStats offers valuable instructions for conducting fluid statistical analyses while finding possible assumption violations in the underlying data, supporting flexible and accurate statistical analyses.",
        "uid": "v-short-1248",
        "time_stamp": "",
        "time_start": "",
        "time_end": "",
        "paper_type": "short",
        "keywords": [],
        "doi": "",
        "fno": "",
        "presentation_mode": "",
        "open_access": false,
        "accessible_pdf": false,
        "has_image": false,
        "has_pdf": false,
        "paper_award": "",
        "image_caption": "",
        "external_paper_link": "",
        "youtube_ff_link": "",
        "youtube_ff_id": "",
        "bunny_ff_link": "",
        "bunny_ff_subtitles": "",
        "youtube_prerecorded_link": "",
        "youtube_prerecorded_id": "",
        "bunny_prerecorded_link": "",
        "bunny_prerecorded_subtitles": ""
    },
    "v-short-1264": {
        "slot_id": "v-short-1264",
        "session_id": "short0",
        "title": "Demystifying Spatial Dependence: Interactive Visualizations for Interpreting Local Spatial Autocorrelation",
        "contributors": [
            "Lee Mason"
        ],
        "authors": [
            {
                "name": "Lee Mason",
                "email": "masonlk@nih.gov",
                "affiliations": [
                    "NIH, Rockville, United States",
                    "Queen's University, Belfast, United Kingdom"
                ],
                "is_corresponding": true
            },
            {
                "name": "Bl\u00e1naid Hicks",
                "email": "b.hicks@qub.ac.uk",
                "affiliations": [
                    "Queen's University Belfast , Belfast , United Kingdom"
                ],
                "is_corresponding": false
            },
            {
                "name": "Jonas S Almeida",
                "email": "jonas.dealmeida@nih.gov",
                "affiliations": [
                    "National Institutes of Health, Rockville, United States"
                ],
                "is_corresponding": false
            }
        ],
        "abstract": "The Local Moran's I statistic is a valuable tool for identifying localized patterns of spatial autocorrelation. Understanding these patterns is crucial in spatial analysis, but interpreting the statistic can be difficult. To simplify this process, we introduce three novel visualizations that enhance the interpretation of Local Moran's I results. These visualizations can be interactively linked to one another, and to established visualizations, to offer a more holistic exploration of the results. We provide a JavaScript library with implementations of these new visual elements, along with a web dashboard that demonstrates their integrated use.",
        "uid": "v-short-1264",
        "time_stamp": "",
        "time_start": "",
        "time_end": "",
        "paper_type": "short",
        "keywords": [],
        "doi": "",
        "fno": "",
        "presentation_mode": "",
        "open_access": false,
        "accessible_pdf": false,
        "has_image": false,
        "has_pdf": false,
        "paper_award": "",
        "image_caption": "",
        "external_paper_link": "",
        "youtube_ff_link": "",
        "youtube_ff_id": "",
        "bunny_ff_link": "",
        "bunny_ff_subtitles": "",
        "youtube_prerecorded_link": "",
        "youtube_prerecorded_id": "",
        "bunny_prerecorded_link": "",
        "bunny_prerecorded_subtitles": ""
    },
    "v-short-1274": {
        "slot_id": "v-short-1274",
        "session_id": "short0",
        "title": "Dark Mode or Light Mode? Exploring the Impact of Contrast Polarity on Visualization Performance Between Age Groups",
        "contributors": [
            "Zack While"
        ],
        "authors": [
            {
                "name": "Zack While",
                "email": "zwhile@cs.umass.edu",
                "affiliations": [
                    "University of Massachusetts Amherst, Amherst, United States"
                ],
                "is_corresponding": true
            },
            {
                "name": "Ali Sarvghad",
                "email": "asarv@cs.umass.edu",
                "affiliations": [
                    "University of Massachusetts Amherst, Amherst, United States"
                ],
                "is_corresponding": false
            }
        ],
        "abstract": "This study examines the impact of positive and negative contrast polarities (i.e., light and dark modes) on the performance of younger adults and people in their late adulthood (PLA). In a crowdsourced study with 134 participants (69 below age 60, 66 aged 60 and above), we assessed their accuracy and time performing analysis tasks across three common visualization types (Bar, Line, Scatterplot) and two contrast polarities (positive and negative). We observed that, across both age groups, the polarity that led to better performance and the resulting amount of improvement varied on an individual basis, with each polarity benefiting comparable proportions of participants. Additionally, we observed that the choice of contrast polarity can have an impact on time similar to that of the choice of visualization type, resulting in an average percent difference of around 36%. These findings indicate that, overall, the effects of contrast polarity on visual analysis performance do not noticeably change with age. Furthermore, they underscore the importance of making visualizations available in both contrast polarities to better-support a broad audience with differing needs.",
        "uid": "v-short-1274",
        "time_stamp": "",
        "time_start": "",
        "time_end": "",
        "paper_type": "short",
        "keywords": [],
        "doi": "",
        "fno": "",
        "presentation_mode": "",
        "open_access": false,
        "accessible_pdf": false,
        "has_image": false,
        "has_pdf": false,
        "paper_award": "",
        "image_caption": "",
        "external_paper_link": "",
        "youtube_ff_link": "",
        "youtube_ff_id": "",
        "bunny_ff_link": "",
        "bunny_ff_subtitles": "",
        "youtube_prerecorded_link": "",
        "youtube_prerecorded_id": "",
        "bunny_prerecorded_link": "",
        "bunny_prerecorded_subtitles": ""
    },
    "v-short-1276": {
        "slot_id": "v-short-1276",
        "session_id": "short0",
        "title": "Representing Charts as Text for Language Models: An In-Depth Study of Question Answering for Bar Charts",
        "contributors": [
            "Victor S. Bursztyn"
        ],
        "authors": [
            {
                "name": "Victor S. Bursztyn",
                "email": "victorbursztyn2022@u.northwestern.edu",
                "affiliations": [
                    "Adobe Research, San Jose, United States"
                ],
                "is_corresponding": true
            },
            {
                "name": "Jane Hoffswell",
                "email": "jhoffs@adobe.com",
                "affiliations": [
                    "Adobe Research, Seattle, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Shunan Guo",
                "email": "sguo@adobe.com",
                "affiliations": [
                    "Adobe Research, San Jose, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Eunyee Koh",
                "email": "eunyee@adobe.com",
                "affiliations": [
                    "Adobe Research, San Jose, United States"
                ],
                "is_corresponding": false
            }
        ],
        "abstract": "Machine Learning models for chart-grounded Q&A (CQA) often treat charts as images, but performing CQA on pixel values has proven challenging. We thus investigate a resource overlooked by current ML-based approaches: the declarative documents describing how charts should visually encode data (i.e., chart specifications). In this work, we use chart specifications to enhance language models (LMs) for chart-reading tasks, such that the resulting system can robustly understand language for CQA. Through a case study with 359 bar charts, we test novel fine tuning schemes on both GPT-3 and T5 using a new dataset curated for two CQA tasks: question-answering and visual explanation generation. Our text-only approaches strongly outperform vision-based GPT-4 on explanation generation (99% vs. 63% accuracy), and show promising results for question-answering (57-67% accuracy). Through in-depth experiments, we also show that our text-only approaches are mostly robust to natural language variation.",
        "uid": "v-short-1276",
        "time_stamp": "",
        "time_start": "",
        "time_end": "",
        "paper_type": "short",
        "keywords": [],
        "doi": "",
        "fno": "",
        "presentation_mode": "",
        "open_access": false,
        "accessible_pdf": false,
        "has_image": false,
        "has_pdf": false,
        "paper_award": "",
        "image_caption": "",
        "external_paper_link": "",
        "youtube_ff_link": "",
        "youtube_ff_id": "",
        "bunny_ff_link": "",
        "bunny_ff_subtitles": "",
        "youtube_prerecorded_link": "",
        "youtube_prerecorded_id": "",
        "bunny_prerecorded_link": "",
        "bunny_prerecorded_subtitles": ""
    },
    "v-short-1277": {
        "slot_id": "v-short-1277",
        "session_id": "short0",
        "title": "Building and Eroding: Exogenous and Endogenous Factors that Influence Subjective Trust in Visualization",
        "contributors": [
            "R. Jordan Crouser"
        ],
        "authors": [
            {
                "name": "R. Jordan Crouser",
                "email": "jcrouser@smith.edu",
                "affiliations": [
                    "Smith College, Northampton, United States"
                ],
                "is_corresponding": true
            },
            {
                "name": "Syrine Matoussi",
                "email": "cmatoussi@smith.edu",
                "affiliations": [
                    "Smith College, Northampton, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Lan Kung",
                "email": "ekung@smith.edu",
                "affiliations": [
                    "Smith College, Northampton, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Saugat Pandey",
                "email": "p.saugat@wustl.edu",
                "affiliations": [
                    "Washington University in St. Louis, St. Louis, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Oen G McKinley",
                "email": "m.oen@wustl.edu",
                "affiliations": [
                    "Washington University in St. Louis, St. Louis, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Alvitta Ottley",
                "email": "alvitta@wustl.edu",
                "affiliations": [
                    "Washington University in St. Louis, St. Louis, United States"
                ],
                "is_corresponding": false
            }
        ],
        "abstract": "Trust is a subjective yet fundamental component of human-computer interaction, and is a determining factor in shaping the efficacy of data visualizations. Prior research has identified five dimensions of trust assessment in visualizations (credibility, clarity, reliability, familiarity, and confidence), and observed that these dimensions tend to vary predictably along with certain features of the visualization being evaluated. This raises a further question: how do the design features driving viewers' trust assessment vary with the characteristics of the viewers themselves? By reanalyzing data from these studies through the lens of individual differences, we build a more detailed map of the relationships between design features, individual characteristics, and trust behaviors. In particular, we model the distinct contributions of endogenous design features (such as visualization type, or the use of color) and exogenous user characteristics (such as visualization literacy), as well as the interactions between them. We then use these findings to make recommendations for individualized and adaptive visualization design.",
        "uid": "v-short-1277",
        "time_stamp": "",
        "time_start": "",
        "time_end": "",
        "paper_type": "short",
        "keywords": [],
        "doi": "",
        "fno": "",
        "presentation_mode": "",
        "open_access": false,
        "accessible_pdf": false,
        "has_image": false,
        "has_pdf": false,
        "paper_award": "",
        "image_caption": "",
        "external_paper_link": "",
        "youtube_ff_link": "",
        "youtube_ff_id": "",
        "bunny_ff_link": "",
        "bunny_ff_subtitles": "",
        "youtube_prerecorded_link": "",
        "youtube_prerecorded_id": "",
        "bunny_prerecorded_link": "",
        "bunny_prerecorded_subtitles": ""
    },
    "v-short-1285": {
        "slot_id": "v-short-1285",
        "session_id": "short0",
        "title": "\"Must Be a Tuesday\": Affect, Attribution, and Geographic Variability in Equity-Oriented Visualizations of Population Health Disparities",
        "contributors": [
            "Lace M. Padilla"
        ],
        "authors": [
            {
                "name": "Eli Holder",
                "email": "eli@3iap.com",
                "affiliations": [
                    "3iap, Raleigh, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Lace M. Padilla",
                "email": "l.padilla@northeastern.edu",
                "affiliations": [
                    "Northeastern University, Boston, United States",
                    "University of California Merced, Merced, United States"
                ],
                "is_corresponding": true
            }
        ],
        "abstract": "This study examines the impact of social-comparison risk visualizations on public health communication, comparing the effects of traditional bar charts against alternative jitter plots emphasizing geographic variability (geo jitter).  The research highlights that whereas both visualization types increased perceived vulnerability, behavioral intent, and policy support, the geo jitter plots were significantly more effective in reducing unjustified personal attributions. Importantly, the findings also underscore the emotional challenges faced by visualization viewers from marginalized communities, indicating a need for designs that are sensitive to the potential for reinforcing stereotypes or eliciting negative emotions. This work suggests a strategic reevaluation of visual communication tools in public health to enhance understanding and engagement without contributing to negative attributions or emotional distress.",
        "uid": "v-short-1285",
        "time_stamp": "",
        "time_start": "",
        "time_end": "",
        "paper_type": "short",
        "keywords": [],
        "doi": "",
        "fno": "",
        "presentation_mode": "",
        "open_access": false,
        "accessible_pdf": false,
        "has_image": false,
        "has_pdf": false,
        "paper_award": "",
        "image_caption": "",
        "external_paper_link": "",
        "youtube_ff_link": "",
        "youtube_ff_id": "",
        "bunny_ff_link": "",
        "bunny_ff_subtitles": "",
        "youtube_prerecorded_link": "",
        "youtube_prerecorded_id": "",
        "bunny_prerecorded_link": "",
        "bunny_prerecorded_subtitles": ""
    },
    "v-short-1292": {
        "slot_id": "v-short-1292",
        "session_id": "short0",
        "title": "Multi-User Mobile Augmented Reality for Cardiovascular Surgical Planning",
        "contributors": [
            "Pratham Darrpan Mehta"
        ],
        "authors": [
            {
                "name": "Pratham Darrpan Mehta",
                "email": "pratham.mehta001@gmail.com",
                "affiliations": [
                    "Georgia Tech, Atlanta, United States"
                ],
                "is_corresponding": true
            },
            {
                "name": "Rahul Ozhur Narayanan",
                "email": "rnarayanan39@gatech.edu",
                "affiliations": [
                    "Georgia Tech, Atlanta, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Harsha Karanth",
                "email": "harsha5431@gmail.com",
                "affiliations": [
                    "Georgia Tech, Atlanta, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Haoyang Yang",
                "email": "alexanderyang@gatech.edu",
                "affiliations": [
                    "Georgia Institute of Technology, Atlanta, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Timothy C Slesnick",
                "email": "slesnickt@kidsheart.com",
                "affiliations": [
                    "Emory University, Atlanta, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Fawwaz Shaw",
                "email": "fawwaz.shaw@choa.org",
                "affiliations": [
                    "Emory University/Children's Healthcare of Atlanta, Atlanta, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Duen Horng (Polo) Chau",
                "email": "polo@gatech.edu",
                "affiliations": [
                    "Georgia Tech, Atlanta, United States"
                ],
                "is_corresponding": false
            }
        ],
        "abstract": "Collaborative planning for congenital heart diseases typically involves creating physical heart models through 3D printing, which are then examined by both surgeons and cardiologists. Recent developments in mobile augmented reality (AR) technologies have presented a viable alternative, known for their ease of use and portability. However, there is still a lack of research examining the utilization of multi-user mobile AR environments to support collaborative planning for cardiovascular surgeries. We created ARCollab, an iOS AR app designed for enabling multiple surgeons and cardiologists to interact with a patient's 3D heart model in a shared environment. ARCollab enables surgeons and cardiologists to import heart models, manipulate them through gestures and collaborate with other users, eliminating the need for fabricating physical heart models. Our evaluation of ARCollab's usability and usefulness  in enhancing collaboration, conducted with three cardiothoracic surgeons and two cardiologists, marks the first human evaluation of a multi-user mobile AR tool for surgical planning. ARCollab is open-source, available at https://github.com/poloclub/arcollab.",
        "uid": "v-short-1292",
        "time_stamp": "",
        "time_start": "",
        "time_end": "",
        "paper_type": "short",
        "keywords": [],
        "doi": "",
        "fno": "",
        "presentation_mode": "",
        "open_access": false,
        "accessible_pdf": false,
        "has_image": false,
        "has_pdf": false,
        "paper_award": "",
        "image_caption": "",
        "external_paper_link": "",
        "youtube_ff_link": "",
        "youtube_ff_id": "",
        "bunny_ff_link": "",
        "bunny_ff_subtitles": "",
        "youtube_prerecorded_link": "",
        "youtube_prerecorded_id": "",
        "bunny_prerecorded_link": "",
        "bunny_prerecorded_subtitles": ""
    },
    "v-short-1301": {
        "slot_id": "v-short-1301",
        "session_id": "short0",
        "title": "Zoomable Glyph Tables for Interpreting Probabilistic Model Outputs for Reactionary Train Delays",
        "contributors": [
            "Aidan Slingsby"
        ],
        "authors": [
            {
                "name": "Aidan Slingsby",
                "email": "a.slingsby@city.ac.uk",
                "affiliations": [
                    "City, University of London, London, United Kingdom"
                ],
                "is_corresponding": true
            },
            {
                "name": "Jonathan Hyde",
                "email": "jonathan.hyde@risksol.co.uk",
                "affiliations": [
                    "Risk Solutions, Warrington, United Kingdom"
                ],
                "is_corresponding": false
            }
        ],
        "abstract": "Reactionary delay'' is a result of the accumulated cascading effects of knock-on train delays.  It is becoming an increasing problem as shared railway infrastructure becomes more crowded. The chaotic nature of its effects is notoriously hard to predict. We use a stochastic Monte-Carto-style simulation of reactionary delay that produces whole distributions of likely reactionary delay. Our contribution is the demonstrating how Zoomable GlyphTables -- case-by-variable tables in which cases are rows, variables are columns, variables are complex composite metrics that incorporate distributions, and cells contain mini-charts that depict these as different level of detail through zoom interaction -- help interpret these results for helping understanding the causes and effects of reactionary delay and how they have been informing timetable robustness testing and tweaking. We describe our design principles, demonstrate how this supported our analytical tasks and we reflect on wider potential for Zoomable GlyphTables to be used more widely.",
        "uid": "v-short-1301",
        "time_stamp": "",
        "time_start": "",
        "time_end": "",
        "paper_type": "short",
        "keywords": [],
        "doi": "",
        "fno": "",
        "presentation_mode": "",
        "open_access": false,
        "accessible_pdf": false,
        "has_image": false,
        "has_pdf": false,
        "paper_award": "",
        "image_caption": "",
        "external_paper_link": "",
        "youtube_ff_link": "",
        "youtube_ff_id": "",
        "bunny_ff_link": "",
        "bunny_ff_subtitles": "",
        "youtube_prerecorded_link": "",
        "youtube_prerecorded_id": "",
        "bunny_prerecorded_link": "",
        "bunny_prerecorded_subtitles": ""
    },
    "v-full-1026": {
        "slot_id": "v-full-1026",
        "session_id": "full0",
        "title": "Revealing Interaction Dynamics: Multi-Level Visual Exploration of User Strategies with an Interactive Digital Environment",
        "contributors": [
            "Peilin Yu"
        ],
        "authors": [
            {
                "name": "Peilin Yu",
                "email": "peilin.yu@liu.se",
                "affiliations": [
                    "Media and Information Technology, Norrk\u00f6ping, Sweden"
                ],
                "is_corresponding": true
            },
            {
                "name": "Aida Nordman",
                "email": "aida.vitoria@liu.se",
                "affiliations": [
                    "Link\u00f6ping University, Norrk\u00f6ping, Sweden"
                ],
                "is_corresponding": false
            },
            {
                "name": "Marta M. Koc-Januchta",
                "email": "marta.koc-januchta@liu.se",
                "affiliations": [
                    "Link\u00f6ping University, Norrk\u00f6ping, Sweden"
                ],
                "is_corresponding": false
            },
            {
                "name": "Konrad J Sch\u00f6nborn",
                "email": "konrad.schonborn@liu.se",
                "affiliations": [
                    "Link\u00f6ping University, Norrk\u00f6ping, Sweden"
                ],
                "is_corresponding": false
            },
            {
                "name": "Lonni Besan\u00e7on",
                "email": "lonni.besancon@gmail.com",
                "affiliations": [
                    "Link\u00f6ping University, Norrk\u00f6ping, Sweden"
                ],
                "is_corresponding": false
            },
            {
                "name": "Katerina Vrotsou",
                "email": "katerina.vrotsou@liu.se",
                "affiliations": [
                    "Link\u00f6ping University, Norrk\u00f6ping, Sweden"
                ],
                "is_corresponding": false
            }
        ],
        "abstract": "We present a visual analytics approach for multi-level visual exploration of users\u2019 interaction strategies in an interactive digital environment. The use of interactive touchscreen exhibits in informal learning environments, such as museums and science centers, often incorporate frameworks that classify learning processes, such as Bloom\u2019s taxonomy, to achieve better user engagement and knowledge transfer. To analyze user behavior within these digital environments, interaction logs are recorded to capture diverse exploration strategies. However, analysis of such logs is challenging, especially in terms of coupling interactions and cognitive learning processes, and existing work within learning and educational contexts remains limited. To address these gaps, we develop a visual analytics approach for analyzing interaction logs that supports exploration at the individual user level and multi-user comparison.  The approach utilizes algorithmic methods to identify similarities in users' interactions and reveal their exploration strategies. We motivate and illustrate our approach through an application scenario, using event sequences derived from interaction log data in an experimental study conducted with science center visitors from diverse backgrounds and demographics. The study involves 14 users completing tasks of increasing complexity, designed to stimulate different levels of cognitive learning processes. We implement our approach in an interactive visual analytics prototype system, named VISID, and together with domain experts, discover a set of task-solving exploration strategies, such as \"cascading\" and \"nested-loop\", which reflect different levels of learning processes from Bloom's taxonomy.  Finally, we discuss the generalizability and scalability of the presented system and the need for further research with data acquired in the wild.",
        "uid": "v-full-1026",
        "time_stamp": "",
        "time_start": "",
        "time_end": "",
        "paper_type": "full",
        "keywords": [],
        "doi": "",
        "fno": "",
        "presentation_mode": "",
        "open_access": false,
        "accessible_pdf": false,
        "has_image": false,
        "has_pdf": false,
        "paper_award": "",
        "image_caption": "",
        "external_paper_link": "",
        "youtube_ff_link": "",
        "youtube_ff_id": "",
        "bunny_ff_link": "",
        "bunny_ff_subtitles": "",
        "youtube_prerecorded_link": "",
        "youtube_prerecorded_id": "",
        "bunny_prerecorded_link": "",
        "bunny_prerecorded_subtitles": ""
    },
    "v-full-1031": {
        "slot_id": "v-full-1031",
        "session_id": "full0",
        "title": "Team-Scouter: Simulative Visual Analytics of Soccer Player Scouting",
        "contributors": [
            "Anqi Cao"
        ],
        "authors": [
            {
                "name": "Anqi Cao",
                "email": "caoanqi28@163.com",
                "affiliations": [
                    "Zhejiang University, Hangzhou, China"
                ],
                "is_corresponding": true
            },
            {
                "name": "Xiao Xie",
                "email": "xxie@zju.edu.cn",
                "affiliations": [
                    "Zhejiang University, Hangzhou, China"
                ],
                "is_corresponding": false
            },
            {
                "name": "Runjin Zhang",
                "email": "2366385033@qq.com",
                "affiliations": [
                    "Zhejiang University, Hangzhou, China"
                ],
                "is_corresponding": false
            },
            {
                "name": "Yuxin Tian",
                "email": "1282533692@qq.com",
                "affiliations": [
                    "Zhejiang University, Hangzhou, China"
                ],
                "is_corresponding": false
            },
            {
                "name": "Mu Fan",
                "email": "fanmu_032@zju.edu.cn",
                "affiliations": [
                    "Zhejiang University, Hangzhou, China"
                ],
                "is_corresponding": false
            },
            {
                "name": "Hui Zhang",
                "email": "zhang_hui@zju.edu.cn",
                "affiliations": [
                    "Zhejiang University, Hangzhou, China"
                ],
                "is_corresponding": false
            },
            {
                "name": "Yingcai Wu",
                "email": "ycwu@zju.edu.cn",
                "affiliations": [
                    "Zhejiang University, Hangzhou, China"
                ],
                "is_corresponding": false
            }
        ],
        "abstract": "In soccer, player scouting aims to find players suitable for a team to increase the winning chance in future matches. To scout suitable players, coaches and analysts need to consider various complicated factors, such as the players' performance in the tactics of a new team, which is hard to learn directly from their historical performance. Match simulation methods have been introduced to scout players by estimating their expected contributions to a new team. However, they usually focus on the simulation of match results and hardly support interactive analysis to navigate potential target players and compare them in fine-grained simulated behaviors. In this work, we propose a visual analytics method to assist soccer player scouting based on match simulation. We construct a two-level match simulation framework for estimating both match results and player behaviors when a player comes to a new team. Based on the framework, we develop a visual analytics system, Team-Scouter, to facilitate the simulative-based soccer player scouting process through player navigation, comparison, and explanation. With our system, coaches and analysts can find potential players suitable for the team and compare them on historical and expected performances. To explain the players' expected performances, the system provides a visual comparison between the simulated behaviors of the player and the actual ones. The usefulness and effectiveness of the system are demonstrated by two case studies on a real-world dataset and an expert interview.",
        "uid": "v-full-1031",
        "time_stamp": "",
        "time_start": "",
        "time_end": "",
        "paper_type": "full",
        "keywords": [],
        "doi": "",
        "fno": "",
        "presentation_mode": "",
        "open_access": false,
        "accessible_pdf": false,
        "has_image": false,
        "has_pdf": false,
        "paper_award": "",
        "image_caption": "",
        "external_paper_link": "",
        "youtube_ff_link": "",
        "youtube_ff_id": "",
        "bunny_ff_link": "",
        "bunny_ff_subtitles": "",
        "youtube_prerecorded_link": "",
        "youtube_prerecorded_id": "",
        "bunny_prerecorded_link": "",
        "bunny_prerecorded_subtitles": ""
    },
    "v-full-1032": {
        "slot_id": "v-full-1032",
        "session_id": "full0",
        "title": "Visualizing Temporal Topic Embeddings with a Compass",
        "contributors": [
            "Daniel Palamarchuk"
        ],
        "authors": [
            {
                "name": "Daniel Palamarchuk",
                "email": "d4n1elp@vt.edu",
                "affiliations": [
                    "Virginia Tech, Blacksburg, United States"
                ],
                "is_corresponding": true
            },
            {
                "name": "Lemara Williams",
                "email": "lemaraw@vt.edu",
                "affiliations": [
                    "Virginia Polytechnic Institute of Technology , Blacksburg, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Brian Mayer",
                "email": "bmayer@cs.vt.edu",
                "affiliations": [
                    "Virginia Tech, Blacksburg, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Thomas Danielson",
                "email": "thomas.danielson@srnl.doe.gov",
                "affiliations": [
                    "Savannah River National Laboratory, Aiken, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Rebecca Faust",
                "email": "rfaust1@tulane.edu",
                "affiliations": [
                    "Tulane University, New Orleans, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Larry M Deschaine PhD",
                "email": "larry.deschaine@srnl.doe.gov",
                "affiliations": [
                    "Savannah River National Laboratory, Aiken, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Chris North",
                "email": "north@vt.edu",
                "affiliations": [
                    "Virginia Tech, Blacksburg, United States"
                ],
                "is_corresponding": false
            }
        ],
        "abstract": "Dynamic topic modeling is useful at discovering the development and change in latent topics over time. However, present methodology relies on algorithms that separate document and word representations. This prevents the creation of a meaningful embedding space where changes in word usage and documents can be directly analyzed in a temporal context. This paper proposes an expansion of the compass-aligned temporal Word2Vec methodology into dynamic topic modeling. Such a method allows for the direct comparison of word and document embeddings across time in dynamic topics. This enables the creation of visualizations that incorporate diachronic word embeddings within the context of documents into topic visualizations. In experiments against the current state-of-the-art, our proposed method demonstrates overall competitive performance in topic relevancy and diversity across temporal datasets of varying size. Simultaneously, it provides insightful visualizations focused on temporal word embeddings while maintaining the insights provided by global topic evolution, advancing our understanding of how topics evolve over time.",
        "uid": "v-full-1032",
        "time_stamp": "",
        "time_start": "",
        "time_end": "",
        "paper_type": "full",
        "keywords": [],
        "doi": "",
        "fno": "",
        "presentation_mode": "",
        "open_access": false,
        "accessible_pdf": false,
        "has_image": false,
        "has_pdf": false,
        "paper_award": "",
        "image_caption": "",
        "external_paper_link": "",
        "youtube_ff_link": "",
        "youtube_ff_id": "",
        "bunny_ff_link": "",
        "bunny_ff_subtitles": "",
        "youtube_prerecorded_link": "",
        "youtube_prerecorded_id": "",
        "bunny_prerecorded_link": "",
        "bunny_prerecorded_subtitles": ""
    },
    "v-full-1039": {
        "slot_id": "v-full-1039",
        "session_id": "full0",
        "title": "Blowing Seeds Across Gardens: Visualizing Implicit Propagation of Cross-Platform Social Media Posts",
        "contributors": [
            "Jianing Yin"
        ],
        "authors": [
            {
                "name": "Jianing Yin",
                "email": "940662579@qq.com",
                "affiliations": [
                    "Zhejiang University, Hangzhou, China"
                ],
                "is_corresponding": true
            },
            {
                "name": "Hanze Jia",
                "email": "hzjia@zju.edu.cn",
                "affiliations": [
                    "Zhejiang University, Hangzhou, China"
                ],
                "is_corresponding": false
            },
            {
                "name": "Buwei Zhou",
                "email": "zhoubuwei@zju.edu.cn",
                "affiliations": [
                    "Zhejiang University, Hangzhou, China"
                ],
                "is_corresponding": false
            },
            {
                "name": "Tan Tang",
                "email": "tangtan@zju.edu.cn",
                "affiliations": [
                    "Zhejiang University, Hangzhou, China"
                ],
                "is_corresponding": false
            },
            {
                "name": "Lu Ying",
                "email": "yingluu@zju.edu.cn",
                "affiliations": [
                    "Zhejiang University, Hangzhou, China"
                ],
                "is_corresponding": false
            },
            {
                "name": "Shuainan Ye",
                "email": "sn_ye@zju.edu.cn",
                "affiliations": [
                    "Zhejiang University, Hangzhou, China"
                ],
                "is_corresponding": false
            },
            {
                "name": "Tai-Quan Peng",
                "email": "pengtaiq@msu.edu",
                "affiliations": [
                    "Michigan State University, East Lansing, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Yingcai Wu",
                "email": "ycwu@zju.edu.cn",
                "affiliations": [
                    "Zhejiang University, Hangzhou, China"
                ],
                "is_corresponding": false
            }
        ],
        "abstract": "Propagation analysis refers to studying how information spreads on social media, a pivotal endeavor for understanding social sentiment and public opinions. Numerous studies contribute to visualizing information spread, but few have considered the implicit and complex diffusion patterns among multiple platforms. To bridge the gap, we collaborated with professionals to discover crucial factors that dissect the mechanism of cross-platform information spread. Based on that, we propose an information diffusion model that estimates the likelihood of a topic/post spreading among different social media platforms. Moreover, we propose a novel visual metaphor that encapsulates cross-platform patterns in a manner analogous to the spread of seeds across gardens. Specifically, we visualize social platforms, posts, implicit cross-platform routes, and salient instances as elements of a virtual ecosystem \u2014 gardens, flowers, winds, and seeds, respectively. We further develop a visual analytic system, namely BloomWind, that enables users to quickly identify the cross-platform diffusion patterns and investigate the relevant social media posts. Ultimately, we demonstrate the usage of BloomWind through two case studies and validate its effectiveness using expert interviews.",
        "uid": "v-full-1039",
        "time_stamp": "",
        "time_start": "",
        "time_end": "",
        "paper_type": "full",
        "keywords": [],
        "doi": "",
        "fno": "",
        "presentation_mode": "",
        "open_access": false,
        "accessible_pdf": false,
        "has_image": false,
        "has_pdf": false,
        "paper_award": "",
        "image_caption": "",
        "external_paper_link": "",
        "youtube_ff_link": "",
        "youtube_ff_id": "",
        "bunny_ff_link": "",
        "bunny_ff_subtitles": "",
        "youtube_prerecorded_link": "",
        "youtube_prerecorded_id": "",
        "bunny_prerecorded_link": "",
        "bunny_prerecorded_subtitles": ""
    },
    "v-full-1059": {
        "slot_id": "v-full-1059",
        "session_id": "full0",
        "title": "DITTO: A Visual Digital Twin for Interventions and Temporal Treatment Outcomes in Head and Neck Cancer",
        "contributors": [
            "Andrew Wentzel"
        ],
        "authors": [
            {
                "name": "Andrew Wentzel",
                "email": "awentze2@uic.edu",
                "affiliations": [
                    "University of Illinois at Chicago, Chicago, United States"
                ],
                "is_corresponding": true
            },
            {
                "name": "Serageldin Attia",
                "email": "skattia@mdanderson.org",
                "affiliations": [
                    "University of Houston, Houston, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Xinhua Zhang",
                "email": "zhangz@uic.edu",
                "affiliations": [
                    "University of Illinois Chicago, Chicago, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Guadalupe Canahuate",
                "email": "guadalupe-canahuate@uiowa.edu",
                "affiliations": [
                    "University of Iowa, Iowa City, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Clifton David Fuller",
                "email": "cdfuller@mdanderson.org",
                "affiliations": [
                    "University of Texas, Houston, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "G. Elisabeta Marai",
                "email": "g.elisabeta.marai@gmail.com",
                "affiliations": [
                    "University of Illinois at Chicago, Chicago, United States"
                ],
                "is_corresponding": false
            }
        ],
        "abstract": "When treating Head and Neck cancer patients, oncologists have to navigate a complicated series of treatment decisions for each patient. The relationship between each treatment decision and the potential tradeoff of tumor control and toxicity risk is poorly understood, leaving oncologists to largely rely on institutional knowledge and general guidelines that do not take into account specific patient circumstances. Evaluating these risks relies on a complicated understanding of several different factors such as patient health, spatial tumor spread and treatment side effect risk that can not be captured through simple heuristics. To support clinicians in better understanding tradeoffs when deciding on treatment courses, we developed DITTO, a digital-twin and visual computing system that allows clinicians to analyze nuanced patient risk for each patient and decide on an optimal treatment plan. DITTO relies on a sequential Deep Reinforcement Learning (DRL) system to deliver personalized risk of both long-term and short-term disease outcome and toxicity risk for HNC patients. Based on a participatory collaborative design alongside oncologists, we also implement several explainability methods to support clinical trust and encourage healthy skepticism when using our models. We evaluate the efficacy of our model through quantitative evaluation of model performance and case studies with qualitative feedback. Finally, we discuss design lessons for developing clinical visual XAI applications for clinical end users.",
        "uid": "v-full-1059",
        "time_stamp": "",
        "time_start": "",
        "time_end": "",
        "paper_type": "full",
        "keywords": [],
        "doi": "",
        "fno": "",
        "presentation_mode": "",
        "open_access": false,
        "accessible_pdf": false,
        "has_image": false,
        "has_pdf": false,
        "paper_award": "",
        "image_caption": "",
        "external_paper_link": "",
        "youtube_ff_link": "",
        "youtube_ff_id": "",
        "bunny_ff_link": "",
        "bunny_ff_subtitles": "",
        "youtube_prerecorded_link": "",
        "youtube_prerecorded_id": "",
        "bunny_prerecorded_link": "",
        "bunny_prerecorded_subtitles": ""
    },
    "v-full-1060": {
        "slot_id": "v-full-1060",
        "session_id": "full0",
        "title": "From Instruction to Insight: Exploring the Semantic and Functional Roles of Text in Interactive Dashboards",
        "contributors": [
            "Nicole Sultanum"
        ],
        "authors": [
            {
                "name": "Nicole Sultanum",
                "email": "nicole.sultanum@gmail.com",
                "affiliations": [
                    "Tableau Research, Seattle, United States"
                ],
                "is_corresponding": true
            },
            {
                "name": "Vidya Setlur",
                "email": "vsetlur@tableau.com",
                "affiliations": [
                    "Tableau Research, Palo Alto, United States"
                ],
                "is_corresponding": false
            }
        ],
        "abstract": "There is increased interest in understanding the interplay between text and visuals in the field of data visualization. However, this attention has predominantly been on the use of text in standalone visualizations (such as text annotation overlays) or augmenting text stories supported by a series of independent views. In this paper, we shift from the traditional focus on single-chart annotations to characterize the nuanced but crucial communication role of text in the complex environment of interactive dashboards. Through a survey and analysis of 190 dashboards in the wild, plus 13 expert interview sessions with experienced dashboard authors, we highlight the distinctive nature of text as an integral component of the dashboard experience, while delving into the categories, semantic levels, and functional roles of text, and exploring how these text elements are coalesced by dashboard authors to guide and inform dashboard users. Our contributions are threefold. First, we distill qualitative and quantitative findings from our studies to characterize current practices of text use in dashboards, including a categorization of text-based components and design patterns. Second, we leverage current practices and existing literature to propose, discuss, and validate recommended practices for text in dashboards, embodied as a set of 12 heuristics that underscore the semantic and functional role of text in offering navigational cues, contextualizing data insights, supporting reading order, among other concerns. Third, we reflect on our findings plus existing literature to identify gaps and propose opportunities for data visualization researchers to push the boundaries on text usage for dashboards, from authoring support and interactivity to text generation and content personalization. Our research underscores the significance of elevating text as a first-class citizen in data visualization, and the need to support the inclusion of textual components and their interactive affordances in dashboard design.",
        "uid": "v-full-1060",
        "time_stamp": "",
        "time_start": "",
        "time_end": "",
        "paper_type": "full",
        "keywords": [],
        "doi": "",
        "fno": "",
        "presentation_mode": "",
        "open_access": false,
        "accessible_pdf": false,
        "has_image": false,
        "has_pdf": false,
        "paper_award": "",
        "image_caption": "",
        "external_paper_link": "",
        "youtube_ff_link": "",
        "youtube_ff_id": "",
        "bunny_ff_link": "",
        "bunny_ff_subtitles": "",
        "youtube_prerecorded_link": "",
        "youtube_prerecorded_id": "",
        "bunny_prerecorded_link": "",
        "bunny_prerecorded_subtitles": ""
    },
    "v-full-1063": {
        "slot_id": "v-full-1063",
        "session_id": "full0",
        "title": "DeLVE into Earth\u2019s Past: A Visualization-Based Exhibit Deployed Across Multiple Museum Contexts",
        "contributors": [
            "Mara Solen"
        ],
        "authors": [
            {
                "name": "Mara Solen",
                "email": "marasolen@gmail.com",
                "affiliations": [
                    "The University of British Columbia, Vancouver, Canada"
                ],
                "is_corresponding": true
            },
            {
                "name": "Nigar Sultana",
                "email": "sultananigar70@gmail.com",
                "affiliations": [
                    "University of British Columbia , Vancouver, Canada"
                ],
                "is_corresponding": false
            },
            {
                "name": "Laura A. Lukes",
                "email": "laura.lukes@ubc.ca",
                "affiliations": [
                    "University of British Columbia, Vancouver, Canada"
                ],
                "is_corresponding": false
            },
            {
                "name": "Tamara Munzner",
                "email": "tmm@cs.ubc.ca",
                "affiliations": [
                    "University of British Columbia, Vancouver, Canada"
                ],
                "is_corresponding": false
            }
        ],
        "abstract": "While previous work has found success in deploying visualizations as museum exhibits, it has not investigated whether museum context impacts visitor behaviour with these exhibits. We present an interactive Deep-time Literacy Visualization Exhibit (DeLVE) to help museum visitors understand deep time (lengths of extremely long geological processes) by improving proportional reasoning skills through comparison of different time periods. DeLVE uses a new visualization idiom, Connected Multi-Tier Ranges, to visualize curated datasets of past events across multiple scales of time, relating extreme scales with concrete scales that have more familiar magnitudes and units. Museum staff at three separate museums approved the deployment of DeLVE as a digital kiosk, and devoted time to curating a unique dataset in each of them. We collect data from two sources, an observational study and system trace logs. We discuss the importance of context: similar museum exhibits in different contexts were received very differently by visitors. We additionally discuss differences in our process from Sedlmair et al.'s design study methodology which is focused on design studies triggered by connection with collaborators rather than the discovery of a concept to communicate. Supplemental materials are available at: https://osf.io/z53dq/?view_only=4df33aad207144aca149982412125541",
        "uid": "v-full-1063",
        "time_stamp": "",
        "time_start": "",
        "time_end": "",
        "paper_type": "full",
        "keywords": [],
        "doi": "",
        "fno": "",
        "presentation_mode": "",
        "open_access": false,
        "accessible_pdf": false,
        "has_image": false,
        "has_pdf": false,
        "paper_award": "",
        "image_caption": "",
        "external_paper_link": "",
        "youtube_ff_link": "",
        "youtube_ff_id": "",
        "bunny_ff_link": "",
        "bunny_ff_subtitles": "",
        "youtube_prerecorded_link": "",
        "youtube_prerecorded_id": "",
        "bunny_prerecorded_link": "",
        "bunny_prerecorded_subtitles": ""
    },
    "v-full-1067": {
        "slot_id": "v-full-1067",
        "session_id": "full0",
        "title": "AdversaFlow: Visual Red Teaming for Large Language Models with Multi-Level Adversarial Flow",
        "contributors": [
            "Dazhen Deng"
        ],
        "authors": [
            {
                "name": "Dazhen Deng",
                "email": "dengdazhen@outlook.com",
                "affiliations": [
                    "Zhejiang University, Ningbo, China"
                ],
                "is_corresponding": true
            },
            {
                "name": "Chuhan Zhang",
                "email": "zhangchuhan024@163.com",
                "affiliations": [
                    "Zhejiang University, Hangzhou, China"
                ],
                "is_corresponding": false
            },
            {
                "name": "Huawei Zheng",
                "email": "huawzheng@gmail.com",
                "affiliations": [
                    "Zhejiang University, Hangzhou, China"
                ],
                "is_corresponding": false
            },
            {
                "name": "Yuwen Pu",
                "email": "yw.pu@zju.edu.cn",
                "affiliations": [
                    "Zhejiang University, Hangzhou, China"
                ],
                "is_corresponding": false
            },
            {
                "name": "Shouling Ji",
                "email": "sji@zju.edu.cn",
                "affiliations": [
                    "Zhejiang University, Hangzhou, China"
                ],
                "is_corresponding": false
            },
            {
                "name": "Yingcai Wu",
                "email": "ycwu@zju.edu.cn",
                "affiliations": [
                    "Zhejiang University, Hangzhou, China"
                ],
                "is_corresponding": false
            }
        ],
        "abstract": "Large Language Models (LLMs), such as ChatGPT and Llama, have revolutionized various domains through their impressive natural language processing capabilities. However, their deployment raises significant ethical and security concerns, including their potential misuse for generating fake news or aiding illegal activities. Thus, ensuring the development of secure and trustworthy LLMs is crucial. Traditional red teaming approaches for identifying vulnerabilities in AI models are limited by their reliance on manual prompt construction and expertise. This paper introduces a novel visual analytics system, AdversaFlow, designed to enhance the security of LLMs against adversarial attacks through human-AI collaboration. Our system, which involves adversarial training between a target model and a red model, is equipped with a unique multi-level adversarial flow visualization and a fluctuation path visualization technique. These features provide a detailed insight into the adversarial dynamics and the robustness of LLMs, thereby enabling AI security experts to identify and mitigate vulnerabilities effectively. We deliver quantitative evaluations for the models and present case studies that validate the utility of our system and share insights for future AI security solutions. Our contributions include a human-AI collaboration framework for LLM red teaming, a comprehensive visual analytics system to support adversarial pattern presentation and fluctuation analysis, and valuable lessons learned in visual analytics for AI security.",
        "uid": "v-full-1067",
        "time_stamp": "",
        "time_start": "",
        "time_end": "",
        "paper_type": "full",
        "keywords": [],
        "doi": "",
        "fno": "",
        "presentation_mode": "",
        "open_access": false,
        "accessible_pdf": false,
        "has_image": false,
        "has_pdf": false,
        "paper_award": "",
        "image_caption": "",
        "external_paper_link": "",
        "youtube_ff_link": "",
        "youtube_ff_id": "",
        "bunny_ff_link": "",
        "bunny_ff_subtitles": "",
        "youtube_prerecorded_link": "",
        "youtube_prerecorded_id": "",
        "bunny_prerecorded_link": "",
        "bunny_prerecorded_subtitles": ""
    },
    "v-full-1077": {
        "slot_id": "v-full-1077",
        "session_id": "full0",
        "title": "Entanglements for Visualization: Changing Research Outcomes through Feminist Theory",
        "contributors": [
            "Derya Akbaba"
        ],
        "authors": [
            {
                "name": "Derya Akbaba",
                "email": "derya.akbaba@liu.se",
                "affiliations": [
                    "Link\u00f6ping University, Norrk\u00f6ping, Sweden"
                ],
                "is_corresponding": true
            },
            {
                "name": "Lauren Klein",
                "email": "lauren.klein@emory.edu",
                "affiliations": [
                    "Emory University, Atlanta, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Miriah Meyer",
                "email": "miriah.meyer@liu.se",
                "affiliations": [
                    "Link\u00f6ping University, N\u00f6rrkoping, Sweden"
                ],
                "is_corresponding": false
            }
        ],
        "abstract": "A growing body of work draws on feminist thinking to challenge assumptions about how people engage with and use visualizations. This work draws on feminist values, driving design and research guidelines that account for the influences of power and neglect. This prior work is largely prescriptive, however, forgoing articulation of how feminist theories of knowledge \u2014 or feminist epistemology \u2014 can alter research design and outcomes. At the core of our work is an engagement with feminist epistemology, drawing attention to how a new framework for how we know what we know enabled us to overcome intellectual tensions in our research. Specifically, we focus on the theoretical concept of entanglement, central to recent feminist scholarship, and contribute: a history of entanglement in the broader scope of feminist theory; an articulation of the main points of entanglement theory for a visualization context; and a case study of research outcomes as evidence of the potential of feminist epistemology to impact visualization research. This work answers a call in the community to embrace a broader set of theoretical and epistemic foundations and provides a starting point for bringing different theories into visualization research.",
        "uid": "v-full-1077",
        "time_stamp": "",
        "time_start": "",
        "time_end": "",
        "paper_type": "full",
        "keywords": [],
        "doi": "",
        "fno": "",
        "presentation_mode": "",
        "open_access": false,
        "accessible_pdf": false,
        "has_image": false,
        "has_pdf": false,
        "paper_award": "",
        "image_caption": "",
        "external_paper_link": "",
        "youtube_ff_link": "",
        "youtube_ff_id": "",
        "bunny_ff_link": "",
        "bunny_ff_subtitles": "",
        "youtube_prerecorded_link": "",
        "youtube_prerecorded_id": "",
        "bunny_prerecorded_link": "",
        "bunny_prerecorded_subtitles": ""
    },
    "v-full-1096": {
        "slot_id": "v-full-1096",
        "session_id": "full0",
        "title": "Fine-Tuned Large Language Model for Visualization System: A Study on Self-Regulated Learning in Education",
        "contributors": [
            "Lin Gao"
        ],
        "authors": [
            {
                "name": "Lin Gao",
                "email": "lgao.lynne@gmail.com",
                "affiliations": [
                    "Fudan University, Shanghai, China"
                ],
                "is_corresponding": true
            },
            {
                "name": "Jing Lu",
                "email": "kingluther6666@gmail.com",
                "affiliations": [
                    "Fudan University, ShangHai, China"
                ],
                "is_corresponding": false
            },
            {
                "name": "Zekai Shao",
                "email": "gemini25szk@gmail.com",
                "affiliations": [
                    "Fudan University, Shanghai, China"
                ],
                "is_corresponding": false
            },
            {
                "name": "Ziyue Lin",
                "email": "ziyuelin917@gmail.com",
                "affiliations": [
                    "Fudan University, Shanghai, China"
                ],
                "is_corresponding": false
            },
            {
                "name": "Shengbin Yue",
                "email": "sbyue23@m.fudan.edu.cn",
                "affiliations": [
                    "Fudan unversity, ShangHai, China"
                ],
                "is_corresponding": false
            },
            {
                "name": "Chiokit Ieong",
                "email": "chiokit0819@gmail.com",
                "affiliations": [
                    "Fudan University, Shanghai, China"
                ],
                "is_corresponding": false
            },
            {
                "name": "Yi Sun",
                "email": "21307130094@m.fudan.edu.cn",
                "affiliations": [
                    "Fudan University, Shanghai, China"
                ],
                "is_corresponding": false
            },
            {
                "name": "Rory Zauner",
                "email": "rory.james.zauner@univie.ac.at",
                "affiliations": [
                    "University of Vienna, Vienna, Austria"
                ],
                "is_corresponding": false
            },
            {
                "name": "Zhongyu Wei",
                "email": "zywei@fudan.edu.cn",
                "affiliations": [
                    "Fudan University, Shanghai, China"
                ],
                "is_corresponding": false
            },
            {
                "name": "Siming Chen",
                "email": "simingchen3@gmail.com",
                "affiliations": [
                    "Fudan University, Shanghai, China"
                ],
                "is_corresponding": false
            }
        ],
        "abstract": "Large Language Models (LLMs) have shown great potential in intelligent visualization systems, especially for domain-specific applications. Integrating LLMs into visualization systems presents challenges, and we categorize these challenges into three alignments: domain problems with LLMs, visualization with LLMs, and interaction with LLMs. To achieve these alignments, we propose a framework and outline a workflow to guide the application of fine-tuned LLMs to enhance visual interactions for domain-specific tasks. These alignment challenges are critical in education as they call for an intelligent visualization system to support beginners' self-regulated learning. Therefore, we apply the framework to education and introduce Tailor-Mind, an interactive visualization system designed to facilitate self-regulated learning for artificial intelligence beginners. Drawing on insights from a preliminary study, we identify self-regulated learning tasks and fine-tuning objectives to guide visualization design and tuning data construction. Our focus on aligning visualization with fine-tuned LLM makes Tailor-Mind more like a personalized tutor. Tailor-Mind also supports interactive recommendations to help beginners better achieve their learning goals. Model performance evaluations and user studies confirm that Tailor-Mind improves the self-regulated learning experience, effectively validating the proposed framework.",
        "uid": "v-full-1096",
        "time_stamp": "",
        "time_start": "",
        "time_end": "",
        "paper_type": "full",
        "keywords": [],
        "doi": "",
        "fno": "",
        "presentation_mode": "",
        "open_access": false,
        "accessible_pdf": false,
        "has_image": false,
        "has_pdf": false,
        "paper_award": "",
        "image_caption": "",
        "external_paper_link": "",
        "youtube_ff_link": "",
        "youtube_ff_id": "",
        "bunny_ff_link": "",
        "bunny_ff_subtitles": "",
        "youtube_prerecorded_link": "",
        "youtube_prerecorded_id": "",
        "bunny_prerecorded_link": "",
        "bunny_prerecorded_subtitles": ""
    },
    "v-full-1099": {
        "slot_id": "v-full-1099",
        "session_id": "full0",
        "title": "Smartboard: Visual Exploration of Team Tactics with LLM Agent",
        "contributors": [
            "Ziao Liu"
        ],
        "authors": [
            {
                "name": "Ziao Liu",
                "email": "ziao_liu@outlook.com",
                "affiliations": [
                    "Zhejiang University, Hangzhou, China"
                ],
                "is_corresponding": true
            },
            {
                "name": "Xiao Xie",
                "email": "xxie@zju.edu.cn",
                "affiliations": [
                    "Zhejiang University, Hangzhou, China"
                ],
                "is_corresponding": false
            },
            {
                "name": "Moqi He",
                "email": "3170101799@zju.edu.cn",
                "affiliations": [
                    "Zhejiang University, Hangzhou, China"
                ],
                "is_corresponding": false
            },
            {
                "name": "Wenshuo Zhao",
                "email": "zhao_ws@zju.edu.cn",
                "affiliations": [
                    "Zhejiang University, Hangzhou, China"
                ],
                "is_corresponding": false
            },
            {
                "name": "Yihong Wu",
                "email": "wuyihong0606@gmail.com",
                "affiliations": [
                    "Zhejiang University, Hangzhou, China"
                ],
                "is_corresponding": false
            },
            {
                "name": "Liqi Cheng",
                "email": "lycheecheng@zju.edu.cn",
                "affiliations": [
                    "Zhejiang University, Hangzhou, China"
                ],
                "is_corresponding": false
            },
            {
                "name": "Hui Zhang",
                "email": "zhang_hui@zju.edu.cn",
                "affiliations": [
                    "Zhejiang University, Hangzhou, China"
                ],
                "is_corresponding": false
            },
            {
                "name": "Yingcai Wu",
                "email": "ycwu@zju.edu.cn",
                "affiliations": [
                    "Zhejiang University, Hangzhou, China"
                ],
                "is_corresponding": false
            }
        ],
        "abstract": "Tactics play an important role in team sports by guiding how players interact on the field. Both sports fans and experts have a demand for analyzing sports tactics. Existing approaches allow users to visually perceive the multivariate tactical effects. However, these approaches usually consider each tactic as a whole, making it difficult for users to connect the complex interactions inside each tactic to the final tactical effect. In this work, we collaborate with basketball experts and propose a progressive approach to help users gain a deeper understanding of how each tactic works and customize tactics on demand. Users can progressively sketch on a tactic board, and a coach agent will simulate the possible actions in each step and present the simulation to users with facet visualizations. We develop an extensible framework that integrates large language models (LLMs) and visualizations to help users communicate with the coach agent with multimodal inputs. Based on the framework, we design and develop Smartboard, an agent-based interactive visualization system for fine-grained tactical analysis. Smartboard provides users with a structured process of setup, simulation, and evolution, allowing for iterative exploration of tactics based on specific personalized scenarios. We conduct case studies based on real-world basketball datasets to demonstrate the usefulness of our system.",
        "uid": "v-full-1099",
        "time_stamp": "",
        "time_start": "",
        "time_end": "",
        "paper_type": "full",
        "keywords": [],
        "doi": "",
        "fno": "",
        "presentation_mode": "",
        "open_access": false,
        "accessible_pdf": false,
        "has_image": false,
        "has_pdf": false,
        "paper_award": "",
        "image_caption": "",
        "external_paper_link": "",
        "youtube_ff_link": "",
        "youtube_ff_id": "",
        "bunny_ff_link": "",
        "bunny_ff_subtitles": "",
        "youtube_prerecorded_link": "",
        "youtube_prerecorded_id": "",
        "bunny_prerecorded_link": "",
        "bunny_prerecorded_subtitles": ""
    },
    "v-full-1100": {
        "slot_id": "v-full-1100",
        "session_id": "full0",
        "title": "Causal Priors and Their Influence on Judgements of Causality in Visualized Data",
        "contributors": [
            "Arran Zeyu Wang"
        ],
        "authors": [
            {
                "name": "Arran Zeyu Wang",
                "email": "zeyuwang@cs.unc.edu",
                "affiliations": [
                    "University of North Carolina-Chapel Hill, Chapel Hill, United States"
                ],
                "is_corresponding": true
            },
            {
                "name": "David Borland",
                "email": "borland@renci.org",
                "affiliations": [
                    "UNC-Chapel Hill, Chapel Hill, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Tabitha C. Peck",
                "email": "tapeck@davidson.edu",
                "affiliations": [
                    "Davidson College, Davidson, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Wenyuan Wang",
                "email": "vaapad@live.unc.edu",
                "affiliations": [
                    "University of North Carolina, Chapel Hill, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "David Gotz",
                "email": "gotz@unc.edu",
                "affiliations": [
                    "University of North Carolina, Chapel Hill, United States"
                ],
                "is_corresponding": false
            }
        ],
        "abstract": "\u201cCorrelation does not imply causation\u201d is a famous mantra in statistical and visual analysis. However, consumers of visualizations often draw causal conclusions when only correlations between variables are shown. In this paper, we investigate factors that contribute to causal relationships users perceive in visualizations. We collected a corpus of concept pairs from variables in widely used datasets and created visualizations that depict varying correlative associations using three typical statistical chart types. We conducted two MTurk studies on (1) preconceived notions on causal relations without charts, and (2) perceived causal relations with charts, for each concept pair. Our results indicate that people make assumptions about causal relationships between pairs of concepts even without seeing any visualized data. Moreover, our results suggest that these assumptions constitute causal priors that, in combination with chart type and visualized association, impact how data visualizations are interpreted. The results also suggest that causal priors may lead to over- or under-estimation in perceived causal relations in different circumstances, and that those priors can also impact users\u2019 confidence in their causal assessments. Using data from the studies, we develop a model to capture the interaction between causal priors and visualized associations as they combine to impact a user\u2019s perceived causal relations. In addition to reporting the study results and analyses, we provide an open dataset of causal priors for 56 specific concept pairs that can serve as a potential benchmark for future studies. We also suggest heuristic-based guidelines to help designers improve visualization design choices to better support visual causal inference.",
        "uid": "v-full-1100",
        "time_stamp": "",
        "time_start": "",
        "time_end": "",
        "paper_type": "full",
        "keywords": [],
        "doi": "",
        "fno": "",
        "presentation_mode": "",
        "open_access": false,
        "accessible_pdf": false,
        "has_image": false,
        "has_pdf": false,
        "paper_award": "",
        "image_caption": "",
        "external_paper_link": "",
        "youtube_ff_link": "",
        "youtube_ff_id": "",
        "bunny_ff_link": "",
        "bunny_ff_subtitles": "",
        "youtube_prerecorded_link": "",
        "youtube_prerecorded_id": "",
        "bunny_prerecorded_link": "",
        "bunny_prerecorded_subtitles": ""
    },
    "v-full-1121": {
        "slot_id": "v-full-1121",
        "session_id": "full0",
        "title": "PhenoFlow: A Human-LLM Driven Visual Analytics System for Exploring Large and Complex Stroke Datasets",
        "contributors": [
            "Jaeyoung Kim"
        ],
        "authors": [
            {
                "name": "Jaeyoung Kim",
                "email": "jykim@hcil.snu.ac.kr",
                "affiliations": [
                    "Seoul National University, Seoul, Korea, Republic of"
                ],
                "is_corresponding": true
            },
            {
                "name": "Sihyeon Lee",
                "email": "sihyeon@hcil.snu.ac.kr",
                "affiliations": [
                    "Seoul National University, Seoul, Korea, Republic of"
                ],
                "is_corresponding": false
            },
            {
                "name": "Hyeon Jeon",
                "email": "hj@hcil.snu.ac.kr",
                "affiliations": [
                    "Seoul National University, Seoul, Korea, Republic of"
                ],
                "is_corresponding": false
            },
            {
                "name": "Keon-Joo Lee",
                "email": "gooday19@gmail.com",
                "affiliations": [
                    "Korea University Guro Hospital, Seoul, Korea, Republic of"
                ],
                "is_corresponding": false
            },
            {
                "name": "Bohyoung Kim",
                "email": "bkim@hufs.ac.kr",
                "affiliations": [
                    "Hankuk University of Foreign Studies, Yongin-si, Korea, Republic of"
                ],
                "is_corresponding": false
            },
            {
                "name": "HEE JOON",
                "email": "braindoc@snu.ac.kr",
                "affiliations": [
                    "Seoul National University Bundang Hospital, Seongnam, Korea, Republic of"
                ],
                "is_corresponding": false
            },
            {
                "name": "Jinwook Seo",
                "email": "jseo@snu.ac.kr",
                "affiliations": [
                    "Seoul National University, Seoul, Korea, Republic of"
                ],
                "is_corresponding": false
            }
        ],
        "abstract": "Acute stroke demands prompt diagnosis and treatment to achieve optimal patient outcomes. However, the intricate and irregular nature of clinical data associated with acute stroke, particularly blood pressure (BP) measurements, presents substantial obstacles to effective visual analytics and decision-making. Through a year-long collaboration with experienced neurologists, we developed PhenoFlow, a visual analytics system that leverages the collaboration between human and Large Language Models (LLMs) to analyze the extensive and complex data of acute ischemic stroke patients. PhenoFlow pioneers an innovative workflow, where the LLM serves as a data wrangler while neurologists explore and supervise the output using visualizations and natural language interactions. This approach enables neurologists to focus more on decision-making with reduced cognitive load. To protect sensitive patient information, PhenoFlow only utilizes metadata to make inferences and synthesize executable codes, without accessing raw patient data. This ensures that the results are both reproducible and interpretable while maintaining patient privacy. The system incorporates a slice-and-wrap design that employs temporal folding to create an overlaid circular visualization. Combined with a linear bar graph, this design aids in exploring meaningful patterns within irregularly measured BP data. Through case studies, PhenoFlow has demonstrated its capability to support iterative analysis of extensive clinical datasets, reducing cognitive load and enabling neurologists to make well-informed decisions. Grounded in long-term collaboration with domain experts, our research demonstrates the potential of utilizing LLMs to tackle current challenges in data-driven clinical decision-making for acute ischemic stroke patients.",
        "uid": "v-full-1121",
        "time_stamp": "",
        "time_start": "",
        "time_end": "",
        "paper_type": "full",
        "keywords": [],
        "doi": "",
        "fno": "",
        "presentation_mode": "",
        "open_access": false,
        "accessible_pdf": false,
        "has_image": false,
        "has_pdf": false,
        "paper_award": "",
        "image_caption": "",
        "external_paper_link": "",
        "youtube_ff_link": "",
        "youtube_ff_id": "",
        "bunny_ff_link": "",
        "bunny_ff_subtitles": "",
        "youtube_prerecorded_link": "",
        "youtube_prerecorded_id": "",
        "bunny_prerecorded_link": "",
        "bunny_prerecorded_subtitles": ""
    },
    "v-full-1128": {
        "slot_id": "v-full-1128",
        "session_id": "full0",
        "title": "PUREsuggest: Citation-based Literature Search and Visual Exploration with Keyword-controlled Rankings",
        "contributors": [
            "Fabian Beck"
        ],
        "authors": [
            {
                "name": "Fabian Beck",
                "email": "fabian.beck@uni-bamberg.de",
                "affiliations": [
                    "University of Bamberg, Bamberg, Germany"
                ],
                "is_corresponding": true
            }
        ],
        "abstract": "Citations allow quickly identifying related research. If multiple publications are selected as seeds, specific suggestions for related literature can be made based on the number of incoming and outgoing citation links to this selection. Interactively adding recommended publications to the selection refines the next suggestion and incrementally builds a relevant collection of publications. Following this approach, the paper presents a search and foraging approach, PUREsuggest, which combines citation-based suggestions with augmented visualizations of the citation network. The focus and novelty of the approach is, first, the transparency of how the rankings are explained visually and, second, that the process can be steered through user-defined keywords, which reflect topics of interests. The system can be used to build new literature collections, to update and assess existing ones, as well as to use the collected literature for identifying relevant experts in the field. We evaluated the recommendation approach through simulated sessions and performed a user study investigating search strategies and usage patterns supported by the interface.",
        "uid": "v-full-1128",
        "time_stamp": "",
        "time_start": "",
        "time_end": "",
        "paper_type": "full",
        "keywords": [],
        "doi": "",
        "fno": "",
        "presentation_mode": "",
        "open_access": false,
        "accessible_pdf": false,
        "has_image": false,
        "has_pdf": false,
        "paper_award": "",
        "image_caption": "",
        "external_paper_link": "",
        "youtube_ff_link": "",
        "youtube_ff_id": "",
        "bunny_ff_link": "",
        "bunny_ff_subtitles": "",
        "youtube_prerecorded_link": "",
        "youtube_prerecorded_id": "",
        "bunny_prerecorded_link": "",
        "bunny_prerecorded_subtitles": ""
    },
    "v-full-1137": {
        "slot_id": "v-full-1137",
        "session_id": "full0",
        "title": "Touching the Ground: Evaluating the Effectiveness of Data Physicalizations for Spatial Data Analysis Tasks",
        "contributors": [
            "Bridger Herman"
        ],
        "authors": [
            {
                "name": "Bridger Herman",
                "email": "bridger.g.herman@gmail.com",
                "affiliations": [
                    "University of Minnesota, Minneapolis, United States"
                ],
                "is_corresponding": true
            },
            {
                "name": "Cullen D. Jackson",
                "email": "cdjackso@bidmc.harvard.edu",
                "affiliations": [
                    "Beth Israel Deaconess Medical Center, Boston, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Daniel F. Keefe",
                "email": "dfk@umn.edu",
                "affiliations": [
                    "University of Minnesota, Minneapolis, United States"
                ],
                "is_corresponding": false
            }
        ],
        "abstract": "Inspired by recent advances in digital fabrication, artists and scientists have demonstrated that physical data encodings (i.e., data physicalizations) can increase engagement with data, foster collaboration, and in some cases, improve data legibility and analysis relative to digital alternatives. However, prior empirical studies have only investigated abstract data encoded in physical form (e.g., laser cut bar charts) and not continuously sampled spatial data fields relevant to climate and medical science (e.g., heights, temperatures, densities, and velocities sampled on a spatial grid). This paper presents the design and results of the first study to characterize human performance in 3D spatial data analysis tasks across analogous physical and digital visualizations. Participants analyzed continuous spatial elevation data with three visualization modalities: (1) 2D digital visualization; (2) perspective-tracked, stereoscopic \"fishtank\" virtual reality; and (3) 3D printed data physicalization. Their tasks included tracing paths downhill, looking up spatial locations and comparing their relative heights, and identifying and reporting the minimum and maximum heights within certain spatial regions. As hypothesized, in most cases, participants performed the tasks just as well or better in the physical modality (based on time and error metrics). Additional results include an analysis of open-ended feedback from participants and discussion of implications for further research on the value of data physicalization. All data and supplemental materials are available at https://osf.io/7xdq4/?view_only=7416f8cfca85473889456fb69527abbc",
        "uid": "v-full-1137",
        "time_stamp": "",
        "time_start": "",
        "time_end": "",
        "paper_type": "full",
        "keywords": [],
        "doi": "",
        "fno": "",
        "presentation_mode": "",
        "open_access": false,
        "accessible_pdf": false,
        "has_image": false,
        "has_pdf": false,
        "paper_award": "",
        "image_caption": "",
        "external_paper_link": "",
        "youtube_ff_link": "",
        "youtube_ff_id": "",
        "bunny_ff_link": "",
        "bunny_ff_subtitles": "",
        "youtube_prerecorded_link": "",
        "youtube_prerecorded_id": "",
        "bunny_prerecorded_link": "",
        "bunny_prerecorded_subtitles": ""
    },
    "v-full-1140": {
        "slot_id": "v-full-1140",
        "session_id": "full0",
        "title": "It's a Good Idea to Put It Into Words: Writing 'Rudders' in the Initial Stages of Visualization Design",
        "contributors": [
            "Chase Stokes"
        ],
        "authors": [
            {
                "name": "Chase Stokes",
                "email": "chase_stokes@berkeley.edu",
                "affiliations": [
                    "UC Berkeley, Berkeley, United States"
                ],
                "is_corresponding": true
            },
            {
                "name": "Clara Hu",
                "email": "clarahu@berkeley.edu",
                "affiliations": [
                    "Self, Berkeley, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Marti Hearst",
                "email": "hearst@berkeley.edu",
                "affiliations": [
                    "UC Berkeley, Berkeley, United States"
                ],
                "is_corresponding": false
            }
        ],
        "abstract": "Written language is a useful mode for non-visual creative activities like writing essays and planning searches. This paper investigates the integration of written language into the visualization design process. We call this idea a `written rudder,' , since it acts as a guiding force or strategy for the design. Via an interview study of 24 working visualization designers, we first established that only a minority of participants systematically use written rudders to aid in design. A second study with 15 visualization designers examined four different variants of rudders: asking questions, stating conclusions, composing a narrative, and writing titles. Overall, participants had a positive reaction; designers recognized the benefits of explicitly writing down components of the design and indicated that they would use this approach in future design work.  More specifically, two approaches \u2013- writing questions and writing conclusions/takeaways \u2013- were seen as beneficial across the design process, while writing narratives showed promise mainly for the creation stage. Although concerns around potential bias during data exploration were raised, participants also discussed strategies to mitigate such concerns. This paper contributes to a deeper understanding of the interplay between language and visualization, and proposes a straightforward, lightweight addition to the visualization design process.",
        "uid": "v-full-1140",
        "time_stamp": "",
        "time_start": "",
        "time_end": "",
        "paper_type": "full",
        "keywords": [],
        "doi": "",
        "fno": "",
        "presentation_mode": "",
        "open_access": false,
        "accessible_pdf": false,
        "has_image": false,
        "has_pdf": false,
        "paper_award": "",
        "image_caption": "",
        "external_paper_link": "",
        "youtube_ff_link": "",
        "youtube_ff_id": "",
        "bunny_ff_link": "",
        "bunny_ff_subtitles": "",
        "youtube_prerecorded_link": "",
        "youtube_prerecorded_id": "",
        "bunny_prerecorded_link": "",
        "bunny_prerecorded_subtitles": ""
    },
    "v-full-1142": {
        "slot_id": "v-full-1142",
        "session_id": "full0",
        "title": "Compress and Compare: Interactively Evaluating Efficiency and Behavior Across ML Model Compression Experiments",
        "contributors": [
            "Angie Boggust"
        ],
        "authors": [
            {
                "name": "Angie Boggust",
                "email": "aboggust@mit.edu",
                "affiliations": [
                    "Massachusetts Institute of Technology, Cambridge, United States"
                ],
                "is_corresponding": true
            },
            {
                "name": "Venkatesh Sivaraman",
                "email": "vsivaram@andrew.cmu.edu",
                "affiliations": [
                    "Carnegie Mellon University, Pittsburgh, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Yannick Assogba",
                "email": "yassogba@gmail.com",
                "affiliations": [
                    "Apple, Cambridge, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Donghao Ren",
                "email": "donghao@apple.com",
                "affiliations": [
                    "Apple, Seattle, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Dominik Moritz",
                "email": "domoritz@cmu.edu",
                "affiliations": [
                    "Apple, Pittsburgh, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Fred Hohman",
                "email": "fred.hohman@gmail.com",
                "affiliations": [
                    "Apple, Seattle, United States"
                ],
                "is_corresponding": false
            }
        ],
        "abstract": "To deploy machine learning (ML) models on-device, practitioners use compression algorithms to shrink and speed up models while maintaining their high-quality output. A critical aspect of compression in practice is model comparison, including tracking many compression experiments, identifying subtle changes in model behavior, and negotiating complex accuracy-efficiency trade-offs. However, existing compression tools poorly support comparison, leading to tedious and, sometimes, incomplete analyses spread across disjoint tools. To support real-world comparative workflows, we develop an interactive visual system called Compress & Compare. Within a single interface, Compress & Compare surfaces promising compression strategies by visualizing provenance relationships between compressed models and reveals compression-induced behavior changes by comparing models' predictions, weights, and activations. We demonstrate how Compress & Compare supports common compression analysis tasks through two case studies\u2014debugging failed compression on generative language models and identifying compression-induced biases in image classification. We further evaluate Compress & Compare in a user study with eight compression experts, illustrating its potential to provide structure to compression workflows, help practitioners build intuition about compression, and encourage thorough analysis of compression\u2019s effect on model behavior. Through these evaluations, we identify compression-specific challenges that future visual analytics tools should consider and Compress & Compare visualizations that may generalize to broader model comparison tasks.",
        "uid": "v-full-1142",
        "time_stamp": "",
        "time_start": "",
        "time_end": "",
        "paper_type": "full",
        "keywords": [],
        "doi": "",
        "fno": "",
        "presentation_mode": "",
        "open_access": false,
        "accessible_pdf": false,
        "has_image": false,
        "has_pdf": false,
        "paper_award": "",
        "image_caption": "",
        "external_paper_link": "",
        "youtube_ff_link": "",
        "youtube_ff_id": "",
        "bunny_ff_link": "",
        "bunny_ff_subtitles": "",
        "youtube_prerecorded_link": "",
        "youtube_prerecorded_id": "",
        "bunny_prerecorded_link": "",
        "bunny_prerecorded_subtitles": ""
    },
    "v-full-1147": {
        "slot_id": "v-full-1147",
        "session_id": "full0",
        "title": "An Empirical Evaluation of the GPT-4 Multimodal Language Model on Visualization Literacy Tasks",
        "contributors": [
            "Alexander Bendeck"
        ],
        "authors": [
            {
                "name": "Alexander Bendeck",
                "email": "abendeck3@gatech.edu",
                "affiliations": [
                    "Georgia Institute of Technology, Atlanta, United States"
                ],
                "is_corresponding": true
            },
            {
                "name": "John Stasko",
                "email": "john.stasko@cc.gatech.edu",
                "affiliations": [
                    "Georgia Institute of Technology, Atlanta, United States"
                ],
                "is_corresponding": false
            }
        ],
        "abstract": "Large Language Models (LLMs) like GPT-4 which support multimodal input (i.e., prompts containing images in addition to text) have immense potential to advance visualization research. However, many questions exist about the visual capabilities of such models, including how well they can read and interpret visually represented data. In our work, we address this question by evaluating the GPT-4 multimodal LLM using a suite of task sets meant to assess the model\u2019s visualization literacy. The task sets are based on existing work in the visualization community addressing both automated chart question answering and human visualization literacy across multiple settings. Our assessment finds that GPT-4 can perform tasks such as recognizing trends and extreme values, and also demonstrates some understanding of visualization design best-practices. By contrast, GPT-4 struggles with simple value retrieval when not provided with the original dataset, lacks the ability to reliably distinguish between colors in charts, and occasionally suffers from hallucination and inconsistency. We conclude by reflecting on the model\u2019s strengths and weaknesses as well as the potential utility of models like GPT-4 for future visualization research. We also release all code, stimuli, and results for the task sets at the following link: (REDACTED FOR REVIEW)",
        "uid": "v-full-1147",
        "time_stamp": "",
        "time_start": "",
        "time_end": "",
        "paper_type": "full",
        "keywords": [],
        "doi": "",
        "fno": "",
        "presentation_mode": "",
        "open_access": false,
        "accessible_pdf": false,
        "has_image": false,
        "has_pdf": false,
        "paper_award": "",
        "image_caption": "",
        "external_paper_link": "",
        "youtube_ff_link": "",
        "youtube_ff_id": "",
        "bunny_ff_link": "",
        "bunny_ff_subtitles": "",
        "youtube_prerecorded_link": "",
        "youtube_prerecorded_id": "",
        "bunny_prerecorded_link": "",
        "bunny_prerecorded_subtitles": ""
    },
    "v-full-1150": {
        "slot_id": "v-full-1150",
        "session_id": "full0",
        "title": "CompositingVis: Exploring Interaction for Creating Composite Visualizations in Immersive Environments",
        "contributors": [
            "Qian Zhu"
        ],
        "authors": [
            {
                "name": "Qian Zhu",
                "email": "qzhual@connect.ust.hk",
                "affiliations": [
                    "The Hong Kong University of Science and Technology, Hong Kong, China",
                    "The Hong Kong University of Science and Technology, Hong Kong, China"
                ],
                "is_corresponding": true
            },
            {
                "name": "Tao Lu",
                "email": "luttul@umich.edu",
                "affiliations": [
                    "Georgia Institute of Technology, Atlanta, United States",
                    "Georgia Institute of Technology, Atlanta, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Shunan Guo",
                "email": "sguo@adobe.com",
                "affiliations": [
                    "Adobe Research, San Jose, United States",
                    "Adobe Research, San Jose, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Xiaojuan Ma",
                "email": "mxj@cse.ust.hk",
                "affiliations": [
                    "Hong Kong University of Science and Technology, Hong Kong, Hong Kong",
                    "Hong Kong University of Science and Technology, Hong Kong, Hong Kong"
                ],
                "is_corresponding": false
            },
            {
                "name": "Yalong Yang",
                "email": "yalongyang@hotmail.com",
                "affiliations": [
                    "Georgia Institute of Technology, Atlanta, United States",
                    "Georgia Institute of Technology, Atlanta, United States"
                ],
                "is_corresponding": false
            }
        ],
        "abstract": "Composite visualization represents a widely embraced design that combines multiple visual representations to create an integrated view.  However, the traditional approach of creating composite visualizations in immersive environments typically occurs asynchronously outside of the immersive space and is carried out by experienced experts.  In this work, we take the first step to empower users to participate in the creation of composite visualization within immersive environments through embodied interactions.  This could provide a flexible and fluid experience for data exploration and facilitate a deep understanding of the relationship between data visualizations. We begin with forming a design space of embodied interactions to create various types of composite visualizations with the consideration of data relationships. Drawing inspiration from people's natural experience of manipulating physical objects, we design interactions to directly assemble composite visualizations in immersive environments. Building upon the design space, we present a series of case studies showcasing the interactive method to create different kinds of composite visualizations in Virtual Reality (VR). Subsequently, we conduct a user study to evaluate the usability of the derived interaction techniques and user experience of embodiedly creating composite visualizations.  We find that empowering users to participate in composite visualizations through embodied interactions enables them to flexibly leverage different visualization representations for understanding and communicating the relationships between different views, which underscores the potential for a set of application scenarios in the future.",
        "uid": "v-full-1150",
        "time_stamp": "",
        "time_start": "",
        "time_end": "",
        "paper_type": "full",
        "keywords": [],
        "doi": "",
        "fno": "",
        "presentation_mode": "",
        "open_access": false,
        "accessible_pdf": false,
        "has_image": false,
        "has_pdf": false,
        "paper_award": "",
        "image_caption": "",
        "external_paper_link": "",
        "youtube_ff_link": "",
        "youtube_ff_id": "",
        "bunny_ff_link": "",
        "bunny_ff_subtitles": "",
        "youtube_prerecorded_link": "",
        "youtube_prerecorded_id": "",
        "bunny_prerecorded_link": "",
        "bunny_prerecorded_subtitles": ""
    },
    "v-full-1153": {
        "slot_id": "v-full-1153",
        "session_id": "full0",
        "title": "SimpleSets: Capturing Categorical Point Patterns with Simple Shapes",
        "contributors": [
            "Steven van den Broek"
        ],
        "authors": [
            {
                "name": "Steven van den Broek",
                "email": "s.w.v.d.broek@tue.nl",
                "affiliations": [
                    "TU Eindhoven, Eindhoven, Netherlands"
                ],
                "is_corresponding": true
            },
            {
                "name": "Wouter Meulemans",
                "email": "w.meulemans@tue.nl",
                "affiliations": [
                    "TU Eindhoven, Eindhoven, Netherlands"
                ],
                "is_corresponding": false
            },
            {
                "name": "Bettina Speckmann",
                "email": "b.speckmann@tue.nl",
                "affiliations": [
                    "TU Eindhoven, Eindhoven, Netherlands"
                ],
                "is_corresponding": false
            }
        ],
        "abstract": "Points of interest on a map such as restaurants, hotels, or subway stations, give rise to categorical point data: data that have a fixed location and one or more categorical attributes. Consequently, recent years have seen various set visualization approaches that visually connect points of the same category to support users in understanding the spatial distribution of categories. Existing methods use complex and often highly irregular shapes to connect points of the same category, leading to high cognitive load for the user. In this paper we introduce SimpleSets that use simple shapes to enclose categorical point patterns and provide a low-complexity overview of the data distribution. We give formal definitions of point patterns that correspond to simple shapes and describe an algorithm that partitions categorical points into few such patterns. Our second contribution is a rendering algorithm that transforms a given partition into a clean set of shapes resulting in an aesthetically pleasing set visualization. Our algorithm pays particular attention to resolving intersections between nearby shapes in a consistent manner. We compare SimpleSets to the state-of-the-art set visualizations using standard datasets from the literature. SimpleSets are designed to visualize disjoint categories, however, we discuss avenues to extend our technique to overlapping set systems.",
        "uid": "v-full-1153",
        "time_stamp": "",
        "time_start": "",
        "time_end": "",
        "paper_type": "full",
        "keywords": [],
        "doi": "",
        "fno": "",
        "presentation_mode": "",
        "open_access": false,
        "accessible_pdf": false,
        "has_image": false,
        "has_pdf": false,
        "paper_award": "",
        "image_caption": "",
        "external_paper_link": "",
        "youtube_ff_link": "",
        "youtube_ff_id": "",
        "bunny_ff_link": "",
        "bunny_ff_subtitles": "",
        "youtube_prerecorded_link": "",
        "youtube_prerecorded_id": "",
        "bunny_prerecorded_link": "",
        "bunny_prerecorded_subtitles": ""
    },
    "v-full-1155": {
        "slot_id": "v-full-1155",
        "session_id": "full0",
        "title": "Charting EDA: How Visualizations and Interactions Shape Analysis in Computational Notebooks.",
        "contributors": [
            "Dylan Wootton"
        ],
        "authors": [
            {
                "name": "Dylan Wootton",
                "email": "dwootton@mit.edu",
                "affiliations": [
                    "MIT, Cambridge, United States"
                ],
                "is_corresponding": true
            },
            {
                "name": "Amy Rae Fox",
                "email": "amyraefoxphd@gmail.com",
                "affiliations": [
                    "MIT, Cambridge, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Evan Peck",
                "email": "evan.peck@colorado.edu",
                "affiliations": [
                    "University of Colorado Boulder, Boulder, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Arvind Satyanarayan",
                "email": "arvindsatya@mit.edu",
                "affiliations": [
                    "MIT, Cambridge, United States"
                ],
                "is_corresponding": false
            }
        ],
        "abstract": "Interactive visualizations are powerful tools for Exploratory Data Analysis (EDA), but how do they affect the observations analysts make about their data? We conducted a qualitative experiment with 13 professional data scientists analyzing two datasets within Jupyter notebooks, collecting a rich dataset of interaction traces and think-aloud utterances. By qualitatively analyzing participant verbalizations, we introduce the concept of \"observation-analysis states.\" These states capture both the dataset characteristics a participant focuses on and the insights they express. Our definition reveals that interactive visualizations on average lead to earlier and more complex insights about relationships between dataset attributes compared to static visualizations. Moreover, this process identified new measures for studying representation use in notebooks such as hover time, revisiting rate and representational diversity. In particular, revisiting rates revealed behavior where analysts revisit particular representations throughout the time course of an analysis, serving more as navigational aids through an EDA than as strict hypothesis answering tools. We show how these measures helped identify other patterns of analysis behavior, such as the \"80-20 rule\", where a small subset of representations drove the majority of observations. Based on these findings, we offer design guidelines for interactive exploratory analysis tooling and reflect on future directions for studying the role that visualizations play in EDA.",
        "uid": "v-full-1155",
        "time_stamp": "",
        "time_start": "",
        "time_end": "",
        "paper_type": "full",
        "keywords": [],
        "doi": "",
        "fno": "",
        "presentation_mode": "",
        "open_access": false,
        "accessible_pdf": false,
        "has_image": false,
        "has_pdf": false,
        "paper_award": "",
        "image_caption": "",
        "external_paper_link": "",
        "youtube_ff_link": "",
        "youtube_ff_id": "",
        "bunny_ff_link": "",
        "bunny_ff_subtitles": "",
        "youtube_prerecorded_link": "",
        "youtube_prerecorded_id": "",
        "bunny_prerecorded_link": "",
        "bunny_prerecorded_subtitles": ""
    },
    "v-full-1179": {
        "slot_id": "v-full-1179",
        "session_id": "full0",
        "title": "ParetoTracker: Understanding Population Dynamics in Multi-objective Evolutionary Algorithms through Visual Analytics",
        "contributors": [
            "Yuxin Ma"
        ],
        "authors": [
            {
                "name": "Zherui Zhang",
                "email": "zhangzr32021@mail.sustech.edu.cn",
                "affiliations": [
                    "Southern University of Science and Technology, Shenzhen, China"
                ],
                "is_corresponding": false
            },
            {
                "name": "Fan Yang",
                "email": "yangf2020@mail.sustech.edu.cn",
                "affiliations": [
                    "Southern University of Science and Technology, Shenzhen, China"
                ],
                "is_corresponding": false
            },
            {
                "name": "Ran Cheng",
                "email": "ranchengcn@gmail.com",
                "affiliations": [
                    "Southern University of Science and Technology, Shenzhen, China"
                ],
                "is_corresponding": false
            },
            {
                "name": "Yuxin Ma",
                "email": "mayx@sustech.edu.cn",
                "affiliations": [
                    "Southern University of Science and Technology, Shenzhen, China"
                ],
                "is_corresponding": true
            }
        ],
        "abstract": "Multi-objective evolutionary algorithms (MOEAs) have emerged as powerful tools for solving complex optimization problems characterized by multiple, often conflicting, objectives. While advancements have been made in computational efficiency as well as diversity and convergence of solutions, a critical challenge persists: the internal evolutionary mechanisms are opaque to human users. Drawing upon the successes of explainable AI in explaining complex algorithms and models, we argue that the need to understand the underlying evolutionary operators and population dynamics in MOEAs aligns well with a visual analytics paradigm. This paper introduces ParetoTracker, a visual analytics framework designed to support the comprehension and inspection of population dynamics in the evolutionary processes of MOEAs. Informed by preliminary literature review and expert interviews, the framework establishes a multi-level analysis scheme, which caters to user engagement and exploration ranging from examining overall trends in performance metrics to conducting fine-grained inspections of evolutionary operations. In contrast to conventional practices that require manual plotting of solutions for each generation, ParetoTracker facilitates the examination of temporal trends and dynamics across consecutive generations in an integrated visual interface. The effectiveness of the framework is demonstrated through case studies and expert interviews focused on widely adopted benchmark optimization problems.",
        "uid": "v-full-1179",
        "time_stamp": "",
        "time_start": "",
        "time_end": "",
        "paper_type": "full",
        "keywords": [],
        "doi": "",
        "fno": "",
        "presentation_mode": "",
        "open_access": false,
        "accessible_pdf": false,
        "has_image": false,
        "has_pdf": false,
        "paper_award": "",
        "image_caption": "",
        "external_paper_link": "",
        "youtube_ff_link": "",
        "youtube_ff_id": "",
        "bunny_ff_link": "",
        "bunny_ff_subtitles": "",
        "youtube_prerecorded_link": "",
        "youtube_prerecorded_id": "",
        "bunny_prerecorded_link": "",
        "bunny_prerecorded_subtitles": ""
    },
    "v-full-1185": {
        "slot_id": "v-full-1185",
        "session_id": "full0",
        "title": "Does This Have a Particular Meaning?: Interactive Pattern Explanation for Network Visualizations",
        "contributors": [
            "Xinhuan Shu"
        ],
        "authors": [
            {
                "name": "Xinhuan Shu",
                "email": "xinhuan.shu@gmail.com",
                "affiliations": [
                    "Newcastle University, Newcastle Upon Tyne, United Kingdom"
                ],
                "is_corresponding": true
            },
            {
                "name": "Alexis Pister",
                "email": "alexis.pister@hotmail.com",
                "affiliations": [
                    "University of Edinburgh, Edinburgh, United Kingdom"
                ],
                "is_corresponding": false
            },
            {
                "name": "Junxiu Tang",
                "email": "tangjunxiu@zju.edu.cn",
                "affiliations": [
                    "Zhejiang University, Hangzhou, China"
                ],
                "is_corresponding": false
            },
            {
                "name": "Fanny Chevalier",
                "email": "fanny@dgp.toronto.edu",
                "affiliations": [
                    "University of Toronto, Toronto, Canada"
                ],
                "is_corresponding": false
            },
            {
                "name": "Benjamin Bach",
                "email": "bbach@inf.ed.ac.uk",
                "affiliations": [
                    "Inria, Bordeaux, France",
                    "University of Edinburgh, Edinburgh, United Kingdom"
                ],
                "is_corresponding": false
            }
        ],
        "abstract": "This paper presents an interactive technique to explain visual patterns in network visualizations to analysts who are unfamiliar with these visualizations and who are learning to read them. Learning a visualization requires mastering its visual grammar and decoding information presented through visual marks, graphical encodings, and spatial configurations. To help people learn unfamiliar network visualization designs and extract meaningful information, we introduce the concept of interactive pattern explanation that allows viewers to select an arbitrary area in a visualization, then mines the underlying data patterns, and eventually explains both visual and data patterns present in the viewer\u2019s selection. In a qualitative and a quantitative user study with a total of 32 participants, we compare interactive pattern explanations to only textual and only visual (cheatsheets) explanations. Our results show that interactive explanations increase learning of i) unfamiliar visualizations, ii) patterns in network science, and iii) the respective network terminology.",
        "uid": "v-full-1185",
        "time_stamp": "",
        "time_start": "",
        "time_end": "",
        "paper_type": "full",
        "keywords": [],
        "doi": "",
        "fno": "",
        "presentation_mode": "",
        "open_access": false,
        "accessible_pdf": false,
        "has_image": false,
        "has_pdf": false,
        "paper_award": "",
        "image_caption": "",
        "external_paper_link": "",
        "youtube_ff_link": "",
        "youtube_ff_id": "",
        "bunny_ff_link": "",
        "bunny_ff_subtitles": "",
        "youtube_prerecorded_link": "",
        "youtube_prerecorded_id": "",
        "bunny_prerecorded_link": "",
        "bunny_prerecorded_subtitles": ""
    },
    "v-full-1193": {
        "slot_id": "v-full-1193",
        "session_id": "full0",
        "title": "Advancing Multimodal Large Language Models in Chart Question Answering with Visualization-Referenced Instruction Tuning",
        "contributors": [
            "Xingchen Zeng"
        ],
        "authors": [
            {
                "name": "Xingchen Zeng",
                "email": "xingchen.zeng@outlook.com",
                "affiliations": [
                    "The Hong Kong University of Science and Technology (Guangzhou), Guangzhou, China"
                ],
                "is_corresponding": true
            },
            {
                "name": "Haichuan Lin",
                "email": "hlin386@connect.hkust-gz.edu.cn",
                "affiliations": [
                    "The Hong Kong University of Science and Technology (Guangzhou), Guangzhou, China"
                ],
                "is_corresponding": false
            },
            {
                "name": "Yilin Ye",
                "email": "yyebd@connect.ust.hk",
                "affiliations": [
                    "The Hong Kong University of Science and Technology (Guangzhou), Guangzhou, China"
                ],
                "is_corresponding": false
            },
            {
                "name": "Wei Zeng",
                "email": "weizeng@hkust-gz.edu.cn",
                "affiliations": [
                    "The Hong Kong University of Science and Technology (Guangzhou), Guangzhou, China",
                    "The Hong Kong University of Science and Technology, Hong Kong SAR, China"
                ],
                "is_corresponding": false
            }
        ],
        "abstract": "Emerging multimodal large language models (MLLMs) exhibit great potential for chart question answering (CQA). Recent efforts primarily focus on scaling up training datasets (\\ie, charts, data tables, and question-answer (QA) pairs) through data collection and synthesis. However, our empirical study on existing MLLMs and CQA datasets reveals notable gaps. First, current data collection and synthesis focus on data volume and lack consideration of fine-grained visual encodings and QA tasks, resulting in unbalanced data distribution divergent from practical CQA scenarios. Second, existing work follows the training recipe of the base MLLMs initially designed for natural images, under-exploring the adaptation to unique chart characteristics, such as rich text elements. To fill the gap, we propose a visualization-referenced instruction tuning approach to guide the training dataset enhancement and model development. Specifically, we propose a novel data engine to effectively filter diverse and high-quality data from existing datasets and subsequently refine and augment the data using LLM-based generation techniques to better align with practical QA tasks and visual encodings. Then, to facilitate the adaptation to chart characteristics, we utilize the enriched data to train an MLLM by unfreezing the vision encoder and incorporating a mixture-of-resolution adaptation strategy for enhanced fine-grained recognition. Experimental results validate the effectiveness of our approach. Even with fewer training examples, our model consistently outperforms state-of-the-art CQA models on established benchmarks. We also contribute a dataset split as a benchmark for future research.",
        "uid": "v-full-1193",
        "time_stamp": "",
        "time_start": "",
        "time_end": "",
        "paper_type": "full",
        "keywords": [],
        "doi": "",
        "fno": "",
        "presentation_mode": "",
        "open_access": false,
        "accessible_pdf": false,
        "has_image": false,
        "has_pdf": false,
        "paper_award": "",
        "image_caption": "",
        "external_paper_link": "",
        "youtube_ff_link": "",
        "youtube_ff_id": "",
        "bunny_ff_link": "",
        "bunny_ff_subtitles": "",
        "youtube_prerecorded_link": "",
        "youtube_prerecorded_id": "",
        "bunny_prerecorded_link": "",
        "bunny_prerecorded_subtitles": ""
    },
    "v-full-1202": {
        "slot_id": "v-full-1202",
        "session_id": "full0",
        "title": "Unmasking Dunning-Kruger Effect in Visual Reasoning and Visual Data Analysis",
        "contributors": [
            "Mengyu Chen"
        ],
        "authors": [
            {
                "name": "Mengyu Chen",
                "email": "mengyu.chen@emory.edu",
                "affiliations": [
                    "Emory University, Atlanta, United States"
                ],
                "is_corresponding": true
            },
            {
                "name": "Yijun Liu",
                "email": "yijun.liu2@emory.edu",
                "affiliations": [
                    "Emory University, Atlanta, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Emily Wall",
                "email": "emily.wall@emory.edu",
                "affiliations": [
                    "Emory University, Atlanta, United States"
                ],
                "is_corresponding": false
            }
        ],
        "abstract": "The Dunning-Kruger Effect (DKE) is a metacognitive phenomenon where low-skilled individuals tend to overestimate their competence while high-skilled individuals tend to underestimate their competence. This effect has been observed in a number of domains including humor, grammar, and logic. In this paper, we explore if and how DKE manifests in visual reasoning and visual data analysis tasks.  Across two online user studies involving (1) a sliding puzzle game and (2) a scatterplot-based categorization task, we demonstrate that individuals are susceptible to DKE in visual tasks: those who performed best underestimated their performance, while bottom performers overestimated their performance. In addition, we contribute novel analyses that correlate susceptibility of DKE with several variables including personality traits and user interactions.  Our findings pave the way for novel modes of bias detection via interaction patterns and establish promising directions towards interventions tailored to an individual's personality traits.",
        "uid": "v-full-1202",
        "time_stamp": "",
        "time_start": "",
        "time_end": "",
        "paper_type": "full",
        "keywords": [],
        "doi": "",
        "fno": "",
        "presentation_mode": "",
        "open_access": false,
        "accessible_pdf": false,
        "has_image": false,
        "has_pdf": false,
        "paper_award": "",
        "image_caption": "",
        "external_paper_link": "",
        "youtube_ff_link": "",
        "youtube_ff_id": "",
        "bunny_ff_link": "",
        "bunny_ff_subtitles": "",
        "youtube_prerecorded_link": "",
        "youtube_prerecorded_id": "",
        "bunny_prerecorded_link": "",
        "bunny_prerecorded_subtitles": ""
    },
    "v-full-1204": {
        "slot_id": "v-full-1204",
        "session_id": "full0",
        "title": "ProvenanceWidgets: A Library of UI Control Elements to Track and Dynamically Overlay Analytic Provenance",
        "contributors": [
            "Arpit Narechania"
        ],
        "authors": [
            {
                "name": "Arpit Narechania",
                "email": "arpitnarechania@gatech.edu",
                "affiliations": [
                    "Georgia Institute of Technology, Atlanta, United States"
                ],
                "is_corresponding": true
            },
            {
                "name": "Kaustubh Odak",
                "email": "kaustubhodak1@gmail.com",
                "affiliations": [
                    "Georgia Institute of Technology, Atlanta, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Mennatallah El-Assady",
                "email": "melassady@ai.ethz.ch",
                "affiliations": [
                    "ETH Z\u00fcrich, Z\u00fcrich, Switzerland"
                ],
                "is_corresponding": false
            },
            {
                "name": "Alex Endert",
                "email": "endert@gatech.edu",
                "affiliations": [
                    "Georgia Institute of Technology, Atlanta, United States"
                ],
                "is_corresponding": false
            }
        ],
        "abstract": "We present ProvenanceWidgets, a Javascript library of UI control elements such as radio buttons, checkboxes, and dropdowns to track and dynamically overlay a user's analytic provenance. These in situ overlays not only save screen space but also minimize the amount of time and effort needed to access the same information from elsewhere in the UI. In this paper, we discuss how we design modular UI control elements to track how often and how recently a user interacts with them and design visual overlays showing an aggregated summary as well as a detailed temporal history. We demonstrate the capability of ProvenanceWidgets by recreating three prior widget libraries: (1) Scented Widgets, (2) Phosphor objects, and (3) Dynamic Query Widgets. We also evaluated its expressiveness and conducted case studies with visualization developers to evaluate its effectiveness. We find that ProvenanceWidgets enables developers to implement custom provenance-tracking applications effectively. ProvenanceWidgets is available as open-source software at https://github.com/ProvenanceWidgets to help application developers build custom provenance-based systems.",
        "uid": "v-full-1204",
        "time_stamp": "",
        "time_start": "",
        "time_end": "",
        "paper_type": "full",
        "keywords": [],
        "doi": "",
        "fno": "",
        "presentation_mode": "",
        "open_access": false,
        "accessible_pdf": false,
        "has_image": false,
        "has_pdf": false,
        "paper_award": "",
        "image_caption": "",
        "external_paper_link": "",
        "youtube_ff_link": "",
        "youtube_ff_id": "",
        "bunny_ff_link": "",
        "bunny_ff_subtitles": "",
        "youtube_prerecorded_link": "",
        "youtube_prerecorded_id": "",
        "bunny_prerecorded_link": "",
        "bunny_prerecorded_subtitles": ""
    },
    "v-full-1214": {
        "slot_id": "v-full-1214",
        "session_id": "full0",
        "title": "Improved Visual Saliency of Graph Clusters with Orderable Node-Link Layouts",
        "contributors": [
            "Mohammad Ghoniem"
        ],
        "authors": [
            {
                "name": "Nora Al-Naami",
                "email": "nora.alnaami@list.lu",
                "affiliations": [
                    "Luxembourg Institute of Science and Technology, Esch-sur-Alzette, Luxembourg"
                ],
                "is_corresponding": false
            },
            {
                "name": "Nicolas Medoc",
                "email": "nicolas.medoc@list.lu",
                "affiliations": [
                    "Luxembourg Institute of Science and Technology, Belvaux, Luxembourg"
                ],
                "is_corresponding": false
            },
            {
                "name": "Matteo Magnani",
                "email": "matteo.magnani@it.uu.se",
                "affiliations": [
                    "Uppsala University, Uppsala, Sweden"
                ],
                "is_corresponding": false
            },
            {
                "name": "Mohammad Ghoniem",
                "email": "mohammad.ghoniem@list.lu",
                "affiliations": [
                    "Luxembourg Institute of Science and Technology, Belvaux, Luxembourg"
                ],
                "is_corresponding": true
            }
        ],
        "abstract": "Graphs are often used to model relationships between entities. The identification and visualization of clusters in graphs enable insight discovery in many application areas, such as life sciences and social sciences. Force-directed graph layout algorithms promote the visual saliency of clusters, as they generally bring adjacent nodes closer together, and push non-adjacent nodes apart. In this work, we study the impact of node ordering on the visual saliency of clusters in orderable node-link diagrams, namely radial diagrams, arc diagrams and symmetric arc diagrams. Through a crowdsourced controlled experiment, we show that users can count clusters consistently more accurately, and to a large extent faster, with orderable node-link diagrams than with three state-of-the art force-directed layout algorithms, i.e., `Linlog', `Backbone' and, `sfdp'. The measured advantage is greater in case of low cluster separability and/or low compactness. A free copy of this paper and all supplemental materials are available at https://osf.io/kc3dg/?view_only=892f7b96752e40a6baefb2e50e866f9d",
        "uid": "v-full-1214",
        "time_stamp": "",
        "time_start": "",
        "time_end": "",
        "paper_type": "full",
        "keywords": [],
        "doi": "",
        "fno": "",
        "presentation_mode": "",
        "open_access": false,
        "accessible_pdf": false,
        "has_image": false,
        "has_pdf": false,
        "paper_award": "",
        "image_caption": "",
        "external_paper_link": "",
        "youtube_ff_link": "",
        "youtube_ff_id": "",
        "bunny_ff_link": "",
        "bunny_ff_subtitles": "",
        "youtube_prerecorded_link": "",
        "youtube_prerecorded_id": "",
        "bunny_prerecorded_link": "",
        "bunny_prerecorded_subtitles": ""
    },
    "v-full-1218": {
        "slot_id": "v-full-1218",
        "session_id": "full0",
        "title": "Graph Transformer for Label Placement",
        "contributors": [
            "Jingwei Qu"
        ],
        "authors": [
            {
                "name": "Jingwei Qu",
                "email": "qujingwei@swu.edu.cn",
                "affiliations": [
                    "Southwest University, Beibei, China"
                ],
                "is_corresponding": true
            },
            {
                "name": "Pingshun Zhang",
                "email": "z2211973606@email.swu.edu.cn",
                "affiliations": [
                    "Southwest University, Chongqing, China"
                ],
                "is_corresponding": false
            },
            {
                "name": "Enyu Che",
                "email": "enyuche@gmail.com",
                "affiliations": [
                    "Southwest University, Beibei, China"
                ],
                "is_corresponding": false
            },
            {
                "name": "Yinan Chen",
                "email": "out1147205215@outlook.com",
                "affiliations": [
                    "COLLEGE OF COMPUTER AND INFORMATION SCIENCE, SOUTHWEST UNIVERSITY SCHOOL OF SOFTWAREC, Chongqin, China"
                ],
                "is_corresponding": false
            },
            {
                "name": "Haibin Ling",
                "email": "hling@cs.stonybrook.edu",
                "affiliations": [
                    "Stony Brook University, New York, United States"
                ],
                "is_corresponding": false
            }
        ],
        "abstract": "Placing text labels is a common way to explain key elements in a given scene. Given a graphic input and original label information, how to place labels to meet both geometric and aesthetic requirements is an open challenging problem. Geometry-wise, traditional rule-driven solutions struggle to capture the complex interactions between labels, let alone consider graphical/appearance content. In terms of aesthetics, training/evaluation data ideally require nontrivial effort and expertise in design, thus resulting in a lack of decent datasets for learning-based methods. To address the above challenges, we formulate the task with a graph representation, where nodes correspond to labels and edges to the between-label interactions, and treat label placement as a node position prediction problem. With this novel representation, we design a Label Placement Graph Transformer (LPGT) to predict label positions. Specifically, edge-level attention, conditioned on node representations, is introduced to reveal potential relationships between labels. To integrate graphic/image information, we design a feature aligning strategy that extracts deep features for nodes and edges efficiently. Next, to address the dataset issue, we collect commercial illustrations with professionally designed label layouts from household appliance manuals, and annotate them with useful information to create a novel dataset named the Appliance Manual Illustration Labels (AMIL) dataset. In the thorough evaluation on AMIL, our LPGT solution achieves promising label placement performance compared with popular baselines.",
        "uid": "v-full-1218",
        "time_stamp": "",
        "time_start": "",
        "time_end": "",
        "paper_type": "full",
        "keywords": [],
        "doi": "",
        "fno": "",
        "presentation_mode": "",
        "open_access": false,
        "accessible_pdf": false,
        "has_image": false,
        "has_pdf": false,
        "paper_award": "",
        "image_caption": "",
        "external_paper_link": "",
        "youtube_ff_link": "",
        "youtube_ff_id": "",
        "bunny_ff_link": "",
        "bunny_ff_subtitles": "",
        "youtube_prerecorded_link": "",
        "youtube_prerecorded_id": "",
        "bunny_prerecorded_link": "",
        "bunny_prerecorded_subtitles": ""
    },
    "v-full-1232": {
        "slot_id": "v-full-1232",
        "session_id": "full0",
        "title": "Aardvark: Composite Visualizations of Trees, Time-Series, and Images",
        "contributors": [
            "Devin Lange"
        ],
        "authors": [
            {
                "name": "Devin Lange",
                "email": "devin@sci.utah.edu",
                "affiliations": [
                    "University of Utah, Salt Lake City, United States"
                ],
                "is_corresponding": true
            },
            {
                "name": "Robert L Judson-Torres",
                "email": "robert.judson-torres@hci.utah.edu",
                "affiliations": [
                    "University of Utah, Salt Lake City, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Thomas A Zangle",
                "email": "tzangle@chemeng.utah.edu",
                "affiliations": [
                    "University of Utah, Salt Lake City, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Alexander Lex",
                "email": "alex@sci.utah.edu",
                "affiliations": [
                    "University of Utah, Salt Lake City, United States"
                ],
                "is_corresponding": false
            }
        ],
        "abstract": "How do cancer cells grow, divide, proliferate and die? How do drugs influence these processes? These are difficult questions that we can attempt to answer with a combination of time-series microscopy experiments, classification algorithms, and data visualization.  However, collecting this type of data and applying algorithms to segment and track cells and construct lineages of proliferation is error-prone; and identifying the errors can be challenging since it often requires cross-checking multiple data types. Similarly, analyzing and communicating the results necessitates synthesizing different data types into a single narrative. State-of-the-art visualization methods for such data use independent line charts, tree diagrams, and images in separate views. However, this spatial separation requires the viewer of these charts to combine the relevant pieces of data in memory. To simplify this challenging task, we describe design principles for weaving cell images, time-series data, and tree data into a cohesive visualization. Our design principles are based on choosing a primary data type that drives the layout and integrates the other data types into that layout. We then introduce Aardvark, a system that uses these principles to implement novel visualization techniques. Based on Aardvark, we demonstrate the utility of each of these approaches for discovery, communication, and data debugging in a series of case studies.",
        "uid": "v-full-1232",
        "time_stamp": "",
        "time_start": "",
        "time_end": "",
        "paper_type": "full",
        "keywords": [],
        "doi": "",
        "fno": "",
        "presentation_mode": "",
        "open_access": false,
        "accessible_pdf": false,
        "has_image": false,
        "has_pdf": false,
        "paper_award": "",
        "image_caption": "",
        "external_paper_link": "",
        "youtube_ff_link": "",
        "youtube_ff_id": "",
        "bunny_ff_link": "",
        "bunny_ff_subtitles": "",
        "youtube_prerecorded_link": "",
        "youtube_prerecorded_id": "",
        "bunny_prerecorded_link": "",
        "bunny_prerecorded_subtitles": ""
    },
    "v-full-1251": {
        "slot_id": "v-full-1251",
        "session_id": "full0",
        "title": "Loops: Leveraging Provenance and Visualization to Support Exploratory Data Analysis in Notebooks",
        "contributors": [
            "Klaus Eckelt"
        ],
        "authors": [
            {
                "name": "Klaus Eckelt",
                "email": "klaus@eckelt.info",
                "affiliations": [
                    "Johannes Kepler University Linz, Linz, Austria"
                ],
                "is_corresponding": true
            },
            {
                "name": "Kiran Gadhave",
                "email": "kirangadhave2@gmail.com",
                "affiliations": [
                    "University of Utah, Salt Lake City, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Alexander Lex",
                "email": "alex@sci.utah.edu",
                "affiliations": [
                    "University of Utah, Salt Lake City, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Marc Streit",
                "email": "marc.streit@jku.at",
                "affiliations": [
                    "Johannes Kepler University Linz, Linz, Austria"
                ],
                "is_corresponding": false
            }
        ],
        "abstract": "Exploratory data science is an iterative process of obtaining, cleaning, profiling, analyzing, and interpreting data. This cyclical way of working creates challenges within the linear structure of computational notebooks that lead to issues with code quality, recall, and reproducibility. To remedy this, we present Loops, a set of visual support techniques for iterative and exploratory data analysis in computational notebooks. Loops leverages provenance information to visualize the impact of changes made within a notebook. In visualizations of the notebook history, we trace the evolution of the notebook over time and highlight differences between versions. Loops visualizes the provenance of code, markdown, tables, visualizations, and images and their respective differences. Analysts can explore these differences in detail in a separate view. Loops not only improves the reproducibility of notebooks, but also supports analysts in their data science work by showing the effects of changes and facilitating comparison of multiple versions. We demonstrate utility and potential impact of our approach in two use cases and feedback from notebook users from a range of backgrounds.",
        "uid": "v-full-1251",
        "time_stamp": "",
        "time_start": "",
        "time_end": "",
        "paper_type": "full",
        "keywords": [],
        "doi": "",
        "fno": "",
        "presentation_mode": "",
        "open_access": false,
        "accessible_pdf": false,
        "has_image": false,
        "has_pdf": false,
        "paper_award": "",
        "image_caption": "",
        "external_paper_link": "",
        "youtube_ff_link": "",
        "youtube_ff_id": "",
        "bunny_ff_link": "",
        "bunny_ff_subtitles": "",
        "youtube_prerecorded_link": "",
        "youtube_prerecorded_id": "",
        "bunny_prerecorded_link": "",
        "bunny_prerecorded_subtitles": ""
    },
    "v-full-1256": {
        "slot_id": "v-full-1256",
        "session_id": "full0",
        "title": "Trust Your Gut: Comparing Human and Machine Inference from Noisy Visualizations",
        "contributors": [
            "Ratanond Koonchanok"
        ],
        "authors": [
            {
                "name": "Ratanond Koonchanok",
                "email": "rkoonch@iu.edu",
                "affiliations": [
                    "Indiana University, Indianapolis, United States"
                ],
                "is_corresponding": true
            },
            {
                "name": "Michael E. Papka",
                "email": "papka@anl.gov",
                "affiliations": [
                    "Argonne National Laboratory, Lemont, United States",
                    "University of Illinois Chicago, Chicago, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Khairi Reda",
                "email": "redak@iu.edu",
                "affiliations": [
                    "Indiana University, Indianapolis, United States"
                ],
                "is_corresponding": false
            }
        ],
        "abstract": "People commonly utilize visualizations not only to examine a given dataset, but also to draw generalizable conclusions about the underlying models or phenomena. Previous research has compared human visual inference to that of an optimal Bayesian agent, with deviations from rational analysis viewed as problematic. However, human reliance on non-normative heuristics may prove advantageous in certain circumstances. We investigate scenarios where human intuition might surpass idealized statistical rationality. In two experiments, we examine individuals' accuracy in characterizing the parameters of known data-generating models from bivariate visualizations. Our findings indicate that, although participants generally exhibited lower accuracy compared to statistical models, they frequently outperformed Bayesian agents, particularly when faced with extreme samples. Participants appeared to rely on their internal models to filter out noisy visualizations, thus improving their resilience against spurious data. However, participants displayed overconfidence and struggled with uncertainty estimation. They also exhibited higher variance than statistical machines. Our findings suggest that analyst gut reactions to visualizations may provide an advantage, even when departing from rationality. These results carry implications for designing visual analytics tools, offering new perspectives on how to integrate statistical models and analyst intuition for improved inference and decision-making.",
        "uid": "v-full-1256",
        "time_stamp": "",
        "time_start": "",
        "time_end": "",
        "paper_type": "full",
        "keywords": [],
        "doi": "",
        "fno": "",
        "presentation_mode": "",
        "open_access": false,
        "accessible_pdf": false,
        "has_image": false,
        "has_pdf": false,
        "paper_award": "",
        "image_caption": "",
        "external_paper_link": "",
        "youtube_ff_link": "",
        "youtube_ff_id": "",
        "bunny_ff_link": "",
        "bunny_ff_subtitles": "",
        "youtube_prerecorded_link": "",
        "youtube_prerecorded_id": "",
        "bunny_prerecorded_link": "",
        "bunny_prerecorded_subtitles": ""
    },
    "v-full-1258": {
        "slot_id": "v-full-1258",
        "session_id": "full0",
        "title": "Beyond Correlation: Incorporating Counterfactual Guidance to Better Support Exploratory Visual Analysis",
        "contributors": [
            "Arran Zeyu Wang"
        ],
        "authors": [
            {
                "name": "Arran Zeyu Wang",
                "email": "zeyuwang@cs.unc.edu",
                "affiliations": [
                    "University of North Carolina-Chapel Hill, Chapel Hill, United States"
                ],
                "is_corresponding": true
            },
            {
                "name": "David Borland",
                "email": "borland@renci.org",
                "affiliations": [
                    "UNC-Chapel Hill, Chapel Hill, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "David Gotz",
                "email": "gotz@unc.edu",
                "affiliations": [
                    "University of North Carolina, Chapel Hill, United States"
                ],
                "is_corresponding": false
            }
        ],
        "abstract": "Providing effective guidance for users has long been an important and challenging task for efficient exploratory visual analytics, especially when selecting variables for visualization in high-dimensional datasets. Correlation is the most widely applied metric for guidance in statistical and analytical tools, however a reliance on correlation may lead users towards false positives when interpreting causal relations in the data. In this work, inspired by prior insights on the benefits of counterfactual visualization in supporting visual causal inference, we propose a novel, simple, and efficient counterfactual guidance method to enhance causal inference performance in guided exploratory analytics based on insights and concerns gathered from expert interviews. Our technique aims to capitalize on the benefits of counterfactual approaches while reducing their complexity for users. We integrated counterfactual guidance into an exploratory visual analytics system, and using a synthetically generated ground-truth causal dataset, conducted a comparative user study and evaluated to what extent counterfactual guidance can help lead users to more precise visual causal inferences. The results suggest that counterfactual guidance improved visual causal inference performance, and also led to different exploratory behaviors compared to correlation-based guidance. Based on these findings, we offer future directions to incorporate and examine counterfactual guidance to better support exploratory visual analytics.",
        "uid": "v-full-1258",
        "time_stamp": "",
        "time_start": "",
        "time_end": "",
        "paper_type": "full",
        "keywords": [],
        "doi": "",
        "fno": "",
        "presentation_mode": "",
        "open_access": false,
        "accessible_pdf": false,
        "has_image": false,
        "has_pdf": false,
        "paper_award": "",
        "image_caption": "",
        "external_paper_link": "",
        "youtube_ff_link": "",
        "youtube_ff_id": "",
        "bunny_ff_link": "",
        "bunny_ff_subtitles": "",
        "youtube_prerecorded_link": "",
        "youtube_prerecorded_id": "",
        "bunny_prerecorded_link": "",
        "bunny_prerecorded_subtitles": ""
    },
    "v-full-1272": {
        "slot_id": "v-full-1272",
        "session_id": "full0",
        "title": "UnDRground Tubes: Exploring Spatial Data With Multidimensional Projections and Set Visualization",
        "contributors": [
            "Nikolaus Piccolotto"
        ],
        "authors": [
            {
                "name": "Nikolaus Piccolotto",
                "email": "nikolaus.piccolotto@tuwien.ac.at",
                "affiliations": [
                    "TU Wien, Vienna, Austria"
                ],
                "is_corresponding": true
            },
            {
                "name": "Markus Wallinger",
                "email": "mwallinger@ac.tuwien.ac.at",
                "affiliations": [
                    "TU Wien, Vienna, Austria"
                ],
                "is_corresponding": false
            },
            {
                "name": "Silvia Miksch",
                "email": "miksch@ifs.tuwien.ac.at",
                "affiliations": [
                    "Institute of Visual Computing and Human-Centered Technology, Vienna, Austria"
                ],
                "is_corresponding": false
            },
            {
                "name": "Markus B\u00f6gl",
                "email": "markus.boegl@tuwien.ac.at",
                "affiliations": [
                    "TU Wien, Vienna, Austria"
                ],
                "is_corresponding": false
            }
        ],
        "abstract": "In various scientific and industrial domains, analyzing multivariate spatial data, i.e., vectors associated with spatial locations, is common practice. To analyze those datasets, analysts may turn to models such as Spatial Blind Source Separation (SBSS). Designed explicitly for spatial data analysis, SBSS finds latent components in the dataset and is superior to popular non-spatial models, like PCA. However, when analysts try different tuning parameter settings, the amount of latent components complicates analytical tasks. Based on our years-long collaboration with SBSS researchers, we propose a visualization approach to tackle this challenge. The main component is UnDRground Tubes (UT), a general-purpose idiom combining ideas from set visualization and multidimensional projections. We describe the UT visualization pipeline and integrate UT into an interactive multiple-view system. We demonstrate its effectiveness through interviews with SBSS experts, a qualitative evaluation with visualization experts, and computational experiments. SBSS experts were excited about our approach. They saw many benefits for their work and potential applications for geostatistical data analysis more generally. UT was also very well received by visualization experts. Our benchmarks show that UT projections and its heuristics are appropriate.",
        "uid": "v-full-1272",
        "time_stamp": "",
        "time_start": "",
        "time_end": "",
        "paper_type": "full",
        "keywords": [],
        "doi": "",
        "fno": "",
        "presentation_mode": "",
        "open_access": false,
        "accessible_pdf": false,
        "has_image": false,
        "has_pdf": false,
        "paper_award": "",
        "image_caption": "",
        "external_paper_link": "",
        "youtube_ff_link": "",
        "youtube_ff_id": "",
        "bunny_ff_link": "",
        "bunny_ff_subtitles": "",
        "youtube_prerecorded_link": "",
        "youtube_prerecorded_id": "",
        "bunny_prerecorded_link": "",
        "bunny_prerecorded_subtitles": ""
    },
    "v-full-1275": {
        "slot_id": "v-full-1275",
        "session_id": "full0",
        "title": "PREVis: Perceived Readability Evaluation for Visualizations",
        "contributors": [
            "Anne-Flore Cabouat"
        ],
        "authors": [
            {
                "name": "Anne-Flore Cabouat",
                "email": "acabouat@gmail.com",
                "affiliations": [
                    "LISN, Universit\u00e9 Paris Saclay, CNRS, Orsay, France",
                    "Aviz, Inria, Saclay, France"
                ],
                "is_corresponding": true
            },
            {
                "name": "Tingying He",
                "email": "tingying.he@inria.fr",
                "affiliations": [
                    "Universit\u00e9 Paris-Saclay, CNRS, Orsay, France",
                    "Inria, Saclay, France"
                ],
                "is_corresponding": false
            },
            {
                "name": "Petra Isenberg",
                "email": "petra.isenberg@inria.fr",
                "affiliations": [
                    "Universit\u00e9 Paris-Saclay, CNRS, Orsay, France",
                    "Inria, Saclay, France"
                ],
                "is_corresponding": false
            },
            {
                "name": "Tobias Isenberg",
                "email": "tobias.isenberg@gmail.com",
                "affiliations": [
                    "Universit\u00e9 Paris-Saclay, CNRS, Orsay, France",
                    "Inria, Saclay, France"
                ],
                "is_corresponding": false
            }
        ],
        "abstract": "We developed and validated an instrument to measure the perceived readability in data visualization: PREVis. Researchers and practitioners can easily use this instrument as part of their evaluations to compare the perceived readability of different visual data representations. Our instrument can complement results from controlled experiments on user task performance or provide additional data during in-depth qualitative work such as design iterations when developing a new technique. Although readability is recognized as an essential quality of data visualizations, so far there has not been a unified definition of the construct in the context of visual representations. As a result, researchers often lack guidance for determining how to ask people to rate their perceived readability of a visualization. To address this issue, we engaged in a rigorous process to develop the first validated instrument targeted at the subjective readability of visual data representations. Our final instrument consists of 11 items across 4 dimensions: understandability, layout clarity, readability of data values, and readability of data patterns. We provide the questionnaire as a document with implementation guidelines on osf.io/9cg8j. Beyond this instrument, we contribute a discussion of how researchers have previously assessed visualization readability, and an analysis of the factors underlying perceived readability in visual data representations.",
        "uid": "v-full-1275",
        "time_stamp": "",
        "time_start": "",
        "time_end": "",
        "paper_type": "full",
        "keywords": [],
        "doi": "",
        "fno": "",
        "presentation_mode": "",
        "open_access": false,
        "accessible_pdf": false,
        "has_image": false,
        "has_pdf": false,
        "paper_award": "",
        "image_caption": "",
        "external_paper_link": "",
        "youtube_ff_link": "",
        "youtube_ff_id": "",
        "bunny_ff_link": "",
        "bunny_ff_subtitles": "",
        "youtube_prerecorded_link": "",
        "youtube_prerecorded_id": "",
        "bunny_prerecorded_link": "",
        "bunny_prerecorded_subtitles": ""
    },
    "v-full-1277": {
        "slot_id": "v-full-1277",
        "session_id": "full0",
        "title": "Uncertainty Visualization of Critical Points of 2D Scalar Fields for Parametric and Nonparametric Probabilistic Models",
        "contributors": [
            "Tushar M. Athawale"
        ],
        "authors": [
            {
                "name": "Tushar M. Athawale",
                "email": "tushar.athawale@gmail.com",
                "affiliations": [
                    "Oak Ridge National Laboratory, Oak Ridge, United States"
                ],
                "is_corresponding": true
            },
            {
                "name": "Zhe Wang",
                "email": "wangz@ornl.gov",
                "affiliations": [
                    "Oak Ridge National Laboratory, Oak Ridge, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "David Pugmire",
                "email": "pugmire@ornl.gov",
                "affiliations": [
                    "Oak Ridge National Laboratory, Oak Ridge, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Kenneth Moreland",
                "email": "kmorel@acm.org",
                "affiliations": [
                    "Oak Ridge National Laboratory, Oak Ridge, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Qian Gong",
                "email": "gongq@ornl.gov",
                "affiliations": [
                    "Oak Ridge National Laboratory, Oak Ridge, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Scott Klasky",
                "email": "klasky@ornl.gov",
                "affiliations": [
                    "Oak Ridge National Laboratory, Oak Ridge, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Chris R. Johnson",
                "email": "crj@sci.utah.edu",
                "affiliations": [
                    "University of Utah, Salt Lake City, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Paul Rosen",
                "email": "paul.rosen@utah.edu",
                "affiliations": [
                    "University of Utah, Salt Lake City, United States"
                ],
                "is_corresponding": false
            }
        ],
        "abstract": "This paper presents a novel end-to-end framework for closed-form computation and visualization of critical point uncertainty in 2D uncertain scalar fields. Critical points are fundamental topological descriptors used in the visualization and analysis of scalar fields. The uncertainty inherent in data (e.g., observational and experimental data, approximations in simulations, and compression), however, creates uncertainty regarding critical point positions. Uncertainty in critical point positions, therefore, cannot be ignored, given their impact on downstream data analysis tasks. In this work, we study uncertainty in critical points as a function of uncertainty in data modeled with probability distributions. Although Monte Carlo (MC) sampling techniques have been used in prior studies to quantify critical point uncertainty, they are often expensive and are infrequently used in production-quality visualization software. We, therefore, propose a new end-to-end framework to address these challenges that comprises a threefold contribution. First, we derive the critical point uncertainty in closed form, which is more accurate and efficient than the conventional MC sampling methods. Specifically, we provide the closed-form and semianalytical (a mix of closed-form and MC methods) solutions for parametric (e.g., uniform, Epanechnikov) and nonparametric models (e.g., histograms) with finite support. Second, we accelerate critical point probability computations using a parallel implementation with the VTK-m library, which is platform portable. Finally, we demonstrate the integration of our implementation with the ParaView software system to demonstrate near-real-time results for real datasets.",
        "uid": "v-full-1277",
        "time_stamp": "",
        "time_start": "",
        "time_end": "",
        "paper_type": "full",
        "keywords": [],
        "doi": "",
        "fno": "",
        "presentation_mode": "",
        "open_access": false,
        "accessible_pdf": false,
        "has_image": false,
        "has_pdf": false,
        "paper_award": "",
        "image_caption": "",
        "external_paper_link": "",
        "youtube_ff_link": "",
        "youtube_ff_id": "",
        "bunny_ff_link": "",
        "bunny_ff_subtitles": "",
        "youtube_prerecorded_link": "",
        "youtube_prerecorded_id": "",
        "bunny_prerecorded_link": "",
        "bunny_prerecorded_subtitles": ""
    },
    "v-full-1281": {
        "slot_id": "v-full-1281",
        "session_id": "full0",
        "title": "What Can Interactive Visualization do for Participatory Budgeting in Chicago?",
        "contributors": [
            "Alex Kale"
        ],
        "authors": [
            {
                "name": "Alex Kale",
                "email": "kalea@uchicago.edu",
                "affiliations": [
                    "University of Chicago, Chicago, United States"
                ],
                "is_corresponding": true
            },
            {
                "name": "Danni Liu",
                "email": "danni6@uchicago.edu",
                "affiliations": [
                    "University of Chicago, Chicago, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Maria Gabriela Ayala",
                "email": "mariagabrielaa@uchicago.edu",
                "affiliations": [
                    "University of Chicago, Chicago, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Harper Schwab",
                "email": "hwschwab@uchicago.edu",
                "affiliations": [
                    "University of Chicago, Chicago, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Andrew M McNutt",
                "email": "mcnutt.andrew@gmail.com",
                "affiliations": [
                    "University of Washington, Seattle, United States",
                    "University of Utah, Salt Lake City, United States"
                ],
                "is_corresponding": false
            }
        ],
        "abstract": "Participatory budgeting (PB) is a democratic approach to allocating municipal spending that has been adopted in many places in recent years, including in Chicago. Current PB voting resembles a ballot where residents are asked which municipal projects, such as school improvements and road repairs, to fund with a limited budget. In this work, we ask how interactive visualization can benefit PB by conducting a design probe-based interview study (N=13) with policy workers and academics with expertise in PB, urban planning, and civic HCI. Our probe explores how graphical elicitation of voter preferences and a dashboard of voting statistics can be incorporated into a realistic PB tool. Through qualitative analysis, we find that visualization creates opportunities for city government to set expectations about budget constraints while also granting their constituents greater freedom to articulate a wider range of preferences. However, using visualization to provide transparency about PB requires efforts to mitigate potential access barriers and mistrust. We call for more visualization professionals to help build civic capacity by working in and studying political systems.",
        "uid": "v-full-1281",
        "time_stamp": "",
        "time_start": "",
        "time_end": "",
        "paper_type": "full",
        "keywords": [],
        "doi": "",
        "fno": "",
        "presentation_mode": "",
        "open_access": false,
        "accessible_pdf": false,
        "has_image": false,
        "has_pdf": false,
        "paper_award": "",
        "image_caption": "",
        "external_paper_link": "",
        "youtube_ff_link": "",
        "youtube_ff_id": "",
        "bunny_ff_link": "",
        "bunny_ff_subtitles": "",
        "youtube_prerecorded_link": "",
        "youtube_prerecorded_id": "",
        "bunny_prerecorded_link": "",
        "bunny_prerecorded_subtitles": ""
    },
    "v-full-1288": {
        "slot_id": "v-full-1288",
        "session_id": "full0",
        "title": "The Effect of Visual Aids on Reading Numeric Data Tables",
        "contributors": [
            "Charles Perin"
        ],
        "authors": [
            {
                "name": "YongFeng Ji",
                "email": "yongfengji@uvic.ca",
                "affiliations": [
                    "University of Victoria, Victoria, Canada"
                ],
                "is_corresponding": false
            },
            {
                "name": "Charles Perin",
                "email": "cperin@uvic.ca",
                "affiliations": [
                    "University of Victoria, Victoria, Canada"
                ],
                "is_corresponding": true
            },
            {
                "name": "Miguel A Nacenta",
                "email": "nacenta@gmail.com",
                "affiliations": [
                    "University of Victoria, Victoria, Canada"
                ],
                "is_corresponding": false
            }
        ],
        "abstract": "Data tables are one of the most common ways in which people encounter data. Although mostly built with text and numbers, data tables have a spatial layout and often exhibit visual elements meant to facilitate their reading. Surprisingly, there is an empirical knowledge gap on how people read and use tables and how different visual aids affect people's ability to use them. In this work, we seek to address this vacuum through a controlled study. We asked participants to repeatedly perform four different tasks with tables in four table representation conditions (plain tables, tables with zebra striping, tables with cell background color encoding cell value, and tables with background bar length in a cell encoding cell value). We analyzed completion time, error rate, gaze-tracking data, mouse movement and participant preferences. We found that visual encodings help for finding maximum values (especially color), but not as much as zebra striping helps in a complex task (comparison of proportional differences). We also characterize typical human behavior for the different tasks. These findings can inform the design of tables and research directions for improving presentation of data in tabular form.",
        "uid": "v-full-1288",
        "time_stamp": "",
        "time_start": "",
        "time_end": "",
        "paper_type": "full",
        "keywords": [],
        "doi": "",
        "fno": "",
        "presentation_mode": "",
        "open_access": false,
        "accessible_pdf": false,
        "has_image": false,
        "has_pdf": false,
        "paper_award": "",
        "image_caption": "",
        "external_paper_link": "",
        "youtube_ff_link": "",
        "youtube_ff_id": "",
        "bunny_ff_link": "",
        "bunny_ff_subtitles": "",
        "youtube_prerecorded_link": "",
        "youtube_prerecorded_id": "",
        "bunny_prerecorded_link": "",
        "bunny_prerecorded_subtitles": ""
    },
    "v-full-1290": {
        "slot_id": "v-full-1290",
        "session_id": "full0",
        "title": "Mixing Linters with GUIs: A Color Palette Design Probe",
        "contributors": [
            "Andrew M McNutt"
        ],
        "authors": [
            {
                "name": "Andrew M McNutt",
                "email": "mcnutt.andrew@gmail.com",
                "affiliations": [
                    "University of Washington, Seattle, United States",
                    "University of Utah, Salt Lake City, United States"
                ],
                "is_corresponding": true
            },
            {
                "name": "Maureen Stone",
                "email": "maureen.stone@gmail.com",
                "affiliations": [
                    "University of Washington, Seattle, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Jeffrey Heer",
                "email": "jheer@uw.edu",
                "affiliations": [
                    "University of Washington, Seattle, United States"
                ],
                "is_corresponding": false
            }
        ],
        "abstract": "Visualization linters are end-user facing evaluators that automatically identify potential chart issues. These spell-checker like systems offer a blend of interpretability and customization that is not found in other forms of automated assistance. However, existing linters do not model context and have primarily targeted users who do not need assistance, resulting in obvious---even annoying---advice. We investigate these issues within the domain of color palette design, which serves as a microcosm of visualization design concerns. We contribute a GUI-based color palette linter as a design probe that covers perception, accessibility, context, and other design criteria, and use it to explore visual explanations, integrated fixes, and user-defined linting rules. Through a formative interview study and theory-driven analysis, we find that linters can be meaningfully integrated into graphical contexts  thereby addressing many of their core issues.  We discuss implications for integrating linters into visualization tools, developing improved assertion languages, and supporting end-user tunable advice---all laying the groundwork for more effective visualization linters in any context.",
        "uid": "v-full-1290",
        "time_stamp": "",
        "time_start": "",
        "time_end": "",
        "paper_type": "full",
        "keywords": [],
        "doi": "",
        "fno": "",
        "presentation_mode": "",
        "open_access": false,
        "accessible_pdf": false,
        "has_image": false,
        "has_pdf": false,
        "paper_award": "",
        "image_caption": "",
        "external_paper_link": "",
        "youtube_ff_link": "",
        "youtube_ff_id": "",
        "bunny_ff_link": "",
        "bunny_ff_subtitles": "",
        "youtube_prerecorded_link": "",
        "youtube_prerecorded_id": "",
        "bunny_prerecorded_link": "",
        "bunny_prerecorded_subtitles": ""
    },
    "v-full-1291": {
        "slot_id": "v-full-1291",
        "session_id": "full0",
        "title": "Quantifying Emotional Responses to Immutable Data Characteristics and Designer Choices in Data Visualizations",
        "contributors": [
            "Charles Perin"
        ],
        "authors": [
            {
                "name": "Carter Blair",
                "email": "cartergblair@gmail.com",
                "affiliations": [
                    "University of Waterloo, Waterloo, Canada",
                    "University of Victoria, Victoria, Canada"
                ],
                "is_corresponding": false
            },
            {
                "name": "Xiyao Wang",
                "email": "xiyao.wang23@gmail.com",
                "affiliations": [
                    "University of Victoria, Victoira, Canada",
                    "Delft University of Technology, Delft, Netherlands"
                ],
                "is_corresponding": false
            },
            {
                "name": "Charles Perin",
                "email": "cperin@uvic.ca",
                "affiliations": [
                    "University of Victoria, Victoria, Canada"
                ],
                "is_corresponding": true
            }
        ],
        "abstract": "Emotion is an important factor to consider when designing visualizations as it can impact the amount of trust viewers place in a visualization, how well they can retrieve information and understand the underlying data, and how much they engage with or connect to a visualization. We conducted five crowdsourced experiments to quantify the effects of color, chart type, data trend, data variability and data density on emotion (measured through self-reported arousal and valence). Results from our experiments show that there are multiple design elements which influence the emotion induced by a visualization and, more surprisingly, that certain data characteristics influence the emotion of viewers even when the data has no meaning. In light of these findings, we offer guidelines on how to use color, scale, and chart type to counterbalance and emphasize the emotional impact of immutable data characteristics.",
        "uid": "v-full-1291",
        "time_stamp": "",
        "time_start": "",
        "time_end": "",
        "paper_type": "full",
        "keywords": [],
        "doi": "",
        "fno": "",
        "presentation_mode": "",
        "open_access": false,
        "accessible_pdf": false,
        "has_image": false,
        "has_pdf": false,
        "paper_award": "",
        "image_caption": "",
        "external_paper_link": "",
        "youtube_ff_link": "",
        "youtube_ff_id": "",
        "bunny_ff_link": "",
        "bunny_ff_subtitles": "",
        "youtube_prerecorded_link": "",
        "youtube_prerecorded_id": "",
        "bunny_prerecorded_link": "",
        "bunny_prerecorded_subtitles": ""
    },
    "v-full-1295": {
        "slot_id": "v-full-1295",
        "session_id": "full0",
        "title": "A Qualitative Analysis of Common Practices in Annotations: A Taxonomy and Design Space",
        "contributors": [
            "Md Dilshadur Rahman"
        ],
        "authors": [
            {
                "name": "Md Dilshadur Rahman",
                "email": "dilshadur@sci.utah.edu",
                "affiliations": [
                    "University of Utah, Salt Lake City, United States",
                    "University of Utah, Salt Lake City, United States"
                ],
                "is_corresponding": true
            },
            {
                "name": "Ghulam Jilani Quadri",
                "email": "quadri@ou.edu",
                "affiliations": [
                    "University of Oklahoma, Norman, United States",
                    "University of Oklahoma, Norman, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Bhavana Doppalapudi",
                "email": "bdoppalapudi@usf.edu",
                "affiliations": [
                    "University of South Florida , Tampa, United States",
                    "University of South Florida , Tampa, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Danielle Albers Szafir",
                "email": "danielle.szafir@cs.unc.edu",
                "affiliations": [
                    "University of North Carolina-Chapel Hill, Chapel Hill, United States",
                    "University of North Carolina-Chapel Hill, Chapel Hill, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Paul Rosen",
                "email": "paul.rosen@utah.edu",
                "affiliations": [
                    "University of Utah, Salt Lake City, United States",
                    "University of Utah, Salt Lake City, United States"
                ],
                "is_corresponding": false
            }
        ],
        "abstract": "Annotations play a vital role in highlighting critical aspects of visualizations, aiding in data externalization and exploration, collaborative data analysis, and visual storytelling. However, despite their widespread use, we identified a lack of a design space for common practices for annotations. In this paper, we evaluated over 1,800 static annotated charts to understand how people annotate visualizations in practice. Through qualitative coding of these diverse real-world annotated charts, we explore three primary aspects of annotation usage patterns: analytic purposes for chart annotations (e.g., present, identify, summarize, or compare data features), mechanisms for chart annotations (e.g., types and combinations of annotations used, frequency of different annotation types across chart types, etc.), and the data source used to generate the annotations. We then synthesized our findings into a design space of annotations, highlighting key design choices for chart annotations. We presented three case studies illustrating our design space as a practical framework for chart annotations to enhance the communication of visualization insights.",
        "uid": "v-full-1295",
        "time_stamp": "",
        "time_start": "",
        "time_end": "",
        "paper_type": "full",
        "keywords": [],
        "doi": "",
        "fno": "",
        "presentation_mode": "",
        "open_access": false,
        "accessible_pdf": false,
        "has_image": false,
        "has_pdf": false,
        "paper_award": "",
        "image_caption": "",
        "external_paper_link": "",
        "youtube_ff_link": "",
        "youtube_ff_id": "",
        "bunny_ff_link": "",
        "bunny_ff_subtitles": "",
        "youtube_prerecorded_link": "",
        "youtube_prerecorded_id": "",
        "bunny_prerecorded_link": "",
        "bunny_prerecorded_subtitles": ""
    },
    "v-full-1302": {
        "slot_id": "v-full-1302",
        "session_id": "full0",
        "title": "Talk to the Wall: The Role of Speech Interaction in Collaborative Visual Analytics",
        "contributors": [
            "Gabriela Molina Le\u00f3n"
        ],
        "authors": [
            {
                "name": "Gabriela Molina Le\u00f3n",
                "email": "molina@uni-bremen.de",
                "affiliations": [
                    "University of Bremen, Bremen, Germany",
                    "University of Bremen, Bremen, Germany"
                ],
                "is_corresponding": true
            },
            {
                "name": "Anastasia Bezerianos",
                "email": "anastasia.bezerianos@universite-paris-saclay.fr",
                "affiliations": [
                    "LISN, Universit\u00e9 Paris-Saclay, CNRS, INRIA, Orsay, France"
                ],
                "is_corresponding": false
            },
            {
                "name": "Olivier Gladin",
                "email": "olivier.gladin@inria.fr",
                "affiliations": [
                    "Inria, Palaiseau, France"
                ],
                "is_corresponding": false
            },
            {
                "name": "Petra Isenberg",
                "email": "petra.isenberg@inria.fr",
                "affiliations": [
                    "Universit\u00e9 Paris-Saclay, CNRS, Orsay, France",
                    "Inria, Saclay, France"
                ],
                "is_corresponding": false
            }
        ],
        "abstract": "We present the results of an exploratory study on how pairs interact with speech commands and touch gestures on a wall-sized display during a collaborative sensemaking task. Previous work has shown that speech commands, alone or in combination with other input modalities, can support visual data exploration by individuals. However, it is still unknown whether and how speech commands can be used in collaboration, and for what tasks. To answer these questions, we developed a functioning prototype that we used as a technology probe. We conducted an in-depth exploratory study with 20 participants (10 pairs) to analyze their interaction choices, the interplay between the input modalities, and their collaboration. While touch was the most used modality, we found that participants preferred speech commands for global operations, used them for distant interaction, and that speech interaction contributed to the awareness of the partner\u2019s actions. Furthermore, the likelihood of using speech commands during collaboration was related to the personality trait of agreeableness. Regarding collaboration styles, participants interacted with speech equally often whether they were in loosely or closely coupled collaboration. While the partners stood closer to each other during close collaboration, they did not walk away from their partner to use speech commands. From our findings, we derive and contribute a set of design considerations for collaborative and multimodal interactive data analysis systems.",
        "uid": "v-full-1302",
        "time_stamp": "",
        "time_start": "",
        "time_end": "",
        "paper_type": "full",
        "keywords": [],
        "doi": "",
        "fno": "",
        "presentation_mode": "",
        "open_access": false,
        "accessible_pdf": false,
        "has_image": false,
        "has_pdf": false,
        "paper_award": "",
        "image_caption": "",
        "external_paper_link": "",
        "youtube_ff_link": "",
        "youtube_ff_id": "",
        "bunny_ff_link": "",
        "bunny_ff_subtitles": "",
        "youtube_prerecorded_link": "",
        "youtube_prerecorded_id": "",
        "bunny_prerecorded_link": "",
        "bunny_prerecorded_subtitles": ""
    },
    "v-full-1307": {
        "slot_id": "v-full-1307",
        "session_id": "full0",
        "title": "BEMTrace: Visualization-driven approach for deriving Building Energy Models from BIM",
        "contributors": [
            "Johanna Schmidt"
        ],
        "authors": [
            {
                "name": "Andreas Walch",
                "email": "walch@vrvis.at",
                "affiliations": [
                    "VRVis Zentrum f\u00fcr Virtual Reality und Visualisierung Forschungs-GmbH, Vienna, Austria"
                ],
                "is_corresponding": false
            },
            {
                "name": "Attila Szabo",
                "email": "szabo@vrvis.at",
                "affiliations": [
                    "VRVis Zentrum f\u00fcr Virtual Reality und Visualisierung Forschungs-GmbH, Vienna, Austria"
                ],
                "is_corresponding": false
            },
            {
                "name": "Harald Steinlechner",
                "email": "hs@vrvis.at",
                "affiliations": [
                    "VRVis Zentrum f\u00fcr Virtual Reality und Visualisierung Forschungs-GmbH, Vienna, Austria"
                ],
                "is_corresponding": false
            },
            {
                "name": "Thomas Ortner",
                "email": "thomas@ortner.fyi",
                "affiliations": [
                    "Independent Researcher, Vienna, Austria"
                ],
                "is_corresponding": false
            },
            {
                "name": "Eduard Gr\u00f6ller",
                "email": "groeller@cg.tuwien.ac.at",
                "affiliations": [
                    "Institute of Visual Computing ",
                    " Human-Centered Technology, Vienna, Austria"
                ],
                "is_corresponding": false
            },
            {
                "name": "Johanna Schmidt",
                "email": "johanna.schmidt@vrvis.at",
                "affiliations": [
                    "VRVis Zentrum f\u00fcr Virtual Reality und Visualisierung Forschungs-GmbH, Vienna, Austria"
                ],
                "is_corresponding": true
            }
        ],
        "abstract": "Building information modeling (BIM) describes a central data pool covering the entire life cycle of a construction project. Similarly, building energy modeling (BEM) describes the process of using a 3D representation of a building as a basis for thermal simulations to assess the building\u2019s energy performance. This paper explores the intersection of BIM and BEM, focusing on the challenges and methodologies in converting BIM data into BEM representations for energy performance analysis. BEMTrace integrates 3D data wrangling techniques with visualization methodologies to enhance the accuracy and traceability of the BIM-to-BEM conversion process. Through parsing, error detection, and algorithmic correction of BIM data, our methods generate valid BEM models suitable for energy simulation. Visualization techniques provide transparent insights into the conversion process, aiding error identification, validation, and user comprehension. We introduce context-adaptive selections to facilitate user interaction and understanding throughout the conversion process. By evaluating user feedback, we could show that BEMTrace can solve domain-specific tasks.",
        "uid": "v-full-1307",
        "time_stamp": "",
        "time_start": "",
        "time_end": "",
        "paper_type": "full",
        "keywords": [],
        "doi": "",
        "fno": "",
        "presentation_mode": "",
        "open_access": false,
        "accessible_pdf": false,
        "has_image": false,
        "has_pdf": false,
        "paper_award": "",
        "image_caption": "",
        "external_paper_link": "",
        "youtube_ff_link": "",
        "youtube_ff_id": "",
        "bunny_ff_link": "",
        "bunny_ff_subtitles": "",
        "youtube_prerecorded_link": "",
        "youtube_prerecorded_id": "",
        "bunny_prerecorded_link": "",
        "bunny_prerecorded_subtitles": ""
    },
    "v-full-1309": {
        "slot_id": "v-full-1309",
        "session_id": "full0",
        "title": "VMC: A Grammar for Visualizing Statistical Model Checks",
        "contributors": [
            "Ziyang Guo"
        ],
        "authors": [
            {
                "name": "Ziyang Guo",
                "email": "ziyangguo1030@gmail.com",
                "affiliations": [
                    "Northwestern University, Evanston, United States"
                ],
                "is_corresponding": true
            },
            {
                "name": "Alex Kale",
                "email": "kalea@uchicago.edu",
                "affiliations": [
                    "University of Chicago, Chicago, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Matthew Kay",
                "email": "matthew.kay@gmail.com",
                "affiliations": [
                    "Northwestern University, Chicago, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Jessica Hullman",
                "email": "jhullman@northwestern.edu",
                "affiliations": [
                    "Northwestern University, Evanston, United States"
                ],
                "is_corresponding": false
            }
        ],
        "abstract": "Visualizations play a critical role in validating and improving statistical models. However, the design space of model check visualizations is not well understood, making it difficult for authors to explore and specify effective graphical model checks. VMC defines a model check visualization using four components: (1) samples of distributions of checkable quantities generated from the model, including predictive distributions for new data and distributions of model parameters; (2) transformations on observed data to facilitate comparison; (3) visual representations of distributions; and (4) layouts to facilitate comparing model samples and observed data. We contribute an implementation of VMC as an R package. We validate VMC by reproducing a set of canonical model check examples, and show how using VMC to generate model checks reduces the edit distance between visualizations relative to existing visualization toolkits. The findings of an interview study with three expert modelers who used VMC highlight challenges and opportunities for encouraging exploration of correct, effective model check visualizations.",
        "uid": "v-full-1309",
        "time_stamp": "",
        "time_start": "",
        "time_end": "",
        "paper_type": "full",
        "keywords": [],
        "doi": "",
        "fno": "",
        "presentation_mode": "",
        "open_access": false,
        "accessible_pdf": false,
        "has_image": false,
        "has_pdf": false,
        "paper_award": "",
        "image_caption": "",
        "external_paper_link": "",
        "youtube_ff_link": "",
        "youtube_ff_id": "",
        "bunny_ff_link": "",
        "bunny_ff_subtitles": "",
        "youtube_prerecorded_link": "",
        "youtube_prerecorded_id": "",
        "bunny_prerecorded_link": "",
        "bunny_prerecorded_subtitles": ""
    },
    "v-full-1316": {
        "slot_id": "v-full-1316",
        "session_id": "full0",
        "title": "The Language of Infographics: Toward Understanding Conceptual Metaphor Use in Scientific Storytelling",
        "contributors": [
            "Hana Pokojn\u00e1"
        ],
        "authors": [
            {
                "name": "Hana Pokojn\u00e1",
                "email": "hana.pokojna@gmail.com",
                "affiliations": [
                    "Masaryk University, Brno, Czech Republic"
                ],
                "is_corresponding": true
            },
            {
                "name": "Tobias Isenberg",
                "email": "tobias.isenberg@gmail.com",
                "affiliations": [
                    "Universit\u00e9 Paris-Saclay, CNRS, Orsay, France",
                    "Inria, Saclay, France"
                ],
                "is_corresponding": false
            },
            {
                "name": "Stefan Bruckner",
                "email": "stefan.bruckner@gmail.com",
                "affiliations": [
                    "University of Rostock, Rostock, Germany"
                ],
                "is_corresponding": false
            },
            {
                "name": "Barbora Kozlikova",
                "email": "kozlikova@fi.muni.cz",
                "affiliations": [
                    "Masaryk University, Brno, Czech Republic"
                ],
                "is_corresponding": false
            },
            {
                "name": "Laura Garrison",
                "email": "laura.garrison@uib.no",
                "affiliations": [
                    "University of Bergen, Bergen, Norway",
                    "Haukeland University Hospital, University of Bergen, Bergen, Norway"
                ],
                "is_corresponding": false
            }
        ],
        "abstract": "We apply an approach from cognitive linguistics by mapping Conceptual Metaphor Theory (CMT) to the visualization domain to address patterns of visual conceptual metaphors that are often used in science infographics. Metaphors play an essential part in visual communication and are frequently employed to explain complex concepts. However, their use is often based on intuition, rather than following a formal process. At present, we lack tools and language for understanding and describing metaphor use in visualization to the extent where taxonomy and grammar could guide the creation of visual components, e.g., infographics. Our classification of the visual conceptual mappings within scientific representations is based on the breakdown of visual components in existing scientific infographics. We demonstrate the development of this mapping through a detailed analysis of data collected from four domains (biomedicine, climate, space, and anthropology) that represent a diverse range of visual conceptual metaphors used in the visual communication of science. This work allows us to identify patterns of visual conceptual metaphor use within the domains, resolve ambiguities about why specific conceptual metaphors are used, and develop a better overall understanding of visual metaphor use in scientific infographics. Our analysis shows that ontological and orientational conceptual metaphors are the most widely applied to translate complex scientific concepts. To support our findings we developed a visual exploratory tool based on the collected database that places the individual infographics on a spatio-temporal scale and illustrates the breakdown of visual conceptual metaphors.",
        "uid": "v-full-1316",
        "time_stamp": "",
        "time_start": "",
        "time_end": "",
        "paper_type": "full",
        "keywords": [],
        "doi": "",
        "fno": "",
        "presentation_mode": "",
        "open_access": false,
        "accessible_pdf": false,
        "has_image": false,
        "has_pdf": false,
        "paper_award": "",
        "image_caption": "",
        "external_paper_link": "",
        "youtube_ff_link": "",
        "youtube_ff_id": "",
        "bunny_ff_link": "",
        "bunny_ff_subtitles": "",
        "youtube_prerecorded_link": "",
        "youtube_prerecorded_id": "",
        "bunny_prerecorded_link": "",
        "bunny_prerecorded_subtitles": ""
    },
    "v-full-1318": {
        "slot_id": "v-full-1318",
        "session_id": "full0",
        "title": "How Good (Or Bad) Are LLMs in Detecting Misleading Visualizations",
        "contributors": [
            "Leo Yu-Ho Lo"
        ],
        "authors": [
            {
                "name": "Leo Yu-Ho Lo",
                "email": "yhload@cse.ust.hk",
                "affiliations": [
                    "The Hong Kong University of Science and Technology, Hong Kong, China"
                ],
                "is_corresponding": true
            },
            {
                "name": "Huamin Qu",
                "email": "huamin@cse.ust.hk",
                "affiliations": [
                    "The Hong Kong University of Science and Technology, Hong Kong, China"
                ],
                "is_corresponding": false
            }
        ],
        "abstract": "In this study, we address the growing issue of misleading charts, a prevalent problem that undermines the integrity of information dissemination. Misleading charts can distort the viewer's perception of data, leading to misinterpretations and decisions based on false information. The development of effective automatic detection methods for misleading charts is an urgent field of research. The advancement of multimodal Large Language Models (LLMs) has introduced a promising direction for addressing this challenge. We explored the capabilities of these models in analyzing complex charts and assessing the impact of different prompting strategies on the models' analyses. We utilized a dataset of misleading charts collected from the internet by prior research and crafted nine distinct prompts, ranging from simple to complex, to test the ability of four different multimodal LLMs in detecting over 21 different chart issues. Through three experiments--from initial exploration to detailed analysis--we progressively gained insights into how to effectively prompt LLMs to identify misleading charts and developed strategies to address the scalability challenges encountered as we expanded our detection range from the initial five issues to 21 issues in the final experiment. Our findings reveal that multimodal LLMs possess a strong capability for chart comprehension and critical thinking in data interpretation. There is significant potential in employing multimodal LLMs to counter misleading information by supporting critical thinking and enhancing visualization literacy. This study demonstrates their applicability in addressing the pressing concern of misleading charts.",
        "uid": "v-full-1318",
        "time_stamp": "",
        "time_start": "",
        "time_end": "",
        "paper_type": "full",
        "keywords": [],
        "doi": "",
        "fno": "",
        "presentation_mode": "",
        "open_access": false,
        "accessible_pdf": false,
        "has_image": false,
        "has_pdf": false,
        "paper_award": "",
        "image_caption": "",
        "external_paper_link": "",
        "youtube_ff_link": "",
        "youtube_ff_id": "",
        "bunny_ff_link": "",
        "bunny_ff_subtitles": "",
        "youtube_prerecorded_link": "",
        "youtube_prerecorded_id": "",
        "bunny_prerecorded_link": "",
        "bunny_prerecorded_subtitles": ""
    },
    "v-full-1325": {
        "slot_id": "v-full-1325",
        "session_id": "full0",
        "title": "Motion-Based Visual Encoding Can Improve Performance on Perceptual Tasks with Dynamic Time Series",
        "contributors": [
            "Songwen Hu"
        ],
        "authors": [
            {
                "name": "Songwen Hu",
                "email": "shu343@gatech.edu",
                "affiliations": [
                    "Georgia Institute of Technology, Atlanta, United States"
                ],
                "is_corresponding": true
            },
            {
                "name": "Ouxun Jiang",
                "email": "ouxunjiang@u.northwestern.edu",
                "affiliations": [
                    "Northwestern University, Evanston, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Jeffrey Riedmiller",
                "email": "jcr@dolby.com",
                "affiliations": [
                    "Dolby Laboratories Inc., San Francisco, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Cindy Xiong Bearfield",
                "email": "cxiong@gatech.edu",
                "affiliations": [
                    "Georgia Tech, Atlanta, United States",
                    "University of Massachusetts Amherst, Amherst, United States"
                ],
                "is_corresponding": false
            }
        ],
        "abstract": "Dynamic data visualizations can convey large amounts of information over time, such as using motion to depict changes in data values for multiple entities. Such dynamic displays put a demand on our visual processing capacities, yet our perception of motion is limited. When tracking multiple objects across space and time, humans can typically track up to four objects, and the capacity is even lower if we also need to remember the history of the objects\u2019 features. Several techniques have been shown to improve the processing of dynamic displays. Staging the animation to sequentially show steps in a transition and tracing object movement by displaying trajectory histories can increase processing by reducing the cognitive load. In this paper, We examine the effectiveness of staging and tracing in dynamic displays. We showed participants animated line charts depicting the movements of lines and asked them to identify the line with the highest mean and variance. We manipulated the animation to display the lines with or without staging, tracing and history, and compared the results to a static chart as a control. Results showed that tracing and staging are preferred by participants, and improve their performance in mean and variance tasks respectively. The preferred display time 3 times shorter when staging is used. Also, encoding animation speed with mean and variance in congruent tasks is associated with higher accuracy. These findings help inform real-world best practices for building dynamic displays that leverage the strength of humans' visual processing.",
        "uid": "v-full-1325",
        "time_stamp": "",
        "time_start": "",
        "time_end": "",
        "paper_type": "full",
        "keywords": [],
        "doi": "",
        "fno": "",
        "presentation_mode": "",
        "open_access": false,
        "accessible_pdf": false,
        "has_image": false,
        "has_pdf": false,
        "paper_award": "",
        "image_caption": "",
        "external_paper_link": "",
        "youtube_ff_link": "",
        "youtube_ff_id": "",
        "bunny_ff_link": "",
        "bunny_ff_subtitles": "",
        "youtube_prerecorded_link": "",
        "youtube_prerecorded_id": "",
        "bunny_prerecorded_link": "",
        "bunny_prerecorded_subtitles": ""
    },
    "v-full-1326": {
        "slot_id": "v-full-1326",
        "session_id": "full0",
        "title": "LLM Comparator: Interactive Analysis of Side-by-Side Evaluation of Large Language Models",
        "contributors": [
            "Minsuk Kahng"
        ],
        "authors": [
            {
                "name": "Minsuk Kahng",
                "email": "minsuk.kahng@gmail.com",
                "affiliations": [
                    "Google, Atlanta, United States"
                ],
                "is_corresponding": true
            },
            {
                "name": "Ian Tenney",
                "email": "iftenney@google.com",
                "affiliations": [
                    "Google Research, Seattle, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Mahima Pushkarna",
                "email": "mahimap@google.com",
                "affiliations": [
                    "Google Research, Cambridge, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Michael Xieyang Liu",
                "email": "lxieyang.cmu@gmail.com",
                "affiliations": [
                    "Google Research, Pittsburgh, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "James Wexler",
                "email": "jwexler@google.com",
                "affiliations": [
                    "Google Research, Cambridge, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Emily Reif",
                "email": "ereif@google.com",
                "affiliations": [
                    "Google, Cambridge, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Krystal Kallarackal",
                "email": "kallarackal@google.com",
                "affiliations": [
                    "Google Research, Mountain View, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Minsuk Chang",
                "email": "minsuk.cs@gmail.com",
                "affiliations": [
                    "Google Research, Seattle, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Michael Terry",
                "email": "michaelterry@google.com",
                "affiliations": [
                    "Google, Cambridge, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Lucas Dixon",
                "email": "ldixon@google.com",
                "affiliations": [
                    "Google, Paris, France"
                ],
                "is_corresponding": false
            }
        ],
        "abstract": "Evaluating the quality of text responses generated by large language models (LLMs) poses unique challenges compared to traditional machine learning. While automatic side-by-side evaluation has emerged as a promising approach, LLM developers face scalability and interpretability challenges in analyzing these evaluation results. In this paper, we present LLM Comparator, a novel visual analytics tool for interactively analyzing results from side-by-side evaluation of LLMs. The tool provides users with interactive workflows to understand when and why a model performs better or worse than a baseline model, and how the responses from two models differ qualitatively. We iteratively designed and developed the tool by closely working with researchers and engineers at a large technology company. Qualitative feedback from users highlights that the tool facilitates in-depth analysis of individual examples while enabling users to visually overview and flexibly slice data. This empowers users to identify undesirable patterns, formulate hypotheses about model behavior, and gain insights for model improvement.",
        "uid": "v-full-1326",
        "time_stamp": "",
        "time_start": "",
        "time_end": "",
        "paper_type": "full",
        "keywords": [],
        "doi": "",
        "fno": "",
        "presentation_mode": "",
        "open_access": false,
        "accessible_pdf": false,
        "has_image": false,
        "has_pdf": false,
        "paper_award": "",
        "image_caption": "",
        "external_paper_link": "",
        "youtube_ff_link": "",
        "youtube_ff_id": "",
        "bunny_ff_link": "",
        "bunny_ff_subtitles": "",
        "youtube_prerecorded_link": "",
        "youtube_prerecorded_id": "",
        "bunny_prerecorded_link": "",
        "bunny_prerecorded_subtitles": ""
    },
    "v-full-1329": {
        "slot_id": "v-full-1329",
        "session_id": "full0",
        "title": "StuGPTViz: A Visual Analytics Approach to Understand Student-ChatGPT Interactions",
        "contributors": [
            "Zixin Chen"
        ],
        "authors": [
            {
                "name": "Zixin Chen",
                "email": "zchendf@connect.ust.hk",
                "affiliations": [
                    "The Hong Kong University of Science and Technology, Hong Kong, China"
                ],
                "is_corresponding": true
            },
            {
                "name": "Jiachen Wang",
                "email": "csejiachenw@ust.hk",
                "affiliations": [
                    "The Hong Kong University of Science and Technology, Sai Kung, China"
                ],
                "is_corresponding": false
            },
            {
                "name": "Meng Xia",
                "email": "xiameng9355@gmail.com",
                "affiliations": [
                    "Texas A",
                    "M University, College Station, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Kento Shigyo",
                "email": "kshigyo@connect.ust.hk",
                "affiliations": [
                    "The Hong Kong University of Science and Technology, Kowloon, Hong Kong"
                ],
                "is_corresponding": false
            },
            {
                "name": "Dingdong Liu",
                "email": "dliuak@connect.ust.hk",
                "affiliations": [
                    "The Hong Kong University of Science and Technology, Hong Kong, China"
                ],
                "is_corresponding": false
            },
            {
                "name": "Rong Zhang",
                "email": "rzhangab@connect.ust.hk",
                "affiliations": [
                    "Hong Kong University of Science and Technology, Hong Kong, Hong Kong"
                ],
                "is_corresponding": false
            },
            {
                "name": "Huamin Qu",
                "email": "huamin@cse.ust.hk",
                "affiliations": [
                    "The Hong Kong University of Science and Technology, Hong Kong, China"
                ],
                "is_corresponding": false
            }
        ],
        "abstract": "The integration of Large Language Models (LLMs), especially ChatGPT, into education is poised to revolutionize students' learning experiences by introducing innovative conversational learning methodologies. To empower students to fully leverage the capabilities of ChatGPT in educational scenarios, understanding students' interaction patterns with ChatGPT is crucial for instructors. However, this endeavor is challenging due to the absence of datasets focused on student-ChatGPT conversations and the complexities in identifying and analyzing the evolutional interaction patterns within conversations. To address these challenges, we collected conversational data from 48 students interacting with ChatGPT in a master's level data visualization course over one semester. We then developed a coding scheme, grounded in the literature on cognitive levels and thematic analysis, to categorize students' interaction patterns with ChatGPT. Furthermore, we present a visual analytics system, StuGPTViz, that tracks and compares temporal patterns in student prompts and the quality of ChatGPT's responses at multiple scales, revealing significant pedagogical insights for instructors. We validated the system's effectiveness through expert interviews with six data visualization instructors and three case studies. The results confirmed StuGPTViz's capacity to enhance educators' insights into the pedagogical value of ChatGPT. We also discussed the potential research opportunities of applying visual analytics in education and developing AI-driven personalized learning solutions.",
        "uid": "v-full-1329",
        "time_stamp": "",
        "time_start": "",
        "time_end": "",
        "paper_type": "full",
        "keywords": [],
        "doi": "",
        "fno": "",
        "presentation_mode": "",
        "open_access": false,
        "accessible_pdf": false,
        "has_image": false,
        "has_pdf": false,
        "paper_award": "",
        "image_caption": "",
        "external_paper_link": "",
        "youtube_ff_link": "",
        "youtube_ff_id": "",
        "bunny_ff_link": "",
        "bunny_ff_subtitles": "",
        "youtube_prerecorded_link": "",
        "youtube_prerecorded_id": "",
        "bunny_prerecorded_link": "",
        "bunny_prerecorded_subtitles": ""
    },
    "v-full-1332": {
        "slot_id": "v-full-1332",
        "session_id": "full0",
        "title": "VisEval: A Benchmark for Data Visualization in the Era of Large Language Models",
        "contributors": [
            "Nan Chen"
        ],
        "authors": [
            {
                "name": "Nan Chen",
                "email": "christy05.chen@gmail.com",
                "affiliations": [
                    "Microsoft Research, Shanghai, China"
                ],
                "is_corresponding": true
            },
            {
                "name": "Yuge Zhang",
                "email": "scottyugochang@gmail.com",
                "affiliations": [
                    "Microsoft Research, Shanghai, China"
                ],
                "is_corresponding": false
            },
            {
                "name": "Jiahang Xu",
                "email": "jiahangxu@microsoft.com",
                "affiliations": [
                    "Microsoft Research, Shanghai, China"
                ],
                "is_corresponding": false
            },
            {
                "name": "Kan Ren",
                "email": "rk.ren@outlook.com",
                "affiliations": [
                    "ShanghaiTech University, Shanghai, China"
                ],
                "is_corresponding": false
            },
            {
                "name": "Yuqing Yang",
                "email": "yuqyang@microsoft.com",
                "affiliations": [
                    "Microsoft Research, Shanghai, China"
                ],
                "is_corresponding": false
            }
        ],
        "abstract": "Translating natural language to visualization (NL2VIS) has shown great promise for visual data analysis, but it remains a challenging task that requires multiple low-level implementations, such as natural language processing and visualization design. Recent advancements in pre-trained large language models (LLMs) are opening new avenues for generating visualizations from natural language. However, the lack of a comprehensive and reliable benchmark hinders our understanding of LLMs\u2019 capabilities in visualization generation. In this paper, we address this gap by proposing a new NL2VIS benchmark called VisEval. Firstly, we introduce a high-quality and large-scale dataset. This dataset includes 2,524 representative queries covering 146 databases, paired with accurately labeled ground truths. Secondly, we advocate for a comprehensive automated evaluation methodology covering multiple dimensions, including validity, legality, and readability. By systematically scanning for potential issues with a number of heterogeneous checkers, VisEval provides reliable and trustworthy evaluation outcomes. We run VisEval on a series of state-of-the-art LLMs. Our evaluation reveals prevalent challenges and delivers essential insights for future advancements.",
        "uid": "v-full-1332",
        "time_stamp": "",
        "time_start": "",
        "time_end": "",
        "paper_type": "full",
        "keywords": [],
        "doi": "",
        "fno": "",
        "presentation_mode": "",
        "open_access": false,
        "accessible_pdf": false,
        "has_image": false,
        "has_pdf": false,
        "paper_award": "",
        "image_caption": "",
        "external_paper_link": "",
        "youtube_ff_link": "",
        "youtube_ff_id": "",
        "bunny_ff_link": "",
        "bunny_ff_subtitles": "",
        "youtube_prerecorded_link": "",
        "youtube_prerecorded_id": "",
        "bunny_prerecorded_link": "",
        "bunny_prerecorded_subtitles": ""
    },
    "v-full-1333": {
        "slot_id": "v-full-1333",
        "session_id": "full0",
        "title": "Telling Data Stories with the Hero\u2019s Journey: Design Guidance for Creating Data Videos",
        "contributors": [
            "Zheng Wei"
        ],
        "authors": [
            {
                "name": "Zheng Wei",
                "email": "zwei302@connect.hkust-gz.edu.cn",
                "affiliations": [
                    "The Hong Kong University of Science and Technology, Guangzhou, China"
                ],
                "is_corresponding": true
            },
            {
                "name": "Huamin Qu",
                "email": "huamin@cse.ust.hk",
                "affiliations": [
                    "The Hong Kong University of Science and Technology, Hong Kong, China"
                ],
                "is_corresponding": false
            },
            {
                "name": "Xian Xu",
                "email": "xxubq@connect.ust.hk",
                "affiliations": [
                    "The Hong Kong University of Science and Technology, Hong Kong, China"
                ],
                "is_corresponding": false
            }
        ],
        "abstract": "Data videos increasingly becoming a popular data storytelling form represented by visual and audio integration. In recent years, more and more researchers have explored many narrative structures for effective and attractive data storytelling. Meanwhile, the Hero's Journey provides a classic narrative framework specific to the Hero's story that has been adopted by various mediums. There are continuous discussions about applying Hero's Journey to data stories. However, so far, little systematic and practical guidance on how to create a data video for a specific story type like the Hero's Journey, as well as how to manipulate its sound and visual designs simultaneously. To fulfill this gap, we first identified 48 data videos aligned with the Hero's Journey as the common storytelling from 109 high-quality data videos. Then, we examined how existing practices apply Hero's Journey for creating data videos. We coded the 48 data videos in terms of the narrative stages, sound design, and visual design according to the Hero's Journey structure. Based on our findings, we proposed a design space to provide practical guidance on the narrative, visual, and sound custom design for different narrative segments of the hero's journey (i.e., Departure, Initiation, Return) through data video creation. To validate our proposed design space, we conducted a user study where 20 participants were invited to design data videos with and without our design space guidance, which was evaluated by two experts. Results show that our design space provides useful and practical guidance for data storytellers effectively creating data videos with the Hero's Journey.",
        "uid": "v-full-1333",
        "time_stamp": "",
        "time_start": "",
        "time_end": "",
        "paper_type": "full",
        "keywords": [],
        "doi": "",
        "fno": "",
        "presentation_mode": "",
        "open_access": false,
        "accessible_pdf": false,
        "has_image": false,
        "has_pdf": false,
        "paper_award": "",
        "image_caption": "",
        "external_paper_link": "",
        "youtube_ff_link": "",
        "youtube_ff_id": "",
        "bunny_ff_link": "",
        "bunny_ff_subtitles": "",
        "youtube_prerecorded_link": "",
        "youtube_prerecorded_id": "",
        "bunny_prerecorded_link": "",
        "bunny_prerecorded_subtitles": ""
    },
    "v-full-1342": {
        "slot_id": "v-full-1342",
        "session_id": "full0",
        "title": "Understanding Visualization Authoring Techniques for Genomics Data in the Context of Personas and Tasks",
        "contributors": [
            "Astrid van den Brandt"
        ],
        "authors": [
            {
                "name": "Astrid van den Brandt",
                "email": "a.v.d.brandt@tue.nl",
                "affiliations": [
                    "Eindhoven University of Technology, Eindhoven, Netherlands"
                ],
                "is_corresponding": true
            },
            {
                "name": "Sehi L'Yi",
                "email": "sehi_lyi@hms.harvard.edu",
                "affiliations": [
                    "Harvard Medical School, Boston, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Huyen N. Nguyen",
                "email": "huyen_nguyen@hms.harvard.edu",
                "affiliations": [
                    "Harvard Medical School, Boston, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Anna Vilanova",
                "email": "a.vilanova@tue.nl",
                "affiliations": [
                    "Eindhoven University of Technology, Eindhoven, Netherlands"
                ],
                "is_corresponding": false
            },
            {
                "name": "Nils Gehlenborg",
                "email": "nils@hms.harvard.edu",
                "affiliations": [
                    "Harvard Medical School, Boston, United States"
                ],
                "is_corresponding": false
            }
        ],
        "abstract": "Genomics experts rely on visualization to extract and share insights from complex and large-scale datasets. Beyond off-the-shelf tools for data exploration, there is an increasing need for platforms that aid experts in authoring customized visualizations for both exploration and communication of insights. A variety of interactive techniques have been proposed for authoring data visualizations, such as template editing, shelf configuration, natural language input, and code editors. However, it remains unclear how genomics experts create visualizations and which techniques best support their visualization tasks and needs. To address this gap, we conducted two user studies with genomics researchers: (1) semi-structured interviews (n=20) to identify the tasks, user contexts, and current visualization authoring techniques and (2) an exploratory study (n=13) using visual probes to elicit users\u2019 intents and desired techniques when creating visualizations. Our contributions include (1) a characterization of how visualization authoring is currently utilized in genomics visualization, identifying limitations and benefits in light of common criteria for authoring tools, and (2) generalizable and actionable design implications for genomics visualization authoring tools based on our findings on task- and user-specific usefulness of authoring techniques.",
        "uid": "v-full-1342",
        "time_stamp": "",
        "time_start": "",
        "time_end": "",
        "paper_type": "full",
        "keywords": [],
        "doi": "",
        "fno": "",
        "presentation_mode": "",
        "open_access": false,
        "accessible_pdf": false,
        "has_image": false,
        "has_pdf": false,
        "paper_award": "",
        "image_caption": "",
        "external_paper_link": "",
        "youtube_ff_link": "",
        "youtube_ff_id": "",
        "bunny_ff_link": "",
        "bunny_ff_subtitles": "",
        "youtube_prerecorded_link": "",
        "youtube_prerecorded_id": "",
        "bunny_prerecorded_link": "",
        "bunny_prerecorded_subtitles": ""
    },
    "v-full-1351": {
        "slot_id": "v-full-1351",
        "session_id": "full0",
        "title": "Sportify: Question Answering with Embedded Visualizations and Personified Narratives for Sports Video",
        "contributors": [
            "Chunggi Lee"
        ],
        "authors": [
            {
                "name": "Chunggi Lee",
                "email": "chungyi347@gmail.com",
                "affiliations": [
                    "Harvard University, Allston, United States"
                ],
                "is_corresponding": true
            },
            {
                "name": "Tica Lin",
                "email": "mlin@g.harvard.edu",
                "affiliations": [
                    "Harvard University, Cambridge, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Chen Zhu-Tian",
                "email": "ztchen@umn.edu",
                "affiliations": [
                    "University of Minnesota-Twin Cities, Minneapolis, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Hanspeter Pfister",
                "email": "pfister@seas.harvard.edu",
                "affiliations": [
                    "Harvard University, Cambridge, United States"
                ],
                "is_corresponding": false
            }
        ],
        "abstract": "As basketball\u2019s popularity surges, fans often find themselves confused and overwhelmed by the rapid game pace and complexity. Basketball tactics, involving a complex series of actions, require substantial knowledge to be fully understood. This complexity leads to a need for additional information and explanation, which can distract fans from the game. To tackle these challenges, we present Sportify, a Visual Question Answering system that integrates narratives and embedded visualization for demystifying basketball tactical questions, aiding fans in understanding various game aspects. We propose three novel action visualizations (i.e., Pass, Cut, and Screen) to demonstrate critical action sequences. To explain the reasoning and logic behind players\u2019 actions, we leverage a large-language model (LLM) to generate narratives. We adopt a storytelling approach for complex scenarios from both first and third-person perspectives, integrating action visualizations. We evaluated Sportify with basketball fans to investigate its impact on understanding of tactics, and how different personal perspectives of narratives impact the understanding of complex tactic with action visualizations. Our evaluation with basketball fans demonstrates Sportify\u2019s capability to deepen tactical insights and amplify the viewing experience. Furthermore, third-person narration assists people in getting in-depth game explanations while first-person narration enhances fans\u2019 game engagement.",
        "uid": "v-full-1351",
        "time_stamp": "",
        "time_start": "",
        "time_end": "",
        "paper_type": "full",
        "keywords": [],
        "doi": "",
        "fno": "",
        "presentation_mode": "",
        "open_access": false,
        "accessible_pdf": false,
        "has_image": false,
        "has_pdf": false,
        "paper_award": "",
        "image_caption": "",
        "external_paper_link": "",
        "youtube_ff_link": "",
        "youtube_ff_id": "",
        "bunny_ff_link": "",
        "bunny_ff_subtitles": "",
        "youtube_prerecorded_link": "",
        "youtube_prerecorded_id": "",
        "bunny_prerecorded_link": "",
        "bunny_prerecorded_subtitles": ""
    },
    "v-full-1363": {
        "slot_id": "v-full-1363",
        "session_id": "full0",
        "title": "FPCS: Feature Preserving Compensated Sampling of Streaming Time Series Data",
        "contributors": [
            "Hongyan Li"
        ],
        "authors": [
            {
                "name": "Hongyan Li",
                "email": "3271961659@qq.com",
                "affiliations": [
                    "China Nanhu Academy of Electronics and Information Technology(CNAEIT), JiaXing, China"
                ],
                "is_corresponding": true
            },
            {
                "name": "Bo Yang",
                "email": "ustcboy@outlook.com",
                "affiliations": [
                    "China Nanhu Academy of Electronics and Information Technology(CNAEIT), JiaXing, China"
                ],
                "is_corresponding": false
            },
            {
                "name": "Yansong Chua",
                "email": "caiyansong@cnaeit.com",
                "affiliations": [
                    "China Nanhu Academy of Electronics and Information Technology, Jiaxing, China"
                ],
                "is_corresponding": false
            }
        ],
        "abstract": "Data visualization aids in making data analysis more intuitive and in-depth, with widespread applications in fields such as biology, finance, and medicine. For massive and continuously growing streaming time series data, these data are typically visualized in the form of line charts, but the data transmission puts significant pressure on the network, leading to visualization lag or even fail to render completely. This paper proposes a universal sampling algorithm FPCS, which retains feature points from continuously received streaming time series data, compensates for the frequent fluctuating feature points, and aims to achieve efficient visualization. This algorithm bridges the gap in sampling for streaming time series data. The algorithm has several advantages: (1) It optimizes the sampling results by compensating for fewer feature points, retaining the visualization features of the original data very well, ensuring high-quality sampled data; (2) The execution time is the shortest compared to similar existing algorithms; (3) It has an almost negligible space overhead; (4) The data sampling process does not depend on the overall data; (5) This algorithm can be applied to infinite streaming data and finite static data.",
        "uid": "v-full-1363",
        "time_stamp": "",
        "time_start": "",
        "time_end": "",
        "paper_type": "full",
        "keywords": [],
        "doi": "",
        "fno": "",
        "presentation_mode": "",
        "open_access": false,
        "accessible_pdf": false,
        "has_image": false,
        "has_pdf": false,
        "paper_award": "",
        "image_caption": "",
        "external_paper_link": "",
        "youtube_ff_link": "",
        "youtube_ff_id": "",
        "bunny_ff_link": "",
        "bunny_ff_subtitles": "",
        "youtube_prerecorded_link": "",
        "youtube_prerecorded_id": "",
        "bunny_prerecorded_link": "",
        "bunny_prerecorded_subtitles": ""
    },
    "v-full-1368": {
        "slot_id": "v-full-1368",
        "session_id": "full0",
        "title": "SLInterpreter: An Exploratory and Iterative Human-AI Collaborative System for GNN-based Synthetic Lethal Prediction",
        "contributors": [
            "Haoran Jiang"
        ],
        "authors": [
            {
                "name": "Haoran Jiang",
                "email": "jianghr2023@shanghaitech.edu.cn",
                "affiliations": [
                    "Shanghaitech University, Shanghai, China"
                ],
                "is_corresponding": true
            },
            {
                "name": "Shaohan Shi",
                "email": "shishh2023@shanghaitech.edu.cn",
                "affiliations": [
                    "ShanghaiTech University, Shanghai, China"
                ],
                "is_corresponding": false
            },
            {
                "name": "Shuhao Zhang",
                "email": "zhangshh2@shanghaitech.edu.cn",
                "affiliations": [
                    "ShanghaiTech University, Shanghai, China"
                ],
                "is_corresponding": false
            },
            {
                "name": "Jie Zheng",
                "email": "zhengjie@shanghaitech.edu.cn",
                "affiliations": [
                    "ShanghaiTech University, Shanghai, China"
                ],
                "is_corresponding": false
            },
            {
                "name": "Quan Li",
                "email": "liquan@shanghaitech.edu.cn",
                "affiliations": [
                    "ShanghaiTech University, Shanghai, China"
                ],
                "is_corresponding": false
            }
        ],
        "abstract": "Synthetic Lethal (SL) relationships, although rare among the vast array of gene combinations, hold substantial promise for targeted cancer therapy. Despite advancements in AI model accuracy, there remains a persistent need among domain experts for interpretive paths and mechanism explorations that better harmonize with domain-specific knowledge, particularly due to the significant costs involved in experimentation. To address this gap, we propose an iterative Human-AI collaborative framework comprising two key components: 1)Human-Engaged Knowledge Graph Refinement based on Metapath Strategies, which leverages insights from interpretive paths and domain expertise to refine the knowledge graph through metapath strategies with appropriate granularity. 2)Cross-Granularity SL Interpretation Enhancement and Mechanism Analysis, which aids domain experts in organizing and comparing prediction results and interpretive paths across different granularities, thereby uncovering new SL relationships, enhancing result interpretation, and elucidating potential mechanisms inferred by Graph Neural Network (GNN) models. These components cyclically optimize model predictions and mechanism explorations, thereby enhancing expert involvement and intervention to build trust. This framework, facilitated by SLInterpreter, ensures that newly generated interpretive paths increasingly align with domain knowledge and adhere more closely to real-world biological principles through iterative Human-AI collaboration. Subsequently, we evaluate the efficacy of the framework through a case study and expert interviews.",
        "uid": "v-full-1368",
        "time_stamp": "",
        "time_start": "",
        "time_end": "",
        "paper_type": "full",
        "keywords": [],
        "doi": "",
        "fno": "",
        "presentation_mode": "",
        "open_access": false,
        "accessible_pdf": false,
        "has_image": false,
        "has_pdf": false,
        "paper_award": "",
        "image_caption": "",
        "external_paper_link": "",
        "youtube_ff_link": "",
        "youtube_ff_id": "",
        "bunny_ff_link": "",
        "bunny_ff_subtitles": "",
        "youtube_prerecorded_link": "",
        "youtube_prerecorded_id": "",
        "bunny_prerecorded_link": "",
        "bunny_prerecorded_subtitles": ""
    },
    "v-full-1391": {
        "slot_id": "v-full-1391",
        "session_id": "full0",
        "title": "StyleRF-VolVis: Style Transfer of Neural Radiance Fields for Expressive Volume Visualization",
        "contributors": [
            "Kaiyuan Tang"
        ],
        "authors": [
            {
                "name": "Kaiyuan Tang",
                "email": "ktang2@nd.edu",
                "affiliations": [
                    "University of Notre Dame, Notre Dame, United States"
                ],
                "is_corresponding": true
            },
            {
                "name": "Chaoli Wang",
                "email": "chaoli.wang@nd.edu",
                "affiliations": [
                    "University of Notre Dame, Notre Dame, United States"
                ],
                "is_corresponding": false
            }
        ],
        "abstract": "In volume visualization, visualization synthesis has attracted much attention due to its ability to generate novel visualizations without following the conventional rendering pipeline. However, existing solutions based on generative adversarial networks often require many training images and take significant training time. Still, issues such as low quality, consistency, and flexibility persist. This paper introduces StyleRF-VolVis, an innovative style transfer framework for expressive volume visualization (VolVis) via neural radiance field (NeRF). The expressiveness of StyleRF-VolVis is upheld by its ability to accurately separate the underlying scene geometry (i.e., content) and color appearance (i.e., style), conveniently modify color, opacity, and lighting of the original rendering while maintaining visual content consistency across the views, and effectively transfer arbitrary styles from reference images to the reconstructed 3D scene. To achieve these, we design a base NeRF model for scene geometry extraction, a palette color network to classify regions of the radiance field for photorealistic editing, and an unrestricted color network to lift the color palette constraint via knowledge distillation for non-photorealistic editing. We demonstrate the superior quality, consistency, and flexibility of StyleRF-VolVis by experimenting with various volume rendering scenes and reference images and comparing StyleRF-VolVis against other image-based (AdaIN), video-based (ReReVST), and NeRF-based (ARF and SNeRF) style rendering solutions.",
        "uid": "v-full-1391",
        "time_stamp": "",
        "time_start": "",
        "time_end": "",
        "paper_type": "full",
        "keywords": [],
        "doi": "",
        "fno": "",
        "presentation_mode": "",
        "open_access": false,
        "accessible_pdf": false,
        "has_image": false,
        "has_pdf": false,
        "paper_award": "",
        "image_caption": "",
        "external_paper_link": "",
        "youtube_ff_link": "",
        "youtube_ff_id": "",
        "bunny_ff_link": "",
        "bunny_ff_subtitles": "",
        "youtube_prerecorded_link": "",
        "youtube_prerecorded_id": "",
        "bunny_prerecorded_link": "",
        "bunny_prerecorded_subtitles": ""
    },
    "v-full-1393": {
        "slot_id": "v-full-1393",
        "session_id": "full0",
        "title": "Practices and Strategies in Responsive Thematic Map Design: A Report from Design Workshops with Experts",
        "contributors": [
            "Sarah Sch\u00f6ttler"
        ],
        "authors": [
            {
                "name": "Sarah Sch\u00f6ttler",
                "email": "sarah.schoettler@ed.ac.uk",
                "affiliations": [
                    "University of Edinburgh, Edinburgh, United Kingdom"
                ],
                "is_corresponding": true
            },
            {
                "name": "Uta Hinrichs",
                "email": "uhinrich@ed.ac.uk",
                "affiliations": [
                    "University of Edinburgh, Edinburgh, United Kingdom"
                ],
                "is_corresponding": false
            },
            {
                "name": "Benjamin Bach",
                "email": "bbach@inf.ed.ac.uk",
                "affiliations": [
                    "Inria, Bordeaux, France",
                    "University of Edinburgh, Edinburgh, United Kingdom"
                ],
                "is_corresponding": false
            }
        ],
        "abstract": "This paper discusses challenges and design strategies in responsive design for thematic maps in information visualization. Thematic maps pose a number of unique challenges for responsiveness, such as inflexible aspect ratios that do not easily adapt to varying screen dimensions, or densely clustered visual elements in urban areas becoming illegible at smaller scales. However, design guidance on how to best address these issues is currently lacking. We conducted design sessions with eight professional designers and developers of web-based thematic maps for information visualization. Participants were asked to redesign a given map for various screen sizes and aspect ratios and to describe their reasoning for when and how they adapted the design. We report general observations of practitioners\u2019 motivations, decision-making processes, and personal design frameworks. We then derive seven challenges commonly encountered in responsive map design, and 17 strategies to address them, such as repositioning elements, segmenting the map, or using alternative visualizations. We compile these challenges and strategies into an illustrated cheat sheet targeted at anyone designing or learning to design responsive maps. The cheat sheet is available online: https://responsive-vis.github.io/map-cheat-sheet.",
        "uid": "v-full-1393",
        "time_stamp": "",
        "time_start": "",
        "time_end": "",
        "paper_type": "full",
        "keywords": [],
        "doi": "",
        "fno": "",
        "presentation_mode": "",
        "open_access": false,
        "accessible_pdf": false,
        "has_image": false,
        "has_pdf": false,
        "paper_award": "",
        "image_caption": "",
        "external_paper_link": "",
        "youtube_ff_link": "",
        "youtube_ff_id": "",
        "bunny_ff_link": "",
        "bunny_ff_subtitles": "",
        "youtube_prerecorded_link": "",
        "youtube_prerecorded_id": "",
        "bunny_prerecorded_link": "",
        "bunny_prerecorded_subtitles": ""
    },
    "v-full-1394": {
        "slot_id": "v-full-1394",
        "session_id": "full0",
        "title": "Discursive Patinas: Anchoring Discussions in Data Visualizations",
        "contributors": [
            "Tobias Kauer"
        ],
        "authors": [
            {
                "name": "Tobias Kauer",
                "email": "tobias.kauer@fh-potsdam.de",
                "affiliations": [
                    "University of Edinburgh, Edinburgh, United Kingdom",
                    "Potsdam University of Applied Sciences, Potsdam, Germany"
                ],
                "is_corresponding": true
            },
            {
                "name": "Derya Akbaba",
                "email": "derya.akbaba@liu.se",
                "affiliations": [
                    "Link\u00f6ping University, Norrk\u00f6ping, Sweden"
                ],
                "is_corresponding": false
            },
            {
                "name": "Marian D\u00f6rk",
                "email": "doerk@fh-potsdam.de",
                "affiliations": [
                    "University of Applied Sciences Potsdam, Potsdam, Germany"
                ],
                "is_corresponding": false
            },
            {
                "name": "Benjamin Bach",
                "email": "bbach@inf.ed.ac.uk",
                "affiliations": [
                    "Inria, Bordeaux, France",
                    "University of Edinburgh, Edinburgh, United Kingdom"
                ],
                "is_corresponding": false
            }
        ],
        "abstract": "This paper presents discursive patinas, a technique to visualize discussions onto data visualizations, inspired by how people leave traces in the physical world. While data visualizations are widely discussed in online communities and social media, comments tend to be displayed separately from the visualization. We lack ways to relate these discussions to the content of the visualization, e.g., to situate comments, explain visual patterns, or question assumptions. In our visualization annotation interface, users can designate areas within the visualization to, e.g., highlight specific visual marks (anchors), attach textual comments, and add category labels, likes, and replies. By coloring and styling these designated areas, a meta visualization emerges, showing what and where people comment and annotate. These patinas show regions of heavy discussions, recent commenting activity, and the distribution of questions, suggestions, or personal stories. To study how people use anchors to discuss visualizations and understand if and how information in patinas influence people's understanding of the discussion, we ran workshops with 90 participants including students, domain experts, and visualization researchers. Our results show that discursive patinas improve the ability to navigate discussions and guide people to comments that help understand, contextualize, or scrutinize the visualization. We discuss the potential of the technique to support discursive engagements, including critical readings of visualizations, design feedback, and feminist approaches to data visualization.",
        "uid": "v-full-1394",
        "time_stamp": "",
        "time_start": "",
        "time_end": "",
        "paper_type": "full",
        "keywords": [],
        "doi": "",
        "fno": "",
        "presentation_mode": "",
        "open_access": false,
        "accessible_pdf": false,
        "has_image": false,
        "has_pdf": false,
        "paper_award": "",
        "image_caption": "",
        "external_paper_link": "",
        "youtube_ff_link": "",
        "youtube_ff_id": "",
        "bunny_ff_link": "",
        "bunny_ff_subtitles": "",
        "youtube_prerecorded_link": "",
        "youtube_prerecorded_id": "",
        "bunny_prerecorded_link": "",
        "bunny_prerecorded_subtitles": ""
    },
    "v-full-1395": {
        "slot_id": "v-full-1395",
        "session_id": "full0",
        "title": "D-Tour: Semi-Automatic Generation of Interactive Guided Tours for Visualization Dashboard Onboarding",
        "contributors": [
            "Vaishali Dhanoa"
        ],
        "authors": [
            {
                "name": "Vaishali Dhanoa",
                "email": "vaishali.dhanoa@pro2future.at",
                "affiliations": [
                    "Pro2Future GmbH, Linz, Austria",
                    "Johannes Kepler University, Linz, Austria"
                ],
                "is_corresponding": true
            },
            {
                "name": "Andreas Hinterreiter",
                "email": "andreas.hinterreiter@jku.at",
                "affiliations": [
                    "Johannes Kepler University, Linz, Austria"
                ],
                "is_corresponding": false
            },
            {
                "name": "Vanessa Fediuk",
                "email": "vanessa.fediuk@jku.at",
                "affiliations": [
                    "Johannes Kepler University, Linz, Austria"
                ],
                "is_corresponding": false
            },
            {
                "name": "Niklas Elmqvist",
                "email": "elm@cs.au.dk",
                "affiliations": [
                    "Aarhus University, Aarhus, Denmark"
                ],
                "is_corresponding": false
            },
            {
                "name": "Eduard Gr\u00f6ller",
                "email": "groeller@cg.tuwien.ac.at",
                "affiliations": [
                    "Institute of Visual Computing ",
                    " Human-Centered Technology, Vienna, Austria"
                ],
                "is_corresponding": false
            },
            {
                "name": "Marc Streit",
                "email": "marc.streit@jku.at",
                "affiliations": [
                    "Johannes Kepler University Linz, Linz, Austria"
                ],
                "is_corresponding": false
            }
        ],
        "abstract": "Onboarding a user to a visualization dashboard entails explaining its various components, including the chart types used, the data loaded, and the interactions provided. Authoring such an onboarding experience is time-consuming and requires significant knowledge, and little guidance exists on how best to do this. End-users being onboarded to a new dashboard can be either confused and overwhelmed, or disinterested and disengaged, depending on the user\u2019s expertise. We propose interactive dashboard tours (d-tours) as semi-automated onboarding experiences for variable user expertise that preserve the user\u2019s agency, interest, and engagement. Our interactive tours concept draws from open-world game design to give the user freedom in choosing their path in the onboarding. We have implemented the concept in a tool called D-TOUR PROTOTYPE that allows authors to craft custom and interactive dashboard tours from scratch or using automatic templates. Automatically generated tours can still be customized to use different media (such as video, audio, or highlighting) or new narratives to produce a tailored onboarding experience for individual users or groups. We demonstrate the usefulness of interactive dashboard tours through use cases and expert interviews.   The evaluation shows that the authors find the automation in the DTour prototype helpful and time-saving and the users find it engaging and intuitive. This paper and all supplemental materials are available at \\url{https://osf.io/6fbjp/}.",
        "uid": "v-full-1395",
        "time_stamp": "",
        "time_start": "",
        "time_end": "",
        "paper_type": "full",
        "keywords": [],
        "doi": "",
        "fno": "",
        "presentation_mode": "",
        "open_access": false,
        "accessible_pdf": false,
        "has_image": false,
        "has_pdf": false,
        "paper_award": "",
        "image_caption": "",
        "external_paper_link": "",
        "youtube_ff_link": "",
        "youtube_ff_id": "",
        "bunny_ff_link": "",
        "bunny_ff_subtitles": "",
        "youtube_prerecorded_link": "",
        "youtube_prerecorded_id": "",
        "bunny_prerecorded_link": "",
        "bunny_prerecorded_subtitles": ""
    },
    "v-full-1414": {
        "slot_id": "v-full-1414",
        "session_id": "full0",
        "title": "Unveiling How Examples Shape Data Visualization Design Outcomes",
        "contributors": [
            "Hannah K. Bako"
        ],
        "authors": [
            {
                "name": "Hannah K. Bako",
                "email": "hbako@umd.edu",
                "affiliations": [
                    "University of Maryland, College Park, United States"
                ],
                "is_corresponding": true
            },
            {
                "name": "Xinyi Liu",
                "email": "xinyi.liu@utexas.edu",
                "affiliations": [
                    "The University of Texas at Austin, Austin, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Grace Ko",
                "email": "gko1@terpmail.umd.edu",
                "affiliations": [
                    "University of Maryland, College Park, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Hyemi Song",
                "email": "hsong02@cs.umd.edu",
                "affiliations": [
                    "Human Data Interaction Lab, College Park, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Leilani Battle",
                "email": "leibatt@cs.washington.edu",
                "affiliations": [
                    "University of Washington, Seattle, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Zhicheng Liu",
                "email": "leozcliu@umd.edu",
                "affiliations": [
                    "University of Maryland, College Park, United States"
                ],
                "is_corresponding": false
            }
        ],
        "abstract": "Visualization designers often rely on examples to explore the space of possible designs, yet we have little insight into how examples shape data visualization design outcomes. While the effects of examples have been studied in other disciplines, such as web design or engineering, the results are not readily applicable to visualization design due to inconsistencies in findings and challenges unique to visualization design. Towards bridging this gap, we conduct an exploratory experiment involving 32 data visualization designers focusing on the influence of five factors (timing, quantity, diversity, data topic similarity, and data schema similarity) on objectively measurable design outcomes (e.g., numbers of designs and idea transfers). Our quantitative analysis shows that when examples are introduced after initial brainstorming, designers curate examples with topics less similar to the dataset they are working on and produce more designs with a high variation in visualization components. Also, designers copy more ideas from examples with higher data schema similarities. Our qualitative analysis of participants\u2019 thought processes provides insights into why designers incorporate examples into their designs, revealing potential factors that have not been previously investigated. Finally, we discuss how our results inform future work on quantifying designs, improving measures of effectiveness, and supporting example-based visualization design. All supplementary materials are available at https://osf.io/sbp2k/?view_only=ca14af497f5845a0b1b2c616699fefc5",
        "uid": "v-full-1414",
        "time_stamp": "",
        "time_start": "",
        "time_end": "",
        "paper_type": "full",
        "keywords": [],
        "doi": "",
        "fno": "",
        "presentation_mode": "",
        "open_access": false,
        "accessible_pdf": false,
        "has_image": false,
        "has_pdf": false,
        "paper_award": "",
        "image_caption": "",
        "external_paper_link": "",
        "youtube_ff_link": "",
        "youtube_ff_id": "",
        "bunny_ff_link": "",
        "bunny_ff_subtitles": "",
        "youtube_prerecorded_link": "",
        "youtube_prerecorded_id": "",
        "bunny_prerecorded_link": "",
        "bunny_prerecorded_subtitles": ""
    },
    "v-full-1416": {
        "slot_id": "v-full-1416",
        "session_id": "full0",
        "title": "Manipulable Semantic Components: a Computational Representation of Data Visualization Scenes",
        "contributors": [
            "Zhicheng Liu"
        ],
        "authors": [
            {
                "name": "Zhicheng Liu",
                "email": "leozcliu@umd.edu",
                "affiliations": [
                    "University of Maryland, College Park, United States"
                ],
                "is_corresponding": true
            },
            {
                "name": "Chen Chen",
                "email": "cchen24@umd.edu",
                "affiliations": [
                    "University of Maryland, College Park, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "John Hooker",
                "email": "hookerj100@gmail.com",
                "affiliations": [
                    "University of Maryland, College Park, United States"
                ],
                "is_corresponding": false
            }
        ],
        "abstract": "Various data visualization downstream applications such as reverse engineering and interactive authoring require a vocabulary that describes the structure of visualization scenes and the procedure to manipulate them. A few scene abstractions have been proposed, but they are restricted to specific applications for a limited set of visualization types. A unified and expressive model of data visualization scenes for different downstream applications has been missing. To fill this gap, we present Manipulable Semantic Components (MSC), a computational representation of data visualization scenes, to support applications in scene understanding and augmentation. MSC consists of two parts: a unified object model describing the structure of a visualization scene in terms of semantic components, and a set of operations to generate and modify the scene components. We demonstrate the benefits of MSC in three applications: visualization authoring, visualization deconstruction and reuse, and animation specification.",
        "uid": "v-full-1416",
        "time_stamp": "",
        "time_start": "",
        "time_end": "",
        "paper_type": "full",
        "keywords": [],
        "doi": "",
        "fno": "",
        "presentation_mode": "",
        "open_access": false,
        "accessible_pdf": false,
        "has_image": false,
        "has_pdf": false,
        "paper_award": "",
        "image_caption": "",
        "external_paper_link": "",
        "youtube_ff_link": "",
        "youtube_ff_id": "",
        "bunny_ff_link": "",
        "bunny_ff_subtitles": "",
        "youtube_prerecorded_link": "",
        "youtube_prerecorded_id": "",
        "bunny_prerecorded_link": "",
        "bunny_prerecorded_subtitles": ""
    },
    "v-full-1422": {
        "slot_id": "v-full-1422",
        "session_id": "full0",
        "title": "Promises and Pitfalls: Using Large Language Models to Generate Visualization Items",
        "contributors": [
            "Yuan Cui"
        ],
        "authors": [
            {
                "name": "Yuan Cui",
                "email": "yuancui2025@u.northwestern.edu",
                "affiliations": [
                    "Northwestern University, Evanston, United States"
                ],
                "is_corresponding": true
            },
            {
                "name": "Lily W. Ge",
                "email": "wanqian.ge@northwestern.edu",
                "affiliations": [
                    "Northwestern University, Evanston, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Yiren Ding",
                "email": "yding5@wpi.edu",
                "affiliations": [
                    "Worcester Polytechnic Institute, Worcester, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Lane Harrison",
                "email": "ltharrison@wpi.edu",
                "affiliations": [
                    "Worcester Polytechnic Institute, Worcester, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Fumeng Yang",
                "email": "fumeng.p.yang@gmail.com",
                "affiliations": [
                    "Northwestern University, Evanston, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Matthew Kay",
                "email": "matthew.kay@gmail.com",
                "affiliations": [
                    "Northwestern University, Chicago, United States"
                ],
                "is_corresponding": false
            }
        ],
        "abstract": "Visualization items\u2014factual questions about visualizations that ask viewers to accomplish visualization tasks\u2014are regularly used in the field of information visualization as educational and evaluative materials. For example, researchers of visualization literacy require large, diverse banks of items to conduct studies where the same skill is measured repeatedly on the same participants. Yet, generating a large number of high-quality, diverse items requires significant time and expertise. To address the critical need for a large number of diverse visualization items in education and research, this paper investigates the potential for large language models (LLMs) to automate the generation of multiple-choice visualization items. Through an iterative design process, we develop an LLM-based pipeline, the VILA (Visualization Items Generated by Large LAnguage Models) pipeline, for efficiently generating visualization items that measure people\u2019s ability to accomplish visualization tasks. We use the VILA pipeline to generate 1,404 candidate items across 12 chart types and 13 visualization tasks. In collaboration with 11 visualization experts, we develop an evaluation rulebook which we then use to rate the quality of all candidate items. The result is a final bank, the VILA bank, of \u223c1,100 items. From this evaluation, we also identify and classify current limitations of LLMs in generating visualization items, and discuss the role of human oversight in ensuring quality. In addition, we demonstrate an application of our work by creating a visualization literacy test, VILA-VLAT, which measures people\u2019s ability to complete a diverse set of tasks on various types of visualizations; to show the potential of this application, we assess the convergent validity of VILA-VLAT by comparing it to the existing test VLAT via an online study (R = 0.70). Lastly, we discuss the application areas of the VILA pipeline and the VILA bank and provide practical recommendations for their use. All supplemental materials are available at https://osf.io/ysrhq/?view_only=e31b3ddf216e4351bb37bcedf744e9d6.",
        "uid": "v-full-1422",
        "time_stamp": "",
        "time_start": "",
        "time_end": "",
        "paper_type": "full",
        "keywords": [],
        "doi": "",
        "fno": "",
        "presentation_mode": "",
        "open_access": false,
        "accessible_pdf": false,
        "has_image": false,
        "has_pdf": false,
        "paper_award": "",
        "image_caption": "",
        "external_paper_link": "",
        "youtube_ff_link": "",
        "youtube_ff_id": "",
        "bunny_ff_link": "",
        "bunny_ff_subtitles": "",
        "youtube_prerecorded_link": "",
        "youtube_prerecorded_id": "",
        "bunny_prerecorded_link": "",
        "bunny_prerecorded_subtitles": ""
    },
    "v-full-1425": {
        "slot_id": "v-full-1425",
        "session_id": "full0",
        "title": "DG Comics: Semi-Automatically Authoring Graph Comics for Dynamic Graphs",
        "contributors": [
            "Joohee Kim"
        ],
        "authors": [
            {
                "name": "Joohee Kim",
                "email": "joohee@unist.ac.kr",
                "affiliations": [
                    "Ulsan National Institute of Science and Technology, Ulsan, Korea, Republic of"
                ],
                "is_corresponding": true
            },
            {
                "name": "Hyunwook Lee",
                "email": "gusdnr0916@unist.ac.kr",
                "affiliations": [
                    "Ulsan National Institute of Science and Technology, Ulsan, Korea, Republic of"
                ],
                "is_corresponding": false
            },
            {
                "name": "Duc M. Nguyen",
                "email": "ducnm@unist.ac.kr",
                "affiliations": [
                    "Ulsan National Institute of Science and Technology, Ulsan, Korea, Republic of"
                ],
                "is_corresponding": false
            },
            {
                "name": "Minjeong Shin",
                "email": "minjeong.shin@anu.edu.au",
                "affiliations": [
                    "Australian National University, Canberra, Australia"
                ],
                "is_corresponding": false
            },
            {
                "name": "Bum Chul Kwon",
                "email": "bumchul.kwon@us.ibm.com",
                "affiliations": [
                    "IBM Research, Cambridge, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Sungahn Ko",
                "email": "sako@unist.ac.kr",
                "affiliations": [
                    "UNIST, Ulsan, Korea, Republic of"
                ],
                "is_corresponding": false
            },
            {
                "name": "Niklas Elmqvist",
                "email": "elm@cs.au.dk",
                "affiliations": [
                    "Aarhus University, Aarhus, Denmark"
                ],
                "is_corresponding": false
            }
        ],
        "abstract": "Comics have been shown to be an effective method for sequential data-driven storytelling, especially for dynamic graphs that change over time. However, manually creating a data-driven comic for a dynamic graph is currently time-consuming, complex, and error-prone. In this paper, we propose DG Comics, a novel comic authoring tool for dynamic graphs that allows users to semi-automatically build the comic and annotate it. The tool uses a hierarchical clustering algorithm that we newly developed for segmenting consecutive snapshots of the dynamic graph while preserving their chronological order. It also provides rich information on both individuals and communities extracted from dynamic graphs in multiple views, where users can explore dynamic graphs and choose what to tell in comics. For evaluation, we provide an example and report results from a user study and expert review.",
        "uid": "v-full-1425",
        "time_stamp": "",
        "time_start": "",
        "time_end": "",
        "paper_type": "full",
        "keywords": [],
        "doi": "",
        "fno": "",
        "presentation_mode": "",
        "open_access": false,
        "accessible_pdf": false,
        "has_image": false,
        "has_pdf": false,
        "paper_award": "",
        "image_caption": "",
        "external_paper_link": "",
        "youtube_ff_link": "",
        "youtube_ff_id": "",
        "bunny_ff_link": "",
        "bunny_ff_subtitles": "",
        "youtube_prerecorded_link": "",
        "youtube_prerecorded_id": "",
        "bunny_prerecorded_link": "",
        "bunny_prerecorded_subtitles": ""
    },
    "v-full-1427": {
        "slot_id": "v-full-1427",
        "session_id": "full0",
        "title": "ParamsDrag: Interactive Parameter Space Exploration via Image-Space Dragging",
        "contributors": [
            "Guan Li"
        ],
        "authors": [
            {
                "name": "Guan Li",
                "email": "liguan@sccas.cn",
                "affiliations": [
                    "Computer Network Information Center, Chinese Academy of Sciences, Beijing, China",
                    "University of Chinese Academy of Sciences, Beijing, China"
                ],
                "is_corresponding": true
            },
            {
                "name": "Yang Liu",
                "email": "leo_edumail@163.com",
                "affiliations": [
                    "Beijing Forestry University, Beijing, China"
                ],
                "is_corresponding": false
            },
            {
                "name": "Guihua Shan",
                "email": "sgh@sccas.cn",
                "affiliations": [
                    "Computer Network Information Center, Chinese Academy of Sciences, Beijing, China"
                ],
                "is_corresponding": false
            },
            {
                "name": "Shiyu Cheng",
                "email": "chengshiyu@cnic.cn",
                "affiliations": [
                    "Chinese Academy of Sciences, Beijing, China"
                ],
                "is_corresponding": false
            },
            {
                "name": "Weiqun Cao",
                "email": "weiqun.cao@126.com",
                "affiliations": [
                    "Beijing Forestry University, Beijing, China"
                ],
                "is_corresponding": false
            },
            {
                "name": "Junpeng Wang",
                "email": "junpeng.wang.nk@gmail.com",
                "affiliations": [
                    "Visa Research, Palo Alto, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Ko-Chih Wang",
                "email": "caseywang777@gmail.com",
                "affiliations": [
                    "National Taiwan Normal University, Taipei City, Taiwan"
                ],
                "is_corresponding": false
            }
        ],
        "abstract": "Numerical simulation serves as a cornerstone in scientific modeling, yet the process of fine-tuning simulation parameters poses significant challenges. Conventionally, parameter adjustment relies on extensive numerical simulations, data analysis, and expert insights, resulting in substantial computational costs and low efficiency. The emergence of deep learning in recent years has provided promising avenues for more efficient exploration of parameter spaces. However, existing approaches often lack intuitive methods for precise parameter adjustment and optimization. To tackle these challenges, we introduce ParamsDrag, a model that facilitates parameter space exploration through direct interaction with visualizations. Inspired by DragGAN, our ParamsDrag model operates in three steps. First, the generative component of ParamsDrag generates visualizations based on the input simulation parameters. Second, by directly dragging structure-related features in the visualizations, users can intuitively understand the controlling effect of different parameters. Third, with the understanding from the earlier step, users can steer ParamsDrag to produce dynamic visual outcomes. Through experiments conducted on real-world simulations and comparisons with state-of-the-art deep learning based approaches, we demonstrate the efficacy of our solution.",
        "uid": "v-full-1427",
        "time_stamp": "",
        "time_start": "",
        "time_end": "",
        "paper_type": "full",
        "keywords": [],
        "doi": "",
        "fno": "",
        "presentation_mode": "",
        "open_access": false,
        "accessible_pdf": false,
        "has_image": false,
        "has_pdf": false,
        "paper_award": "",
        "image_caption": "",
        "external_paper_link": "",
        "youtube_ff_link": "",
        "youtube_ff_id": "",
        "bunny_ff_link": "",
        "bunny_ff_subtitles": "",
        "youtube_prerecorded_link": "",
        "youtube_prerecorded_id": "",
        "bunny_prerecorded_link": "",
        "bunny_prerecorded_subtitles": ""
    },
    "v-full-1438": {
        "slot_id": "v-full-1438",
        "session_id": "full0",
        "title": "Defogger: A Visual Analysis Approach for Data Exploration of Sensitive Data Protected by Differential Privacy",
        "contributors": [
            "Xumeng Wang"
        ],
        "authors": [
            {
                "name": "Xumeng Wang",
                "email": "wangxumeng@nankai.edu.cn",
                "affiliations": [
                    "Nankai University, Tianjin, China"
                ],
                "is_corresponding": true
            },
            {
                "name": "Shuangcheng Jiao",
                "email": "jiaoshuangcheng@mail.nankai.edu.cn",
                "affiliations": [
                    "Nankai University, Tianjin, China"
                ],
                "is_corresponding": false
            },
            {
                "name": "Chris Bryan",
                "email": "cbryan16@asu.edu",
                "affiliations": [
                    "Arizona State University, Tempe, United States"
                ],
                "is_corresponding": false
            }
        ],
        "abstract": "Differential privacy ensures the security of individual privacy but poses challenges to data exploration processes because the limited privacy budget incapacitates the flexibility of exploration and the noisy feedback of data requests leads to confusing uncertainty. In this study, we take the lead in describing corresponding exploration scenarios, including underlying requirements and available exploration strategies. To facilitate practical applications, we propose a visual analysis approach to the formulation of exploration strategies. Our approach applies a reinforcement learning model to provide diverse suggestions for exploration strategies according to the exploration intent of users. A novel visual design for representing uncertainty in correlation patterns is integrated into our prototype system to support the proposed approach. Finally, we implemented a user study and two case studies. The results of these studies verified that our approach can help develop strategies that satisfy the exploration intent of users.",
        "uid": "v-full-1438",
        "time_stamp": "",
        "time_start": "",
        "time_end": "",
        "paper_type": "full",
        "keywords": [],
        "doi": "",
        "fno": "",
        "presentation_mode": "",
        "open_access": false,
        "accessible_pdf": false,
        "has_image": false,
        "has_pdf": false,
        "paper_award": "",
        "image_caption": "",
        "external_paper_link": "",
        "youtube_ff_link": "",
        "youtube_ff_id": "",
        "bunny_ff_link": "",
        "bunny_ff_subtitles": "",
        "youtube_prerecorded_link": "",
        "youtube_prerecorded_id": "",
        "bunny_prerecorded_link": "",
        "bunny_prerecorded_subtitles": ""
    },
    "v-full-1446": {
        "slot_id": "v-full-1446",
        "session_id": "full0",
        "title": "Visualization Atlases: Explaining and Exploring Complex Topics through Data, Visualization, and Narration",
        "contributors": [
            "Jinrui Wang"
        ],
        "authors": [
            {
                "name": "Jinrui Wang",
                "email": "jinrui.w@outlook.com",
                "affiliations": [
                    "The University of Edinburgh, Edinburgh, United Kingdom"
                ],
                "is_corresponding": true
            },
            {
                "name": "Xinhuan Shu",
                "email": "xinhuan.shu@gmail.com",
                "affiliations": [
                    "Newcastle University, Newcastle Upon Tyne, United Kingdom"
                ],
                "is_corresponding": false
            },
            {
                "name": "Benjamin Bach",
                "email": "bbach@inf.ed.ac.uk",
                "affiliations": [
                    "Inria, Bordeaux, France",
                    "University of Edinburgh, Edinburgh, United Kingdom"
                ],
                "is_corresponding": false
            },
            {
                "name": "Uta Hinrichs",
                "email": "uhinrich@ed.ac.uk",
                "affiliations": [
                    "University of Edinburgh, Edinburgh, United Kingdom"
                ],
                "is_corresponding": false
            }
        ],
        "abstract": "We are currently witnessing an increase in web-based, data-driven initiatives that explain complex, contemporary issues through data and visualizations: climate change, sustainability, AI, or cultural discoveries. Many of these projects call themselves \"atlases\", a term that historically referred to collections of maps or scientific illustrations. To answer the question of what makes a \"visualization atlas\", we conducted a systematic analysis of 33 visualization atlases and semi-structured interviews with eight visualization atlas creators. Based on our results, we contribute (1) a definition of visualization atlases as an emerging format to present complex topics in a holistic, data-driven, and curated way through visualization, (2) a set of design patterns and design dimensions that led to (3) defining 5 visualization atlas genres, and (4) insights into the atlas creation from interviews. We found that visualization atlases are unique in that they combine exploratory visualization with narrative elements from data-driven storytelling and structured navigation mechanisms. They can act as a reference, communication or discovery tools targeting a wide range of audiences with different levels of domain knowledge. We conclude with a discussion of current design practices and emerging questions around the ethics and potential real-world impact of visualization atlases, aimed to inform the design and study of visualization atlases.",
        "uid": "v-full-1446",
        "time_stamp": "",
        "time_start": "",
        "time_end": "",
        "paper_type": "full",
        "keywords": [],
        "doi": "",
        "fno": "",
        "presentation_mode": "",
        "open_access": false,
        "accessible_pdf": false,
        "has_image": false,
        "has_pdf": false,
        "paper_award": "",
        "image_caption": "",
        "external_paper_link": "",
        "youtube_ff_link": "",
        "youtube_ff_id": "",
        "bunny_ff_link": "",
        "bunny_ff_subtitles": "",
        "youtube_prerecorded_link": "",
        "youtube_prerecorded_id": "",
        "bunny_prerecorded_link": "",
        "bunny_prerecorded_subtitles": ""
    },
    "v-full-1451": {
        "slot_id": "v-full-1451",
        "session_id": "full0",
        "title": "User Experience of Visualizations in Motion: A Case Study and Design Considerations",
        "contributors": [
            "Lijie Yao"
        ],
        "authors": [
            {
                "name": "Lijie Yao",
                "email": "yaolijie0219@gmail.com",
                "affiliations": [
                    "Universit\u00e9 Paris-Saclay, CNRS, Orsay, France",
                    "Inria, Saclay, France"
                ],
                "is_corresponding": true
            },
            {
                "name": "Federica Bucchieri",
                "email": "federicabucchieri@gmail.com",
                "affiliations": [
                    "Univerisit\u00e9 Paris-Saclay, CNRS, Orsay, France",
                    "Inria, Saclay, France"
                ],
                "is_corresponding": false
            },
            {
                "name": "Victoria McArthur",
                "email": "dieselfish@gmail.com",
                "affiliations": [
                    "Carleton University, Ottawa, Canada"
                ],
                "is_corresponding": false
            },
            {
                "name": "Anastasia Bezerianos",
                "email": "anastasia.bezerianos@universite-paris-saclay.fr",
                "affiliations": [
                    "LISN, Universit\u00e9 Paris-Saclay, CNRS, INRIA, Orsay, France"
                ],
                "is_corresponding": false
            },
            {
                "name": "Petra Isenberg",
                "email": "petra.isenberg@inria.fr",
                "affiliations": [
                    "Universit\u00e9 Paris-Saclay, CNRS, Orsay, France",
                    "Inria, Saclay, France"
                ],
                "is_corresponding": false
            }
        ],
        "abstract": "We present a systematic review, an empirical study, and a first set of considerations for designing visualizations in motion, derived from a concrete scenario in which these visualizations were used to support a primary task. In practice, when viewers are confronted with embedded visualizations, they often have to focus on a primary task and can only quickly glance at a visualization showing rich, often dynamically updated, information. As such, the visualizations must be designed so as not to distract from the primary task, while at the same time being readable and useful for aiding the primary task. For example, in games, players who are engaged in a battle have to look at their enemies but also read the remaining health of their own game character from the health bar over their character's head. Many trade-offs are possible in the design of embedded visualizations in such dynamic scenarios, which we explore in-depth in this paper with a focus on user experience. We use video games as an example of an application context with a rich existing set of visualizations in motion. We begin our work with a systematic review of in-game visualizations in motion. Next, we conduct an empirical user study to investigate how different embedded visualizations in motion designs impact user experience. We conclude with a set of considerations and trade-offs for designing visualizations in motion more broadly as derived from what we learned about video games. All supplemental materials of this paper are available at osf.io/3v8wm/.",
        "uid": "v-full-1451",
        "time_stamp": "",
        "time_start": "",
        "time_end": "",
        "paper_type": "full",
        "keywords": [],
        "doi": "",
        "fno": "",
        "presentation_mode": "",
        "open_access": false,
        "accessible_pdf": false,
        "has_image": false,
        "has_pdf": false,
        "paper_award": "",
        "image_caption": "",
        "external_paper_link": "",
        "youtube_ff_link": "",
        "youtube_ff_id": "",
        "bunny_ff_link": "",
        "bunny_ff_subtitles": "",
        "youtube_prerecorded_link": "",
        "youtube_prerecorded_id": "",
        "bunny_prerecorded_link": "",
        "bunny_prerecorded_subtitles": ""
    },
    "v-full-1461": {
        "slot_id": "v-full-1461",
        "session_id": "full0",
        "title": "A Practical Solver for Scalar Data Topological Simplification",
        "contributors": [
            "Mohamed KISSI"
        ],
        "authors": [
            {
                "name": "Mohamed KISSI",
                "email": "mohamed.kissi@lip6.fr",
                "affiliations": [
                    "CNRS, Paris, France",
                    "SORBONNE UNIVERSITE, Paris, France"
                ],
                "is_corresponding": true
            },
            {
                "name": "Mathieu Pont",
                "email": "mathieu.pont@lip6.fr",
                "affiliations": [
                    "CNRS, Paris, France",
                    "Sorbonne Universit\u00e9, Paris, France"
                ],
                "is_corresponding": false
            },
            {
                "name": "Joshua A Levine",
                "email": "josh@cs.arizona.edu",
                "affiliations": [
                    "University of Arizona, Tucson, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Julien Tierny",
                "email": "julien.tierny@sorbonne-universite.fr",
                "affiliations": [
                    "CNRS, Paris, France",
                    "Sorbonne Universit\u00e9, Paris, France"
                ],
                "is_corresponding": false
            }
        ],
        "abstract": "This paper presents a practical approach for the optimization of topological simplification, a central pre-processing step for the analysis and visualization of scalar data. Given an input scalar field f and a set of \u201csignal\u201d persistence pairs to maintain, our approaches produces an output field g that is close to f and which optimizes (i) the cancellation of \u201cnon-signal\u201d pairs, while (ii) preserving the \u201csignal\u201d pairs. In contrast to pre-existing simplification approaches, our method is not restricted to persistence pairs involving extrema and can thus address a larger class of topological features, in particular saddle pairs in three-dimensional scalar data. Our approach leverages recent generic persistence optimization frameworks and extends them with tailored accelerations specific to the problem of topological simplification. Extensive experiments report substantial accelerations over these frameworks, thereby making topological simplification optimization practical for real-life datasets. Our work enables a direct visualization and analysis of the topologically simplified data, e.g., via isosurfaces of simplified topology (fewer components and handles). We apply our approach to the extraction of prominent filament structures in three-dimensional data. Specifically, we show that our pre-simplification of the data leads to practical improvements over standard topological techniques for removing filament loops. We also show how our framework can be used to repair genus defects in surface processing. Finally, we provide a C++ implementation for reproducibility purposes.",
        "uid": "v-full-1461",
        "time_stamp": "",
        "time_start": "",
        "time_end": "",
        "paper_type": "full",
        "keywords": [],
        "doi": "",
        "fno": "",
        "presentation_mode": "",
        "open_access": false,
        "accessible_pdf": false,
        "has_image": false,
        "has_pdf": false,
        "paper_award": "",
        "image_caption": "",
        "external_paper_link": "",
        "youtube_ff_link": "",
        "youtube_ff_id": "",
        "bunny_ff_link": "",
        "bunny_ff_subtitles": "",
        "youtube_prerecorded_link": "",
        "youtube_prerecorded_id": "",
        "bunny_prerecorded_link": "",
        "bunny_prerecorded_subtitles": ""
    },
    "v-full-1472": {
        "slot_id": "v-full-1472",
        "session_id": "full0",
        "title": "DracoGPT: Extracting Visualization Design Preferences from Large Language Models",
        "contributors": [
            "Huichen Will Wang"
        ],
        "authors": [
            {
                "name": "Huichen Will Wang",
                "email": "wwill@cs.washington.edu",
                "affiliations": [
                    "University of Washington, Seattle, United States"
                ],
                "is_corresponding": true
            },
            {
                "name": "Mitchell L. Gordon",
                "email": "mgord@cs.stanford.edu",
                "affiliations": [
                    "University of Washington, Seattle, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Leilani Battle",
                "email": "leibatt@cs.washington.edu",
                "affiliations": [
                    "University of Washington, Seattle, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Jeffrey Heer",
                "email": "jheer@uw.edu",
                "affiliations": [
                    "University of Washington, Seattle, United States"
                ],
                "is_corresponding": false
            }
        ],
        "abstract": "Trained on vast corpora, Large Language Models (LLMs) have the potential to encode visualization design knowledge and best practices. However, if they fail to do so, they might provide unreliable visualization recommendations. What visualization design preferences, then, have LLMs learned? We contribute DracoGPT, an approach for extracting and modeling visualization design preferences from LLMs. To assess varied tasks, we develop two pipelines---DracoGPT-Rank and DracoGPT-Recommend---to model LLMs prompted to either rank or recommend visual encoding specifications. We use Draco as a shared knowledge base in which to represent LLM design preferences and compare them to best practices from empirical research. We demonstrate that DracoGPT models the preferences expressed by LLMs well, enabling analysis in terms of Draco design constraints. Across a suite of backing LLMs, we find that DracoGPT-Rank and DracoGPT-Recommend moderately agree with each other, but both substantively diverge from guidelines drawn from human subjects experiments. Future work can build on our approach to expand Draco's knowledge base to model a richer set of preferences and serve as a reliable and cost-effective stand-in for LLMs.",
        "uid": "v-full-1472",
        "time_stamp": "",
        "time_start": "",
        "time_end": "",
        "paper_type": "full",
        "keywords": [],
        "doi": "",
        "fno": "",
        "presentation_mode": "",
        "open_access": false,
        "accessible_pdf": false,
        "has_image": false,
        "has_pdf": false,
        "paper_award": "",
        "image_caption": "",
        "external_paper_link": "",
        "youtube_ff_link": "",
        "youtube_ff_id": "",
        "bunny_ff_link": "",
        "bunny_ff_subtitles": "",
        "youtube_prerecorded_link": "",
        "youtube_prerecorded_id": "",
        "bunny_prerecorded_link": "",
        "bunny_prerecorded_subtitles": ""
    },
    "v-full-1474": {
        "slot_id": "v-full-1474",
        "session_id": "full0",
        "title": "Towards Dataset-scale and Feature-oriented Evaluation of Text Summarization in Large Language Model Prompts",
        "contributors": [
            "Sam Yu-Te Lee"
        ],
        "authors": [
            {
                "name": "Sam Yu-Te Lee",
                "email": "ytlee@ucdavis.edu",
                "affiliations": [
                    "University of California Davis, Davis, United States"
                ],
                "is_corresponding": true
            },
            {
                "name": "Aryaman Bahukhandi",
                "email": "abahukhandi@ucdavis.edu",
                "affiliations": [
                    "University of California, Davis, Davis, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Dongyu Liu",
                "email": "dyuliu@ucdavis.edu",
                "affiliations": [
                    "University of California at Davis, Davis, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Kwan-Liu Ma",
                "email": "ma@cs.ucdavis.edu",
                "affiliations": [
                    "University of California at Davis, Davis, United States"
                ],
                "is_corresponding": false
            }
        ],
        "abstract": "Recent advancements in Large Language Models (LLMs) and Prompt Engineering have made chatbot customization more accessible, significantly reducing barriers to tasks that previously required programming skills. However, prompt evaluation, especially at the dataset scale, remains complex due to the need to assess prompts across thousands of test instances within a dataset. Our study, based on a comprehensive literature review and pilot study, summarized five critical challenges in prompt evaluation. In response, we introduce a feature-oriented workflow for systematic prompt evaluation, focusing on text summarization. Our workflow advocates feature metrics such as complexity, formality, or naturalness, instead of using traditional quality metrics like ROUGE. This design choice enables a more user-friendly evaluation of prompts, as it guides users in sorting through the ambiguity inherent in natural language. To support this workflow, we introduce Awesum, a visual analytics system that facilitates identifying optimal prompt refinements through interactive visualizations, featuring a novel Prompt Comparator design that employs a BubbleSet-inspired design enhanced by dimensionality reduction techniques. We evaluate the effectiveness and general applicability of the system with practitioners from various domains and found that (1) our design helps overcome the learning curve for non-technical people to conduct a systematic evaluation, and (2) our feature-oriented workflow has the potential to generalize to other NLG and image-generation tasks. For future works, we advocate moving towards feature-oriented evaluation of LLM prompts and discuss unsolved challenges in terms of human-agent interaction.",
        "uid": "v-full-1474",
        "time_stamp": "",
        "time_start": "",
        "time_end": "",
        "paper_type": "full",
        "keywords": [],
        "doi": "",
        "fno": "",
        "presentation_mode": "",
        "open_access": false,
        "accessible_pdf": false,
        "has_image": false,
        "has_pdf": false,
        "paper_award": "",
        "image_caption": "",
        "external_paper_link": "",
        "youtube_ff_link": "",
        "youtube_ff_id": "",
        "bunny_ff_link": "",
        "bunny_ff_subtitles": "",
        "youtube_prerecorded_link": "",
        "youtube_prerecorded_id": "",
        "bunny_prerecorded_link": "",
        "bunny_prerecorded_subtitles": ""
    },
    "v-full-1480": {
        "slot_id": "v-full-1480",
        "session_id": "full0",
        "title": "Attention-Aware Visualization: Tracking and Responding to User Perception Over Time",
        "contributors": [
            "Arvind Srinivasan"
        ],
        "authors": [
            {
                "name": "Arvind Srinivasan",
                "email": "arvind@cs.au.dk",
                "affiliations": [
                    "Aarhus University, Aarhus, Denmark"
                ],
                "is_corresponding": true
            },
            {
                "name": "Johannes Ellemose",
                "email": "johannes@ellemose.eu",
                "affiliations": [
                    "Aarhus University, Aarhus N, Denmark"
                ],
                "is_corresponding": false
            },
            {
                "name": "Peter W. S. Butcher",
                "email": "p.butcher@bangor.ac.uk",
                "affiliations": [
                    "Bangor University, Bangor, United Kingdom"
                ],
                "is_corresponding": false
            },
            {
                "name": "Panagiotis D. Ritsos",
                "email": "p.ritsos@bangor.ac.uk",
                "affiliations": [
                    "Bangor University, Bangor, United Kingdom"
                ],
                "is_corresponding": false
            },
            {
                "name": "Niklas Elmqvist",
                "email": "elm@cs.au.dk",
                "affiliations": [
                    "Aarhus University, Aarhus, Denmark"
                ],
                "is_corresponding": false
            }
        ],
        "abstract": "We propose the notion of Attention-aware Visualizations (AAVs) that track the user's perception of a visual representation over time and feed this information back to the visualization.This idea is particularly useful for ubiquitous and immersive analytics where knowing which embedded visualizations the user is looking at can be used to make visualizations react appropriately to the user's attention: for example, by highlighting data the user has not yet seen. We can separate the approach into three components: (1) measuring the user's gaze on a visualization and its parts; (2) tracking the user's attention over time; and (3) reactively modifying the visual representation based on the current attention metric. In this paper, we present two separate implementations of AAV: a 2D numeric integration of attention for web-based visualizations that can use an embodied eye-tracker to capture the user's gaze, and a 3D implementation that uses the stencil buffer to track the visibility of each individual mark in a visualization. Both methods provide similar mechanisms for accumulating attention over time and changing the appearance of marks in response. We also present results from a controlled laboratory experiment studying different visual feedback mechanisms for attention.",
        "uid": "v-full-1480",
        "time_stamp": "",
        "time_start": "",
        "time_end": "",
        "paper_type": "full",
        "keywords": [],
        "doi": "",
        "fno": "",
        "presentation_mode": "",
        "open_access": false,
        "accessible_pdf": false,
        "has_image": false,
        "has_pdf": false,
        "paper_award": "",
        "image_caption": "",
        "external_paper_link": "",
        "youtube_ff_link": "",
        "youtube_ff_id": "",
        "bunny_ff_link": "",
        "bunny_ff_subtitles": "",
        "youtube_prerecorded_link": "",
        "youtube_prerecorded_id": "",
        "bunny_prerecorded_link": "",
        "bunny_prerecorded_subtitles": ""
    },
    "v-full-1483": {
        "slot_id": "v-full-1483",
        "session_id": "full0",
        "title": "SpreadLine: Visualizing Egocentric Dynamic Influence",
        "contributors": [
            "Yun-Hsin Kuo"
        ],
        "authors": [
            {
                "name": "Yun-Hsin Kuo",
                "email": "yskuo@ucdavis.edu",
                "affiliations": [
                    "University of California, Davis, Davis, United States"
                ],
                "is_corresponding": true
            },
            {
                "name": "Dongyu Liu",
                "email": "dyuliu@ucdavis.edu",
                "affiliations": [
                    "University of California at Davis, Davis, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Kwan-Liu Ma",
                "email": "ma@cs.ucdavis.edu",
                "affiliations": [
                    "University of California at Davis, Davis, United States"
                ],
                "is_corresponding": false
            }
        ],
        "abstract": "Egocentric networks, often visualized as node-link diagrams, portray the complex relationship (link) dynamics between an entity (node) and others. However, common analytics tasks are multifaceted, encompassing interactions among four key aspects: strength, function, structure, and content. Current node-link visualization designs may fall short, focusing narrowly on certain aspects and neglecting the holistic, dynamic nature of egocentric networks.  To bridge this gap, we introduce SpreadLine, a novel visualization framework designed to enable the visual exploration of egocentric networks from these four aspects at the microscopic level.  Leveraging the intuitive appeal of storyline visualizations, SpreadLine adopts a storyline-based design to represent entities and their evolving relationships. We further encode essential topological information in the layout and condense the contextual information in a metro map metaphor, allowing for a more engaging and effective way to explore temporal and attribute-based information. To guide our work, with a thorough review of pertinent literature, we have distilled a task taxonomy that addresses the analytical needs specific to egocentric network exploration.  Acknowledging the diverse analytical requirements of users, SpreadLine offers customizable encodings to enable users to tailor the framework for their tasks.  We demonstrate the efficacy and general applicability of SpreadLine through three diverse real-world case studies and a usability study.",
        "uid": "v-full-1483",
        "time_stamp": "",
        "time_start": "",
        "time_end": "",
        "paper_type": "full",
        "keywords": [],
        "doi": "",
        "fno": "",
        "presentation_mode": "",
        "open_access": false,
        "accessible_pdf": false,
        "has_image": false,
        "has_pdf": false,
        "paper_award": "",
        "image_caption": "",
        "external_paper_link": "",
        "youtube_ff_link": "",
        "youtube_ff_id": "",
        "bunny_ff_link": "",
        "bunny_ff_subtitles": "",
        "youtube_prerecorded_link": "",
        "youtube_prerecorded_id": "",
        "bunny_prerecorded_link": "",
        "bunny_prerecorded_subtitles": ""
    },
    "v-full-1487": {
        "slot_id": "v-full-1487",
        "session_id": "full0",
        "title": "A Deixis-Centered Approach for Documenting Remote Synchronous Communication around Data Visualizations",
        "contributors": [
            "Chang Han"
        ],
        "authors": [
            {
                "name": "Chang Han",
                "email": "hatch.on27@gmail.com",
                "affiliations": [
                    "University of Utah, Salt Lake City, United States"
                ],
                "is_corresponding": true
            },
            {
                "name": "Katherine E. Isaacs",
                "email": "kisaacs@sci.utah.edu",
                "affiliations": [
                    "The University of Utah, Salt Lake City, United States"
                ],
                "is_corresponding": false
            }
        ],
        "abstract": "Referential gestures, or as termed in linguistics, {\\em deixis}, are an essential part of communication around data visualizations. Despite their importance, such gestures are often overlooked when documenting data analysis meetings. Transcripts, for instance, fail to capture gestures, and video recordings may not adequately capture or emphasize them. We introduce a novel method for documenting collaborative data meetings that treats deixis as a first-class citizen. Our proposed framework captures cursor-based gestural data along with audio and converts them into interactive documents. The framework leverages a large language model to identify word correspondences with gestures. These identified references are used to create context-based annotations in the resulting interactive document. We assess the effectiveness of our proposed method through a user study, finding that participants preferred our automated interactive documentation over recordings, transcripts, and manual note-taking. Furthermore, we derive a preliminary taxonomy of cursor-based deictic gestures from participant actions during the study. This taxonomy offers further opportunities for better utilizing cursor-based deixis in collaborative data analysis scenarios.",
        "uid": "v-full-1487",
        "time_stamp": "",
        "time_start": "",
        "time_end": "",
        "paper_type": "full",
        "keywords": [],
        "doi": "",
        "fno": "",
        "presentation_mode": "",
        "open_access": false,
        "accessible_pdf": false,
        "has_image": false,
        "has_pdf": false,
        "paper_award": "",
        "image_caption": "",
        "external_paper_link": "",
        "youtube_ff_link": "",
        "youtube_ff_id": "",
        "bunny_ff_link": "",
        "bunny_ff_subtitles": "",
        "youtube_prerecorded_link": "",
        "youtube_prerecorded_id": "",
        "bunny_prerecorded_link": "",
        "bunny_prerecorded_subtitles": ""
    },
    "v-full-1488": {
        "slot_id": "v-full-1488",
        "session_id": "full0",
        "title": "The Backstory to \u201cSwaying the Public\u201d: A Design Chronicle of Election Forecast Visualizations",
        "contributors": [
            "Fumeng Yang"
        ],
        "authors": [
            {
                "name": "Fumeng Yang",
                "email": "fumeng.p.yang@gmail.com",
                "affiliations": [
                    "Northwestern University, Evanston, United States"
                ],
                "is_corresponding": true
            },
            {
                "name": "Mandi Cai",
                "email": "mandicai2028@u.northwestern.edu",
                "affiliations": [
                    "Northwestern University, Evanston, United States",
                    "Northwestern University, Evanston, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Chloe Rose Mortenson",
                "email": "chloemortenson2026@u.northwestern.edu",
                "affiliations": [
                    "Northwestern University, Evanston, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Hoda Fakhari",
                "email": "hoda@u.northwestern.edu",
                "affiliations": [
                    "Northwestern University, Evanston, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Ayse Deniz Lokmanoglu",
                "email": "aysedlokmanoglu@gmail.com",
                "affiliations": [
                    "Northwestern University, Evanston, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Nicholas Diakopoulos",
                "email": "nicholas.diakopoulos@gmail.com",
                "affiliations": [
                    "Northwestern University, Evanston, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Erik Nisbet",
                "email": "erik.nisbet@northwestern.edu",
                "affiliations": [
                    "Northwestern University, Evanston, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Matthew Kay",
                "email": "matthew.kay@gmail.com",
                "affiliations": [
                    "Northwestern University, Chicago, United States"
                ],
                "is_corresponding": false
            }
        ],
        "abstract": "A year ago, we submitted an IEEE VIS paper entitled \u201cSwaying the Public? Impacts of Election Forecast Visualizations on Emotion, Trust, and Intention in the 2022 U.S. Midterms\u201d [68], which was later bestowed with the honor of a best paper award. Yet, studying such a complex phenomenon required us to explore many more design paths than we could count, and certainly more than we could document in a single paper. This paper, then, is the unwritten prequel\u2014the backstory. It chronicles our journey from a simple idea\u2014to study visualizations for election forecasts\u2014through obstacles such as developing meaningfully different, easy-to-understand forecast visualizations, crafting professional-looking forecasts, and grappling with how to study perceptions of the forecasts before, during, and after the 2022 U.S. midterm elections. Our backstory began with developing a design space for two-party election forecasts, de\ufb01ning dimensions such as data transformations, visual channels, layouts, and types of animated narratives. We then qualitatively evaluated ten representative prototypes in this design space through interviews with 13 participants. The interviews yielded invaluable insights into how people interpret uncertainty visualizations and reason about probability in a U.S. election context, such as confounding win probability with vote share and erroneously forming connections between concrete visual representations (like dots) and real-world entities (like votes). Informed by these insights, we revised our prototypes to address ambiguity in interpreting visual encodings, particularly through the inclusion of extensive annotations. As we navigated these design paths, we contributed a design space and insights that may help others when designing uncertainty visualizations. We also hope that our design lessons and research process can inspire the research community when exploring topics related to designing visualizations for the general public.",
        "uid": "v-full-1488",
        "time_stamp": "",
        "time_start": "",
        "time_end": "",
        "paper_type": "full",
        "keywords": [],
        "doi": "",
        "fno": "",
        "presentation_mode": "",
        "open_access": false,
        "accessible_pdf": false,
        "has_image": false,
        "has_pdf": false,
        "paper_award": "",
        "image_caption": "",
        "external_paper_link": "",
        "youtube_ff_link": "",
        "youtube_ff_id": "",
        "bunny_ff_link": "",
        "bunny_ff_subtitles": "",
        "youtube_prerecorded_link": "",
        "youtube_prerecorded_id": "",
        "bunny_prerecorded_link": "",
        "bunny_prerecorded_subtitles": ""
    },
    "v-full-1489": {
        "slot_id": "v-full-1489",
        "session_id": "full0",
        "title": "A General Framework for Comparing Embedding Visualizations Across Class-Label Hierarchies",
        "contributors": [
            "Trevor Manz"
        ],
        "authors": [
            {
                "name": "Trevor Manz",
                "email": "trevor_manz@g.harvard.edu",
                "affiliations": [
                    "Harvard Medical School, Boston, United States"
                ],
                "is_corresponding": true
            },
            {
                "name": "Fritz Lekschas",
                "email": "f.lekschas@gmail.com",
                "affiliations": [
                    "Ozette Technologies, Seattle, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Evan Greene",
                "email": "palmergreene@gmail.com",
                "affiliations": [
                    "Ozette Technologies, Seattle, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Greg Finak",
                "email": "greg@ozette.com",
                "affiliations": [
                    "Ozette Technologies, Seattle, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Nils Gehlenborg",
                "email": "nils@hms.harvard.edu",
                "affiliations": [
                    "Harvard Medical School, Boston, United States"
                ],
                "is_corresponding": false
            }
        ],
        "abstract": "Projecting high-dimensional vectors into two dimensions for visualization, known as embedding visualization, facilitates perceptual reasoning and interpretation. Comparison of multiple embedding visualizations drives decision-making in many domains, but conventional comparison methods are limited by a reliance on direct point correspondences. This requirement precludes embedding comparisons without point correspondences, such as two different datasets of annotated images, and fails to capture meaningful higher-level relationships among point groups. To address these shortcomings, we propose a general framework to compare embedding visualizations based on shared class labels rather than individual points. Our approach partitions points into regions corresponding to three key class concepts---confusion, neighborhood, and relative size---to characterize intra- and inter-class relationships. Informed by a preliminary user study, we realize an implementation of our framework using perceptual neighborhood graphs to define these regions and introduce metrics to quantify each concept. We demonstrate the generality of our framework with use cases from machine learning and single-cell biology, highlighting our metrics' ability to surface insightful comparisons across label hierarchies. To assess the effectiveness of our approach, we conducted a user study with five machine learning researchers and six single-cell biologists using an interactive and scalable prototype developed in Python and Rust. Our metrics enable more structured comparison through visual guidance and increased participants\u2019 confidence in their findings.",
        "uid": "v-full-1489",
        "time_stamp": "",
        "time_start": "",
        "time_end": "",
        "paper_type": "full",
        "keywords": [],
        "doi": "",
        "fno": "",
        "presentation_mode": "",
        "open_access": false,
        "accessible_pdf": false,
        "has_image": false,
        "has_pdf": false,
        "paper_award": "",
        "image_caption": "",
        "external_paper_link": "",
        "youtube_ff_link": "",
        "youtube_ff_id": "",
        "bunny_ff_link": "",
        "bunny_ff_subtitles": "",
        "youtube_prerecorded_link": "",
        "youtube_prerecorded_id": "",
        "bunny_prerecorded_link": "",
        "bunny_prerecorded_subtitles": ""
    },
    "v-full-1494": {
        "slot_id": "v-full-1494",
        "session_id": "full0",
        "title": "Localized Evaluation for Constructing Discrete Vector Fields",
        "contributors": [
            "Tanner Finken"
        ],
        "authors": [
            {
                "name": "Tanner Finken",
                "email": "finkent@arizona.edu",
                "affiliations": [
                    "University of Arizona, Tucson, United States"
                ],
                "is_corresponding": true
            },
            {
                "name": "Julien Tierny",
                "email": "julien.tierny@sorbonne-universite.fr",
                "affiliations": [
                    "Sorbonne Universit\u00e9, Paris, France"
                ],
                "is_corresponding": false
            },
            {
                "name": "Joshua A Levine",
                "email": "josh@cs.arizona.edu",
                "affiliations": [
                    "University of Arizona, Tucson, United States"
                ],
                "is_corresponding": false
            }
        ],
        "abstract": "Topological abstractions offer a method to summarize the behavior of vector fields, but computing them robustly can be challenging due to numerical precision issues. One alternative is to represent the vector field using a discrete approach, which constructs a collection of pairs of simplices in the input mesh that satisfies criteria introduced by Forman\u2019s discrete Morse theory. While numerous approaches exist to compute pairs in the restricted case of the gradient of a scalar field, state-of-the-art algorithms for the general case of vector fields require expensive optimization procedures. This paper introduces a fast, novel approach for pairing simplices of two-dimensional, triangulated vector fields that do not vary in time. The key insight of our approach is that we can employ a local evaluation, inspired by the approach used to construct a discrete gradient field, where every cell in a mesh is considered by no more than one of its vertices. Specifically, we observe that for any edge in the input mesh, we can uniquely assign an outward direction of flow. We can further expand this consistent notion of outward flow at each vertex, which corresponds to the concept of a downhill flow in the case of scalar fields. Working with outward flow enables a linear-time algorithm that processes the (outward) neighborhoods of each vertex one-by-one, similar to the approach used for scalar fields. We couple our approach to constructing discrete vector fields with a method to extract, simplify, and visualize topological features. Empirical results on analytic and simulation data demonstrate drastic improvements in running time, produce features similar to the current state-of-the-art, and show the application of simplification to large, complex flows.",
        "uid": "v-full-1494",
        "time_stamp": "",
        "time_start": "",
        "time_end": "",
        "paper_type": "full",
        "keywords": [],
        "doi": "",
        "fno": "",
        "presentation_mode": "",
        "open_access": false,
        "accessible_pdf": false,
        "has_image": false,
        "has_pdf": false,
        "paper_award": "",
        "image_caption": "",
        "external_paper_link": "",
        "youtube_ff_link": "",
        "youtube_ff_id": "",
        "bunny_ff_link": "",
        "bunny_ff_subtitles": "",
        "youtube_prerecorded_link": "",
        "youtube_prerecorded_id": "",
        "bunny_prerecorded_link": "",
        "bunny_prerecorded_subtitles": ""
    },
    "v-full-1500": {
        "slot_id": "v-full-1500",
        "session_id": "full0",
        "title": "Evaluating Force-based Haptics for Immersive Tangible Interactions with Surface Visualizations",
        "contributors": [
            "Hamza Afzaal"
        ],
        "authors": [
            {
                "name": "Hamza Afzaal",
                "email": "hamza.afzaal@ucalgary.ca",
                "affiliations": [
                    "University of Calgary, Calgary, Canada"
                ],
                "is_corresponding": true
            },
            {
                "name": "Usman Alim",
                "email": "ualim@ucalgary.ca",
                "affiliations": [
                    "University of Calgary, Calgary, Canada"
                ],
                "is_corresponding": false
            }
        ],
        "abstract": "Haptic feedback provides an essential sensory stimulus crucial for interacting and analyzing three-dimensional spatio-temporal phenomena on surface visualizations. Given its ability to provide enhanced spatial perception and scene maneuverability, virtual reality (VR) catalyzes haptic interactions on surface visualizations. Various interaction modes, encompassing both mid-air and on-surface interactions---with or without the application of assisting force stimuli---have been explored using haptic force feedback devices. In this paper, we evaluate the use of on-surface and assisted on-surface haptic modes of interaction compared to a no-haptic interaction mode. A force-based haptic stylus is used for all three modalities; the on-surface mode uses collision based forces, whereas the assisted on-surface mode is accompanied by an additional snapping force. We conducted a within-subjects user study involving fundamental interaction tasks performed on surface visualizations. Keeping a consistent visual design across all three modes, our study incorporates tasks that require the localization of the highest, lowest, and random points on surfaces; and tasks that focus on brushing curves on surfaces with varying complexity and occlusion levels. Our findings show that participants took almost the same time to brush curves using all the interaction modes. They could draw smoother curves using the on-surface interaction modes compared to the no-haptic mode. However, the assisted on-surface mode provided better accuracy than the on-surface mode. The on-surface mode was slower in point localization, but the accuracy depended on the visual cues and occlusions associated with the tasks. Finally, we discuss participant feedback on using haptic force feedback as a tangible input modality and share takeaways to aid the design of haptics-based tangible interactions for surface visualizations.",
        "uid": "v-full-1500",
        "time_stamp": "",
        "time_start": "",
        "time_end": "",
        "paper_type": "full",
        "keywords": [],
        "doi": "",
        "fno": "",
        "presentation_mode": "",
        "open_access": false,
        "accessible_pdf": false,
        "has_image": false,
        "has_pdf": false,
        "paper_award": "",
        "image_caption": "",
        "external_paper_link": "",
        "youtube_ff_link": "",
        "youtube_ff_id": "",
        "bunny_ff_link": "",
        "bunny_ff_subtitles": "",
        "youtube_prerecorded_link": "",
        "youtube_prerecorded_id": "",
        "bunny_prerecorded_link": "",
        "bunny_prerecorded_subtitles": ""
    },
    "v-full-1502": {
        "slot_id": "v-full-1502",
        "session_id": "full0",
        "title": "DataGarden: Formalizing Personal Sketches into Structured Visualization Templates",
        "contributors": [
            "Anna Offenwanger"
        ],
        "authors": [
            {
                "name": "Anna Offenwanger",
                "email": "anna.offenwanger@gmail.com",
                "affiliations": [
                    "Universit\u00e9 Paris-Saclay, Orsay, France"
                ],
                "is_corresponding": true
            },
            {
                "name": "Theophanis Tsandilas",
                "email": "theophanis.tsandilas@inria.fr",
                "affiliations": [
                    "Universit\u00e9 Paris-Saclay, CNRS, Inria, LISN, Orsay, France"
                ],
                "is_corresponding": false
            },
            {
                "name": "Fanny Chevalier",
                "email": "fanny@dgp.toronto.edu",
                "affiliations": [
                    "University of Toronto, Toronto, Canada"
                ],
                "is_corresponding": false
            }
        ],
        "abstract": "Visualization is widely used for exploring personal data, but many visualization authoring systems do not support expressing data in flexible, personal, and organic layouts. Sketching is an accessible tool for experimenting with visualization designs, but formalizing sketched elements into structured data representations is difficult, as modifying hand-drawn glyphs to encode data when available is labour-intensive and error prone. We propose an approach where authors structure their own expressive templates, capturing implicit style as well as explicit data mappings, through sketching a representative visualization for an envisioned or partial dataset. Our approach seeks to support freeform exploration and partial specification, balanced against interactive machine support for specifying the generative procedural rules. We implement this approach in DataGarden, a system designed to support hierarchical data visualizations, and evaluate it with 12 participants in a reproduction study and four experts in a freeform creative task. Participants readily picked up the core idea of template authoring, and the variety of workflows we observed highlight how this process serves design and data ideation as well as visual constraint iteration. We discuss challenges in implementing the design considerations underpinning DataGarden, and illustrate its potential in a gallery of visualizations generated from authored templates.",
        "uid": "v-full-1502",
        "time_stamp": "",
        "time_start": "",
        "time_end": "",
        "paper_type": "full",
        "keywords": [],
        "doi": "",
        "fno": "",
        "presentation_mode": "",
        "open_access": false,
        "accessible_pdf": false,
        "has_image": false,
        "has_pdf": false,
        "paper_award": "",
        "image_caption": "",
        "external_paper_link": "",
        "youtube_ff_link": "",
        "youtube_ff_id": "",
        "bunny_ff_link": "",
        "bunny_ff_subtitles": "",
        "youtube_prerecorded_link": "",
        "youtube_prerecorded_id": "",
        "bunny_prerecorded_link": "",
        "bunny_prerecorded_subtitles": ""
    },
    "v-full-1503": {
        "slot_id": "v-full-1503",
        "session_id": "full0",
        "title": "Guided Health-related Information Seeking from LLMs via Knowledge Graph Integration",
        "contributors": [
            "Qianwen Wang"
        ],
        "authors": [
            {
                "name": "Youfu Yan",
                "email": "yan00111@umn.edu",
                "affiliations": [
                    "University of Minnesota, Minneapolis, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Yu Hou",
                "email": "hou00127@umn.edu",
                "affiliations": [
                    "University of Minnesota, Minneapolis, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Yongkang Xiao",
                "email": "xiao0290@umn.edu",
                "affiliations": [
                    "University of Minnesota, Minneapolis, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Rui Zhang",
                "email": "zhan1386@umn.edu",
                "affiliations": [
                    "University of Minnesota, Minneapolis, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Qianwen Wang",
                "email": "qianwen@umn.edu",
                "affiliations": [
                    "University of Minnesota, Minneapolis , United States"
                ],
                "is_corresponding": true
            }
        ],
        "abstract": "The increasing reliance on Large Language Models (LLMs) for health information seeking can pose severe risks due to the potential for misinformation and the complexity of these topics. This paper introduces KnowNet, a visualization system that integrates LLMs with Knowledge Graphs (KG) to provide enhanced accuracy and structured exploration. One core idea in KnowNet is to conceptualize the understanding of a subject as the gradual construction of graph visualization, aligning the user's cognitive process with both the structured data in KGs and the unstructured outputs from LLMs. Specifically, we extracted triples (e.g., entities and their relations) from LLM outputs and mapped them into the validated information and supported evidence in external KGs. Based on the neighborhood of the currently explored entities in KGs, KnowNet provides recommendations for further inquiry, aiming to guide a comprehensive understanding without overlooking critical aspects. A progressive graph visualization is proposed to show the alignment between LLMs and KGs, track previous inquiries, and connect this history with current queries and next-step recommendations. We demonstrate the effectiveness of our system via use cases and expert interviews.",
        "uid": "v-full-1503",
        "time_stamp": "",
        "time_start": "",
        "time_end": "",
        "paper_type": "full",
        "keywords": [],
        "doi": "",
        "fno": "",
        "presentation_mode": "",
        "open_access": false,
        "accessible_pdf": false,
        "has_image": false,
        "has_pdf": false,
        "paper_award": "",
        "image_caption": "",
        "external_paper_link": "",
        "youtube_ff_link": "",
        "youtube_ff_id": "",
        "bunny_ff_link": "",
        "bunny_ff_subtitles": "",
        "youtube_prerecorded_link": "",
        "youtube_prerecorded_id": "",
        "bunny_prerecorded_link": "",
        "bunny_prerecorded_subtitles": ""
    },
    "v-full-1504": {
        "slot_id": "v-full-1504",
        "session_id": "full0",
        "title": "Learnable and Expressive Visualization Authoring Through Blended Interfaces",
        "contributors": [
            "Sehi L'Yi"
        ],
        "authors": [
            {
                "name": "Sehi L'Yi",
                "email": "sehi_lyi@hms.harvard.edu",
                "affiliations": [
                    "Harvard Medical School, Boston, United States"
                ],
                "is_corresponding": true
            },
            {
                "name": "Astrid van den Brandt",
                "email": "a.v.d.brandt@tue.nl",
                "affiliations": [
                    "Eindhoven University of Technology, Eindhoven, Netherlands"
                ],
                "is_corresponding": false
            },
            {
                "name": "Etowah Adams",
                "email": "etowah_adams@hms.harvard.edu",
                "affiliations": [
                    "Harvard Medical School, Boston, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Huyen N. Nguyen",
                "email": "huyen_nguyen@hms.harvard.edu",
                "affiliations": [
                    "Harvard Medical School, Boston, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Nils Gehlenborg",
                "email": "nils@hms.harvard.edu",
                "affiliations": [
                    "Harvard Medical School, Boston, United States"
                ],
                "is_corresponding": false
            }
        ],
        "abstract": "A wide range of visualization authoring interfaces enable the creation of highly customized visualizations. However, prioritizing expressiveness often impedes the learnability of the authoring interface. The diversity of users, such as varying computational skills and prior experiences in user interfaces, makes it even more challenging for a single authoring interface to satisfy the needs of a broad audience. In this paper, we introduce a framework to balance learnability and expressivity in a visualization authoring system. Adopting insights from learnability studies, such as multimodal interaction and visualization literacy, we explore the design space of blending multiple visualization authoring interfaces for supporting authoring tasks in a complementary and flexible manner. To evaluate the effectiveness of blending interfaces, we implemented a proof-of-concept system, Blace, that combines four common visualization authoring interfaces\u2014template-based, shelf configuration, natural language, and code editor\u2014that are tightly linked to one another to help users easily relate unfamiliar interfaces to more familiar ones. Using the system, we conducted a user study with 12 domain experts who regularly visualize genomics data as part of their analysis workflow. Participants with varied visualization and programming backgrounds were able to successfully reproduce complex visualization examples without a guided tutorial in the study. Feedback from a post-study qualitative questionnaire further suggests that blending interfaces enabled participants to learn the system easily and assisted them in confidently editing unfamiliar visualization grammar in the code editor, enabling expressive customization. Reflecting on our study results and the design of our system, we discuss the different interaction patterns that we identified and design implications for blending visualization authoring interfaces.",
        "uid": "v-full-1504",
        "time_stamp": "",
        "time_start": "",
        "time_end": "",
        "paper_type": "full",
        "keywords": [],
        "doi": "",
        "fno": "",
        "presentation_mode": "",
        "open_access": false,
        "accessible_pdf": false,
        "has_image": false,
        "has_pdf": false,
        "paper_award": "",
        "image_caption": "",
        "external_paper_link": "",
        "youtube_ff_link": "",
        "youtube_ff_id": "",
        "bunny_ff_link": "",
        "bunny_ff_subtitles": "",
        "youtube_prerecorded_link": "",
        "youtube_prerecorded_id": "",
        "bunny_prerecorded_link": "",
        "bunny_prerecorded_subtitles": ""
    },
    "v-full-1522": {
        "slot_id": "v-full-1522",
        "session_id": "full0",
        "title": "When Refreshable Tactile Displays Meet Conversational Agents: Investigating Accessible Data Presentation and Analysis with Touch and Speech",
        "contributors": [
            "Samuel Reinders"
        ],
        "authors": [
            {
                "name": "Samuel Reinders",
                "email": "samuel.reinders@monash.edu",
                "affiliations": [
                    "Monash University, Melbourne, Australia"
                ],
                "is_corresponding": true
            },
            {
                "name": "Matthew Butler",
                "email": "matthew.butler@monash.edu",
                "affiliations": [
                    "Monash University, Melbourne, Australia"
                ],
                "is_corresponding": false
            },
            {
                "name": "Ingrid Zukerman",
                "email": "ingrid.zukerman@monash.edu",
                "affiliations": [
                    "Monash University, Clayton, Australia"
                ],
                "is_corresponding": false
            },
            {
                "name": "Bongshin Lee",
                "email": "b.lee@yonsei.ac.kr",
                "affiliations": [
                    "Yonsei University, Seoul, Korea, Republic of",
                    "Microsoft Research, Redmond, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Lizhen Qu",
                "email": "lizhen.qu@monash.edu",
                "affiliations": [
                    "Monash University, Melbourne, Australia"
                ],
                "is_corresponding": false
            },
            {
                "name": "Kim Marriott",
                "email": "kim.marriott@monash.edu",
                "affiliations": [
                    "Monash University, Melbourne, Australia"
                ],
                "is_corresponding": false
            }
        ],
        "abstract": "Despite the recent surge of research efforts to make data visualizations accessible to people who are blind or have low-vision (BLV), how to support BLV people's data analysis remains an important and challenging question. As refreshable tactile displays (RTDs) become cheaper and conversational agents continue to improve, their combination provides a promising approach to support BLV people's interactive data analysis. To understand how BLV people would use and react to a system combining an RTD with a conversational agent, we conducted a Wizard-of-Oz study with 11 BLV participants involving line graphs, bar charts, and isarithmic maps. From an analysis of participant interactions, we identified nine distinct patterns and learned that the choice of modalities depended on the type of task and prior experience with tactile graphics. We also found that participants strongly preferred the combination of RTD and speech to a single modality, and that participants with more tactile experience described how tactile images facilitated deeper engagement with the data and supported independent interpretation. Our findings will inform the design of interfaces for such interactive mixed-modality systems.",
        "uid": "v-full-1522",
        "time_stamp": "",
        "time_start": "",
        "time_end": "",
        "paper_type": "full",
        "keywords": [],
        "doi": "",
        "fno": "",
        "presentation_mode": "",
        "open_access": false,
        "accessible_pdf": false,
        "has_image": false,
        "has_pdf": false,
        "paper_award": "",
        "image_caption": "",
        "external_paper_link": "",
        "youtube_ff_link": "",
        "youtube_ff_id": "",
        "bunny_ff_link": "",
        "bunny_ff_subtitles": "",
        "youtube_prerecorded_link": "",
        "youtube_prerecorded_id": "",
        "bunny_prerecorded_link": "",
        "bunny_prerecorded_subtitles": ""
    },
    "v-full-1533": {
        "slot_id": "v-full-1533",
        "session_id": "full0",
        "title": "DiffFit: Visually-Guided Differentiable Fitting of Molecule Structures to a Cryo-EM Map",
        "contributors": [
            "Deng Luo"
        ],
        "authors": [
            {
                "name": "Deng Luo",
                "email": "deng.luo@kaust.edu.sa",
                "affiliations": [
                    "King Abdullah University of Science and Technology, Thuwal, Saudi Arabia"
                ],
                "is_corresponding": true
            },
            {
                "name": "Zainab Alsuwaykit",
                "email": "zainab.alsuwaykit@kaust.edu.sa",
                "affiliations": [
                    "King Abdullah University of Science and Technology, Thuwal, Saudi Arabia"
                ],
                "is_corresponding": false
            },
            {
                "name": "Dawar Khan",
                "email": "dawar.khan@kaust.edu.sa",
                "affiliations": [
                    "King Abdullah University of Science and Technology, Thuwal, Saudi Arabia"
                ],
                "is_corresponding": false
            },
            {
                "name": "Ond\u0159ej Strnad",
                "email": "ondrej.strnad@kaust.edu.sa",
                "affiliations": [
                    "King Abdullah University of Science and Technology, Thuwal, Saudi Arabia"
                ],
                "is_corresponding": false
            },
            {
                "name": "Tobias Isenberg",
                "email": "tobias.isenberg@gmail.com",
                "affiliations": [
                    "Universit\u00e9 Paris-Saclay, CNRS, Orsay, France",
                    "Inria, Saclay, France"
                ],
                "is_corresponding": false
            },
            {
                "name": "Ivan Viola",
                "email": "ivan.viola@kaust.edu.sa",
                "affiliations": [
                    "King Abdullah University of Science and Technology, Thuwal, Saudi Arabia"
                ],
                "is_corresponding": false
            }
        ],
        "abstract": "We introduce DiffFit, a differentiable algorithm for fitting protein atomistic structures into experimental reconstructed Cryo-Electron Microscopy (cryo-EM) volume map. This process is essential in structural biology to semi-automatically reconstruct large meso-scale models of complex protein assemblies and complete cellular structures that are based on measured cryo-EM  data. Current approaches require manual fitting in 3D that already results in approximately aligned structures followed by an automated fine-tuning of the alignment. With our DiffFit approach, we enable domain scientists to automatically fit new structures and visualize the fitting results for inspection and interactive revision. Our fitting begins with differentiable 3D rigid transformations of the protein atom coordinates, followed by sampling the density values at its atom coordinates from the target cryo-EM volume. To ensure a meaningful correlation between the sampled densities and the protein structure, we propose a novel loss function based on a multi-resolution volume-array approach and the exploitation of the negative space. Such loss function serves as a critical metric for assessing the fitting quality, ensuring both fitting accuracy and improved visualization of the results. We assessed the placement quality of DiffFit with several large, realistic datasets and found its quality to be superior to that of previous methods. We further evaluated our method in two use cases. First, we demonstrate its use in the process of automating the integration of known composite structures into larger protein complexes. Second, we show that it facilitates the fitting of predicted protein domains into volume densities to aid researchers in the identification of unknown proteins. We implemented our algorithm as an open-source plugin (github.com/nanovis/DiffFitViewer) in ChimeraX, a leading visualization software in the field. All supplemental materials are available at osf.io/5tx4q.",
        "uid": "v-full-1533",
        "time_stamp": "",
        "time_start": "",
        "time_end": "",
        "paper_type": "full",
        "keywords": [],
        "doi": "",
        "fno": "",
        "presentation_mode": "",
        "open_access": false,
        "accessible_pdf": false,
        "has_image": false,
        "has_pdf": false,
        "paper_award": "",
        "image_caption": "",
        "external_paper_link": "",
        "youtube_ff_link": "",
        "youtube_ff_id": "",
        "bunny_ff_link": "",
        "bunny_ff_subtitles": "",
        "youtube_prerecorded_link": "",
        "youtube_prerecorded_id": "",
        "bunny_prerecorded_link": "",
        "bunny_prerecorded_subtitles": ""
    },
    "v-full-1544": {
        "slot_id": "v-full-1544",
        "session_id": "full0",
        "title": "How Aligned are Human Chart Takeaways and LLM Predictions? A Case Study on Bar Charts with Varying Layouts",
        "contributors": [
            "Huichen Will Wang"
        ],
        "authors": [
            {
                "name": "Huichen Will Wang",
                "email": "wwill@cs.washington.edu",
                "affiliations": [
                    "University of Washington, Seattle, United States"
                ],
                "is_corresponding": true
            },
            {
                "name": "Jane Hoffswell",
                "email": "jhoffs@adobe.com",
                "affiliations": [
                    "Adobe Research, Seattle, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Sao Myat Thazin Thane",
                "email": "yukithane@gmail.com",
                "affiliations": [
                    "University of Massachusetts Amherst, Amherst, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Victor S. Bursztyn",
                "email": "victorbursztyn2022@u.northwestern.edu",
                "affiliations": [
                    "Adobe Research, San Jose, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Cindy Xiong Bearfield",
                "email": "cxiong@gatech.edu",
                "affiliations": [
                    "Georgia Tech, Atlanta, United States"
                ],
                "is_corresponding": false
            }
        ],
        "abstract": "Large Language Models (LLMs) have been successfully adopted for a variety of visualizations tasks, but how far are we from perceptually aware LLMs that can predict human takeaways from visualizations? Graphical perception literature has shown that human chart takeaways are sensitive to visualization design choices, such as the spatial arrangement. In this work, we examine how well LLMs can predict such design choice sensitivity when generating takeaways, using bar charts with varying spatial layouts as a case study. We test four common chart arrangements: vertically juxtaposed, horizontally juxtaposed, overlaid, and stacked, through three experimental phases. In Phase I, we identified the optimal configuration of LLMs to generate meaningful chart takeaways, across three LLM models (GPT3.5, GPT4, GPT4V, and Gemini 1.0 Pro), two temperature settings (0, 0.7), four chart specifications (Vega-Lite, Matplotlib, ggplot2, and scene graphs), and several prompting strategies. We found that even state-of-the-art LLMs can struggle to generate factually accurate takeaways. In Phase 2, using the most optimal LLM configuration, we generated 30 chart takeaways across the four arrangements of bar charts using two datasets, with both zero-shot and one-shot settings. Compared to data on human takeaways from prior work, we found that the takeaways LLMs generate often do not align with human comparisons. In Phase 3, we examined the effect of the charts\u2019 underlying data values on takeaway alignment between humans and LLMs, and found both matches and mismatches. Overall, our work evaluates the ability of LLMs to emulate human interpretations of data and points to challenges and opportunities in using LLMs to predict human-aligned chart takeaways.",
        "uid": "v-full-1544",
        "time_stamp": "",
        "time_start": "",
        "time_end": "",
        "paper_type": "full",
        "keywords": [],
        "doi": "",
        "fno": "",
        "presentation_mode": "",
        "open_access": false,
        "accessible_pdf": false,
        "has_image": false,
        "has_pdf": false,
        "paper_award": "",
        "image_caption": "",
        "external_paper_link": "",
        "youtube_ff_link": "",
        "youtube_ff_id": "",
        "bunny_ff_link": "",
        "bunny_ff_subtitles": "",
        "youtube_prerecorded_link": "",
        "youtube_prerecorded_id": "",
        "bunny_prerecorded_link": "",
        "bunny_prerecorded_subtitles": ""
    },
    "v-full-1547": {
        "slot_id": "v-full-1547",
        "session_id": "full0",
        "title": "Beware of Validation by Eye: Visual Validation of Linear Trends in Scatterplots",
        "contributors": [
            "Daniel Braun"
        ],
        "authors": [
            {
                "name": "Daniel Braun",
                "email": "braun@cs.uni-koeln.de",
                "affiliations": [
                    "University of Cologne, Cologne, Germany"
                ],
                "is_corresponding": true
            },
            {
                "name": "Remco Chang",
                "email": "remco@cs.tufts.edu",
                "affiliations": [
                    "Tufts University, Medford, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Michael Gleicher",
                "email": "gleicher@cs.wisc.edu",
                "affiliations": [
                    "University of Wisconsin - Madison, Madison, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Tatiana von Landesberger",
                "email": "landesberger@cs.uni-koeln.de",
                "affiliations": [
                    "University of Cologne, Cologne, Germany"
                ],
                "is_corresponding": false
            }
        ],
        "abstract": "Visual validation of regression models in scatterplots is a common practice for assessing model quality, yet its efficacy remains unquantified. We conducted two empirical experiments to investigate individuals' ability to visually validate linear regression models (linear trends) and to examine the impact of common visualization designs on validation quality. The first experiment showed that the level of accuracy for visual estimation of slope (i.e., fitting a line to data) is higher than for visual validation of slope (i.e., accepting a shown line). Notably, we found bias toward slopes that are ''too steep'' in both cases. This lead to novel insights that participants naturally assessed regression with orthogonal distances between the points and the line (i.e., ODR regression) rather than the common vertical distances (OLS regression). In the second experiment, we investigated whether incorporating common designs for regression visualization (error lines, bounding boxes, and confidence intervals) would improve visual validation. Even though error lines reduced validation bias, results failed to show the desired improvements in accuracy for any design. Overall, our findings suggest caution in using visual model validation for linear trends in scatterplots.",
        "uid": "v-full-1547",
        "time_stamp": "",
        "time_start": "",
        "time_end": "",
        "paper_type": "full",
        "keywords": [],
        "doi": "",
        "fno": "",
        "presentation_mode": "",
        "open_access": false,
        "accessible_pdf": false,
        "has_image": false,
        "has_pdf": false,
        "paper_award": "",
        "image_caption": "",
        "external_paper_link": "",
        "youtube_ff_link": "",
        "youtube_ff_id": "",
        "bunny_ff_link": "",
        "bunny_ff_subtitles": "",
        "youtube_prerecorded_link": "",
        "youtube_prerecorded_id": "",
        "bunny_prerecorded_link": "",
        "bunny_prerecorded_subtitles": ""
    },
    "v-full-1568": {
        "slot_id": "v-full-1568",
        "session_id": "full0",
        "title": "DimBridge: Interactive Explanation of Visual Patterns in Dimensionality Reductions with Predicate Logic",
        "contributors": [
            "Brian Montambault"
        ],
        "authors": [
            {
                "name": "Brian Montambault",
                "email": "brianmontambault@gmail.com",
                "affiliations": [
                    "Tufts University, Medford, United States"
                ],
                "is_corresponding": true
            },
            {
                "name": "Gabriel Appleby",
                "email": "gabriel.appleby@tufts.edu",
                "affiliations": [
                    "Tufts University, Medford, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Jen Rogers",
                "email": "jen@cs.tufts.edu",
                "affiliations": [
                    "Tufts University, Boston, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Camelia D. Brumar",
                "email": "camelia_daniela.brumar@tufts.edu",
                "affiliations": [
                    "Tufts University, Medford, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Mingwei Li",
                "email": "mingwei.li@tufts.edu",
                "affiliations": [
                    "Vanderbilt University, Nashville, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Remco Chang",
                "email": "remco@cs.tufts.edu",
                "affiliations": [
                    "Tufts University, Medford, United States"
                ],
                "is_corresponding": false
            }
        ],
        "abstract": "Dimensionality reduction techniques are widely used for visualizing high-dimensional data. However, support for interpreting patterns in dimensionality reduction results in the context of the original data space is often insufficient. Consequently, users may struggle to extract insights from the projections. In this paper we introduce DimBridge, a visual analytics tool that allows users to interact with visual patterns in a projection and retrieve corresponding data patterns. DimBridge supports several interactions, allowing users to perform various analyses, from contrasting multiple clusters to explaining complex latent structures. Leveraging first-order predicate logic, DimBridge identifies subspaces in the original dimensions relevant to a queried pattern and provides an interface for users to visualize and interact with them. We demonstrate how DimBridge can help users overcome the challenges associated with interpreting visual patterns in projections.",
        "uid": "v-full-1568",
        "time_stamp": "",
        "time_start": "",
        "time_end": "",
        "paper_type": "full",
        "keywords": [],
        "doi": "",
        "fno": "",
        "presentation_mode": "",
        "open_access": false,
        "accessible_pdf": false,
        "has_image": false,
        "has_pdf": false,
        "paper_award": "",
        "image_caption": "",
        "external_paper_link": "",
        "youtube_ff_link": "",
        "youtube_ff_id": "",
        "bunny_ff_link": "",
        "bunny_ff_subtitles": "",
        "youtube_prerecorded_link": "",
        "youtube_prerecorded_id": "",
        "bunny_prerecorded_link": "",
        "bunny_prerecorded_subtitles": ""
    },
    "v-full-1571": {
        "slot_id": "v-full-1571",
        "session_id": "full0",
        "title": "Who Let the Guards Out: Visual Support for Patrolling Games",
        "contributors": [
            "Mat\u011bj Lang"
        ],
        "authors": [
            {
                "name": "Mat\u011bj Lang",
                "email": "langm@mail.muni.cz",
                "affiliations": [
                    "Masaryk University, Brno, Czech Republic"
                ],
                "is_corresponding": true
            },
            {
                "name": "Adam \u0160t\u011bp\u00e1nek",
                "email": "469242@mail.muni.cz",
                "affiliations": [
                    "Masaryk University, Brno, Czech Republic"
                ],
                "is_corresponding": false
            },
            {
                "name": "R\u00f3bert Zvara",
                "email": "514179@mail.muni.cz",
                "affiliations": [
                    "Faculty of Informatics, Masaryk University, Brno, Czech Republic"
                ],
                "is_corresponding": false
            },
            {
                "name": "Vojt\u011bch \u0158eh\u00e1k",
                "email": "rehak@fi.muni.cz",
                "affiliations": [
                    "Faculty of Informatics, Masaryk University, Brno, Czech Republic"
                ],
                "is_corresponding": false
            },
            {
                "name": "Barbora Kozlikova",
                "email": "kozlikova@fi.muni.cz",
                "affiliations": [
                    "Masaryk University, Brno, Czech Republic"
                ],
                "is_corresponding": false
            }
        ],
        "abstract": "Effective security patrol management is critical for ensuring safety in diverse environments such as art galleries, airports, and factories. The behavior of patrols in these situations can be modeled by patrolling games. They simulate the behavior of the patrol and adversary in the building, which is modeled as a graph of interconnected nodes representing rooms. The designers of algorithms solving the game face the problem of analyzing complex graph layouts with temporal dependencies. Therefore, appropriate visual support is crucial for them to work effectively. In this paper, we present a novel tool that helps the designers of patrolling games explore the outcomes of the proposed algorithms and approaches, evaluate their success rate, and propose modifications that can improve their solutions. Our tool offers an intuitive and interactive interface, featuring a detailed exploration of patrol routes and probabilities of taking them, simulation of patrols, and other requested features. In close collaboration with experts in designing patrolling games, we conducted three case studies demonstrating the usage and usefulness of our tool. The prototype of the tool, along with exemplary datasets, is available at https://gitlab.fi.muni.cz/formela/strategy-vizualizer.",
        "uid": "v-full-1571",
        "time_stamp": "",
        "time_start": "",
        "time_end": "",
        "paper_type": "full",
        "keywords": [],
        "doi": "",
        "fno": "",
        "presentation_mode": "",
        "open_access": false,
        "accessible_pdf": false,
        "has_image": false,
        "has_pdf": false,
        "paper_award": "",
        "image_caption": "",
        "external_paper_link": "",
        "youtube_ff_link": "",
        "youtube_ff_id": "",
        "bunny_ff_link": "",
        "bunny_ff_subtitles": "",
        "youtube_prerecorded_link": "",
        "youtube_prerecorded_id": "",
        "bunny_prerecorded_link": "",
        "bunny_prerecorded_subtitles": ""
    },
    "v-full-1574": {
        "slot_id": "v-full-1574",
        "session_id": "full0",
        "title": "Objective Lagrangian Vortex Cores and their Visual Representations",
        "contributors": [
            "Tobias G\u00fcnther"
        ],
        "authors": [
            {
                "name": "Tobias G\u00fcnther",
                "email": "tobias.guenther@fau.de",
                "affiliations": [
                    "Friedrich-Alexander-University Erlangen-N\u00fcrnberg, Erlangen, Germany"
                ],
                "is_corresponding": true
            },
            {
                "name": "Holger Theisel",
                "email": "theisel@ovgu.de",
                "affiliations": [
                    "University of Magdeburg, Magdeburg, Germany"
                ],
                "is_corresponding": false
            }
        ],
        "abstract": "The numerical extraction of vortex cores from time-dependent fluid flow attracted much attention over the past decades. A commonly agreed upon vortex definition remained elusive since a proper vortex core needs to satisfy two hard constraints: it must be objective and Lagrangian. Recent methods on objectivization met the first but not the second constraint, since there was no formal guarantee that the resulting vortex coreline is indeed a pathline of the fluid flow. In this paper, we propose the first vortex core definition that is both objective and Lagrangian. Our approach restricts observer motions to follow along pathlines, which reduces the degrees of freedoms: we only need to optimize for an observer rotation that makes the observed flow as steady as possible. This optimization succeeds along Lagrangian vortex corelines and will result in a non-zero time-partial everywhere else. By performing this optimization at each point of a spatial grid, we obtain a residual scalar field, which we call vortex deviation error. The local minima on the grid serve as seed points for a gradient descent optimization that delivers sub-voxel accurate corelines. The visualization of both 2D and 3D vortex cores is based on the separation of the movement of the vortex core and the swirling flow behavior around it. While the vortex core is represented by a pathline, the swirling motion around it is visualized by streamlines in the correct frame. We demonstrate the utility of the approach on several 2D and 3D time-dependent vector fields.",
        "uid": "v-full-1574",
        "time_stamp": "",
        "time_start": "",
        "time_end": "",
        "paper_type": "full",
        "keywords": [],
        "doi": "",
        "fno": "",
        "presentation_mode": "",
        "open_access": false,
        "accessible_pdf": false,
        "has_image": false,
        "has_pdf": false,
        "paper_award": "",
        "image_caption": "",
        "external_paper_link": "",
        "youtube_ff_link": "",
        "youtube_ff_id": "",
        "bunny_ff_link": "",
        "bunny_ff_subtitles": "",
        "youtube_prerecorded_link": "",
        "youtube_prerecorded_id": "",
        "bunny_prerecorded_link": "",
        "bunny_prerecorded_subtitles": ""
    },
    "v-full-1594": {
        "slot_id": "v-full-1594",
        "session_id": "full0",
        "title": "I Came Across a Junk: Understanding Design Flaws of Data Visualization from the Public's Perspective",
        "contributors": [
            "Xingyu Lan"
        ],
        "authors": [
            {
                "name": "Xingyu Lan",
                "email": "xingyulan96@gmail.com",
                "affiliations": [
                    "Fudan University, Shanghai, China",
                    "Fudan University, Shanghai, China"
                ],
                "is_corresponding": true
            },
            {
                "name": "Yu Liu",
                "email": "coraline.liu.dataviz@gmail.com",
                "affiliations": [
                    "University of Edinburgh, Edinburgh, United Kingdom",
                    "University of Edinburgh, Edinburgh, United Kingdom"
                ],
                "is_corresponding": false
            }
        ],
        "abstract": "The visualization community has a rich history of reflecting upon visualization design flaws. Although research in this area has remained lively, we believe it is essential to continuously revisit this classic and critical topic in visualization research by incorporating more empirical evidence from diverse sources, characterizing new design flaws, building more systematic theoretical frameworks, and understanding the underlying reasons for these flaws. To address the above gaps, this work investigated visualization design flaws through the lens of the public, constructed a framework to summarize and categorize the identified flaws, and explored why these flaws occur. Specifically, we analyzed 2227 flawed data visualizations collected from an online gallery and derived a design task-associated taxonomy containing 76 specific design flaws. These flaws were further classified into three high-level categories (i.e., misinformation, uninformativeness, unsociability) and ten subcategories (e.g., inaccuracy, unfairness, ambiguity). Next, we organized five focus groups to explore why these design flaws occur and identified seven causes of the flaws. Finally, we proposed a research agenda for combating visualization design flaws and summarize nine research opportunities.",
        "uid": "v-full-1594",
        "time_stamp": "",
        "time_start": "",
        "time_end": "",
        "paper_type": "full",
        "keywords": [],
        "doi": "",
        "fno": "",
        "presentation_mode": "",
        "open_access": false,
        "accessible_pdf": false,
        "has_image": false,
        "has_pdf": false,
        "paper_award": "",
        "image_caption": "",
        "external_paper_link": "",
        "youtube_ff_link": "",
        "youtube_ff_id": "",
        "bunny_ff_link": "",
        "bunny_ff_subtitles": "",
        "youtube_prerecorded_link": "",
        "youtube_prerecorded_id": "",
        "bunny_prerecorded_link": "",
        "bunny_prerecorded_subtitles": ""
    },
    "v-full-1595": {
        "slot_id": "v-full-1595",
        "session_id": "full0",
        "title": "Dynamic Color Assignment for Hierarchical Data",
        "contributors": [
            "Jiashu Chen"
        ],
        "authors": [
            {
                "name": "Jiashu Chen",
                "email": "jiashu0717c@gmail.com",
                "affiliations": [
                    "Tsinghua University, Beijing, China"
                ],
                "is_corresponding": true
            },
            {
                "name": "Weikai Yang",
                "email": "vicayang496@gmail.com",
                "affiliations": [
                    "Tsinghua University, Beijing, China"
                ],
                "is_corresponding": false
            },
            {
                "name": "Zelin Jia",
                "email": "jiazl22@mails.tsinghua.edu.cn",
                "affiliations": [
                    "Tsinghua University, Beijing, China"
                ],
                "is_corresponding": false
            },
            {
                "name": "Lanxi Xiao",
                "email": "tarolancy@gmail.com",
                "affiliations": [
                    "Tsinghua University, Beijing, China"
                ],
                "is_corresponding": false
            },
            {
                "name": "Shixia Liu",
                "email": "shixia@tsinghua.edu.cn",
                "affiliations": [
                    "Tsinghua University, Beijing, China"
                ],
                "is_corresponding": false
            }
        ],
        "abstract": "Assigning discriminable and harmonic colors to samples according to their class labels and spatial distribution can generate attractive visualizations and facilitate data exploration. However, as the number of classes increases, it is challenging to generate a high-quality color assignment result that accommodates all classes simultaneously. A practical solution is to organize classes into a hierarchy and then dynamically assign colors during exploration. However, existing color assignment methods fall short in generating high-quality color assignment results and dynamically aligning them with hierarchical structures. To address this issue, we develop a dynamic color assignment method for hierarchical data, which is formulated as a multi-objective optimization problem. This method simultaneously considers color discriminability, color harmony, and spatial distribution at each hierarchical level. By using the colors of parent classes to guide the color assignment of their child classes, our method further promotes both consistency and clarity across hierarchical levels. We demonstrate the effectiveness of our method in generating dynamic color assignment results with quantitative experiments and a user study.",
        "uid": "v-full-1595",
        "time_stamp": "",
        "time_start": "",
        "time_end": "",
        "paper_type": "full",
        "keywords": [],
        "doi": "",
        "fno": "",
        "presentation_mode": "",
        "open_access": false,
        "accessible_pdf": false,
        "has_image": false,
        "has_pdf": false,
        "paper_award": "",
        "image_caption": "",
        "external_paper_link": "",
        "youtube_ff_link": "",
        "youtube_ff_id": "",
        "bunny_ff_link": "",
        "bunny_ff_subtitles": "",
        "youtube_prerecorded_link": "",
        "youtube_prerecorded_id": "",
        "bunny_prerecorded_link": "",
        "bunny_prerecorded_subtitles": ""
    },
    "v-full-1597": {
        "slot_id": "v-full-1597",
        "session_id": "full0",
        "title": "Visual Support for the Loop Grafting Workflow on Proteins",
        "contributors": [
            "Katar\u00edna Furmanov\u00e1"
        ],
        "authors": [
            {
                "name": "Filip Op\u00e1len\u00fd",
                "email": "kiraa@mail.muni.cz",
                "affiliations": [
                    "Masaryk University, Brno, Czech Republic"
                ],
                "is_corresponding": false
            },
            {
                "name": "Pavol Ulbrich",
                "email": "paloulbrich@gmail.com",
                "affiliations": [
                    "Masaryk University, Brno, Czech Republic"
                ],
                "is_corresponding": false
            },
            {
                "name": "Joan Planas-Iglesias",
                "email": "joan.planas@mail.muni.cz",
                "affiliations": [
                    "Masaryk University, Brno, Czech Republic",
                    "St. Anne\u2019s University Hospital, Brno, Czech Republic"
                ],
                "is_corresponding": false
            },
            {
                "name": "Jan By\u0161ka",
                "email": "xbyska@fi.muni.cz",
                "affiliations": [
                    "Masaryk University, Brno, Czech Republic",
                    "University of Bergen, Bergen, Norway"
                ],
                "is_corresponding": false
            },
            {
                "name": "Jan \u0160toura\u010d",
                "email": "stourac.jan@gmail.com",
                "affiliations": [
                    "Masaryk University, Brno, Czech Republic",
                    "St. Anne\u2019s University Hospital, Brno, Czech Republic"
                ],
                "is_corresponding": false
            },
            {
                "name": "David Bedn\u00e1\u0159",
                "email": "222755@mail.muni.cz",
                "affiliations": [
                    "Faculty of Science, Masaryk University, Brno, Czech Republic",
                    "St. Anne\u2019s University Hospital Brno, Brno, Czech Republic"
                ],
                "is_corresponding": false
            },
            {
                "name": "Katar\u00edna Furmanov\u00e1",
                "email": "katarina.furmanova@gmail.com",
                "affiliations": [
                    "Masaryk University, Brno, Czech Republic"
                ],
                "is_corresponding": true
            },
            {
                "name": "Barbora Kozlikova",
                "email": "kozlikova@fi.muni.cz",
                "affiliations": [
                    "Masaryk University, Brno, Czech Republic"
                ],
                "is_corresponding": false
            }
        ],
        "abstract": "In understanding and redesigning the function of proteins in modern biochemistry, protein engineers are increasingly focusing on exploring regions in proteins called loops. Analyzing various characteristics of these regions helps the experts design the transfer of the desired function from one protein to another. This process is denoted as loop grafting. We designed a set of interactive visualizations that provide experts with visual support through all the loop grafting pipeline steps. The workflow is divided into several phases, reflecting the steps of the pipeline. Each phase is supported by a specific set of abstracted 2D visual representations of proteins and their loops that are interactively linked with the 3D View of proteins. By sequentially passing through the individual phases, the user shapes the list of loops that are potential candidates for loop grafting. Finally, the actual in-silico insertion of the loop candidates from one protein to the other is performed, and the results are visually presented to the user. In this way, the fully computational rational design of proteins and their loops results in newly designed protein structures that can be further assembled and tested through in-vitro experiments. We showcase the contribution of our visual support design on a real case scenario changing the enantiomer selectivity of the engineered enzyme. Moreover, we provide the readers with the experts' feedback.",
        "uid": "v-full-1597",
        "time_stamp": "",
        "time_start": "",
        "time_end": "",
        "paper_type": "full",
        "keywords": [],
        "doi": "",
        "fno": "",
        "presentation_mode": "",
        "open_access": false,
        "accessible_pdf": false,
        "has_image": false,
        "has_pdf": false,
        "paper_award": "",
        "image_caption": "",
        "external_paper_link": "",
        "youtube_ff_link": "",
        "youtube_ff_id": "",
        "bunny_ff_link": "",
        "bunny_ff_subtitles": "",
        "youtube_prerecorded_link": "",
        "youtube_prerecorded_id": "",
        "bunny_prerecorded_link": "",
        "bunny_prerecorded_subtitles": ""
    },
    "v-full-1599": {
        "slot_id": "v-full-1599",
        "session_id": "full0",
        "title": "SurroFlow: A Flow-Based Surrogate Model for Parameter Space Exploration and Uncertainty Quantification",
        "contributors": [
            "JINGYI SHEN"
        ],
        "authors": [
            {
                "name": "JINGYI SHEN",
                "email": "shen.1250@osu.edu",
                "affiliations": [
                    "The Ohio State University, Columbus, United States",
                    "The Ohio State University, Columbus, United States"
                ],
                "is_corresponding": true
            },
            {
                "name": "Yuhan Duan",
                "email": "duan.418@osu.edu",
                "affiliations": [
                    "The Ohio State University, Columbus, United States",
                    "The Ohio State University, Columbus, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Han-Wei Shen",
                "email": "hwshen@cse.ohio-state.edu",
                "affiliations": [
                    "The Ohio State University , Columbus , United States",
                    "The Ohio State University , Columbus , United States"
                ],
                "is_corresponding": false
            }
        ],
        "abstract": "Existing deep learning-based surrogate models facilitate efficient data generation, but fall short in uncertainty quantification, efficient parameter space exploration, and reverse prediction. In our work, we introduce SurroFlow, a novel normalizing flow-based surrogate model, to learn the invertible transformation between simulation parameters and simulation outputs. The model not only allows accurate predictions of simulation outcomes for a given simulation parameter but also supports uncertainty quantification in the data generation process. Additionally, it enables efficient simulation parameter recommendation and exploration. We integrate SurroFlow and a genetic algorithm as the backend of a visual interface to support effective user-guided ensemble simulation exploration and visualization. Our framework significantly reduces the computational costs while enhancing the reliability and exploration capabilities of scientific surrogate models.",
        "uid": "v-full-1599",
        "time_stamp": "",
        "time_start": "",
        "time_end": "",
        "paper_type": "full",
        "keywords": [],
        "doi": "",
        "fno": "",
        "presentation_mode": "",
        "open_access": false,
        "accessible_pdf": false,
        "has_image": false,
        "has_pdf": false,
        "paper_award": "",
        "image_caption": "",
        "external_paper_link": "",
        "youtube_ff_link": "",
        "youtube_ff_id": "",
        "bunny_ff_link": "",
        "bunny_ff_subtitles": "",
        "youtube_prerecorded_link": "",
        "youtube_prerecorded_id": "",
        "bunny_prerecorded_link": "",
        "bunny_prerecorded_subtitles": ""
    },
    "v-full-1603": {
        "slot_id": "v-full-1603",
        "session_id": "full0",
        "title": "ModalChorus: Visual Probing and Alignment of Multi-modal Embeddings via Modal Fusion Map",
        "contributors": [
            "Yilin Ye"
        ],
        "authors": [
            {
                "name": "Yilin Ye",
                "email": "yyebd@connect.ust.hk",
                "affiliations": [
                    "The Hong Kong University of Science and Technology (Guangzhou), Guangzhou, China"
                ],
                "is_corresponding": true
            },
            {
                "name": "Shishi Xiao",
                "email": "sxiao713@connect.hkust-gz.edu.cn",
                "affiliations": [
                    "The Hong Kong University of Science and Technology(Guangzhou), Guangzhou, China"
                ],
                "is_corresponding": false
            },
            {
                "name": "Xingchen Zeng",
                "email": "xingchen.zeng@outlook.com",
                "affiliations": [
                    "the Hong Kong University of Science and Technology (Guangzhou), Guangzhou, China"
                ],
                "is_corresponding": false
            },
            {
                "name": "Wei Zeng",
                "email": "weizeng@hkust-gz.edu.cn",
                "affiliations": [
                    "The Hong Kong University of Science and Technology (Guangzhou), Guangzhou, China",
                    "The Hong Kong University of Science and Technology, Hong Kong SAR, China"
                ],
                "is_corresponding": false
            }
        ],
        "abstract": "Multi-modal embeddings form the foundation for vision-language models, such as CLIP embeddings, the most widely used text-image embeddings. However, these embeddings are hard to interpret and vulnerable to subtle misalignment of cross-modal features, resulting in decreased model performance and diminished generalization. To address this problem, we design ModalChorus, an interactive system for visual probing and alignment of multi-modal embeddings. ModalChorus primarily offers a two-stage process: 1) embedding probing with Modal Fusion Map (MFM), a novel parametric dimensionality reduction method that integrates both metric and nonmetric objectives to enhance modality fusion; and 2) embedding alignment that allows users to interactively articulate intentions for both point-set and set-set alignments. Quantitative and qualitative comparisons for CLIP embeddings with existing dimensionality reduction (e.g., t-SNE and MDS) and data fusion (e.g., data context map) methods demonstrate the advantages of MFM in showcasing cross-modal features over common vision-language datasets. Case studies reveal that ModalChorus can facilitate intuitive discovery of misalignment and efficient re-alignment in scenarios ranging from zero-shot classification to cross-modal retrieval and generation.",
        "uid": "v-full-1603",
        "time_stamp": "",
        "time_start": "",
        "time_end": "",
        "paper_type": "full",
        "keywords": [],
        "doi": "",
        "fno": "",
        "presentation_mode": "",
        "open_access": false,
        "accessible_pdf": false,
        "has_image": false,
        "has_pdf": false,
        "paper_award": "",
        "image_caption": "",
        "external_paper_link": "",
        "youtube_ff_link": "",
        "youtube_ff_id": "",
        "bunny_ff_link": "",
        "bunny_ff_subtitles": "",
        "youtube_prerecorded_link": "",
        "youtube_prerecorded_id": "",
        "bunny_prerecorded_link": "",
        "bunny_prerecorded_subtitles": ""
    },
    "v-full-1606": {
        "slot_id": "v-full-1606",
        "session_id": "full0",
        "title": "AdaMotif: Graph Simplification via Adaptive Motif Design",
        "contributors": [
            "Hong Zhou"
        ],
        "authors": [
            {
                "name": "Hong Zhou",
                "email": "hzhou@szu.edu.cn",
                "affiliations": [
                    "Shenzhen University, Shenzhen, China"
                ],
                "is_corresponding": true
            },
            {
                "name": "Peifeng Lai",
                "email": "laipeifeng1111@gmail.com",
                "affiliations": [
                    "Shenzhen University, Shenzhen, China"
                ],
                "is_corresponding": false
            },
            {
                "name": "Zhida Sun",
                "email": "zhida.sun@connect.ust.hk",
                "affiliations": [
                    "Shenzhen University, Shenzhen, China"
                ],
                "is_corresponding": false
            },
            {
                "name": "Xiangyuan Chen",
                "email": "2310274034@email.szu.edu.cn",
                "affiliations": [
                    "Shenzhen University, Shenzhen, China"
                ],
                "is_corresponding": false
            },
            {
                "name": "Yang Chen",
                "email": "275621136@qq.com",
                "affiliations": [
                    "Shenzhen University, Shen Zhen, China"
                ],
                "is_corresponding": false
            },
            {
                "name": "Huisi Wu",
                "email": "hswu@szu.edu.cn",
                "affiliations": [
                    "Shenzhen University, Shenzhen, China"
                ],
                "is_corresponding": false
            },
            {
                "name": "Yong WANG",
                "email": "yong-wang@ntu.edu.sg",
                "affiliations": [
                    "Nanyang Technological University, Singapore, Singapore"
                ],
                "is_corresponding": false
            }
        ],
        "abstract": "With the increase of graph size, it becomes difficult or even impossible to visualize graph structures clearly within the limited screen space. Consequently, it is crucial to design effective visual representations for large graphs. In this paper, we propose AdaMotif, a novel approach that can capture the essential structure patterns of large graphs and effectively reveal the overall structures via adaptive motif designs. Specifically, our approach involves partitioning a given large graph into multiple subgraphs, then clustering similar subgraphs and extracting similar structural information within each cluster. Subsequently, adaptive motifs representing each cluster are generated and utilized to replace the corresponding subgraphs, leading to a simplified visualization. Our approach aims to preserve as much information from the subgraphs as possible, effectively simplifying graphs while minimizing information loss. Notably, our approach successfully visualizes crucial community information within a large graph. We conduct case studies and a user study using both synthetic and real-world graphs to validate the effectiveness of our proposed approach. The results demonstrate the capability of our approach in simplifying graphs while retaining important structural and community information.",
        "uid": "v-full-1606",
        "time_stamp": "",
        "time_start": "",
        "time_end": "",
        "paper_type": "full",
        "keywords": [],
        "doi": "",
        "fno": "",
        "presentation_mode": "",
        "open_access": false,
        "accessible_pdf": false,
        "has_image": false,
        "has_pdf": false,
        "paper_award": "",
        "image_caption": "",
        "external_paper_link": "",
        "youtube_ff_link": "",
        "youtube_ff_id": "",
        "bunny_ff_link": "",
        "bunny_ff_subtitles": "",
        "youtube_prerecorded_link": "",
        "youtube_prerecorded_id": "",
        "bunny_prerecorded_link": "",
        "bunny_prerecorded_subtitles": ""
    },
    "v-full-1612": {
        "slot_id": "v-full-1612",
        "session_id": "full0",
        "title": "2D Embeddings of Multi-dimensional Partitionings",
        "contributors": [
            "Marina Evers"
        ],
        "authors": [
            {
                "name": "Marina Evers",
                "email": "m_ever14@uni-muenster.de",
                "affiliations": [
                    "University of M\u00fcnster, M\u00fcnster, Germany"
                ],
                "is_corresponding": true
            },
            {
                "name": "Lars Linsen",
                "email": "linsen@uni-muenster.de",
                "affiliations": [
                    "University of M\u00fcnster, M\u00fcnster, Germany"
                ],
                "is_corresponding": false
            }
        ],
        "abstract": "Partitionings (or segmentations) divide a given domain into disjoint connected regions whose union forms again the entire domain. Multi-dimensional partitionings occur, for example, when analyzing parameter spaces of simulation models, where each segment of the partitioning represents a region of similar model behavior. Having computed a partitioning, one is commonly interested in understanding how large the segments are and which segments lie next to each other. While visual representations of 2D domain partitionings that reveal sizes and neighborhoods are straightforward, this is no longer the case when considering multi-dimensional domains of three or more dimensions. We propose an algorithm for computing 2D embeddings of multi-dimensional partitionings. The embedding shall have the following properties: It shall maintain the topology of the partitioning and optimize the area sizes and joint boundary lengths of the embedded segments to match the respective sizes and lengths in the multi-dimensional domain. We demonstrate the effectiveness of our approach by applying it to different use cases, including the visual exploration of 3D spatial domain segmentations and multi-dimensional parameter space partitionings of simulation ensembles.  We numerically evaluate our algorithm with respect to how well sizes and lengths are preserved depending on the dimensionality of the domain and the number of segments.",
        "uid": "v-full-1612",
        "time_stamp": "",
        "time_start": "",
        "time_end": "",
        "paper_type": "full",
        "keywords": [],
        "doi": "",
        "fno": "",
        "presentation_mode": "",
        "open_access": false,
        "accessible_pdf": false,
        "has_image": false,
        "has_pdf": false,
        "paper_award": "",
        "image_caption": "",
        "external_paper_link": "",
        "youtube_ff_link": "",
        "youtube_ff_id": "",
        "bunny_ff_link": "",
        "bunny_ff_subtitles": "",
        "youtube_prerecorded_link": "",
        "youtube_prerecorded_id": "",
        "bunny_prerecorded_link": "",
        "bunny_prerecorded_subtitles": ""
    },
    "v-full-1613": {
        "slot_id": "v-full-1613",
        "session_id": "full0",
        "title": "Path-based Design Model for Constructing and Exploring Alternative Visualisations",
        "contributors": [
            "Jonathan C Roberts"
        ],
        "authors": [
            {
                "name": "James R Jackson",
                "email": "james.ogge@gmail.com",
                "affiliations": [
                    "ExaDev, Gaerwen, United Kingdom",
                    "Bangor University, Bangor, United Kingdom"
                ],
                "is_corresponding": false
            },
            {
                "name": "Panagiotis D. Ritsos",
                "email": "p.ritsos@bangor.ac.uk",
                "affiliations": [
                    "Bangor University, Bangor, United Kingdom"
                ],
                "is_corresponding": false
            },
            {
                "name": "Peter W. S. Butcher",
                "email": "p.butcher@bangor.ac.uk",
                "affiliations": [
                    "Bangor University, Bangor, United Kingdom"
                ],
                "is_corresponding": false
            },
            {
                "name": "Jonathan C Roberts",
                "email": "j.c.roberts@bangor.ac.uk",
                "affiliations": [
                    "Bangor University, Bangor, United Kingdom"
                ],
                "is_corresponding": true
            }
        ],
        "abstract": "We present a path-based design model and system for designing and creating visualisations. Our model represents a systematic approach to constructing visual representations of data or concepts following a predefined sequence of steps. The initial step involves outlining the overall appearance of the visualisation by creating a skeleton structure, referred to as a flowpath. Subsequently, we specify objects, visual marks, properties, and appearance, storing them in a gene. Lastly, we map data onto the flowpath, ensuring suitable morphisms. Alternative designs are created by exchanging values in the gene. For example, designs that share similar traits, are created by making small incremental changes to the gene. Our design method develops a wide variety of creative ideas, space-filling visualisations, and traditional designs (bar chart, pie chart etc.) Our implementation, demonstrates the model, and we apply the output visualisations onto a smart-watch and on visualisation dashboards. In this article we (1) introduce, define and explain the path model and discuss possibilities for its use, (2) present our implementation, results, and evaluation, and (3) demonstrate and evaluate an application of its use on a mobile watch.",
        "uid": "v-full-1613",
        "time_stamp": "",
        "time_start": "",
        "time_end": "",
        "paper_type": "full",
        "keywords": [],
        "doi": "",
        "fno": "",
        "presentation_mode": "",
        "open_access": false,
        "accessible_pdf": false,
        "has_image": false,
        "has_pdf": false,
        "paper_award": "",
        "image_caption": "",
        "external_paper_link": "",
        "youtube_ff_link": "",
        "youtube_ff_id": "",
        "bunny_ff_link": "",
        "bunny_ff_subtitles": "",
        "youtube_prerecorded_link": "",
        "youtube_prerecorded_id": "",
        "bunny_prerecorded_link": "",
        "bunny_prerecorded_subtitles": ""
    },
    "v-full-1615": {
        "slot_id": "v-full-1615",
        "session_id": "full0",
        "title": "Cell2Cell: Explorative Cell Interaction Analysis in Multi-Volumetric Tissue Data",
        "contributors": [
            "Eric M\u00f6rth"
        ],
        "authors": [
            {
                "name": "Eric M\u00f6rth",
                "email": "eric.moerth@gmx.at",
                "affiliations": [
                    "Harvard Medical School, Boston, United States"
                ],
                "is_corresponding": true
            },
            {
                "name": "Kevin Sidak",
                "email": "kevin.sidak@univie.ac.at",
                "affiliations": [
                    "University of Vienna, Vienna, Austria"
                ],
                "is_corresponding": false
            },
            {
                "name": "Zoltan Maliga",
                "email": "zoltan_maliga@hms.harvard.edu",
                "affiliations": [
                    "Harvard Medical School, Boston, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Torsten M\u00f6ller",
                "email": "torsten.moeller@univie.ac.at",
                "affiliations": [
                    "University of Vienna, Vienna, Austria"
                ],
                "is_corresponding": false
            },
            {
                "name": "Nils Gehlenborg",
                "email": "nils@hms.harvard.edu",
                "affiliations": [
                    "Harvard Medical School, Boston, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Peter Sorger",
                "email": "peter_sorger@hms.harvard.edu",
                "affiliations": [
                    "Harvard University, Cambridge, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Hanspeter Pfister",
                "email": "pfister@seas.harvard.edu",
                "affiliations": [
                    "Harvard University, Cambridge, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Johanna Beyer",
                "email": "jbeyer@g.harvard.edu",
                "affiliations": [
                    "Harvard University, Cambridge, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Robert Kr\u00fcger",
                "email": "rk4815@nyu.edu",
                "affiliations": [
                    "New York University, New York, United States",
                    "Harvard University, Boston, United States"
                ],
                "is_corresponding": false
            }
        ],
        "abstract": "We present Cell2Cell, a novel visual analytics approach for quantifying and visualizing networks of cell-cell interactions in three-dimensional (3D) multi-channel cancerous tissue data. By analyzing cellular interactions, biomedical domain experts can gain a more accurate understanding of the intricate relationships between cancer and immune cells. Recent methods have focused on inferring interaction based on the proximity of cells in low-resolution 2D multi-channel imaging data. By contrast, we analyze cell interactions by quantifying the intensities of protein expressions extracted from high-resolution 3D multi-channel volume data. Such analyses have a strong exploratory nature and require a tight integration of domain experts in the analysis loop to leverage their deep knowledge. We propose two complementary semi-automated approaches to cope with the increasing size and complexity of the data in an interactive fashion: On the one hand, we interpret cell-to-cell interactions as edges in a cell graph and analyze the image signal (protein expressions) along those edges, using spatial as well as abstract data visualizations. Complementary, we propose a cell-centered approach, enabling scientists to visually analyze polarized distributions of proteins in three dimensions, which also captures neighboring cells with biochemical and cell biological consequences. We evaluate our application in two case studies, where computational biologists and medical experts use \\tool to investigate tumor micro-environments to identify and quantify T-cell activation in human tissue data. We confirmed that our tool can fully solve both use cases and enables a streamlined and detailed analysis of cell-cell interactions.",
        "uid": "v-full-1615",
        "time_stamp": "",
        "time_start": "",
        "time_end": "",
        "paper_type": "full",
        "keywords": [],
        "doi": "",
        "fno": "",
        "presentation_mode": "",
        "open_access": false,
        "accessible_pdf": false,
        "has_image": false,
        "has_pdf": false,
        "paper_award": "",
        "image_caption": "",
        "external_paper_link": "",
        "youtube_ff_link": "",
        "youtube_ff_id": "",
        "bunny_ff_link": "",
        "bunny_ff_subtitles": "",
        "youtube_prerecorded_link": "",
        "youtube_prerecorded_id": "",
        "bunny_prerecorded_link": "",
        "bunny_prerecorded_subtitles": ""
    },
    "v-full-1626": {
        "slot_id": "v-full-1626",
        "session_id": "full0",
        "title": "SpatialTouch: Exploring Spatial Data Visualizations in Cross-reality",
        "contributors": [
            "Lingyun Yu"
        ],
        "authors": [
            {
                "name": "Lixiang Zhao",
                "email": "lixiang.zhao17@student.xjtlu.edu.cn",
                "affiliations": [
                    "Xi'an Jiaotong-Liverpool University, Suzhou, China"
                ],
                "is_corresponding": false
            },
            {
                "name": "Tobias Isenberg",
                "email": "tobias.isenberg@gmail.com",
                "affiliations": [
                    "Universit\u00e9 Paris-Saclay, CNRS, Orsay, France",
                    "Inria, Saclay, France"
                ],
                "is_corresponding": false
            },
            {
                "name": "Fuqi Xie",
                "email": "fuqi.xie20@student.xjtlu.edu.cn",
                "affiliations": [
                    "Xi'an Jiaotong-Liverpool University, Suzhou, China"
                ],
                "is_corresponding": false
            },
            {
                "name": "Hai-Ning Liang",
                "email": "hainingliang@hkust-gz.edu.cn",
                "affiliations": [
                    "Xi'an Jiaotong-Liverpool University, Suzhou, China"
                ],
                "is_corresponding": false
            },
            {
                "name": "Lingyun Yu",
                "email": "lingyun.yu@xjtlu.edu.cn",
                "affiliations": [
                    "Xi'an Jiaotong-Liverpool University, Suzhou, China"
                ],
                "is_corresponding": true
            }
        ],
        "abstract": "We propose and study a novel cross-reality environment that seamlessly integrates a monoscopic 2D surface (an interactive screen with touch and pen input) with a stereoscopic 3D space (an augmented reality HMD) to jointly host spatial data visualizations. This innovative approach combines the best of two conventional methods of displaying and manipulating spatial 3D data, enabling users to fluidly explore diverse visual forms using tailored interaction techniques. Providing such effective 3D data exploration techniques is pivotal for conveying its intricate spatial structures---often at multiple spatial or semantic scales---across various application domains and requiring diverse visual representations for effective visualization. To understand user reactions to our new environment, we began with an elicitation user study, in which we captured their responses and interactions. We observed that users adapted their interaction approaches based on perceived visual representations, with natural transitions in spatial awareness and actions while navigating across the physical surface. Our findings then informed the development of a design space for spatial data exploration in cross-reality. We thus developed cross-reality environments tailored to three distinct domains: for 3D molecular structure data, for 3D point cloud data, and for 3D anatomical data. In particular, we designed interaction techniques that account for the inherent features of interactions in both spaces, facilitating various forms of interaction including mid-air gestures, touch interactions, pen interactions, and combinations thereof to enhance the users' sense of presence and engagement. We assessed the usability of our environment with biologists, focusing on its use for domain research. In addition, we evaluated our interaction transition designs with virtual and mixed-reality experts to gather further insights. As a result, we provide our design suggestions for the cross-reality environment, emphasizing the interaction with diverse visual representations and seamless interaction transitions between 2D and 3D spaces.",
        "uid": "v-full-1626",
        "time_stamp": "",
        "time_start": "",
        "time_end": "",
        "paper_type": "full",
        "keywords": [],
        "doi": "",
        "fno": "",
        "presentation_mode": "",
        "open_access": false,
        "accessible_pdf": false,
        "has_image": false,
        "has_pdf": false,
        "paper_award": "",
        "image_caption": "",
        "external_paper_link": "",
        "youtube_ff_link": "",
        "youtube_ff_id": "",
        "bunny_ff_link": "",
        "bunny_ff_subtitles": "",
        "youtube_prerecorded_link": "",
        "youtube_prerecorded_id": "",
        "bunny_prerecorded_link": "",
        "bunny_prerecorded_subtitles": ""
    },
    "v-full-1632": {
        "slot_id": "v-full-1632",
        "session_id": "full0",
        "title": "TopoMap++: A faster and more space efficient technique to compute projections with topological guarantees",
        "contributors": [
            "Vitoria Guardieiro"
        ],
        "authors": [
            {
                "name": "Vitoria Guardieiro",
                "email": "vitoriaguardieiro@gmail.com",
                "affiliations": [
                    "New York University, New York City, United States"
                ],
                "is_corresponding": true
            },
            {
                "name": "Felipe Inagaki de Oliveira",
                "email": "felipedeoliveira1407@gmail.com",
                "affiliations": [
                    "New York University, New York City, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Harish Doraiswamy",
                "email": "harish.doraiswamy@microsoft.com",
                "affiliations": [
                    "Microsoft Research India, Bangalore, India"
                ],
                "is_corresponding": false
            },
            {
                "name": "Luis Gustavo Nonato",
                "email": "gnonato@icmc.usp.br",
                "affiliations": [
                    "University of Sao Paulo, Sao Carlos, Brazil"
                ],
                "is_corresponding": false
            },
            {
                "name": "Claudio Silva",
                "email": "csilva@nyu.edu",
                "affiliations": [
                    "New York University, New York City, United States"
                ],
                "is_corresponding": false
            }
        ],
        "abstract": "High-dimensional data, characterized by many features, can be difficult to visualize effectively. Dimensionality reduction techniques, such as PCA, UMAP, and t-SNE, address this challenge by projecting the data into a lower-dimensional space while preserving important relationships. TopoMap is another technique that excels at preserving the underlying structure of the data, leading to interpretable visualizations. In particular, TopoMap maps the high-dimensional data into a visual space, guaranteeing that the 0-dimensional persistence diagram of the Rips filtration of the visual space matches the one from the high-dimensional data. However, the original Topomap algorithm can be slow and its layout can be too sparse for large and complex datasets. In this paper, we propose three improvements to TopoMap: 1) a more space-efficient layout, 2) a significantly faster implementation, and 3) a novel treemap-based representation to aid the exploration of the projections. These advancements make TopoMap, now referred to as TopoMap++, a more powerful tool for visualizing high-dimensional data, similar to how t-SNE surpassed SNE in popularity.",
        "uid": "v-full-1632",
        "time_stamp": "",
        "time_start": "",
        "time_end": "",
        "paper_type": "full",
        "keywords": [],
        "doi": "",
        "fno": "",
        "presentation_mode": "",
        "open_access": false,
        "accessible_pdf": false,
        "has_image": false,
        "has_pdf": false,
        "paper_award": "",
        "image_caption": "",
        "external_paper_link": "",
        "youtube_ff_link": "",
        "youtube_ff_id": "",
        "bunny_ff_link": "",
        "bunny_ff_subtitles": "",
        "youtube_prerecorded_link": "",
        "youtube_prerecorded_id": "",
        "bunny_prerecorded_link": "",
        "bunny_prerecorded_subtitles": ""
    },
    "v-full-1638": {
        "slot_id": "v-full-1638",
        "session_id": "full0",
        "title": "The Impact of Vertical Scaling on Normal Probability Density Function Plots",
        "contributors": [
            "Racquel Fygenson"
        ],
        "authors": [
            {
                "name": "Racquel Fygenson",
                "email": "racquel.fygenson@gmail.com",
                "affiliations": [
                    "Northeastern University, Boston, United States"
                ],
                "is_corresponding": true
            },
            {
                "name": "Lace M. Padilla",
                "email": "l.padilla@northeastern.edu",
                "affiliations": [
                    "Northeastern University, Boston, United States"
                ],
                "is_corresponding": false
            }
        ],
        "abstract": "Probability density function (PDF) curves are among the few charts on a Cartesian coordinate system that are commonly presented without y-axes. This design decision may be due to the lack of relevance of vertical scaling in normal PDFs. In fact, as long as two normal PDFs have the same mean and standard deviations (SDs), they can be scaled to occupy different amounts of vertical space while still remaining statistically identical. Because unscaled PDF height increases as SD decreases, visualization designers may find themselves tempted to vertically shrink low-SD PDFs to avoid occlusion or save white space in their figures. While irregular vertical scaling has been explored in bar and line charts, the visualization community has yet to investigate how this purely visual manipulation may affect reader comparisons of PDFs. In this paper, we present two preregistered quantitative experiments (n=600, n=401) that systematically demonstrate that vertical scaling can lead to misinterpretations of PDFs. We also test visual interventions to mitigate misinterpretation. In some contexts, we find that including a y-axis reduces this effect. Overall, we find that keeping vertical scaling consistent, and therefore maintaining equal pixel areas under PDF curves, results in the highest likelihood of accurate comparisons. Our findings provide the first insights into the impact of vertical scaling on PDFs, and reveal the complicated nature of proportional area comparisons.",
        "uid": "v-full-1638",
        "time_stamp": "",
        "time_start": "",
        "time_end": "",
        "paper_type": "full",
        "keywords": [],
        "doi": "",
        "fno": "",
        "presentation_mode": "",
        "open_access": false,
        "accessible_pdf": false,
        "has_image": false,
        "has_pdf": false,
        "paper_award": "",
        "image_caption": "",
        "external_paper_link": "",
        "youtube_ff_link": "",
        "youtube_ff_id": "",
        "bunny_ff_link": "",
        "bunny_ff_subtitles": "",
        "youtube_prerecorded_link": "",
        "youtube_prerecorded_id": "",
        "bunny_prerecorded_link": "",
        "bunny_prerecorded_subtitles": ""
    },
    "v-full-1642": {
        "slot_id": "v-full-1642",
        "session_id": "full0",
        "title": "A Multi-Level Task Framework for Event Sequence Analysis",
        "contributors": [
            "Kazi Tasnim Zinat"
        ],
        "authors": [
            {
                "name": "Kazi Tasnim Zinat",
                "email": "kzintas@umd.edu",
                "affiliations": [
                    "University of Maryland, College Park, College Park, United States"
                ],
                "is_corresponding": true
            },
            {
                "name": "Saimadhav Naga Sakhamuri",
                "email": "ssakhamu@terpmail.umd.edu",
                "affiliations": [
                    "University of Maryland, College Park, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Aaron Sun Chen",
                "email": "achen151@terpmail.umd.edu",
                "affiliations": [
                    "University of Maryland, College Park, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Zhicheng Liu",
                "email": "leozcliu@umd.edu",
                "affiliations": [
                    "University of Maryland, College Park, United States"
                ],
                "is_corresponding": false
            }
        ],
        "abstract": "Despite the development of numerous visual analytics tools for event sequence data across various domains, including, but not limited to healthcare, digital marketing, and user behavior analysis, comparing these domain-specific investigations and transferring the results to new datasets and problem areas remain challenging. Task abstractions can help us go beyond domain-specific details, but existing visualization task abstractions are insufficient for event sequence visual analytics because they primarily focus on tabular datasets and often overlook automated analytical techniques. To address this gap, we propose a domain-agnostic multi-level task framework for event sequence analysis, derived from an analysis of 58 papers that present event sequence visualization systems. Our framework consists of four levels: objective, intent, strategy, and technique. Overall objectives identify the main goals of analysis. Intents comprises five high-level approaches adopted at each analysis step: augment data, simplify data, configure data, configure visualization, and create provenance. Each intent is accomplished through a number of strategies, for instance, data simplification can be achieved through aggregation, summarization, or segmentation. Finally, each strategy can be implemented by a set of techniques depending on the input and output components. We further show that techniques can be expressed through a quartet of action-input-output-criteria. We demonstrate the framework\u2019s power through mapping case studies and discuss its similarities and differences with previous event sequence task taxonomies.",
        "uid": "v-full-1642",
        "time_stamp": "",
        "time_start": "",
        "time_end": "",
        "paper_type": "full",
        "keywords": [],
        "doi": "",
        "fno": "",
        "presentation_mode": "",
        "open_access": false,
        "accessible_pdf": false,
        "has_image": false,
        "has_pdf": false,
        "paper_award": "",
        "image_caption": "",
        "external_paper_link": "",
        "youtube_ff_link": "",
        "youtube_ff_id": "",
        "bunny_ff_link": "",
        "bunny_ff_subtitles": "",
        "youtube_prerecorded_link": "",
        "youtube_prerecorded_id": "",
        "bunny_prerecorded_link": "",
        "bunny_prerecorded_subtitles": ""
    },
    "v-full-1681": {
        "slot_id": "v-full-1681",
        "session_id": "full0",
        "title": "CSLens: Towards Better Deploying Charging Stations via Visual Analytics \u2014\u2014 A Coupled Networks Perspective",
        "contributors": [
            "Haipeng Zeng"
        ],
        "authors": [
            {
                "name": "Yutian Zhang",
                "email": "zhangyt85@mail2.sysu.edu.cn",
                "affiliations": [
                    "Sun Yat-sen University, Shenzhen, China"
                ],
                "is_corresponding": false
            },
            {
                "name": "Liwen Xu",
                "email": "xulw8@mail2.sysu.edu.cn",
                "affiliations": [
                    "Sun Yat-sen University, Shenzhen, China"
                ],
                "is_corresponding": false
            },
            {
                "name": "Shaocong Tao",
                "email": "taoshc@mail2.sysu.edu.cn",
                "affiliations": [
                    "Sun Yat-sen University, Shenzhen, China"
                ],
                "is_corresponding": false
            },
            {
                "name": "Quanxue Guan",
                "email": "guanqx3@mail.sysu.edu.cn",
                "affiliations": [
                    "Sun Yat-sen University, Shenzhen, China"
                ],
                "is_corresponding": false
            },
            {
                "name": "Quan Li",
                "email": "liquan@shanghaitech.edu.cn",
                "affiliations": [
                    "ShanghaiTech University, Shanghai, China"
                ],
                "is_corresponding": false
            },
            {
                "name": "Haipeng Zeng",
                "email": "zenghp5@mail.sysu.edu.cn",
                "affiliations": [
                    "Sun Yat-sen University, Shenzhen, China"
                ],
                "is_corresponding": true
            }
        ],
        "abstract": "In recent years, the global adoption of electric vehicles (EVs) has surged, prompting a corresponding rise in the installation of charging stations. This proliferation has underscored the importance of expediting the deployment of charging infrastructure. Both academia and industry have thus devoted to addressing the charging station location problem (CSLP) to streamline this process. However, prevailing algorithms addressing CSLP are hampered by restrictive assumptions and computational overhead, leading to a dearth of comprehensive evaluations in the spatiotemporal dimensions. Consequently, their practical viability is restricted. Moreover, the placement of charging stations exerts a significant impact on both the road network and the power grid, which necessitates the evaluation of the potential post-deployment impacts on these interconnected networks holistically. In this study, we propose CSLens, a visual analytics system designed to inform charging station deployment decisions through the lens of coupled transportation and power networks. CSLens offers multiple visualizations and interactive features, empowering users to delve into the existing charging station layout, explore alternative deployment solutions, and assess the ensuring impact. To validate the efficacy of CSLens, we conducted two case studies and engaged in interviews with domain experts. Through these efforts, we substantiated the usability and practical utility of CSLens in enhancing the decision-making process surrounding charging station deployment. Our findings underscore CSLens\u2019s potential to serve as a valuable asset in navigating the complexities of charging infrastructure planning.",
        "uid": "v-full-1681",
        "time_stamp": "",
        "time_start": "",
        "time_end": "",
        "paper_type": "full",
        "keywords": [],
        "doi": "",
        "fno": "",
        "presentation_mode": "",
        "open_access": false,
        "accessible_pdf": false,
        "has_image": false,
        "has_pdf": false,
        "paper_award": "",
        "image_caption": "",
        "external_paper_link": "",
        "youtube_ff_link": "",
        "youtube_ff_id": "",
        "bunny_ff_link": "",
        "bunny_ff_subtitles": "",
        "youtube_prerecorded_link": "",
        "youtube_prerecorded_id": "",
        "bunny_prerecorded_link": "",
        "bunny_prerecorded_subtitles": ""
    },
    "v-full-1693": {
        "slot_id": "v-full-1693",
        "session_id": "full0",
        "title": "Visual Analysis of Multi-outcome Causal Graphs",
        "contributors": [
            "Mengjie Fan"
        ],
        "authors": [
            {
                "name": "Mengjie Fan",
                "email": "mengjiefan@bjmu.edu.cn",
                "affiliations": [
                    "Institute of Medical Technology, Peking University Health Science Center, Beijing, China",
                    "National Institute of Health Data Science, Peking University, Beijing, China"
                ],
                "is_corresponding": true
            },
            {
                "name": "Jinlu Yu",
                "email": "yu.jinlu@qq.com",
                "affiliations": [
                    "Beihang University, Beijing, China",
                    "Peking University, Beijing, China"
                ],
                "is_corresponding": false
            },
            {
                "name": "Daniel Weiskopf",
                "email": "weiskopf@visus.uni-stuttgart.de",
                "affiliations": [
                    "University of Stuttgart, Stuttgart, Germany"
                ],
                "is_corresponding": false
            },
            {
                "name": "Nan Cao",
                "email": "nan.cao@gmail.com",
                "affiliations": [
                    "Tongji College of Design and Innovation, Shanghai, China"
                ],
                "is_corresponding": false
            },
            {
                "name": "Huaiyu Wang",
                "email": "wanghuaiyuelva@126.com",
                "affiliations": [
                    "Beijing University of Chinese Medicine, Beijing, China"
                ],
                "is_corresponding": false
            },
            {
                "name": "Liang Zhou",
                "email": "zhoulng@pku.edu.cn",
                "affiliations": [
                    "Peking University, Beijing, China"
                ],
                "is_corresponding": false
            }
        ],
        "abstract": "We introduce a visual analysis method for multiple causality graphs with different outcome variables, namely, multi-outcome causality graphs. Multi-outcome causality graphs are important in healthcare for understanding multimorbidity and comorbidity. To support the visual analysis, we collaborated with medical experts to devise two comparative visualization techniques at different stages of the analysis process. First, a progressive visualization method is proposed for comparing multiple state-of-the-art causal discovery algorithms. The method can handle mixed-type datasets comprising both continuous and categorical variables and assist in the creation of a fine-tuned causality graph of a single outcome. Second, a comparative graph layout technique and specialized visual encodings are devised for the quick comparison of multiple causality graphs. In our visual analysis approach, analysts start by building individual causality graphs for each outcome variable, and then, multi-outcome causality graphs are generated and visualized with our comparative technique for analyzing differences and commonalities of these causality graphs. Evaluation includes quantitative measurements on benchmark datasets, a case study with a medical expert, and expert user studies with real-world health research data.",
        "uid": "v-full-1693",
        "time_stamp": "",
        "time_start": "",
        "time_end": "",
        "paper_type": "full",
        "keywords": [],
        "doi": "",
        "fno": "",
        "presentation_mode": "",
        "open_access": false,
        "accessible_pdf": false,
        "has_image": false,
        "has_pdf": false,
        "paper_award": "",
        "image_caption": "",
        "external_paper_link": "",
        "youtube_ff_link": "",
        "youtube_ff_id": "",
        "bunny_ff_link": "",
        "bunny_ff_subtitles": "",
        "youtube_prerecorded_link": "",
        "youtube_prerecorded_id": "",
        "bunny_prerecorded_link": "",
        "bunny_prerecorded_subtitles": ""
    },
    "v-full-1699": {
        "slot_id": "v-full-1699",
        "session_id": "full0",
        "title": "Precise Embodied Data Selection in Room-scale Visualisations While Retaining View Context",
        "contributors": [
            "Shaozhang Dai"
        ],
        "authors": [
            {
                "name": "Shaozhang Dai",
                "email": "dai.shaozhang@gmail.com",
                "affiliations": [
                    "Monash University, Melbourne, Australia"
                ],
                "is_corresponding": true
            },
            {
                "name": "Yi Li",
                "email": "yi.li5@monash.edu",
                "affiliations": [
                    "Monash University, Melbourne, Australia"
                ],
                "is_corresponding": false
            },
            {
                "name": "Barrett Ens",
                "email": "barrett.ens@ubc.ca",
                "affiliations": [
                    "The University of British Columbia (Okanagan Campus), Kelowna, Canada"
                ],
                "is_corresponding": false
            },
            {
                "name": "Lonni Besan\u00e7on",
                "email": "lonni.besancon@gmail.com",
                "affiliations": [
                    "Link\u00f6ping University, Norrk\u00f6ping, Sweden"
                ],
                "is_corresponding": false
            },
            {
                "name": "Tim Dwyer",
                "email": "tgdwyer@gmail.com",
                "affiliations": [
                    "Monash University, Melbourne, Australia"
                ],
                "is_corresponding": false
            }
        ],
        "abstract": "Room-scale immersive data visualisations provide viewers a wide-scale overview of a large dataset, but to interact precisely with individual data points they typically have to navigate to change their point of view.  In traditional screen-based visualisations, focus-and-context techniques allow visualisation users to keep a full dataset in view while making detailed selections.  Such techniques have been studied extensively on desktop to allow precise selection within large data sets, but they have not been explored in immersive 3D modalities. In this paper we develop a novel immersive focus-and-context technique based on a ``magic portal'' metaphor adapted specifically for data visualisation scenarios.  An extendable-hand interaction technique is used to place a portal close to the region of interest.  The other end of the portal then opens comfortably within the user's physical reach such that they can reach through to precisely select individual data points.  Through a controlled study with 24 participants, we find strong evidence that portals reduce overshoots in selection and overall hand trajectory length, reducing arm fatigue compared to ranged interaction without the portal.  The portals also enable us to use a robot arm to provide haptic feedback for data within the limited volume of the portal region. We demonstrate applications for portal-based selection through two use-case scenarios.",
        "uid": "v-full-1699",
        "time_stamp": "",
        "time_start": "",
        "time_end": "",
        "paper_type": "full",
        "keywords": [],
        "doi": "",
        "fno": "",
        "presentation_mode": "",
        "open_access": false,
        "accessible_pdf": false,
        "has_image": false,
        "has_pdf": false,
        "paper_award": "",
        "image_caption": "",
        "external_paper_link": "",
        "youtube_ff_link": "",
        "youtube_ff_id": "",
        "bunny_ff_link": "",
        "bunny_ff_subtitles": "",
        "youtube_prerecorded_link": "",
        "youtube_prerecorded_id": "",
        "bunny_prerecorded_link": "",
        "bunny_prerecorded_subtitles": ""
    },
    "v-full-1705": {
        "slot_id": "v-full-1705",
        "session_id": "full0",
        "title": "Distributed Augmentation, Hypersweeps, and Branch Decomposition of Contour Trees for Scientific Exploration",
        "contributors": [
            "Mingzhe Li"
        ],
        "authors": [
            {
                "name": "Mingzhe Li",
                "email": "mingzhefluorite@gmail.com",
                "affiliations": [
                    "University of Utah, Salt Lake City, United States"
                ],
                "is_corresponding": true
            },
            {
                "name": "Hamish Carr",
                "email": "h.carr@leeds.ac.uk",
                "affiliations": [
                    "University of Leeds, Leeds, United Kingdom"
                ],
                "is_corresponding": false
            },
            {
                "name": "Oliver R\u00fcbel",
                "email": "oruebel@lbl.gov",
                "affiliations": [
                    "Lawrence Berkeley National Laboratory, Berkeley, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Bei Wang",
                "email": "wang.bei@gmail.com",
                "affiliations": [
                    "University of Utah, Salt Lake City, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Gunther H Weber",
                "email": "ghweber@lbl.gov",
                "affiliations": [
                    "Lawrence Berkeley National Laboratory, Berkeley, United States"
                ],
                "is_corresponding": false
            }
        ],
        "abstract": "Contour trees describe the topology of level sets in scalar fields and are widely used in topological data analysis and visualization. A main challenge for utilizing contour trees for large-scale scientific data is their computation at scale using high-performance computing. To address this challenge, recent work has introduced distributed hierarchical contour trees for distributed computation and storage of contour trees. However, effective use of these distributed structures in analysis and visualization requires subsequent computation of geometric properties and branch decomposition to support contour extraction and exploration. In this work, we introduce distributed algorithms for augmentation, hypersweeps, and branch decomposition that enable parallel computation of geometric properties, and support the use of distributed contour trees as a query structure for scientific exploration. We evaluate the parallel performance of these algorithms and apply them to identify and extract important contours for scientific visualization.",
        "uid": "v-full-1705",
        "time_stamp": "",
        "time_start": "",
        "time_end": "",
        "paper_type": "full",
        "keywords": [],
        "doi": "",
        "fno": "",
        "presentation_mode": "",
        "open_access": false,
        "accessible_pdf": false,
        "has_image": false,
        "has_pdf": false,
        "paper_award": "",
        "image_caption": "",
        "external_paper_link": "",
        "youtube_ff_link": "",
        "youtube_ff_id": "",
        "bunny_ff_link": "",
        "bunny_ff_subtitles": "",
        "youtube_prerecorded_link": "",
        "youtube_prerecorded_id": "",
        "bunny_prerecorded_link": "",
        "bunny_prerecorded_subtitles": ""
    },
    "v-full-1708": {
        "slot_id": "v-full-1708",
        "session_id": "full0",
        "title": "Uncertainty-Aware Deep Neural Representations for Visual Analysis of Vector Field Data",
        "contributors": [
            "Soumya Dutta"
        ],
        "authors": [
            {
                "name": "Atul Kumar",
                "email": "atulkrfcb@gmail.com",
                "affiliations": [
                    "Indian Institute of Technology Kanpur , Kanpur, India"
                ],
                "is_corresponding": false
            },
            {
                "name": "Siddharth Garg",
                "email": "gsiddharth2209@gmail.com",
                "affiliations": [
                    "Indian Institute of Technology Kanpur , Kanpur , India"
                ],
                "is_corresponding": false
            },
            {
                "name": "Soumya Dutta",
                "email": "soumya.cvpr@gmail.com",
                "affiliations": [
                    "Indian Institute of Technology Kanpur (IIT Kanpur), Kanpur, India"
                ],
                "is_corresponding": true
            }
        ],
        "abstract": "The widespread use of Deep Neural Networks (DNNs) has recently resulted in their application to challenging scientific visualization tasks. While advanced DNNs demonstrate impressive generalization abilities, understanding factors like prediction quality, confidence, robustness, and uncertainty is crucial. These insights aid application scientists in making informed decisions. However, DNNs lack inherent mechanisms to measure prediction uncertainty, prompting the creation of distinct frameworks for constructing robust uncertainty-aware models tailored to various visualization tasks. In this work, we develop uncertainty-aware implicit neural representations to model steady-state vector fields effectively. We comprehensively evaluate the efficacy of two principled deep uncertainty estimation techniques: (1) Deep Ensemble and (2) Monte Carlo Dropout, aimed at enabling uncertainty-informed visual analysis of features within steady vector field data. Our detailed exploration using several vector data sets indicate that uncertainty-aware models generate informative visualization results of vector field features. Furthermore, incorporating prediction uncertainty improves the resilience and interpretability of our DNN model, rendering it applicable for the analysis of complex vector field data sets.",
        "uid": "v-full-1708",
        "time_stamp": "",
        "time_start": "",
        "time_end": "",
        "paper_type": "full",
        "keywords": [],
        "doi": "",
        "fno": "",
        "presentation_mode": "",
        "open_access": false,
        "accessible_pdf": false,
        "has_image": false,
        "has_pdf": false,
        "paper_award": "",
        "image_caption": "",
        "external_paper_link": "",
        "youtube_ff_link": "",
        "youtube_ff_id": "",
        "bunny_ff_link": "",
        "bunny_ff_subtitles": "",
        "youtube_prerecorded_link": "",
        "youtube_prerecorded_id": "",
        "bunny_prerecorded_link": "",
        "bunny_prerecorded_subtitles": ""
    },
    "v-full-1726": {
        "slot_id": "v-full-1726",
        "session_id": "full0",
        "title": "Mind Drifts, Data Shifts: Utilizing Mind Wandering to Track the Evolution of User Experience with Data Visualizations",
        "contributors": [
            "Anjana Arunkumar"
        ],
        "authors": [
            {
                "name": "Anjana Arunkumar",
                "email": "aarunku5@asu.edu",
                "affiliations": [
                    "Arizona State University, Tempe, United States"
                ],
                "is_corresponding": true
            },
            {
                "name": "Lace M. Padilla",
                "email": "l.padilla@northeastern.edu",
                "affiliations": [
                    "Northeastern University, Boston, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Chris Bryan",
                "email": "cbryan16@asu.edu",
                "affiliations": [
                    "Arizona State University, Tempe, United States"
                ],
                "is_corresponding": false
            }
        ],
        "abstract": "User experience in data visualization is typically assessed through post-viewing self-reports, but these overlook the dynamic cognitive processes during interaction. This study explores the use of mind wandering as a dynamic measure during visualization exploration. Participants reported mind wandering while viewing visualizations from a pre-labeled visualization database and then provided quantitative ratings of trust, engagement, and design quality, along with qualitative descriptions and short-term/long-term recall assessments. Results show that mind wandering negatively affects short-term visualization recall and various post-viewing measures, particularly for visualizations with little text annotation. Further, the type of mind wandering impacts engagement and emotional response. Mind wandering also acts as a serial mediator between visualization design elements and post-viewing measures.  Overall, this research underscores the importance of incorporating mind wandering as a dynamic measure in visualization design and evaluation, offering novel avenues for enhancing user engagement and comprehension.",
        "uid": "v-full-1726",
        "time_stamp": "",
        "time_start": "",
        "time_end": "",
        "paper_type": "full",
        "keywords": [],
        "doi": "",
        "fno": "",
        "presentation_mode": "",
        "open_access": false,
        "accessible_pdf": false,
        "has_image": false,
        "has_pdf": false,
        "paper_award": "",
        "image_caption": "",
        "external_paper_link": "",
        "youtube_ff_link": "",
        "youtube_ff_id": "",
        "bunny_ff_link": "",
        "bunny_ff_subtitles": "",
        "youtube_prerecorded_link": "",
        "youtube_prerecorded_id": "",
        "bunny_prerecorded_link": "",
        "bunny_prerecorded_subtitles": ""
    },
    "v-full-1730": {
        "slot_id": "v-full-1730",
        "session_id": "full0",
        "title": "Ferry: Toward Better Understanding of Input/Output Space for Data Wrangling Scripts",
        "contributors": [
            "Zhongsu Luo"
        ],
        "authors": [
            {
                "name": "Zhongsu Luo",
                "email": "rickyluozs@gmail.com",
                "affiliations": [
                    "Zhejiang University, Hangzhou, China"
                ],
                "is_corresponding": true
            },
            {
                "name": "Kai Xiong",
                "email": "kaixiong@zju.edu.cn",
                "affiliations": [
                    "Zhejiang University, Hangzhou, China"
                ],
                "is_corresponding": false
            },
            {
                "name": "Jiajun Zhu",
                "email": "3220105578@zju.edu.cn",
                "affiliations": [
                    "Zhejiang University, Hangzhou,Zhejiang, China"
                ],
                "is_corresponding": false
            },
            {
                "name": "Ran Chen",
                "email": "chenran928@zju.edu.cn",
                "affiliations": [
                    "Zhejiang University, Hangzhou, China"
                ],
                "is_corresponding": false
            },
            {
                "name": "Xinhuan Shu",
                "email": "xinhuan.shu@gmail.com",
                "affiliations": [
                    "Newcastle University, Newcastle Upon Tyne, United Kingdom"
                ],
                "is_corresponding": false
            },
            {
                "name": "Di Weng",
                "email": "dweng@zju.edu.cn",
                "affiliations": [
                    "Zhejiang University, Ningbo, China"
                ],
                "is_corresponding": false
            },
            {
                "name": "Yingcai Wu",
                "email": "ycwu@zju.edu.cn",
                "affiliations": [
                    "Zhejiang University, Hangzhou, China"
                ],
                "is_corresponding": false
            }
        ],
        "abstract": "Understanding the input and output of data wrangling scripts is crucial for various tasks like debugging codes and onboarding new data. However, existing research on script understanding primarily focuses on revealing the process of data transformations, lacking the ability to analyze the potential scope, i.e., the space of script inputs and outputs. Meanwhile, constructing input/output space during script analysis is challenging, as the wrangling scripts could be semantically complex and diverse, and the association between different data objects is intricate. To facilitate data workers in understanding the input and output spaces of wrangling scripts, we summarize ten types of constraints to express table spaces, and build a mapping between data transformations and these constraints to guide the construction of the input/output for individual transformations. Then, we propose a constraint generation model for integrating table constraints across multiple transformations. Based on the model, we develop Ferry, an interactive system that extracts and visualizes the data constraints describing the input and output spaces of data wrangling scripts, thereby enabling users to grasp the high-level semantics of complex scripts and locate the origins of faulty data transformations. Besides, Ferry provides example input and output data to assist users in interpreting the extracted constraints, checking and resolving the conflicts between these constraints and any uploaded dataset. Ferry's effectiveness and usability are evaluated via a usage scenario and two case studies: the first assists users in onboarding new data and debugging scripts, while the second verifies input-output compatibility across data processing modules. Furthermore, an illustrative application is presented to demonstrate Ferry's flexibility.",
        "uid": "v-full-1730",
        "time_stamp": "",
        "time_start": "",
        "time_end": "",
        "paper_type": "full",
        "keywords": [],
        "doi": "",
        "fno": "",
        "presentation_mode": "",
        "open_access": false,
        "accessible_pdf": false,
        "has_image": false,
        "has_pdf": false,
        "paper_award": "",
        "image_caption": "",
        "external_paper_link": "",
        "youtube_ff_link": "",
        "youtube_ff_id": "",
        "bunny_ff_link": "",
        "bunny_ff_subtitles": "",
        "youtube_prerecorded_link": "",
        "youtube_prerecorded_id": "",
        "bunny_prerecorded_link": "",
        "bunny_prerecorded_subtitles": ""
    },
    "v-full-1738": {
        "slot_id": "v-full-1738",
        "session_id": "full0",
        "title": "What University Students Learn In Visualization Classes",
        "contributors": [
            "Maryam Hedayati"
        ],
        "authors": [
            {
                "name": "Maryam Hedayati",
                "email": "maryam.hedayati@u.northwestern.edu",
                "affiliations": [
                    "Northwestern University, Evanston, United States"
                ],
                "is_corresponding": true
            },
            {
                "name": "Matthew Kay",
                "email": "matthew.kay@gmail.com",
                "affiliations": [
                    "Northwestern University, Chicago, United States"
                ],
                "is_corresponding": false
            }
        ],
        "abstract": "As a step towards improving visualization literacy, we investigated how students approach reading visualizations differently after taking a university-level visualization course. We asked students to verbally walk through their process of making sense of unfamiliar visualizations, and conducted a qualitative analysis of these walkthroughs.   Our qualitative analysis found changes in students' walkthroughs consistent with explicit learning goals of visualization courses. After taking a visualization course, students also engaged with visualizations in more sophisticated ways not fully captured by explicit learning goals: they were more likely to exhibit design empathy by thinking critically about the tradeoffs behind why a chart was designed in a particular way, and were better able to deconstruct a chart to make sense of it. We also gave students a quantitative assessment of visualization literacy and found no evidence of scores improving after the class,  likely because the test we used focused on a different set of skills than those emphasized in visualization classes. While current measurement instruments for visualization literacy are useful, we propose developing standardized assessments for additional aspects of visualization literacy, such as deconstruction and design empathy. We also suggest those additional aspects could be made more explicit in learning goals set by visualization educators. All supplemental materials are available at https://osf.io/w5pum/?view_only=f9eca3fa4711425582d454031b9c482e.",
        "uid": "v-full-1738",
        "time_stamp": "",
        "time_start": "",
        "time_end": "",
        "paper_type": "full",
        "keywords": [],
        "doi": "",
        "fno": "",
        "presentation_mode": "",
        "open_access": false,
        "accessible_pdf": false,
        "has_image": false,
        "has_pdf": false,
        "paper_award": "",
        "image_caption": "",
        "external_paper_link": "",
        "youtube_ff_link": "",
        "youtube_ff_id": "",
        "bunny_ff_link": "",
        "bunny_ff_subtitles": "",
        "youtube_prerecorded_link": "",
        "youtube_prerecorded_id": "",
        "bunny_prerecorded_link": "",
        "bunny_prerecorded_subtitles": ""
    },
    "v-full-1746": {
        "slot_id": "v-full-1746",
        "session_id": "full0",
        "title": "Structure-Aware Simplification for Hypergraph Visualization",
        "contributors": [
            "Eugene Zhang"
        ],
        "authors": [
            {
                "name": "Peter D Oliver",
                "email": "oliverpe@oregonstate.edu",
                "affiliations": [
                    "Oregon State University, Corvallis, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Eugene Zhang",
                "email": "zhange@eecs.oregonstate.edu",
                "affiliations": [
                    "Oregon State University, Corvallis, United States"
                ],
                "is_corresponding": true
            },
            {
                "name": "Yue Zhang",
                "email": "zhangyue@oregonstate.edu",
                "affiliations": [
                    "Oregon State University, Corvallis, United States"
                ],
                "is_corresponding": false
            }
        ],
        "abstract": "Hypergraphs provide a natural way to represent polyadic relationships in network data. For large hypergraphs, it is often difficult to visually detect structures within the data. Recently, a scalable polygon-based visualization framework was developed allowing hypergraphs with thousands of hyperedges to be simplified and examined at different levels of detail. However, this approach does not consider structures such as cycles, bridges, and branches. Consequently, structures can be lost at simplified scales, making interpretations for real-world applications unreliable. In this paper, we define hypergraph structures using the bipartite graph representation. Powered by our analysis, we provide an algorithm to decompose large hypergraphs into meaningful features and to identify regions of non-planarity. We also introduce a set of topology preserving and topology altering atomic operations, enabling the preservation of important structures while removing topological noise in simplified scales. We demonstrate our approach in several real-world applications.",
        "uid": "v-full-1746",
        "time_stamp": "",
        "time_start": "",
        "time_end": "",
        "paper_type": "full",
        "keywords": [],
        "doi": "",
        "fno": "",
        "presentation_mode": "",
        "open_access": false,
        "accessible_pdf": false,
        "has_image": false,
        "has_pdf": false,
        "paper_award": "",
        "image_caption": "",
        "external_paper_link": "",
        "youtube_ff_link": "",
        "youtube_ff_id": "",
        "bunny_ff_link": "",
        "bunny_ff_subtitles": "",
        "youtube_prerecorded_link": "",
        "youtube_prerecorded_id": "",
        "bunny_prerecorded_link": "",
        "bunny_prerecorded_subtitles": ""
    },
    "v-full-1770": {
        "slot_id": "v-full-1770",
        "session_id": "full0",
        "title": "A Large-Scale Sensitivity Analysis on Latent Embeddings and Dimensionality Reductions for Text Spatializations",
        "contributors": [
            "Daniel Atzberger"
        ],
        "authors": [
            {
                "name": "Daniel Atzberger",
                "email": "daniel.atzberger@hpi.de",
                "affiliations": [
                    "University of Potsdam, Digital Engineering Faculty, Hasso Plattner Institute, Potsdam, Germany"
                ],
                "is_corresponding": true
            },
            {
                "name": "Tim Cech",
                "email": "tcech@uni-potsdam.de",
                "affiliations": [
                    "University of Potsdam, Potsdam, Germany"
                ],
                "is_corresponding": false
            },
            {
                "name": "Willy Scheibel",
                "email": "willy.scheibel@hpi.de",
                "affiliations": [
                    "Hasso Plattner Institute, Faculty of Digital Engineering, University of Potsdam, Potsdam, Germany"
                ],
                "is_corresponding": false
            },
            {
                "name": "J\u00fcrgen D\u00f6llner",
                "email": "juergen.doellner@hpi.de",
                "affiliations": [
                    "Hasso Plattner Institute"
                ],
                "is_corresponding": false
            },
            {
                "name": "Michael Behrisch",
                "email": "m.behrisch@uu.nl",
                "affiliations": [
                    "Faculty of Digital Engineering, University of Potsdam, Potsdam, Germany"
                ],
                "is_corresponding": false
            },
            {
                "name": "Tobias Schreck",
                "email": "tobias.schreck@cgv.tugraz.at",
                "affiliations": [
                    "Utrecht University, Utrecht, Netherlands"
                ],
                "is_corresponding": false
            }
        ],
        "abstract": "The semantic similarity between documents of a text corpus can be visualized using map-like metaphors based on two-dimensional scatterplot layouts. These layouts result from a dimensionality reduction on the document-term matrix or a representation within a latent embedding, including topic models. Thereby, the resulting layout depends on the input data and hyperparameters of the dimensionality reduction and is therefore affected by changes in them. Furthermore, the resulting layout is affected by changes in the input data and hyperparameters of the dimensionality reduction. However, such changes to the layout require additional cognitive efforts from the user. In this work, we present a sensitivity study that analyzes the stability of these layouts concerning (1) changes in the text corpora, (2) changes in the hyperparameter, and (3) randomness in the initialization. Our approach has two stages: data measurement and data analysis. First, we derived layouts for the combination of three text corpora and six text embeddings and a grid-search-inspired hyperparameter selection of the dimensionality reductions. Afterward, we quantified the similarity of the layouts through ten metrics, concerning local and global structures and class separation. Second, we analyzed the resulting 42817 tabular data points in a descriptive statistical analysis. From this, we derived guidelines for informed decisions on the layout algorithm and highlight specific hyperparameter settings. We provide our implementation and results as a Git repository at https://github.com/hpicgs/Topic-Models-and-Dimensionality-Reduction-Sensitivity-Study .",
        "uid": "v-full-1770",
        "time_stamp": "",
        "time_start": "",
        "time_end": "",
        "paper_type": "full",
        "keywords": [],
        "doi": "",
        "fno": "",
        "presentation_mode": "",
        "open_access": false,
        "accessible_pdf": false,
        "has_image": false,
        "has_pdf": false,
        "paper_award": "",
        "image_caption": "",
        "external_paper_link": "",
        "youtube_ff_link": "",
        "youtube_ff_id": "",
        "bunny_ff_link": "",
        "bunny_ff_subtitles": "",
        "youtube_prerecorded_link": "",
        "youtube_prerecorded_id": "",
        "bunny_prerecorded_link": "",
        "bunny_prerecorded_subtitles": ""
    },
    "v-full-1793": {
        "slot_id": "v-full-1793",
        "session_id": "full0",
        "title": "MSz: An Efficient Parallel Algorithm for Correcting Morse-Smale Segmentations in Error-Bounded Lossy Compressors",
        "contributors": [
            "Yuxiao Li"
        ],
        "authors": [
            {
                "name": "Yuxiao Li",
                "email": "li.14025@osu.edu",
                "affiliations": [
                    "The Ohio State University, Columbus, United States"
                ],
                "is_corresponding": true
            },
            {
                "name": "Xin Liang",
                "email": "xlian007@ucr.edu",
                "affiliations": [
                    "University of California, Riverside, Riverside, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Bei Wang",
                "email": "wang.bei@gmail.com",
                "affiliations": [
                    "University of Utah, Salt Lake City, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Yongfeng Qiu",
                "email": "qiu.722@osu.edu",
                "affiliations": [
                    "The Ohio State University, Columbus, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Lin Yan",
                "email": "lyan@anl.gov",
                "affiliations": [
                    "Argonne National Laboratory, Lemont, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Hanqi Guo",
                "email": "guo.2154@osu.edu",
                "affiliations": [
                    "The Ohio State University, Columbus, United States"
                ],
                "is_corresponding": false
            }
        ],
        "abstract": "This research explores a novel paradigm for preserving topological segmentations in existing error-bounded lossy compressors. Today's lossy compressors rarely consider preserving topologies such as Morse-Smale complexes, and the discrepancies in topology between original and decompressed datasets could potentially result in erroneous interpretations or even incorrect scientific conclusions. In this paper, we focus on preserving Morse-Smale segmentations in 2D/3D piecewise linear scalar fields, targeting the precise reconstruction of minimum/maximum labels induced by the integral curve of each vertex. The key is to derive a series of edits during compression time; the edits are applied to the decompressed data, leading to an accurate reconstruction of segmentations while keeping the error within the prescribed error bound. To this end, we developed a workflow to fix extrema and integral curves alternatively until convergence within finite iterations; we accelerate each workflow component with shared-memory/GPU parallelism to make the performance practical for coupling with compressors. We demonstrate use cases with fluid dynamics, ocean, and cosmology application datasets with a 1000x acceleration with an NVIDIA A100 GPU.",
        "uid": "v-full-1793",
        "time_stamp": "",
        "time_start": "",
        "time_end": "",
        "paper_type": "full",
        "keywords": [],
        "doi": "",
        "fno": "",
        "presentation_mode": "",
        "open_access": false,
        "accessible_pdf": false,
        "has_image": false,
        "has_pdf": false,
        "paper_award": "",
        "image_caption": "",
        "external_paper_link": "",
        "youtube_ff_link": "",
        "youtube_ff_id": "",
        "bunny_ff_link": "",
        "bunny_ff_subtitles": "",
        "youtube_prerecorded_link": "",
        "youtube_prerecorded_id": "",
        "bunny_prerecorded_link": "",
        "bunny_prerecorded_subtitles": ""
    },
    "v-full-1802": {
        "slot_id": "v-full-1802",
        "session_id": "full0",
        "title": "VADIS: A Visual Analytics Pipeline for Dynamic Document Representation and Information Seeking",
        "contributors": [
            "Rui Qiu"
        ],
        "authors": [
            {
                "name": "Rui Qiu",
                "email": "qiu.580@buckeyemail.osu.edu",
                "affiliations": [
                    "Ohio State University, Columbus, United States"
                ],
                "is_corresponding": true
            },
            {
                "name": "Yamei Tu",
                "email": "tu.253@osu.edu",
                "affiliations": [
                    "The Ohio State University, Columbus, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Po-Yin Yen",
                "email": "yenp@wustl.edu",
                "affiliations": [
                    "Washington University School of Medicine in St. Louis, St. Louis, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Han-Wei Shen",
                "email": "hwshen@cse.ohio-state.edu",
                "affiliations": [
                    "The Ohio State University , Columbus , United States"
                ],
                "is_corresponding": false
            }
        ],
        "abstract": "In the biomedical domain, visualizing the document embeddings of an extensive corpus has been widely used in information-seeking tasks. However, three key challenges with existing visualizations make it difficult for clinicians to find information efficiently. First, the document embeddings used in these visualizations are generated statically by pretrained language models, which cannot adapt to the user's evolving interest.  Second, existing document visualization techniques cannot effectively display how the documents are relevant to users\u2019 interest, making it difficult for users to identify the most pertinent information. Third, existing embedding generation and visualization processes suffer from a lack of interpretability, making it difficult to understand, trust and use the result for decision-making. In this paper, we present a novel visual analytics pipeline for user-driven document representation and iterative information seeking (VADIS). VADIS introduces a prompt-based attention model (PAM) that generates dynamic document embedding and document relevance adjusted to the user's query. To effectively visualize these two pieces of information, we design a new document map that leverages a circular grid layout to display documents based on both their relevance to the query and the semantic similarity. Additionally, to improve the interpretability, we introduce a corpus-level attention visualization method to improve the user's understanding of the model focus and to enable the users to identify potential oversight. This visualization, in turn, empowers users to refine, update and introduce new queries, thereby facilitating a dynamic and iterative information-seeking experience. We evaluated VADIS quantitatively and qualitatively on a real-world dataset of biomedical research papers to demonstrate its effectiveness.",
        "uid": "v-full-1802",
        "time_stamp": "",
        "time_start": "",
        "time_end": "",
        "paper_type": "full",
        "keywords": [],
        "doi": "",
        "fno": "",
        "presentation_mode": "",
        "open_access": false,
        "accessible_pdf": false,
        "has_image": false,
        "has_pdf": false,
        "paper_award": "",
        "image_caption": "",
        "external_paper_link": "",
        "youtube_ff_link": "",
        "youtube_ff_id": "",
        "bunny_ff_link": "",
        "bunny_ff_subtitles": "",
        "youtube_prerecorded_link": "",
        "youtube_prerecorded_id": "",
        "bunny_prerecorded_link": "",
        "bunny_prerecorded_subtitles": ""
    },
    "v-full-1803": {
        "slot_id": "v-full-1803",
        "session_id": "full0",
        "title": "Fast Comparative Analysis of Merge Trees Using Locality-Sensitive Hashing",
        "contributors": [
            "Raghavendra Sridharamurthy"
        ],
        "authors": [
            {
                "name": "Weiran Lyu",
                "email": "lyuweiran@gmail.com",
                "affiliations": [
                    "University of Utah, SALT LAKE CITY, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Raghavendra Sridharamurthy",
                "email": "g.s.raghavendra@gmail.com",
                "affiliations": [
                    "University of Utah, Salt Lake City, United States"
                ],
                "is_corresponding": true
            },
            {
                "name": "Jeff M. Phillips",
                "email": "jeffp@cs.utah.edu",
                "affiliations": [
                    "University of Utah, Salt Lake City, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Bei Wang",
                "email": "wang.bei@gmail.com",
                "affiliations": [
                    "University of Utah, Salt Lake City, United States"
                ],
                "is_corresponding": false
            }
        ],
        "abstract": "Scalar field comparison is a fundamental task in scientific visualization. In topological data analysis, we compare topological descriptors of scalar fields---such as persistence diagrams and merge trees---as they provide succinct and robust abstract representations. While several similarity measures for topological descriptors seem to be both asymptotically and practically efficient with polynomial time algorithms, they do not scale well when handling large-scale, time-varying scientific data and ensembles. In this paper, we propose a new framework to facilitate the comparative analysis of merge trees, inspired by tools from locality sensitive hashing (LSH). LSH hashes similar objects into the same hash buckets with high probability. We propose two new similarity measures for merge trees that can be computed via LSH, using new extensions to Recursive MinHash and subpath signature, respectively. Our similarity measures are extremely efficient to compute and closely resemble the results of existing measures such as merge tree edit distance or geometric interleaving distance. Our experiments demonstrate the utility of our LSH framework in applications such as shape matching, clustering, key event detection, and ensemble summarization.",
        "uid": "v-full-1803",
        "time_stamp": "",
        "time_start": "",
        "time_end": "",
        "paper_type": "full",
        "keywords": [],
        "doi": "",
        "fno": "",
        "presentation_mode": "",
        "open_access": false,
        "accessible_pdf": false,
        "has_image": false,
        "has_pdf": false,
        "paper_award": "",
        "image_caption": "",
        "external_paper_link": "",
        "youtube_ff_link": "",
        "youtube_ff_id": "",
        "bunny_ff_link": "",
        "bunny_ff_subtitles": "",
        "youtube_prerecorded_link": "",
        "youtube_prerecorded_id": "",
        "bunny_prerecorded_link": "",
        "bunny_prerecorded_subtitles": ""
    },
    "v-full-1805": {
        "slot_id": "v-full-1805",
        "session_id": "full0",
        "title": "Interactive Design-of-Experiments: Optimizing a Cooling System",
        "contributors": [
            "Kresimir Matkovic"
        ],
        "authors": [
            {
                "name": "Rainer Splechtna",
                "email": "splechtna@vrvis.at",
                "affiliations": [
                    "VRVis Research Center, Vienna, Austria"
                ],
                "is_corresponding": false
            },
            {
                "name": "Majid Behravan",
                "email": "behravan@vt.edu",
                "affiliations": [
                    "Virginia Tech, Blacksburg, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Mario Jelovic",
                "email": "mario.jelovic@avl.com",
                "affiliations": [
                    "AVL AST doo, Zagreb, Croatia"
                ],
                "is_corresponding": false
            },
            {
                "name": "Denis Gracanin",
                "email": "gracanin@vt.edu",
                "affiliations": [
                    "Virginia Tech, Blacksburg, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Helwig Hauser",
                "email": "helwig.hauser@uib.no",
                "affiliations": [
                    "University of Bergen, Bergen, Norway"
                ],
                "is_corresponding": false
            },
            {
                "name": "Kresimir Matkovic",
                "email": "matkovic@vrvis.at",
                "affiliations": [
                    "VRVis Research Center, Vienna, Austria"
                ],
                "is_corresponding": true
            }
        ],
        "abstract": "he optimization of cooling systems is important in many cases, for example for cabin and battery cooling in electric cars. Such an optimization is governed by multiple, conflicting objectives and it is performed across a multi-dimensional parameter space. The extent of the parameter space, the complexity of the non-linear model of the system, as well as the time needed per simulation run and factors that are not modeled in the simulation necessitate an iterative, semi-automatic approach. We present an interactive visual optimization approach, where the user works with a p-h diagram to steer an iterative, guided optimization process. A deep learning (DL) model provides estimates for parameters, given a target characterization of the system, while numerical simulation is used to predict system characteristics for an ensemble of parameter sets. Since the DL model only serves as an approximation of the inverse of the cooling system and since target characteristics can be chosen according to different, competing objectives, an iterative optimization process is realized, developing multiple sets of intermediate solutions, which are visually related to each other. The standard p-h diagram, integrated interactively in this approach, is complemented by a dual, also interactive visual representation of additional expressive measures representing the system characteristics. We show how the known four-points semantic of the p-h diagram meaningfully transfers to the dual data representation. When evaluating this approach with our partners in the automotive domain, we found that our solution helped with the overall comprehension of the cooling system and that it lead to a faster convergence during optimization.",
        "uid": "v-full-1805",
        "time_stamp": "",
        "time_start": "",
        "time_end": "",
        "paper_type": "full",
        "keywords": [],
        "doi": "",
        "fno": "",
        "presentation_mode": "",
        "open_access": false,
        "accessible_pdf": false,
        "has_image": false,
        "has_pdf": false,
        "paper_award": "",
        "image_caption": "",
        "external_paper_link": "",
        "youtube_ff_link": "",
        "youtube_ff_id": "",
        "bunny_ff_link": "",
        "bunny_ff_subtitles": "",
        "youtube_prerecorded_link": "",
        "youtube_prerecorded_id": "",
        "bunny_prerecorded_link": "",
        "bunny_prerecorded_subtitles": ""
    },
    "v-full-1809": {
        "slot_id": "v-full-1809",
        "session_id": "full0",
        "title": "Quality Metrics and Reordering Strategies for Revealing Patterns in BioFabric Visualizations",
        "contributors": [
            "Johannes Fuchs"
        ],
        "authors": [
            {
                "name": "Johannes Fuchs",
                "email": "fuchs@dbvis.inf.uni-konstanz.de",
                "affiliations": [
                    "University of Konstanz, Konstanz, Germany"
                ],
                "is_corresponding": true
            },
            {
                "name": "Alexander Frings",
                "email": "alexander.frings@uni-konstanz.de",
                "affiliations": [
                    "University of Konstanz, Konstanz, Germany"
                ],
                "is_corresponding": false
            },
            {
                "name": "Maria-Viktoria Heinle",
                "email": "maria-viktoria.heinle@uni-konstanz.de",
                "affiliations": [
                    "University of Konstanz, Konstanz, Germany"
                ],
                "is_corresponding": false
            },
            {
                "name": "Daniel Keim",
                "email": "keim@uni-konstanz.de",
                "affiliations": [
                    "University of Konstanz, Konstanz, Germany"
                ],
                "is_corresponding": false
            },
            {
                "name": "Sara Di Bartolomeo",
                "email": "sara.di-bartolomeo@uni-konstanz.de",
                "affiliations": [
                    "University of Konstanz, Konstanz, Germany"
                ],
                "is_corresponding": false
            }
        ],
        "abstract": "Visualizing relational data is crucial for understanding complex connections between entities in social networks, political affiliations, or biological interactions. Well-known representations like node-link diagrams and adjacency matrices offer valuable insights, but their effectiveness relies on the ability to identify patterns in the underlying topological structure. Reordering strategies and layout algorithms play a vital role in the visualization process since the arrangement of nodes, edges, or cells influences the visibility of these patterns. The BioFabric visualization combines elements of node-link diagrams and adjacency matrices, leveraging the strengths of both, the visual clarity of node-link diagrams and the tabular organization of adjacency matrices. A unique characteristic of BioFabric is the possibility to reorder nodes and edges separately. This raises the question of which combination of layout algorithms best reveals certain patterns.  In this paper, we discuss patterns and anti-patterns in BioFabric, such as staircases or escalators, relate them to already established patterns, and propose metrics to evaluate their quality. Based on these quality metrics, we compared combinations of well-established reordering techniques applied to BioFabric with a well-known benchmark data set. Our experiments indicate that the edge order has a stronger influence on revealing patterns than the node layout. The results show that the best combination for revealing staircases is a barycentric node layout, together with an edge order based on node indices and length. Our research contributes a first building block for many promising future research directions, which we also share and discuss. A free copy of this paper and all supplemental materials are available at OSF.",
        "uid": "v-full-1809",
        "time_stamp": "",
        "time_start": "",
        "time_end": "",
        "paper_type": "full",
        "keywords": [],
        "doi": "",
        "fno": "",
        "presentation_mode": "",
        "open_access": false,
        "accessible_pdf": false,
        "has_image": false,
        "has_pdf": false,
        "paper_award": "",
        "image_caption": "",
        "external_paper_link": "",
        "youtube_ff_link": "",
        "youtube_ff_id": "",
        "bunny_ff_link": "",
        "bunny_ff_subtitles": "",
        "youtube_prerecorded_link": "",
        "youtube_prerecorded_id": "",
        "bunny_prerecorded_link": "",
        "bunny_prerecorded_subtitles": ""
    },
    "v-full-1810": {
        "slot_id": "v-full-1810",
        "session_id": "full0",
        "title": "CataAnno: An Ancient Catalog Annotator for Annotation Cleaning by Recommendation",
        "contributors": [
            "Hanning Shao"
        ],
        "authors": [
            {
                "name": "Hanning Shao",
                "email": "hanning.shao@pku.edu.cn",
                "affiliations": [
                    "Peking University, Beijing, China"
                ],
                "is_corresponding": true
            },
            {
                "name": "Xiaoru Yuan",
                "email": "xiaoru.yuan@pku.edu.cn",
                "affiliations": [
                    "Peking University, Beijing, China"
                ],
                "is_corresponding": false
            }
        ],
        "abstract": "Classical bibliography, by scrutinizing preserved catalogs from both official archives and personal collections of accumulated books, examines the books throughout history, thereby elucidating cultural development across historical periods. In this work, we collaborate with domain experts to accomplish the task of data annotation concerning Chinese ancient catalogs. We introduce the CataAnno system that facilitates users in completing annotations more efficiently through cross-linked views, recommendation methods and convenient annotation interactions. The recommendation method can learn the background knowledge and annotation patterns that experts subconsciously integrate into the data during prior annotation processes. CataAnno searches for the most relevant examples previously annotated and recommends to the user. Meanwhile, the cross-linked views assist users in comprehending the correlations between entries and offer explanations for these recommendations. Evaluation and expert feedback confirm that the CataAnno system, by offering high-quality recommendations and visualizing the relationships between entries, can mitigate the necessity for specialized knowledge during the annotation process. This results in enhanced accuracy and consistency in annotations, thereby enhancing the overall efficiency",
        "uid": "v-full-1810",
        "time_stamp": "",
        "time_start": "",
        "time_end": "",
        "paper_type": "full",
        "keywords": [],
        "doi": "",
        "fno": "",
        "presentation_mode": "",
        "open_access": false,
        "accessible_pdf": false,
        "has_image": false,
        "has_pdf": false,
        "paper_award": "",
        "image_caption": "",
        "external_paper_link": "",
        "youtube_ff_link": "",
        "youtube_ff_id": "",
        "bunny_ff_link": "",
        "bunny_ff_subtitles": "",
        "youtube_prerecorded_link": "",
        "youtube_prerecorded_id": "",
        "bunny_prerecorded_link": "",
        "bunny_prerecorded_subtitles": ""
    },
    "v-full-1830": {
        "slot_id": "v-full-1830",
        "session_id": "full0",
        "title": "Curio: A Dataflow-Based Framework for Collaborative Urban Visual Analytics",
        "contributors": [
            "Fabio Miranda"
        ],
        "authors": [
            {
                "name": "Gustavo Moreira",
                "email": "gmorei3@uic.edu",
                "affiliations": [
                    "University of Illinois at Chicago, Chicago, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Maryam Hosseini",
                "email": "maryamh@mit.edu",
                "affiliations": [
                    "Massachusetts Institute of Technology , Somerville, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Carolina Veiga Ferreira de Souza",
                "email": "carolinavfs@id.uff.br",
                "affiliations": [
                    "University of Illinois Urbana-Champaign, Urbana-Champaign, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Lucas Alexandre",
                "email": "lucasalexandre.s.cc@gmail.com",
                "affiliations": [
                    "Universidade Federal Fluminense, Niteroi, Brazil"
                ],
                "is_corresponding": false
            },
            {
                "name": "Nicola Colaninno",
                "email": "nicola.colaninno@polimi.it",
                "affiliations": [
                    "Politecnico di Milano, Milano, Italy"
                ],
                "is_corresponding": false
            },
            {
                "name": "Daniel de Oliveira",
                "email": "danielcmo@ic.uff.br",
                "affiliations": [
                    "Universidade Federal Fluminense, Niter\u00f3i, Brazil"
                ],
                "is_corresponding": false
            },
            {
                "name": "Nivan Ferreira",
                "email": "nivan@cin.ufpe.br",
                "affiliations": [
                    "Universidade Federal de Pernambuco, Recife, Brazil"
                ],
                "is_corresponding": false
            },
            {
                "name": "Marcos Lage",
                "email": "mlage@ic.uff.br",
                "affiliations": [
                    "Universidade Federal Fluminense , Niteroi, Brazil"
                ],
                "is_corresponding": false
            },
            {
                "name": "Fabio Miranda",
                "email": "fabiom@uic.edu",
                "affiliations": [
                    "University of Illinois Chicago, Chicago, United States"
                ],
                "is_corresponding": true
            }
        ],
        "abstract": "Over the past decade, several urban visual analytics systems have been proposed to tackle a host of challenges faced by cities, in areas as diverse as transportation, weather, and real estate. Many of these systems have been designed through engagement with urban experts, aiming to distill intricate urban analysis workflows into interactive visualizations and interfaces. The design, implementation, and practical use of these systems, however, still rely on siloed approaches that lead to bespoke tools that are hard to reproduce and extend. At the design level, these systems undervalue rich data workflows from urban experts by usually only treating them as data providers and evaluators. At the implementation level, these systems lack interoperability with other technical frameworks. At the practical use level, these systems tend to be narrowly focused on specific fields, inadvertently creating barriers for cross-domain collaboration. To tackle these gaps, we present Curio, a framework for collaborative urban visual analytics. Curio uses a dataflow model with multiple abstraction levels (code, grammar, GUI elements) to facilitate collaboration across the design and implementation of visual analytics components. The framework allows experts to intertwine preprocessing, managing, and visualization stages while tracking provenance of code and visualizations. In collaboration with urban experts, we evaluate Curio through a diverse series of use cases targeting urban accessibility, urban microclimate, and sunlight access. These cases use different types of urban data and domain methodologies to illustrate Curio's flexibility in tackling pressing societal challenges.",
        "uid": "v-full-1830",
        "time_stamp": "",
        "time_start": "",
        "time_end": "",
        "paper_type": "full",
        "keywords": [],
        "doi": "",
        "fno": "",
        "presentation_mode": "",
        "open_access": false,
        "accessible_pdf": false,
        "has_image": false,
        "has_pdf": false,
        "paper_award": "",
        "image_caption": "",
        "external_paper_link": "",
        "youtube_ff_link": "",
        "youtube_ff_id": "",
        "bunny_ff_link": "",
        "bunny_ff_subtitles": "",
        "youtube_prerecorded_link": "",
        "youtube_prerecorded_id": "",
        "bunny_prerecorded_link": "",
        "bunny_prerecorded_subtitles": ""
    },
    "v-full-1831": {
        "slot_id": "v-full-1831",
        "session_id": "full0",
        "title": "HiRegEx: Interactive Visual Query and Exploration of Multivariate Hierarchical Data",
        "contributors": [
            "Guozheng Li"
        ],
        "authors": [
            {
                "name": "Guozheng Li",
                "email": "guozhg.li@gmail.com",
                "affiliations": [
                    "Beijing Institute of Technology, Beijing, China"
                ],
                "is_corresponding": true
            },
            {
                "name": "haotian mi",
                "email": "haotian.mi1@gmail.com",
                "affiliations": [
                    "Beijing Institute of Technology, Beijing, China"
                ],
                "is_corresponding": false
            },
            {
                "name": "Chi Harold Liu",
                "email": "liuchi02@gmail.com",
                "affiliations": [
                    "Beijing Institute of Technology, Beijing, China"
                ],
                "is_corresponding": false
            },
            {
                "name": "Takayuki Itoh",
                "email": "itot@is.ocha.ac.jp",
                "affiliations": [
                    "Ochanomizu University, Tokyo, Japan"
                ],
                "is_corresponding": false
            },
            {
                "name": "Guoren Wang",
                "email": "wanggrbit@126.com",
                "affiliations": [
                    "Beijing Institute of Technology, Beijing, China"
                ],
                "is_corresponding": false
            }
        ],
        "abstract": "When using exploratory visual analysis to examine multivariate hierarchical data, users often need to query data to narrow down the scope of analysis. However, formulating effective query expressions remains a challenge for multivariate hierarchical data, particularly when datasets become very large. To address this issue, we develop a declarative grammar,  HiRegEx (Hierarchical data Regular Expression), for querying and exploring multivariate hierarchical data. Rooted in the extended multi-level task topology framework for tree visualizations (e-MLTT), HiRegEx delineates three query targets (node, path, and subtree) and two aspects for querying these targets (features and positions), and uses operators developed based on classical regular expressions for query construction. We develop a prototype system, TreeQueryER, to integrate an exploratory framework for querying and exploring multivariate hierarchical data based on HiRegEx. The exploratory framework includes three major components: top-down pattern specification, bottom-up data-driven inquiry, and context-creation data overview. We validate the expressiveness of HiRegEx with the tasks from the e-MLTT framework and showcase its utility and effectiveness through a usage scenario involving expert users in the analysis of a citation tree dataset.",
        "uid": "v-full-1831",
        "time_stamp": "",
        "time_start": "",
        "time_end": "",
        "paper_type": "full",
        "keywords": [],
        "doi": "",
        "fno": "",
        "presentation_mode": "",
        "open_access": false,
        "accessible_pdf": false,
        "has_image": false,
        "has_pdf": false,
        "paper_award": "",
        "image_caption": "",
        "external_paper_link": "",
        "youtube_ff_link": "",
        "youtube_ff_id": "",
        "bunny_ff_link": "",
        "bunny_ff_subtitles": "",
        "youtube_prerecorded_link": "",
        "youtube_prerecorded_id": "",
        "bunny_prerecorded_link": "",
        "bunny_prerecorded_subtitles": ""
    },
    "v-full-1833": {
        "slot_id": "v-full-1833",
        "session_id": "full0",
        "title": "HuBar: A Visual Analytics Tool to Explore Human Behaviour based on fNIRS in AR guidance systems",
        "contributors": [
            "Sonia Castelo Quispe"
        ],
        "authors": [
            {
                "name": "Sonia Castelo Quispe",
                "email": "s.castelo@nyu.edu",
                "affiliations": [
                    "New York University, New York, United States"
                ],
                "is_corresponding": true
            },
            {
                "name": "Jo\u00e3o Rulff",
                "email": "jlrulff@gmail.com",
                "affiliations": [
                    "New York University, New York, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Parikshit Solunke",
                "email": "pss442@nyu.edu",
                "affiliations": [
                    "New York University, Brooklyn, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Erin McGowan",
                "email": "erin.mcgowan@nyu.edu",
                "affiliations": [
                    "New York University, New York, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Guande Wu",
                "email": "guandewu@nyu.edu",
                "affiliations": [
                    "New York University, New York CIty, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Iran Roman",
                "email": "iran@ccrma.stanford.edu",
                "affiliations": [
                    "New York University, Brooklyn, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Roque Lopez",
                "email": "rlopez@nyu.edu",
                "affiliations": [
                    "New York University, New York, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Bea Steers",
                "email": "bs3639@nyu.edu",
                "affiliations": [
                    "New York University, Brooklyn, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Qi Sun",
                "email": "qisun@nyu.edu",
                "affiliations": [
                    "New York University, New York, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Juan Pablo Bello",
                "email": "jpbello@nyu.edu",
                "affiliations": [
                    "New York University, New York, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Bradley S Feest",
                "email": "bradley.feest@ngc.com",
                "affiliations": [
                    "Northrop Grumman Mission Systems, Redondo Beach, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Michael Middleton",
                "email": "michael.middleton@ngc.com",
                "affiliations": [
                    "Northrop Grumman, Aurora, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Ryan McKendrick",
                "email": "ryan.mckendrick@ngc.com",
                "affiliations": [
                    "Northrop Grumman, Falls Church, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Claudio Silva",
                "email": "csilva@nyu.edu",
                "affiliations": [
                    "New York University, New York City, United States"
                ],
                "is_corresponding": false
            }
        ],
        "abstract": "The concept of an intelligent augmented reality (AR) assistant has applications as significant as they are wide-ranging, with potential uses in medicine, military endeavors, and mechanics. Such an assistant must be able to perceive the performer\u2019s environment and actions, reason about the state of the environment in relation to a given task, and seamlessly interact with the performer. These interactions typically involve an AR headset equipped with a variety of sensors which capture video, audio, and haptic feedback. Previous works have sought to facilitate the development of such an assistant by visualizing these sensor data streams as well as the machine learning model outputs that support an assistant\u2019s perception and reasoning capabilities. However, existing visual analytics systems do not include biometric data or focus on user modeling, and are only capable of visualizing a single task session for a single performer at a time. Furthermore, they mainly focus on traditional task analysis that typically assumes a linear progression from one step to the next. We propose a visual analytics system that allows users to compare performance during multiple task sessions focusing on non-linear tasks where different paths or sequences can lead to the successful completion of the task. In particular, we design visualizations for understanding user behavior through functional near-infrared spectroscopy (fNIRS) data as a proxy for perception, attention, and memory as well as corresponding motion data (acceleration, angular velocity, and eye gaze). We distill these insights into visual embeddings that allow users to easily select groups of sessions with similar behaviors. We provide case studies that explore how insights into task performance can be gleaned from these visualizations using data collected during helicopter copilot training tasks. Finally, we evaluate our approach by conducting an in-depth examination of a think-aloud experiment with five domain experts.",
        "uid": "v-full-1833",
        "time_stamp": "",
        "time_start": "",
        "time_end": "",
        "paper_type": "full",
        "keywords": [],
        "doi": "",
        "fno": "",
        "presentation_mode": "",
        "open_access": false,
        "accessible_pdf": false,
        "has_image": false,
        "has_pdf": false,
        "paper_award": "",
        "image_caption": "",
        "external_paper_link": "",
        "youtube_ff_link": "",
        "youtube_ff_id": "",
        "bunny_ff_link": "",
        "bunny_ff_subtitles": "",
        "youtube_prerecorded_link": "",
        "youtube_prerecorded_id": "",
        "bunny_prerecorded_link": "",
        "bunny_prerecorded_subtitles": ""
    },
    "v-full-1836": {
        "slot_id": "v-full-1836",
        "session_id": "full0",
        "title": "An Empirically Grounded Approach for Designing Shape Palettes",
        "contributors": [
            "Chin Tseng"
        ],
        "authors": [
            {
                "name": "Chin Tseng",
                "email": "chint@cs.unc.edu",
                "affiliations": [
                    "University of North Carolina-Chapel Hill, Chapel Hill, United States"
                ],
                "is_corresponding": true
            },
            {
                "name": "Arran Zeyu Wang",
                "email": "zeyuwang@cs.unc.edu",
                "affiliations": [
                    "University of North Carolina-Chapel Hill, Chapel Hill, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Ghulam Jilani Quadri",
                "email": "quadri@ou.edu",
                "affiliations": [
                    "University of Oklahoma, Norman, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Danielle Albers Szafir",
                "email": "danielle.szafir@cs.unc.edu",
                "affiliations": [
                    "University of North Carolina-Chapel Hill, Chapel Hill, United States"
                ],
                "is_corresponding": false
            }
        ],
        "abstract": "Shape is commonly used to distinguish between categories in multi-class scatterplots. However, existing guidelines for choosing effective shape palettes rely largely on intuition and do not consider how these needs may change as the number of categories increases. Although shapes can be a finite number compared to colors, they can not be represented by a numerical space, making it difficult to propose a general guideline for shape choices or shed light on the design heuristics of designer-crafted shape palettes. This paper presents a series of four experiments evaluating the efficiency of 39 shapes across three tasks -- relative mean judgment tasks, expert choices, and data correlation estimation. Given how complex and tangled results are, rather than relying on conventional features for modeling, we built a model and introduced a corresponding design tool that offers recommendations for shape encodings. The perceptual effectiveness of shapes significantly varies across specific pairs, and certain shapes may enhance perceptual efficiency and accuracy. However, how performance varies does not map well to classical features of shape such as angles, fill, or convex hull. We developed a model based on pairwise relations between shapes measured in our experiments and the number of shapes required to intelligently recommend shape palettes for a given design. This tool provides designers with agency over shape selection while incorporating empirical elements of perceptual performance captured in our study. Our model advances the understanding of shape perception in visualization contexts and provides practical design guidelines for advanced shape usage in visualization design that optimize perceptual efficiency.",
        "uid": "v-full-1836",
        "time_stamp": "",
        "time_start": "",
        "time_end": "",
        "paper_type": "full",
        "keywords": [],
        "doi": "",
        "fno": "",
        "presentation_mode": "",
        "open_access": false,
        "accessible_pdf": false,
        "has_image": false,
        "has_pdf": false,
        "paper_award": "",
        "image_caption": "",
        "external_paper_link": "",
        "youtube_ff_link": "",
        "youtube_ff_id": "",
        "bunny_ff_link": "",
        "bunny_ff_subtitles": "",
        "youtube_prerecorded_link": "",
        "youtube_prerecorded_id": "",
        "bunny_prerecorded_link": "",
        "bunny_prerecorded_subtitles": ""
    },
    "v-full-1865": {
        "slot_id": "v-full-1865",
        "session_id": "full0",
        "title": "DaedalusData: Exploration, Knowledge Externalization and Labeling of Particles in Medical Manufacturing - A Design Study",
        "contributors": [
            "Alexander Wyss"
        ],
        "authors": [
            {
                "name": "Alexander Wyss",
                "email": "alexander.wyss@protonmail.com",
                "affiliations": [
                    "University of Z\u00fcrich, Z\u00fcrich, Switzerland",
                    "Roche pRED, Basel, Switzerland"
                ],
                "is_corresponding": true
            },
            {
                "name": "Gabriela Morgenshtern",
                "email": "gab.morgenshtern@gmail.com",
                "affiliations": [
                    "University of Zurich, Zurich, Switzerland"
                ],
                "is_corresponding": false
            },
            {
                "name": "Amanda Hirsch-H\u00fcsler",
                "email": "a.hirschhuesler@gmail.com",
                "affiliations": [
                    "Roche Diagnostics International, Rotkreuz, Switzerland"
                ],
                "is_corresponding": false
            },
            {
                "name": "J\u00fcrgen Bernard",
                "email": "bernard@ifi.uzh.ch",
                "affiliations": [
                    "University of Zurich, Zurich, Switzerland"
                ],
                "is_corresponding": false
            }
        ],
        "abstract": "In medical diagnostics of both early disease detection and routine patient care, particle-based contamination of in-vitro diagnostics (IVD) consumables poses a significant threat to patients. Objective data-driven decision making on the severity of contamination is key for reducing risk to patients, while saving time and cost in the quality assessment process. Our collaborators introduced us to their quality control process, including particle data acquisition through image recognition, feature extraction, and attributes reflecting the production context of particles. Shortcomings of the current process are analysis problems, like weak support in exploring thousands of particle images, associated attributes, and ineffective knowledge externalization for sense-making. Following the design study methodology, our contributions are a characterization of the problem space and requirements, the development and validation of DaedalusData, a comprehensive discussion of our study\u2019s learnings, and a generalizable approach for knowledge externalization. DaedalusData is a visual analytics system that empowers domain experts to explore particle contamination patterns, to label particles in label alphabets, and to externalize knowledge through semi-supervised label-informed data projections. The results of our case study show that DaedalusData supports experts in generating meaningful, comprehensive data overviews. Additionally, our user study evaluation shows high usability of DaedalusData and efficiently supports the labeling of large quantities of particles, and utilizes externalized knowledge to augment the dataset. Reflecting on our approach, we discuss insights on dataset augmentation via human knowledge externalization, and on the scalabilty and trade-offs that come with the adoption of this approach in practice.",
        "uid": "v-full-1865",
        "time_stamp": "",
        "time_start": "",
        "time_end": "",
        "paper_type": "full",
        "keywords": [],
        "doi": "",
        "fno": "",
        "presentation_mode": "",
        "open_access": false,
        "accessible_pdf": false,
        "has_image": false,
        "has_pdf": false,
        "paper_award": "",
        "image_caption": "",
        "external_paper_link": "",
        "youtube_ff_link": "",
        "youtube_ff_id": "",
        "bunny_ff_link": "",
        "bunny_ff_subtitles": "",
        "youtube_prerecorded_link": "",
        "youtube_prerecorded_id": "",
        "bunny_prerecorded_link": "",
        "bunny_prerecorded_subtitles": ""
    },
    "v-full-1866": {
        "slot_id": "v-full-1866",
        "session_id": "full0",
        "title": "Regularized Multi-Decoder Ensemble for an Error-Aware Scene Representation Network",
        "contributors": [
            "Tianyu Xiong"
        ],
        "authors": [
            {
                "name": "Tianyu Xiong",
                "email": "xiong.336@osu.edu",
                "affiliations": [
                    "The Ohio State University, Columbus, United States"
                ],
                "is_corresponding": true
            },
            {
                "name": "Skylar Wolfgang Wurster",
                "email": "wurster.18@osu.edu",
                "affiliations": [
                    "The Ohio State University, Columbus, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Hanqi Guo",
                "email": "guo.2154@osu.edu",
                "affiliations": [
                    "The Ohio State University, Columbus, United States",
                    "Argonne National Laboratory, Lemont, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Tom Peterka",
                "email": "tpeterka@mcs.anl.gov",
                "affiliations": [
                    "Argonne National Laboratory, Lemont, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Han-Wei Shen",
                "email": "hwshen@cse.ohio-state.edu",
                "affiliations": [
                    "The Ohio State University , Columbus , United States"
                ],
                "is_corresponding": false
            }
        ],
        "abstract": "Feature grid Scene Representation Networks (SRNs) have been applied to scientific data as compact functional surrogates for analysis and visualization. As SRNs are black-box lossy data representations, assessing the prediction quality is critical for scientific visualization applications to ensure that scientists can trust the information being visualized.  Currently, existing architectures do not support inference time reconstruction quality assessment, as voxel-wise errors cannot be evaluated in the absence of ground truth data. By employing uncertain neural network architectures in feature grid SRNs, we obtain prediction variances during inference time to facilitate confidence-aware data reconstruction. Specifically, we propose a parameter-efficient multi-decoder Ensemble SRN (E-SRN) architecture consisting of a shared feature grid with multiple lightweight multi-layer perceptron decoders. E-SRN can generate a set of plausible predictions for a given input coordinate to compute the mean as the ensemble prediction and the variance as a confidence score. The voxel-wise variance can be rendered along with the data to inform the reconstruction quality, or be integrated into uncertainty-aware volume visualization algorithms. To prevent the misalignment between the quantified variance and the prediction quality, we propose a novel variance regularization loss for ensemble learning that promotes the Regularized Ensemble SRN (RE-SRN) to obtain a more reliable variance that correlates closely to the true model error. We comprehensively evaluate the quality of variance quantification and data reconstruction of Monte Carlo Dropout (MCD), Mean Field Variational Inference (MFVI), Deep Ensemble (DE), and Predicting Variance (PV) in comparison with our proposed E-SRN and RE-SRN applied to state-of-the-art feature grid SRNs across diverse scalar field datasets. We demonstrate that RE-SRN attains the most accurate data reconstruction and competitive variance-error correlation among uncertain SRNs under the same neural network parameter budgets. Furthermore, we present an adaptation of uncertainty-aware volume rendering and shed light on the potential of incorporating uncertain predictions in improving the quality of volume rendering for uncertain SRNs. Through ablation studies on the regularization strength and ensemble size, we show that E-SRN and RE-SRN are expected to perform sufficiently well with a default configuration without requiring customized hyperparameter settings for different datasets.",
        "uid": "v-full-1866",
        "time_stamp": "",
        "time_start": "",
        "time_end": "",
        "paper_type": "full",
        "keywords": [],
        "doi": "",
        "fno": "",
        "presentation_mode": "",
        "open_access": false,
        "accessible_pdf": false,
        "has_image": false,
        "has_pdf": false,
        "paper_award": "",
        "image_caption": "",
        "external_paper_link": "",
        "youtube_ff_link": "",
        "youtube_ff_id": "",
        "bunny_ff_link": "",
        "bunny_ff_subtitles": "",
        "youtube_prerecorded_link": "",
        "youtube_prerecorded_id": "",
        "bunny_prerecorded_link": "",
        "bunny_prerecorded_subtitles": ""
    },
    "v-full-1874": {
        "slot_id": "v-full-1874",
        "session_id": "full0",
        "title": "Evaluating and extending speedup techniques for optimal crossing minimization in layered graph drawings",
        "contributors": [
            "Connor Wilson"
        ],
        "authors": [
            {
                "name": "Connor Wilson",
                "email": "wilson.conn@northeastern.edu",
                "affiliations": [
                    "Northeastern University, Boston, United States"
                ],
                "is_corresponding": true
            },
            {
                "name": "Eduardo Puerta",
                "email": "eduardopuertac@gmail.com",
                "affiliations": [
                    "Northeastern University, Boston, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Tarik Crnovrsanin",
                "email": "turokhunter@gmail.com",
                "affiliations": [
                    "northeastern university, Boston, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Sara Di Bartolomeo",
                "email": "sara.di-bartolomeo@uni-konstanz.de",
                "affiliations": [
                    "University of Konstanz, Konstanz, Germany",
                    "Northeastern University, Boston, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Cody Dunne",
                "email": "c.dunne@northeastern.edu",
                "affiliations": [
                    "Northeastern University, Boston, United States"
                ],
                "is_corresponding": false
            }
        ],
        "abstract": "A layered network is an important category of graph in which every node is assigned to a layer and layers are drawn as parallel or radial lines. They are commonly used to display temporal data or hierarchical networks. Previous research has demonstrated that minimizing edge crossings is the most important criterion to consider when looking to improve the readability of such networks. While heuristic approaches exist for crossing minimization, we are interested in optimal approaches to the problem that prioritize human readability over computational scalability. We aim to improve the usefulness and applicability of such optimal methods by understanding and improving their scalability to larger graphs. This paper categorizes and evaluates the state-of-the-art linear programming formulations for exact crossing minimization and describes nine new and existing techniques that could plausibly accelerate the optimization algorithm. Through a computational evaluation, we explore each technique's effect on calculation time and how the techniques assist or inhibit one another, allowing researchers and practitioners to adapt them to the characteristics of their networks. Our best-performing techniques yielded a median improvement of 2.5--17x depending on the solver used, giving us the capability to create optimal layouts faster and for larger networks. We provide an open-source implementation of our methodology in Python, where users can pick which combination of techniques to enable according to their use case. A free copy of this paper and all supplemental materials, datasets used, and source code are available at {https://osf.io/}.",
        "uid": "v-full-1874",
        "time_stamp": "",
        "time_start": "",
        "time_end": "",
        "paper_type": "full",
        "keywords": [],
        "doi": "",
        "fno": "",
        "presentation_mode": "",
        "open_access": false,
        "accessible_pdf": false,
        "has_image": false,
        "has_pdf": false,
        "paper_award": "",
        "image_caption": "",
        "external_paper_link": "",
        "youtube_ff_link": "",
        "youtube_ff_id": "",
        "bunny_ff_link": "",
        "bunny_ff_subtitles": "",
        "youtube_prerecorded_link": "",
        "youtube_prerecorded_id": "",
        "bunny_prerecorded_link": "",
        "bunny_prerecorded_subtitles": ""
    },
    "v-full-1880": {
        "slot_id": "v-full-1880",
        "session_id": "full0",
        "title": "Rapid and Precise Topological Comparison with Merge Tree Neural Networks",
        "contributors": [
            "Yu Qin"
        ],
        "authors": [
            {
                "name": "Yu Qin",
                "email": "yqin2@tulane.edu",
                "affiliations": [
                    "Tulane University, New Orleans, United States"
                ],
                "is_corresponding": true
            },
            {
                "name": "Brittany Terese Fasy",
                "email": "brittany.fasy@montana.edu",
                "affiliations": [
                    "Montana State University, Bozeman, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Carola Wenk",
                "email": "cwenk@tulane.edu",
                "affiliations": [
                    "Tulane University, New Orleans, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Brian Summa",
                "email": "bsumma@tulane.edu",
                "affiliations": [
                    "Tulane University, New Orleans, United States"
                ],
                "is_corresponding": false
            }
        ],
        "abstract": "Merge trees are a valuable tool in scientific visualization of scalar fields; however, current methods for merge tree comparisons are computationally expensive, primarily due to the exhaustive matching between tree nodes. To address this challenge, we introduce the merge tree neural networks (MTNN), a learned neural network model designed for merge tree comparison. The MTNN enables rapid and high-quality similarity computation. We first demonstrate how graph neural networks (GNNs), which emerged as an effective encoder for graphs, can be trained to produce embeddings of merge trees in vector spaces that enable efficient similarity comparison.  Next, we formulate the novel MTNN  model that further improves the similarity comparisons by integrating the tree and node embeddings with a new topological attention mechanism. We demonstrate the effectiveness of our model on real-world data in different domains and examine our model's generalizability across various datasets. Our experimental analysis demonstrates our approach's superiority in accuracy and efficiency. In particular, we speed up the prior state-of-the-art by more than 100x on the benchmark datasets while maintaining an error rate below 0.1%.",
        "uid": "v-full-1880",
        "time_stamp": "",
        "time_start": "",
        "time_end": "",
        "paper_type": "full",
        "keywords": [],
        "doi": "",
        "fno": "",
        "presentation_mode": "",
        "open_access": false,
        "accessible_pdf": false,
        "has_image": false,
        "has_pdf": false,
        "paper_award": "",
        "image_caption": "",
        "external_paper_link": "",
        "youtube_ff_link": "",
        "youtube_ff_id": "",
        "bunny_ff_link": "",
        "bunny_ff_subtitles": "",
        "youtube_prerecorded_link": "",
        "youtube_prerecorded_id": "",
        "bunny_prerecorded_link": "",
        "bunny_prerecorded_subtitles": ""
    },
    "v-full-1917": {
        "slot_id": "v-full-1917",
        "session_id": "full0",
        "title": "Towards Enhancing Low Vision Usability of Data Charts on Smartphones",
        "contributors": [
            "Yash Prakash"
        ],
        "authors": [
            {
                "name": "Yash Prakash",
                "email": "yprak001@odu.edu",
                "affiliations": [
                    "Old Dominion University, Norfolk, United States"
                ],
                "is_corresponding": true
            },
            {
                "name": "Pathan Aseef Khan",
                "email": "pkhan002@odu.edu",
                "affiliations": [
                    "Old Dominion University, Norfolk, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Akshay Kolgar Nayak",
                "email": "anaya001@odu.edu",
                "affiliations": [
                    "Old Dominion University, Norfolk, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Sampath Jayarathna",
                "email": "uksjayarathna@gmail.com",
                "affiliations": [
                    "Old Dominion University, Norfolk, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Hae-Na Lee",
                "email": "leehaena@msu.edu",
                "affiliations": [
                    "Michigan State University, East Lansing, United States"
                ],
                "is_corresponding": false
            },
            {
                "name": "Vikas Ashok",
                "email": "vganjigu@odu.edu",
                "affiliations": [
                    "Old Dominion University, Norfolk, United States"
                ],
                "is_corresponding": false
            }
        ],
        "abstract": "The importance of data charts is self-evident, given their ability to express complex data in a simple format that facilitates quick and easy comparisons, analysis, and consumption. However, the inherent visual nature of the charts creates barriers for people with visual impairments to reap the associated benefits to the same extent as their sighted peers. While extant research has predominantly focused on understanding and addressing these barriers for blind screen reader users, the needs of low-vision screen magnifier users have been largely overlooked. In an interview study, almost all low-vision participants stated that it was challenging to interact with data charts on small screen devices such as smartphones and tablets, even though they could technically \u201csee\u201d the chart content. They ascribed these challenges mainly to the magnification-induced loss of visual context that connected data points with each other and also with chart annotations, e.g., axis values. In this paper, we present a method that addresses this problem by automatically transforming charts that are typically non-interactive images into personalizable interactive charts which allow selective viewing of desired data points and preserve visual context as much as possible under screen enlargement. We evaluated our method in a usability study with 26 low-vision participants, who all performed a set of representative chart-related tasks under different study conditions. In the study, we observed that our method significantly improved the usability of charts over both the status quo screen magnifier and a state-of-the-art space compaction-based solution.",
        "uid": "v-full-1917",
        "time_stamp": "",
        "time_start": "",
        "time_end": "",
        "paper_type": "full",
        "keywords": [],
        "doi": "",
        "fno": "",
        "presentation_mode": "",
        "open_access": false,
        "accessible_pdf": false,
        "has_image": false,
        "has_pdf": false,
        "paper_award": "",
        "image_caption": "",
        "external_paper_link": "",
        "youtube_ff_link": "",
        "youtube_ff_id": "",
        "bunny_ff_link": "",
        "bunny_ff_subtitles": "",
        "youtube_prerecorded_link": "",
        "youtube_prerecorded_id": "",
        "bunny_prerecorded_link": "",
        "bunny_prerecorded_subtitles": ""
    }
}