{
    "v-full-1240": {
        "slot_id": "v-full-1240-qa",
        "session_id": "full1",
        "type": "In Person Q+A",
        "title": "Affective Learning Objectives for Communicative Visualizations (Q+A)",
        "contributors": [
            "Elsie Lee-Robbins"
        ],
        "authors": [
            "Elsie Lee-Robbins",
            "Eytan Adar"
        ],
        "abstract": "",
        "uid": "v-full-1240",
        "file_name": "",
        "time_stamp": "2022-10-18T22:13:00Z",
        "time_start": "2022-10-18T22:13:00Z",
        "time_end": "2022-10-18T22:15:00Z"
    },
    "v-full-1636": {
        "slot_id": "v-full-1636-qa",
        "session_id": "full1",
        "type": "In Person Q+A",
        "title": "The Goldilocks Zone: Balancing Trust and Task-Based Performance in Multiple COVID-19 Forecast Visualization Choices (Q+A)",
        "contributors": [
            "Dr. Lace Padilla"
        ],
        "authors": [
            "Lace Padilla",
            "Racquel Fygenson",
            "Spencer C. Castro",
            "Enrico Bertini"
        ],
        "abstract": "",
        "uid": "v-full-1636",
        "file_name": "",
        "time_stamp": "2022-10-18T22:28:00Z",
        "time_start": "2022-10-18T22:28:00Z",
        "time_end": "2022-10-18T22:30:00Z"
    },
    "v-full-1075": {
        "slot_id": "v-full-1075-qa",
        "session_id": "full1",
        "type": "In Person Q+A",
        "title": "Uncertainty-Aware Multidimensional Scaling (Q+A)",
        "contributors": [
            "David H\u00e4gele"
        ],
        "authors": [
            "David H\u00e4gele",
            "Tim Krake",
            "Daniel Weiskopf"
        ],
        "abstract": "",
        "uid": "v-full-1075",
        "file_name": "",
        "time_stamp": "2022-10-18T22:43:00Z",
        "time_start": "2022-10-18T22:43:00Z",
        "time_end": "2022-10-18T22:45:00Z"
    },
    "v-short-1097": {
        "slot_id": "v-short-1097-qa",
        "session_id": "full1",
        "type": "In Person Q+A",
        "title": "Exploring D3 Implementation Challenges on Stack Overflow (Q+A)",
        "contributors": [
            "Leilani Battle"
        ],
        "authors": [
            "Leilani Battle",
            "Danni Feng",
            "Kelli Webber"
        ],
        "abstract": "Visualization languages help to standardize the process of designing effective visualizations, one of the most prominent being D3. However, few researchers have analyzed at scale how users incorporate these languages into existing visualization programming processes, i.e., implementation workflows. In this paper, we present a new method for evaluating visualization languages. Our method emphasizes the experiences of users as observed through the online communities that have sprouted to facilitate public discussion and support around visualization languages. We demonstrate our method by analyzing D3 implementation workflows and challenges discussed on Stack Overflow. Our results show how the visualization community may be limiting its understanding of users' visualization implementation challenges by ignoring the larger context in which languages such as D3 are used. Based on our findings, we suggest new research directions to enhance the user experience with visualization languages. All our data and code are available at: https://osf.io/fup48/.",
        "uid": "v-short-1097",
        "file_name": "",
        "time_stamp": "2022-10-18T22:58:00Z",
        "time_start": "2022-10-18T22:58:00Z",
        "time_end": "2022-10-18T23:00:00Z"
    },
    "v-full-1467": {
        "slot_id": "v-full-1467-qa",
        "session_id": "full2",
        "type": "In Person Q+A",
        "title": "KiriPhys: Exploring New Data Physicalization Opportunities (Q+A)",
        "contributors": [
            "Foroozan Daneshzand"
        ],
        "authors": [
            "Foroozan Daneshzand",
            "Charles Perin",
            "Sheelagh Carpendale"
        ],
        "abstract": "",
        "uid": "v-full-1467",
        "file_name": "",
        "time_stamp": "2022-10-19T21:55:00Z",
        "time_start": "2022-10-19T21:55:00Z",
        "time_end": "2022-10-19T21:57:00Z"
    },
    "v-full-1197": {
        "slot_id": "v-full-1197-qa",
        "session_id": "full2",
        "type": "Virtual Q+A",
        "title": "Transtopia: Supporting Expressive and Faithful Pictograph Design with Visual Style Transfer (Q+A)",
        "contributors": [
            "Yang Shi"
        ],
        "authors": [
            "Yang Shi",
            "Pei Liu",
            "Siji Chen",
            "Mengdi Sun",
            "Nan Cao"
        ],
        "abstract": "",
        "uid": "v-full-1197",
        "file_name": "",
        "time_stamp": "2022-10-19T22:07:00Z",
        "time_start": "2022-10-19T22:07:00Z",
        "time_end": "2022-10-19T22:09:00Z"
    },
    "v-full-1221": {
        "slot_id": "v-full-1221-qa",
        "session_id": "full2",
        "type": "Virtual Q+A",
        "title": "Self-Supervised Color-Concept Association via Image Colorization (Q+A)",
        "contributors": [
            "Ruizhen Hu",
            "Ziqi Ye"
        ],
        "authors": [
            "Ruizhen Hu",
            "Ziqi Ye",
            "Bin Chen",
            "Oliver van Kaick",
            "Hui Huang"
        ],
        "abstract": "",
        "uid": "v-full-1221",
        "file_name": "",
        "time_stamp": "2022-10-19T22:19:00Z",
        "time_start": "2022-10-19T22:19:00Z",
        "time_end": "2022-10-19T22:21:00Z"
    },
    "v-full-1616": {
        "slot_id": "v-full-1616-qa",
        "session_id": "full2",
        "type": "In Person Q+A",
        "title": "Cultivating Visualization Literacy for Children through Curiosity and Play (Q+A)",
        "contributors": [
            "S. Sandra Bae"
        ],
        "authors": [
            "S. Sandra Bae",
            "Rishi Vanukuru",
            "Ruhan Yang",
            "Peter Gyory",
            "Ran Zhou",
            "Ellen Yi-Luen Do",
            "Danielle Albers Szafir"
        ],
        "abstract": "",
        "uid": "v-full-1616",
        "file_name": "",
        "time_stamp": "2022-10-19T22:31:00Z",
        "time_start": "2022-10-19T22:31:00Z",
        "time_end": "2022-10-19T22:33:00Z"
    },
    "v-full-1237": {
        "slot_id": "v-full-1237-qa",
        "session_id": "full2",
        "type": "In Person Q+A",
        "title": "Roboviz: A Game-Centered Project for Information Visualization Education (Q+A)",
        "contributors": [
            "Eytan Adar"
        ],
        "authors": [
            "Eytan Adar",
            "Elsie Lee-Robbins"
        ],
        "abstract": "",
        "uid": "v-full-1237",
        "file_name": "",
        "time_stamp": "2022-10-19T22:43:00Z",
        "time_start": "2022-10-19T22:43:00Z",
        "time_end": "2022-10-19T22:45:00Z"
    },
    "v-full-1621": {
        "slot_id": "v-full-1621-qa",
        "session_id": "full2",
        "type": "In Person Q+A",
        "title": "Relaxed Dot Plots: Faithful Visualization of Samples and Their Distribution (Q+A)",
        "contributors": [
            "Nils Rodrigues"
        ],
        "authors": [
            "Nils Rodrigues",
            "Christoph Schulz",
            "S\u00f6ren D\u00f6ring",
            "Daniel Baumgartner",
            "Tim Krake",
            "Daniel Weiskopf"
        ],
        "abstract": "",
        "uid": "v-full-1621",
        "file_name": "",
        "time_stamp": "2022-10-19T22:55:00Z",
        "time_start": "2022-10-19T22:55:00Z",
        "time_end": "2022-10-19T22:57:00Z"
    },
    "v-full-1406": {
        "slot_id": "v-full-1406-qa",
        "session_id": "full3",
        "type": "In Person Q+A",
        "title": "Multivariate Probabilistic Range Queries for Scalable Interactive 3D Visualization (Q+A)",
        "contributors": [
            "Markus Hadwiger"
        ],
        "authors": [
            "Amani Waleed Ageeli",
            "Alberto Jaspe-Villanueva",
            "Ronell Sicat",
            "Florian Mannuss",
            "Peter Rautek",
            "Markus Hadwiger"
        ],
        "abstract": "",
        "uid": "v-full-1406",
        "file_name": "",
        "time_stamp": "2022-10-20T02:55:00Z",
        "time_start": "2022-10-20T02:55:00Z",
        "time_end": "2022-10-20T02:57:00Z"
    },
    "v-full-1394": {
        "slot_id": "v-full-1394-qa",
        "session_id": "full3",
        "type": "Virtual Q+A",
        "title": "Dual Space Coupling Model Guided Overlap-free Scatterplot (Q+A)",
        "contributors": [
            "zeyu li"
        ],
        "authors": [
            "zeyu li",
            "RuiZhi Shi",
            "Yan Liu",
            "Shizhuo Long",
            "Ziheng Guo",
            "Shichao Jia",
            "Jiawan Zhang"
        ],
        "abstract": "",
        "uid": "v-full-1394",
        "file_name": "",
        "time_stamp": "2022-10-20T03:07:00Z",
        "time_start": "2022-10-20T03:07:00Z",
        "time_end": "2022-10-20T03:09:00Z"
    },
    "v-full-1124": {
        "slot_id": "v-full-1124-qa",
        "session_id": "full3",
        "type": "In Person Q+A",
        "title": "Measuring Effects of Spatial Visualization and Domain on Visualization Task Performance: A Comparative Study (Q+A)",
        "contributors": [
            "Sara Tandon"
        ],
        "authors": [
            "Sara Tandon",
            "Alfie Abdul-Rahman",
            "Rita Borgo"
        ],
        "abstract": "",
        "uid": "v-full-1124",
        "file_name": "",
        "time_stamp": "2022-10-20T03:19:00Z",
        "time_start": "2022-10-20T03:19:00Z",
        "time_end": "2022-10-20T03:21:00Z"
    },
    "v-tvcg-9735308": {
        "slot_id": "v-tvcg-9735308-qa",
        "session_id": "full3",
        "type": "In Person Q+A",
        "title": "Local Latent Representation based on Geometric Convolution for Particle Data Feature Exploration (Q+A)",
        "contributors": [
            "Li, Haoyu",
            "Haoyu Li"
        ],
        "authors": [
            "Haoyu Li",
            "Han-Wei Shen"
        ],
        "abstract": "Feature related particle data analysis plays an important role in many scientific applications such as fluid simulations, cosmology simulations and molecular dynamics. Compared to conventional methods that use hand-crafted feature descriptors, some recent studies focus on transforming the data into a new latent space, where features are easier to be identified, compared and extracted. However, it is challenging to transform particle data into latent representations, since the convolution neural networks used in prior studies require the data presented in regular grids. In this paper, we adopt Geometric Convolution, a neural network building block designed for 3D point clouds, to create latent representations for scientific particle data. These latent representations capture both the particle positions and their physical attributes in the local neighborhood so that features can be extracted by clustering in the latent space, and tracked by applying tracking algorithms such as mean-shift. We validate the extracted features and tracking results from our approach using datasets from three applications and show that they are comparable to the methods that define hand-crafted features for each specific dataset.",
        "uid": "v-tvcg-9735308",
        "file_name": "",
        "time_stamp": "2022-10-20T03:31:00Z",
        "time_start": "2022-10-20T03:31:00Z",
        "time_end": "2022-10-20T03:33:00Z"
    },
    "v-tvcg-9765327": {
        "slot_id": "v-tvcg-9765327-qa",
        "session_id": "full3",
        "type": "In Person Q+A",
        "title": "Graphical Enhancements for Effective Exemplar Identification in Contextual Data Visualizations (Q+A)",
        "contributors": [
            "Shenghui Cheng",
            "Xinyu Zhang"
        ],
        "authors": [
            "Xinyu Zhang",
            "Shenghui Cheng",
            "Klaus Mueller"
        ],
        "abstract": "An exemplar is an entity that represents a desirable instance in a multi-attribute configuration space. It offers certain strengths in some of its attributes without unduly compromising the strengths in other attributes. Exemplars are frequently sought after in real life applications, such as systems engineering, investment banking, drug advisory, product marketing and many others. We study a specific method for the visualization of multi-attribute configuration spaces, the Data Context Map (DCM), for its capacity in enabling users to identify proper exemplars. The DCM produces a 2D embedding where users can view the data objects in the context of the data attributes. We ask whether certain graphical enhancements can aid users to gain a better understanding of the attribute-wise tradeoffs and so select better exemplar sets.  We conducted several user studies for three different graphical designs, namely iso-contour, value-shaded topographic rendering and terrain topographic rendering, and compare these with a baseline DCM display. As a benchmark we use an exemplar set generated via Pareto optimization which has similar goals but unlike humans can operate in the native high-dimensional data space. Our study finds that the two topographic maps are statistically superior to both the iso-contour and the DCM baseline display.",
        "uid": "v-tvcg-9765327",
        "file_name": "",
        "time_stamp": "2022-10-20T03:43:00Z",
        "time_start": "2022-10-20T03:43:00Z",
        "time_end": "2022-10-20T03:45:00Z"
    },
    "v-tvcg-9514468": {
        "slot_id": "v-tvcg-9514468-qa",
        "session_id": "full3",
        "type": "In Person Q+A",
        "title": "Effectiveness Error: Measuring and Improving RadViz Visual Effectiveness (Q+A)",
        "contributors": [
            "Marco Angelini"
        ],
        "authors": [
            "Marco Angelini",
            "Graziano Blasilli",
            "Simone Lenti",
            "Alessia Palleschi",
            "Giuseppe Santucci"
        ],
        "abstract": "RadViz contributes to multidimensional analysis by using 2D points for encoding data elements and interpreting them alongthe original data dimensions. For these characteristics it is used in different application domains, like clustering, anomaly detection, and software visualization. However, it is likely that using the dimension arrangement that comes with the data will produce a plot that leads users to make inaccurate conclusions about points values and data distribution. This paper attacks this problem without altering the original RadViz design: it defines, for both a single point and a set of points, the metric of effectiveness error, and uses it to define the objective function of a dimension arrangement strategy, arguing that minimizing it increases the overall RadViz visual quality. This paper investigated the intuition that reducing the effectiveness error is beneficial for other well-known RadViz problems, like points clumping toward the center, many-to-one plotting of non-proportional points, and cluster separation. It presents an algorithm that reduces to zero the effectiveness error for a single point and a heuristic that approximates the dimension arrangement minimizing the effectiveness error for an arbitrary set of points. A set of experiments based on 21 real datasets has been performed, with the goals of analyzing the advantages of reducing the effectiveness error, comparing the proposed dimension arrangement strategy with other related proposals, and investigating the heuristic accuracy. The Effectiveness Error metric, the algorithm, and the heuristic presented in this paper have been made available in a d3.jsplugin at  https://aware-diag-sapienza.github.io/d3-radviz.",
        "uid": "v-tvcg-9514468",
        "file_name": "",
        "time_stamp": "2022-10-20T03:55:00Z",
        "time_start": "2022-10-20T03:55:00Z",
        "time_end": "2022-10-20T03:57:00Z"
    },
    "v-tvcg-9801603": {
        "slot_id": "v-tvcg-9801603-qa",
        "session_id": "full4",
        "type": "Virtual Q+A",
        "title": "A Unified Understanding of Deep NLP Models for Text Classification (Q+A)",
        "contributors": [
            "Weikai Yang",
            "Zhen Li"
        ],
        "authors": [
            "Zhen Li",
            "Xiting Wang",
            "Weikai Yang",
            "Jing Wu",
            "Zhengyan Zhang",
            "Zhiyuan Liu",
            "Maosong Sun",
            "Hui Zhang",
            "Shixia Liu"
        ],
        "abstract": "The rapid development of deep natural language processing (NLP) models for text classification has led to an urgent need for a unified understanding of these models proposed individually. Existing methods cannot meet the need for understanding different models in one framework due to the lack of a unified measure for explaining both low-level (e.g., words) and high-level (e.g., phrases) features. We have developed a visual analysis tool, DeepNLPVis, to enable a unified understanding of NLP models for text classification. The key idea is a mutual information-based measure, which provides quantitative explanations on how each layer of a model maintains the information of input words in a sample. We model the intra- and inter-word information at each layer measuring the importance of a word to the final prediction as well as the relationships between words, such as the formation of phrases. A multi-level visualization, which consists of a corpus-level, a sample-level, and a word-level visualization, supports the analysis from the overall training set to individual samples. Two case studies on classification tasks and comparison between models demonstrate that DeepNLPVis can help users effectively identify potential problems caused by samples and model architectures and then make informed improvements.",
        "uid": "v-tvcg-9801603",
        "file_name": "",
        "time_stamp": "2022-10-21T02:55:00Z",
        "time_start": "2022-10-21T02:55:00Z",
        "time_end": "2022-10-21T02:57:00Z"
    },
    "v-tvcg-9801527": {
        "slot_id": "v-tvcg-9801527-qa",
        "session_id": "full4",
        "type": "In Person Q+A",
        "title": "EBBE-Text: Explaining Neural Networks by Exploring Text Classification Decision Boundaries (Q+A)",
        "contributors": [
            "Alexis Delaforge"
        ],
        "authors": [
            "Alexis Delaforge",
            "J\u00e9r\u00f4me Az\u00e9",
            "Sandra Bringay",
            "Caroline Mollevi",
            "Arnaud Sallaberry",
            "Maximilien Servajean"
        ],
        "abstract": "While neural networks (NN) have been successfully applied to many NLP tasks, the way they function is often difficult to interpret. In this article, we focus on binary text classification via NNs and propose a new tool, which includes a visualization of the decision boundary and the distances of data elements to this boundary. This tool increases the interpretability of NN. Our approach uses two innovative views: (1) an overview of the text representation space and (2) a local view allowing data exploration around the decision boundary for various localities of this representation space. These views are integrated into a visual platform, EBBE-Text, which also contains state-of-the-art visualizations of NN representation spaces and several kinds of information obtained from the classification process. The various views are linked through numerous interactive functionalities that enable easy exploration of texts and classification results via the various complementary views. A user study shows the effectiveness of the visual encoding and a case study illustrates the benefits of using our tool for the analysis of the classifications obtained with several recent NNs and two datasets.",
        "uid": "v-tvcg-9801527",
        "file_name": "",
        "time_stamp": "2022-10-21T03:07:00Z",
        "time_start": "2022-10-21T03:07:00Z",
        "time_end": "2022-10-21T03:09:00Z"
    },
    "v-tvcg-9716779": {
        "slot_id": "v-tvcg-9716779-qa",
        "session_id": "full4",
        "type": "In Person Q+A",
        "title": "LegalVis: Exploring and Inferring Precedent Citations in Legal Documents (Q+A)",
        "contributors": [
            "Jean Roberto",
            "Lucas Resck"
        ],
        "authors": [
            "Lucas E. Resck",
            "Jean R. Ponciano",
            "Luis Gustavo Nonato",
            "Jorge Poco"
        ],
        "abstract": "To reduce the number of pending cases and conflicting rulings in the Brazilian Judiciary, the National Congress amended the Constitution, allowing the Brazilian Supreme Court (STF) to create binding precedents (BPs), i.e., a set of understandings that both Executive and lower Judiciary branches must follow. The STF's justices frequently cite the 58 existing BPs in their decisions, and it is of primary relevance that judicial experts could identify and analyze such citations. To assist in this problem, we propose LegalVis, a web-based visual analytics system designed to support the analysis of legal documents that cite or could potentially cite a BP. We model the problem of identifying potential citations (i.e., non-explicit) as a classification problem. However, a simple score is not enough to explain the results; that is why we use an interpretability machine learning method to explain the reason behind each identified citation. For a compelling visual exploration of documents and BPs, LegalVis comprises three interactive visual components: the first presents an overview of the data showing temporal patterns, the second allows filtering and grouping relevant documents by topic, and the last one shows a document's text aiming to interpret the model's output by pointing out which paragraphs are likely to mention the BP, even if not explicitly specified. We evaluated our identification model and obtained an accuracy of 96%; we also made a quantitative and qualitative analysis of the results. The usefulness and effectiveness of LegalVis were evaluated through two usage scenarios and feedback from six domain experts.",
        "uid": "v-tvcg-9716779",
        "file_name": "",
        "time_stamp": "2022-10-21T03:19:00Z",
        "time_start": "2022-10-21T03:19:00Z",
        "time_end": "2022-10-21T03:21:00Z"
    },
    "v-tvcg-9488285": {
        "slot_id": "v-tvcg-9488285-qa",
        "session_id": "full4",
        "type": "Virtual Q+A",
        "title": "DeHumor: Visual Analytics for Decomposing Humor (Q+A)",
        "contributors": [
            "Xingbo Wang"
        ],
        "authors": [
            "Xingbo Wang",
            "Yao Ming",
            "Tongshuang Wu",
            "Haipeng Zeng",
            " Yong Wang",
            "Huamin Qu"
        ],
        "abstract": "Despite being a critical communication skill, grasping humor is challenging\u2014a successful use of humor requires a mixture of both engaging content build-up and an appropriate vocal delivery (e.g., pause). Prior studies on computational humor emphasize the textual and audio features immediately next to the punchline, yet overlooking longer-term context setup. Moreover, the theories are usually too abstract for understanding each concrete humor snippet. To fill in the gap, we develop DeHumor, a visual analytical system for analyzing humorous behaviors in public speaking. To intuitively reveal the building blocks of each concrete example, DeHumor decomposes each humorous video into multimodal features and provides inline annotations of them on the video script. In particular, to better capture the build-ups, we introduce content repetition as a complement to features introduced in theories of computational humor and visualize them in a context linking graph. To help users locate the punchlines that have the desired features to learn, we summarize the content (with keywords) and humor feature statistics on an augmented time matrix. With case studies on stand-up comedy shows and TED talks, we show that DeHumor is able to highlight various building blocks of humor examples. In addition, expert interviews with communication coaches and humor researchers demonstrate the effectiveness of DeHumor for multimodal humor analysis of speech content and vocal delivery.",
        "uid": "v-tvcg-9488285",
        "file_name": "",
        "time_stamp": "2022-10-21T03:31:00Z",
        "time_start": "2022-10-21T03:31:00Z",
        "time_end": "2022-10-21T03:33:00Z"
    },
    "v-full-1651": {
        "slot_id": "v-full-1651-qa",
        "session_id": "full4",
        "type": "In Person Q+A",
        "title": "Interactive and visual prompt engineering for ad-hoc task adaptation with large language models (Q+A)",
        "contributors": [
            "Hendrik Strobelt"
        ],
        "authors": [
            "Hendrik Strobelt",
            "Albert Webson",
            "Victor Sanh",
            "Benjamin Hoover",
            "Johanna Beyer",
            "Hanspeter Pfister",
            "Alexander Rush"
        ],
        "abstract": "",
        "uid": "v-full-1651",
        "file_name": "",
        "time_stamp": "2022-10-21T03:43:00Z",
        "time_start": "2022-10-21T03:43:00Z",
        "time_end": "2022-10-21T03:45:00Z"
    },
    "v-tvcg-9765476": {
        "slot_id": "v-tvcg-9765476-qa",
        "session_id": "full4",
        "type": "In Person Q+A",
        "title": "A Hybrid In Situ Approach for Cost Efficient Image Database Generation (Q+A)",
        "contributors": [
            "Steffen Frey"
        ],
        "authors": [
            "Valentin Bruder",
            "Matthew Larsen",
            "Thomas Ertl",
            "Hank Childs",
            "Steffen Frey"
        ],
        "abstract": "The visualization of results while the simulation is running is increasingly common in extreme scale computing environments. We present a novel approach for in situ generation of image databases to achieve cost savings on supercomputers. Our approach, a hybrid between traditional inline and in transit techniques, dynamically distributes visualization tasks between simulation nodes and visualization nodes, using probing as a basis to estimate rendering cost. Our hybrid design differs from previous works in that it creates opportunities to minimize idle time from four fundamental types of inefficiency: variability, limited scalability, overhead, and rightsizing. We demonstrate our results by comparing our method against both inline and in transit methods for a variety of configurations, including two simulation codes and a scaling study that goes above 19K cores. Our findings show that our approach is superior in many configurations. As in situ visualization becomes increasingly ubiquitous, we believe our technique could lead to significant amounts of reclaimed cycles on supercomputers.",
        "uid": "v-tvcg-9765476",
        "file_name": "",
        "time_stamp": "2022-10-21T03:55:00Z",
        "time_start": "2022-10-21T03:55:00Z",
        "time_end": "2022-10-21T03:57:00Z"
    },
    "v-full-1346": {
        "slot_id": "v-full-1346-qa",
        "session_id": "full5",
        "type": "Virtual Q+A",
        "title": "Revealing the Semantics of Data Wrangling Script With COMANTICS (Q+A)",
        "contributors": [
            "Kai Xiong"
        ],
        "authors": [
            "Kai Xiong",
            "Zhongsu Luo",
            "Siwei Fu",
            "Yongheng Wang",
            "Mingliang Xu",
            "Yingcai Wu"
        ],
        "abstract": "",
        "uid": "v-full-1346",
        "file_name": "",
        "time_stamp": "2022-10-19T20:10:00Z",
        "time_start": "2022-10-19T20:10:00Z",
        "time_end": "2022-10-19T20:12:00Z"
    },
    "v-tvcg-9693232": {
        "slot_id": "v-tvcg-9693232-qa",
        "session_id": "full5",
        "type": "Virtual Q+A",
        "title": "Visualizing the Scripts of Data Wrangling with SOMNUS (Q+A)",
        "contributors": [
            "Kai Xiong,"
        ],
        "authors": [
            "Kai Xiong",
            "Siwei Fu",
            "Guoming Ding",
            "Zhongsu Luo",
            "Rong Yu",
            "Wei Chen",
            "Hujun Bao",
            "Yingcai Wu"
        ],
        "abstract": "Data workers use various scripting languages for data transformation, such as SAS, R, and Python. However, understanding intricate code pieces requires advanced programming skills, which hinders data workers from grasping the idea of data transformation at ease. Program visualization is beneficial for debugging and education and has the potential to illustrate transformations intuitively and interactively. In this paper, we explore visualization design for demonstrating the semantics of code pieces in the context of data transformation. First, to depict individual data transformations, we structure a design space by two primary dimensions, i.e., key parameters to encode and possible visual channels to be mapped. Then, we derive a collection of 23 glyphs that visualize the semantics of transformations. Next, we design a pipeline, named Somnus, that provides an overview of the creation and evolution of data tables using a provenance graph. At the same time, it allows detailed investigation of individual transformations. User feedback on Somnus is positive. Our study participants achieved better accuracy with less time using Somnus, and preferred it over carefully-crafted textual description. Further, we provide two example applications to demonstrate the utility and versatility of Somnus.",
        "uid": "v-tvcg-9693232",
        "file_name": "",
        "time_stamp": "2022-10-19T20:22:00Z",
        "time_start": "2022-10-19T20:22:00Z",
        "time_end": "2022-10-19T20:24:00Z"
    },
    "v-full-1171": {
        "slot_id": "v-full-1171-qa",
        "session_id": "full5",
        "type": "In Person Q+A",
        "title": "Rigel: Transforming Tabular Data By Declarative Mapping (Q+A)",
        "contributors": [
            "Ran Chen"
        ],
        "authors": [
            "Ran Chen",
            "Di Weng",
            "Yanwei Huang",
            "Xinhuan Shu",
            "Jiayi Zhou",
            "Guodao Sun",
            "Yingcai Wu"
        ],
        "abstract": "",
        "uid": "v-full-1171",
        "file_name": "",
        "time_stamp": "2022-10-19T20:34:00Z",
        "time_start": "2022-10-19T20:34:00Z",
        "time_end": "2022-10-19T20:36:00Z"
    },
    "v-full-1413": {
        "slot_id": "v-full-1413-qa",
        "session_id": "full5",
        "type": "Virtual Q+A",
        "title": "HiTailor: Interactive Transformation and Visualization for Hierarchical Tabular Data (Q+A)",
        "contributors": [
            "Guozheng Li",
            "Runfei Li"
        ],
        "authors": [
            "Guozheng Li",
            "Runfei Li",
            "Zicheng Wang",
            "Chi Harold Liu",
            "Min Lu",
            "Guoren Wang"
        ],
        "abstract": "",
        "uid": "v-full-1413",
        "file_name": "",
        "time_stamp": "2022-10-19T20:46:00Z",
        "time_start": "2022-10-19T20:46:00Z",
        "time_end": "2022-10-19T20:48:00Z"
    },
    "v-full-1138": {
        "slot_id": "v-full-1138-qa",
        "session_id": "full5",
        "type": "In Person Q+A",
        "title": "Animated Vega-Lite: A Unified Grammar of Interactive and Animated Visualizations (Q+A)",
        "contributors": [
            "Jonathan Zong"
        ],
        "authors": [
            "Jonathan Zong",
            "Josh M. Pollock",
            "Dylan Wootton",
            "Arvind Satyanarayan"
        ],
        "abstract": "",
        "uid": "v-full-1138",
        "file_name": "",
        "time_stamp": "2022-10-19T20:58:00Z",
        "time_start": "2022-10-19T20:58:00Z",
        "time_end": "2022-10-19T21:00:00Z"
    },
    "v-full-1045": {
        "slot_id": "v-full-1045-qa",
        "session_id": "full5",
        "type": "In Person Q+A",
        "title": "No Grammar to Rule Them All: A Survey of JSON-style DSLs for Visualization (Q+A)",
        "contributors": [
            "Andrew M McNutt"
        ],
        "authors": [
            "Andrew M McNutt"
        ],
        "abstract": "",
        "uid": "v-full-1045",
        "file_name": "",
        "time_stamp": "2022-10-19T21:10:00Z",
        "time_start": "2022-10-19T21:10:00Z",
        "time_end": "2022-10-19T21:12:00Z"
    },
    "v-tvcg-9576578": {
        "slot_id": "v-tvcg-9576578-qa",
        "session_id": "full6",
        "type": "In Person Q+A",
        "title": "Real-Time Visualization of Large-Scale Geological Models with Nonlinear Feature-Preserving  Levels of Detail (Q+A)",
        "contributors": [
            "Ronell Sicat"
        ],
        "authors": [
            "Ronell Sicat",
            "Mohamed Ibrahim",
            "Amani Ageeli",
            "Florian Mannuss",
            "Peter Rautek",
            "Markus Hadwiger"
        ],
        "abstract": "The rapidly growing size and complexity of 3D geological models has increased the need for level-of-detail techniques and compact encodings to facilitate interactive visualization. For large-scale hexahedral meshes, state-of-the-art approaches often employ wavelet schemes for level of detail as well as for data compression. Here, wavelet transforms serve two  purposes: (1) they achieve substantial compression for data reduction; and (2) the multiresolution encoding provides levels of detail for visualization. However, in coarser detail levels, important geometric features, such as geological faults, often get too smoothed out or lost, due to linear translation invariant filtering. The same is true for attribute features, such as discontinuities in porosity or permeability.We present a novel, integrated approach addressing both purposes above, while preserving critical data features of both model geometry and its attributes. Our first major contribution is that we completely decouple the computation of levels of detail from data compression, and perform nonlinear filtering in a high-dimensional data space jointly representing the geological model geometry with its attributes. Computing detail levels in this space enables us to jointly preserve features in both geometry and attributes. While designed in a general way, our framework specifically employs joint bilateral filters, computed efficiently on a high-dimensional permutohedral grid. For data compression, after the computation of all detail levels, each level is separately encoded with a standard wavelet transform. Our second major contribution is a compact GPU data structure for the encoded mesh and attributes that enables direct real-time GPU visualization without prior decoding.",
        "uid": "v-tvcg-9576578",
        "file_name": "",
        "time_stamp": "2022-10-21T20:10:00Z",
        "time_start": "2022-10-21T20:10:00Z",
        "time_end": "2022-10-21T20:12:00Z"
    },
    "v-tvcg-9736650": {
        "slot_id": "v-tvcg-9736650-qa",
        "session_id": "full6",
        "type": "In Person Q+A",
        "title": "CosmoVis: An Interactive Visual Analysis Tool for Exploring Hydrodynamic Cosmological Simulations (Q+A)",
        "contributors": [
            "Angus G. Forbes",
            "David Abramov"
        ],
        "authors": [
            "David Abramov",
            "Joseph N. Burchett",
            "Oskar Elek",
            "Cameron Hummels",
            "J. Xavier Prochaska",
            "Angus G. Forbes"
        ],
        "abstract": "We introduce CosmoVis, an open source web-based visualization tool for the interactive analysis of massive hydrodynamic cosmological simulation data. CosmoVis was designed in close collaboration with astrophysicists to enable researchers and citizen scientists to share and explore these datasets, and to use them to investigate a range of scientific questions. CosmoVis visualizes many key gas, dark matter, and stellar attributes extracted from the source simulations, which typically consist of complex data structures multiple terabytes in size, often requiring extensive data wrangling. CosmoVis introduces a range of features to facilitate real-time analysis of these simulations, including the use of \"virtual skewers,\" simulated analogues of absorption line spectroscopy that act as spectral probes piercing the volume of gaseous cosmic medium. We explain how such synthetic spectra can be used to gain insight into the source datasets and to make functional comparisons with observational data. Furthermore, we identify the main analysis tasks that CosmoVis enables and present implementation details of the software interface and the client-server architecture. We conclude by providing details of three contemporary scientific use cases that were conducted by domain experts using the software and by documenting expert feedback from astrophysicists at different career levels.",
        "uid": "v-tvcg-9736650",
        "file_name": "",
        "time_stamp": "2022-10-21T20:22:00Z",
        "time_start": "2022-10-21T20:22:00Z",
        "time_end": "2022-10-21T20:24:00Z"
    },
    "v-tvcg-9599597": {
        "slot_id": "v-tvcg-9599597-qa",
        "session_id": "full6",
        "type": "Virtual Q+A",
        "title": "Dynamic Mode Decomposition for Large-Scale Coherent Structure Extraction in Shear Flows (Q+A)",
        "contributors": [
            "Duong Nguyen"
        ],
        "authors": [
            "Duong Nguyen",
            "Panruo Wu",
            "Rodolfo Ostilla Monico",
            "Guoning Chen"
        ],
        "abstract": "Large-scale structures have been observed in many shear flows which are the fluid generated between two surfaces moving with different velocity. A better understanding of the physics of the structures (especially large-scale structures) in shear flows will help explain a diverse range of physical phenomena and improve our capability of modeling more complex turbulence flows. Many efforts have been made in order to capture such structures; however, conventional methods have their limitations, such as arbitrariness in parameter choice or specificity to certain setups. To address this challenge, we propose to use Multi-Resolution Dynamic Mode Decomposition (mrDMD), for large-scale structure extraction in shear flows. In particular, we show that the slow-motion DMD modes are able to reveal large-scale structures in shear flows that also have slow dynamics. In most cases, we find that the slowest DMD mode and its reconstructed flow can sufficiently capture the large-scale dynamics in the shear flows, which leads to a parameter-free strategy for large-scale structure extraction. Effective visualization of the large-scale structures can then be produced with the aid of the slowest DMD mode. To speed up the computation of mrDMD, we provide a fast GPU-based implementation. We also apply our method to some non-shear flows that need not behave quasi-linearly to demonstrate the limitation of our strategy of using the slowest DMD mode. For non-shear flows, we show that multiple modes from different levels of mrDMD may be needed to sufficiently characterize the flow behavior.",
        "uid": "v-tvcg-9599597",
        "file_name": "",
        "time_stamp": "2022-10-21T20:34:00Z",
        "time_start": "2022-10-21T20:34:00Z",
        "time_end": "2022-10-21T20:36:00Z"
    },
    "v-full-1239": {
        "slot_id": "v-full-1239-qa",
        "session_id": "full6",
        "type": "Virtual Q+A",
        "title": "A Comparison of Spatiotemporal Visualizations for 3D Urban Analytics (Q+A)",
        "contributors": [
            "Roberta Mota",
            "Roberta Cabral Ramos Mota"
        ],
        "authors": [
            "Roberta Mota",
            "Fabio Miranda",
            "Julio Daniel Silva",
            "Marius Horga",
            "Marcos Lage",
            "Luis Ceferino",
            "Usman Raza Alim",
            "Ehud Sharlin",
            "Nivan Ferreira"
        ],
        "abstract": "",
        "uid": "v-full-1239",
        "file_name": "",
        "time_stamp": "2022-10-21T20:46:00Z",
        "time_start": "2022-10-21T20:46:00Z",
        "time_end": "2022-10-21T20:48:00Z"
    },
    "v-tvcg-9750868": {
        "slot_id": "v-tvcg-9750868-qa",
        "session_id": "full6",
        "type": "In Person Q+A",
        "title": "EpiMob: Interactive Visual Analytics of Citywide Human Mobility Restrictions for Epidemic Control (Q+A)",
        "contributors": [
            "Chuang Yang"
        ],
        "authors": [
            "Chuang Yang",
            "Zhiwen Zhang",
            "Zipei Fan",
            "Renhe Jiang",
            "Quanjun Chen",
            "Xuan Song",
            "Ryosuke Shibasaki."
        ],
        "abstract": "The outbreak of coronavirus disease (COVID-19) has swept across more than 180 countries and territories since late January 2020. As a worldwide emergency response, governments have implemented various measures and policies, such as self-quarantine, travel restrictions, work from home, and regional lockdown, to control the spread of the epidemic. These countermeasures seek to restrict human mobility because COVID-19 is a highly contagious disease that is spread by human-to-human transmission. Medical experts and policymakers have expressed the urgency to effectively evaluate the outcome of human restriction policies with the aid of big data and information technology. Thus, based on big human mobility data and city POI data, an interactive visual analytics system called Epidemic Mobility (EpiMob) was designed in this study. The system interactively simulates the changes in human mobility and infection status in response to the implementation of a certain restriction policy or a combination of policies (e.g., regional lockdown, telecommuting, screening). Users can conveniently designate the spatial and temporal ranges for different mobility restriction policies. Then, the results reflecting the infection situation under different policies are dynamically displayed and can be flexibly compared and analyzed in depth. Multiple case studies consisting of interviews with domain experts were conducted in the largest metropolitan area of Japan (i.e., Greater Tokyo Area) to demonstrate that the system can provide insight into the effects of different human mobility restriction policies for epidemic control, through measurements and comparisons.",
        "uid": "v-tvcg-9750868",
        "file_name": "",
        "time_stamp": "2022-10-21T20:58:00Z",
        "time_start": "2022-10-21T20:58:00Z",
        "time_end": "2022-10-21T21:00:00Z"
    },
    "v-full-1291": {
        "slot_id": "v-full-1291-qa",
        "session_id": "full6",
        "type": "Virtual Q+A",
        "title": "On-Tube Attribute Visualization for Multivariate Trajectory Data (Q+A)",
        "contributors": [
            "Benjamin Russig"
        ],
        "authors": [
            "Benjamin Russig",
            "David Gro\u00df",
            "Raimund Dachselt",
            "Stefan Gumhold"
        ],
        "abstract": "",
        "uid": "v-full-1291",
        "file_name": "",
        "time_stamp": "2022-10-21T21:10:00Z",
        "time_start": "2022-10-21T21:10:00Z",
        "time_end": "2022-10-21T21:12:00Z"
    },
    "v-full-1307": {
        "slot_id": "v-full-1307-qa",
        "session_id": "full7",
        "type": "Virtual Q+A",
        "title": "Fiber Uncertainty Visualization for Bivariate Data With Parametric and Nonparametric Noise Models (Q+A)",
        "contributors": [
            "Dr. Tushar M. Athawale"
        ],
        "authors": [
            "Tushar M. Athawale",
            "Chris R. Johnson",
            "Sudhanshu Sane",
            "David Pugmire"
        ],
        "abstract": "",
        "uid": "v-full-1307",
        "file_name": "",
        "time_stamp": "2022-10-20T02:55:00Z",
        "time_start": "2022-10-20T02:55:00Z",
        "time_end": "2022-10-20T02:57:00Z"
    },
    "v-full-1121": {
        "slot_id": "v-full-1121-qa",
        "session_id": "full7",
        "type": "Virtual Q+A",
        "title": "Evaluating the Use of Uncertainty Visualisations for Imputations of Missing Values in Scatterplots (Q+A)",
        "contributors": [
            "Abhraneel Sarma"
        ],
        "authors": [
            "Abhraneel Sarma",
            "Shunan Guo",
            "Jane Hoffswell",
            "Ryan Rossi",
            "Fan Du",
            "Eunyee Koh",
            "Matthew Kay"
        ],
        "abstract": "",
        "uid": "v-full-1121",
        "file_name": "",
        "time_stamp": "2022-10-20T03:07:00Z",
        "time_start": "2022-10-20T03:07:00Z",
        "time_end": "2022-10-20T03:09:00Z"
    },
    "v-full-1062": {
        "slot_id": "v-full-1062-qa",
        "session_id": "full7",
        "type": "In Person Q+A",
        "title": "Dispersion vs Disparity: Hiding Uncertainty Can Encourage Stereotyping When Visualizing Social Outcomes (Q+A)",
        "contributors": [
            "Cindy Xiong",
            "Eli Holder"
        ],
        "authors": [
            "Eli Holder",
            "Cindy Xiong"
        ],
        "abstract": "",
        "uid": "v-full-1062",
        "file_name": "",
        "time_stamp": "2022-10-20T03:19:00Z",
        "time_start": "2022-10-20T03:19:00Z",
        "time_end": "2022-10-20T03:21:00Z"
    },
    "v-full-1269": {
        "slot_id": "v-full-1269-qa",
        "session_id": "full7",
        "type": "In Person Q+A",
        "title": "Communicating Uncertainty in Digital Humanities Visualization Research (Q+A)",
        "contributors": [
            "Georgia Panagiotidou"
        ],
        "authors": [
            "Georgia Panagiotidou",
            "Houda Lamqaddam",
            "Jeroen Poblome",
            "Koenraad Brosens",
            "Katrien Verbert",
            "Andrew Vande Moere"
        ],
        "abstract": "",
        "uid": "v-full-1269",
        "file_name": "",
        "time_stamp": "2022-10-20T03:31:00Z",
        "time_start": "2022-10-20T03:31:00Z",
        "time_end": "2022-10-20T03:33:00Z"
    },
    "v-tvcg-9566799": {
        "slot_id": "v-tvcg-9566799-qa",
        "session_id": "full7",
        "type": "In Person Q+A",
        "title": "Fuzzy Spreadsheet: Understanding and Exploring Uncertainties in Tabular Calculations (Q+A)",
        "contributors": [
            "Vaishali Dhanoa"
        ],
        "authors": [
            "Vaishali Dhanoa",
            "Conny Walchshofer",
            "Andreas Hinterreiter",
            "Eduard Gr\u00f6ller",
            "Marc Streit"
        ],
        "abstract": "Spreadsheet-based tools provide a simple yet effective way of calculating values, which makes them the number-one choice for building and formalizing simple models for budget planning and many other applications. A cell in a spreadsheet holds one specific value and gives a discrete, over precise view of the underlying model. Therefore, spreadsheets are of limited use when investigating the inherent uncertainties of such models and answering what-if questions. Existing extensions typically require a complex modeling process that cannot easily be embedded in a tabular layout. In Fuzzy Spreadsheet, a cell can hold and display a distribution of values. This integrated uncertainty-handling immediately conveys sensitivity and robustness information. The fuzzification of the cells enables calculations not only with precise values but also with distributions, and probabilities. We conservatively added and carefully crafted visuals to maintain the look and feel of a traditional spreadsheet while facilitating what-if analyses. Given a user-specified reference cell, Fuzzy Spreadsheet automatically extracts and visualizes contextually relevant information, such as impact, uncertainty, and degree of neighborhood, for the selected and related cells. To evaluate its usability and the perceived mental effort required, we conducted a user study. The results show that our approach outperforms traditional spreadsheets in terms of answer correctness, response time, and perceived mental effort in almost all tasks tested.",
        "uid": "v-tvcg-9566799",
        "file_name": "",
        "time_stamp": "2022-10-20T03:43:00Z",
        "time_start": "2022-10-20T03:43:00Z",
        "time_end": "2022-10-20T03:45:00Z"
    },
    "v-tvcg-9492011": {
        "slot_id": "v-tvcg-9492011-qa",
        "session_id": "full8",
        "type": "In Person Q+A",
        "title": "A Survey of Perception-Based Visualization Studies by Task (Q+A)",
        "contributors": [
            "Ghulam Jilani Quadri"
        ],
        "authors": [
            "Ghulam Jilani Quadri",
            "Paul Rosen"
        ],
        "abstract": "Knowledge of human perception has long been incorporated into visualizations to enhance their quality and effectiveness. The last decade, in particular, has shown an increase in perception-based visualization research studies. With all of this recent progress, the visualization community lacks a comprehensive guide to contextualize their results. In this report, we provide a systematic and comprehensive review of research studies on perception related to visualization. This survey reviews perception-focused visualization studies since 1980 and summarizes their research developments focusing on low-level tasks, further breaking techniques down by visual encoding and visualization type. In particular, we focus on how perception is used to evaluate the effectiveness of visualizations, to help readers understand and apply the principles of perception of their visualization designs through a task-optimized approach. We concluded our report with a summary of the weaknesses and open research questions in the area.",
        "uid": "v-tvcg-9492011",
        "file_name": "",
        "time_stamp": "2022-10-19T21:55:00Z",
        "time_start": "2022-10-19T21:55:00Z",
        "time_end": "2022-10-19T21:57:00Z"
    },
    "v-full-1219": {
        "slot_id": "v-full-1219-qa",
        "session_id": "full8",
        "type": "Virtual Q+A",
        "title": "BeauVis: A Validated Scale for Measuring the Aesthetic Pleasure of Visual Representations (Q+A)",
        "contributors": [
            "Tingying He"
        ],
        "authors": [
            "Tingying He",
            "Petra Isenberg",
            "Raimund Dachselt",
            "Tobias Isenberg"
        ],
        "abstract": "",
        "uid": "v-full-1219",
        "file_name": "",
        "time_stamp": "2022-10-19T22:07:00Z",
        "time_start": "2022-10-19T22:07:00Z",
        "time_end": "2022-10-19T22:09:00Z"
    },
    "v-full-1217": {
        "slot_id": "v-full-1217-qa",
        "session_id": "full8",
        "type": "In Person Q+A",
        "title": "Photosensitive Accessibility for Interactive Data Visualizations (Q+A)",
        "contributors": [
            "Laura South"
        ],
        "authors": [
            "Laura South",
            "Michelle A. Borkin"
        ],
        "abstract": "",
        "uid": "v-full-1217",
        "file_name": "",
        "time_stamp": "2022-10-19T22:19:00Z",
        "time_start": "2022-10-19T22:19:00Z",
        "time_end": "2022-10-19T22:21:00Z"
    },
    "v-full-1115": {
        "slot_id": "v-full-1115-qa",
        "session_id": "full8",
        "type": "In Person Q+A",
        "title": "Extending assignment inference to colormap data visualizations (Q+A)",
        "contributors": [
            "Melissa A. Schoenlein"
        ],
        "authors": [
            "Melissa A. Schoenlein",
            "Johnny Campos",
            "Kevin Lande",
            "Laurent Lessard",
            "Karen Schloss"
        ],
        "abstract": "",
        "uid": "v-full-1115",
        "file_name": "",
        "time_stamp": "2022-10-19T22:31:00Z",
        "time_start": "2022-10-19T22:31:00Z",
        "time_end": "2022-10-19T22:33:00Z"
    },
    "v-full-1448": {
        "slot_id": "v-full-1448-qa",
        "session_id": "full8",
        "type": "In Person Q+A",
        "title": "A Scanner Deeply: Predicting Human Eye Movement on Visualizations Using Crowdsourced Data Collection (Q+A)",
        "contributors": [
            "Sungbok Shin"
        ],
        "authors": [
            "Sungbok Shin",
            "Sunghyo Chung",
            "Sanghyun Hong",
            "Niklas Elmqvist"
        ],
        "abstract": "",
        "uid": "v-full-1448",
        "file_name": "",
        "time_stamp": "2022-10-19T22:43:00Z",
        "time_start": "2022-10-19T22:43:00Z",
        "time_end": "2022-10-19T22:45:00Z"
    },
    "v-full-1254": {
        "slot_id": "v-full-1254-qa",
        "session_id": "full8",
        "type": "In Person Q+A",
        "title": "Studying Early Decision Making with Progressive Bar Charts (Q+A)",
        "contributors": [
            "Ameya B Patil"
        ],
        "authors": [
            "Ameya B Patil",
            "Ga\u00eblle Richer",
            "Dominik Moritz",
            "Christopher Jermaine",
            "Jean-Daniel Fekete"
        ],
        "abstract": "",
        "uid": "v-full-1254",
        "file_name": "",
        "time_stamp": "2022-10-19T22:55:00Z",
        "time_start": "2022-10-19T22:55:00Z",
        "time_end": "2022-10-19T22:57:00Z"
    },
    "v-tvcg-9695246": {
        "slot_id": "v-tvcg-9695246-qa",
        "session_id": "full9",
        "type": "In Person Q+A",
        "title": "StrategyAtlas: Strategy Analysis for Machine Learning Interpretability (Q+A)",
        "contributors": [
            "Collaris, Dennis"
        ],
        "authors": [
            "Dennis Collaris",
            "Jarke J. van Wijk"
        ],
        "abstract": "Businesses in high-risk environments have been reluctant to adopt modern machine learning approaches due to their complex and uninterpretable nature. Most current solutions provide local, instance-level explanations, but this is insufficient for understanding the model as a whole. In this work, we show that strategy clusters (i.e., groups of data instances that are treated distinctly by the model) can be used to understand the global behavior of a complex ML model. To support effective exploration and understanding of these clusters, we introduce StrategyAtlas, a system designed to analyze and explain model strategies. Furthermore, it supports multiple ways to utilize these strategies for simplifying and improving the reference model. In collaboration with a large insurance company, we present a use case in automatic insurance acceptance, and show how professional data scientists were enabled to understand a complex model and improve the production model based on these insights.",
        "uid": "v-tvcg-9695246",
        "file_name": "",
        "time_stamp": "2022-10-20T21:55:00Z",
        "time_start": "2022-10-20T21:55:00Z",
        "time_end": "2022-10-20T21:57:00Z"
    },
    "v-full-1696": {
        "slot_id": "v-full-1696-qa",
        "session_id": "full9",
        "type": "In Person Q+A",
        "title": "VDL-Surrogate: A View-Dependent Latent-based Model for Parameter Space Exploration of Ensemble Simulations (Q+A)",
        "contributors": [
            "Neng Shi"
        ],
        "authors": [
            "Neng Shi",
            "Jiayi Xu",
            "Haoyu Li",
            "Hanqi Guo",
            "Jonathan Woodring",
            "Han-Wei Shen"
        ],
        "abstract": "",
        "uid": "v-full-1696",
        "file_name": "",
        "time_stamp": "2022-10-20T22:07:00Z",
        "time_start": "2022-10-20T22:07:00Z",
        "time_end": "2022-10-20T22:09:00Z"
    },
    "v-full-1074": {
        "slot_id": "v-full-1074-qa",
        "session_id": "full9",
        "type": "In Person Q+A",
        "title": "ConceptExplainer: Understanding the Mental Model of Deep Learning Algorithms via Interactive Concept-based Explanations (Q+A)",
        "contributors": [
            "Jinbin Huang"
        ],
        "authors": [
            "Jinbin Huang",
            "Aditi Mishra",
            "Bum Chul Kwon",
            "Chris Bryan"
        ],
        "abstract": "",
        "uid": "v-full-1074",
        "file_name": "",
        "time_stamp": "2022-10-20T22:19:00Z",
        "time_start": "2022-10-20T22:19:00Z",
        "time_end": "2022-10-20T22:21:00Z"
    },
    "v-full-1320": {
        "slot_id": "v-full-1320-qa",
        "session_id": "full9",
        "type": "In Person Q+A",
        "title": "SliceTeller : A Data Slice-Driven Approach for Machine Learning Model Validation (Q+A)",
        "contributors": [
            "Jorge H Piazentin Ono",
            "Xiaoyu Zhang"
        ],
        "authors": [
            "Xiaoyu Zhang",
            "Jorge H Piazentin Ono",
            "Huan Song",
            "Liang Gou",
            "Kwan-Liu Ma",
            "Liu Ren"
        ],
        "abstract": "",
        "uid": "v-full-1320",
        "file_name": "",
        "time_stamp": "2022-10-20T22:31:00Z",
        "time_start": "2022-10-20T22:31:00Z",
        "time_end": "2022-10-20T22:33:00Z"
    },
    "v-full-1657": {
        "slot_id": "v-full-1657-qa",
        "session_id": "full9",
        "type": "In Person Q+A",
        "title": "Calibrate: Interactive Analysis of Probabilistic Model Output (Q+A)",
        "contributors": [
            "Peter Xenopoulos"
        ],
        "authors": [
            "Peter Xenopoulos",
            "Jo\u00e3o Rulff",
            "Luis Gustavo Nonato",
            "Brian Barr",
            "Claudio Silva"
        ],
        "abstract": "",
        "uid": "v-full-1657",
        "file_name": "",
        "time_stamp": "2022-10-20T22:43:00Z",
        "time_start": "2022-10-20T22:43:00Z",
        "time_end": "2022-10-20T22:45:00Z"
    },
    "v-full-1319": {
        "slot_id": "v-full-1319-qa",
        "session_id": "full9",
        "type": "In Person Q+A",
        "title": "Visualizing Ensemble Predictions of Music Mood (Q+A)",
        "contributors": [
            "Mr Zelin Ye"
        ],
        "authors": [
            "Zelin Ye",
            "Min Chen"
        ],
        "abstract": "",
        "uid": "v-full-1319",
        "file_name": "",
        "time_stamp": "2022-10-20T22:55:00Z",
        "time_start": "2022-10-20T22:55:00Z",
        "time_end": "2022-10-20T22:57:00Z"
    },
    "v-full-1198": {
        "slot_id": "v-full-1198-qa",
        "session_id": "full10",
        "type": "Virtual Q+A",
        "title": "Breaking the Fourth Wall of Data Stories through Interaction (Q+A)",
        "contributors": [
            "Yang Shi",
            "Tian Gao"
        ],
        "authors": [
            "Yang Shi",
            "Tian Gao",
            "Xiaohan Jiao",
            "Nan Cao"
        ],
        "abstract": "",
        "uid": "v-full-1198",
        "file_name": "",
        "time_stamp": "2022-10-21T01:13:00Z",
        "time_start": "2022-10-21T01:13:00Z",
        "time_end": "2022-10-21T01:15:00Z"
    },
    "v-full-1495": {
        "slot_id": "v-full-1495-qa",
        "session_id": "full10",
        "type": "Virtual Q+A",
        "title": "Erato: Cooperative Data Story Editing via Fact Interpolation (Q+A)",
        "contributors": [
            "Prof. Nan Cao",
            "Mengdi Sun"
        ],
        "authors": [
            "Mengdi Sun",
            "Ligan Cai",
            "Weiwei Cui",
            "Yanqiu Wu",
            "Yang Shi",
            "Nan Cao"
        ],
        "abstract": "",
        "uid": "v-full-1495",
        "file_name": "",
        "time_stamp": "2022-10-21T01:28:00Z",
        "time_start": "2022-10-21T01:28:00Z",
        "time_end": "2022-10-21T01:30:00Z"
    },
    "v-full-1180": {
        "slot_id": "v-full-1180-qa",
        "session_id": "full10",
        "type": "Virtual Q+A",
        "title": "Geo-Storylines: Integrating Maps into Storyline Visualizations (Q+A)",
        "contributors": [
            "Vanessa Pe\u00f1a-Araya"
        ],
        "authors": [
            "Golina Hulstein",
            "Vanessa Pe\u00f1a-Araya",
            "Anastasia Bezerianos"
        ],
        "abstract": "",
        "uid": "v-full-1180",
        "file_name": "",
        "time_stamp": "2022-10-21T01:43:00Z",
        "time_start": "2022-10-21T01:43:00Z",
        "time_end": "2022-10-21T01:45:00Z"
    },
    "v-tvcg-9695173": {
        "slot_id": "v-tvcg-9695173-qa",
        "session_id": "full10",
        "type": "In Person Q+A",
        "title": "Roslingifier: Semi-Automated Storytelling for Animated Scatterplots (Q+A)",
        "contributors": [
            "Sungahn Ko"
        ],
        "authors": [
            "Minjeong Shin",
            "Joohee Kim",
            "Yunha Han",
            "Lexing Xie",
            "Mitchell Whitelaw",
            "Bum Chul Kwon",
            "Sungahn Ko",
            "Niklas Elmqvist"
        ],
        "abstract": "We present Roslingifier, a data-driven storytelling method for animated scatterplots. Like its namesake, Hans Rosling (1948--2017), a professor of public health and a spellbinding public speaker, Roslingifier turns a sequence of entities changing over time---such as countries and continents with their demographic data---into an engaging narrative telling the story of the data. This data-driven storytelling method with an in-person presenter is a new genre of storytelling technique and has never been studied before. In this paper, we aim to define a design space for this new genre---data presentation---and provide a semi-automated authoring tool for helping presenters create quality presentations. From an in-depth analysis of video clips of presentations using interactive visualizations, we derive three specific techniques to achieve this: natural language narratives, visual effects that highlight events, and temporal branching that changes playback time of the animation. Our implementation of the Roslingifier method is capable of identifying and clustering significant movements, automatically generating visual highlighting and a narrative for playback, and enabling the user to customize. From two user studies, we show that Roslingifier allows users to effectively create engaging data stories and the system features help both presenters and viewers find diverse insights.",
        "uid": "v-tvcg-9695173",
        "file_name": "",
        "time_stamp": "2022-10-21T01:58:00Z",
        "time_start": "2022-10-21T01:58:00Z",
        "time_end": "2022-10-21T02:00:00Z"
    },
    "v-tvcg-9645360": {
        "slot_id": "v-tvcg-9645360-qa",
        "session_id": "full10",
        "type": "In Person Q+A",
        "title": "Nanotilus: Generator of Immersive Guided-Tours in Crowded 3D Environments (Q+A)",
        "contributors": [
            "Ruwayda Alharbi"
        ],
        "authors": [
            "Ruwayda Alharbi",
            "Ond\u02c7rej Strnad",
            "Laura R. Luidolt",
            "Manuela Waldner",
            "David Kou\u02c7ril",
            "Ciril Bohak",
            "Tobias Klein",
            "Eduard Groller",
            "Ivan Viola"
        ],
        "abstract": "Immersive virtual reality environments are gaining popularity for studying and exploring crowded three-dimensional structures. When reaching very high structural densities, the natural depiction of the scene produces impenetrable clutter and requires visibility and occlusion management strategies for exploration and orientation. Strategies developed to address the crowdedness in desktop applications, however, inhibit the feeling of immersion. They result in nonimmersive, desktop-style outside-in viewing in virtual reality. This paper proposes Nanotilus---a new visibility and guidance approach for very dense environments that generates an endoscopic inside-out experience instead of outside-in viewing, preserving the immersive aspect of virtual reality. The approach consists of two novel, tightly coupled mechanisms that control scene sparsification simultaneously with camera path planning. The sparsification strategy is localized around the camera and is realized as a multi-scale, multi-shell, variety-preserving technique. When Nanotilus dives into the structures to capture internal details residing on multiple scales, it guides the camera using depth-based path planning. In addition to sparsification and path planning, we complete the tour generation with an animation controller, textual annotation, and text-to-visualization conversion. We demonstrate the generated guided tours on mesoscopic biological models -- SARS-CoV-2 and HIV. We evaluate the Nanotilus experience with a baseline outside-in sparsification and navigational technique in a formal user study with 29 participants. While users can maintain a better overview using the outside-in sparsification, the study confirms our hypothesis that Nanotilus leads to stronger engagement and immersion.",
        "uid": "v-tvcg-9645360",
        "file_name": "",
        "time_stamp": "2022-10-21T02:13:00Z",
        "time_start": "2022-10-21T02:13:00Z",
        "time_end": "2022-10-21T02:15:00Z"
    },
    "v-full-1103": {
        "slot_id": "v-full-1103-qa",
        "session_id": "full10",
        "type": "In Person Q+A",
        "title": "How Do Viewers Synthesize Conflicting Information from Data Visualizations? (Q+A)",
        "contributors": [
            "Mr. Prateek Mantri",
            "Cindy Xiong"
        ],
        "authors": [
            "Prateek Mantri",
            "Hariharan Subramonyam",
            "Audrey Michal",
            "Cindy Xiong"
        ],
        "abstract": "",
        "uid": "v-full-1103",
        "file_name": "",
        "time_stamp": "2022-10-21T02:28:00Z",
        "time_start": "2022-10-21T02:28:00Z",
        "time_end": "2022-10-21T02:30:00Z"
    },
    "v-full-1525": {
        "slot_id": "v-full-1525-qa",
        "session_id": "full11",
        "type": "Virtual Q+A",
        "title": "RankAxis: Towards a Systematic Combination of Projection and Ranking in Multi-Attribute Data Exploration (Q+A)",
        "contributors": [
            "Quan Li"
        ],
        "authors": [
            "Qiangqiang Liu",
            "Yukun Ren",
            "Zhihua Zhu",
            "Dai Li",
            "Xiaojuan Ma",
            "Quan Li"
        ],
        "abstract": "",
        "uid": "v-full-1525",
        "file_name": "",
        "time_stamp": "2022-10-20T20:13:00Z",
        "time_start": "2022-10-20T20:13:00Z",
        "time_end": "2022-10-20T20:15:00Z"
    },
    "v-full-1631": {
        "slot_id": "v-full-1631-qa",
        "session_id": "full11",
        "type": "In Person Q+A",
        "title": "PC-Expo: A Metrics-Based Interactive Axes Reordering Method for Parallel Coordinate Displays (Q+A)",
        "contributors": [
            "Anjul Kumar Tyagi"
        ],
        "authors": [
            "Anjul Kumar Tyagi",
            "Tyler Estro",
            "Geoff Kuenning",
            "Erez Zadok",
            "Klaus Mueller"
        ],
        "abstract": "",
        "uid": "v-full-1631",
        "file_name": "",
        "time_stamp": "2022-10-20T20:28:00Z",
        "time_start": "2022-10-20T20:28:00Z",
        "time_end": "2022-10-20T20:30:00Z"
    },
    "v-full-1359": {
        "slot_id": "v-full-1359-qa",
        "session_id": "full11",
        "type": "Virtual Q+A",
        "title": "Incorporation of Human Knowledge into Data Embeddings to Improve Pattern Significance and Interpretability (Q+A)",
        "contributors": [
            "Jie Li"
        ],
        "authors": [
            "Jie Li",
            "Chunqi Zhou"
        ],
        "abstract": "",
        "uid": "v-full-1359",
        "file_name": "",
        "time_stamp": "2022-10-20T20:43:00Z",
        "time_start": "2022-10-20T20:43:00Z",
        "time_end": "2022-10-20T20:45:00Z"
    },
    "v-full-1350": {
        "slot_id": "v-full-1350-qa",
        "session_id": "full11",
        "type": "Virtual Q+A",
        "title": "Interactive Visual Cluster Analysis by Contrastive Dimensionality Reduction (Q+A)",
        "contributors": [
            "Jiazhi Xia",
            "Linquan Huang"
        ],
        "authors": [
            "Jiazhi Xia",
            "Linquan Huang",
            "Weixing Lin",
            "Xin Zhao",
            "Jing Wu",
            "Yang Chen",
            "Ying Zhao",
            "Wei Chen"
        ],
        "abstract": "",
        "uid": "v-full-1350",
        "file_name": "",
        "time_stamp": "2022-10-20T20:58:00Z",
        "time_start": "2022-10-20T20:58:00Z",
        "time_end": "2022-10-20T21:00:00Z"
    },
    "v-full-1447": {
        "slot_id": "v-full-1447-qa",
        "session_id": "full11",
        "type": "In Person Q+A",
        "title": "Predicting User Preferences of Dimensionality Reduction Embedding Quality (Q+A)",
        "contributors": [
            "Cristina Morariu"
        ],
        "authors": [
            "Cristina Morariu",
            "Adrien Bibal",
            "Rene Cutura",
            "Benoit Frenay",
            "Michael Sedlmair"
        ],
        "abstract": "",
        "uid": "v-full-1447",
        "file_name": "",
        "time_stamp": "2022-10-20T21:13:00Z",
        "time_start": "2022-10-20T21:13:00Z",
        "time_end": "2022-10-20T21:15:00Z"
    },
    "v-tvcg-9382912": {
        "slot_id": "v-tvcg-9382912-qa",
        "session_id": "full11",
        "type": "In Person Q+A",
        "title": "VERTIGo: A Visual Platform for Querying and Exploring Large Multilayer Networks (Q+A)",
        "contributors": [
            "Erick Cuenca"
        ],
        "authors": [
            "Erick Cuenca",
            "Arnaud Sallaberry",
            "Dino Ienco",
            "Pascal Poncelet"
        ],
        "abstract": "Many real world data can be modeled by a graph with a set of nodes interconnected to each other by multiple relationships. Such a rich graph is called multilayer graph or network. Providing useful visualization tools to support the query process for such graphs is challenging. Although many approaches have addressed the visual query construction, few efforts have been done to provide a contextualized exploration of query results and suggestion strategies to refine the original query. This is due to several issues such as i) the size of the graphs ii) the large number of retrieved results and iii) the way they can be organized to facilitate their exploration. In this paper, we present VERTIGo, a novel visual platform to query, explore and support the analysis of large multilayer graphs. VERTIGo provides coordinated views to navigate and explore the large set of retrieved results at different granularity levels. In addition, the proposed system supports the refinement of the query by visual suggestions to guide the user through the exploration process. Two examples and a user study demonstrate how VERTIGo can be used to perform visual analysis query, exploration, and suggestion) on real world multilayer networks.",
        "uid": "v-tvcg-9382912",
        "file_name": "",
        "time_stamp": "2022-10-20T21:28:00Z",
        "time_start": "2022-10-20T21:28:00Z",
        "time_end": "2022-10-20T21:30:00Z"
    },
    "v-full-1653": {
        "slot_id": "v-full-1653-qa",
        "session_id": "full12",
        "type": "In Person Q+A",
        "title": "Probablement, Wahrscheinlich, Likely? A Cross-Language Study of How People Verbalize Probabilities in Icon Array Visualizations (Q+A)",
        "contributors": [
            "No\u00eblle Rakotondravony"
        ],
        "authors": [
            "No\u00eblle Rakotondravony",
            "Yiren Ding",
            "Lane Harrison"
        ],
        "abstract": "",
        "uid": "v-full-1653",
        "file_name": "",
        "time_stamp": "2022-10-21T20:13:00Z",
        "time_start": "2022-10-21T20:13:00Z",
        "time_end": "2022-10-21T20:15:00Z"
    },
    "v-full-1577": {
        "slot_id": "v-full-1577-qa",
        "session_id": "full12",
        "type": "Virtual Q+A",
        "title": "FlowNL: Asking the Flow Data in Natural Languages (Q+A)",
        "contributors": [
            "Jun Tao"
        ],
        "authors": [
            "Jieying Huang",
            "Yang Xi",
            "Junnan Hu",
            "Jun Tao"
        ],
        "abstract": "",
        "uid": "v-full-1577",
        "file_name": "",
        "time_stamp": "2022-10-21T20:28:00Z",
        "time_start": "2022-10-21T20:28:00Z",
        "time_end": "2022-10-21T20:30:00Z"
    },
    "v-full-1056": {
        "slot_id": "v-full-1056-qa",
        "session_id": "full12",
        "type": "In Person Q+A",
        "title": "Comparison Conundrum and the Chamber of Visualizations: An Exploration of How Language Influences Visual Design (Q+A)",
        "contributors": [
            "Aimen Gaba"
        ],
        "authors": [
            "Aimen Gaba",
            "Vidya Setlur",
            "Arjun Srinivasan",
            "Jane Hoffswell",
            "Cindy Xiong"
        ],
        "abstract": "",
        "uid": "v-full-1056",
        "file_name": "",
        "time_stamp": "2022-10-21T20:43:00Z",
        "time_start": "2022-10-21T20:43:00Z",
        "time_end": "2022-10-21T20:45:00Z"
    },
    "v-tvcg-9615008": {
        "slot_id": "v-tvcg-9615008-qa",
        "session_id": "full12",
        "type": "In Person Q+A",
        "title": "Explaining with Examples: Lessons Learned from Crowdsourced Introductory Description of Information Visualizations (Q+A)",
        "contributors": [
            "Leni YANG"
        ],
        "authors": [
            "Leni Yang; Cindy Xiong; Jason K. Wong; Aoyu Wu; Huamin Qu"
        ],
        "abstract": "Data visualizations have been increasingly used in oral presentations to communicate data patterns to the general public. Clear verbal introductions of visualizations to explain how to interpret the visually encoded information are essential to convey the takeaways and avoid misunderstandings. We contribute a series of studies to investigate how to effectively introduce visualizations to the audience with varying degrees of visualization literacy. We begin with understanding how people are introducing visualizations. We crowdsource 110 introductions of visualizations and categorize them based on their content and structures. From these crowdsourced introductions, we identify different introduction strategies and generate a set of introductions for evaluation. We conduct experiments to systematically compare the effectiveness of different introduction strategies across four visualizations with 1,080 participants. We find that introductions explaining visual encodings with concrete examples are the most effective. Our study provides both qualitative and quantitative insights into how to construct effective verbal introductions of visualizations in presentations, inspiring further research in data storytelling.",
        "uid": "v-tvcg-9615008",
        "file_name": "",
        "time_stamp": "2022-10-21T20:58:00Z",
        "time_start": "2022-10-21T20:58:00Z",
        "time_end": "2022-10-21T21:00:00Z"
    },
    "v-full-1260": {
        "slot_id": "v-full-1260-qa",
        "session_id": "full12",
        "type": "Virtual Q+A",
        "title": "Towards Natural Language-Based Visualization Authoring (Q+A)",
        "contributors": [
            "Yun Wang"
        ],
        "authors": [
            "Yun Wang",
            "Zhitao Hou",
            "Jiaqi Wang",
            "Tongshuang Wu",
            "Leixian Shen",
            "He Huang",
            "Haidong Zhang",
            "Dongmei Zhang"
        ],
        "abstract": "",
        "uid": "v-full-1260",
        "file_name": "",
        "time_stamp": "2022-10-21T21:13:00Z",
        "time_start": "2022-10-21T21:13:00Z",
        "time_end": "2022-10-21T21:15:00Z"
    },
    "v-full-1024": {
        "slot_id": "v-full-1024-qa",
        "session_id": "full12",
        "type": "In Person Q+A",
        "title": "Under- and Over-Texting: Exploring the Effect of Quantity and Content on Reader Preferences and Takeaways (Q+A)",
        "contributors": [
            "Chase Stokes"
        ],
        "authors": [
            "Chase Stokes",
            "Vidya Setlur",
            "Bridget Cogley",
            "Arvind Satyanarayan",
            "Marti Hearst"
        ],
        "abstract": "",
        "uid": "v-full-1024",
        "file_name": "",
        "time_stamp": "2022-10-21T21:28:00Z",
        "time_start": "2022-10-21T21:28:00Z",
        "time_end": "2022-10-21T21:30:00Z"
    },
    "v-tvcg-9733942": {
        "slot_id": "v-tvcg-9733942-qa",
        "session_id": "full13",
        "type": "In Person Q+A",
        "title": "Scientometric Analysis of Interdisciplinary Collaboration and Gender Trends in 30 Years of IEEE VIS Publications (Q+A)",
        "contributors": [
            "Ali Sarvghad"
        ],
        "authors": [
            "Ali Sarvghad",
            "Rolando Franqui-Nadal",
            "Rebecca Reznik-Zellen",
            "Ria Chawla",
            "Narges Mahyar"
        ],
        "abstract": "We present the results of a scientometric analysis of 30 years of IEEE VIS publications between 1990-2020, in which we conducted a multifaceted analysis of interdisciplinary collaboration and gender composition among authors. To this end, we curated BiblioVIS, a bibliometric dataset that contains rich metadata about IEEE VIS publications, including 3032 papers and 6113 authors. One of the main factors differentiating BiblioVIS from similar datasets is the authors' gender and discipline data, which we inferred through iterative rounds of computational and manual processes. Our analysis shows that, by and large, inter-institutional and interdisciplinary collaboration has been steadily growing over the past 30 years. However, interdisciplinary research was mainly between a few fields, including Computer Science, Engineering and Technology, and Medicine and Health disciplines. Our analysis of gender shows steady growth in women's authorship. Despite this growth, the gender distribution is still highly skewed, with men dominating (~75%) of this space. Our predictive analysis of gender balance shows that if the current trends continue, gender parity in the visualization field will not be reached before the third quarter of the century (~2070). Our primary goal in this work is to call the visualization community's attention to the critical topics of collaboration, diversity, and gender. Our research offers critical insights through the lens of diversity and gender to help accelerate progress towards a more diverse and representative research community.",
        "uid": "v-tvcg-9733942",
        "file_name": "",
        "time_stamp": "2022-10-21T01:13:00Z",
        "time_start": "2022-10-21T01:13:00Z",
        "time_end": "2022-10-21T01:15:00Z"
    },
    "v-full-1364": {
        "slot_id": "v-full-1364-qa",
        "session_id": "full13",
        "type": "In Person Q+A",
        "title": "Thirty-Two Years of IEEE VIS: Authors, Fields of Study and Citations (Q+A)",
        "contributors": [
            "Hongtao Hao"
        ],
        "authors": [
            "Hongtao Hao",
            "Yumian Cui",
            "Zhengxiang Wang",
            "Yea-Seul Kim"
        ],
        "abstract": "",
        "uid": "v-full-1364",
        "file_name": "",
        "time_stamp": "2022-10-21T01:28:00Z",
        "time_start": "2022-10-21T01:28:00Z",
        "time_end": "2022-10-21T01:30:00Z"
    },
    "v-tvcg-9747941": {
        "slot_id": "v-tvcg-9747941-qa",
        "session_id": "full13",
        "type": "In Person Q+A",
        "title": "SD^2: Slicing and Dicing Scholarly Data for Interactive Evaluation of Academic Performance (Q+A)",
        "contributors": [
            "Zhichun Guo"
        ],
        "authors": [
            "Zhichun Guo",
            "Jun Tao",
            "Siming Chen",
            "Nitesh V. Chawla",
            "Chaoli Wang"
        ],
        "abstract": "Comprehensively evaluating and comparing researchers' academic performance is complicated due to the intrinsic complexity of scholarly data. Different scholarly evaluation tasks often require the publication and citation data to be investigated in various manners. In this paper, we present an interactive visualization framework, SD^2, to enable flexible data partition and composition to support various analysis requirements within a single system. SD^2 features the hierarchical histogram, a novel visual representation for flexibly slicing and dicing the data, allowing different aspects of scholarly performance to be studied and compared. We also leverage the state-of-the-art set visualization technique to select individual researchers or combine multiple scholars for comprehensive visual comparison. We conduct multiple rounds of expert evaluation to study the effectiveness and usability of SD^2 and revise the design and system implementation accordingly. The effectiveness of SD^2 is demonstrated via multiple usage scenarios with each aiming to answer a specific, commonly raised question.",
        "uid": "v-tvcg-9747941",
        "file_name": "",
        "time_stamp": "2022-10-21T01:43:00Z",
        "time_start": "2022-10-21T01:43:00Z",
        "time_end": "2022-10-21T01:45:00Z"
    },
    "v-full-1344": {
        "slot_id": "v-full-1344-qa",
        "session_id": "full13",
        "type": "In Person Q+A",
        "title": "In Defence of Visual Analytics Systems: Replies to Critics (Q+A)",
        "contributors": [
            "Aoyu Wu"
        ],
        "authors": [
            "Aoyu Wu",
            "Dazhen Deng",
            "Furui Cheng",
            "Yingcai Wu",
            "Shixia Liu",
            "Huamin Qu"
        ],
        "abstract": "",
        "uid": "v-full-1344",
        "file_name": "",
        "time_stamp": "2022-10-21T01:58:00Z",
        "time_start": "2022-10-21T01:58:00Z",
        "time_end": "2022-10-21T02:00:00Z"
    },
    "v-full-1152": {
        "slot_id": "v-full-1152-qa",
        "session_id": "full13",
        "type": "In Person Q+A",
        "title": "Visualization Design Practices in a Crisis: Behind the Scenes with COVID-19 Dashboard Creators (Q+A)",
        "contributors": [
            "Yixuan Zhang"
        ],
        "authors": [
            "Yixuan Zhang",
            "Joseph D Gaggiano",
            "Yifan Sun",
            "Neha Kumar",
            "Clio Andris",
            "Andrea G Parker"
        ],
        "abstract": "",
        "uid": "v-full-1152",
        "file_name": "",
        "time_stamp": "2022-10-21T02:13:00Z",
        "time_start": "2022-10-21T02:13:00Z",
        "time_end": "2022-10-21T02:15:00Z"
    },
    "v-full-1456": {
        "slot_id": "v-full-1456-qa",
        "session_id": "full13",
        "type": "In Person Q+A",
        "title": "Understanding how Designers Find and Use Data Visualization Examples (Q+A)",
        "contributors": [
            "Hannah K. Bako"
        ],
        "authors": [
            "Hannah K. Bako",
            "Xinyi Liu",
            "Leilani Battle",
            "Zhicheng Liu"
        ],
        "abstract": "",
        "uid": "v-full-1456",
        "file_name": "",
        "time_stamp": "2022-10-21T02:28:00Z",
        "time_start": "2022-10-21T02:28:00Z",
        "time_end": "2022-10-21T02:30:00Z"
    },
    "v-full-1006": {
        "slot_id": "v-full-1006-qa",
        "session_id": "full14",
        "type": "In Person Q+A",
        "title": "A Systematic Review of BGP Visualization Tools: Mapping Techniques to Cyberattack and Anomaly Attributes (Q+A)",
        "contributors": [
            "Justin Raynor"
        ],
        "authors": [
            "Justin Raynor",
            "Tarik Crnovrsanin",
            "Sara Di Bartolomeo",
            "Laura South",
            "David Saffo",
            "Cody Dunne"
        ],
        "abstract": "",
        "uid": "v-full-1006",
        "file_name": "",
        "time_stamp": "2022-10-21T02:58:00Z",
        "time_start": "2022-10-21T02:58:00Z",
        "time_end": "2022-10-21T03:00:00Z"
    },
    "v-full-1329": {
        "slot_id": "v-full-1329-qa",
        "session_id": "full14",
        "type": "Virtual Q+A",
        "title": "RISeer: Inspecting the Status and Dynamics of Regional Industrial Structure via Visual Analytics (Q+A)",
        "contributors": [
            "Longfei Chen"
        ],
        "authors": [
            "Longfei Chen",
            "Yang Ouyang",
            "Haipeng Zhang",
            "Suting Hong",
            "Quan Li"
        ],
        "abstract": "",
        "uid": "v-full-1329",
        "file_name": "",
        "time_stamp": "2022-10-21T03:13:00Z",
        "time_start": "2022-10-21T03:13:00Z",
        "time_end": "2022-10-21T03:15:00Z"
    },
    "v-full-1161": {
        "slot_id": "v-full-1161-qa",
        "session_id": "full14",
        "type": "In Person Q+A",
        "title": "PMU Tracker: A Visualization Platform for Egocentric Event Propagation Analysis in the Power Grid (Q+A)",
        "contributors": [
            "Anjana Arunkumar"
        ],
        "authors": [
            "Anjana Arunkumar",
            "Andrea Pinceti",
            "Lalitha Sankar",
            "Chris Bryan"
        ],
        "abstract": "",
        "uid": "v-full-1161",
        "file_name": "",
        "time_stamp": "2022-10-21T03:28:00Z",
        "time_start": "2022-10-21T03:28:00Z",
        "time_end": "2022-10-21T03:30:00Z"
    },
    "v-full-1143": {
        "slot_id": "v-full-1143-qa",
        "session_id": "full14",
        "type": "Virtual Q+A",
        "title": "ECoalVis: Visual Analysis of Control Strategies in Coal-fired Power Plants (Q+A)",
        "contributors": [
            "Shuhan Liu"
        ],
        "authors": [
            "Shuhan Liu",
            "Di Weng",
            "Yuan Tian",
            "Zikun Deng",
            "Haoran Xu",
            "Xiangyu Zhu",
            "Honglei Yin",
            "Xianyuan Zhan",
            "Yingcai Wu"
        ],
        "abstract": "",
        "uid": "v-full-1143",
        "file_name": "",
        "time_stamp": "2022-10-21T03:43:00Z",
        "time_start": "2022-10-21T03:43:00Z",
        "time_end": "2022-10-21T03:45:00Z"
    },
    "v-full-1491": {
        "slot_id": "v-full-1491-qa",
        "session_id": "full14",
        "type": "In Person Q+A",
        "title": "A Visual Analytics System for Improving Traffic Forecasting Models (Q+A)",
        "contributors": [
            "Seungmin Jin"
        ],
        "authors": [
            "Seungmin Jin",
            "Hyunwook Lee",
            "Cheonbok Park",
            "Hyeshin Chu",
            "Yunwon Tae",
            "Jaegul Choo",
            "Sungahn Ko"
        ],
        "abstract": "",
        "uid": "v-full-1491",
        "file_name": "",
        "time_stamp": "2022-10-21T03:58:00Z",
        "time_start": "2022-10-21T03:58:00Z",
        "time_end": "2022-10-21T04:00:00Z"
    },
    "v-tvcg-9632413": {
        "slot_id": "v-tvcg-9632413-qa",
        "session_id": "full14",
        "type": "Virtual Q+A",
        "title": "RCMVis: A Visual Analytics System for Route Choice Modeling (Q+A)",
        "contributors": [
            "DongHwa Shin"
        ],
        "authors": [
            "DongHwa Shin",
            "Jaemin Jo",
            "Bohyoung Kim",
            "Hyunjoo Song",
            "Shin-Hyung Cho",
            "Jinwook Seo"
        ],
        "abstract": "We present RCMVis, a visual analytics system to support interactive Route Choice Modeling analysis. It aims to model which characteristics of routes, such as distance and the number of traffic lights, affect travelers' route choice behaviors and how much they affect the choice during their trips. Through close collaboration with domain experts, we designed a visual analytics framework for Route Choice Modeling. The framework supports three interactive analysis stages: exploration, modeling, and reasoning. In the exploration stage, we help analysts interactively explore trip data from multiple origin-destination (OD) pairs and choose a subset of data they want to focus on. To this end, we provide coordinated multiple OD views with different foci that allow analysts to inspect, rank, and compare OD pairs in terms of their multidimensional attributes. In the modeling stage, we integrate a k-medoids clustering method and a path-size logit model into our system to enable analysts to model route choice behaviors from trips with support for feature selection, hyperparameter tuning, and model comparison. Finally, in the reasoning stage, we help analysts rationalize and refine the model by selectively inspecting the trips that strongly support the modeling result. For evaluation, we conducted a case study and interviews with domain experts. The domain experts discovered unexpected insights from numerous modeling results, allowing them to explore the hyperparameter space more effectively to gain better results. In addition, they gained OD- and road-level insights into which data mainly supported the modeling result, enabling further discussion of the model.",
        "uid": "v-tvcg-9632413",
        "file_name": "",
        "time_stamp": "2022-10-21T04:13:00Z",
        "time_start": "2022-10-21T04:13:00Z",
        "time_end": "2022-10-21T04:15:00Z"
    },
    "v-full-1238": {
        "slot_id": "v-full-1238-qa",
        "session_id": "full15",
        "type": "In Person Q+A",
        "title": "Sporthesia: Augmenting Sports Videos Using Natural Language (Q+A)",
        "contributors": [
            "Zhutian Chen"
        ],
        "authors": [
            "Zhutian Chen",
            "Qisen Yang",
            "Xiao Xie",
            "Johanna Beyer",
            "Haijun Xia",
            "Yingcai Wu",
            "Hanspeter Pfister"
        ],
        "abstract": "",
        "uid": "v-full-1238",
        "file_name": "",
        "time_stamp": "2022-10-20T21:58:00Z",
        "time_start": "2022-10-20T21:58:00Z",
        "time_end": "2022-10-20T22:00:00Z"
    },
    "v-full-1083": {
        "slot_id": "v-full-1083-qa",
        "session_id": "full15",
        "type": "Virtual Q+A",
        "title": "OBTracker: Visual Analytics of Off-ball Movements in Basketball (Q+A)",
        "contributors": [
            "Yihong Wu"
        ],
        "authors": [
            "Yihong Wu",
            "Dazhen Deng",
            "Xiao Xie",
            "Moqi He",
            "Jie Xu",
            "Hongzeng Zhang",
            "Hui Zhang",
            "Yingcai Wu"
        ],
        "abstract": "",
        "uid": "v-full-1083",
        "file_name": "",
        "time_stamp": "2022-10-20T22:13:00Z",
        "time_start": "2022-10-20T22:13:00Z",
        "time_end": "2022-10-20T22:15:00Z"
    },
    "v-tvcg-9802784": {
        "slot_id": "v-tvcg-9802784-qa",
        "session_id": "full15",
        "type": "Virtual Q+A",
        "title": "Visualization in Motion: A Research Agenda and Two Evaluations (Q+A)",
        "contributors": [
            "Lijie Yao"
        ],
        "authors": [
            "Lijie Yao",
            "Anastasia Bezerianos",
            "Romain Vuillemot",
            "Petra Isenberg"
        ],
        "abstract": "We contribute a research agenda for visualization in motion and two experiments to understand how well viewers can read data from moving visualizations. We define visualizations in motion as visual data representations that are used in contexts that exhibit relative motion between a viewer and an entire visualization. Sports analytics, video games, wearable devices, or data physicalizations are example contexts that involve different types of relative motion between a viewer and a visualization. To analyze the opportunities and challenges for designing visualization in motion, we show example scenarios and outline a first research agenda. Motivated primarily by the prevalence of and opportunities for visualizations in sports and video games we started to investigate a small aspect of our research agenda: the impact of two important characteristics of motion---speed and trajectory on a stationary viewer's ability to read data from moving donut and bar charts. We found that increasing speed and trajectory complexity did negatively affect the accuracy of reading values from the charts and that bar charts were more negatively impacted. In practice, however, this impact was small: both charts were still read fairly accurately.",
        "uid": "v-tvcg-9802784",
        "file_name": "",
        "time_stamp": "2022-10-20T22:28:00Z",
        "time_start": "2022-10-20T22:28:00Z",
        "time_end": "2022-10-20T22:30:00Z"
    },
    "v-full-1041": {
        "slot_id": "v-full-1041-qa",
        "session_id": "full15",
        "type": "Virtual Q+A",
        "title": "RASIPAM: Interactive Pattern Mining of Multivariate Event Sequences in Racket Sports (Q+A)",
        "contributors": [
            "Jiang Wu"
        ],
        "authors": [
            "Jiang Wu",
            "Dongyu Liu",
            "Ziyang Guo",
            "Yingcai Wu"
        ],
        "abstract": "",
        "uid": "v-full-1041",
        "file_name": "",
        "time_stamp": "2022-10-20T22:43:00Z",
        "time_start": "2022-10-20T22:43:00Z",
        "time_end": "2022-10-20T22:45:00Z"
    },
    "v-full-1031": {
        "slot_id": "v-full-1031-qa",
        "session_id": "full15",
        "type": "Virtual Q+A",
        "title": "Tac-Trainer: A Visual Analytics System for IoT-based Racket Sports Training (Q+A)",
        "contributors": [
            "Jiachen Wang"
        ],
        "authors": [
            "Jiachen Wang",
            "Ji Ma",
            "Kangping Hu",
            "Zheng Zhou",
            "Hui Zhang",
            "Xiao Xie",
            "Yingcai Wu"
        ],
        "abstract": "",
        "uid": "v-full-1031",
        "file_name": "",
        "time_stamp": "2022-10-20T22:58:00Z",
        "time_start": "2022-10-20T22:58:00Z",
        "time_end": "2022-10-20T23:00:00Z"
    },
    "v-full-1223": {
        "slot_id": "v-full-1223-qa",
        "session_id": "full15",
        "type": "In Person Q+A",
        "title": "Omnioculars: Embedded Visualization for Augmenting Basketball Game Viewing Experiences (Q+A)",
        "contributors": [
            "Tica Lin"
        ],
        "authors": [
            "Tica Lin",
            "Zhutian Chen",
            "Yalong Yang",
            "Daniele Chiappalupi",
            "Johanna Beyer",
            "Hanspeter Pfister"
        ],
        "abstract": "",
        "uid": "v-full-1223",
        "file_name": "",
        "time_stamp": "2022-10-20T23:13:00Z",
        "time_start": "2022-10-20T23:13:00Z",
        "time_end": "2022-10-20T23:15:00Z"
    },
    "v-tvcg-9779066": {
        "slot_id": "v-tvcg-9779066-qa",
        "session_id": "full16",
        "type": "In Person Q+A",
        "title": "ClinicalPath: a Visualization tool to Improve the Evaluation of Electronic Health Records in Clinical Decision-Making (Q+A)",
        "contributors": [
            "Claudio Linhares"
        ],
        "authors": [
            "Claudio D. G. Linhares",
            "Daniel M. Lima",
            "Jean R. Ponciano",
            "Mauro M. Olivatto",
            "Marco A. Gutierrez",
            "Jorge Poco",
            "Caetano Traina Jr.",
            "Agma J. M. Traina"
        ],
        "abstract": "Physicians work at a very tight schedule and need decision-making support tools to help on improving and doing their work in a timely and dependable manner. Examining piles of sheets with test results and using systems with little visualization support to provide diagnostics is daunting, but that is still the usual way for the physicians' daily procedure, especially in developing countries. Electronic Health Records systems have been designed to keep the patients' history and reduce the time spent analyzing the patient's data. However, better tools to support decision-making are still needed. In this paper, we propose \\systemname, a visualization tool for users to track a patient's clinical path through a series of tests and data, which can aid in treatments and diagnoses. Our proposal is focused on patient's data analysis, presenting the test results and clinical history longitudinally. Both the visualization design and the system functionality were developed in close collaboration with experts in the medical domain to ensure a right fit of the technical solutions and the real needs of the professionals. We validated the proposed visualization based on case studies and user assessments through tasks based on the physician's daily activities. Our results show that our proposed system improves the physicians' experience in decision-making tasks, made with more confidence and better usage of the physicians' time, allowing them to take other needed care for the patients.",
        "uid": "v-tvcg-9779066",
        "file_name": "",
        "time_stamp": "2022-10-21T20:13:00Z",
        "time_start": "2022-10-21T20:13:00Z",
        "time_end": "2022-10-21T20:15:00Z"
    },
    "v-tvcg-9754243": {
        "slot_id": "v-tvcg-9754243-qa",
        "session_id": "full16",
        "type": "Virtual Q+A",
        "title": "Visual Assistance in Development and Validation of Bayesian Networks for Clinical Decision Support (Q+A)",
        "contributors": [
            "Juliane M\u00fcller-Sielaff,",
            "Juliane M\u00fcller-Sielaff"
        ],
        "authors": [
            "Juliane M\u00fcller-Sielaff",
            "Seyed Behnam Beladi",
            "Stephanie W. Vrede",
            "Monique Meuschke",
            "Peter J.F. Lucas",
            "Johanna M.A. Pijnenborg",
            "Steffen Oeltze-Jafra"
        ],
        "abstract": "The development and validation of Clinical Decision Support Models (CDSM) based on Bayesian networks (BN) is commonly done in a collaborative work between medical researchers providing the domain expertise and computer scientists developing the decision support model. Although modern tools provide facilities for data-driven model generation, domain experts are required to validate the accuracy of the learned model and to provide expert knowledge for fine-tuning it while computer scientists are needed to integrate this knowledge in the learned model (hybrid modeling approach). This generally time-expensive procedure hampers CDSM generation and updating. To address this problem, we developed a novel interactive visual approach allowing medical researchers with less knowledge in CDSM to develop and validate BNs based on domain specific data mainly independently and thus, diminishing the need for an additional computer scientist. In this context, we abstracted and simplified the common workflow in BN development as well as adjusted the workflow to medical experts' needs. We demonstrate our visual approach with data of endometrial cancer patients and evaluated it with six medical researchers who are domain experts in the gynecological field.",
        "uid": "v-tvcg-9754243",
        "file_name": "",
        "time_stamp": "2022-10-21T20:28:00Z",
        "time_start": "2022-10-21T20:28:00Z",
        "time_end": "2022-10-21T20:30:00Z"
    },
    "v-full-1228": {
        "slot_id": "v-full-1228-qa",
        "session_id": "full16",
        "type": "In Person Q+A",
        "title": "Chartwalk: Navigating large collections of text notes in electronic health records for clinical chart review (Q+A)",
        "contributors": [
            "Nicole Sultanum"
        ],
        "authors": [
            "Nicole Sultanum",
            "Farooq Naeem",
            "Michael Brudno",
            "Fanny Chevalier"
        ],
        "abstract": "",
        "uid": "v-full-1228",
        "file_name": "",
        "time_stamp": "2022-10-21T20:43:00Z",
        "time_start": "2022-10-21T20:43:00Z",
        "time_end": "2022-10-21T20:45:00Z"
    },
    "v-full-1111": {
        "slot_id": "v-full-1111-qa",
        "session_id": "full16",
        "type": "In Person Q+A",
        "title": "Development and Evaluation of Two Approaches of Visual Sensitivity Analysis to Support Epidemiological Modelling (Q+A)",
        "contributors": [
            "Erik Rydow"
        ],
        "authors": [
            "Erik Rydow",
            "Rita Borgo",
            "Hui Fang",
            "Thomas Torsney-Weir",
            "Ben Swallow",
            "Thibaud Porphyre",
            "Cagatay Turkay",
            "Min Chen"
        ],
        "abstract": "",
        "uid": "v-full-1111",
        "file_name": "",
        "time_stamp": "2022-10-21T20:58:00Z",
        "time_start": "2022-10-21T20:58:00Z",
        "time_end": "2022-10-21T21:00:00Z"
    },
    "v-tvcg-9721816": {
        "slot_id": "v-tvcg-9721816-qa",
        "session_id": "full16",
        "type": "Virtual Q+A",
        "title": "A framework for evaluating dashboards in healthcare (Q+A)",
        "contributors": [
            "Mengdie Zhuang"
        ],
        "authors": [
            "Mengdie Zhuang",
            "David Concannon",
            "Ed Manley"
        ],
        "abstract": "In the era of \u2018information overload\u2019, effective information provision is essential for enabling rapid response and critical decision making. In making sense of diverse information sources, dashboards have become an indispensable tool, providing fast, effective, adaptable, and personalized access to information for professionals and the general public alike. However, these objectives place heavy requirements on dashboards as information systems in usability and effective design. Understanding these issues is challenging given the absence of consistent and comprehensive approaches to dashboard evaluation. In this paper we systematically review literature on dashboard implementation in healthcare, where dashboards have been employed widely, and where there is widespread interest for improving the current state of the art, and subsequently analyse approaches taken towards evaluation. We draw upon consolidated dashboard literature and our own observations to introduce a general definition of dashboards which is more relevant to current trends, together with seven evaluation scenarios - task performance, behaviour change, interaction workflow,  perceived engagement, potential utility, algorithm performance and system implementation. These scenarios distinguish different evaluation purposes which we illustrate through measurements, example studies, and common challenges in evaluation study design. We provide a breakdown of each evaluation scenario, and highlight some of the more subtle questions. We demonstrate the use of the proposed framework by a design study guided by this framework. We conclude by comparing this framework with existing literature,  outlining a number of active discussion points and a set of dashboard evaluation best practices for the academic, clinical and software development communities alike.",
        "uid": "v-tvcg-9721816",
        "file_name": "",
        "time_stamp": "2022-10-21T21:13:00Z",
        "time_start": "2022-10-21T21:13:00Z",
        "time_end": "2022-10-21T21:15:00Z"
    },
    "v-full-1584": {
        "slot_id": "v-full-1584-qa",
        "session_id": "full16",
        "type": "In Person Q+A",
        "title": "Extending the Nested Model for User-Centric XAI: A Design Study on GNN-based Drug Repurposing (Q+A)",
        "contributors": [
            "Qianwen Wang"
        ],
        "authors": [
            "Qianwen Wang",
            "Kexin Huang",
            "Payal Chandak",
            "Marinka Zitnik",
            "Nils Gehlenborg"
        ],
        "abstract": "",
        "uid": "v-full-1584",
        "file_name": "",
        "time_stamp": "2022-10-21T21:28:00Z",
        "time_start": "2022-10-21T21:28:00Z",
        "time_end": "2022-10-21T21:30:00Z"
    },
    "v-tvcg-9495259": {
        "slot_id": "v-tvcg-9495259-qa",
        "session_id": "full17",
        "type": "Virtual Q+A",
        "title": "AI4VIS: Survey on Artificial Intelligence Approaches for Data Visualization (Q+A)",
        "contributors": [
            "Aoyu Wu"
        ],
        "authors": [
            "Aoyu Wu",
            "Yun Wang",
            "Xinhuan Shu",
            "Dominik Moritz",
            "Weiwei Cui",
            "Haidong Zhang",
            "Dongmei Zhang",
            "Huamin Qu"
        ],
        "abstract": "Visualizations themselves have become a data format. Akin to other data formats such as text and images, visualizations are increasingly created, stored, shared, and (re-)used with artificial intelligence (AI) techniques. In this survey, we probe the underlying vision of formalizing visualizations as an emerging data format and review the recent advance in applying AI techniques to visualization data (AI4VIS). We define visualization data as the digital representations of visualizations in computers and focus on data visualization (e.g., charts and infographics). We build our survey upon a corpus spanning ten different fields in computer science with an eye toward identifying important common interests. Our resulting taxonomy is organized around WHAT is visualization data and its representation, WHY and HOW to apply AI to visualization data. We highlight a set of common tasks that researchers apply to the visualization data and present a detailed discussion of AI approaches developed to accomplish those tasks. Drawing upon our literature review, we discuss several important research questions surrounding the management and exploitation of visualization data, as well as the role of AI in support of those processes.",
        "uid": "v-tvcg-9495259",
        "file_name": "",
        "time_stamp": "2022-10-20T20:13:00Z",
        "time_start": "2022-10-20T20:13:00Z",
        "time_end": "2022-10-20T20:15:00Z"
    },
    "v-tvcg-9760126": {
        "slot_id": "v-tvcg-9760126-qa",
        "session_id": "full17",
        "type": "In Person Q+A",
        "title": "DL4SciVis: A State-of-the-Art Survey on Deep Learning for Scientific Visualization (Q+A)",
        "contributors": [
            "Chaoli Wang"
        ],
        "authors": [
            "Chaoli Wang",
            "Jun Han"
        ],
        "abstract": "Since 2016, we have witnessed the tremendous growth of artificial intelligence+visualization (AI+VIS) research. However, existing survey papers on AI+VIS focus on visual analytics and information visualization, not scientific visualization (SciVis). In this paper, we survey related deep learning (DL) works in SciVis, specifically in the direction of DL4SciVis: designing DL solutions for solving SciVis problems. To stay focused, we primarily consider works that handle scalar and vector field data but exclude mesh data. We classify and discuss these works along six dimensions: domain setting, research task, learning type, network architecture, loss function, and evaluation metric. The paper concludes with a discussion of the remaining gaps to fill along the discussed dimensions and the grand challenges we need to tackle as a community. This state-of-the-art survey guides SciVis researchers in gaining an overview of this emerging topic and points out future directions to grow this research.",
        "uid": "v-tvcg-9760126",
        "file_name": "",
        "time_stamp": "2022-10-20T20:28:00Z",
        "time_start": "2022-10-20T20:28:00Z",
        "time_end": "2022-10-20T20:30:00Z"
    },
    "v-tvcg-9523770": {
        "slot_id": "v-tvcg-9523770-qa",
        "session_id": "full17",
        "type": "In Person Q+A",
        "title": "A Survey on ML4VIS: Applying Machine Learning Advances to Data Visualization (Q+A)",
        "contributors": [
            "Wang, Qianwen"
        ],
        "authors": [
            "Qianwen Wang",
            "Zhutian Chen",
            "Yong Wang",
            "Huamin Qu"
        ],
        "abstract": "Inspired by the great success of machine learning (ML), researchers have applied ML techniques to visualizations to achieve a better design, development, and evaluation of visualizations. This branch of studies, known as ML4VIS, is gaining increasing research attention in recent years. To successfully adapt ML techniques for visualizations, a structured understanding of the integration of ML4VISis needed. In this paper, we systematically survey ML4VIS studies, aiming to answer two motivating questions: \"what visualization processes can be assisted by ML?\" and \"how ML techniques can be used to solve visualization problems?\" This survey reveals seven main processes where the employment of ML techniques can benefit visualizations: Data Processing4VIS, Data-VIS Mapping, Insight Communication, Style Imitation, VIS Interaction, VIS Reading, and User Profiling. The seven processes are related to existing visualization theoretical models in an ML4VIS pipeline, aiming to illuminate the role of ML-assisted visualization in general visualizations. Meanwhile, the seven processes are mapped into main learning tasks in ML to align the capabilities of ML with the needs in visualization. Current practices and future opportunities of ML4VIS are discussed in the context of the ML4VIS pipeline and the ML-VIS mapping. While more studies are still needed in the area of ML4VIS, we hope this paper can provide a stepping-stone for future exploration. A web-based interactive browser of this survey is available at https://ml4vis.github.io",
        "uid": "v-tvcg-9523770",
        "file_name": "",
        "time_stamp": "2022-10-20T20:43:00Z",
        "time_start": "2022-10-20T20:43:00Z",
        "time_end": "2022-10-20T20:45:00Z"
    },
    "v-tvcg-9706326": {
        "slot_id": "v-tvcg-9706326-qa",
        "session_id": "full17",
        "type": "Virtual Q+A",
        "title": "Reinforcement Learning for Load-balanced Parallel Particle Tracing (Q+A)",
        "contributors": [
            "Jiayi Xu"
        ],
        "authors": [
            "Jiayi Xu",
            "Hanqi Guo",
            "Han-Wei Shen",
            "Mukund Raj",
            "Skylar W. Wurster",
            "Tom Peterka"
        ],
        "abstract": "We explore an online reinforcement learning (RL) paradigm to dynamically optimize parallel particle tracing performance in distributed-memory systems. Our method combines three novel components: (1) a work donation algorithm, (2) a high-order workload estimation model, and (3) a communication cost model. First, we design an RL-based work donation algorithm. Our algorithm monitors workloads of processes and creates RL agents to donate data blocks and particles from high-workload processes to low-workload processes to minimize program execution time. The agents learn the donation strategy on the fly based on reward and cost functions designed to consider processes' workload changes and data transfer costs of donation actions. Second, we propose a workload estimation model, helping RL agents estimate the workload distribution of processes in future computations. Third, we design a communication cost model that considers both block and particle data exchange costs, helping RL agents make effective decisions with minimized communication costs. We demonstrate that our algorithm adapts to different flow behaviors in large-scale fluid dynamics, ocean, and weather simulation data. Our algorithm improves parallel particle tracing performance in terms of parallel efficiency, load balance, and costs of I/O and communication for evaluations with up to 16,384 processors.",
        "uid": "v-tvcg-9706326",
        "file_name": "",
        "time_stamp": "2022-10-20T20:58:00Z",
        "time_start": "2022-10-20T20:58:00Z",
        "time_end": "2022-10-20T21:00:00Z"
    },
    "v-full-1018": {
        "slot_id": "v-full-1018-qa",
        "session_id": "full17",
        "type": "In Person Q+A",
        "title": "IDL: An Importance-Driven Latent Generation Method for Scientific Data (Q+A)",
        "contributors": [
            "JINGYI SHEN"
        ],
        "authors": [
            "JINGYI SHEN",
            "Haoyu Li",
            "Jiayi Xu",
            "Ayan Biswas",
            "Han-Wei Shen"
        ],
        "abstract": "",
        "uid": "v-full-1018",
        "file_name": "",
        "time_stamp": "2022-10-20T21:13:00Z",
        "time_start": "2022-10-20T21:13:00Z",
        "time_end": "2022-10-20T21:15:00Z"
    },
    "v-full-1033": {
        "slot_id": "v-full-1033-qa",
        "session_id": "full17",
        "type": "Virtual Q+A",
        "title": "DashBot: Insight-Driven Dashboard Generation Based on Deep Reinforcement Learning (Q+A)",
        "contributors": [
            "Dazhen Deng"
        ],
        "authors": [
            "Dazhen Deng",
            "Aoyu Wu",
            "Huamin Qu",
            "Yingcai Wu"
        ],
        "abstract": "",
        "uid": "v-full-1033",
        "file_name": "",
        "time_stamp": "2022-10-20T21:28:00Z",
        "time_start": "2022-10-20T21:28:00Z",
        "time_end": "2022-10-20T21:30:00Z"
    },
    "v-tvcg-9497654": {
        "slot_id": "v-tvcg-9497654-qa",
        "session_id": "full18",
        "type": "Virtual Q+A",
        "title": "Survey on Visual Analysis of Event Sequence Data (Q+A)",
        "contributors": [
            "Yi Guo"
        ],
        "authors": [
            "Yi Guo",
            "Shunan Guo",
            "Zhuochen Jin",
            "Smiti Kaul",
            "David Gotz",
            "Nan Cao"
        ],
        "abstract": "Event sequence data record series of discrete events in the time order of occurrence. They are commonly observed in a variety of applications ranging from electronic health records to network logs, with the characteristics of large-scale, high-dimensional and heterogeneous. This high complexity of event sequence data makes it difficult for analysts to manually explore and find patterns, resulting in ever-increasing needs for computational and perceptual aids from visual analytics techniques to extract and communicate insights from event sequence datasets. In this paper, we review the state-of-the-art visual analytics approaches, characterize them with our proposed design space, and categorize them based on analytical tasks and applications. From our review of relevant literature, we have also identified several remaining research challenges and future research opportunities.",
        "uid": "v-tvcg-9497654",
        "file_name": "",
        "time_stamp": "2022-10-19T20:13:00Z",
        "time_start": "2022-10-19T20:13:00Z",
        "time_end": "2022-10-19T20:15:00Z"
    },
    "v-tvcg-9664269": {
        "slot_id": "v-tvcg-9664269-qa",
        "session_id": "full18",
        "type": "Virtual Q+A",
        "title": "Towards Better Caption Supervision for Object Detection (Q+A)",
        "contributors": [
            "Shixia Liu"
        ],
        "authors": [
            "Changjian Chen",
            "Jing Wu",
            "Xiaohan Wang",
            "Shouxing Xiang",
            "Song-Hai Zhang",
            "Qifeng Tang",
            "Shixia Liu"
        ],
        "abstract": "As training high-performance object detectors requires expensive bounding box annotations, recent methods resort to free available image captions. However, detectors trained on caption supervision perform poorly because captions are usually noisy and cannot provide precise location information. To tackle this issue, we present a visual analysis method, which tightly integrates caption supervision with object detection to mutually enhance each other. In particular, object labels are first extracted from captions, which are utilized to train the detectors. Then, the label information from images is fed into caption supervision for further improvement. To effectively loop users into the object detection process, a node-link-based set visualization supported by a multi-type relational co-clustering algorithm is developed to explain the relationships between the extracted labels and the images with detected objects. The co-clustering algorithm clusters labels and images simultaneously by utilizing both their representations and their relationships. Quantitative evaluations and a case study are conducted to demonstrate the efficiency and effectiveness of the developed method in improving the performance of object detectors.",
        "uid": "v-tvcg-9664269",
        "file_name": "",
        "time_stamp": "2022-10-19T20:28:00Z",
        "time_start": "2022-10-19T20:28:00Z",
        "time_end": "2022-10-19T20:30:00Z"
    },
    "v-tvcg-9672706": {
        "slot_id": "v-tvcg-9672706-qa",
        "session_id": "full18",
        "type": "In Person Q+A",
        "title": "FeatureEnVi: Visual Analytics for Feature Engineering Using Stepwise Selection and Semi-Automatic Extraction Approaches (Q+A)",
        "contributors": [
            "Angelos Chatzimparmpas"
        ],
        "authors": [
            "Angelos Chatzimparmpas",
            "Rafael M. Martins",
            "Kostiantyn Kucher",
            "Andreas Kerren"
        ],
        "abstract": "The machine learning (ML) life cycle involves a series of iterative steps, from the effective gathering and preparation of the data\u2014including complex feature engineering processes\u2014to the presentation and improvement of results, with various algorithms to choose from in every step. Feature engineering in particular can be very beneficial for ML, leading to numerous improvements such as boosting the predictive results, decreasing computational times, reducing excessive noise, and increasing the transparency behind the decisions taken during the training. Despite that, while several visual analytics tools exist to monitor and control the different stages of the ML life cycle (especially those related to data and algorithms), feature engineering support remains inadequate. In this paper, we present FeatureEnVi, a visual analytics system specifically designed to assist with the feature engineering process. Our proposed system helps users to choose the most important feature, to transform the original features into powerful alternatives, and to experiment with different feature generation combinations. Additionally, data space slicing allows users to explore the impact of features on both local and global scales. FeatureEnVi utilizes multiple automatic feature selection techniques; furthermore, it visually guides users with statistical evidence about the influence of each feature (or subsets of features). The final outcome is the extraction of heavily engineered features, evaluated by multiple validation metrics. The usefulness and applicability of FeatureEnVi are demonstrated with two use cases and a case study. We also report feedback from interviews with two ML experts and a visualization researcher who assessed the effectiveness of our system.",
        "uid": "v-tvcg-9672706",
        "file_name": "",
        "time_stamp": "2022-10-19T20:43:00Z",
        "time_start": "2022-10-19T20:43:00Z",
        "time_end": "2022-10-19T20:45:00Z"
    },
    "v-full-1569": {
        "slot_id": "v-full-1569-qa",
        "session_id": "full18",
        "type": "In Person Q+A",
        "title": "A Design Space for Surfacing Content Recommendations in Visual Analytic Platforms (Q+A)",
        "contributors": [
            "David Gotz",
            "Zhilan Zhou"
        ],
        "authors": [
            "Zhilan Zhou",
            "Wenyuan Wang",
            "Mengtian Guo",
            "Yue Wang",
            "David Gotz"
        ],
        "abstract": "",
        "uid": "v-full-1569",
        "file_name": "",
        "time_stamp": "2022-10-19T20:58:00Z",
        "time_start": "2022-10-19T20:58:00Z",
        "time_end": "2022-10-19T21:00:00Z"
    },
    "v-full-1452": {
        "slot_id": "v-full-1452-qa",
        "session_id": "full18",
        "type": "In Person Q+A",
        "title": "Diverse Interaction Recommendation for Public Users Exploring Multi-view Visualization using Deep Learning (Q+A)",
        "contributors": [
            "Yixuan Li"
        ],
        "authors": [
            "Yixuan Li",
            "Yusheng Qi",
            "Yang Shi",
            "Qing Chen",
            "Nan Cao",
            "Siming Chen"
        ],
        "abstract": "",
        "uid": "v-full-1452",
        "file_name": "",
        "time_stamp": "2022-10-19T21:13:00Z",
        "time_start": "2022-10-19T21:13:00Z",
        "time_end": "2022-10-19T21:15:00Z"
    },
    "v-full-1674": {
        "slot_id": "v-full-1674-qa",
        "session_id": "full18",
        "type": "In Person Q+A",
        "title": "Visinity: Visual Spatial Neighborhood Analysis for Multiplexed Tissue Imaging Data (Q+A)",
        "contributors": [
            "Simon Alexander Warchol"
        ],
        "authors": [
            "Simon Alexander Warchol",
            "Robert Kr\u00fcger",
            "Ajit Johnson Nirmal",
            "Giorgio Gaglia",
            "Jared Jessup Jessup",
            "Cecily C. Ritch",
            "John Hoffer",
            "Jeremy Muhlich",
            "Megan L Burger",
            "Tyler Jacks",
            "Sandro Santagata Santagata",
            "Peter Sorger",
            "Hanspeter Pfister"
        ],
        "abstract": "",
        "uid": "v-full-1674",
        "file_name": "",
        "time_stamp": "2022-10-19T21:28:00Z",
        "time_start": "2022-10-19T21:28:00Z",
        "time_end": "2022-10-19T21:30:00Z"
    },
    "v-tvcg-9705076": {
        "slot_id": "v-tvcg-9705076-qa",
        "session_id": "full19",
        "type": "In Person Q+A",
        "title": "GNNLens: A Visual Analytics Approach for Prediction Error Diagnosis of Graph Neural Networks (Q+A)",
        "contributors": [
            "Zhihua JIN"
        ],
        "authors": [
            "Zhihua Jin",
            "Yong Wang",
            "Qianwen Wang",
            "Yao Ming",
            "Tengfei Ma",
            "Huamin Qu"
        ],
        "abstract": "Graph Neural Networks (GNNs) aim to extend deep learning techniques to graph data and have achieved significant progress in graph analysis tasks (e.g., node classification) in recent years. However, similar to other deep neural networks like Convolutional Neural Networks (CNNs) and Recurrent Neural Networks (RNNs), GNNs behave like a black box with their details hidden from model developers and users. It is therefore difficult to diagnose possible errors of GNNs. Despite many visual analytics studies being done on CNNs and RNNs, little research has addressed the challenges for GNNs. This paper fills the research gap with an interactive visual analysis tool, GNNLens, to assist model developers and users in understanding and analyzing GNNs. Specifically, Parallel Sets View and Projection View enable users to quickly identify and validate error patterns in the set of wrong predictions; Graph View and Feature Matrix View offer a detailed analysis of individual nodes to assist users in forming hypotheses about the error patterns. Since GNNs jointly model the graph structure and the node features, we reveal the relative influences of the two types of information by comparing the predictions of three models: GNN, Multi-Layer Perceptron (MLP), and GNN Without Using Features (GNNWUF). Two case studies and interviews with domain experts demonstrate the effectiveness of GNNLens in facilitating the understanding of GNN models and their errors.",
        "uid": "v-tvcg-9705076",
        "file_name": "",
        "time_stamp": "2022-10-19T21:58:00Z",
        "time_start": "2022-10-19T21:58:00Z",
        "time_end": "2022-10-19T22:00:00Z"
    },
    "v-full-1119": {
        "slot_id": "v-full-1119-qa",
        "session_id": "full19",
        "type": "Virtual Q+A",
        "title": "Visual Analysis of Neural Architecture Spaces for Summarizing Design Principles (Q+A)",
        "contributors": [
            "Jun Yuan"
        ],
        "authors": [
            "Jun Yuan",
            "Mengchen Liu",
            "Fengyuan Tian",
            "Shixia Liu"
        ],
        "abstract": "",
        "uid": "v-full-1119",
        "file_name": "",
        "time_stamp": "2022-10-19T22:13:00Z",
        "time_start": "2022-10-19T22:13:00Z",
        "time_end": "2022-10-19T22:15:00Z"
    },
    "v-full-1626": {
        "slot_id": "v-full-1626-qa",
        "session_id": "full19",
        "type": "In Person Q+A",
        "title": "NAS-Navigator: Visual Steering for Explainable One-Shot Deep Neural Network Synthesis (Q+A)",
        "contributors": [
            "Anjul Kumar Tyagi"
        ],
        "authors": [
            "Anjul Kumar Tyagi",
            "Cong Xie",
            "Klaus Mueller"
        ],
        "abstract": "",
        "uid": "v-full-1626",
        "file_name": "",
        "time_stamp": "2022-10-19T22:28:00Z",
        "time_start": "2022-10-19T22:28:00Z",
        "time_end": "2022-10-19T22:30:00Z"
    },
    "v-full-1338": {
        "slot_id": "v-full-1338-qa",
        "session_id": "full19",
        "type": "Virtual Q+A",
        "title": "HetVis: A Visual Analysis Approach for Identifying Heterogeneity in Federated Learning (Q+A)",
        "contributors": [
            "Xumeng Wang"
        ],
        "authors": [
            "Xumeng Wang",
            "Wei Chen",
            "Jiazhi Xia",
            "Zhen Wen",
            "Rongchen Zhu",
            "Tobias Schreck"
        ],
        "abstract": "",
        "uid": "v-full-1338",
        "file_name": "",
        "time_stamp": "2022-10-19T22:43:00Z",
        "time_start": "2022-10-19T22:43:00Z",
        "time_end": "2022-10-19T22:45:00Z"
    },
    "v-full-1205": {
        "slot_id": "v-full-1205-qa",
        "session_id": "full19",
        "type": "In Person Q+A",
        "title": "Visual Exploration of Large-Scale Image Datasets for Machine Learning with Treemaps (Q+A)",
        "contributors": [
            "Minsuk Kahng",
            "Donald Bertucci"
        ],
        "authors": [
            "Donald R Bertucci",
            "Md Montaser Hamid",
            "Yashwanthi Anand",
            "Anita Ruangrotsakun",
            "Delyar Tabatabai",
            "Melissa Perez",
            "Minsuk Kahng"
        ],
        "abstract": "",
        "uid": "v-full-1205",
        "file_name": "",
        "time_stamp": "2022-10-19T22:58:00Z",
        "time_start": "2022-10-19T22:58:00Z",
        "time_end": "2022-10-19T23:00:00Z"
    },
    "v-tvcg-9795241": {
        "slot_id": "v-tvcg-9795241-qa",
        "session_id": "full19",
        "type": "Virtual Q+A",
        "title": "Diagnosing Ensemble Few-Shot Classifiers (Q+A)",
        "contributors": [
            "Weikai Yang"
        ],
        "authors": [
            "Weikai Yang",
            "Xi Ye",
            "Xingxing Zhang",
            "Lanxi Xiao",
            "Jiazhi Xia",
            "Zhongyuan Wang",
            "Jun Zhu",
            "Hanspeter Pfister",
            "Shixia Liu"
        ],
        "abstract": "The base learners and labeled samples (shots) in an ensemble few-shot classifier greatly affect the model performance. When the performance is not satisfactory, it is usually difficult to understand the underlying causes and make improvements. To tackle this issue, we propose a visual analysis method, FSLDiagnotor. Given a set of base learners and a collection of samples with a few shots, we consider two problems: 1) finding a subset of base learners that well predict the sample collections; and 2) replacing the low-quality shots with more representative ones to adequately represent the sample collections. We formulate both problems as sparse subset selection and develop two selection algorithms to recommend appropriate learners and shots, respectively. A matrix visualization and a scatterplot are combined to explain the recommended learners and shots in context and facilitate users in adjusting them. Based on the adjustment, the algorithm updates the recommendation results for another round of improvement. Two case studies are conducted to demonstrate that FSLDiagnotor helps build a few-shot classifier efficiently and increases the accuracy by 12% and 21%, respectively.",
        "uid": "v-tvcg-9795241",
        "file_name": "",
        "time_stamp": "2022-10-19T23:13:00Z",
        "time_start": "2022-10-19T23:13:00Z",
        "time_end": "2022-10-19T23:15:00Z"
    },
    "v-tvcg-9492002": {
        "slot_id": "v-tvcg-9492002-qa",
        "session_id": "full20",
        "type": "Virtual Q+A",
        "title": "MV2Net: Multi-Variate Multi-View Brain Network Comparison over Uncertain Data (Q+A)",
        "contributors": [
            "Lei Shi"
        ],
        "authors": [
            "Lei Shi",
            "Junnan Hu",
            "Zhihao Tan",
            "Jun Tao",
            "Jiayan Ding",
            "Yan Jin",
            "Yanjun Wu",
            "Paul M. Thompson"
        ],
        "abstract": "Visually identifying effective bio-markers from human brain networks poses non-trivial challenges to the field of data visualization and analysis. Existing methods in the literature and neuroscience practice are generally limited to the study of individual connectivity features in the brain (e.g., the strength of neural connection among brain regions). Pairwise comparisons between contrasting subject groups (e.g., the diseased and the healthy controls) are normally performed. The underlying neuroimaging and brain network construction process is assumed to have 100% fidelity. Yet, real-world user requirements on brain network visual comparison lean against these assumptions. In this work, we present MV^2Net, a visual analytics system that tightly integrates multi-variate multi-view visualization for brain network comparison with an interactive wrangling mechanism to deal with data uncertainty. On the analysis side, the system integrates multiple extraction methods on diffusion and geometric connectivity features of brain networks, an anomaly detection algorithm for data quality assessment, single- and multi-connection feature selection methods for bio-marker detection. On the visualization side, novel designs are introduced which optimize network comparisons among contrasting subject groups and related connectivity features. Our design provides level-of-detail comparisons, from juxtaposed and explicit-coding views for subject group comparisons, to high-order composite view for correlation of network comparisons, and to fiber tract detail view for voxel-level comparisons. The proposed techniques are inspired and evaluated in expert studies, as well as through case analyses on diffusion and geometric bio-markers of certain neurology diseases. Results in these experiments demonstrate the effectiveness and superiority of MV^2Net over state-of-the-art approaches.",
        "uid": "v-tvcg-9492002",
        "file_name": "",
        "time_stamp": "2022-10-20T20:10:00Z",
        "time_start": "2022-10-20T20:10:00Z",
        "time_end": "2022-10-20T20:12:00Z"
    },
    "v-tvcg-9529035": {
        "slot_id": "v-tvcg-9529035-qa",
        "session_id": "full20",
        "type": "Virtual Q+A",
        "title": "NeuroConstruct: 3D Reconstruction and Visualization of Neurites in Optical Microscopy Brain Images (Q+A)",
        "contributors": [
            "Parmida Ghahremani"
        ],
        "authors": [
            "Parmida Ghahremani",
            "Saeed Boorboor",
            "Pooya Mirhosseini",
            "Chetan Gudisagar",
            "Mala Ananth",
            "David Talmage",
            "Lorna W. Role",
            "Arie E. Kaufman"
        ],
        "abstract": "We introduce NeuroConstruct, a novel end-to-end application for the segmentation, registration, and visualization of brain volumes imaged using wide-field microscopy. NeuroConstruct offers a Segmentation Toolbox with various annotation helper functions that aid experts to effectively and precisely annotate micrometer resolution neurites. It also offers an automatic neurites segmentation using convolutional neuronal networks (CNN) trained by the Toolbox annotations and somas segmentation using thresholding. To visualize neurites in a given volume, NeuroConstruct offers a hybrid rendering by combining iso-surface rendering of high-confidence classified neurites, along with real-time rendering of raw volume using a 2D transfer function for voxel classification score vs. voxel intensity value. For a complete reconstruction of the 3D neurites, we introduce a Registration Toolbox that provides automatic coarse-to-fine alignment of serially sectioned samples. The quantitative and qualitative analysis show that NeuroConstruct outperforms the state-of-the-art in all design aspects. NeuroConstruct was developed as a collaboration between computer scientists and neuroscientists, with an application to the study of cholinergic neurons, which are severely affected in Alzheimer\u2019s disease.",
        "uid": "v-tvcg-9529035",
        "file_name": "",
        "time_stamp": "2022-10-20T20:22:00Z",
        "time_start": "2022-10-20T20:22:00Z",
        "time_end": "2022-10-20T20:24:00Z"
    },
    "v-tvcg-9555234": {
        "slot_id": "v-tvcg-9555234-qa",
        "session_id": "full20",
        "type": "In Person Q+A",
        "title": "DXplorer: A Unified Visualization Framework for Interactive Dendritic Spine Analysis using 3D morphological Features (Q+A)",
        "contributors": [
            "Won-Ki Jeong",
            "JunYoung Choi"
        ],
        "authors": [
            "JunYoung Choi",
            "Sang-Eun Lee",
            "YeIn Lee",
            "Eunji Cho",
            "Sunghoe Chang",
            "Won-Ki Jeong"
        ],
        "abstract": "Dendritic spines are dynamic, submicron-scale protrusions on neuronal dendrites that receive neuronal inputs. Morphological changes in the dendritic spine often reflect alterations in physiological conditions and are indicators of various neuropsychiatric conditions. However, owing to the highly dynamic and heterogeneous nature of spines, accurate measurement and objective analysis of spine morphology are major challenges in neuroscience research. Most conventional approaches for analyzing dendritic spines are based on two-dimensional (2D) images, which barely reflect the actual three-dimensional (3D) shapes. Although some recent studies have attempted to analyze spines with various 3D-based features, it is still difficult to objectively categorize and analyze spines based on 3D morphology. Here, we propose a unified visualization framework for an interactive 3D dendritic spine analysis system, DXplorer, that displays 3D rendering of spines and plots the high-dimensional features extracted from the 3D mesh of spines. With this system, users can perform the clustering of spines interactively and explore and analyze dendritic spines based on high-dimensional features. We propose a series of high-dimensional morphological features extracted from a 3D mesh of dendritic spines. In addition, an interactive machine learning classifier with visual exploration and user feedback using an interactive 3D mesh grid view ensures a more precise classification based on the spine phenotype. A user study and two case studies were conducted to quantitatively verify the performance and usability of the DXplorer. We demonstrate that the system performs the entire analytic process effectively and provides high-quality, accurate, and objective analysis.",
        "uid": "v-tvcg-9555234",
        "file_name": "",
        "time_stamp": "2022-10-20T20:34:00Z",
        "time_start": "2022-10-20T20:34:00Z",
        "time_end": "2022-10-20T20:36:00Z"
    },
    "v-tvcg-9665344": {
        "slot_id": "v-tvcg-9665344-qa",
        "session_id": "full20",
        "type": "Virtual Q+A",
        "title": "A Predictive Visual Analytics System for Studying Neurodegenerative Disease Based on DTI Fiber Tracts (Q+A)",
        "contributors": [
            "Chaoqing Xu"
        ],
        "authors": [
            "Chaoqing Xu",
            "Tyson Neuroth",
            "Takanori Fujiwara",
            "Ronghua Liang",
            "Kwan-Liu Ma"
        ],
        "abstract": "Diffusion tensor imaging (DTI) has been used to study the effects of neurodegenerative diseases on neural pathways, which may lead to more reliable and early diagnosis of these diseases as well as a better understanding of how they affect the brain. We introduce a predictive visual analytics system for studying patient groups based on their labeled DTI fiber tract data and corresponding statistics. The system\u2019s machine-learning-augmented interface guides the user through an organized and holistic analysis space, including the statistical feature space, the physical space, and the space of patients over different groups. We use a custom machine learning pipeline to help narrow down this large analysis space and then explore it pragmatically through a range of linked visualizations. We conduct several case studies using DTI and T1-weighted images from the research database of Parkinson\u2019s Progression Markers Initiative.",
        "uid": "v-tvcg-9665344",
        "file_name": "",
        "time_stamp": "2022-10-20T20:46:00Z",
        "time_start": "2022-10-20T20:46:00Z",
        "time_end": "2022-10-20T20:48:00Z"
    },
    "v-tvcg-9610985": {
        "slot_id": "v-tvcg-9610985-qa",
        "session_id": "full20",
        "type": "In Person Q+A",
        "title": "NeuRegenerate: A Framework for Visualizing Neurodegeneration (Q+A)",
        "contributors": [
            "Saeed Boorboor"
        ],
        "authors": [
            "Saeed Boorboor",
            "Shawn Mathew",
            "Mala Ananth",
            "David Talmage",
            "Lorna W. Role",
            "Arie E. Kaufman."
        ],
        "abstract": "Recent advances in high-resolution microscopy have allowed scientists to better understand the underlying brain connectivity. However, due to the limitation that biological specimens can only be imaged at a single timepoint, studying changes to neural projections over time is limited to observations gathered using population analysis. In this paper, we introduce NeuRegenerate, a novel end-to-end framework for the prediction and visualization of changes in neural fiber morphology within a subject across specified age-timepoints. To predict projections, we present  neuReGANerator, a deep-learning network based on cycle-consistent generative adversarial network (GAN) that translates features of neuronal structures across age-timepoints for large brain microscopy volumes. We improve the reconstruction quality of the predicted neuronal structures by implementing a density multiplier and a new loss function, called the hallucination loss. Moreover, to alleviate artifacts that occur due to tiling of large input volumes, we introduce a spatial-consistency module in the training pipeline of neuReGANerator. Finally, to visualize the change in projections, predicted using neuReGANerator, NeuRegenerate offers two modes: (i) neuroCompare to simultaneously visualize the difference in the structures of the neuronal projections, from two age domains (using structural view and bounded view), and (ii) em neuroMorph, a vesselness-based morphing technique to interactively visualize the transformation of the structures from one age-timepoint to the other. Our framework is designed specifically for volumes acquired using wide-field microscopy. We demonstrate our framework by visualizing the structural changes within the cholinergic system of the mouse brain between a young and old specimen.",
        "uid": "v-tvcg-9610985",
        "file_name": "",
        "time_stamp": "2022-10-20T20:58:00Z",
        "time_start": "2022-10-20T20:58:00Z",
        "time_end": "2022-10-20T21:00:00Z"
    },
    "v-tvcg-9645173": {
        "slot_id": "v-tvcg-9645173-qa",
        "session_id": "full20",
        "type": "Virtual Q+A",
        "title": "GUCCI - Guided Cardiac Cohort Investigation of Blood Flow Data (Q+A)",
        "contributors": [
            "Monique Meuschke"
        ],
        "authors": [
            "Monique Meuschke",
            "Uli Niemann",
            "Benjamin Behrendt",
            "Matthias Gutberlet",
            "Bernhard Preim",
            "Kai Lawonn"
        ],
        "abstract": "We present the framework GUCCI (Guided Cardiac Cohort Investigation), which provides a guided visual analytics workflow to analyze cohort-based measured blood flow data in the aorta. In the past, many specialized techniques have been developed for the visual exploration of such data sets for a better understanding of the influence of morphological and hemodynamic conditions on cardiovascular diseases. However, there is a lack of dedicated techniques that allow visual comparison of multiple datasets and defined cohorts, which is essential to characterize pathologies. GUCCI offers visual analytics techniques and novel visualization methods to guide the user through the comparison of predefined cohorts, such as healthy volunteers and patients with a pathologically altered aorta.  The combination of overview and glyph-based depictions together with statistical cohort-specific information allows investigating differences and similarities of the time-dependent data. Our framework was evaluated in a qualitative user study with three radiologists specialized in cardiac imaging and two experts in medical blood flow visualization. They were able to discover cohort-specific characteristics, which supports the derivation of standard values as well as the assessment of pathology-related severity and the need for treatment.",
        "uid": "v-tvcg-9645173",
        "file_name": "",
        "time_stamp": "2022-10-20T21:10:00Z",
        "time_start": "2022-10-20T21:10:00Z",
        "time_end": "2022-10-20T21:12:00Z"
    },
    "v-full-1380": {
        "slot_id": "v-full-1380-qa",
        "session_id": "full21",
        "type": "Virtual Q+A",
        "title": "Visualizing the Passage of Time with Video Temporal Pyramids (Q+A)",
        "contributors": [
            "Melissa E Swift",
            "Melissa Swift"
        ],
        "authors": [
            "Melissa E Swift",
            "Wyatt Ayers",
            "Sophie Pallanck",
            "Scott Wehrwein"
        ],
        "abstract": "",
        "uid": "v-full-1380",
        "file_name": "",
        "time_stamp": "2022-10-19T20:10:00Z",
        "time_start": "2022-10-19T20:10:00Z",
        "time_end": "2022-10-19T20:12:00Z"
    },
    "v-full-1289": {
        "slot_id": "v-full-1289-qa",
        "session_id": "full21",
        "type": "In Person Q+A",
        "title": "Constrained Dynamic Mode Decomposition (Q+A)",
        "contributors": [
            "Tim Krake"
        ],
        "authors": [
            "Tim Krake",
            "Daniel Kl\u00f6tzl",
            "Bernhard Eberhardt",
            "Daniel Weiskopf"
        ],
        "abstract": "",
        "uid": "v-full-1289",
        "file_name": "",
        "time_stamp": "2022-10-19T20:22:00Z",
        "time_start": "2022-10-19T20:22:00Z",
        "time_end": "2022-10-19T20:24:00Z"
    },
    "v-full-1163": {
        "slot_id": "v-full-1163-qa",
        "session_id": "full21",
        "type": "Virtual Q+A",
        "title": "SizePairs: Achieving Stable and Balanced Temporal Treemaps using Hierarchical Size-based Paring (Q+A)",
        "contributors": [
            "Yunhai Wang",
            "Chang Han"
        ],
        "authors": [
            "Chang Han",
            "Anyi Li",
            "Jaemin Jo",
            "Bongshin Lee",
            "Oliver Deussen",
            "Yunhai Wang"
        ],
        "abstract": "",
        "uid": "v-full-1163",
        "file_name": "",
        "time_stamp": "2022-10-19T20:34:00Z",
        "time_start": "2022-10-19T20:34:00Z",
        "time_end": "2022-10-19T20:36:00Z"
    },
    "v-full-1230": {
        "slot_id": "v-full-1230-qa",
        "session_id": "full21",
        "type": "In Person Q+A",
        "title": "LargeNetVis: visual exploration of large temporal networks based on community taxonomies (Q+A)",
        "contributors": [
            "Claudio Linhares",
            "Jean Ponciano"
        ],
        "authors": [
            "Claudio Linhares",
            "Jean Roberto Ponciano",
            "Diogenes Pedro",
            "Luis Rocha",
            "Agma Traina",
            "Jorge Poco"
        ],
        "abstract": "",
        "uid": "v-full-1230",
        "file_name": "",
        "time_stamp": "2022-10-19T20:46:00Z",
        "time_start": "2022-10-19T20:46:00Z",
        "time_end": "2022-10-19T20:48:00Z"
    },
    "v-full-1069": {
        "slot_id": "v-full-1069-qa",
        "session_id": "full21",
        "type": "Virtual Q+A",
        "title": "ASTF: Visual Abstractions of Time-Varying Patterns in Radio Signals (Q+A)",
        "contributors": [
            "Ying Zhao"
        ],
        "authors": [
            "Ying Zhao",
            "Luhao Ge",
            "Huixuan Xie",
            "Genghuai Bai",
            "Zhao Zhang",
            "Qiang Wei",
            "Yun Lin",
            "Yuchao Liu",
            "Fangfang Zhou"
        ],
        "abstract": "",
        "uid": "v-full-1069",
        "file_name": "",
        "time_stamp": "2022-10-19T20:58:00Z",
        "time_start": "2022-10-19T20:58:00Z",
        "time_end": "2022-10-19T21:00:00Z"
    },
    "v-full-1544": {
        "slot_id": "v-full-1544-qa",
        "session_id": "full21",
        "type": "Virtual Q+A",
        "title": "RankFIRST: Visual Analysis for Factor Investment by Ranking Stock Timeseries (Q+A)",
        "contributors": [
            "Lei Shi",
            "Huijie Guo"
        ],
        "authors": [
            "Huijie Guo",
            "Meijun Liu",
            "Bowen Yang",
            "Ye Sun",
            "Lei Shi",
            "Huamin Qu"
        ],
        "abstract": "",
        "uid": "v-full-1544",
        "file_name": "",
        "time_stamp": "2022-10-19T21:10:00Z",
        "time_start": "2022-10-19T21:10:00Z",
        "time_end": "2022-10-19T21:12:00Z"
    },
    "v-tvcg-9744472": {
        "slot_id": "v-tvcg-9744472-qa",
        "session_id": "full22",
        "type": "In Person Q+A",
        "title": "Geometry-Aware Merge Tree Comparisons for Time-Varying Data with Interleaving Distances (Q+A)",
        "contributors": [
            "Lin Yan"
        ],
        "authors": [
            "Lin Yan",
            "Talha Bin Masood",
            "Farhan Rasheed",
            "Ingrid Hotz",
            "Bei Wang"
        ],
        "abstract": "Merge trees, a type of topological descriptor, serve to identify and summarize the topological characteristics associated with scalar fields. They have great potential for analyzing and visualizing time-varying data. First, they give compressed and topology-preserving representations of data instances. Second, their comparisons provide a basis for studying the relations among data instances, such as their distributions, clusters, outliers, and periodicities. A number of comparative measures have been developed for merge trees. However, these measures are often computationally expensive since they implicitly consider all possible correspondences between critical points of the merge trees. In this paper, we perform geometry-aware comparisons of merge trees using labeled interleaving distances. The main idea is to decouple the computation of a comparative measure into two steps: a labeling step that generates a correspondence between the critical points of two merge trees, and a comparison step that computes distances between a pair of labeled merge trees by encoding them as matrices. We show that our approach is general, computationally efficient, and practically useful. Our framework makes it possible to integrate geometric information of the data domain in the labeling process. At the same time, the framework reduces the computational complexity since not all possible correspondences have to be considered. We demonstrate via experiments that such geometry-aware merge tree comparisons help to detect transitions, clusters, and periodicities of time-varying datasets, as well as to diagnose and highlight the topological changes between adjacent data instances.",
        "uid": "v-tvcg-9744472",
        "file_name": "",
        "time_stamp": "2022-10-21T20:10:00Z",
        "time_start": "2022-10-21T20:10:00Z",
        "time_end": "2022-10-21T20:12:00Z"
    },
    "v-tvcg-9468958": {
        "slot_id": "v-tvcg-9468958-qa",
        "session_id": "full22",
        "type": "Virtual Q+A",
        "title": "Interpretable Anomaly Detection in Event Sequences via Sequence Matching and Visual Comparison (Q+A)",
        "contributors": [
            "Shunan Guo"
        ],
        "authors": [
            "Shunan Guo",
            "Zhuochen Jin",
            "Qing Chen",
            "David Gotz",
            "Hongyuan Zha",
            "Nan Cao"
        ],
        "abstract": "Anomaly detection is a common analytical task that aims to identify rare cases that differ from the typical cases that make up the majority of a dataset. When analyzing event sequence data, the task of anomaly detection can be complex because the sequential and temporal nature of such data results in diverse definitions and flexible forms of anomalies. This, in turn, increases the difficulty in interpreting detected anomalies. In this paper, we propose a visual analytic approach for detecting anomalous sequences in an event sequence dataset via an unsupervised anomaly detection algorithm based on Variational AutoEncoders. We further compare the anomalous sequences with their reconstructions and with the normal sequences through a sequence matching algorithm to identify event anomalies. A visual analytics system is developed to support interactive exploration and interpretations of anomalies through novel visualization designs that facilitate the comparison between anomalous sequences and normal sequences. Finally, we quantitatively evaluate the performance of our anomaly detection algorithm, demonstrate the effectiveness of our system through case studies, and report feedback collected from study participants.",
        "uid": "v-tvcg-9468958",
        "file_name": "",
        "time_stamp": "2022-10-21T20:22:00Z",
        "time_start": "2022-10-21T20:22:00Z",
        "time_end": "2022-10-21T20:24:00Z"
    },
    "v-tvcg-9585392": {
        "slot_id": "v-tvcg-9585392-qa",
        "session_id": "full22",
        "type": "In Person Q+A",
        "title": "Comparative Analysis of Merge Trees using Local Tree Edit Distance (Q+A)",
        "contributors": [
            "Vijay Natarajan",
            "Raghavendra Sridharamurthy"
        ],
        "authors": [
            "Raghavendra Sridharamurthy",
            "Vijay Natarajan"
        ],
        "abstract": "Comparative analysis of scalar fields is an important problem with various applications including feature-directed visualization and feature tracking in time-varying data. Comparing topological structures that are abstract and succinct representations of the scalar fields lead to faster and meaningful comparison. While there are many distance or similarity measures to compare topological structures in a global context, there are no known measures for comparing topological structures locally. While the global measures have many applications, they do not directly lend themselves to fine-grained analysis across multiple scales. We define a local variant of the tree edit distance and apply it towards local comparative analysis of merge trees with support for finer analysis. We also present experimental results on time-varying scalar fields, 3D cryo-electron microscopy data, and other synthetic data sets to show the utility of this approach in applications like symmetry detection and feature tracking.",
        "uid": "v-tvcg-9585392",
        "file_name": "",
        "time_stamp": "2022-10-21T20:34:00Z",
        "time_start": "2022-10-21T20:34:00Z",
        "time_end": "2022-10-21T20:36:00Z"
    },
    "v-tvcg-9729550": {
        "slot_id": "v-tvcg-9729550-qa",
        "session_id": "full22",
        "type": "In Person Q+A",
        "title": "Visual Exploration of Relationships and Structure in Low-Dimensional Embeddings (Q+A)",
        "contributors": [
            "Klaus Eckelt"
        ],
        "authors": [
            "Klaus Eckelt",
            "Andreas Hinterreiter",
            "Patrick Adelberger",
            "Conny Walchshofer",
            "Vaishali Dhanoa",
            "Christina Humer",
            "Moritz Heckmann",
            "Christian A. Steinparz",
            "Marc Streit"
        ],
        "abstract": "In this work, we propose an interactive visual approach for the exploration and formation of structural relationships in embeddings of high-dimensional data. These structural relationships, such as item sequences, associations of items with groups, and hierarchies between groups of items, are defining properties of many real-world datasets. Nevertheless, most existing methods for the visual exploration of embeddings treat these structures as second-class citizens or do not take them into account at all. In our proposed analysis workflow, users explore enriched scatterplots of the embedding, in which relationships between items and\\slash or groups are visually highlighted. The original high-dimensional data for single items, groups of items, or differences between connected items and groups are accessible through additional summary visualizations. We carefully tailored these summary and difference visualizations to the various data types and semantic contexts. During their exploratory analysis, users can externalize their insights by setting up additional groups and relationships between items and\\slash or groups. We demonstrate the utility and potential impact of our approach by means of two use cases and multiple examples from various domains.",
        "uid": "v-tvcg-9729550",
        "file_name": "",
        "time_stamp": "2022-10-21T20:46:00Z",
        "time_start": "2022-10-21T20:46:00Z",
        "time_end": "2022-10-21T20:48:00Z"
    },
    "v-tvcg-9716867": {
        "slot_id": "v-tvcg-9716867-qa",
        "session_id": "full22",
        "type": "Virtual Q+A",
        "title": "View Composition Algebra for Ad Hoc Comparison (Q+A)",
        "contributors": [
            "Eugene Wu"
        ],
        "authors": [
            "Eugene Wu"
        ],
        "abstract": "Comparison is a core task in visual analysis.  Although there are numerous guidelines to help users design effective visualizations to aid known comparison tasks, there are few techniques available when users want to make ad hoc comparisons between marks, trends, or charts during data exploration and visual analysis.  For instance, to compare voting count maps from different years, two stock trends in a line chart, or a scatterplot of country GDPs with a textual summary of the average GDP.  Ideally, users can directly select the comparison targets and compare them, however what elements of a visualization should be candidate targets, which combinations of targets are safe to compare, and what comparison operations make sense?  This paper proposes a conceptual model that lets users compose combinations of values, marks, legend elements, and charts using a set of composition operators that summarize, compute differences, merge, and model their operands.  We further define a View Composition Algebra (VCA) that is compatible with datacube-based visualizations, derive an interaction design based on this algebra that supports ad hoc visual comparisons, and illustrate its utility through several use cases.",
        "uid": "v-tvcg-9716867",
        "file_name": "",
        "time_stamp": "2022-10-21T20:58:00Z",
        "time_start": "2022-10-21T20:58:00Z",
        "time_end": "2022-10-21T21:00:00Z"
    },
    "v-full-1607": {
        "slot_id": "v-full-1607-qa",
        "session_id": "full22",
        "type": "Virtual Q+A",
        "title": "Visual Comparison of Language Model Adaptation (Q+A)",
        "contributors": [
            "Rita Sevastjanova"
        ],
        "authors": [
            "Rita Sevastjanova",
            "Eren Cakmak",
            "Shauli Ravfogel",
            "Ryan Cotterell",
            "Mennatallah El-Assady"
        ],
        "abstract": "",
        "uid": "v-full-1607",
        "file_name": "",
        "time_stamp": "2022-10-21T21:10:00Z",
        "time_start": "2022-10-21T21:10:00Z",
        "time_end": "2022-10-21T21:12:00Z"
    },
    "v-full-1051": {
        "slot_id": "v-full-1051-qa",
        "session_id": "full23",
        "type": "In Person Q+A",
        "title": "Temporal Merge Tree Maps: A Topology-Based Static Visualization for Temporal Scalar Data (Q+A)",
        "contributors": [
            "Wiebke K\u00f6pp"
        ],
        "authors": [
            "Wiebke K\u00f6pp",
            "Tino Weinkauf"
        ],
        "abstract": "",
        "uid": "v-full-1051",
        "file_name": "",
        "time_stamp": "2022-10-21T02:55:00Z",
        "time_start": "2022-10-21T02:55:00Z",
        "time_end": "2022-10-21T02:57:00Z"
    },
    "v-tvcg-9583888": {
        "slot_id": "v-tvcg-9583888-qa",
        "session_id": "full23",
        "type": "In Person Q+A",
        "title": "TopoCluster: A Localized Data Structure for Topology-based Visualization (Q+A)",
        "contributors": [
            "Guoxi Liu"
        ],
        "authors": [
            "Guoxi Liu",
            "Federico Iuricich",
            "Riccardo Fellegara",
            "Leila De Floriani"
        ],
        "abstract": "Unstructured data are collections of points with irregular topology, often represented through simplicial meshes, such as triangle and tetrahedral meshes. Whenever possible such representations are avoided in visualization since they are computationally demanding if compared with regular grids. In this work, we aim at simplifying the encoding and processing of simplicial meshes. The paper proposes TopoCluster, a new localized data structure for tetrahedral meshes. TopoCluster provides efficient computation of the connectivity of the mesh elements with a low memory footprint. The key idea of TopoCluster is to subdivide the simplicial mesh into clusters. Then, the connectivity information is computed locally for each cluster and discarded when it is no longer needed. We define two instances of TopoCluster. The first instance prioritizes time efficiency and provides only modest savings in memory, while the second instance drastically reduces memory consumption up to an order of magnitude with respect to comparable data structures. Thanks to the simple interface provided by TopoCluster, we have been able to integrate both data structures into the existing Topological Toolkit (TTK) framework. As a result, users can run any plugin of TTK using TopoCluster without changing a single line of code.",
        "uid": "v-tvcg-9583888",
        "file_name": "",
        "time_stamp": "2022-10-21T03:07:00Z",
        "time_start": "2022-10-21T03:07:00Z",
        "time_end": "2022-10-21T03:09:00Z"
    },
    "v-full-1233": {
        "slot_id": "v-full-1233-qa",
        "session_id": "full23",
        "type": "Virtual Q+A",
        "title": "Computing a Stable Distance on Merge Trees (Q+A)",
        "contributors": [
            "Brian C Bollen"
        ],
        "authors": [
            "Brian C Bollen",
            "Joshua A Levine",
            "Pasindu P. Tennakoon"
        ],
        "abstract": "",
        "uid": "v-full-1233",
        "file_name": "",
        "time_stamp": "2022-10-21T03:19:00Z",
        "time_start": "2022-10-21T03:19:00Z",
        "time_end": "2022-10-21T03:21:00Z"
    },
    "v-tvcg-9721603": {
        "slot_id": "v-tvcg-9721603-qa",
        "session_id": "full23",
        "type": "In Person Q+A",
        "title": "Topological Simplifications of Hypergraphs (Q+A)",
        "contributors": [
            "Youjia Zhou"
        ],
        "authors": [
            "Youjia Zhou",
            "Archit Rathore",
            "Emilie Purvine",
            "Bei Wang"
        ],
        "abstract": "We study hypergraph visualization via its topological simplification. We explore both vertex simplification and hyperedge simplification of hypergraphs using tools from topological data analysis. In particular, we transform a hypergraph into its graph representations known as the line graph and clique expansion. A topological simplification of such a graph representation induces a simplification of the hypergraph. In simplifying a hypergraph, we allow vertices to be combined if they belong to almost the same set of hyperedges, and hyperedges to be merged if they share almost the same set of vertices. Our proposed approaches are general, mathematically justifiable, and put vertex simplification and hyperedge simplification in a unifying framework.",
        "uid": "v-tvcg-9721603",
        "file_name": "",
        "time_stamp": "2022-10-21T03:31:00Z",
        "time_start": "2022-10-21T03:31:00Z",
        "time_end": "2022-10-21T03:33:00Z"
    },
    "v-tvcg-9531544": {
        "slot_id": "v-tvcg-9531544-qa",
        "session_id": "full23",
        "type": "In Person Q+A",
        "title": "Persistence cycles for visual exploration of persistent homology (Q+A)",
        "contributors": [
            "Federico Iuricich"
        ],
        "authors": [
            "Federico Iuricich"
        ],
        "abstract": "Persistent homology is a fundamental tool in topological data analysis used for the most diverse applications. Information captured by persistent homology is commonly visualized using scatter plots representations. Despite being widely adopted, such a visualization technique limits user understanding and is prone to misinterpretation. This paper proposes a new approach for the efficient computation of persistence cycles, a geometric representation of the features captured by persistent homology. We illustrate the importance of rendering persistence cycles when analyzing scalar fields, and we discuss the advantages that our approach provides compared to other techniques in topology-based visualization. We provide an efficient implementation of our approach based on discrete Morse theory, as a new module for the Topology Toolkit. We show that our implementation has comparable performance with respect to state-of-the-art toolboxes while providing a better framework for visually analyzing persistent homology information.",
        "uid": "v-tvcg-9531544",
        "file_name": "",
        "time_stamp": "2022-10-21T03:43:00Z",
        "time_start": "2022-10-21T03:43:00Z",
        "time_end": "2022-10-21T03:45:00Z"
    },
    "v-tvcg-9721643": {
        "slot_id": "v-tvcg-9721643-qa",
        "session_id": "full23",
        "type": "Virtual Q+A",
        "title": "Geometry-Aware Planar Embedding of Treelike Structures (Q+A)",
        "contributors": [
            "Ping Hu"
        ],
        "authors": [
            "Ping Hu",
            "Saeed Boorboor",
            "Joseph Marino",
            "Arie E. Kaufman"
        ],
        "abstract": "The growing complexity of spatial and structural information in 3D data makes data inspection and visualization a challenging task. We describe a method to create a planar embedding of 3D treelike structures using their skeleton representations. Our method maintains the original geometry, without overlaps, to the best extent possible, allowing exploration of the topology within a single view. We present a novel camera view generation method which maximizes the visible geometric attributes (segment shape and relative placement between segments). Camera views are created for individual segments and are used to determine local bending angles at each node by projecting them to 2D. The final embedding is generated by minimizing an energy function (the weights of which are user adjustable) based on branch length and the 2D angles, while avoiding intersections. The user can also interactively modify segment placement within the 2D embedding, and the overall embedding will update accordingly. A global to local interactive exploration is provided using hierarchical camera views that are created for subtrees within the structure. We evaluate our method both qualitatively and quantitatively and demonstrate our results by constructing planar visualizations of line data (traced neurons) and volume data (CT vascular and bronchial data).",
        "uid": "v-tvcg-9721643",
        "file_name": "",
        "time_stamp": "2022-10-21T03:55:00Z",
        "time_start": "2022-10-21T03:55:00Z",
        "time_end": "2022-10-21T03:57:00Z"
    },
    "v-full-1551": {
        "slot_id": "v-full-1551-qa",
        "session_id": "full24",
        "type": "In Person Q+A",
        "title": "MosaicSets: Embedding Set Systems into Grid Graphs (Q+A)",
        "contributors": [
            "Markus Wallinger",
            "Peter Rottmann"
        ],
        "authors": [
            "Peter Rottmann",
            "Markus Wallinger",
            "Annika Bonerath",
            "Sven Gedicke",
            "Martin N\u00f6llenburg",
            "Jan-Henrik Haunert"
        ],
        "abstract": "",
        "uid": "v-full-1551",
        "file_name": "",
        "time_stamp": "2022-10-20T21:55:00Z",
        "time_start": "2022-10-20T21:55:00Z",
        "time_end": "2022-10-20T21:57:00Z"
    },
    "v-full-1165": {
        "slot_id": "v-full-1165-qa",
        "session_id": "full24",
        "type": "Virtual Q+A",
        "title": "Taurus: Towards A Unified Force Representation and Universal Solver for Graph Layout (Q+A)",
        "contributors": [
            "Yunhai Wang",
            "Mingliang Xue"
        ],
        "authors": [
            "Mingliang Xue",
            "Zhi Wang",
            "Fahai Zhong",
            "Yong Wang",
            "Mingliang Xu",
            "Oliver Deussen",
            "Yunhai Wang"
        ],
        "abstract": "",
        "uid": "v-full-1165",
        "file_name": "",
        "time_stamp": "2022-10-20T22:07:00Z",
        "time_start": "2022-10-20T22:07:00Z",
        "time_end": "2022-10-20T22:09:00Z"
    },
    "v-tvcg-9705082": {
        "slot_id": "v-tvcg-9705082-qa",
        "session_id": "full24",
        "type": "Virtual Q+A",
        "title": "Visualizing Graph Neural Networks with CorGIE: Corresponding a Graph to Its Embedding (Q+A)",
        "contributors": [
            "Zipeng Liu"
        ],
        "authors": [
            "Zipeng Liu",
            "Yang Wang",
            "J\u00fcrgen Bernard",
            "Tamara Munzner"
        ],
        "abstract": "Graph neural networks (GNNs) are a class of powerful machine learning tools that model node relations for making predictions of nodes or links. GNN developers rely on quantitative metrics of the predictions to evaluate a GNN, but similar to many other neural networks, it is difficult for them to understand if the GNN truly learns characteristics of a graph as expected. We propose an approach to corresponding an input graph to its node embedding (aka latent space), a common component of GNNs that is later used for prediction. We abstract the data and tasks, and develop an interactive multi-view interface called CorGIE to instantiate the abstraction. As the key function in CorGIE, we propose the K-hop graph layout to show topological neighbors in hops and their clustering structure. To evaluate the functionality and usability of CorGIE, we present how to use CorGIE in two usage scenarios, and conduct a case study with five GNN experts.",
        "uid": "v-tvcg-9705082",
        "file_name": "",
        "time_stamp": "2022-10-20T22:19:00Z",
        "time_start": "2022-10-20T22:19:00Z",
        "time_end": "2022-10-20T22:21:00Z"
    },
    "v-full-1541": {
        "slot_id": "v-full-1541-qa",
        "session_id": "full24",
        "type": "In Person Q+A",
        "title": "Comparative Evaluation of Bipartite, Node-Link, and Matrix-Based Network Representations (Q+A)",
        "contributors": [
            "Moataz Abdelaal"
        ],
        "authors": [
            "Moataz Abdelaal",
            "Nathan D Schiele",
            "Katrin Angerbauer",
            "Kuno Kurzhals",
            "Michael Sedlmair",
            "Daniel Weiskopf"
        ],
        "abstract": "",
        "uid": "v-full-1541",
        "file_name": "",
        "time_stamp": "2022-10-20T22:31:00Z",
        "time_start": "2022-10-20T22:31:00Z",
        "time_end": "2022-10-20T22:33:00Z"
    },
    "v-full-1154": {
        "slot_id": "v-full-1154-qa",
        "session_id": "full24",
        "type": "In Person Q+A",
        "title": "Understanding Barriers to Network Exploration with Visualization: A Report from the Trenches (Q+A)",
        "contributors": [
            "Mashael AlKadi"
        ],
        "authors": [
            "Mashael AlKadi",
            "Vanessa Serrano",
            "James Scott-Brown",
            "Uta Hinrichs",
            "Catherine Plaisant",
            "Jean-Daniel Fekete",
            "Benjamin Bach"
        ],
        "abstract": "",
        "uid": "v-full-1154",
        "file_name": "",
        "time_stamp": "2022-10-20T22:43:00Z",
        "time_start": "2022-10-20T22:43:00Z",
        "time_end": "2022-10-20T22:45:00Z"
    },
    "v-tvcg-9720180": {
        "slot_id": "v-tvcg-9720180-qa",
        "session_id": "full24",
        "type": "Virtual Q+A",
        "title": "VividGraph: Learning to Extract and Redesign Network Graphs from Visualization Images (Q+A)",
        "contributors": [
            "Sicheng Song"
        ],
        "authors": [
            "Sicheng Song",
            "Chenhui Li",
            "Yujing Sun",
            "Changbo Wang"
        ],
        "abstract": "Network graphs are common visualization charts. They often appear in the form of bitmaps in papers, web pages, magazine prints, and designer sketches. People often want to modify network graphs because of their poor design, but it is difficult to obtain their underlying data. In this paper, we present VividGraph, a pipeline for automatically extracting and redesigning network graphs from static images. We propose using convolutional neural networks to solve the problem of network graph data extraction. Our method is robust to hand-drawn graphs, blurred graph images, and large graph images. We also present a network graph classification module to make it effective for directed graphs. We propose two evaluation methods to demonstrate the effectiveness of our approach. It can be used to quickly transform designer sketches, extract underlying data from existing network graphs, and interactively redesign poorly designed network graphs.",
        "uid": "v-tvcg-9720180",
        "file_name": "",
        "time_stamp": "2022-10-20T22:55:00Z",
        "time_start": "2022-10-20T22:55:00Z",
        "time_end": "2022-10-20T22:57:00Z"
    },
    "v-tvcg-9508898": {
        "slot_id": "v-tvcg-9508898-qa",
        "session_id": "full25",
        "type": "Virtual Q+A",
        "title": "Towards Systematic Design Considerations for Visualizing Cross-View Data Relationships (Q+A)",
        "contributors": [
            "Maoyuan Sun"
        ],
        "authors": [
            "Maoyuan Sun",
            "Akhil Namburi",
            "David Koop",
            "Jian Zhao",
            "Tianyi Li",
            "Haeyong Chung"
        ],
        "abstract": "Due to the scale of data and the complexity of analysis tasks, insight discovery often requires coordinating multiple visualizations (views), with each view displaying different parts of data or the same data from different perspectives. For example, to analyze car sales records, a marketing analyst uses a line chart to visualize the trend of car sales, a scatterplot to inspect the price and horsepower of different cars, and a matrix to compare the transaction amounts in types of deals. To explore related information across multiple views, current visual analysis tools heavily rely on brushing and linking techniques, which may require a significant amount of user effort (e.g., many trial-and-error attempts). There may be other efficient and effective ways of displaying cross-view data relationships to support data analysis with multiple views, but currently there are no guidelines to address this design challenge. In this paper, we present systematic design considerations for visualizing cross-view data relationships, which leverages descriptive aspects of relationships and usable visual context of multi-view visualizations. We discuss pros and cons of different designs for showing cross-view data relationships, and provide a set of recommendations for helping practitioners make design decisions.",
        "uid": "v-tvcg-9508898",
        "file_name": "",
        "time_stamp": "2022-10-19T21:55:00Z",
        "time_start": "2022-10-19T21:55:00Z",
        "time_end": "2022-10-19T21:57:00Z"
    },
    "v-tvcg-9465643": {
        "slot_id": "v-tvcg-9465643-qa",
        "session_id": "full25",
        "type": "Virtual Q+A",
        "title": "Designing with Pictographs: Envision Topics without Sacrificing Understanding (Q+A)",
        "contributors": [
            "Alyx Burns"
        ],
        "authors": [
            "Alyxander Burns",
            "Cindy Xiong",
            "Steven Franconeri",
            "Alberto Cairo",
            "Narges Mahyar"
        ],
        "abstract": "Past studies have shown that when a visualization uses pictographs to encode data, they have a positive effect on memory, engagement, and assessment of risk. However, little is known about how pictographs affect one's ability to understand a visualization, beyond memory for values and trends. We conducted two crowdsourced experiments to compare the effectiveness of using pictographs when showing part-to-whole relationships. In Experiment 1, we compared pictograph arrays to more traditional bar and pie charts. We tested participants' ability to generate high-level insights following Bloom's taxonomy of educational objectives via 6 free-response questions. We found that accuracy for extracting information and generating insights did not differ overall between the two versions. To explore the motivating differences between the designs, we conducted a second experiment where participants compared charts containing pictograph arrays to more traditional charts on 5 metrics and explained their reasoning. We found that some participants preferred the way that pictographs allowed them to envision the topic more easily, while others preferred traditional bar and pie charts because they seem less cluttered and faster to read. These results suggest that, at least in simple visualizations depicting part-to-whole relationships, the choice of using pictographs has little influence on sensemaking and insight extraction. When deciding whether to use pictograph arrays, designers should consider visual appeal, perceived comprehension time, ease of envisioning the topic, and clutteredness.",
        "uid": "v-tvcg-9465643",
        "file_name": "",
        "time_stamp": "2022-10-19T22:07:00Z",
        "time_start": "2022-10-19T22:07:00Z",
        "time_end": "2022-10-19T22:09:00Z"
    },
    "v-tvcg-9737134": {
        "slot_id": "v-tvcg-9737134-qa",
        "session_id": "full25",
        "type": "In Person Q+A",
        "title": "Visualizing Higher-Order 3D Tensors by Multipole Lines (Q+A)",
        "contributors": [
            "Chiara Hergl"
        ],
        "authors": [
            "Chiara Hergl",
            "Thomas Nagel",
            "Gerik Scheuermann"
        ],
        "abstract": "Physics, medicine, earth sciences, mechanical engineering, geo-engineering, bio-engineering and many more application areas use tensorial data. For example, tensors are used in formulating the balance equations of charge, mass, momentum, or energy as well as the constitutive relations that complement them. Some of these tensors (i.e. stiffness tensor, strain gradient, photo-elastic tensor) are of order higher than two. Currently, there are nearly no visualization techniques for such data beyond glyphs. An important reason for this is the limit of currently used tensor decomposition techniques. In this article, we propose to use the deviatoric decomposition to draw lines describing tensors of arbitrary order in three dimensions. The deviatoric decomposition splits a three-dimensional tensor of any order with any type of index symmetry into totally symmetric, traceless tensors. These tensors, called deviators, can be described by a unique set of directions (called multipoles by J.~C. Maxwell) and scalars. These multipoles allow the definition of multipole lines which can be computed in a similar fashion to tensor lines and allow a line-based visualization of three-dimensional tensors of any order. We give examples for the visualization of symmetric, second-order tensor fields as well as fourth-order tensor fields. To allow an interpretation of the multipole lines, we analyze the connection between the multipoles and the eigenvectors/eigenvalues in the second-order case. For the fourth-order stiffness tensor, we prove relations between multipoles and important physical quantities such as shear moduli as well as the eigenvectors of the second-order right Cauchy-Green tensor.",
        "uid": "v-tvcg-9737134",
        "file_name": "",
        "time_stamp": "2022-10-19T22:19:00Z",
        "time_start": "2022-10-19T22:19:00Z",
        "time_end": "2022-10-19T22:21:00Z"
    },
    "v-full-1335": {
        "slot_id": "v-full-1335-qa",
        "session_id": "full25",
        "type": "Virtual Q+A",
        "title": "MetaGlyph: Automatic Generation of Metaphoric Glyph-based Visualization (Q+A)",
        "contributors": [
            "Lu Ying"
        ],
        "authors": [
            "Lu Ying",
            "Yuchen Yang",
            "Xinhuan Shu",
            "Dazhen Deng",
            "Tan Tang",
            "Lingyun Yu",
            "Yingcai Wu"
        ],
        "abstract": "",
        "uid": "v-full-1335",
        "file_name": "",
        "time_stamp": "2022-10-19T22:31:00Z",
        "time_start": "2022-10-19T22:31:00Z",
        "time_end": "2022-10-19T22:33:00Z"
    },
    "v-full-1554": {
        "slot_id": "v-full-1554-qa",
        "session_id": "full25",
        "type": "In Person Q+A",
        "title": "Dashboard Design Patterns (Q+A)",
        "contributors": [
            "Benjamin Bach"
        ],
        "authors": [
            "Benjamin Bach",
            "Euan Freeman",
            "Alfie Abdul-Rahman",
            "Cagatay Turkay",
            "Saiful Khan",
            "Yulei Fan",
            "Min Chen"
        ],
        "abstract": "",
        "uid": "v-full-1554",
        "file_name": "",
        "time_stamp": "2022-10-19T22:43:00Z",
        "time_start": "2022-10-19T22:43:00Z",
        "time_end": "2022-10-19T22:45:00Z"
    },
    "v-full-1206": {
        "slot_id": "v-full-1206-qa",
        "session_id": "full25",
        "type": "Virtual Q+A",
        "title": "A Representative Design Space of Multiclass Contour Visualization (Q+A)",
        "contributors": [
            "Sihang Li"
        ],
        "authors": [
            "Sihang Li",
            "Jiacheng Yu",
            "Mingxuan Li",
            "Le Liu",
            "Xiaolong (Luke) Zhang",
            "Xiaoru Yuan"
        ],
        "abstract": "",
        "uid": "v-full-1206",
        "file_name": "",
        "time_stamp": "2022-10-19T22:55:00Z",
        "time_start": "2022-10-19T22:55:00Z",
        "time_end": "2022-10-19T22:57:00Z"
    },
    "v-full-1101": {
        "slot_id": "v-full-1101-qa",
        "session_id": "full26",
        "type": "In Person Q+A",
        "title": "Exploring Interactions with Printed Data Visualizations in Augmented Reality (Q+A)",
        "contributors": [
            "Wai Tong"
        ],
        "authors": [
            "Wai Tong",
            "Zhutian Chen",
            "Meng Xia",
            "Leo Yu-Ho Lo",
            "Linping Yuan",
            "Benjamin Bach",
            "Huamin Qu"
        ],
        "abstract": "",
        "uid": "v-full-1101",
        "file_name": "",
        "time_stamp": "2022-10-20T01:10:00Z",
        "time_start": "2022-10-20T01:10:00Z",
        "time_end": "2022-10-20T01:12:00Z"
    },
    "v-tvcg-9729627": {
        "slot_id": "v-tvcg-9729627-qa",
        "session_id": "full26",
        "type": "In Person Q+A",
        "title": "RagRug: A Toolkit for Situated Analytics (Q+A)",
        "contributors": [
            "Philipp Fleck"
        ],
        "authors": [
            "Philipp Fleck",
            "Aimee Sousa Calepso",
            "Sebastian Hubenschmid,Michael Sedlmair",
            "Dieter Schmalstieg"
        ],
        "abstract": "We present RagRug, an open-source toolkit for situated analytics. The  , abilities of RagRug go beyond previous immersiveanalytics toolkits by  , focusing on specific requirements emerging when using augmented  , reality (AR) rather than virtual reality. RagRugcombines state of the  , art visual encoding capabilities with a comprehensive physical-virtual  , model, which lets application developerssystematically describe the  , physical objects in the real world and their role in AR. We connect AR  , visualizations with data streams fromthe Internet of Things using  , distributed dataflow. To this end, we use reactive programming  , patterns so that visualizations becomecontext-aware, i.e., they adapt  , to events coming in from the environment. The resulting authoring  , system is low-code; it emphasisesdescribing the physical and the  , virtual world and the dataflow between the elements contained therein.  , We describe the technicaldesign and implementation of RagRug, and  , report on five example applications illustrating the toolkit\u2019s  , abilities.",
        "uid": "v-tvcg-9729627",
        "file_name": "",
        "time_stamp": "2022-10-20T01:22:00Z",
        "time_start": "2022-10-20T01:22:00Z",
        "time_end": "2022-10-20T01:24:00Z"
    },
    "v-full-1272": {
        "slot_id": "v-full-1272-qa",
        "session_id": "full26",
        "type": "Virtual Q+A",
        "title": "PuzzleFixer: A Visual Reassembly System for Immersive Fragments Restoration (Q+A)",
        "contributors": [
            "Shuainan Ye"
        ],
        "authors": [
            "Shuainan Ye",
            "Zhutian Chen",
            "Xiangtong Chu",
            "Kang Li",
            "Juntong Luo",
            "Yi Li",
            "Guohua Geng",
            "Yingcai Wu"
        ],
        "abstract": "",
        "uid": "v-full-1272",
        "file_name": "",
        "time_stamp": "2022-10-20T01:34:00Z",
        "time_start": "2022-10-20T01:34:00Z",
        "time_end": "2022-10-20T01:36:00Z"
    },
    "v-full-1382": {
        "slot_id": "v-full-1382-qa",
        "session_id": "full26",
        "type": "Virtual Q+A",
        "title": "Effects of View Layout on Situated Analytics for Multiple Representations in Immersive Visualizaiton (Q+A)",
        "contributors": [
            "Zhen Wen"
        ],
        "authors": [
            "Zhen Wen",
            "Wei Zeng",
            "Luoxuan Weng",
            "Yihan Liu",
            "Mingliang Xu",
            "Wei Chen"
        ],
        "abstract": "",
        "uid": "v-full-1382",
        "file_name": "",
        "time_stamp": "2022-10-20T01:46:00Z",
        "time_start": "2022-10-20T01:46:00Z",
        "time_end": "2022-10-20T01:48:00Z"
    },
    "v-full-1523": {
        "slot_id": "v-full-1523-qa",
        "session_id": "full26",
        "type": "Virtual Q+A",
        "title": "RoboHapalytics: A Robot Assisted Haptic Controller for Immersive Analytics (Q+A)",
        "contributors": [
            "shaozhang dai"
        ],
        "authors": [
            "Shaozhang Dai",
            "Tim Dwyer",
            "Barrett Ens",
            "Jim Smiley",
            "Lonni Besan\u00e7on"
        ],
        "abstract": "",
        "uid": "v-full-1523",
        "file_name": "",
        "time_stamp": "2022-10-20T01:58:00Z",
        "time_start": "2022-10-20T01:58:00Z",
        "time_end": "2022-10-20T02:00:00Z"
    },
    "v-tvcg-9645242": {
        "slot_id": "v-tvcg-9645242-qa",
        "session_id": "full26",
        "type": "In Person Q+A",
        "title": "Labeling Out-of-View Objects in Immersive Analytics to Support Situated Visual Searching (Q+A)",
        "contributors": [
            "Tica Lin"
        ],
        "authors": [
            "Tica Lin",
            "Yalong Yang",
            "Johanna Beyer",
            "Hanspeter Pfister"
        ],
        "abstract": "Augmented Reality (AR) embeds digital information into objects of the physical world. Data can be shown in-situ, thereby enabling real-time visual comparisons and object search in real-life user tasks, such as comparing products and looking up scores in a sports game. While there have been studies on designing AR interfaces for situated information retrieval, there has only been limited research on AR object labeling for visual search tasks in the spatial environment. In this paper, we identify and categorize different design aspects in AR label design and report on a formal user study on labels for out-of-view objects to support visual search tasks in AR. We design three visualization techniques for out-of-view object labeling in AR, which respectively encode the relative physical position (height-encoded), the rotational direction (angle-encoded), and the label values (value-encoded) of the objects. We further implement two traditional in-view object labeling techniques, where labels are placed either next to the respective objects (situated) or at the edge of the AR FoV (boundary). We evaluate these five different label conditions in three visual search tasks for static objects. Our study shows that out-of-view object labels are beneficial when searching for objects outside the FoV, spatial orientation, and when comparing multiple spatially sparse objects. Angle-encoded labels with directional cues of the surrounding objects have the overall best performance with the highest user satisfaction. We discuss the implications of our findings for future immersive AR interface design.",
        "uid": "v-tvcg-9645242",
        "file_name": "",
        "time_stamp": "2022-10-20T02:10:00Z",
        "time_start": "2022-10-20T02:10:00Z",
        "time_end": "2022-10-20T02:12:00Z"
    },
    "v-full-1351": {
        "slot_id": "v-full-1351-qa",
        "session_id": "full27",
        "type": "Virtual Q+A",
        "title": "VACSEN: A Visualization Approach for Noise Awareness in Quantum Computing (Q+A)",
        "contributors": [
            "Shaolun Ruan"
        ],
        "authors": [
            "Shaolun Ruan",
            "Yong Wang",
            "Weiwen Jiang",
            "Ying Mao",
            "Qiang Guan"
        ],
        "abstract": "",
        "uid": "v-full-1351",
        "file_name": "",
        "time_stamp": "2022-10-20T01:10:00Z",
        "time_start": "2022-10-20T01:10:00Z",
        "time_end": "2022-10-20T01:12:00Z"
    },
    "v-tvcg-9676662": {
        "slot_id": "v-tvcg-9676662-qa",
        "session_id": "full27",
        "type": "In Person Q+A",
        "title": "Do you believe your (social media) data? A personal story on location data biases, errors, and plausibility as well as their visualization (Q+A)",
        "contributors": [
            "Tobias Isenberg"
        ],
        "authors": [
            "Tobias Isenberg",
            "Zujany Salazar",
            "Rafael Blanco",
            "Catherine Plaisant"
        ],
        "abstract": "We present a case study on a journey about a personal data collection of carnivorous plant species habitats, and the resulting scientific exploration of location data biases, data errors, location hiding, and data plausibility. While initially driven by personal interest, our work led to the analysis and development of various means for visualizing threats to insight from geo-tagged social media data. In the course of this endeavor we analyzed local and global geographic distributions and their inaccuracies. We also contribute Motion Plausibility Profiles---a new means for visualizing how believable a specific contributor\u2019s location data is or if it was likely manipulated. We then compared our own repurposed social media dataset with data from a dedicated citizen science project. Compared to biases and errors in the literature on traditional citizen science data, with our visualizations we could also identify some new types or show new aspects for known ones. Moreover, we demonstrate several types of errors and biases for repurposed social media data.",
        "uid": "v-tvcg-9676662",
        "file_name": "",
        "time_stamp": "2022-10-20T01:22:00Z",
        "time_start": "2022-10-20T01:22:00Z",
        "time_end": "2022-10-20T01:24:00Z"
    },
    "v-full-1050": {
        "slot_id": "v-full-1050-qa",
        "session_id": "full27",
        "type": "In Person Q+A",
        "title": "D-BIAS: A Causality-Based Human-in-the-Loop System for Tackling Algorithmic Bias (Q+A)",
        "contributors": [
            "Bhavya Ghai"
        ],
        "authors": [
            "Bhavya Ghai",
            "Klaus Mueller"
        ],
        "abstract": "",
        "uid": "v-full-1050",
        "file_name": "",
        "time_stamp": "2022-10-20T01:34:00Z",
        "time_start": "2022-10-20T01:34:00Z",
        "time_end": "2022-10-20T01:36:00Z"
    },
    "v-full-1618": {
        "slot_id": "v-full-1618-qa",
        "session_id": "full27",
        "type": "In Person Q+A",
        "title": "A Unified Comparison of User Modeling Techniques for Information Relevance and Exploration Bias (Q+A)",
        "contributors": [
            "Sunwoo Ha"
        ],
        "authors": [
            "Sunwoo Ha",
            "Shayan Monadjemi",
            "Alvitta Ottley",
            "Roman Garnett"
        ],
        "abstract": "",
        "uid": "v-full-1618",
        "file_name": "",
        "time_stamp": "2022-10-20T01:46:00Z",
        "time_start": "2022-10-20T01:46:00Z",
        "time_end": "2022-10-20T01:48:00Z"
    },
    "v-full-1151": {
        "slot_id": "v-full-1151-qa",
        "session_id": "full27",
        "type": "In Person Q+A",
        "title": "Seeing What You Believe or Believing What You See? Belief Biases Correlation Estimation (Q+A)",
        "contributors": [
            "Cindy Xiong"
        ],
        "authors": [
            "Cindy Xiong",
            "Chase Stokes",
            "Yea-Seul Kim",
            "Steven Franconeri"
        ],
        "abstract": "",
        "uid": "v-full-1151",
        "file_name": "",
        "time_stamp": "2022-10-20T01:58:00Z",
        "time_start": "2022-10-20T01:58:00Z",
        "time_end": "2022-10-20T02:00:00Z"
    },
    "v-full-1155": {
        "slot_id": "v-full-1155-qa",
        "session_id": "full27",
        "type": "In Person Q+A",
        "title": "Data Hunches: Incorporating Personal Knowledge into Visualizations (Q+A)",
        "contributors": [
            "Haihan Lin"
        ],
        "authors": [
            "Haihan Lin",
            "Derya Akbaba",
            "Miriah Meyer",
            "Alexander Lex"
        ],
        "abstract": "",
        "uid": "v-full-1155",
        "file_name": "",
        "time_stamp": "2022-10-20T02:10:00Z",
        "time_start": "2022-10-20T02:10:00Z",
        "time_end": "2022-10-20T02:12:00Z"
    },
    "v-full-1193": {
        "slot_id": "v-full-1193-qa",
        "session_id": "full28",
        "type": "In Person Q+A",
        "title": "Multi-View Design Patterns and Responsive Visualization for Genomics Data (Q+A)",
        "contributors": [
            "Sehi L'Yi"
        ],
        "authors": [
            "Sehi L'Yi",
            "Nils Gehlenborg"
        ],
        "abstract": "",
        "uid": "v-full-1193",
        "file_name": "",
        "time_stamp": "2022-10-20T02:55:00Z",
        "time_start": "2022-10-20T02:55:00Z",
        "time_end": "2022-10-20T02:57:00Z"
    },
    "v-tvcg-9523759": {
        "slot_id": "v-tvcg-9523759-qa",
        "session_id": "full28",
        "type": "Virtual Q+A",
        "title": "Vivern \u2013 A Virtual Environment for Multiscale Visualization and Modeling of DNA Nanostructures (Q+A)",
        "contributors": [
            "David Ku\u0165\u00e1k"
        ],
        "authors": [
            "David Ku\u0165\u00e1k",
            "Matias Nicol\u00e1s Selzer",
            "Jan By\u0161ka",
            "Mar\u00eda Luj\u00e1n Ganuza",
            "Ivan Bari\u0161i\u0107",
            "Barbora Kozl\u00edkov\u00e1",
            "Haichao Miao"
        ],
        "abstract": "DNA nanostructures offer promising applications, particularly in the biomedical domain, as they can be used for targeted drug delivery, construction of nanorobots, or as a basis for molecular motors. One of the most prominent techniques for assembling these structures is DNA origami. Nowadays, desktop applications are used for the in silico design of such structures. However, as such structures are often spatially complex, their assembly and analysis are complicated. Since virtual reality was proven to be advantageous for such spatial-related tasks and there are no existing VR solutions focused on this domain, we propose Vivern, a VR application that allows domain experts to design and visually examine DNA origami nanostructures. Our approach presents different abstracted visual representations of the nanostructures, various color schemes, and an ability to place several DNA nanostructures and proteins in one environment, thus allowing for the detailed analysis of complex assemblies. We also present two novel examination tools, the Magic Scale Lens and the DNA Untwister, that allow the experts to visually embed different representations into local regions to preserve the context and support detailed investigation. To showcase the capabilities of our solution, prototypes of novel nanodevices conceptualized by our collaborating experts, such as DNA-protein hybrid structures and DNA origami superstructures, are presented. Finally, the results of two rounds of evaluations are summarized. They demonstrate the advantages of our solution, especially for scenarios where current desktop tools are very limited, while also presenting possible future research directions.",
        "uid": "v-tvcg-9523759",
        "file_name": "",
        "time_stamp": "2022-10-20T03:07:00Z",
        "time_start": "2022-10-20T03:07:00Z",
        "time_end": "2022-10-20T03:09:00Z"
    },
    "v-full-1212": {
        "slot_id": "v-full-1212-qa",
        "session_id": "full28",
        "type": "In Person Q+A",
        "title": "GenoREC: A Recommendation System for Interactive Genomics Data Visualization (Q+A)",
        "contributors": [
            "Aditeya Pandey"
        ],
        "authors": [
            "Aditeya Pandey",
            "Sehi L'Yi",
            "Qianwen Wang",
            "Michelle A. Borkin",
            "Nils Gehlenborg"
        ],
        "abstract": "",
        "uid": "v-full-1212",
        "file_name": "",
        "time_stamp": "2022-10-20T03:19:00Z",
        "time_start": "2022-10-20T03:19:00Z",
        "time_end": "2022-10-20T03:21:00Z"
    },
    "v-full-1175": {
        "slot_id": "v-full-1175-qa",
        "session_id": "full28",
        "type": "In Person Q+A",
        "title": "sMolBoxes: Dataflow Model for Molecular Dynamics Exploration (Q+A)",
        "contributors": [
            "Pavol Ulbrich"
        ],
        "authors": [
            "Pavol Ulbrich",
            "Manuela Waldner",
            "Katar\u00edna Furmanov\u00e1",
            "S\u00e9rgio M. Marques",
            "David Bedn\u00e1\u0159",
            "Barbora Kozlikova",
            "Jan By\u0161ka"
        ],
        "abstract": "",
        "uid": "v-full-1175",
        "file_name": "",
        "time_stamp": "2022-10-20T03:31:00Z",
        "time_start": "2022-10-20T03:31:00Z",
        "time_end": "2022-10-20T03:33:00Z"
    },
    "v-tvcg-9627526": {
        "slot_id": "v-tvcg-9627526-qa",
        "session_id": "full28",
        "type": "In Person Q+A",
        "title": "Molecumentary: Adaptable Narrated Documentaries Using Molecular Visualization (Q+A)",
        "contributors": [
            "David Kouril",
            "Ond\u0159ej Strnad"
        ],
        "authors": [
            "David Kou\u0159il",
            "Ond\u0159ej Strnad",
            "Peter Mindek",
            "Sarkis Halladjian",
            "Tobias Isenberg",
            "M. Eduard Gr\u00f6ller",
            "Ivan Viola"
        ],
        "abstract": "We present a method for producing documentary-style content using real-time scientific visualization. We introduce molecumentaries, i.e., molecular documentaries featuring structural models from molecular biology, created through adaptable methods instead of the rigid traditional production pipeline. Our work is motivated by the rapid evolution of scientific visualization and it potential in science dissemination. Without some form of explanation or guidance, however, novices and lay-persons often find it difficult to gain insights from the visualization itself. We integrate such knowledge using the verbal channel and provide it along an engaging visual presentation. To realize the synthesis of a molecumentary, we provide technical solutions along two major production steps: (1) preparing a story structure and (2) turning the story into a concrete narrative. In the first step, we compile information about the model from heterogeneous sources into a story graph. We combine local knowledge with external sources to complete the story graph and enrich the final result. In the second step, we synthesize a narrative, i.e., story elements presented in sequence, using the story graph. We then traverse the story graph and generate a virtual tour, using automated camera and visualization transitions. We turn texts written by domain experts into verbal representations using text-to-speech functionality and provide them as a commentary. Using the described framework, we synthesize fly-throughs with descriptions: automatic ones that mimic a manually authored documentary or semi-automatic ones which guide the documentary narrative solely through curated textual input.",
        "uid": "v-tvcg-9627526",
        "file_name": "",
        "time_stamp": "2022-10-20T03:43:00Z",
        "time_start": "2022-10-20T03:43:00Z",
        "time_end": "2022-10-20T03:45:00Z"
    },
    "v-full-1578": {
        "slot_id": "v-full-1578-qa",
        "session_id": "full28",
        "type": "Virtual Q+A",
        "title": "Polyphony: an Interactive Transfer Learning Framework for Single-Cell Data Analysis (Q+A)",
        "contributors": [
            "Furui Cheng"
        ],
        "authors": [
            "Furui Cheng",
            "Mark S Keller",
            "Huamin Qu",
            "Nils Gehlenborg",
            "Qianwen Wang"
        ],
        "abstract": "",
        "uid": "v-full-1578",
        "file_name": "",
        "time_stamp": "2022-10-20T03:55:00Z",
        "time_start": "2022-10-20T03:55:00Z",
        "time_end": "2022-10-20T03:57:00Z"
    },
    "v-full-1426": {
        "slot_id": "v-full-1426-qa",
        "session_id": "full29",
        "type": "In Person Q+A",
        "title": "CohortVA: A Visual Analytic System for Progressive Exploration of Cohorts based on Historical Data (Q+A)",
        "contributors": [
            "Wei Zhang",
            "Jason Wong"
        ],
        "authors": [
            "Wei Zhang",
            "Jason Kamkwai Wong",
            "Xumeng Wang",
            "Youcheng Gong",
            "Rongchen Zhu",
            "Kai Liu",
            "Zihan Yan",
            "Siwei Tan",
            "Huamin Qu",
            "Siming Chen",
            "Wei Chen"
        ],
        "abstract": "",
        "uid": "v-full-1426",
        "file_name": "",
        "time_stamp": "2022-10-20T21:55:00Z",
        "time_start": "2022-10-20T21:55:00Z",
        "time_end": "2022-10-20T21:57:00Z"
    },
    "v-full-1421": {
        "slot_id": "v-full-1421-qa",
        "session_id": "full29",
        "type": "In Person Q+A",
        "title": "PromotionLens: Inspecting Promotion Strategies of Online E-commerce via Visual Analytics (Q+A)",
        "contributors": [
            "Chenyang Zhang"
        ],
        "authors": [
            "Chenyang Zhang",
            "Xiyuan Wang",
            "Chuyi Zhao",
            "Yijing Ren",
            "Tianyu Zhang",
            "Zhenhui Peng",
            "Xiaomeng Fan",
            "Xiaojuan Ma",
            "Quan Li"
        ],
        "abstract": "",
        "uid": "v-full-1421",
        "file_name": "",
        "time_stamp": "2022-10-20T22:07:00Z",
        "time_start": "2022-10-20T22:07:00Z",
        "time_end": "2022-10-20T22:09:00Z"
    },
    "v-full-1547": {
        "slot_id": "v-full-1547-qa",
        "session_id": "full29",
        "type": "In Person Q+A",
        "title": "Interactive Visual Analysis of Structure-borne Noise Data (Q+A)",
        "contributors": [
            "Kresimir Matkovic",
            "Rainer Splechtna"
        ],
        "authors": [
            "Rainer Splechtna",
            "Denis Gracanin",
            "Goran Todorovic",
            "Stanislav Goja",
            "Boris Bedic",
            "Helwig Hauser",
            "Kresimir Matkovic"
        ],
        "abstract": "",
        "uid": "v-full-1547",
        "file_name": "",
        "time_stamp": "2022-10-20T22:19:00Z",
        "time_start": "2022-10-20T22:19:00Z",
        "time_end": "2022-10-20T22:21:00Z"
    },
    "v-full-1312": {
        "slot_id": "v-full-1312-qa",
        "session_id": "full29",
        "type": "In Person Q+A",
        "title": "Traveler: Navigating Task Parallel Traces for Performance Analysis (Q+A)",
        "contributors": [
            "Sayef Azad Sakin"
        ],
        "authors": [
            "Sayef Azad Sakin",
            "Alex Bigelow",
            "Mohammad Tohid",
            "Connor Scully-Allison",
            "Carlos Scheidegger",
            "Steven Robert Brandt",
            "Christopher P. Taylor",
            "Kevin A. Huck",
            "Hartmut Kaiser",
            "Katherine E. Isaacs"
        ],
        "abstract": "",
        "uid": "v-full-1312",
        "file_name": "",
        "time_stamp": "2022-10-20T22:31:00Z",
        "time_start": "2022-10-20T22:31:00Z",
        "time_end": "2022-10-20T22:33:00Z"
    },
    "v-full-1617": {
        "slot_id": "v-full-1617-qa",
        "session_id": "full29",
        "type": "In Person Q+A",
        "title": "Visual Analysis and Detection of Contrails in Aircraft Engine Simulations (Q+A)",
        "contributors": [
            "Md Nafiul Alam Nipu"
        ],
        "authors": [
            "Md Nafiul Alam Nipu",
            "Carla Gabriela Floricel",
            "Negar Naghash Zadeh",
            "Roberto Paoli",
            "G. Elisabeta Marai"
        ],
        "abstract": "",
        "uid": "v-full-1617",
        "file_name": "",
        "time_stamp": "2022-10-20T22:43:00Z",
        "time_start": "2022-10-20T22:43:00Z",
        "time_end": "2022-10-20T22:45:00Z"
    },
    "v-full-1478": {
        "slot_id": "v-full-1478-qa",
        "session_id": "full29",
        "type": "Virtual Q+A",
        "title": "DPVisCreator: Incorporating visual constraints to privacy-preserving visualization via differential privacy (Q+A)",
        "contributors": [
            "Jiehui Zhou"
        ],
        "authors": [
            "Jiehui Zhou",
            "Xumeng Wang",
            "Jason Kamkwai Wong",
            "Huanliang Wang",
            "Zhongwei Wang",
            "Xiaoyu Yang",
            "Xiaoran Yan",
            "Haozhe Feng",
            "Huamin Qu",
            "Haochao Ying",
            "Wei Chen"
        ],
        "abstract": "",
        "uid": "v-full-1478",
        "file_name": "",
        "time_stamp": "2022-10-20T22:55:00Z",
        "time_start": "2022-10-20T22:55:00Z",
        "time_end": "2022-10-20T22:57:00Z"
    },
    "v-full-1568": {
        "slot_id": "v-full-1568-qa",
        "session_id": "full30",
        "type": "Virtual Q+A",
        "title": "ErgoExplorer: Interactive Ergonomic Risk Assessment from Video Collections (Q+A)",
        "contributors": [
            "Claudio Delrieux"
        ],
        "authors": [
            "Manlio Miguel Massiris Fernandez",
            "Sanjin Rados",
            "Eduard Gr\u00f6ller",
            "Claudio Delrieux",
            "Kresimir Matkovic"
        ],
        "abstract": "",
        "uid": "v-full-1568",
        "file_name": "",
        "time_stamp": "2022-10-19T20:10:00Z",
        "time_start": "2022-10-19T20:10:00Z",
        "time_end": "2022-10-19T20:12:00Z"
    },
    "v-full-1614": {
        "slot_id": "v-full-1614-qa",
        "session_id": "full30",
        "type": "In Person Q+A",
        "title": "TrafficVis: Visualizing Organized Activity and Spatio-Temporal Patterns for Detecting Human Trafficking (Q+A)",
        "contributors": [
            "Catalina Vajiac"
        ],
        "authors": [
            "Catalina Vajiac",
            "Duen Horng Chau",
            "Andreas Olligschlaeger",
            "Rebecca Mackenzie",
            "Pratheeksha Nair",
            "Meng-Chieh Lee",
            "Yifei Li",
            "Namyong Park",
            "Reihaneh Rabbany",
            "Christos Faloutsos"
        ],
        "abstract": "",
        "uid": "v-full-1614",
        "file_name": "",
        "time_stamp": "2022-10-19T20:22:00Z",
        "time_start": "2022-10-19T20:22:00Z",
        "time_end": "2022-10-19T20:24:00Z"
    },
    "v-tvcg-9507307": {
        "slot_id": "v-tvcg-9507307-qa",
        "session_id": "full30",
        "type": "In Person Q+A",
        "title": "Outcome-Explorer: A Causality Guided Interactive Visual Interface for Interpretable Algorithmic Decision Making (Q+A)",
        "contributors": [
            "Md Naimul Hoque"
        ],
        "authors": [
            "Hoque",
            "Md Naimul\uff0cMueller",
            "Klaus"
        ],
        "abstract": "The widespread adoption of algorithmic decision-making systems has brought about the necessity to interpret the reasoning behind these decisions. The majority of these systems are complex black box models, and auxiliary models are often used to approximate and then explain their behavior. However, recent research suggests that such explanations are not overly accessible to lay users with no specific expertise in machine learning and this can lead to an incorrect interpretation of the underlying model. In this paper, we show that a predictive and interactive model based on causality is inherently interpretable, does not require any auxiliary model, and allows both expert and non-expert users to understand the model comprehensively. To demonstrate our method we developed Outcome Explorer, a causality guided interactive interface, and evaluated it by conducting think-aloud sessions with three expert users and a user study with 18 non-expert users. All three expert users found our tool to be comprehensive in supporting their explanation needs while the non-expert users were able to understand the inner workings of a model easily.",
        "uid": "v-tvcg-9507307",
        "file_name": "",
        "time_stamp": "2022-10-19T20:34:00Z",
        "time_start": "2022-10-19T20:34:00Z",
        "time_end": "2022-10-19T20:36:00Z"
    },
    "v-full-1400": {
        "slot_id": "v-full-1400-qa",
        "session_id": "full30",
        "type": "Virtual Q+A",
        "title": "MedChemLens: An Interactive Visual Tool to Support Direction Selection in Interdisciplinary Experimental Research of Medicinal Chemistry (Q+A)",
        "contributors": [
            "Chuhan Shi"
        ],
        "authors": [
            "Chuhan Shi",
            "Fei Nie",
            "Yicheng Hu",
            "Yige Xu",
            "Lei Chen",
            "Xiaojuan Ma",
            "Qiong Luo"
        ],
        "abstract": "",
        "uid": "v-full-1400",
        "file_name": "",
        "time_stamp": "2022-10-19T20:46:00Z",
        "time_start": "2022-10-19T20:46:00Z",
        "time_end": "2022-10-19T20:48:00Z"
    },
    "v-tvcg-9761750": {
        "slot_id": "v-tvcg-9761750-qa",
        "session_id": "full30",
        "type": "Virtual Q+A",
        "title": "GestureLens: Visual Analysis of Gestures in Presentation Videos (Q+A)",
        "contributors": [
            "Haipeng Zeng"
        ],
        "authors": [
            "Haipeng Zeng",
            "Xingbo Wang",
            "Yong Wang",
            "Aoyu Wu",
            "Ting Chuen Pong",
            "Huamin Qu"
        ],
        "abstract": "Appropriate gestures can enhance message delivery and audience engagement in both daily communication and public presentations. In this paper, we contribute a visual analytic approach that assists professional public speaking coaches in improving their practice of gesture training through analyzing presentation videos. Manually checking and exploring gesture usage in the presentation videos is often tedious and time-consuming. There lacks an efficient method to help users conduct gesture exploration, which is challenging due to the intrinsically temporal evolution of gestures and their complex correlation to speech content. In this paper, we propose GestureLens, a visual analytics system to facilitate gesture-based and content-based exploration of gesture usage in presentation videos. Specifically, the exploration view enables users to obtain a quick overview of the spatial and temporal distributions of gestures. The dynamic hand movements are firstly aggregated through a heatmap in the gesture space for uncovering spatial patterns, and then decomposed into two mutually perpendicular timelines for revealing temporal patterns. The relation view allows users to explicitly explore the correlation between speech content and gestures by enabling linked analysis and intuitive glyph designs. The video view and dynamic view show the context and overall dynamic movement of the selected gestures, respectively. Two usage scenarios and expert interviews with professional presentation coaches demonstrate the effectiveness and usefulness of GestureLens in facilitating gesture exploration and analysis of presentation videos.",
        "uid": "v-tvcg-9761750",
        "file_name": "",
        "time_stamp": "2022-10-19T20:58:00Z",
        "time_start": "2022-10-19T20:58:00Z",
        "time_end": "2022-10-19T21:00:00Z"
    },
    "v-full-1667": {
        "slot_id": "v-full-1667-qa",
        "session_id": "full30",
        "type": "In Person Q+A",
        "title": "Visual Concept Programming: A Visual Analytics Approach to Injecting Human Intelligence at Scale (Q+A)",
        "contributors": [
            "Wenbin He"
        ],
        "authors": [
            "Md Naimul Hoque",
            "Wenbin He",
            "Shekar Arvind Kumar",
            "Liang Gou",
            "Liu Ren"
        ],
        "abstract": "",
        "uid": "v-full-1667",
        "file_name": "",
        "time_stamp": "2022-10-19T21:10:00Z",
        "time_start": "2022-10-19T21:10:00Z",
        "time_end": "2022-10-19T21:12:00Z"
    },
    "v-full-1089": {
        "slot_id": "v-full-1089-qa",
        "session_id": "full31",
        "type": "In Person Q+A",
        "title": "Provenance Representations and Resulting Strategies in an Exploratory Data Analysis Scenario (Q+A)",
        "contributors": [
            "Jeremy E Block"
        ],
        "authors": [
            "Jeremy E Block",
            "Shaghayegh Esmaeili",
            "Eric Ragan",
            "John Goodall",
            "Gregory David Richardson"
        ],
        "abstract": "",
        "uid": "v-full-1089",
        "file_name": "",
        "time_stamp": "2022-10-21T02:55:00Z",
        "time_start": "2022-10-21T02:55:00Z",
        "time_end": "2022-10-21T02:57:00Z"
    },
    "v-tvcg-9768153": {
        "slot_id": "v-tvcg-9768153-qa",
        "session_id": "full31",
        "type": "Virtual Q+A",
        "title": "Understanding How In-Visualization Provenance Can Support Trade-off Analysis (Q+A)",
        "contributors": [
            "Mehdi Chakhchoukh"
        ],
        "authors": [
            "Mehdi Chakhchoukh",
            "Nadia Boukhelifa",
            "Anastasia Bezerianos"
        ],
        "abstract": "In domains such as agronomy or manufacturing, experts need to consider trade-offs when making decisions that involve several, often competing, objectives. Such analysis is complex and may be conducted over long periods of time, making it hard to revisit.In this paper, we consider the use of analytic provenance mechanisms to aid experts recall and keep track of trade-off analysis. Weimplemented VisProm, a web-based trade-off analysis system, that incorporates in-visualization provenance views, designed to helpexperts keep track of trade-offs and their objectives. We used VisProm as a technology probe to understand user needs and explore thepotential role of provenance in this context. Through observation sessions with three groups of experts analyzing their own data, we makethe following contributions. We first, identify eight high-level tasks that experts engaged in during trade-off analysis, such as locating andcharacterizing interest zones in the trade-off space, and show how these tasks can be supported by provenance visualization. Second,we refine findings from previous work on provenance purposes such as recall and reproduce, by identifying specific objects of thesepurposes related to trade-off analysis, such as interest zones, and exploration structure (e.g., exploration of alternatives and branches).Third, we discuss insights on how the identified provenance objects and our designs support these trade-off analysis tasks, both whenrevisiting past analysis and while actively exploring. And finally, we identify new opportunities for provenance-driven trade-off analysis, forexample related to monitoring the coverage of the trade-off space, and tracking alternative trade-off scenario.",
        "uid": "v-tvcg-9768153",
        "file_name": "",
        "time_stamp": "2022-10-21T03:07:00Z",
        "time_start": "2022-10-21T03:07:00Z",
        "time_end": "2022-10-21T03:09:00Z"
    },
    "v-tvcg-9652041": {
        "slot_id": "v-tvcg-9652041-qa",
        "session_id": "full31",
        "type": "Virtual Q+A",
        "title": "Provectories: Embedding-based Analysis of Interaction Provenance Data (Q+A)",
        "contributors": [
            "Conny Walchshofer",
            "Andreas Hinterreiter"
        ],
        "authors": [
            "Conny Walchshofer",
            "Andreas Hinterreiter",
            "Kai Xu",
            "Holger Stitz",
            "Marc Streit"
        ],
        "abstract": "Understanding user behavior patterns and visual analysis strategies is a long-standing challenge. Existing approaches rely largely on time-consuming manual processes such as interviews and the analysis of observational data. While it is technically possible to capture a history of user interactions and application states, it remains difficult to extract and describe analysis strategies based on interaction provenance. In this paper, we propose a novel visual approach to the meta-analysis of interaction provenance. We capture single and multiple user sessions as graphs of high-dimensional application states. Our meta-analysis is based on two different types of two-dimensional embeddings of these high-dimensional states: layouts based on (i) topology and (ii) attribute similarity. We applied these visualization approaches to synthetic and real user provenance data captured in two user studies. From our visualizations, we were able to extract patterns for data types and analytical reasoning strategies.",
        "uid": "v-tvcg-9652041",
        "file_name": "",
        "time_stamp": "2022-10-21T03:19:00Z",
        "time_start": "2022-10-21T03:19:00Z",
        "time_end": "2022-10-21T03:21:00Z"
    },
    "v-full-1003": {
        "slot_id": "v-full-1003-qa",
        "session_id": "full31",
        "type": "Virtual Q+A",
        "title": "Lotse: A Practical Framework for Guidance in Visual Analytics (Q+A)",
        "contributors": [
            "Fabian Sperrle"
        ],
        "authors": [
            "Fabian Sperrle",
            "Davide Ceneda",
            "Mennatallah El-Assady"
        ],
        "abstract": "",
        "uid": "v-full-1003",
        "file_name": "",
        "time_stamp": "2022-10-21T03:31:00Z",
        "time_start": "2022-10-21T03:31:00Z",
        "time_end": "2022-10-21T03:33:00Z"
    },
    "v-full-1142": {
        "slot_id": "v-full-1142-qa",
        "session_id": "full31",
        "type": "In Person Q+A",
        "title": "Medley: Intent-based Recommendations to Support Dashboard Composition (Q+A)",
        "contributors": [
            "Arjun Srinivasan"
        ],
        "authors": [
            "Aditeya Pandey",
            "Arjun Srinivasan",
            "Vidya Setlur"
        ],
        "abstract": "",
        "uid": "v-full-1142",
        "file_name": "",
        "time_stamp": "2022-10-21T03:43:00Z",
        "time_start": "2022-10-21T03:43:00Z",
        "time_end": "2022-10-21T03:45:00Z"
    },
    "v-tvcg-9524484": {
        "slot_id": "v-tvcg-9524484-qa",
        "session_id": "full31",
        "type": "In Person Q+A",
        "title": "GEViTRec: Data Reconnaissance Through Recommendation Using a Domain-Specific Visualization Prevalence Design Space (Q+A)",
        "contributors": [
            "Tamara Munzner",
            "Anamaria Crisan"
        ],
        "authors": [
            "Anamaria Crisan",
            "Shannah Fisher",
            "Jennifer L. Gardy",
            "Tamara Munzner"
        ],
        "abstract": "Genomic Epidemiology (genEpi) is a branch of public health that uses many different data types including tabular, network, genomic, and geographic, to identify and contain outbreaks of deadly diseases. Due to the volume and variety of data, it is challenging for genEpi domain experts to conduct data reconnaissance; that is, have an overview of the data they have and make assessments toward its quality, completeness, and suitability. We present an algorithm for data reconnaissance through automatic visualization recommendation, GEViTRec. Our approach handles a broad variety of dataset types and automatically generates visually coherent combinations of charts, in contrast to existing systems that primarily focus on singleton visual encodings of tabular datasets. We automatically detect linkages across multiple input datasets by analyzing non-numeric attribute fields, creating a data source graph within which we analyze and rank paths. For each high-ranking path, we specify chart combinations with positional and color alignments between shared fields, using a gradual binding approach to transform initial partial specifications of singleton charts to complete specifications that are aligned and oriented consistently. A novel aspect of our approach is its combination of domain-agnostic elements with domain-specific information that is captured through a domain-specific visualization prevalence design space. Our implementation is applied to both synthetic data and real Ebola outbreak data. We compare GEViTRec\u2019s output to what previous visualization recommendation systems would generate, and to manually crafted visualizations used by practitioners. We conducted formative evaluations with ten genEpi experts to assess the relevance and interpretability of our results.",
        "uid": "v-tvcg-9524484",
        "file_name": "",
        "time_stamp": "2022-10-21T03:55:00Z",
        "time_start": "2022-10-21T03:55:00Z",
        "time_end": "2022-10-21T03:57:00Z"
    },
    "v-full-1010": {
        "slot_id": "v-full-1010-qa",
        "session_id": "full32",
        "type": "In Person Q+A",
        "title": "FoVolNet: Fast Volume Rendering using Foveated Deep Neural Networks (Q+A)",
        "contributors": [
            "David Bauer"
        ],
        "authors": [
            "David Bauer",
            "Qi Wu",
            "Kwan-Liu Ma"
        ],
        "abstract": "",
        "uid": "v-full-1010",
        "file_name": "",
        "time_stamp": "2022-10-20T01:10:00Z",
        "time_start": "2022-10-20T01:10:00Z",
        "time_end": "2022-10-20T01:12:00Z"
    },
    "v-full-1418": {
        "slot_id": "v-full-1418-qa",
        "session_id": "full32",
        "type": "Virtual Q+A",
        "title": "GRay: Ray Casting for Visualization and Interactive Data Exploration of Gaussian Mixture Models (Q+A)",
        "contributors": [
            "Kai Lawonn"
        ],
        "authors": [
            "Kai Lawonn",
            "Monique Meuschke",
            "Pepe Eulzer",
            "Matthias Mitterreiter",
            "Joachim Giesen",
            "Tobias G\u00fcnther"
        ],
        "abstract": "",
        "uid": "v-full-1418",
        "file_name": "",
        "time_stamp": "2022-10-20T01:22:00Z",
        "time_start": "2022-10-20T01:22:00Z",
        "time_end": "2022-10-20T01:24:00Z"
    },
    "v-full-1339": {
        "slot_id": "v-full-1339-qa",
        "session_id": "full32",
        "type": "In Person Q+A",
        "title": "Quick Clusters: A GPU-Parallel Partitioning for Efficient Path Tracing of Unstructured Volumetric Grids (Q+A)",
        "contributors": [
            "Nate Morrical"
        ],
        "authors": [
            "Nate Morrical",
            "Alper Sahistan",
            "Ugur Gudukbay",
            "Ingo Wald",
            "Valerio Pascucci"
        ],
        "abstract": "",
        "uid": "v-full-1339",
        "file_name": "",
        "time_stamp": "2022-10-20T01:34:00Z",
        "time_start": "2022-10-20T01:34:00Z",
        "time_end": "2022-10-20T01:36:00Z"
    },
    "v-tvcg-9806341": {
        "slot_id": "v-tvcg-9806341-qa",
        "session_id": "full32",
        "type": "In Person Q+A",
        "title": "Finding Nano-\u00d6tzi: Cryo-Electron Tomography Visualization Guided by Learned Segmentation (Q+A)",
        "contributors": [
            "Ngan Nguyen"
        ],
        "authors": [
            "Ngan Nguyen",
            "Ciril Bohak",
            "Dominik Engel",
            "Peter Mindek",
            "Ond\u0159ej Strnad",
            "Peter Wonka",
            "Sai Li",
            "Timo Ropinski",
            "Ivan Viola"
        ],
        "abstract": "Cryo-electron tomography (cryo-ET) is a new 3D imaging technique with unprecedented potential for resolving submicron structural details. Existing volume visualization methods, however, are not able to reveal details of interest due to low signal-to-noise ratio. In order to design more powerful transfer functions, we propose leveraging soft segmentation as an explicit component of visualization for noisy volumes. Our technical realization is based on semi-supervised learning, where we combine the advantages of two segmentation algorithms. First, the weak segmentation algorithm provides good results for propagating sparse user-provided labels to other voxels in the same volume and is used to generate dense pseudo-labels. Second, the powerful deep-learning-based segmentation algorithm learns from these pseudo-labels to generalize the segmentation to other unseen volumes, a task that the weak segmentation algorithm fails at completely. The proposed volume visualization uses deep-learning-based segmentation as a component for segmentation-aware transfer function design. Appropriate ramp parameters can be suggested automatically through frequency distribution analysis. Furthermore, our visualization uses gradient-free ambient occlusion shading to further suppress the visual presence of noise, and to give structural detail the desired prominence. The cryo-ET data studied in our technical experiments are based on the highest-quality tilted series of intact SARS-CoV-2 virions. Our technique shows the high impact in target sciences for visual data analysis of very noisy volumes that cannot be visualized with existing techniques.",
        "uid": "v-tvcg-9806341",
        "file_name": "",
        "time_stamp": "2022-10-20T01:46:00Z",
        "time_start": "2022-10-20T01:46:00Z",
        "time_end": "2022-10-20T01:48:00Z"
    },
    "v-full-1496": {
        "slot_id": "v-full-1496-qa",
        "session_id": "full32",
        "type": "Virtual Q+A",
        "title": "Level Set Restricted Voronoi Decomposition for Large Scale Spatial Statistical Analysis (Q+A)",
        "contributors": [
            "Tyson Neuroth"
        ],
        "authors": [
            "Tyson Neuroth",
            "Martin Rieth",
            "Myoungkyu Lee",
            "Konduri Aditya",
            "Jacqueline Chen",
            "Kwan-Liu Ma"
        ],
        "abstract": "",
        "uid": "v-full-1496",
        "file_name": "",
        "time_stamp": "2022-10-20T01:58:00Z",
        "time_start": "2022-10-20T01:58:00Z",
        "time_end": "2022-10-20T02:00:00Z"
    },
    "v-tvcg-9770381": {
        "slot_id": "v-tvcg-9770381-qa",
        "session_id": "full32",
        "type": "In Person Q+A",
        "title": "Watertight Incremental Heightfield Tessellation (Q+A)",
        "contributors": [
            "Daniel Cornel"
        ],
        "authors": [
            "Daniel Cornel; Silvana Zechmeister; Eduard Gr\u00f6ller; J\u00fcrgen Waser"
        ],
        "abstract": "In this paper, we propose a method for the interactive visualization of medium-scale dynamic heightfields without visual artifacts. Our data fall into a category too large to be rendered directly at full resolution, but small enough to fit into GPU memory without pre-filtering and data streaming. We present the real-world use case of unfiltered flood simulation data of such medium scale that need to be visualized in real time for scientific purposes. Our solution facilitates compute shaders to maintain a guaranteed watertight triangulation in GPU memory that approximates the interpolated heightfields with view-dependent, continuous levels of detail. In each frame, the triangulation is updated incrementally by iteratively refining the cached result of the previous frame to minimize the computational effort. In particular, we minimize the number of heightfield sampling operations to make adaptive and higher-order interpolations viable options. We impose no restriction on the number of subdivisions and the achievable level of detail to allow for extreme zoom ranges required in geospatial visualization. Our method provides a stable runtime performance and can be executed with a limited time budget. We present a comparison of our method to three state-of-the-art methods, in which our method is competitive to previous non-watertight methods in terms of runtime, while outperforming them in terms of accuracy.",
        "uid": "v-tvcg-9770381",
        "file_name": "",
        "time_stamp": "2022-10-20T02:10:00Z",
        "time_start": "2022-10-20T02:10:00Z",
        "time_end": "2022-10-20T02:12:00Z"
    },
    "v-short-1012": {
        "slot_id": "v-short-1012-qa",
        "session_id": "short1",
        "type": "In Person Q+A",
        "title": "Facilitating Conversational Interaction in Natural Language Interfaces for Visualization (Q+A)",
        "contributors": [
            "Arpit Narechania"
        ],
        "authors": [
            "Rishab Mitra",
            "Arpit Narechania",
            "Alex Endert",
            "John Stasko"
        ],
        "abstract": "Natural language (NL) toolkits enable visualization developers, who may not have a background in natural language processing (NLP), to create natural language interfaces (NLIs) for end-users to flexibly specify and interact with visualizations. However, these toolkits currently only support one-off utterances, with minimal capability to facilitate a multi-turn dialog between the user and the system. Developing NLIs with such conversational interaction capabilities remains a challenging task, requiring implementations of low-level NLP techniques to process a new query as an intent to follow-up on an older query. We extend an existing Python-based toolkit, NL4DV, that processes an NL query about a tabular dataset and returns an analytic specification containing data attributes, analytic tasks, and relevant visualizations, modeled as a JSON object. Specifically, NL4DV now enables developers to facilitate multiple simultaneous conversations about a dataset and resolve associated ambiguities, augmenting new conversational information into the output JSON object. We demonstrate these capabilities through three examples: (1) an NLI to learn aspects of the Vega-Lite grammar, (2) a mind mapping application to create free-flowing conversations, and (3) a chatbot to answer questions and resolve ambiguities.",
        "uid": "v-short-1012",
        "file_name": "",
        "time_stamp": "2022-10-20T01:07:00Z",
        "time_start": "2022-10-20T01:07:00Z",
        "time_end": "2022-10-20T01:09:00Z"
    },
    "v-short-1044": {
        "slot_id": "v-short-1044-qa",
        "session_id": "short1",
        "type": "In Person Q+A",
        "title": "VegaFusion: Automatic Server-Side Scaling for Interactive Vega Visualizations (Q+A)",
        "contributors": [
            "Nicolas Kruchten"
        ],
        "authors": [
            "Nicolas Kruchten",
            "Jon Mease",
            "Dominik Moritz"
        ],
        "abstract": "The Vega grammar has been broadly adopted by a growing ecosystem of browser-based visualization tools. However, the reference Vega renderer does not scale well to large datasets (e.g., millions of rows or hundreds of megabytes) because it requires the entire dataset to be loaded into browser memory. We introduce VegaFusion, which brings automatic server-side scaling to the Vega ecosystem. VegaFusion accepts generic Vega specifications and partitions the required computation between the client and an out-of-browser, natively-compiled server-side process. Large datasets can be processed server-side to avoid loading them into the browser and to take advantage of multi-threading, more powerful server hardware and caching. We demonstrate how VegaFusion can be integrated into the existing Vega ecosystem, and show that VegaFusion greatly outperforms the reference implementation. We demonstrate these benefits with VegaFusion running on the same machine as the client as well as on a remote machine.",
        "uid": "v-short-1044",
        "file_name": "",
        "time_stamp": "2022-10-20T01:16:00Z",
        "time_start": "2022-10-20T01:16:00Z",
        "time_end": "2022-10-20T01:18:00Z"
    },
    "v-short-1098": {
        "slot_id": "v-short-1098-qa",
        "session_id": "short1",
        "type": "In Person Q+A",
        "title": "Streamlining Visualization Authoring in D3 Through User-Driven Templates (Q+A)",
        "contributors": [
            "Hannah K. Bako"
        ],
        "authors": [
            "Hannah K. Bako",
            "Alisha Varma",
            "Anuoluwapo Faboro",
            "Mahreen Haider",
            "Favour Nerrise",
            "Bissaka Kenah",
            "Leilani Battle"
        ],
        "abstract": "D3 is arguably the most popular tool for implementing web-based visualizations. Yet D3 has a steep learning curve that may hinder its adoption and continued use. To simplify the process of programming D3 visualizations, we must first understand the space of implementation practices that D3 users engage in. We present a qualitative analysis of 2500 D3 visualizations and their corresponding implementations. We find that 5 visualization types (Bar Charts, Geomaps, Line Charts, Scatterplots, and Force Directed Graphs) account for 80% of D3 visualizations found in our corpus. While implementation styles vary slightly across designs, the underlying code structure for all visualization types remains the same; presenting an opportunity for code reuse. Using our corpus of D3 examples, we synthesize reusable code templates for eight popular D3 visualization types and share them in our open source repository. Based on our results, we discuss design considerations for leveraging users\u2019 implementation patterns to reduce visualization design effort through design templates and auto-generated code recommendations.",
        "uid": "v-short-1098",
        "file_name": "",
        "time_stamp": "2022-10-20T01:25:00Z",
        "time_start": "2022-10-20T01:25:00Z",
        "time_end": "2022-10-20T01:27:00Z"
    },
    "v-short-1021": {
        "slot_id": "v-short-1021-qa",
        "session_id": "short1",
        "type": "In Person Q+A",
        "title": "Plotly-Resampler: Effective Visual Analytics for Large Time Series (Q+A)",
        "contributors": [
            "Jonas Van Der Donckt"
        ],
        "authors": [
            "Jonas Van Der Donckt",
            "Jeroen Van Der Donckt",
            "Emiel Deprost",
            "Sofie Van Hoecke"
        ],
        "abstract": "Visual analytics is arguably the most important step in getting acquainted with your data. This is especially the case for time series, as this data type is hard to describe and cannot be fully understood when using for example summary statistics. To realize effective time series visualization, four requirements have to be met; a tool should be (1) interactive, (2) scalable to millions of data points, (3) integrable in conventional data science environments, and (4) highly configurable. We observe that open source Python visualization toolkits empower data scientists in most visual analytics tasks, but lack the combination of scalability and interactivity to realize effective time series visualization. As a means to facilitate these requirements, we created Plotly-Resampler, an open source Python library. Plotly-Resampler is an add-on for Plotly's Python bindings, enhancing line chart scalability on top of an interactive toolkit by aggregating the underlying data depending on the current graph view. Plotly-Resampler is built to be snappy, as the reactivity of a tool qualitatively affects how analysts visually explore and analyze data. A benchmark task highlights how our toolkit scales better than alternatives in terms of number of samples and time series. Additionally, Plotly-Resampler's flexible data aggregation functionality paves the path towards researching novel aggregation techniques. Plotly-Resampler's integrability, together with its configurability, convenience, and high scalability, allows to effectively analyze high-frequency data in your day-to-day Python environment.",
        "uid": "v-short-1021",
        "file_name": "",
        "time_stamp": "2022-10-20T01:34:00Z",
        "time_start": "2022-10-20T01:34:00Z",
        "time_end": "2022-10-20T01:36:00Z"
    },
    "v-short-1011": {
        "slot_id": "v-short-1011-qa",
        "session_id": "short1",
        "type": "In Person Q+A",
        "title": "Explaining Website Reliability by Visualizing Hyperlink Connectivity (Q+A)",
        "contributors": [
            "Seongmin Lee"
        ],
        "authors": [
            "Seongmin Lee",
            "Sadia Afroz",
            "Haekyu Park",
            "Zijie J. Wang",
            "Omar Shaikh",
            "Vibhor Sehgal",
            "Ankit Peshin",
            "Duen Horng Chau"
        ],
        "abstract": "As the information on the Internet continues growing exponentially, understanding and assessing the reliability of a website is becoming increasingly important. Misinformation has far-ranging repercussions, from sowing mistrust in media to undermining democratic elections. While some research investigates how to alert people to misinformation on the web, much less research has been conducted on explaining how websites engage in spreading false information. To fill the research gap, we present MisVis, a web-based interactive visualization tool that helps users assess a website\u2019s reliability by understanding how it engages in spreading false information on the World Wide Web. MisVis visualizes the hyperlink connectivity of the website and summarizes key characteristics of the Twitter accounts that mention the site. A large-scale user study with 139 participants demonstrates that MisVis facilitates users to assess and understand false information on the web and node-link diagrams can be used to communicate with non-experts. MisVis is available at the public demo link: https://poloclub.github.io/MisVis.",
        "uid": "v-short-1011",
        "file_name": "",
        "time_stamp": "2022-10-20T01:43:00Z",
        "time_start": "2022-10-20T01:43:00Z",
        "time_end": "2022-10-20T01:45:00Z"
    },
    "v-short-1137": {
        "slot_id": "v-short-1137-qa",
        "session_id": "short1",
        "type": "In Person Q+A",
        "title": "Paths through Spatial Networks (Q+A)",
        "contributors": [
            "Alex Godwin"
        ],
        "authors": [
            "Alex Godwin"
        ],
        "abstract": "Spatial networks present unique challenges to understanding topological structure. Each node occupies a location in physical space (e.g., longitude and latitude) and each link may either indicate a fixed and well-described path (e.g., along streets) or a logical connection only. Placing these elements in a map maintains these physical relationships while making it more difficult to identify topological features of interest such as clusters, cliques, and paths. While some systems provide coordinated representations of a spatial network, it is almost universally assumed that the network itself is the sole mechanism for movement across space. In this paper, I present a novel approach for exploring spatial networks by orienting them along a point or path in physical space that provides the guide for parameters in a force-directed layout. By specifying a path across topography, networks can be spatially filtered independently of the topology of the network. Initial case studies indicate promising results for exploring spatial networks in transportation and energy distribution.",
        "uid": "v-short-1137",
        "file_name": "",
        "time_stamp": "2022-10-20T01:52:00Z",
        "time_start": "2022-10-20T01:52:00Z",
        "time_end": "2022-10-20T01:54:00Z"
    },
    "v-short-1070": {
        "slot_id": "v-short-1070-qa",
        "session_id": "short1",
        "type": "Virtual Q+A",
        "title": "LineCap: Line Charts for Data Visualization Captioning Models (Q+A)",
        "contributors": [
            "Anita Mahinpei"
        ],
        "authors": [
            "Anita Mahinpei",
            "Zona Kostic",
            "Chris Tanner"
        ],
        "abstract": "Data visualization captions help readers understand the purpose of a visualization and are crucial for individuals with visual impairments. The prevalence of poor figure captions and the successful application of deep learning approaches to image captioning motivate the use of similar techniques for automated figure captioning. However, research in this field has been stunted by the lack of suitable datasets. We introduce LineCap, a novel figure captioning dataset of 3,528 figures, and we provide insights from curating this dataset and using end-to-end deep learning models for automated figure captioning.",
        "uid": "v-short-1070",
        "file_name": "",
        "time_stamp": "2022-10-20T02:01:00Z",
        "time_start": "2022-10-20T02:01:00Z",
        "time_end": "2022-10-20T02:03:00Z"
    },
    "v-short-1048": {
        "slot_id": "v-short-1048-qa",
        "session_id": "short1",
        "type": "In Person Q+A",
        "title": "Intentable: A Mixed-Initiative System for Intent-Based Chart Captioning (Q+A)",
        "contributors": [
            "Jiwon Choi"
        ],
        "authors": [
            "Jiwon Choi",
            "Jaemin Jo"
        ],
        "abstract": "We present Intentable, a mixed-initiative caption authoring system that allows the author to steer an automatic caption generation process to reflect their intent, e.g., the finding that the author gained from visualization and thus wants to write a caption for. We first derive a grammar for specifying the intent, i.e., a caption recipe, and build a neural network that generates caption sentences given a recipe. Our quantitative evaluation revealed that our intent-based generation system not only allows the author to engage in the generation process but also produces more fluent captions than the previous end-to-end approaches without user intervention. Finally, we demonstrate the versatility of our system, such as context adaptation, unit conversion, and sentence reordering.",
        "uid": "v-short-1048",
        "file_name": "",
        "time_stamp": "2022-10-20T02:10:00Z",
        "time_start": "2022-10-20T02:10:00Z",
        "time_end": "2022-10-20T02:12:00Z"
    },
    "v-short-1004": {
        "slot_id": "v-short-1004-qa",
        "session_id": "short2",
        "type": "Virtual Q+A",
        "title": "Visual Auditor: Interactive Visualization for Detection and Summarization of Model Biases (Q+A)",
        "contributors": [
            "David Munechika"
        ],
        "authors": [
            "David Munechika",
            "Zijie J. Wang",
            "Jack Reidy",
            "Josh Rubin",
            "Krishna Gade",
            "Krishnaram Kenthapadi",
            "Duen Horng Chau"
        ],
        "abstract": "As machine learning (ML) systems become increasingly widespread, it is necessary to audit these systems for biases prior to their deployment. Recent research has developed algorithms for effectively identifying intersectional bias in the form of interpretable, underperforming subsets (or slices) of the data. However, these solutions and their insights are limited without a tool for visually understanding and interacting with the results of these algorithms. We propose Visual Auditor, an interactive visualization tool for auditing and summarizing model biases. Visual Auditor assists model validation by providing an interpretable overview of intersectional bias (bias that is present when examining populations defined by multiple features), details about relationships between problematic data slices, and a comparison between underperforming and overperforming data slices in a model. Our open-source tool runs directly in both computational notebooks and web browsers, making model auditing accessible and easily integrated into current ML development workflows. An observational user study in collaboration with domain experts at Fiddler AI highlights that our tool can help ML practitioners identify and understand model biases.",
        "uid": "v-short-1004",
        "file_name": "",
        "time_stamp": "2022-10-20T02:52:00Z",
        "time_start": "2022-10-20T02:52:00Z",
        "time_end": "2022-10-20T02:54:00Z"
    },
    "v-short-1076": {
        "slot_id": "v-short-1076-qa",
        "session_id": "short2",
        "type": "In Person Q+A",
        "title": "RMExplorer: A Visual Analytics Approach to Explore the Performance and the Fairness of Disease Risk Models on Population Subgroups (Q+A)",
        "contributors": [
            "Bum Chul Kwon"
        ],
        "authors": [
            "Bum Chul Kwon",
            "Uri Kartoun",
            "Shaan Khurshid",
            "Mikhail Yurochkin",
            "Subha Maity",
            "Deanna G Brockman",
            "Amit V Khera",
            "Patrick T Ellinor",
            "Steven A Lubitz",
            "Kenney Ng"
        ],
        "abstract": "Disease risk models can identify high-risk patients and help clinicians provide more personalized care. However, risk models developed on one dataset may not generalize across diverse subpopulations of patients in different datasets and may have unexpected performance. It is challenging for clinical researchers to inspect risk models across different subgroups without any tools. Therefore, we developed an interactive visualization system called RMExplorer (Risk Model Explorer) to enable interactive risk model assessment. Specifically, the system allows users to define subgroups of patients by selecting clinical, demographic, or other characteristics, to explore the performance and fairness of risk models on the subgroups, and to understand the feature contributions to risk scores. To demonstrate the usefulness of the tool, we conduct a case study, where we use RMExplorer to explore three atrial fibrillation risk models by applying them to the UK Biobank dataset of 445,329 individuals. RMExplorer can help researchers to evaluate the performance and biases of risk models on subpopulations of interest in their data.",
        "uid": "v-short-1076",
        "file_name": "",
        "time_stamp": "2022-10-20T03:01:00Z",
        "time_start": "2022-10-20T03:01:00Z",
        "time_end": "2022-10-20T03:03:00Z"
    },
    "v-short-1105": {
        "slot_id": "v-short-1105-qa",
        "session_id": "short2",
        "type": "Virtual Q+A",
        "title": "Visualizing Rule-based Classifiers for Clinical Risk Prognosis (Q+A)",
        "contributors": [
            "Dario Antweiler"
        ],
        "authors": [
            "Dario Antweiler",
            "Georg Fuchs"
        ],
        "abstract": "Deteriorating conditions in hospital patients are a major factor in clinical patient mortality. Currently, timely detection is based on clinical experience, expertise, and attention. However, healthcare trends towards larger patient cohorts, more data, and the desire for better and more personalized care are pushing the existing, simple scoring systems to their limits. Data-driven approaches can extract decision rules from available medical coding data, which offer good interpretability and thus are key for successful adoption in practice. Before deployment, models need to be scrutinized by domain experts to identify errors and check them against existing medical knowledge. We propose a visual analytics system to support healthcare professionals in inspecting and enhancing rule-based classifier through identification of similarities and contradictions, as well as modification of rules. This work was developed iteratively in close collaboration with medical professionals. We discuss how our tool supports the inspection and assessment of rule-based classifiers in the clinical coding domain and propose possible extensions.",
        "uid": "v-short-1105",
        "file_name": "",
        "time_stamp": "2022-10-20T03:10:00Z",
        "time_start": "2022-10-20T03:10:00Z",
        "time_end": "2022-10-20T03:12:00Z"
    },
    "v-short-1006": {
        "slot_id": "v-short-1006-qa",
        "session_id": "short2",
        "type": "In Person Q+A",
        "title": "TimberTrek: Exploring and Curating Sparse Decision Trees with Interactive Visualization (Q+A)",
        "contributors": [
            "Zijie J. Wang"
        ],
        "authors": [
            "Zijie J. Wang",
            "Chudi Zhong",
            "Rui Xin",
            "Takuya Takagi",
            "Zhi Chen",
            "Duen Horng Chau",
            "Cynthia Rudin",
            "Margo Seltzer"
        ],
        "abstract": "Given thousands of equally accurate machine learning (ML) models, how can users choose among them? A recent ML technique enables domain experts and data scientists to generate a complete Rashomon set for sparse decision trees\u2014a huge set of almost-optimal interpretable ML models. To help ML practitioners identify models with desirable properties from this Rashomon set, we develop TimberTrek, the first interactive visualization system that summarizes thousands of sparse decision trees at scale. Two usage scenarios highlight how TimberTrek can empower users to easily explore, compare, and curate models that align with their domain knowledge and values. Our open-source tool runs directly in users' computational notebooks and web browsers, lowering the barrier to creating more responsible ML models. TimberTrek is available at the following public demo link: https://poloclub.github.io/timbertrek.",
        "uid": "v-short-1006",
        "file_name": "",
        "time_stamp": "2022-10-20T03:19:00Z",
        "time_start": "2022-10-20T03:19:00Z",
        "time_end": "2022-10-20T03:21:00Z"
    },
    "v-short-1083": {
        "slot_id": "v-short-1083-qa",
        "session_id": "short2",
        "type": "In Person Q+A",
        "title": "FairFuse: Interactive Visual Support for Fair Consensus Ranking (Q+A)",
        "contributors": [
            "Hilson Shrestha"
        ],
        "authors": [
            "Hilson Shrestha",
            "Kathleen Cachel",
            "Mallak Alkhathlan",
            "Elke A Rundensteiner",
            "Lane Harrison"
        ],
        "abstract": "Fair consensus building combines the preferences of multiple rankers into a single consensus ranking, while ensuring any ranked candidate group defined by a protected attribute (such as race or gender) is not disadvantaged compared to other groups. Manually generating a fair consensus ranking is time-consuming and impractical\u2014 even for a fairly small number of candidates. While algorithmic approaches for auditing and generating fair consensus rankings have been developed recently, these have not been operationalized in interactive systems. To bridge this gap, we introduce FairFuse, a visualization-enabled tool for generating, analyzing, and auditing fair consensus rankings. In developing FairFuse, we construct a data model which includes several base rankings entered by rankers, augmented with measures of group fairness, algorithms for generating consensus rankings with varying degrees of fairness, and other fairness and rank-related capabilities. We design novel visualizations that encode these measures in a parallel-coordinates style rank visualization, with interactive capabilities for generating and exploring fair consensus rankings. We provide case studies in which FairFuse supports a decision-maker in ranking scenarios in which fairness is important. Finally, we discuss emerging challenges for future efforts supporting fairness-oriented rank analysis; including handling intersectionality, defined by multiple protected attributes, and the need for user studies targeting peoples\u2019 perceptions and use of fairness oriented visualization systems. Code and demo videos available at https://osf.io/hd639/.",
        "uid": "v-short-1083",
        "file_name": "",
        "time_stamp": "2022-10-20T03:28:00Z",
        "time_start": "2022-10-20T03:28:00Z",
        "time_end": "2022-10-20T03:30:00Z"
    },
    "v-short-1041": {
        "slot_id": "v-short-1041-qa",
        "session_id": "short2",
        "type": "In Person Q+A",
        "title": "Guided Data Discovery in Interactive Visualizations via Active Search (Q+A)",
        "contributors": [
            "Shayan Monadjemi"
        ],
        "authors": [
            "Shayan Monadjemi",
            "Sunwoo Ha",
            "Quan Nguyen",
            "Henry Chai",
            "Roman Garnett",
            "Alvitta Ottley"
        ],
        "abstract": "Recent advances in visual analytics have enabled us to learn from user interactions and uncover analytic goals. These innovations set the foundation for actively guiding users during data exploration. Providing such guidance will become more critical as datasets grow in size and complexity, precluding exhaustive investigation. Meanwhile, the machine learning community also struggles with datasets growing in size and complexity, precluding exhaustive labeling. Active learning is a broad family of algorithms developed for actively guiding models during training. We will consider the intersection of these analogous research thrusts. First, we discuss the nuances of matching the choice of an active learning algorithm to the task at hand. This is critical for performance, a fact we demonstrate in a simulation study. We then present results of a user study for the particular task of data discovery guided by an active learning algorithm specifically designed for this task.",
        "uid": "v-short-1041",
        "file_name": "",
        "time_stamp": "2022-10-20T03:37:00Z",
        "time_start": "2022-10-20T03:37:00Z",
        "time_end": "2022-10-20T03:39:00Z"
    },
    "v-short-1028": {
        "slot_id": "v-short-1028-qa",
        "session_id": "short2",
        "type": "In Person Q+A",
        "title": "Parametric Dimension Reduction by Preserving Local Structure (Q+A)",
        "contributors": [
            "Yu-Shuen Wang",
            "Yun Hsuan Lien"
        ],
        "authors": [
            "Chien-Hsun Lai",
            "Ming-Feng Kuo",
            "Yun-Hsuan Lien",
            "Kuan-An Su",
            "Yu-Shuen Wang"
        ],
        "abstract": "We extend a well-known dimension reduction method, t-distributed stochastic neighbor embedding (t-SNE), from non-parametric to parametric by training neural networks. The main advantage of a parametric technique is the generalization of handling new data, which is beneficial for streaming data visualization. While previous parametric methods either require a network pre-training by the restricted Boltzmann machine or intermediate results obtained from the traditional non-parametric t-SNE, we found that recent network training skills can enable a direct optimization for the t-SNE objective function. Accordingly, our method achieves high embedding quality while enjoying generalization. Due to mini-batch network training, our parametric dimension reduction method is highly efficient. For evaluation, we compared our method to several baselines on a variety of datasets. Experiment results demonstrate the feasibility of our method. The source code is available at https://github.com/a07458666/parametric_dr.",
        "uid": "v-short-1028",
        "file_name": "",
        "time_stamp": "2022-10-20T03:46:00Z",
        "time_start": "2022-10-20T03:46:00Z",
        "time_end": "2022-10-20T03:48:00Z"
    },
    "v-short-1047": {
        "slot_id": "v-short-1047-qa",
        "session_id": "short2",
        "type": "In Person Q+A",
        "title": "Uniform Manifold Approximation with Two-phase Optimization (Q+A)",
        "contributors": [
            "Hyeon Jeon"
        ],
        "authors": [
            "Hyeon Jeon",
            "Hyung-Kwon Ko",
            "Soohyun Lee",
            "Jaemin Jo",
            "Jinwook Seo"
        ],
        "abstract": "We introduce Uniform Manifold Approximation with Two-phase Optimization (UMATO), a dimensionality reduction (DR) technique that improves UMAP to capture the global structure of high-dimensional data more accurately. In UMATO, optimization is divided into two phases so that the resulting embeddings can depict the global structure reliably while preserving the local structure with sufficient accuracy. In the first phase, hub points are identified and projected to construct a skeletal layout for the global structure. In the second phase, the remaining points are added to the embedding preserving the regional characteristics of local areas. Through quantitative experiments, we found that UMATO (1) outperformed widely used DR techniques in preserving the global structure while (2) producing competitive accuracy in representing the local structure. We also verified that UMATO is preferable in terms of robustness over diverse initialization methods, numbers of epochs, and subsampling techniques.",
        "uid": "v-short-1047",
        "file_name": "",
        "time_stamp": "2022-10-20T03:55:00Z",
        "time_start": "2022-10-20T03:55:00Z",
        "time_end": "2022-10-20T03:57:00Z"
    },
    "v-short-1072": {
        "slot_id": "v-short-1072-qa",
        "session_id": "short3",
        "type": "In Person Q+A",
        "title": "Color Coding of Large Value Ranges Applied to Meteorological Data (Q+A)",
        "contributors": [
            "Daniel Braun"
        ],
        "authors": [
            "Daniel Braun",
            "Kerstin Ebell",
            "Vera Schemann",
            "Laura Pelchmann",
            "Susanne Crewell",
            "Rita Borgo",
            "Tatiana von Landesberger"
        ],
        "abstract": "This paper presents a novel color scheme designed to address the challenge of visualizing data series with large value ranges, where scale transformation provides limited support. We focus on meteorological data, where the presence of large value ranges is common. We apply our approach to meteorological scatterplots, as one of the most common plots used in this domain area. Our approach leverages the numerical representation of mantissa and exponent of the values to guide the design of novel ``nested'' color schemes, able to emphasize differences between magnitudes. Our user study evaluates the new designs, the state of the art color scales and representative color schemes used in the analysis of meteorological data: ColorCrafter, Viridis, and Rainbow. We assess accuracy, time and confidence in the context of discrimination (comparison) and interpretation (reading) tasks. Our proposed color scheme significantly outperforms the others in interpretation tasks, while showing comparable performances in discrimination tasks.",
        "uid": "v-short-1072",
        "file_name": "",
        "time_stamp": "2022-10-21T01:07:00Z",
        "time_start": "2022-10-21T01:07:00Z",
        "time_end": "2022-10-21T01:09:00Z"
    },
    "v-short-1085": {
        "slot_id": "v-short-1085-qa",
        "session_id": "short3",
        "type": "Virtual Q+A",
        "title": "Volume Puzzle: visual analysis of segmented volume data with multivariate attributes (Q+A)",
        "contributors": [
            "Marco Agus"
        ],
        "authors": [
            "Marco Agus",
            "Amal Aboulhassan",
            "Khaled Ahmed Lutf Al-Thelaya",
            "Giovanni Pintore",
            "Enrico Gobbetti",
            "Corrado Cali'",
            "Jens Schneider"
        ],
        "abstract": "A variety of application domains, including material science, neuroscience, and connectomics, commonly use segmented volume data for explorative visual analysis. In many cases, segmented objects are characterized by multivariate attributes expressing specific geometric or physical features. Objects with similar characteristics, determined by selected attribute configurations, can create peculiar spatial patterns, whose detection and study is of fundamental importance. This task is notoriously difficult, especially when the number of attributes per segment is large. In this work, we propose an interactive framework that combines a state-of-the-art direct volume renderer for categorical volumes with techniques for the analysis of the attribute space and for the automatic creation of 2D transfer function. We show, in particular, how dimensionality reduction, kernel-density estimation, and topological techniques such as Morse analysis combined with scatter and density plots allow the efficient design of two-dimensional color maps that highlight spatial patterns. The capabilities of our framework are demonstrated on synthetic and real-world data from several domains.",
        "uid": "v-short-1085",
        "file_name": "",
        "time_stamp": "2022-10-21T01:16:00Z",
        "time_start": "2022-10-21T01:16:00Z",
        "time_end": "2022-10-21T01:18:00Z"
    },
    "v-short-1075": {
        "slot_id": "v-short-1075-qa",
        "session_id": "short3",
        "type": "In Person Q+A",
        "title": "Droplet-Local Line Integration for Multiphase Flow (Q+A)",
        "contributors": [
            "Alexander Straub"
        ],
        "authors": [
            "Alexander Straub",
            "Sebastian Boblest",
            "Grzegorz Karch",
            "Filip Sadlo",
            "Thomas Ertl"
        ],
        "abstract": "Line integration of stream-, streak-, and pathlines is widely used and popular for visualizing single-phase flow. In multiphase flow, i.e., where the fluid consists, e.g., of a liquid and a gaseous phase, these techniques could also provide valuable insights into the internal flow of droplets and ligaments, and thus into their dynamics. However, since such structures tend to act as entities, high translational and rotational velocities often obfuscate their detail. As a remedy, we present a method for deriving a droplet-local velocity field, using a decomposition of the original velocity field removing translational and rotational velocity parts, and adapt path- and streaklines. Generally, the resulting integral lines are thus shorter and less tangled, which simplifies their analysis. We demonstrate and discuss the utility of our approach on droplets in two-phase flow data, and visualize the removed velocity parts employing glyphs for context.",
        "uid": "v-short-1075",
        "file_name": "",
        "time_stamp": "2022-10-21T01:25:00Z",
        "time_start": "2022-10-21T01:25:00Z",
        "time_end": "2022-10-21T01:27:00Z"
    },
    "v-short-1037": {
        "slot_id": "v-short-1037-qa",
        "session_id": "short3",
        "type": "In Person Q+A",
        "title": "Efficient Interpolation-based Pathline Tracing with B-spline Curves in Particle Dataset (Q+A)",
        "contributors": [
            "Haoyu Li"
        ],
        "authors": [
            "Haoyu Li",
            "Tianyu Xiong",
            "Han-Wei Shen"
        ],
        "abstract": "Particle tracing through numerical integration is a well-known approach to generating pathlines for visualization. However, for particle simulations, the computation of pathlines is expensive, since the interpolation method is complicated due to the lack of connectivity information. Previous studies utilize the $k$-d tree to reduce the time for neighborhood search. However, the efficiency is still limited by the number of tracing time steps. Therefore, we propose a novel interpolation-based particle tracing method that first represents particle data as B-spline curves and interpolates B-spline control points to reduce the number of interpolation time steps. We demonstrate our approach achieves good tracing accuracy with much less computation time.",
        "uid": "v-short-1037",
        "file_name": "",
        "time_stamp": "2022-10-21T01:34:00Z",
        "time_start": "2022-10-21T01:34:00Z",
        "time_end": "2022-10-21T01:36:00Z"
    },
    "v-short-1081": {
        "slot_id": "v-short-1081-qa",
        "session_id": "short3",
        "type": "In Person Q+A",
        "title": "Visualizing Confidence Intervals for Critical Point Probabilities in 2D Scalar Field Ensembles (Q+A)",
        "contributors": [
            "Dominik Vietinghoff"
        ],
        "authors": [
            "Dominik Vietinghoff",
            "Michael B\u00f6ttinger",
            "Gerik Scheuermann",
            "Christian Heine"
        ],
        "abstract": "An important task in visualization is the extraction and highlighting of dominant features in data to support users in their analysis process. Topological methods are a well-known means of identifying such features in deterministic fields. However, many real-world phenomena studied today are the result of a chaotic system that cannot be fully described by a single simulation. Instead, the variability of such systems is usually captured with ensemble simulations that produce a variety of possible outcomes of the simulated process. The topological analysis of such ensemble data sets and uncertain data, in general, is less well studied. In this work, we present an approach for the computation and visual representation of confidence intervals for the occurrence probabilities of critical points in ensemble data sets. We demonstrate the added value of our approach over existing methods for critical point prediction in uncertain data on a synthetic data set and show its applicability to a data set from climate research.",
        "uid": "v-short-1081",
        "file_name": "",
        "time_stamp": "2022-10-21T01:43:00Z",
        "time_start": "2022-10-21T01:43:00Z",
        "time_end": "2022-10-21T01:45:00Z"
    },
    "v-short-1103": {
        "slot_id": "v-short-1103-qa",
        "session_id": "short3",
        "type": "In Person Q+A",
        "title": "ASEVis: Visual Exploration of Active System Ensembles to Define Characteristic Measures (Q+A)",
        "contributors": [
            "Marina Evers"
        ],
        "authors": [
            "Marina Evers",
            "Raphael Wittkowski",
            "Lars Linsen"
        ],
        "abstract": "Simulation ensembles are a common tool in physics for understanding how a model outcome depends on input parameters. We analyze an active particle system, where each particle can use energy from its surroundings to propel itself. A multi-dimensional feature vector containing all particles' motion information can describe the whole system at each time step. The system's behavior strongly depends on input parameters like the propulsion mechanism of the particles. To understand how the time-varying behavior depends on the input parameters, it is necessary to introduce new measures to quantify the difference of the dynamics of the ensemble members. We propose a tool that supports the interactive visual analysis of time-varying feature-vector ensembles. A core component of our tool allows for the interactive definition and refinement of new measures that can then be used to understand the system's behavior and compare the ensemble members. Different visualizations support the user in finding a characteristic measure for the system. By visualizing the user-defined measure, the user can then investigate the parameter dependencies and gain insights into the relationship between input parameters and simulation output.",
        "uid": "v-short-1103",
        "file_name": "",
        "time_stamp": "2022-10-21T01:52:00Z",
        "time_start": "2022-10-21T01:52:00Z",
        "time_end": "2022-10-21T01:54:00Z"
    },
    "v-short-1110": {
        "slot_id": "v-short-1110-qa",
        "session_id": "short3",
        "type": "In Person Q+A",
        "title": "Accelerated Probabilistic Marching Cubes by Deep Learning for Time-Varying Scalar Ensembles (Q+A)",
        "contributors": [
            "Mengjiao Han"
        ],
        "authors": [
            "Mengjiao Han",
            "Tushar M. Athawale",
            "David Pugmire",
            "Chris R. Johnson"
        ],
        "abstract": "Visualizing the uncertainty of ensemble simulations is challenging due to the large size and multivariate and temporal features of ensemble data sets. One popular approach to studying the uncertainty of ensembles is analyzing the positional uncertainty of the level sets. Probabilistic marching cubes is a technique that performs Monte Carlo sampling of multivariate Gaussian noise distributions for positional uncertainty visualization of level sets. However, the technique suffers from high computational time, making interactive visualization and analysis impossible to achieve. This paper introduces a deep-learning-based approach to learning the level-set uncertainty for two-dimensional ensemble data with a multivariate Gaussian noise assumption. We train the model using the first few time steps from time-varying ensemble data in our workflow. We demonstrate that our trained model accurately infers uncertainty in level sets for new time steps and is up to 170X faster than that of the original probabilistic model with serial computation and 10X faster than that of the original parallel computation.",
        "uid": "v-short-1110",
        "file_name": "",
        "time_stamp": "2022-10-21T02:01:00Z",
        "time_start": "2022-10-21T02:01:00Z",
        "time_end": "2022-10-21T02:03:00Z"
    },
    "v-short-1055": {
        "slot_id": "v-short-1055-qa",
        "session_id": "short3",
        "type": "In Person Q+A",
        "title": "Beyond Visuals : Examining the Experiences of Geoscience Professionals With Vision Disabilities in Accessing Data Visualizations (Q+A)",
        "contributors": [
            "Dr Nihanth W Cherukuru"
        ],
        "authors": [
            "Nihanth W Cherukuru",
            "David Bailey",
            "Tiffany Fourment",
            "Becca Hatheway",
            "Marika Holland",
            "Matt Rehme"
        ],
        "abstract": "Data visualizations are ubiquitous in all disciplines and have become the primary means of analysing data and communicating insights. However, the predominant reliance on visual encoding of data continues to create accessibility barriers for people who are blind/vision impaired resulting in their under representation in Science, Technology, Engineering and Mathematics (STEM) disciplines. This research study seeks to understand the experiences of professionals who are blind/vision impaired in one such STEM discipline (geosciences) in accessing data visualizations. In-depth, semi-structured interviews with seven professionals were conducted to examine the accessibility barriers and areas for improvement to inform accessibility research pertaining to data visualizations through a socio-technical lens. A reflexive thematic analysis revealed the negative impact of visualizations in influencing their career path, lack of data exploration tools for research, barriers in accessing works of peers and mismatched pace of visualization and accessibility research. The article also includes recommendations from the participants to address some of these accessibility barriers.",
        "uid": "v-short-1055",
        "file_name": "",
        "time_stamp": "2022-10-21T02:10:00Z",
        "time_start": "2022-10-21T02:10:00Z",
        "time_end": "2022-10-21T02:12:00Z"
    },
    "v-short-1064": {
        "slot_id": "v-short-1064-qa",
        "session_id": "short4",
        "type": "In Person Q+A",
        "title": "Let's Get Personal: Exploring the Design of Personalized Visualizations (Q+A)",
        "contributors": [
            "Beleicia Bullock"
        ],
        "authors": [
            "Beleicia Bullock",
            "Shunan Guo",
            "Eunyee Koh",
            "Ryan Rossi",
            "Fan Du",
            "Jane Hoffswell"
        ],
        "abstract": "Media outlets often publish visualizations that can be personalized based on users\u2019 demographics, such as location, race, and age. However, the design of such personalized visualizations remains underexplored. In this work, we contribute a design space analysis of 47 public-facing articles with personalized visualizations to understand how designers structure content, encourage exploration, and present insights. We find that articles often lack explicit exploration suggestions or instructions, data notices, and personalized visual insights. We then outline three trajectories for future research: (1) explore how users choose to personalize visualizations, (2) examine how exploration suggestions and examples impact user interaction, and (3) investigate how personalization influences user insights.",
        "uid": "v-short-1064",
        "file_name": "",
        "time_stamp": "2022-10-20T20:07:00Z",
        "time_start": "2022-10-20T20:07:00Z",
        "time_end": "2022-10-20T20:09:00Z"
    },
    "v-short-1016": {
        "slot_id": "v-short-1016-qa",
        "session_id": "short4",
        "type": "Virtual Q+A",
        "title": "Who benefits from Visualization Adaptations? Towards a better Understanding of the Influence of Visualization Literacy (Q+A)",
        "contributors": [
            "Marc Satkowski"
        ],
        "authors": [
            "Marc Satkowski",
            "Franziska Kessler",
            "Susanne Narciss",
            "Raimund Dachselt"
        ],
        "abstract": "The ability to read, understand, and comprehend visual information representations is subsumed under the term visualization literacy (VL). One possibility to improve the use of information visualizations is to introduce adaptations. However, it is yet unclear whether people with different VL benefit from adaptations to the same degree. We conducted an online experiment (n = 42) to investigate whether the effect of an adaptation (here: De-Emphasis) of visualizations (bar charts, scatter plots) on performance (accuracy, time) and user experiences depends on users\u2019 VL level. Using linear mixed models for the analyses, we found a positive impact of the De-Emphasis adaptation across all conditions, as well as an interaction effect of adaptation and VL on the task completion time for bar charts. This work contributes to a better understanding of the intertwined relationship of VL and visual adaptations and motivates future research.",
        "uid": "v-short-1016",
        "file_name": "",
        "time_stamp": "2022-10-20T20:16:00Z",
        "time_start": "2022-10-20T20:16:00Z",
        "time_end": "2022-10-20T20:18:00Z"
    },
    "v-short-1141": {
        "slot_id": "v-short-1141-qa",
        "session_id": "short4",
        "type": "In Person Q+A",
        "title": "VisQuiz: Exploring Feedback Mechanisms to Improve Graphical Perception (Q+A)",
        "contributors": [
            "No\u00eblle Rakotondravony",
            "Yiren Ding"
        ],
        "authors": [
            "Ryan Birchfield",
            "Maddison Caten",
            "Errica Cheng",
            "Madyson Kelly",
            "Truman Larson",
            "Hoang Phan Pham",
            "Yiren Ding",
            "No\u00eblle Rakotondravony",
            "Lane Harrison"
        ],
        "abstract": "Graphical perception studies are a key element of visualization research, forming the basis of design recommendations and contributing to our understanding of how people make sense of visualizations. However, graphical perception studies typically include only brief training sessions, and the impact of longer and more in-depth feedback remains unclear. In this paper, we explore the design and evaluation of feedback for graphical perception tasks, called VisQuiz. Using a quiz-like metaphor, we design feedback for a typical visualization comparison experiment, showing participants their answer alongside the correct answer in an animated sequence in each trial. We extend this quiz metaphor to include summary feedback after each stage of the experiment, providing additional moments for participants to reflect on their performance. To evaluate VisQuiz, we conduct a between-subjects experiment, including three stages of 40 trials each with a control condition that included only summary feedback. Results from n = 80 participants show that once participants started receiving trial feedback (Stage 2) they performed significantly better with bubble charts than those in the control condition. This effect carried over when feedback was removed (Stage 3). Results also suggest an overall trend of improved performance due to feedback. We discuss these findings in the context of other visualization literacy efforts, and possible future work at the intersection of visualization, feedback, and learning. Experiment data and analysis scripts are available at the following repository https://osf.io/jys5d/",
        "uid": "v-short-1141",
        "file_name": "",
        "time_stamp": "2022-10-20T20:25:00Z",
        "time_start": "2022-10-20T20:25:00Z",
        "time_end": "2022-10-20T20:27:00Z"
    },
    "v-short-1100": {
        "slot_id": "v-short-1100-qa",
        "session_id": "short4",
        "type": "In Person Q+A",
        "title": "OSCAR: A Semantic-based Data Binning Approach (Q+A)",
        "contributors": [
            "Vidya Setlur"
        ],
        "authors": [
            "Vidya Setlur",
            "Michael Correll",
            "Sarah Battersby"
        ],
        "abstract": "Binning is applied to categorize data values or to see distributions of data. Existing binning algorithms often rely on statistical properties of data. However, there are semantic considerations for selecting appropriate binning schemes. Surveys, for instance, gather respondent data for demographic-related questions such as age, salary, number of employees, etc., that are bucketed into defined semantic categories. In this paper, we leverage common semantic categories from survey data and Tableau Public visualizations to identify a set of semantic binning categories. We employ these semantic binning categories in OSCAR: a method for automatically selecting bins based on the inferred semantic type of the field. We conducted a crowdsourced study with 120 participants to better understand user preferences for bins generated by OSCAR vs. binning provided in Tableau. We find that maps and histograms using binned values generated by OSCAR are preferred by users as compared to binning schemes based purely on the statistical properties of the data.",
        "uid": "v-short-1100",
        "file_name": "",
        "time_stamp": "2022-10-20T20:34:00Z",
        "time_start": "2022-10-20T20:34:00Z",
        "time_end": "2022-10-20T20:36:00Z"
    },
    "v-short-1143": {
        "slot_id": "v-short-1143-qa",
        "session_id": "short4",
        "type": "Virtual Q+A",
        "title": "Towards Systematic Design Considerations of Organizing Multiple Views (Q+A)",
        "contributors": [
            "Abdul Rahman Shaikh"
        ],
        "authors": [
            "Abdul Rahman Shaikh",
            "David Koop",
            "Hamed Alhoori",
            "Maoyuan Sun"
        ],
        "abstract": "Multiple-view visualization (MV) has been used for visual analytics in various fields (e.g., bioinformatics, cybersecurity, and intelligence analysis). Because each view encodes data from a particular perspective, analysts often use a set of views laid out in 2D space to link and synthesize information. The difficulty of this process is impacted by the spatial organization of these views. For instance, connecting information from views far from each other can be more challenging than neighboring ones. However, most visual analysis tools currently either fix the positions of the views or completely delegate this organization of views to users (who must manually drag and move views). This either limits user involvement in managing the layout of MV or is overly flexible without much guidance. Then, a key design challenge in MV layout is determining the factors in a spatial organization that impact understanding. To address this, we review a set of MV-based systems and identify considerations for MV layout rooted in two key concerns: perception, which considers how users perceive view relationships, and content, which considers the relationships in the data. We show how these allow us to study and analyze the design of MV layout systematically.",
        "uid": "v-short-1143",
        "file_name": "",
        "time_stamp": "2022-10-20T20:43:00Z",
        "time_start": "2022-10-20T20:43:00Z",
        "time_end": "2022-10-20T20:45:00Z"
    },
    "v-short-1133": {
        "slot_id": "v-short-1133-qa",
        "session_id": "short4",
        "type": "Virtual Q+A",
        "title": "Toward Systematic Considerations of Missingness in Visual Analytics (Q+A)",
        "contributors": [
            "Maoyuan Sun"
        ],
        "authors": [
            "Maoyuan Sun",
            "Yue Ma",
            "Yuanxin Wang",
            "Tianyi Li",
            "Jian Zhao",
            "Yujun Liu",
            "Ping-Shou Zhong"
        ],
        "abstract": "Data-driven decision making has been a common task in today\u2019s big data era, from simple choices such as finding a fast way to drive home, to complex decisions on medical treatment. It is often supported by visual analytics. For various reasons (e.g., system failure, interrupted network, intentional information hiding, or bias), visual analytics for sensemaking of data involves missingness (e.g., data loss and incomplete analysis), which impacts human decisions. For example, missing data can cost a business millions of dollars, and failing to recognize key evidence can put an innocent person in jail. Being aware of missingness is critical to avoid such catastrophes. To fulfill this, as an initial step, we consider missingness in visual analytics from two aspects: data-centric and human-centric. The former emphasizes missingness in three data-related categories: data composition, data relationship, and data usage. The latter focuses on the human-perceived missingness at three levels: observed-level, inferred-level, and ignored-level. Based on them, we discuss possible roles of visualizations for handling missingness, and conclude our discussion with future research opportunities.",
        "uid": "v-short-1133",
        "file_name": "",
        "time_stamp": "2022-10-20T20:52:00Z",
        "time_start": "2022-10-20T20:52:00Z",
        "time_end": "2022-10-20T20:54:00Z"
    },
    "v-short-1061": {
        "slot_id": "v-short-1061-qa",
        "session_id": "short4",
        "type": "Virtual Q+A",
        "title": "The role of extended reality for planning coronary artery bypass graft surgery (Q+A)",
        "contributors": [
            "Prof. Amanda Randles",
            "David Urick"
        ],
        "authors": [
            "Madhurima Vardhan",
            "Harvey Shi",
            "David Urick",
            "Manesh Patel",
            "Jane A. Leopold",
            "Amanda Randles"
        ],
        "abstract": "Immersive visual displays are becoming more common in the diagnostic imaging and pre-procedural planning of complex cardiology revascularization surgeries. One such procedure is coronary artery bypass grafting (CABG) surgery, which is a gold standard treatment for patients with advanced coronary heart disease. Treatment planning of the CABG surgery can be aided by extended reality (XR) displays as they are known for offering advantageous visualization of spatially heterogeneous and complex tasks. Despite the benefits of XR, it remains unknown whether clinicians will benefit from higher visual immersion offered by XR. In order to assess the impact of increased immersion as well as the latent factor of geometrical complexity, a quantitative user evaluation (n=14) was performed with clinicians of advanced cardiology training simulating CABG placement on sixteen 3D arterial tree models derived from 6 patients two levels of anatomic complexity. These arterial models were rendered on 3D/XR and 2D display modes with the same tactile interaction input device. The findings of this study reveal that compared to a monoscopic 2D display, the greater visual immersion of 3D/XR does not significantly alter clinician accuracy in the task of bypass graft placement. Latent factors such as arterial complexity and clinical experience both influence the accuracy of graft placement. In addition, an anatomically less complex model and greater clinical experience improve accuracy on both 3D/XR and 2D display modes. The findings of this study can help inform design guidelines of efficient and robust XR tools for pre-procedural planning of complex coronary revascularization surgeries such as CABG.",
        "uid": "v-short-1061",
        "file_name": "",
        "time_stamp": "2022-10-20T21:01:00Z",
        "time_start": "2022-10-20T21:01:00Z",
        "time_end": "2022-10-20T21:03:00Z"
    },
    "v-short-1092": {
        "slot_id": "v-short-1092-qa",
        "session_id": "short4",
        "type": "In Person Q+A",
        "title": "ARShopping: In-Store Shopping Decision Support Through Augmented Reality and Immersive Visualization (Q+A)",
        "contributors": [
            "Shunan Guo"
        ],
        "authors": [
            "Bingjie Xu",
            "Shunan Guo",
            "Eunyee Koh",
            "Jane Hoffswell",
            "Ryan Rossi",
            "Fan Du"
        ],
        "abstract": "Online shopping gives customers boundless options to choose from, backed by extensive product details and customer reviews, all from the comfort of home; yet, no amount of detailed, online information can outweigh the instant gratification and hands-on understanding of a product that is provided by physical stores. However, making purchasing decisions in physical stores can be challenging due to a large number of similar alternatives and limited accessibility of the relevant product information (e.g., features, ratings, and reviews). In this work, we present ARShopping: a web-based prototype to visually communicate detailed product information from an online setting on portable smart devices (e.g., phones, tablets, glasses), within the physical space at the point of purchase. This prototype uses augmented reality (AR) to identify products and display detailed information to help consumers make purchasing decisions that fulfill their needs while decreasing the decision-making time. In particular, we use a data fusion algorithm to improve the precision of the product detection; we then integrate AR visualizations into the scene to facilitate comparisons across multiple products and features. We designed our prototype based on interviews with 14 participants to better understand the utility and ease of use of the prototype.",
        "uid": "v-short-1092",
        "file_name": "",
        "time_stamp": "2022-10-20T21:10:00Z",
        "time_start": "2022-10-20T21:10:00Z",
        "time_end": "2022-10-20T21:12:00Z"
    },
    "v-cga-9547792": {
        "slot_id": "v-cga-9547792-qa",
        "session_id": "cga1",
        "type": "Virtual Q+A",
        "title": "What Students Learn With Personal Data Physicalization (Q+A)",
        "contributors": [
            "Charles Perin"
        ],
        "authors": [
            "Charles Perin"
        ],
        "abstract": "",
        "uid": "v-cga-9547792",
        "file_name": "",
        "time_stamp": "2022-10-21T02:55:00Z",
        "time_start": "2022-10-21T02:55:00Z",
        "time_end": "2022-10-21T02:57:00Z"
    },
    "v-cga-9476996": {
        "slot_id": "v-cga-9476996-qa",
        "session_id": "cga1",
        "type": "In Person Q+A",
        "title": "DeepGD: A Deep Learning Framework for Graph Drawing Using GNN (Q+A)",
        "contributors": [
            "Xiaoqi Wang"
        ],
        "authors": [
            "Xiaoqi Wang",
            "Kevin Yen",
            "Yifan Hu",
            "Han-Wei Shen"
        ],
        "abstract": "",
        "uid": "v-cga-9476996",
        "file_name": "",
        "time_stamp": "2022-10-21T03:07:00Z",
        "time_start": "2022-10-21T03:07:00Z",
        "time_end": "2022-10-21T03:09:00Z"
    },
    "v-cga-9490338": {
        "slot_id": "v-cga-9490338-qa",
        "session_id": "cga1",
        "type": "In Person Q+A",
        "title": "Interactive Visualization of Hyperspectral Images based on Neural Networks (Q+A)",
        "contributors": [
            "Hongfeng Yu"
        ],
        "authors": [
            "Feiyu Zhu",
            "Yu Pan",
            "Tian Gao",
            "Harkamal Walia",
            "Hongfeng Yu"
        ],
        "abstract": "",
        "uid": "v-cga-9490338",
        "file_name": "",
        "time_stamp": "2022-10-21T03:19:00Z",
        "time_start": "2022-10-21T03:19:00Z",
        "time_end": "2022-10-21T03:21:00Z"
    },
    "v-cga-9488227": {
        "slot_id": "v-cga-9488227-qa",
        "session_id": "cga1",
        "type": "Virtual Q+A",
        "title": "STSRNet: Deep Joint Space\u2013Time Super-Resolution for Vector Field Visualization (Q+A)",
        "contributors": [
            "Guihua Shan"
        ],
        "authors": [
            "Yifei An",
            "Han-Wei Shen",
            "Guihua Shan",
            "Guan Li",
            "Jun Liu"
        ],
        "abstract": "",
        "uid": "v-cga-9488227",
        "file_name": "",
        "time_stamp": "2022-10-21T03:31:00Z",
        "time_start": "2022-10-21T03:31:00Z",
        "time_end": "2022-10-21T03:33:00Z"
    },
    "v-cga-9495208": {
        "slot_id": "v-cga-9495208-qa",
        "session_id": "cga1",
        "type": "Virtual Q+A",
        "title": "Visual Clustering Factors in Scatterplots (Q+A)",
        "contributors": [
            "Weixing Lin"
        ],
        "authors": [
            "Jiazhi Xia",
            "Weixing Lin",
            "Guang Jiang",
            "Yunhai Wang",
            "Wei Chen",
            "Tobias Schreck"
        ],
        "abstract": "",
        "uid": "v-cga-9495208",
        "file_name": "",
        "time_stamp": "2022-10-21T03:43:00Z",
        "time_start": "2022-10-21T03:43:00Z",
        "time_end": "2022-10-21T03:45:00Z"
    },
    "v-cga-9238399": {
        "slot_id": "v-cga-9238399-qa",
        "session_id": "cga1",
        "type": "In Person Q+A",
        "title": "Cartolabe: A Web-Based Scalable Visualization of Large Document Collections (Q+A)",
        "contributors": [
            "Jean-Daniel Fekete"
        ],
        "authors": [
            "Philippe Caillou",
            "Jonas Renault",
            "Jean-Daniel Fekete",
            "Anne-Catherine Letournel",
            "Mich\u00e8le Sebag"
        ],
        "abstract": "",
        "uid": "v-cga-9238399",
        "file_name": "",
        "time_stamp": "2022-10-21T03:55:00Z",
        "time_start": "2022-10-21T03:55:00Z",
        "time_end": "2022-10-21T03:57:00Z"
    },
    "v-cga-9556564": {
        "slot_id": "v-cga-9556564-qa",
        "session_id": "cga2",
        "type": "Virtual Q+A",
        "title": "A Taxonomy-Driven Model for Designing Educational Games in  Visualization (Q+A)",
        "contributors": [
            "Renata Raidou"
        ],
        "authors": [
            "Lorenzo Amabili",
            "Kuhu Gupta",
            "Renata Georgia Raidou"
        ],
        "abstract": "",
        "uid": "v-cga-9556564",
        "file_name": "",
        "time_stamp": "2022-10-21T01:10:00Z",
        "time_start": "2022-10-21T01:10:00Z",
        "time_end": "2022-10-21T01:12:00Z"
    },
    "v-cga-9551781": {
        "slot_id": "v-cga-9551781-qa",
        "session_id": "cga2",
        "type": "In Person Q+A",
        "title": "Remote Instruction for Data Visualization Design-A Report From the Trenches (Q+A)",
        "contributors": [
            "Jan Aerts"
        ],
        "authors": [
            "Jan Aerts",
            "Jannes Peeters",
            "Jelmer Bot",
            "Danai Kafetzaki",
            "Houda Lamqaddam"
        ],
        "abstract": "",
        "uid": "v-cga-9551781",
        "file_name": "",
        "time_stamp": "2022-10-21T01:22:00Z",
        "time_start": "2022-10-21T01:22:00Z",
        "time_end": "2022-10-21T01:24:00Z"
    },
    "v-cga-9556143": {
        "slot_id": "v-cga-9556143-qa",
        "session_id": "cga2",
        "type": "Virtual Q+A",
        "title": "A Didactic Framework for Analyzing Learning Activities to Design InfoVis Courses (Q+A)",
        "contributors": [
            "Mandy Keck or Dietrich Kammer",
            "Elena Stoll"
        ],
        "authors": [
            "Mandy Keck",
            "Elena Stoll",
            "Dietrich Kammer"
        ],
        "abstract": "",
        "uid": "v-cga-9556143",
        "file_name": "",
        "time_stamp": "2022-10-21T01:34:00Z",
        "time_start": "2022-10-21T01:34:00Z",
        "time_end": "2022-10-21T01:36:00Z"
    },
    "v-cga-9547773": {
        "slot_id": "v-cga-9547773-qa",
        "session_id": "cga2",
        "type": "In Person Q+A",
        "title": "Through the Looking Glass: Insights Into Visualization Pedagogy Through Sentiment Analysis of Peer Review Text (Q+A)",
        "contributors": [
            "Paul Rosen"
        ],
        "authors": [
            "Zachariah J. Beasley",
            "Alon Friedman",
            "Paul Rosen"
        ],
        "abstract": "",
        "uid": "v-cga-9547773",
        "file_name": "",
        "time_stamp": "2022-10-21T01:46:00Z",
        "time_start": "2022-10-21T01:46:00Z",
        "time_end": "2022-10-21T01:48:00Z"
    },
    "v-cga-9547834": {
        "slot_id": "v-cga-9547834-qa",
        "session_id": "cga2",
        "type": "In Person Q+A",
        "title": "Visualization Design Sprints for Online and On-Campus Courses (Q+A)",
        "contributors": [
            "Johanna Beyer"
        ],
        "authors": [
            "Johanna Beyer",
            "Yalong Yang",
            "Hanspeter Pfister"
        ],
        "abstract": "",
        "uid": "v-cga-9547834",
        "file_name": "",
        "time_stamp": "2022-10-21T01:58:00Z",
        "time_start": "2022-10-21T01:58:00Z",
        "time_end": "2022-10-21T02:00:00Z"
    },
    "v-cga-9547790": {
        "slot_id": "v-cga-9547790-qa",
        "session_id": "cga2",
        "type": "In Person Q+A",
        "title": "Activity Worksheets for Teaching and Learning Data Visualization (Q+A)",
        "contributors": [
            "Vetria Byrd"
        ],
        "authors": [
            "Vetria L. Byrd",
            "Nicole Dwenger"
        ],
        "abstract": "",
        "uid": "v-cga-9547790",
        "file_name": "",
        "time_stamp": "2022-10-21T02:10:00Z",
        "time_start": "2022-10-21T02:10:00Z",
        "time_end": "2022-10-21T02:12:00Z"
    },
    "v-cga-8948290": {
        "slot_id": "v-cga-8948290-qa",
        "session_id": "cga3",
        "type": "Virtual Q+A",
        "title": "Surgical Navigation System for Low-Dose-Rate Brachytherapy Based on Mixed Reality (Q+A)",
        "contributors": [
            "Zeyang Zhou"
        ],
        "authors": [
            "Zeyang Zhou",
            "Zhiyong Yang",
            "Shan Jiang",
            "Xiaodong Ma",
            "Fujun Zhang",
            "Huzheng Yan"
        ],
        "abstract": "",
        "uid": "v-cga-8948290",
        "file_name": "",
        "time_stamp": "2022-10-20T02:55:00Z",
        "time_start": "2022-10-20T02:55:00Z",
        "time_end": "2022-10-20T02:57:00Z"
    },
    "v-cga-9709159": {
        "slot_id": "v-cga-9709159-qa",
        "session_id": "cga3",
        "type": "In Person Q+A",
        "title": "Visualization for Architecture, Engineering, and Construction: Shaping the Future of Our Built World (Q+A)",
        "contributors": [
            "Moataz Abdelaal"
        ],
        "authors": [
            "Moataz Abdelaal",
            "Felix Amtsberg",
            "Michael Becher",
            "Rebeca Duque Estrada",
            "Fabian Kannenberg",
            "Aimee Sousa Calepso",
            "Hans Jakob Wagner",
            "Guido Reina",
            "Michael Sedlmair",
            "Achim Menges",
            "Daniel Weiskopf"
        ],
        "abstract": "",
        "uid": "v-cga-9709159",
        "file_name": "",
        "time_stamp": "2022-10-20T03:07:00Z",
        "time_start": "2022-10-20T03:07:00Z",
        "time_end": "2022-10-20T03:09:00Z"
    },
    "v-cga-9726809": {
        "slot_id": "v-cga-9726809-qa",
        "session_id": "cga3",
        "type": "In Person Q+A",
        "title": "Visual Parameter Space Analysis for Optimizing the Quality of  Industrial Nonwovens (Q+A)",
        "contributors": [
            "Viny Saajan Victor"
        ],
        "authors": [
            "Viny Saajan Victor",
            "Andre Schmeiser",
            "Heike Leitte",
            "Simone Gramsch"
        ],
        "abstract": "",
        "uid": "v-cga-9726809",
        "file_name": "",
        "time_stamp": "2022-10-20T03:19:00Z",
        "time_start": "2022-10-20T03:19:00Z",
        "time_end": "2022-10-20T03:21:00Z"
    },
    "v-cga-9729397": {
        "slot_id": "v-cga-9729397-qa",
        "session_id": "cga3",
        "type": "In Person Q+A",
        "title": "Reflections on Visualization Research Projects in the Manufacturing Industry (Q+A)",
        "contributors": [
            "Johanna Schmidt"
        ],
        "authors": [
            "Lena Cibulski",
            "Johanna Schmidt",
            "Wolfgang Aigner"
        ],
        "abstract": "",
        "uid": "v-cga-9729397",
        "file_name": "",
        "time_stamp": "2022-10-20T03:31:00Z",
        "time_start": "2022-10-20T03:31:00Z",
        "time_end": "2022-10-20T03:33:00Z"
    },
    "v-cga-9732172": {
        "slot_id": "v-cga-9732172-qa",
        "session_id": "cga3",
        "type": "In Person Q+A",
        "title": "Situated Visual Analysis and Live Monitoring for Manufacturing (Q+A)",
        "contributors": [
            "Michael Becher"
        ],
        "authors": [
            "Michael Becher",
            "Dominik Herr",
            "Christoph Muller",
            "Kuno Kurzhals",
            "Guido Reina",
            "Lena Wagner",
            "Thomas Ertl",
            "Daniel Weiskopf"
        ],
        "abstract": "",
        "uid": "v-cga-9732172",
        "file_name": "",
        "time_stamp": "2022-10-20T03:43:00Z",
        "time_start": "2022-10-20T03:43:00Z",
        "time_end": "2022-10-20T03:45:00Z"
    },
    "v-cga-9709109": {
        "slot_id": "v-cga-9709109-qa",
        "session_id": "cga3",
        "type": "Virtual Q+A",
        "title": "Stress Visualization for Interface Optimization of a Hybrid Component Using Surface Tensor Spines (Q+A)",
        "contributors": [
            "Vanessa Kretzschmar"
        ],
        "authors": [
            "Vanessa Kretzschmar",
            "Allan Rocha",
            "Fabian Gunther",
            "Markus Stommel",
            "Gerik Scheuermann"
        ],
        "abstract": "",
        "uid": "v-cga-9709109",
        "file_name": "",
        "time_stamp": "2022-10-20T03:55:00Z",
        "time_start": "2022-10-20T03:55:00Z",
        "time_end": "2022-10-20T03:57:00Z"
    },
    "v-siggraph-1": {
        "slot_id": "v-siggraph-1-qa",
        "session_id": "sig1",
        "type": "In Person Q+A",
        "title": "Image Features Influence Reaction Time: A Learned Probabilistic Perceptual Model for Saccade Latency (Q+A)",
        "contributors": [
            "Budmonde Duinkharjav"
        ],
        "authors": [
            "Budmonde Duinkharjav",
            "Praneeth Chakravarthula",
            "Rachel Brown",
            "Anjul Patney",
            "Qi Sun"
        ],
        "abstract": "We aim to ask and answer an essential question \"how quickly do we react after observing a displayed visual target?\" To this end, we present psychophysical studies that characterize the remarkable disconnect between human saccadic behaviors and spatial visual acuity. Building on the results of our studies, we develop a perceptual model to predict temporal gaze behavior, particularly saccadic latency, as a function of the statistics of a displayed image. Specifically, we implement a neurologically-inspired probabilistic model that mimics the accumulation of confidence that leads to a perceptual decision. We validate our model with a series of objective measurements and user studies using an eye-tracked VR display. The results demonstrate that our model prediction is in statistical alignment with real-world human behavior. Further, we establish that many sub-threshold image modifications commonly introduced in graphics pipelines may significantly alter human reaction timing, even if the differences are visually undetectable. Finally, we show that our model can serve as a metric to predict and alter reaction latency of users in interactive computer graphics applications, thus may improve gaze-contingent rendering, design of virtual experiences, and player performance in e-sports. We illustrate this with two examples: estimating competition fairness in a video game with two different team colors, and tuning display viewing distance to minimize player reaction time.",
        "uid": "v-siggraph-1",
        "file_name": "",
        "time_stamp": "2022-10-19T21:55:00Z",
        "time_start": "2022-10-19T21:55:00Z",
        "time_end": "2022-10-19T21:57:00Z"
    },
    "v-siggraph-2": {
        "slot_id": "v-siggraph-2-qa",
        "session_id": "sig1",
        "type": "In Person Q+A",
        "title": "CLIPasso: Semantically Aware Object Sketching (Q+A)",
        "contributors": [
            "Arik Shamir"
        ],
        "authors": [
            "Yael Vinker",
            "Ehsan Pajouheshgar",
            "Jessica Y. Bo",
            "Roman Christian Bachmann",
            "Amit Bermano",
            "Daniel Cohen-Or",
            "Amir Zamir",
            "Ariel Shamir"
        ],
        "abstract": "Abstraction is at the heart of sketching due to the simple and minimal nature of line drawings. Abstraction entails identifying the essential visual properties of an object or scene, which requires semantic understanding and prior knowledge of high-level concepts. Abstract depictions are therefore challenging for artists, and even more so for machines. We present CLIPasso, an object sketching method that can achieve different levels of abstraction, guided by geometric and semantic simplifications. While sketch generation methods often rely on explicit sketch datasets for training, we utilize the remarkable ability of CLIP (Contrastive-Language-Image-Pretraining) to distill semantic concepts from sketches and images alike. We define a sketch as a set of B\u00e9zier curves and use a differentiable rasterizer to optimize the parameters of the curves directly with respect to a CLIP-based perceptual loss. The abstraction degree is controlled by varying the number of strokes. The generated sketches demonstrate multiple levels of abstraction while maintaining recognizability, underlying structure, and essential visual components of the subject drawn.",
        "uid": "v-siggraph-2",
        "file_name": "",
        "time_stamp": "2022-10-19T22:07:00Z",
        "time_start": "2022-10-19T22:07:00Z",
        "time_end": "2022-10-19T22:09:00Z"
    },
    "v-siggraph-3": {
        "slot_id": "v-siggraph-3-qa",
        "session_id": "sig1",
        "type": "Virtual Q+A",
        "title": "Joint Neural Phase Retrieval and Compression for Energy- and Computation-efficient Holography on the Edge (Q+A)",
        "contributors": [
            "Yujie Wang"
        ],
        "authors": [
            "Yujie Wang",
            "Praneeth Chakravarthula",
            "Qi Sun",
            "Baoquan Chen"
        ],
        "abstract": "Recent deep learning approaches have shown remarkable promise to enable high fidelity holographic displays. However, lightweight wearable display devices cannot afford the computation demand and energy consumption for hologram generation due to the limited onboard compute capability and battery life. On the other hand, if the computation is conducted entirely remotely on a cloud server, transmitting lossless hologram data is not only challenging but also result in prohibitively high latency and storage. In this work, by distributing the computation and optimizing the transmission, we propose the first framework that jointly generates and compresses high-quality phase-only holograms. Specifically, our framework asymmetrically separates the hologram generation process into high-compute remote encoding (on the server), and low-compute decoding (on the edge) stages. Our encoding enables light weight latent space data, thus faster and efficient transmission to the edge device. With our framework, we observed a reduction of 76% computation and consequently 83% in energy cost on edge devices, compared to the existing hologram generation methods. Our framework is robust to transmission and decoding errors, and approach high image fidelity for as low as 2 bits-per-pixel, and further reduced average bit-rates and decoding time for holographic videos.",
        "uid": "v-siggraph-3",
        "file_name": "",
        "time_stamp": "2022-10-19T22:19:00Z",
        "time_start": "2022-10-19T22:19:00Z",
        "time_end": "2022-10-19T22:21:00Z"
    },
    "v-siggraph-4": {
        "slot_id": "v-siggraph-4-qa",
        "session_id": "sig1",
        "type": "Virtual Q+A",
        "title": "Umbrella Meshes: Volumetric Elastic Mechanisms for Freeform Shape Deployment (Q+A)",
        "contributors": [
            "Yingying Ren",
            "Uday Kusupati, Yingying Ren"
        ],
        "authors": [
            "Yingying Ren",
            "Uday Kusupati",
            "Julian Panetta",
            "Florin Isvoranu",
            "Davide Pellis",
            "Tian Chen",
            "Mark Pauly"
        ],
        "abstract": "We present a computational inverse design framework for a new class of volumetric deployable structures that have compact rest states and deploy into bending-active 3D target surfaces. Umbrella meshes consist of elastic beams, rigid plates, and hinge joints that can be directly printed or assembled in a zero-energy fabrication state. During deployment, as the elastic beams of varying heights rotate from vertical to horizontal configurations, the entire structure transforms from a compact block into a target curved surface. Umbrella Meshes encode both intrinsic and extrinsic curvature of the target surface and in principle are free from the area expansion ratio bounds of past auxetic material systems.  We build a reduced physics-based simulation framework to accurately and efficiently model the complex interaction between the elastically deforming components. To determine the mesh topology and optimal shape parameters for approximating a given target surface, we propose an inverse design optimization algorithm initialized with conformal flattening. Our algorithm minimizes the structure's strain energy in its deployed state and optimizes actuation forces so that the final deployed structure is in stable equilibrium close to the desired surface with few or no external constraints. We validate our approach by fabricating a series of physical models at various scales using different manufacturing techniques.",
        "uid": "v-siggraph-4",
        "file_name": "",
        "time_stamp": "2022-10-19T22:31:00Z",
        "time_start": "2022-10-19T22:31:00Z",
        "time_end": "2022-10-19T22:33:00Z"
    },
    "v-siggraph-5": {
        "slot_id": "v-siggraph-5-qa",
        "session_id": "sig1",
        "type": "Virtual Q+A",
        "title": "Sketch2Pose: estimating a 3D character pose from a bitmap sketch (Q+A)",
        "contributors": [
            "Kirill Brodt"
        ],
        "authors": [
            "Kirill Brodt",
            "Mikhail Bessmeltsev"
        ],
        "abstract": "Artists frequently capture character poses via raster sketches, then use these drawings as a reference while posing a 3D character in a specialized 3D software --- a time-consuming process, requiring specialized 3D training and mental effort. We tackle this challenge by proposing the first system for automatically inferring a 3D character pose from a single bitmap sketch, producing poses consistent with viewer expectations. Algorithmically interpreting bitmap sketches is challenging, as they contain significantly distorted proportions and foreshortening. We address this by predicting three key elements of a drawing, necessary to disambiguate the drawn poses: 2D bone tangents, self-contacts, and bone foreshortening. These elements are then leveraged in an optimization inferring the 3D character pose consistent with the artist's intent. Our optimization balances cues derived from artistic literature and perception research to compensate for distorted character proportions. We demonstrate a gallery of results on sketches of numerous styles. We validate our method via numerical evaluations, user studies, and comparisons to manually posed characters and previous work. Code and data for our paper are available at http://www-labs.iro.umontreal.ca/bmpix/sketch2pose/.",
        "uid": "v-siggraph-5",
        "file_name": "",
        "time_stamp": "2022-10-19T22:43:00Z",
        "time_start": "2022-10-19T22:43:00Z",
        "time_end": "2022-10-19T22:45:00Z"
    },
    "v-siggraph-6": {
        "slot_id": "v-siggraph-6-qa",
        "session_id": "sig1",
        "type": "Virtual Q+A",
        "title": "Spelunking the Deep: Guaranteed Queries on General Neural Implicit Surfaces (Q+A)",
        "contributors": [
            "Nicholas Sharp"
        ],
        "authors": [
            "Nicholas Sharp",
            "Alec Jacobson"
        ],
        "abstract": "Neural implicit representations, which encode a surface as the level set of a neural network applied to spatial coordinates, have proven to be remarkably effective for optimizing, compressing, and generating 3D geometry. Although these representations are easy to fit, it is not clear how to best evaluate geometric queries on the shape, such as intersecting against a ray or finding a closest point. The predominant approach is to encourage the network to have a signed distance property. However, this property typically holds only approximately, leading to robustness issues, and holds only at the conclusion of training, inhibiting the use of queries in loss functions. Instead, this work presents a new approach to perform queries directly on general neural implicit functions for a wide range of existing architectures. Our key tool is the application of range analysis to neural networks, using automatic arithmetic rules to bound the output of a network over a region; we conduct a study of range analysis on neural networks, and identify variants of affine arithmetic which are highly effective. We use the resulting bounds to develop geometric queries including ray casting, intersection testing, constructing spatial hierarchies, fast mesh extraction, closest-point evaluation, evaluating bulk properties, and more. Our queries can be efficiently evaluated on GPUs, and offer concrete accuracy guarantees even on randomly-initialized networks, enabling their use in training objectives and beyond. We also show a preliminary application to inverse rendering.",
        "uid": "v-siggraph-6",
        "file_name": "",
        "time_stamp": "2022-10-19T22:55:00Z",
        "time_start": "2022-10-19T22:55:00Z",
        "time_end": "2022-10-19T22:57:00Z"
    },
    "v-vr-9714117": {
        "slot_id": "v-vr-9714117-qa",
        "session_id": "vr1",
        "type": "Virtual Q+A",
        "title": "Breaking Plausibility Without Breaking Presence - Evidence For The Multi-Layer Nature Of Plausibility (Q+A)",
        "contributors": [
            "Franziska Westermeier"
        ],
        "authors": [
            "Larissa Br\u00fcbach",
            "Franziska Westermeier",
            "Carolin Wienrich",
            "Marc Erich Latoschik"
        ],
        "abstract": "A novel theoretical model recently introduced coherence and plausibility as the essential conditions of XR experiences, challenging contemporary presence-oriented concepts. This article reports on two experiments validating this model, which assumes coherence activation on three layers (cognition, perception, and sensation) as the potential sources leading to a condition of plausibility and from there to other XR qualia such as presence or body ownership. The experiments introduce and utilize breaks in plausibility (in analogy to breaks in presence): We induce incoherence on the perceptual and the cognitive layer simultaneously by a simulation of object behaviors that do not conform to the laws of physics, i.e., gravity. We show that this manipulation breaks plausibility and hence confirm that it results in the desired effects in the theorized condition space but that the breaks in plausibility did not affect presence. In addition, we show that a cognitive manipulation by a storyline framing is too weak to successfully counteract the strong bottom-up inconsistencies. Both results are in line with the predictions of the recently introduced three-layer model of coherence and plausibility, which incorporates well-known top-down and bottom-up rivalries and its theorized increased independence between plausibility and presence.",
        "uid": "v-vr-9714117",
        "file_name": "",
        "time_stamp": "2022-10-21T01:10:00Z",
        "time_start": "2022-10-21T01:10:00Z",
        "time_end": "2022-10-21T01:12:00Z"
    },
    "v-vr-9756791": {
        "slot_id": "v-vr-9756791-qa",
        "session_id": "vr1",
        "type": "Virtual Q+A",
        "title": "Combining Real-World Constraints on User Behavior with Deep Neural Networks for Virtual Reality (VR) Biometrics (Q+A)",
        "contributors": [
            "Sean Banerjee"
        ],
        "authors": [
            "Robert Miller",
            "Natasha Kholgade Banerjee",
            "Sean Banerjee"
        ],
        "abstract": "Deep networks have demonstrated enormous potential for identification and authentication using behavioral biometrics in virtual reality (VR). However, existing VR behavioral biometrics datasets have small sample sizes which can make it challenging for deep networks to automatically learn features that characterize real-world user behavior and that may enable high success, e.g., high-level spatial relationships between headset and hand controller devices and underlying smoothness of trajectories despite noise. We provide an approach to perform behavioral biometrics using deep networks while incorporating spatial and smoothing constraints on input data to represent real-world behavior. We represent the input data to neural networks as a combination of scale- and translation-invariant device-centric position and orientation features, and displacement vectors representing spatial relationships between device pairs. We assess identification and authentication by including spatial relationships and by performing Gaussian smoothing of the position features. We evaluate our approach against baseline methods that use the raw data directly and that perform a global normalization of the data. By using displacement vectors, our work shows higher success over baseline methods in 36 out of 42 cases of analysis done by varying user sets and pairings of VR systems and sessions.",
        "uid": "v-vr-9756791",
        "file_name": "",
        "time_stamp": "2022-10-21T01:22:00Z",
        "time_start": "2022-10-21T01:22:00Z",
        "time_end": "2022-10-21T01:24:00Z"
    },
    "v-vr-9756796": {
        "slot_id": "v-vr-9756796-qa",
        "session_id": "vr1",
        "type": "Virtual Q+A",
        "title": "Real-Time Gaze Tracking with Event-Driven Eye Segmentation (Q+A)",
        "contributors": [
            "Feng Yu"
        ],
        "authors": [
            "Yu Feng",
            "Nathan Goulding-Hotta",
            "Asif Khan",
            "Hans Reyserhove",
            "Yuhao Zhu"
        ],
        "abstract": "Gaze tracking is increasingly becoming an essential component in Augmented and Virtual Reality. Modern gaze tracking algorithms are heavyweight; they operate at most 5 Hz on mobile processors despite that near-eye cameras comfortably operate at a real-time rate (> 30 Hz). This paper presents a real-time eye tracking algorithm that, on average, operates at 30 Hz on a mobile processor, achieves 0.1\u00b0\u20130.5\u00b0 gaze accuracies, all the while requiring only 30K parameters, one to two orders of magnitude smaller than state-of-the-art eye tracking algorithms. The crux of our algorithm is an Auto ROI mode, which continuously predicts the Regions of Interest (ROIs) of near-eye images and judiciously processes only the ROIs for gaze estimation. To that end, we introduce a novel, lightweight ROI prediction algorithm by emulating an event camera. We discuss how a software emulation of events enables accurate ROI prediction without requiring special hardware. The code of our paper is available at https://github.com/horizon-research/edgaze.",
        "uid": "v-vr-9756796",
        "file_name": "",
        "time_stamp": "2022-10-21T01:34:00Z",
        "time_start": "2022-10-21T01:34:00Z",
        "time_end": "2022-10-21T01:36:00Z"
    },
    "v-vr-9714118": {
        "slot_id": "v-vr-9714118-qa",
        "session_id": "vr1",
        "type": "Virtual Q+A",
        "title": "Mood-Driven Colorization of Virtual Indoor Scenes (Q+A)",
        "contributors": [
            "Michael Solah"
        ],
        "authors": [
            "Michael S Solah",
            "Haikun Huang",
            "Jiachuan Sheng",
            "Tian Feng",
            "Marc Pomplun",
            "Lap-Fai Yu"
        ],
        "abstract": "One of the challenging tasks in virtual scene design for Virtual Reality (VR) is causing it to invoke a particular mood in viewers. The subjective nature of moods brings uncertainty to the purpose. We propose a novel approach to automatic adjustment of the colors of textures for objects in a virtual indoor scene, enabling it to match a target mood. A dataset of 25,000 images, including building/home interiors, was used to train a classifier with the features extracted via deep learning. It contributes to an optimization process that colorizes virtual scenes automatically according to the target mood. Our approach was tested on four different indoor scenes, and we conducted a user study demonstrating its efficacy through statistical analysis with the focus on the impact of the scenes experienced with a VR headset.",
        "uid": "v-vr-9714118",
        "file_name": "",
        "time_stamp": "2022-10-21T01:46:00Z",
        "time_start": "2022-10-21T01:46:00Z",
        "time_end": "2022-10-21T01:48:00Z"
    },
    "v-vr-9714040": {
        "slot_id": "v-vr-9714040-qa",
        "session_id": "vr1",
        "type": "Virtual Q+A",
        "title": "Omnidirectional Galvanic Vestibular Stimulation in Virtual Reality (Q+A)",
        "contributors": [
            "Colin Groth"
        ],
        "authors": [
            "Colin Groth",
            "Jan-Philipp Tauscher",
            "Nikkel Heesen",
            "Max Hattenbach",
            "Susana Castillo",
            "Marcus Magnor"
        ],
        "abstract": "In this paper we propose omnidirectional galvanic vestibular stimulation (GVS) to mitigate cybersickness in virtual reality applications. One of the most accepted theories indicates that Cybersickness is caused by the visually induced impression of ego motion while physically remaining at rest. As a result of this sensory mismatch, people associate negative symptoms with VR and sometimes avoid the technology altogether. To reconcile the two contradicting sensory perceptions, we investigate GVS to stimulate the vestibular canals behind our ears with low-current electrical signals that are specifically attuned to the visually displayed camera motion. We describe how to calibrate and generate the appropriate GVS signals in real-time for pre-recorded omnidirectional videos exhibiting ego-motion in all three spatial directions. For validation, we conduct an experiment presenting real-world 360\u00b0 videos shot from a moving first-person perspective in a VR head-mounted display. Our findings indicate that GVS is able to significantly reduce discomfort for cybersickness-susceptible VR users, creating a deeper and more enjoyable immersive experience for many people.",
        "uid": "v-vr-9714040",
        "file_name": "",
        "time_stamp": "2022-10-21T01:58:00Z",
        "time_start": "2022-10-21T01:58:00Z",
        "time_end": "2022-10-21T02:00:00Z"
    },
    "v-vr-9714044": {
        "slot_id": "v-vr-9714044-qa",
        "session_id": "vr1",
        "type": "Virtual Q+A",
        "title": "Comparing Direct and Indirect Methods of Audio Quality Evaluation in Virtual Reality Scenes of Varying Complexity (Q+A)",
        "contributors": [
            "Thomas Robotham"
        ],
        "authors": [
            "Thomas Robotham",
            "Olli S. Rummukainen",
            "Miriam Kurz",
            "Marie Eckert",
            "Emanu\u00ebl A. P. Habets"
        ],
        "abstract": "Many quality evaluation methods are used to assess uni-modal audio or video content without considering perceptual, cognitive, and interactive aspects present in virtual reality (VR) settings. Consequently, little is known regarding the repercussions of the employed evaluation method, content, and subject behavior on the quality ratings in VR. This mixed between- and within-subjects study uses four subjective audio quality evaluation methods (viz. multiple-stimulus with and without reference for direct scaling, and rank-order elimination and pairwise comparison for indirect scaling) to investigate the contributing factors present in multi-modal 6-DoF VR on quality ratings of real-time audio rendering. For each between-subjects employed method, two sets of conditions in five VR scenes were evaluated within-subjects. The conditions targeted relevant attributes for binaural audio reproduction using scenes with various amounts of user interactivity. Our results show all referenceless methods produce similar results using both condition sets. However, rank-order elimination proved to be the fastest method, required the least amount of repetitive motion, and yielded the highest discrimination between spatial conditions. Scene complexity was found to be a main effect within results, with behavioral and task load index results implying more complex scenes and interactive aspects of 6-DoF VR can impede quality judgments.",
        "uid": "v-vr-9714044",
        "file_name": "",
        "time_stamp": "2022-10-21T02:10:00Z",
        "time_start": "2022-10-21T02:10:00Z",
        "time_end": "2022-10-21T02:12:00Z"
    }
}