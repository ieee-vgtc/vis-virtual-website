{
    "conf": {
        "event": "Conference Events",
        "long_name": "Conference Events",
        "event_type": "VIS",
        "event_prefix": "conf",
        "event_description": "",
        "event_url": "",
        "organizers": [],
        "sessions": [
            {
                "title": "VIS Welcome | VGTC Awards | Test of Time Awards",
                "session_id": "conf1",
                "event_prefix": "conf",
                "track": "ok4",
                "livestream_id": "ok4-tue",
                "session_image": "conf1.png",
                "chair": [
                    "David Ebert",
                    "Danielle Szafir",
                    "Hendrik Strobelt",
                    "Chuck Hansen",
                    "Robert Moorhead"
                ],
                "organizers": [],
                "time_start": "2022-10-18T13:30:00Z",
                "time_end": "2022-10-18T15:15:00Z",
                "discord_category": "",
                "discord_channel": "oklahoma-st-4",
                "discord_channel_id": "1024600674924765264",
                "discord_link": "https://discord.com/channels/978320435554947082/1024600674924765264",
                "slido_link": "https://app.sli.do/event/i28UETZaiRWr1XwX6eKjcf?section=51658b34-0f5b-40c9-a335-1fd1468fad8d",
                "youtube_url": "https://youtu.be/UTWn068SfrA",
                "youtube_id": "UTWn068SfrA",
                "zoom_meeting": "",
                "zoom_password": "",
                "zoom_link": "",
                "ff_link": "",
                "ff_playlist": "",
                "time_slots": [
                    {
                        "slot_id": "conf-prog-16",
                        "session_id": "conf1",
                        "type": "In Person Presentation",
                        "title": "VIS Welcome",
                        "contributors": [
                            "David Ebert",
                            "Danielle Szafir",
                            "Hendrik Strobelt",
                            "Melanie Tory"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "",
                        "file_name": "",
                        "time_stamp": "2022-10-18T13:30:00Z",
                        "time_start": "2022-10-18T13:30:00Z",
                        "time_end": "2022-10-18T13:45:00Z",
                        "paper_type": "",
                        "keywords": [],
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "conf-prog-1",
                        "session_id": "conf1",
                        "type": "In Person ",
                        "title": "VGTC Awards Presentation",
                        "contributors": [
                            "Chuck Hansen"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "",
                        "file_name": "",
                        "time_stamp": "2022-10-18T13:45:00Z",
                        "time_start": "2022-10-18T13:45:00Z",
                        "time_end": "2022-10-18T13:46:00Z",
                        "paper_type": "",
                        "keywords": [],
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "conf-prog-2",
                        "session_id": "conf1",
                        "type": "In Person Presentation",
                        "title": "VGTC Dissertation Award",
                        "contributors": [
                            "Chuck Hansen"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "",
                        "file_name": "",
                        "time_stamp": "2022-10-18T13:46:00Z",
                        "time_start": "2022-10-18T13:46:00Z",
                        "time_end": "2022-10-18T13:48:00Z",
                        "paper_type": "",
                        "keywords": [],
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "conf-prog-3",
                        "session_id": "conf1",
                        "type": "In Person Presentation",
                        "title": "VGTC Significant New Researcher Award",
                        "contributors": [
                            "Chuck Hansen"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "",
                        "file_name": "",
                        "time_stamp": "2022-10-18T13:48:00Z",
                        "time_start": "2022-10-18T13:48:00Z",
                        "time_end": "2022-10-18T13:49:00Z",
                        "paper_type": "",
                        "keywords": [],
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "conf-prog-4",
                        "session_id": "conf1",
                        "type": "In Person Presentation",
                        "title": "VGTC Significant New Researcher Award",
                        "contributors": [
                            "Danielle Sazfir"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "",
                        "file_name": "",
                        "time_stamp": "2022-10-18T13:49:00Z",
                        "time_start": "2022-10-18T13:49:00Z",
                        "time_end": "2022-10-18T13:54:00Z",
                        "paper_type": "",
                        "keywords": [],
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "conf-prog-5",
                        "session_id": "conf1",
                        "type": "In Person Presentation",
                        "title": "VGTC Significant New Researcher Award",
                        "contributors": [
                            "Chuck Hansen"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "",
                        "file_name": "",
                        "time_stamp": "2022-10-18T13:54:00Z",
                        "time_start": "2022-10-18T13:54:00Z",
                        "time_end": "2022-10-18T13:55:00Z",
                        "paper_type": "",
                        "keywords": [],
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "conf-prog-6",
                        "session_id": "conf1",
                        "type": "In Person Presentation",
                        "title": "VGTC Significant New Researcher Award",
                        "contributors": [
                            "Arvind Satyanarayan"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "",
                        "file_name": "",
                        "time_stamp": "2022-10-18T13:55:00Z",
                        "time_start": "2022-10-18T13:55:00Z",
                        "time_end": "2022-10-18T14:00:00Z",
                        "paper_type": "",
                        "keywords": [],
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "conf-prog-7",
                        "session_id": "conf1",
                        "type": "In Person Presentation",
                        "title": "VGTC Technical Achievement Award",
                        "contributors": [
                            "Chuck Hansen"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "",
                        "file_name": "",
                        "time_stamp": "2022-10-18T14:00:00Z",
                        "time_start": "2022-10-18T14:00:00Z",
                        "time_end": "2022-10-18T14:01:00Z",
                        "paper_type": "",
                        "keywords": [],
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "conf-prog-8",
                        "session_id": "conf1",
                        "type": "In Person Presentation",
                        "title": "VGTC Technical Achievement Award",
                        "contributors": [
                            "Valerio Pascucci"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "",
                        "file_name": "",
                        "time_stamp": "2022-10-18T14:01:00Z",
                        "time_start": "2022-10-18T14:01:00Z",
                        "time_end": "2022-10-18T14:06:00Z",
                        "paper_type": "",
                        "keywords": [],
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "conf-prog-9",
                        "session_id": "conf1",
                        "type": "In Person Presentation",
                        "title": "VGTC Technical Achievement Award",
                        "contributors": [
                            "Chuck Hansen"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "",
                        "file_name": "",
                        "time_stamp": "2022-10-18T14:06:00Z",
                        "time_start": "2022-10-18T14:06:00Z",
                        "time_end": "2022-10-18T14:07:00Z",
                        "paper_type": "",
                        "keywords": [],
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "conf-prog-10",
                        "session_id": "conf1",
                        "type": "Virtual Presentation (pre-recorded)",
                        "title": "VGTC Technical Achievement Award",
                        "contributors": [
                            "Shixia Liu"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "",
                        "file_name": "",
                        "time_stamp": "2022-10-18T14:07:00Z",
                        "time_start": "2022-10-18T14:07:00Z",
                        "time_end": "2022-10-18T14:12:00Z",
                        "paper_type": "",
                        "keywords": [],
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "conf-prog-11",
                        "session_id": "conf1",
                        "type": "In Person Presentation",
                        "title": "VGTC Lifetime Achievement Award",
                        "contributors": [
                            "Chuck Hansen"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "",
                        "file_name": "",
                        "time_stamp": "2022-10-18T14:12:00Z",
                        "time_start": "2022-10-18T14:12:00Z",
                        "time_end": "2022-10-18T14:13:00Z",
                        "paper_type": "",
                        "keywords": [],
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "conf-prog-12",
                        "session_id": "conf1",
                        "type": "In Person Presentation",
                        "title": "VGTC Lifetime Achievement Award",
                        "contributors": [
                            "Colin Ware"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "",
                        "file_name": "",
                        "time_stamp": "2022-10-18T14:13:00Z",
                        "time_start": "2022-10-18T14:13:00Z",
                        "time_end": "2022-10-18T14:18:00Z",
                        "paper_type": "",
                        "keywords": [],
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "conf-prog-13",
                        "session_id": "conf1",
                        "type": "In Person Presentation",
                        "title": "VGTC Service Award",
                        "contributors": [
                            "Chuck Hansen"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "",
                        "file_name": "",
                        "time_stamp": "2022-10-18T14:18:00Z",
                        "time_start": "2022-10-18T14:18:00Z",
                        "time_end": "2022-10-18T14:19:00Z",
                        "paper_type": "",
                        "keywords": [],
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "conf-prog-14",
                        "session_id": "conf1",
                        "type": "In Person Presentation",
                        "title": "VGTC Service Award",
                        "contributors": [
                            "Gautam Chaudhary"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "",
                        "file_name": "",
                        "time_stamp": "2022-10-18T14:19:00Z",
                        "time_start": "2022-10-18T14:19:00Z",
                        "time_end": "2022-10-18T14:24:00Z",
                        "paper_type": "",
                        "keywords": [],
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "conf-prog-15",
                        "session_id": "conf1",
                        "type": "In Person Presentation",
                        "title": "VGTC Academy Inductees",
                        "contributors": [
                            "Chuck Hansen"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "",
                        "file_name": "",
                        "time_stamp": "2022-10-18T14:24:00Z",
                        "time_start": "2022-10-18T14:24:00Z",
                        "time_end": "2022-10-18T14:24:00Z",
                        "paper_type": "",
                        "keywords": [],
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "conf-prog-17",
                        "session_id": "conf1",
                        "type": "In Person Presentation",
                        "title": "ToT Awards Presentation",
                        "contributors": [
                            "Robert Moorhead"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "",
                        "file_name": "",
                        "time_stamp": "2022-10-18T14:30:00Z",
                        "time_start": "2022-10-18T14:30:00Z",
                        "time_end": "2022-10-18T14:33:00Z",
                        "paper_type": "",
                        "keywords": [],
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "conf-prog-18",
                        "session_id": "conf1",
                        "type": "In Person Presentation",
                        "title": "VAST 2012 Winner Announcement",
                        "contributors": [
                            "Jonathan Roberts"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "",
                        "file_name": "",
                        "time_stamp": "2022-10-18T14:33:00Z",
                        "time_start": "2022-10-18T14:33:00Z",
                        "time_end": "2022-10-18T14:35:00Z",
                        "paper_type": "",
                        "keywords": [],
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "conf-prog-19",
                        "session_id": "conf1",
                        "type": "Virtual Presentation (pre-recorded)",
                        "title": "VAST 2012 Winner Presentation",
                        "contributors": [
                            "Sean Kandel"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "",
                        "file_name": "x-tot-kandel-vast2012.mp4",
                        "time_stamp": "2022-10-18T14:35:00Z",
                        "time_start": "2022-10-18T14:35:00Z",
                        "time_end": "2022-10-18T14:40:00Z",
                        "paper_type": "",
                        "keywords": [],
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "conf-prog-20",
                        "session_id": "conf1",
                        "type": "In Person Presentation",
                        "title": "InfoVis 2002 Winner Announcement",
                        "contributors": [
                            "Tim Dwyer"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "",
                        "file_name": "",
                        "time_stamp": "2022-10-18T14:40:00Z",
                        "time_start": "2022-10-18T14:40:00Z",
                        "time_end": "2022-10-18T14:42:00Z",
                        "paper_type": "",
                        "keywords": [],
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "conf-prog-21",
                        "session_id": "conf1",
                        "type": "Virtual Presentation (pre-recorded)",
                        "title": "InfoVis 2002 Winner Presentation",
                        "contributors": [
                            "Catherine Plaisant"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "",
                        "file_name": "x-tot-plaisant-spacetree.mp4",
                        "time_stamp": "2022-10-18T14:42:00Z",
                        "time_start": "2022-10-18T14:42:00Z",
                        "time_end": "2022-10-18T14:47:00Z",
                        "paper_type": "",
                        "keywords": [],
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "conf-prog-22",
                        "session_id": "conf1",
                        "type": "In Person Presentation",
                        "title": "InfoVis 2012 Winner Announcement",
                        "contributors": [
                            "Tim Dwyer"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "",
                        "file_name": "",
                        "time_stamp": "2022-10-18T14:47:00Z",
                        "time_start": "2022-10-18T14:47:00Z",
                        "time_end": "2022-10-18T14:49:00Z",
                        "paper_type": "",
                        "keywords": [],
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "conf-prog-23",
                        "session_id": "conf1",
                        "type": "In Person Presentation",
                        "title": "InfoVis 2012 Winner Presentation",
                        "contributors": [
                            "Tamara Munzner"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "",
                        "file_name": "",
                        "time_stamp": "2022-10-18T14:49:00Z",
                        "time_start": "2022-10-18T14:49:00Z",
                        "time_end": "2022-10-18T14:54:00Z",
                        "paper_type": "",
                        "keywords": [],
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "conf-prog-24",
                        "session_id": "conf1",
                        "type": "In Person Presentation",
                        "title": "SciVis 1997 Winner Announcement",
                        "contributors": [
                            "Kelly Gaither"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "",
                        "file_name": "",
                        "time_stamp": "2022-10-18T14:54:00Z",
                        "time_start": "2022-10-18T14:54:00Z",
                        "time_end": "2022-10-18T14:56:00Z",
                        "paper_type": "",
                        "keywords": [],
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "conf-prog-25",
                        "session_id": "conf1",
                        "type": "Virtual Presentation (pre-recorded)",
                        "title": "SciVis 1997 Winner Presentation",
                        "contributors": [
                            "Mark Dechaineau"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "",
                        "file_name": "x-tot-duchaineau-scivis97.mp4",
                        "time_stamp": "2022-10-18T14:56:00Z",
                        "time_start": "2022-10-18T14:56:00Z",
                        "time_end": "2022-10-18T15:00:00Z",
                        "paper_type": "",
                        "keywords": [],
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "conf-prog-26",
                        "session_id": "conf1",
                        "type": "In Person Presentation",
                        "title": "SciVis 2007 Winner Announcement",
                        "contributors": [
                            "Hamish Carr"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "",
                        "file_name": "",
                        "time_stamp": "2022-10-18T15:00:00Z",
                        "time_start": "2022-10-18T15:00:00Z",
                        "time_end": "2022-10-18T15:02:00Z",
                        "paper_type": "",
                        "keywords": [],
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "conf-prog-27",
                        "session_id": "conf1",
                        "type": "Virtual Presentation (pre-recorded)",
                        "title": "SciVis 2007 Winner Presentation",
                        "contributors": [
                            "Christopher Garth"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "",
                        "file_name": "x-tot-garth-scivis07.mp4",
                        "time_stamp": "2022-10-18T15:02:00Z",
                        "time_start": "2022-10-18T15:02:00Z",
                        "time_end": "2022-10-18T15:07:00Z",
                        "paper_type": "",
                        "keywords": [],
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "conf-prog-28",
                        "session_id": "conf1",
                        "type": "In Person Presentation",
                        "title": "SciVis 2008 Winner Announcement",
                        "contributors": [
                            "Kelly Gaither"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "",
                        "file_name": "",
                        "time_stamp": "2022-10-18T15:07:00Z",
                        "time_start": "2022-10-18T15:07:00Z",
                        "time_end": "2022-10-18T15:09:00Z",
                        "paper_type": "",
                        "keywords": [],
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "conf-prog-29",
                        "session_id": "conf1",
                        "type": "In Person Presentation",
                        "title": "SciVis 2008 Winner Presentation",
                        "contributors": [
                            "Attila Gyulassy"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "",
                        "file_name": "",
                        "time_stamp": "2022-10-18T15:09:00Z",
                        "time_start": "2022-10-18T15:09:00Z",
                        "time_end": "2022-10-18T15:15:00Z",
                        "paper_type": "",
                        "keywords": [],
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    }
                ]
            },
            {
                "title": "Industry Keynote | VIS Keynote",
                "session_id": "conf2",
                "event_prefix": "conf",
                "track": "ok4",
                "livestream_id": "ok4-tue",
                "session_image": "conf2.png",
                "chair": [
                    "David Ebert",
                    "Danielle Szafir",
                    "Hendrik Strobelt"
                ],
                "organizers": [],
                "time_start": "2022-10-18T19:00:00Z",
                "time_end": "2022-10-18T20:15:00Z",
                "discord_category": "",
                "discord_channel": "oklahoma-st-4",
                "discord_channel_id": "1024600674924765264",
                "discord_link": "https://discord.com/channels/978320435554947082/1024600674924765264",
                "slido_link": "https://app.sli.do/event/i28UETZaiRWr1XwX6eKjcf?section=51658b34-0f5b-40c9-a335-1fd1468fad8d",
                "youtube_url": "https://youtu.be/UTWn068SfrA",
                "youtube_id": "UTWn068SfrA",
                "zoom_meeting": "",
                "zoom_password": "",
                "zoom_link": "",
                "ff_link": "",
                "ff_playlist": "",
                "time_slots": [
                    {
                        "slot_id": "conf2-prog-1",
                        "session_id": "conf2",
                        "type": "In Person Presentation",
                        "title": "Industry Keynote",
                        "contributors": [
                            "Sybren Raajimakers"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "",
                        "file_name": "",
                        "time_stamp": "2022-10-18T19:00:00Z",
                        "time_start": "2022-10-18T19:00:00Z",
                        "time_end": "2022-10-18T19:15:00Z",
                        "paper_type": "",
                        "keywords": [],
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "conf2-prog-2",
                        "session_id": "conf2",
                        "type": "In Person Presentation",
                        "title": "VIS Keynote",
                        "contributors": [
                            "Marti Hearst"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "",
                        "file_name": "",
                        "time_stamp": "2022-10-18T19:15:00Z",
                        "time_start": "2022-10-18T19:15:00Z",
                        "time_end": "2022-10-18T20:15:00Z",
                        "paper_type": "",
                        "keywords": [],
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    }
                ]
            },
            {
                "title": "VIS Supporters Forum | Featured Sponsor Talks | Supporters Panel",
                "session_id": "conf3",
                "event_prefix": "conf",
                "track": "ok4",
                "livestream_id": "ok4-tue",
                "session_image": "conf3.png",
                "chair": [
                    "Eli T. Brown",
                    "Benjamin Bach",
                    "J\u00fcrgen Bernhard"
                ],
                "organizers": [],
                "time_start": "2022-10-18T20:45:00Z",
                "time_end": "2022-10-18T22:00:00Z",
                "discord_category": "",
                "discord_channel": "oklahoma-st-4",
                "discord_channel_id": "1024600674924765264",
                "discord_link": "https://discord.com/channels/978320435554947082/1024600674924765264",
                "slido_link": "https://app.sli.do/event/i28UETZaiRWr1XwX6eKjcf?section=51658b34-0f5b-40c9-a335-1fd1468fad8d",
                "youtube_url": "https://youtu.be/UTWn068SfrA",
                "youtube_id": "UTWn068SfrA",
                "zoom_meeting": "",
                "zoom_password": "",
                "zoom_link": "",
                "ff_link": "",
                "ff_playlist": "",
                "time_slots": [
                    {
                        "slot_id": "conf3-prog-1",
                        "session_id": "conf3",
                        "type": "In Person Presentation",
                        "title": "Opening Presentation",
                        "contributors": [
                            "Eli T Brown (host)"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "",
                        "file_name": "",
                        "time_stamp": "2022-10-18T20:45:00Z",
                        "time_start": "2022-10-18T20:45:00Z",
                        "time_end": "2022-10-18T20:46:00Z",
                        "paper_type": "",
                        "keywords": [],
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "conf3-prog-2",
                        "session_id": "conf3",
                        "type": "In Person Presentation",
                        "title": "Feat. Talk: Northern Data",
                        "contributors": [
                            "Sybren Raaijmakers"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "",
                        "file_name": "",
                        "time_stamp": "2022-10-18T20:46:00Z",
                        "time_start": "2022-10-18T20:46:00Z",
                        "time_end": "2022-10-18T20:51:00Z",
                        "paper_type": "",
                        "keywords": [],
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "conf3-prog-3",
                        "session_id": "conf3",
                        "type": "In Person Presentation",
                        "title": "Feat. Talk: Tableau",
                        "contributors": [
                            "Vidya Setlur"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "",
                        "file_name": "",
                        "time_stamp": "2022-10-18T20:51:00Z",
                        "time_start": "2022-10-18T20:51:00Z",
                        "time_end": "2022-10-18T20:56:00Z",
                        "paper_type": "",
                        "keywords": [],
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "conf3-prog-4",
                        "session_id": "conf3",
                        "type": "Virtual Presentation (pre-recorded)",
                        "title": "Feat. Talk: Autodesk",
                        "contributors": [
                            "Bon Aseniero"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "",
                        "file_name": "autodesk-2021-brand-sizzle-en.mp4",
                        "time_stamp": "2022-10-18T20:56:00Z",
                        "time_start": "2022-10-18T20:56:00Z",
                        "time_end": "2022-10-18T21:01:00Z",
                        "paper_type": "",
                        "keywords": [],
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "conf3-prog-5",
                        "session_id": "conf3",
                        "type": "Virtual Presentation (pre-recorded)",
                        "title": "Feat. Talk: Kaust",
                        "contributors": [
                            "Ivan Viola, Vivian Castaneda, Miriam Syed"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "",
                        "file_name": "VIS22 KAUST 5-MINUTE SUPPORTER FORUM.mp4",
                        "time_stamp": "2022-10-18T21:01:00Z",
                        "time_start": "2022-10-18T21:01:00Z",
                        "time_end": "2022-10-18T21:06:00Z",
                        "paper_type": "",
                        "keywords": [],
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "conf3-prog-6",
                        "session_id": "conf3",
                        "type": "Virtual Presentation (pre-recorded)",
                        "title": "Feat. Talk: Oklahoma",
                        "contributors": [
                            "David Ebert, Oklahoma U"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "",
                        "file_name": "University of Oklahoma 5-min.mp4",
                        "time_stamp": "2022-10-18T21:06:00Z",
                        "time_start": "2022-10-18T21:06:00Z",
                        "time_end": "2022-10-18T21:11:00Z",
                        "paper_type": "",
                        "keywords": [],
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "conf3-prog-7",
                        "session_id": "conf3",
                        "type": "In Person Panel",
                        "title": "Panel",
                        "contributors": [
                            "Sybren Raajimakers",
                            "Northern Data Panelist",
                            "Vidya Setlur",
                            "Tableau Panelist",
                            "Bon Adriel",
                            "Autodesk Panelist",
                            "David Ebert"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "",
                        "file_name": "",
                        "time_stamp": "2022-10-18T21:11:00Z",
                        "time_start": "2022-10-18T21:11:00Z",
                        "time_end": "2022-10-18T22:00:00Z",
                        "paper_type": "",
                        "keywords": [],
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    }
                ]
            },
            {
                "title": "VIS Capstone | VIS Closing",
                "session_id": "conf4",
                "event_prefix": "conf",
                "track": "ok4",
                "livestream_id": "ok4-fri",
                "session_image": "conf4.png",
                "chair": [
                    "David Ebert",
                    "Danielle Szafir",
                    "Hendrik Strobelt"
                ],
                "organizers": [],
                "time_start": "2022-10-21T15:45:00Z",
                "time_end": "2022-10-21T17:00:00Z",
                "discord_category": "",
                "discord_channel": "oklahoma-st-4",
                "discord_channel_id": "1024600674924765264",
                "discord_link": "https://discord.com/channels/978320435554947082/1024600674924765264",
                "slido_link": "https://app.sli.do/event/i28UETZaiRWr1XwX6eKjcf?section=51658b34-0f5b-40c9-a335-1fd1468fad8d",
                "youtube_url": "https://youtu.be/XhPpTVMgmhU",
                "youtube_id": "XhPpTVMgmhU",
                "zoom_meeting": "",
                "zoom_password": "",
                "zoom_link": "",
                "ff_link": "",
                "ff_playlist": "",
                "time_slots": [
                    {
                        "slot_id": "conf4-prog-1",
                        "session_id": "conf4",
                        "type": "In Person Presentation",
                        "title": "VIS Captstone",
                        "contributors": [
                            "Kerry Magruder"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "",
                        "file_name": "",
                        "time_stamp": "2022-10-21T15:45:00Z",
                        "time_start": "2022-10-21T15:45:00Z",
                        "time_end": "2022-10-21T16:45:00Z",
                        "paper_type": "",
                        "keywords": [],
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "conf4-prog-2",
                        "session_id": "conf4",
                        "type": "In Person Other",
                        "title": "VIS Closing",
                        "contributors": [
                            "David Ebert",
                            "Danielle Szafir",
                            "Hendrik Strobelt"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "",
                        "file_name": "",
                        "time_stamp": "2022-10-21T16:45:00Z",
                        "time_start": "2022-10-21T16:45:00Z",
                        "time_end": "2022-10-21T17:00:00Z",
                        "paper_type": "",
                        "keywords": [],
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    }
                ]
            },
            {
                "title": "VISAP Opening Reception (6:00pm-8:00pm) | -- Art Exhibit Location: Automobile Alley",
                "session_id": "a-visap-1",
                "event_prefix": "conf",
                "track": "ok1",
                "livestream_id": "various-wed",
                "session_image": "a-visap-1.png",
                "chair": [],
                "organizers": [],
                "time_start": "2022-10-18T23:00:00Z",
                "time_end": "2022-10-19T01:00:00Z",
                "discord_category": "",
                "discord_channel": "general",
                "discord_channel_id": "978320435554947085",
                "discord_link": "https://discord.com/channels/978320435554947082/978320435554947085",
                "slido_link": "",
                "youtube_url": "https://youtu.be/_evorVC17Yg",
                "youtube_id": "_evorVC17Yg",
                "zoom_meeting": "",
                "zoom_password": "",
                "zoom_link": "",
                "ff_link": "",
                "ff_playlist": "",
                "time_slots": [
                    {
                        "slot_id": "a-visap-prog-1",
                        "session_id": "a-visap-1",
                        "type": "In Person Other",
                        "title": "Welcome remarks",
                        "contributors": [
                            "Rebecca Ruige Xu",
                            "Uta Hinrichs"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "",
                        "file_name": "",
                        "time_stamp": "2022-10-18T23:00:00Z",
                        "time_start": "2022-10-18T23:00:00Z",
                        "time_end": "2022-10-18T23:35:00Z",
                        "paper_type": "",
                        "keywords": [],
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "a-visap-prog-2",
                        "session_id": "a-visap-1",
                        "type": "In Person Other",
                        "title": "VISAP Fast Forward",
                        "contributors": [
                            "Rebecca Ruige Xu",
                            "Uta Hinrichs"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "",
                        "file_name": "",
                        "time_stamp": "2022-10-18T23:35:00Z",
                        "time_start": "2022-10-18T23:35:00Z",
                        "time_end": "2022-10-18T23:40:00Z",
                        "paper_type": "",
                        "keywords": [],
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "a-visap-1047-pres",
                        "session_id": "a-visap-1",
                        "type": "In Person Presentation",
                        "title": "Neuroknitting Beethoven",
                        "contributors": [
                            "Varvara Guljajeva"
                        ],
                        "authors": [
                            "varvara guljajeva",
                            "Mar Canet Sola"
                        ],
                        "abstract": "NeuroKnitting Beethoven was developed to celebrate Ludwig van Beethoven\u2019s 250th anniversary and to re-visit the composer\u2019s classical compositions from an interdisciplinary viewpoint. Suddenly, the public could hear not only an artistic interpretation of the composition but also the musician's emotional state, which has resulted in the movement of a circular knitting machine installation, visuals, and plotted pattern to the produced garment in real-time. It means the pianist\u2019s (in Seoul\u2019s performance, it was a monk) affective response to music was captured every second and memorized in the knitted textile pattern, which was sprayed on the yarn before being knitted. High attention level resulted in a dense pattern, and the knitting machine\u2019s speed followed the meditation level. All these processes were real-time and took place simultaneously. Furthermore, the sound-responsive AI-generated visuals were created and displayed alongside the data visualization to accompany brain data visualizations.",
                        "uid": "a-visap-1047",
                        "file_name": "a-visap-1047_Guljajeva_Presentation.mp4",
                        "time_stamp": "2022-10-18T23:40:00Z",
                        "time_start": "2022-10-18T23:40:00Z",
                        "time_end": "2022-10-18T23:50:00Z",
                        "paper_type": "associated",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "239",
                        "paper_award": "",
                        "image_caption": "Screenshot of NeuroKnitting Beethoven telematic performance's video stream: a concert venue from where EEG data traveled to the artists' studio affecting the knitting process and garment pattern in real-time. Thematic audio-reactive AI-generated videos were overlaid by EEG data visualization.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "a-visap-1031-pres",
                        "session_id": "a-visap-1",
                        "type": "In Person Presentation",
                        "title": "At the Pump",
                        "contributors": [
                            "Joseph Insley"
                        ],
                        "authors": [
                            "Joseph Insley"
                        ],
                        "abstract": "\u201cat the pump\u201d is an observation of items left behind at various gas stations. The digital print, which measures 360\u201d x 10\u201d, consists of 110 photos collected over the course of about three and a half years. Along with the photos themselves are plots of associated data. This includes information related to when the photo was taken, as well as data related to the content of the image. The gas station is a common shared public space that many people move in and out of, often without giving a thought to those that are there before or after them. By focusing on these items that were left behind, we raise awareness that we are not here alone. And while we may not know or ever directly encounter these other people, our actions can leave an imprint on them, and the environment. The installation will also include the results of an #atthepump Twitter campaign.",
                        "uid": "a-visap-1031",
                        "file_name": "a-visap-1031_Insley_Presentation.mp4",
                        "time_stamp": "2022-10-18T23:50:00Z",
                        "time_start": "2022-10-18T23:50:00Z",
                        "time_end": "2022-10-18T23:55:00Z",
                        "paper_type": "associated",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "183",
                        "paper_award": "",
                        "image_caption": "The gas station is a common shared public space that many people move in and out of, often without giving a thought to those that are there before or after them. By focusing on items left behind, we raise awareness that we are not here alone.  And while we may not know or ever directly encounter these other people, our actions can leave an imprint on them, and the environment. \u201cat the pump\u201d is a project that takes a closer look through a series of photos, including information related to both when the photo was taken, and its content.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "a-visap-1078-pres",
                        "session_id": "a-visap-1",
                        "type": "In Person Presentation",
                        "title": "Ray",
                        "contributors": [
                            "Weidi Zhang"
                        ],
                        "authors": [
                            "weidi zhang"
                        ],
                        "abstract": "RAY provides a responsive art experience that re-interprets Rayograph (photogram) \u2013 a 20th Century cameraless image-making technique \u2013 in the perspective of Artificial Intelligent (AI) surveillance and the changing ontology of images. The system implements Image-to-Image Translation with Conditional Adversarial Networks and a computer vision system to translate human portraits into new images of Rayograph with semantic meanings, which are further developed algorithmically through visualizing in the aesthetics of light painting. RAY bridges intelligent visualization with cameraless photography Rayograph to engage audiences with an interactive poetic experience that conveys meanings.",
                        "uid": "a-visap-1078",
                        "file_name": "a-visap-1078_Zhang_Presentation.mp4",
                        "time_stamp": "2022-10-18T23:55:00Z",
                        "time_start": "2022-10-18T23:55:00Z",
                        "time_end": "2022-10-19T00:00:00Z",
                        "paper_type": "associated",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "234",
                        "paper_award": "",
                        "image_caption": "RAY, Installation view, Weidi Zhang, 2021",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "a-visap-1085-pres",
                        "session_id": "a-visap-1",
                        "type": "In Person Presentation",
                        "title": "Sifting Strands",
                        "contributors": [
                            "Elek, Oskar"
                        ],
                        "authors": [
                            "Oskar Elek",
                            "Weston Mossman",
                            "Angus Forbes"
                        ],
                        "abstract": "Sifting Strands is an interdisciplinary art project which arose from a wider scientific collaboration between computational researchers and astrophysicists. Sifting Sands brings bio-inspired patterns rooted in optimal transport networks into the aesthetic visual realm. Through it we explore how compter-generated art and live audience can collaborate in the creative act. The core of this work is a generalized simulation of Physarum polycephalum which meaningfully reacts to music, video and scene depth information. The resulting computational graph is implemented in Touch Designer and has been adapted to live performances and interactive installations. Universally, our works bring their audience together by creating a mingling space around the computational artifact.",
                        "uid": "a-visap-1085",
                        "file_name": "a-visap-1085_Elek_Presentation.mp4",
                        "time_stamp": "2022-10-19T00:00:00Z",
                        "time_start": "2022-10-19T00:00:00Z",
                        "time_end": "2022-10-19T00:10:00Z",
                        "paper_type": "associated",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "257",
                        "paper_award": "",
                        "image_caption": "Sifting Sands brings bio-inspired patterns rooted optimal transport networks into the aesthetic visual realm.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "a-visap-1082-pres",
                        "session_id": "a-visap-1",
                        "type": "Virtual Presentation (live)",
                        "title": "Presentation of Self in the Machine",
                        "contributors": [
                            "Ray LC"
                        ],
                        "authors": [
                            "RAY LC"
                        ],
                        "abstract": "The world has been driven apart by recent events, making long distance performative mingling difficult to achieve, especially those employing in-person collaboration between humans and machines. How shall we reclaim a tangible exchange between the US and other parts of the world that has presence and meaning, as opposed to impersonal virtual interactions? We created and choreographed and an art technology performance exchange that would allow viewers in Oklahoma City to immerse themselves in a collaborative narrative space between a dancer in the US and a robot arm in City University of Hong Kong's Studio for Narrative Spaces. The performance can be shown in either online or offline form to audiences, who witness the narrative of a dancer and a robot who communicate with each other through movement, sometimes leading one another, sometimes frustrating each other, but always communicating as if each other are present to each other across a 12 hour divide.  We propose an evening 9pm performance in Oklahoma City presented by dancer Mizuho Kappa, remotely choregraphed by RAY LC, which is simultaneously streamed live in the morning 10am at the School of Creative Media in Hong Kong, where a robot arm moves in response to movements undertaken by Mizuho. The live stream from Hong Kong is also shared with Oklahoma City, enabling Mizuho and RAY to interactively alter the dance movements and choreography live through visual comparison with the robot in a collaborative digital space. The robot imitates Mizuho\u2019s head using its hand and her body using its arm. The choreography follows Mizuho as she steps outside the digital realm of the virtual platform and into the physical stage, enticing the robot to dance with her. The robot starts with only block-like movements but eventually learns to mimic her with his body. Still he cannot run around or use hands like Mizuho, and eventually seeks help. Soon Mizuho begins performing actions that the arm is not capable of, such as jumping and lying flat on the ground, leading the robot to wonder on his own: is there also something I can do that the human cannot?",
                        "uid": "a-visap-1082",
                        "file_name": "a-visap-1082_Lc_Presentation.mp4",
                        "time_stamp": "2022-10-19T00:10:00Z",
                        "time_start": "2022-10-19T00:10:00Z",
                        "time_end": "2022-10-19T00:35:00Z",
                        "paper_type": "associated",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "245",
                        "paper_award": "",
                        "image_caption": "A dancer performs with the robot arm over a connection over a 12 hour time difference.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "",
                        "ff_id": ""
                    }
                ]
            },
            {
                "title": "VIS combined Poster Session (5:30pm-6:45pm)| -- includes all Workshops/Associated Event Posters| -- Exhibit Hall Location: OK Station 2+3",
                "session_id": "conf6",
                "event_prefix": "conf",
                "track": "ok4",
                "livestream_id": "various-thu",
                "session_image": "conf6.png",
                "chair": [],
                "organizers": [],
                "time_start": "2022-10-19T22:30:00Z",
                "time_end": "2022-10-19T23:45:00Z",
                "discord_category": "",
                "discord_channel": "general",
                "discord_channel_id": "978320435554947085",
                "discord_link": "https://discord.com/channels/978320435554947082/978320435554947085",
                "slido_link": "",
                "youtube_url": "https://youtu.be/_evorVC17Yg",
                "youtube_id": "_evorVC17Yg",
                "zoom_meeting": "",
                "zoom_password": "",
                "zoom_link": "",
                "ff_link": "",
                "ff_playlist": "",
                "time_slots": []
            },
            {
                "title": "VIS Banquet (6:45pm-10:00pm)| -- Location: First Americans Museum| -- Shuttles depart starting at 6:30pm",
                "session_id": "conf7",
                "event_prefix": "conf",
                "track": "ok1",
                "livestream_id": "various-wed",
                "session_image": "conf7.png",
                "chair": [],
                "organizers": [],
                "time_start": "2022-10-19T23:45:00Z",
                "time_end": "2022-10-20T03:00:00Z",
                "discord_category": "",
                "discord_channel": "general",
                "discord_channel_id": "978320435554947085",
                "discord_link": "https://discord.com/channels/978320435554947082/978320435554947085",
                "slido_link": "",
                "youtube_url": "https://youtu.be/_evorVC17Yg",
                "youtube_id": "_evorVC17Yg",
                "zoom_meeting": "",
                "zoom_password": "",
                "zoom_link": "",
                "ff_link": "",
                "ff_playlist": "",
                "time_slots": []
            },
            {
                "title": "OU Open House (6:00pm-9:00pm)| -- Location: off site, shuttles depart 5:45pm",
                "session_id": "conf8",
                "event_prefix": "conf",
                "track": "ok1",
                "livestream_id": "various-fri",
                "session_image": "conf8.png",
                "chair": [],
                "organizers": [],
                "time_start": "2022-10-20T23:00:00Z",
                "time_end": "2022-10-21T02:00:00Z",
                "discord_category": "",
                "discord_channel": "general",
                "discord_channel_id": "978320435554947085",
                "discord_link": "https://discord.com/channels/978320435554947082/978320435554947085",
                "slido_link": "",
                "youtube_url": "https://youtu.be/_evorVC17Yg",
                "youtube_id": "_evorVC17Yg",
                "zoom_meeting": "",
                "zoom_password": "",
                "zoom_link": "",
                "ff_link": "",
                "ff_playlist": "",
                "time_slots": []
            },
            {
                "title": "VIS Townhall",
                "session_id": "conf9",
                "event_prefix": "conf",
                "track": "ok4",
                "livestream_id": "ok4-wed",
                "session_image": "conf9.png",
                "chair": [],
                "organizers": [],
                "time_start": "2022-10-19T17:00:00Z",
                "time_end": "2022-10-19T19:00:00Z",
                "discord_category": "",
                "discord_channel": "oklahoma-st-4",
                "discord_channel_id": "1024600674924765264",
                "discord_link": "https://discord.com/channels/978320435554947082/1024600674924765264",
                "slido_link": "https://app.sli.do/event/i28UETZaiRWr1XwX6eKjcf?section=51658b34-0f5b-40c9-a335-1fd1468fad8d",
                "youtube_url": "https://youtu.be/BKQz_UXnWMA",
                "youtube_id": "BKQz_UXnWMA",
                "zoom_meeting": "",
                "zoom_password": "",
                "zoom_link": "",
                "ff_link": "",
                "ff_playlist": "",
                "time_slots": [
                    {
                        "slot_id": "conf9-prog-1",
                        "session_id": "conf9",
                        "type": "In Person Panel",
                        "title": "Panel",
                        "contributors": [],
                        "authors": [],
                        "abstract": "",
                        "uid": "",
                        "file_name": "",
                        "time_stamp": "2022-10-19T17:00:00Z",
                        "time_start": "2022-10-19T17:00:00Z",
                        "time_end": "2022-10-19T18:30:00Z",
                        "paper_type": "",
                        "keywords": [],
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    }
                ]
            },
            {
                "title": "Supporters Exhibits, Networking + Hiring, Posters",
                "session_id": "ex1",
                "event_prefix": "conf",
                "track": "ok1",
                "livestream_id": "various-tue",
                "session_image": "ex1.png",
                "chair": [],
                "organizers": [],
                "time_start": "2022-10-18T13:30:00Z",
                "time_end": "2022-10-18T15:15:00Z",
                "discord_category": "",
                "discord_channel": "general",
                "discord_channel_id": "978320435554947085",
                "discord_link": "https://discord.com/channels/978320435554947082/978320435554947085",
                "slido_link": "",
                "youtube_url": "https://youtu.be/_evorVC17Yg",
                "youtube_id": "_evorVC17Yg",
                "zoom_meeting": "",
                "zoom_password": "",
                "zoom_link": "",
                "ff_link": "",
                "ff_playlist": "",
                "time_slots": []
            },
            {
                "title": "Supporters Exhibits, Networking + Hiring, Posters",
                "session_id": "ex2",
                "event_prefix": "conf",
                "track": "ok1",
                "livestream_id": "various-wed",
                "session_image": "ex2.png",
                "chair": [],
                "organizers": [],
                "time_start": "2022-10-19T14:00:00Z",
                "time_end": "2022-10-19T15:15:00Z",
                "discord_category": "",
                "discord_channel": "general",
                "discord_channel_id": "978320435554947085",
                "discord_link": "https://discord.com/channels/978320435554947082/978320435554947085",
                "slido_link": "",
                "youtube_url": "https://youtu.be/_evorVC17Yg",
                "youtube_id": "_evorVC17Yg",
                "zoom_meeting": "",
                "zoom_password": "",
                "zoom_link": "",
                "ff_link": "",
                "ff_playlist": "",
                "time_slots": []
            },
            {
                "title": "Supporters Exhibits, Networking + Hiring, Posters",
                "session_id": "ex3",
                "event_prefix": "conf",
                "track": "ok1",
                "livestream_id": "various-thu",
                "session_image": "ex3.png",
                "chair": [],
                "organizers": [],
                "time_start": "2022-10-20T14:00:00Z",
                "time_end": "2022-10-20T15:15:00Z",
                "discord_category": "",
                "discord_channel": "general",
                "discord_channel_id": "978320435554947085",
                "discord_link": "https://discord.com/channels/978320435554947082/978320435554947085",
                "slido_link": "",
                "youtube_url": "https://youtu.be/_evorVC17Yg",
                "youtube_id": "_evorVC17Yg",
                "zoom_meeting": "",
                "zoom_password": "",
                "zoom_link": "",
                "ff_link": "",
                "ff_playlist": "",
                "time_slots": []
            }
        ]
    },
    "v-full": {
        "event": "VIS Full Papers",
        "long_name": "VIS Full Papers",
        "event_type": "Paper Presentations",
        "event_prefix": "v-full",
        "event_description": "",
        "event_url": "",
        "organizers": [],
        "sessions": [
            {
                "title": "VIS Opening (10:45am-11:03am)| Best Papers (11:03am-12:00pm)",
                "session_id": "full1",
                "event_prefix": "v-full",
                "track": "ok4",
                "livestream_id": "ok4-tue",
                "session_image": "full1.png",
                "chair": [
                    "Tamara Munzner"
                ],
                "organizers": [],
                "time_start": "2022-10-18T15:45:00Z",
                "time_end": "2022-10-18T17:00:00Z",
                "discord_category": "",
                "discord_channel": "oklahoma-st-4",
                "discord_channel_id": "1024600674924765264",
                "discord_link": "https://discord.com/channels/978320435554947082/1024600674924765264",
                "slido_link": "https://app.sli.do/event/i28UETZaiRWr1XwX6eKjcf?section=51658b34-0f5b-40c9-a335-1fd1468fad8d",
                "youtube_url": "https://youtu.be/UTWn068SfrA",
                "youtube_id": "UTWn068SfrA",
                "zoom_meeting": "",
                "zoom_password": "",
                "zoom_link": "",
                "ff_link": "https://youtu.be/xjn-E3RIduo",
                "ff_playlist": "",
                "time_slots": [
                    {
                        "slot_id": "v-full-1240-pres",
                        "session_id": "full1",
                        "type": "In Person Presentation",
                        "title": "Affective Learning Objectives for Communicative Visualizations",
                        "contributors": [
                            "Elsie Lee-Robbins"
                        ],
                        "authors": [
                            "Elsie Lee-Robbins",
                            "Eytan Adar"
                        ],
                        "abstract": "When designing communicative visualizations, we often focus on goals that seek to convey patterns, relations, or comparisons (cognitive learning objectives). We pay less attention to affective intents\u2013those that seek to influence or leverage the audience's opinions, attitudes, or values in some way. Affective objectives may range in outcomes from making the viewer care about the subject, strengthening a stance on an opinion, or leading them to take further action. Because such goals are often considered a violation of perceived \u2018neutrality\u2019 or are \u2018political,\u2019 designers may resist or be unable to describe these intents, let alone formalize them as learning objectives. While there are notable exceptions\u2013such as advocacy visualizations or persuasive cartography\u2013we find that visualization designers rarely acknowledge or formalize affective objectives. Through interviews with visualization designers, we expand on prior work on using learning objectives as a framework for describing and assessing communicative intent. Specifically, we extend and revise the framework to include a set of affective learning objectives. This structured taxonomy can help designers identify and declare their goals and compare and assess designs in a more principled way. Additionally, the taxonomy can enable external critique and analysis of visualizations. We illustrate the use of the taxonomy with a critical analysis of an affective visualization.",
                        "uid": "v-full-1240",
                        "file_name": "v-full-1240_Leerobbins_Presentation.mp4",
                        "time_stamp": "2022-10-18T16:00:00Z",
                        "time_start": "2022-10-18T16:00:00Z",
                        "time_end": "2022-10-18T16:13:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "607",
                        "paper_award": "",
                        "image_caption": "Three examples of learning objectives and how they connect to the affective learning objectives taxonomy (left) and the example visualization from the Economist (right).",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "https://youtu.be/UrW92ubvSdo",
                        "ff_id": "UrW92ubvSdo"
                    },
                    {
                        "slot_id": "v-full-1240-qa",
                        "session_id": "full1",
                        "type": "In Person Q+A",
                        "title": "Affective Learning Objectives for Communicative Visualizations (Q+A)",
                        "contributors": [
                            "Elsie Lee-Robbins"
                        ],
                        "authors": [],
                        "abstract": "When designing communicative visualizations, we often focus on goals that seek to convey patterns, relations, or comparisons (cognitive learning objectives). We pay less attention to affective intents\u2013those that seek to influence or leverage the audience's opinions, attitudes, or values in some way. Affective objectives may range in outcomes from making the viewer care about the subject, strengthening a stance on an opinion, or leading them to take further action. Because such goals are often considered a violation of perceived \u2018neutrality\u2019 or are \u2018political,\u2019 designers may resist or be unable to describe these intents, let alone formalize them as learning objectives. While there are notable exceptions\u2013such as advocacy visualizations or persuasive cartography\u2013we find that visualization designers rarely acknowledge or formalize affective objectives. Through interviews with visualization designers, we expand on prior work on using learning objectives as a framework for describing and assessing communicative intent. Specifically, we extend and revise the framework to include a set of affective learning objectives. This structured taxonomy can help designers identify and declare their goals and compare and assess designs in a more principled way. Additionally, the taxonomy can enable external critique and analysis of visualizations. We illustrate the use of the taxonomy with a critical analysis of an affective visualization.",
                        "uid": "v-full-1240",
                        "file_name": "",
                        "time_stamp": "2022-10-18T16:13:00Z",
                        "time_start": "2022-10-18T16:13:00Z",
                        "time_end": "2022-10-18T16:15:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "607",
                        "paper_award": "",
                        "image_caption": "Three examples of learning objectives and how they connect to the affective learning objectives taxonomy (left) and the example visualization from the Economist (right).",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "https://youtu.be/UrW92ubvSdo",
                        "ff_id": "UrW92ubvSdo"
                    },
                    {
                        "slot_id": "v-full-1636-pres",
                        "session_id": "full1",
                        "type": "In Person Presentation",
                        "title": "Multiple Forecast Visualizations (MFVs): Trade-offs in Trust and Performance in Multiple COVID-19 Forecast Visualizations",
                        "contributors": [
                            "Dr. Lace Padilla"
                        ],
                        "authors": [
                            "Lace Padilla",
                            "Racquel Fygenson",
                            "Spencer C. Castro",
                            "Enrico Bertini"
                        ],
                        "abstract": "The prevalence of inadequate SARS-COV-2 (COVID-19) responses may indicate a lack of trust in forecasts and risk communication. However, no work has empirically tested how multiple forecast visualization choices impact trust and task-based performance. The three studies presented in this paper (N = 1299) examine how visualization choices impact trust in COVID-19 mortality forecasts and how they influence performance in a trend prediction task. These studies focus on line charts populated with real-time COVID-19 data that varied the number and color encoding of the forecasts and the presence of best/worst-case forecasts. The studies reveal that trust in COVID-19 forecast visualizations initially increases with the number of forecasts and then plateaus after 6-9 forecasts. However, participants were most trusting of visualizations that showed less visual information, including a 95% confidence interval, single forecast, and grayscale encoded forecasts. Participants maintained high trust in intervals labeled with 50% and 25% and did not proportionally scale their trust to the indicated interval size. Despite the high trust, the 95% CI condition was the most likely to evoke predictions that did not correspond with the actual COVID-19 trend. Qualitative analysis of participants\u2019 strategies confirmed that many participants trusted both the simplistic visualizations and those with numerous forecasts. This work provides practical guides for how COVID-19 forecast visualizations influence trust, including recommendations for identifying the range where forecasts balance trade-offs between trust and task-based performance.",
                        "uid": "v-full-1636",
                        "file_name": "v-full-1636_Padilla_Presentation.mp4",
                        "time_stamp": "2022-10-18T16:15:00Z",
                        "time_start": "2022-10-18T16:15:00Z",
                        "time_end": "2022-10-18T16:28:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "940",
                        "paper_award": "",
                        "image_caption": "Multiple forecast visualizations (MFVs) used in Experiments 1 and 2 showing COVID-19 mortality forecasts for November 13, 2021 in the US. Each line depicts a different group\u2019s forecast, and the experiments examined the impact of the number of forecasts shown on trust and predictions of the COVID-19 trends. Each participant was shown the 16 stimuli in one row of this figure in a randomized order.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "https://youtu.be/O9IqmSVsfXI",
                        "ff_id": "O9IqmSVsfXI"
                    },
                    {
                        "slot_id": "v-full-1636-qa",
                        "session_id": "full1",
                        "type": "In Person Q+A",
                        "title": "Multiple Forecast Visualizations (MFVs): Trade-offs in Trust and Performance in Multiple COVID-19 Forecast Visualizations (Q+A)",
                        "contributors": [
                            "Dr. Lace Padilla"
                        ],
                        "authors": [],
                        "abstract": "The prevalence of inadequate SARS-COV-2 (COVID-19) responses may indicate a lack of trust in forecasts and risk communication. However, no work has empirically tested how multiple forecast visualization choices impact trust and task-based performance. The three studies presented in this paper (N = 1299) examine how visualization choices impact trust in COVID-19 mortality forecasts and how they influence performance in a trend prediction task. These studies focus on line charts populated with real-time COVID-19 data that varied the number and color encoding of the forecasts and the presence of best/worst-case forecasts. The studies reveal that trust in COVID-19 forecast visualizations initially increases with the number of forecasts and then plateaus after 6-9 forecasts. However, participants were most trusting of visualizations that showed less visual information, including a 95% confidence interval, single forecast, and grayscale encoded forecasts. Participants maintained high trust in intervals labeled with 50% and 25% and did not proportionally scale their trust to the indicated interval size. Despite the high trust, the 95% CI condition was the most likely to evoke predictions that did not correspond with the actual COVID-19 trend. Qualitative analysis of participants\u2019 strategies confirmed that many participants trusted both the simplistic visualizations and those with numerous forecasts. This work provides practical guides for how COVID-19 forecast visualizations influence trust, including recommendations for identifying the range where forecasts balance trade-offs between trust and task-based performance.",
                        "uid": "v-full-1636",
                        "file_name": "",
                        "time_stamp": "2022-10-18T16:28:00Z",
                        "time_start": "2022-10-18T16:28:00Z",
                        "time_end": "2022-10-18T16:30:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "940",
                        "paper_award": "",
                        "image_caption": "Multiple forecast visualizations (MFVs) used in Experiments 1 and 2 showing COVID-19 mortality forecasts for November 13, 2021 in the US. Each line depicts a different group\u2019s forecast, and the experiments examined the impact of the number of forecasts shown on trust and predictions of the COVID-19 trends. Each participant was shown the 16 stimuli in one row of this figure in a randomized order.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "https://youtu.be/O9IqmSVsfXI",
                        "ff_id": "O9IqmSVsfXI"
                    },
                    {
                        "slot_id": "v-full-1075-pres",
                        "session_id": "full1",
                        "type": "In Person Presentation",
                        "title": "Uncertainty-Aware Multidimensional Scaling",
                        "contributors": [
                            "David H\u00e4gele"
                        ],
                        "authors": [
                            "David H\u00e4gele",
                            "Tim Krake",
                            "Daniel Weiskopf"
                        ],
                        "abstract": "We present an extension of multidimensional scaling (MDS) to uncertain data, facilitating uncertainty visualization of multidimensional data. Our approach uses local projection operators that map high-dimensional random vectors to low-dimensional space to formulate a generalized stress. In this way, our generic model supports arbitrary distributions and various stress types. We use our uncertainty-aware multidimensional scaling (UAMDS) concept to derive a formulation for the case of normally distributed random vectors and a squared stress. The resulting minimization problem is numerically solved via gradient descent. We complement UAMDS by additional visualization techniques that address the sensitivity and trustworthiness of dimensionality reduction under uncertainty. With several examples, we demonstrate the usefulness of our approach and the importance of uncertainty-aware techniques.",
                        "uid": "v-full-1075",
                        "file_name": "v-full-1075_Haegele_Presentation.mp4",
                        "time_stamp": "2022-10-18T16:30:00Z",
                        "time_start": "2022-10-18T16:30:00Z",
                        "time_end": "2022-10-18T16:43:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "490",
                        "paper_award": "",
                        "image_caption": "Uncertainty-Aware Multidimensional Scaling applied to a set of 6-dimensional distributions. The distributions describe the variance in stats (Hit Points, Attack, Defense, Sp. Att., Sp. Def., Speed) for different Pokemon. The plot shows the projected probability densities with isolines for different quantiles.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "https://youtu.be/edxsNZJMVAE",
                        "ff_id": "edxsNZJMVAE"
                    },
                    {
                        "slot_id": "v-full-1075-qa",
                        "session_id": "full1",
                        "type": "In Person Q+A",
                        "title": "Uncertainty-Aware Multidimensional Scaling (Q+A)",
                        "contributors": [
                            "David H\u00e4gele"
                        ],
                        "authors": [],
                        "abstract": "We present an extension of multidimensional scaling (MDS) to uncertain data, facilitating uncertainty visualization of multidimensional data. Our approach uses local projection operators that map high-dimensional random vectors to low-dimensional space to formulate a generalized stress. In this way, our generic model supports arbitrary distributions and various stress types. We use our uncertainty-aware multidimensional scaling (UAMDS) concept to derive a formulation for the case of normally distributed random vectors and a squared stress. The resulting minimization problem is numerically solved via gradient descent. We complement UAMDS by additional visualization techniques that address the sensitivity and trustworthiness of dimensionality reduction under uncertainty. With several examples, we demonstrate the usefulness of our approach and the importance of uncertainty-aware techniques.",
                        "uid": "v-full-1075",
                        "file_name": "",
                        "time_stamp": "2022-10-18T16:43:00Z",
                        "time_start": "2022-10-18T16:43:00Z",
                        "time_end": "2022-10-18T16:45:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "490",
                        "paper_award": "",
                        "image_caption": "Uncertainty-Aware Multidimensional Scaling applied to a set of 6-dimensional distributions. The distributions describe the variance in stats (Hit Points, Attack, Defense, Sp. Att., Sp. Def., Speed) for different Pokemon. The plot shows the projected probability densities with isolines for different quantiles.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "https://youtu.be/edxsNZJMVAE",
                        "ff_id": "edxsNZJMVAE"
                    },
                    {
                        "slot_id": "v-short-1097-pres",
                        "session_id": "full1",
                        "type": "In Person Presentation",
                        "title": "Exploring D3 Implementation Challenges on Stack Overflow",
                        "contributors": [
                            "Leilani Battle"
                        ],
                        "authors": [
                            "Leilani Battle",
                            "Danni Feng",
                            "Kelli Webber"
                        ],
                        "abstract": "Visualization languages help to standardize the process of designing effective visualizations, one of the most prominent being D3. However, few researchers have analyzed at scale how users incorporate these languages into existing visualization programming processes, i.e., implementation workflows. In this paper, we present a new method for evaluating visualization languages. Our method emphasizes the experiences of users as observed through the online communities that have sprouted to facilitate public discussion and support around visualization languages. We demonstrate our method by analyzing D3 implementation workflows and challenges discussed on Stack Overflow. Our results show how the visualization community may be limiting its understanding of users' visualization implementation challenges by ignoring the larger context in which languages such as D3 are used. Based on our findings, we suggest new research directions to enhance the user experience with visualization languages. All our data and code are available at: https://osf.io/fup48/.",
                        "uid": "v-short-1097",
                        "file_name": "v-short-1097_Battle_Presentation.mp4",
                        "time_stamp": "2022-10-18T16:45:00Z",
                        "time_start": "2022-10-18T16:45:00Z",
                        "time_end": "2022-10-18T16:58:00Z",
                        "paper_type": "short",
                        "keywords": [
                            "Web mining, visualization language evaluation"
                        ],
                        "has_image": "1",
                        "has_video": "550",
                        "paper_award": "",
                        "image_caption": "By analyzing how and why D3 users post on Stack Overflow, we can understand how they program D3 visualizations and what challenges they encounter during the implementation process. Here are examples of visualizations and interactions that D3 users have discussed on Stack Overflow, as well as reference images they have used for inspiration.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "https://youtu.be/xQnAyz9kOv0",
                        "ff_id": "xQnAyz9kOv0"
                    },
                    {
                        "slot_id": "v-short-1097-qa",
                        "session_id": "full1",
                        "type": "In Person Q+A",
                        "title": "Exploring D3 Implementation Challenges on Stack Overflow (Q+A)",
                        "contributors": [
                            "Leilani Battle"
                        ],
                        "authors": [],
                        "abstract": "Visualization languages help to standardize the process of designing effective visualizations, one of the most prominent being D3. However, few researchers have analyzed at scale how users incorporate these languages into existing visualization programming processes, i.e., implementation workflows. In this paper, we present a new method for evaluating visualization languages. Our method emphasizes the experiences of users as observed through the online communities that have sprouted to facilitate public discussion and support around visualization languages. We demonstrate our method by analyzing D3 implementation workflows and challenges discussed on Stack Overflow. Our results show how the visualization community may be limiting its understanding of users' visualization implementation challenges by ignoring the larger context in which languages such as D3 are used. Based on our findings, we suggest new research directions to enhance the user experience with visualization languages. All our data and code are available at: https://osf.io/fup48/.",
                        "uid": "v-short-1097",
                        "file_name": "",
                        "time_stamp": "2022-10-18T16:58:00Z",
                        "time_start": "2022-10-18T16:58:00Z",
                        "time_end": "2022-10-18T17:00:00Z",
                        "paper_type": "short",
                        "keywords": [
                            "Web mining, visualization language evaluation"
                        ],
                        "has_image": "1",
                        "has_video": "550",
                        "paper_award": "",
                        "image_caption": "By analyzing how and why D3 users post on Stack Overflow, we can understand how they program D3 visualizations and what challenges they encounter during the implementation process. Here are examples of visualizations and interactions that D3 users have discussed on Stack Overflow, as well as reference images they have used for inspiration.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "https://youtu.be/xQnAyz9kOv0",
                        "ff_id": "xQnAyz9kOv0"
                    }
                ]
            },
            {
                "title": "Visualization Opportunities",
                "session_id": "full2",
                "event_prefix": "v-full",
                "track": "ok1",
                "livestream_id": "ok1-wed",
                "session_image": "full2.png",
                "chair": [
                    "Melanie Tory"
                ],
                "organizers": [],
                "time_start": "2022-10-19T15:45:00Z",
                "time_end": "2022-10-19T17:00:00Z",
                "discord_category": "",
                "discord_channel": "oklahoma-st-1",
                "discord_channel_id": "1024600861340606484",
                "discord_link": "https://discord.com/channels/978320435554947082/1024600861340606484",
                "slido_link": "https://app.sli.do/event/i28UETZaiRWr1XwX6eKjcf?section=740d9835-fe47-4cb4-b260-353262a7f8dd",
                "youtube_url": "https://youtu.be/EK30lilKKdM",
                "youtube_id": "EK30lilKKdM",
                "zoom_meeting": "",
                "zoom_password": "",
                "zoom_link": "",
                "ff_link": "https://youtu.be/FOWEaifCcxs",
                "ff_playlist": "",
                "time_slots": [
                    {
                        "slot_id": "full2-opening",
                        "session_id": "full2",
                        "type": "In Person Other",
                        "title": "Session Opening",
                        "contributors": [
                            "Melanie Tory"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "",
                        "file_name": "",
                        "time_stamp": "2022-10-19T15:45:00Z",
                        "time_start": "2022-10-19T15:45:00Z",
                        "time_end": "2022-10-19T15:45:00Z",
                        "paper_type": "",
                        "keywords": [],
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "v-full-1467-pres",
                        "session_id": "full2",
                        "type": "In Person Presentation",
                        "title": "KiriPhys: Exploring New Data Physicalization Opportunities",
                        "contributors": [
                            "Foroozan Daneshzand"
                        ],
                        "authors": [
                            "Foroozan Daneshzand",
                            "Charles Perin",
                            "Sheelagh Carpendale"
                        ],
                        "abstract": "We present KiriPhys, a new type of data physicalization based on kirigami, a traditional Japanese art form that uses\n paper-cutting. Within the kirigami possibilities, we investigate how different aspects of cutting patterns offer opportunities for mapping\n data to both independent and dependent physical variables. As a first step towards understanding the data physicalization opportunities\n in KiriPhys, we conducted a qualitative study in which 12 participants interacted with four KiriPhys examples. Our observations of how\n people interact with, understand, and respond to KiriPhys suggest that KiriPhys: 1) provides new opportunities for interactive, layered\n data exploration, 2) introduces elastic expansion as a new sensation that can reveal data, and 3) offers data mapping possibilities while\n providing a pleasurable experience that stimulates curiosity and engagement",
                        "uid": "v-full-1467",
                        "file_name": "v-full-1467_Daneshzand_Presentation.mp4",
                        "time_stamp": "2022-10-19T15:45:00Z",
                        "time_start": "2022-10-19T15:45:00Z",
                        "time_end": "2022-10-19T15:55:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "567",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "https://youtu.be/KqPcvUkoyK4",
                        "ff_id": "KqPcvUkoyK4"
                    },
                    {
                        "slot_id": "v-full-1467-qa",
                        "session_id": "full2",
                        "type": "In Person Q+A",
                        "title": "KiriPhys: Exploring New Data Physicalization Opportunities (Q+A)",
                        "contributors": [
                            "Foroozan Daneshzand"
                        ],
                        "authors": [],
                        "abstract": "We present KiriPhys, a new type of data physicalization based on kirigami, a traditional Japanese art form that uses\n paper-cutting. Within the kirigami possibilities, we investigate how different aspects of cutting patterns offer opportunities for mapping\n data to both independent and dependent physical variables. As a first step towards understanding the data physicalization opportunities\n in KiriPhys, we conducted a qualitative study in which 12 participants interacted with four KiriPhys examples. Our observations of how\n people interact with, understand, and respond to KiriPhys suggest that KiriPhys: 1) provides new opportunities for interactive, layered\n data exploration, 2) introduces elastic expansion as a new sensation that can reveal data, and 3) offers data mapping possibilities while\n providing a pleasurable experience that stimulates curiosity and engagement",
                        "uid": "v-full-1467",
                        "file_name": "",
                        "time_stamp": "2022-10-19T15:55:00Z",
                        "time_start": "2022-10-19T15:55:00Z",
                        "time_end": "2022-10-19T15:57:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "567",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "https://youtu.be/KqPcvUkoyK4",
                        "ff_id": "KqPcvUkoyK4"
                    },
                    {
                        "slot_id": "v-full-1197-pres",
                        "session_id": "full2",
                        "type": "Virtual Presentation (pre-recorded)",
                        "title": "Supporting Expressive and Faithful Pictorial Visualization Design with Visual Style Transfer",
                        "contributors": [
                            "Yang Shi"
                        ],
                        "authors": [
                            "Yang Shi",
                            "Pei Liu",
                            "Siji Chen",
                            "Mengdi Sun",
                            "Nan Cao"
                        ],
                        "abstract": "Pictorial visualizations portray data with figurative messages and approximate the audience to the visualization. Previous research on pictorial visualizations has developed authoring tools or generation systems, but their methods are restricted to specific visualization types and templates. Instead, we propose to augment pictorial visualization authoring with visual style transfer, enabling a more extensible approach to visualization design. To explore this, our work presents Vistylist, a design support tool that disentangles the visual style of a source pictorial visualization from its content and transfers the visual style to one or more intended pictorial visualizations. We evaluated Vistylist through a survey of example pictorial visualizations, a controlled user study, and a series of expert interviews. The results of our evaluation indicated that Vistylist is useful for creating expressive and faithful pictorial visualizations.",
                        "uid": "v-full-1197",
                        "file_name": "v-full-1197_Shi_Presentation.mp4",
                        "time_stamp": "2022-10-19T15:57:00Z",
                        "time_start": "2022-10-19T15:57:00Z",
                        "time_end": "2022-10-19T16:04:16Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "436",
                        "paper_award": "",
                        "image_caption": "Vistylist is a design support tool that disentangles the visual style of a source pictorial visualization from its content and transfers the visual style to one or more intended pictorial visualizations. The user interface of Vistylist consists of four main components: (1) users upload their source visualizations to the Design Panel (A) to extract visual elements, (2) users then upload their datasets to the Data Panel (B) to create field-channel mappings, (3) users can view the results on the Canvas (C), where they can adjust icons using the Icon Panel (D), and (4) the Suggestion Panel (E) suggested alternative designs.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "https://youtu.be/8Vuf7bUTJMk",
                        "ff_id": "8Vuf7bUTJMk"
                    },
                    {
                        "slot_id": "v-full-1197-qa",
                        "session_id": "full2",
                        "type": "Virtual Q+A",
                        "title": "Supporting Expressive and Faithful Pictorial Visualization Design with Visual Style Transfer (Q+A)",
                        "contributors": [
                            "Yang Shi"
                        ],
                        "authors": [],
                        "abstract": "Pictorial visualizations portray data with figurative messages and approximate the audience to the visualization. Previous research on pictorial visualizations has developed authoring tools or generation systems, but their methods are restricted to specific visualization types and templates. Instead, we propose to augment pictorial visualization authoring with visual style transfer, enabling a more extensible approach to visualization design. To explore this, our work presents Vistylist, a design support tool that disentangles the visual style of a source pictorial visualization from its content and transfers the visual style to one or more intended pictorial visualizations. We evaluated Vistylist through a survey of example pictorial visualizations, a controlled user study, and a series of expert interviews. The results of our evaluation indicated that Vistylist is useful for creating expressive and faithful pictorial visualizations.",
                        "uid": "v-full-1197",
                        "file_name": "",
                        "time_stamp": "2022-10-19T16:04:16Z",
                        "time_start": "2022-10-19T16:04:16Z",
                        "time_end": "2022-10-19T16:09:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "436",
                        "paper_award": "",
                        "image_caption": "Vistylist is a design support tool that disentangles the visual style of a source pictorial visualization from its content and transfers the visual style to one or more intended pictorial visualizations. The user interface of Vistylist consists of four main components: (1) users upload their source visualizations to the Design Panel (A) to extract visual elements, (2) users then upload their datasets to the Data Panel (B) to create field-channel mappings, (3) users can view the results on the Canvas (C), where they can adjust icons using the Icon Panel (D), and (4) the Suggestion Panel (E) suggested alternative designs.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "https://youtu.be/8Vuf7bUTJMk",
                        "ff_id": "8Vuf7bUTJMk"
                    },
                    {
                        "slot_id": "v-full-1221-pres",
                        "session_id": "full2",
                        "type": "Virtual Presentation (pre-recorded)",
                        "title": "Self-Supervised Color-Concept Association via Image Colorization",
                        "contributors": [
                            "Ruizhen Hu",
                            "Ziqi Ye"
                        ],
                        "authors": [
                            "Ruizhen Hu",
                            "Ziqi Ye",
                            "Bin Chen",
                            "Oliver van Kaick",
                            "Hui Huang"
                        ],
                        "abstract": "The interpretation of colors in visualizations is facilitated when the assignments between colors and concepts in the visualizations match human\u2019s expectations, implying that the colors can be interpreted in a semantic manner. However, manually creating a dataset of suitable associations between colors and concepts for use in visualizations is costly, as such associations would have to be collected from humans for a large variety of concepts. To address the challenge of collecting this data, we introduce a method to extract color-concept associations automatically from a set of concept images. While the state-of-the-art method extracts associations from data with supervised learning, we developed a self-supervised method based on colorization that does not require the preparation of ground truth color-concept associations. Our key insight is that a set of images of a concept should be sufficient for learning color-concept associations, since humans also learn to associate colors to concepts mainly from past visual input. Thus, we propose to use an automatic colorization method to extract statistical models of the color-concept associations that appear in concept images. Specifically, we take a colorization model pre-trained on ImageNet and fine-tune it on the set of images associated with a given concept, to predict pixel-wise probability distributions in Lab color space for the images. Then, we convert the predicted probability distributions into color ratings for a given color library and aggregate them for all the images of a concept to obtain the final color-concept associations. We evaluate our method using four different evaluation metrics and via a user study. Experiments show that, although the state-of-the-art method based on supervised learning with user-provided ratings is more effective at capturing relative associations, our self-supervised method obtains overall better results according to metrics like Earth Mover\u2019s Distance (EMD) and Entropy Difference (ED), which are closer to human perception of color distributions.",
                        "uid": "v-full-1221",
                        "file_name": "v-full-1221_Hu_Presentation.mp4",
                        "time_stamp": "2022-10-19T16:09:00Z",
                        "time_start": "2022-10-19T16:09:00Z",
                        "time_end": "2022-10-19T16:19:17Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "617",
                        "paper_award": "",
                        "image_caption": "We introduce a self-supervised method for automatically extracting color-concept associations from natural images. We apply a colorization neural network to predict color distributions for input images. The distributions are transformed into ratings for a color library, which are then aggregated across multiple images of a concept.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "https://youtu.be/iSXB800cU7w",
                        "ff_id": "iSXB800cU7w"
                    },
                    {
                        "slot_id": "v-full-1221-qa",
                        "session_id": "full2",
                        "type": "Virtual Q+A",
                        "title": "Self-Supervised Color-Concept Association via Image Colorization (Q+A)",
                        "contributors": [
                            "Ruizhen Hu",
                            "Ziqi Ye"
                        ],
                        "authors": [],
                        "abstract": "The interpretation of colors in visualizations is facilitated when the assignments between colors and concepts in the visualizations match human\u2019s expectations, implying that the colors can be interpreted in a semantic manner. However, manually creating a dataset of suitable associations between colors and concepts for use in visualizations is costly, as such associations would have to be collected from humans for a large variety of concepts. To address the challenge of collecting this data, we introduce a method to extract color-concept associations automatically from a set of concept images. While the state-of-the-art method extracts associations from data with supervised learning, we developed a self-supervised method based on colorization that does not require the preparation of ground truth color-concept associations. Our key insight is that a set of images of a concept should be sufficient for learning color-concept associations, since humans also learn to associate colors to concepts mainly from past visual input. Thus, we propose to use an automatic colorization method to extract statistical models of the color-concept associations that appear in concept images. Specifically, we take a colorization model pre-trained on ImageNet and fine-tune it on the set of images associated with a given concept, to predict pixel-wise probability distributions in Lab color space for the images. Then, we convert the predicted probability distributions into color ratings for a given color library and aggregate them for all the images of a concept to obtain the final color-concept associations. We evaluate our method using four different evaluation metrics and via a user study. Experiments show that, although the state-of-the-art method based on supervised learning with user-provided ratings is more effective at capturing relative associations, our self-supervised method obtains overall better results according to metrics like Earth Mover\u2019s Distance (EMD) and Entropy Difference (ED), which are closer to human perception of color distributions.",
                        "uid": "v-full-1221",
                        "file_name": "",
                        "time_stamp": "2022-10-19T16:19:17Z",
                        "time_start": "2022-10-19T16:19:17Z",
                        "time_end": "2022-10-19T16:21:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "617",
                        "paper_award": "",
                        "image_caption": "We introduce a self-supervised method for automatically extracting color-concept associations from natural images. We apply a colorization neural network to predict color distributions for input images. The distributions are transformed into ratings for a color library, which are then aggregated across multiple images of a concept.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "https://youtu.be/iSXB800cU7w",
                        "ff_id": "iSXB800cU7w"
                    },
                    {
                        "slot_id": "v-full-1616-pres",
                        "session_id": "full2",
                        "type": "In Person Presentation",
                        "title": "Cultivating Visualization Literacy for Children through Curiosity and Play",
                        "contributors": [
                            "S. Sandra Bae"
                        ],
                        "authors": [
                            "S. Sandra Bae",
                            "Rishi Vanukuru",
                            "Ruhan Yang",
                            "Peter Gyory",
                            "Ran Zhou",
                            "Ellen Yi-Luen Do",
                            "Danielle Albers Szafir"
                        ],
                        "abstract": "Fostering data visualization literacy (DVL) as part of childhood education could lead to a more data literate society. However, most work in DVL for children relies on a more formal educational context (i.e., a teacher-led approach) that limits children's engagement with data to classroom-based environments and, consequently, children's ability to ask questions about and explore data on topics they find personally meaningful. We explore how a curiosity-driven, child-led approach can provide more agency to children when they are authoring data visualizations. This paper explores how informal learning with crafting physicalizations through play and curiosity may foster increased literacy and engagement with data. Employing a constructionist approach, we designed a do-it-yourself toolkit made out of everyday materials (e.g., paper, cardboard, mirrors) that enables children to create, customize, and personalize three different interactive visualizations (bar, line, pie). We used the toolkit as a design probe in a series of in-person workshops with 5 children (6 to 11-year-olds) and interviews with 5 educators. Our observations reveal that the toolkit helped children creatively engage and interact with visualizations. Children with prior knowledge of data visualization reported the toolkit serving as more of an authoring tool that they envision using in their daily lives, while children with little to no experience found the toolkit as an engaging introduction to data visualization. Our study demonstrates the potential of using the constructionist approach to cultivate children's DVL through curiosity and play.",
                        "uid": "v-full-1616",
                        "file_name": "v-full-1616__Presentation.mp4",
                        "time_stamp": "2022-10-19T16:21:00Z",
                        "time_start": "2022-10-19T16:21:00Z",
                        "time_end": "2022-10-19T16:31:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "718",
                        "paper_award": "",
                        "image_caption": "Data is Yours. A toolkit to help children personalize visualizations made out of everyday materials and accumulate playful learning experiences.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "https://youtu.be/MCx2sg0oWN0",
                        "ff_id": "MCx2sg0oWN0"
                    },
                    {
                        "slot_id": "v-full-1616-qa",
                        "session_id": "full2",
                        "type": "In Person Q+A",
                        "title": "Cultivating Visualization Literacy for Children through Curiosity and Play (Q+A)",
                        "contributors": [
                            "S. Sandra Bae"
                        ],
                        "authors": [],
                        "abstract": "Fostering data visualization literacy (DVL) as part of childhood education could lead to a more data literate society. However, most work in DVL for children relies on a more formal educational context (i.e., a teacher-led approach) that limits children's engagement with data to classroom-based environments and, consequently, children's ability to ask questions about and explore data on topics they find personally meaningful. We explore how a curiosity-driven, child-led approach can provide more agency to children when they are authoring data visualizations. This paper explores how informal learning with crafting physicalizations through play and curiosity may foster increased literacy and engagement with data. Employing a constructionist approach, we designed a do-it-yourself toolkit made out of everyday materials (e.g., paper, cardboard, mirrors) that enables children to create, customize, and personalize three different interactive visualizations (bar, line, pie). We used the toolkit as a design probe in a series of in-person workshops with 5 children (6 to 11-year-olds) and interviews with 5 educators. Our observations reveal that the toolkit helped children creatively engage and interact with visualizations. Children with prior knowledge of data visualization reported the toolkit serving as more of an authoring tool that they envision using in their daily lives, while children with little to no experience found the toolkit as an engaging introduction to data visualization. Our study demonstrates the potential of using the constructionist approach to cultivate children's DVL through curiosity and play.",
                        "uid": "v-full-1616",
                        "file_name": "",
                        "time_stamp": "2022-10-19T16:31:00Z",
                        "time_start": "2022-10-19T16:31:00Z",
                        "time_end": "2022-10-19T16:33:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "718",
                        "paper_award": "",
                        "image_caption": "Data is Yours. A toolkit to help children personalize visualizations made out of everyday materials and accumulate playful learning experiences.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "https://youtu.be/MCx2sg0oWN0",
                        "ff_id": "MCx2sg0oWN0"
                    },
                    {
                        "slot_id": "v-full-1237-pres",
                        "session_id": "full2",
                        "type": "In Person Presentation",
                        "title": "Roboviz: A Game-Centered Project for Information Visualization Education",
                        "contributors": [
                            "Eytan Adar",
                            "Elsie Lee-Robbins"
                        ],
                        "authors": [
                            "Eytan Adar",
                            "Elsie Lee-Robbins"
                        ],
                        "abstract": "Due to their pedagogical advantages, large final projects in information visualization courses have become standard practice. Students take on a client--real or simulated--a dataset, and a vague set of goals to create a complete visualization or visual analytics product. Unfortunately, many projects suffer from ambiguous goals, over or under-constrained client expectations, and data constraints that have students spending their time on non-visualization problems (e.g., data cleaning). These are important skills, but are often secondary course objectives, and unforeseen problems can majorly hinder students. We created an alternative for our information visualization course: Roboviz, a real-time game for students to play by building a visualization-focused interface. By designing the game mechanics around four different data types, the project allows students to create a wide array of interactive visualizations. Student teams play against their classmates with the objective to collect the most (good) robots. The flexibility of the strategies encourages variability, a range of approaches, and solving wicked design constraints. We describe the construction of this game and report on student projects over two years. We further show how the game mechanics can be extended or adapted to other game-based projects.",
                        "uid": "v-full-1237",
                        "file_name": "v-full-1237_Adar_Presentation.mp4",
                        "time_stamp": "2022-10-19T16:33:00Z",
                        "time_start": "2022-10-19T16:33:00Z",
                        "time_end": "2022-10-19T16:43:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "659",
                        "paper_award": "",
                        "image_caption": "A couple of robots from our Roboviz project documentation (left) and a set of projects produced by students (right). The top three visualizations (A, B, C) were different ways to visualize robot \"productivity\"--a key way to earn points in the game. The bottom three (D, E, F) are example visualizations of the network and hierarchical data from one team's dashboard. Each reflected a different possible strategy to earn points in the game.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "https://youtu.be/V7QcQRRmg7k",
                        "ff_id": "V7QcQRRmg7k"
                    },
                    {
                        "slot_id": "v-full-1237-qa",
                        "session_id": "full2",
                        "type": "In Person Q+A",
                        "title": "Roboviz: A Game-Centered Project for Information Visualization Education (Q+A)",
                        "contributors": [
                            "Eytan Adar",
                            "Elsie Lee-Robbins"
                        ],
                        "authors": [],
                        "abstract": "Due to their pedagogical advantages, large final projects in information visualization courses have become standard practice. Students take on a client--real or simulated--a dataset, and a vague set of goals to create a complete visualization or visual analytics product. Unfortunately, many projects suffer from ambiguous goals, over or under-constrained client expectations, and data constraints that have students spending their time on non-visualization problems (e.g., data cleaning). These are important skills, but are often secondary course objectives, and unforeseen problems can majorly hinder students. We created an alternative for our information visualization course: Roboviz, a real-time game for students to play by building a visualization-focused interface. By designing the game mechanics around four different data types, the project allows students to create a wide array of interactive visualizations. Student teams play against their classmates with the objective to collect the most (good) robots. The flexibility of the strategies encourages variability, a range of approaches, and solving wicked design constraints. We describe the construction of this game and report on student projects over two years. We further show how the game mechanics can be extended or adapted to other game-based projects.",
                        "uid": "v-full-1237",
                        "file_name": "",
                        "time_stamp": "2022-10-19T16:43:00Z",
                        "time_start": "2022-10-19T16:43:00Z",
                        "time_end": "2022-10-19T16:45:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "659",
                        "paper_award": "",
                        "image_caption": "A couple of robots from our Roboviz project documentation (left) and a set of projects produced by students (right). The top three visualizations (A, B, C) were different ways to visualize robot \"productivity\"--a key way to earn points in the game. The bottom three (D, E, F) are example visualizations of the network and hierarchical data from one team's dashboard. Each reflected a different possible strategy to earn points in the game.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "https://youtu.be/V7QcQRRmg7k",
                        "ff_id": "V7QcQRRmg7k"
                    },
                    {
                        "slot_id": "v-full-1621-pres",
                        "session_id": "full2",
                        "type": "In Person Presentation",
                        "title": "Relaxed Dot Plots: Faithful Visualization of Samples and Their Distribution",
                        "contributors": [
                            "Nils Rodrigues"
                        ],
                        "authors": [
                            "Nils Rodrigues",
                            "Christoph Schulz",
                            "S\u00f6ren D\u00f6ring",
                            "Daniel Baumgartner",
                            "Tim Krake",
                            "Daniel Weiskopf"
                        ],
                        "abstract": "We introduce relaxed dot plots as an improvement of nonlinear dot plots for unit visualization. Our plots produce more faithful data representations and reduce moir\u00e9 effects. Their contour is based on a customized kernel frequency estimation to match the shape of the distribution of underlying data values. Previous nonlinear layouts introduce column-centric nonlinear scaling of dot diameters for visualization of high-dynamic-range data with high peaks. We provide a mathematical approach to convert that column-centric scaling to our smooth envelope shape. This formalism allows us to use linear, root, and logarithmic scaling to find ideal dot sizes. Our method iteratively relaxes the dot layout for more correct and aesthetically pleasing results. To achieve this, we modified Lloyd's algorithm with additional constraints and heuristics. We evaluate the layouts of relaxed dot plots against a previously existing nonlinear variant and show that our algorithm produces less error regarding the underlying data while establishing the blue noise property that works against moir\u00e9 effects. Further, we analyze the readability of our relaxed plots in three crowd-sourced experiments. The results indicate that our proposed technique surpasses traditional dot plots.",
                        "uid": "v-full-1621",
                        "file_name": "v-full-1621_Rodrigues_Presentation.mp4",
                        "time_stamp": "2022-10-19T16:45:00Z",
                        "time_start": "2022-10-19T16:45:00Z",
                        "time_end": "2022-10-19T16:55:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "533",
                        "paper_award": "",
                        "image_caption": "Relaxed dot plot of the percentage of renewables in electricity production. Dots represent countries as indicated by flag. Outlines show the corresponding continent: Africa (blue), America (green), Asia (red), Europe (yellow), Oceania (black). Root scaling shrinks dots in areas with high value frequency. The technique represents subtle differences in values more faithfully than traditional dot plots through relaxation of stacks.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "https://youtu.be/r4letvZDJWQ",
                        "ff_id": "r4letvZDJWQ"
                    },
                    {
                        "slot_id": "v-full-1621-qa",
                        "session_id": "full2",
                        "type": "In Person Q+A",
                        "title": "Relaxed Dot Plots: Faithful Visualization of Samples and Their Distribution (Q+A)",
                        "contributors": [
                            "Nils Rodrigues"
                        ],
                        "authors": [],
                        "abstract": "We introduce relaxed dot plots as an improvement of nonlinear dot plots for unit visualization. Our plots produce more faithful data representations and reduce moir\u00e9 effects. Their contour is based on a customized kernel frequency estimation to match the shape of the distribution of underlying data values. Previous nonlinear layouts introduce column-centric nonlinear scaling of dot diameters for visualization of high-dynamic-range data with high peaks. We provide a mathematical approach to convert that column-centric scaling to our smooth envelope shape. This formalism allows us to use linear, root, and logarithmic scaling to find ideal dot sizes. Our method iteratively relaxes the dot layout for more correct and aesthetically pleasing results. To achieve this, we modified Lloyd's algorithm with additional constraints and heuristics. We evaluate the layouts of relaxed dot plots against a previously existing nonlinear variant and show that our algorithm produces less error regarding the underlying data while establishing the blue noise property that works against moir\u00e9 effects. Further, we analyze the readability of our relaxed plots in three crowd-sourced experiments. The results indicate that our proposed technique surpasses traditional dot plots.",
                        "uid": "v-full-1621",
                        "file_name": "",
                        "time_stamp": "2022-10-19T16:55:00Z",
                        "time_start": "2022-10-19T16:55:00Z",
                        "time_end": "2022-10-19T16:57:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "533",
                        "paper_award": "",
                        "image_caption": "Relaxed dot plot of the percentage of renewables in electricity production. Dots represent countries as indicated by flag. Outlines show the corresponding continent: Africa (blue), America (green), Asia (red), Europe (yellow), Oceania (black). Root scaling shrinks dots in areas with high value frequency. The technique represents subtle differences in values more faithfully than traditional dot plots through relaxation of stacks.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "https://youtu.be/r4letvZDJWQ",
                        "ff_id": "r4letvZDJWQ"
                    }
                ]
            },
            {
                "title": "Dealing with Scale, Space, and Dimension",
                "session_id": "full3",
                "event_prefix": "v-full",
                "track": "ok6",
                "livestream_id": "ok6-wed",
                "session_image": "full3.png",
                "chair": [
                    "Joshua A. Levine"
                ],
                "organizers": [],
                "time_start": "2022-10-19T20:45:00Z",
                "time_end": "2022-10-19T22:00:00Z",
                "discord_category": "",
                "discord_channel": "oklahoma-st-6",
                "discord_channel_id": "1024600906689421372",
                "discord_link": "https://discord.com/channels/978320435554947082/1024600906689421372",
                "slido_link": "https://app.sli.do/event/i28UETZaiRWr1XwX6eKjcf?section=29ba1f79-04cd-4568-a83c-ab81aa3c28b8",
                "youtube_url": "https://youtu.be/L523gBLIM5c",
                "youtube_id": "L523gBLIM5c",
                "zoom_meeting": "",
                "zoom_password": "",
                "zoom_link": "",
                "ff_link": "https://youtu.be/q5JFwUFYCHw",
                "ff_playlist": "",
                "time_slots": [
                    {
                        "slot_id": "full3-opening",
                        "session_id": "full3",
                        "type": "In Person Other",
                        "title": "Session Opening",
                        "contributors": [
                            "Joshua A. Levine"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "",
                        "file_name": "",
                        "time_stamp": "2022-10-19T20:45:00Z",
                        "time_start": "2022-10-19T20:45:00Z",
                        "time_end": "2022-10-19T20:45:00Z",
                        "paper_type": "",
                        "keywords": [],
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "v-full-1406-pres",
                        "session_id": "full3",
                        "type": "In Person Presentation",
                        "title": "Multivariate Probabilistic Range Queries for Scalable Interactive 3D Visualization",
                        "contributors": [
                            "Markus Hadwiger",
                            "Amani Ageeli"
                        ],
                        "authors": [
                            "Amani Waleed Ageeli",
                            "Alberto Jaspe-Villanueva",
                            "Ronell Sicat",
                            "Florian Mannuss",
                            "Peter Rautek",
                            "Markus Hadwiger"
                        ],
                        "abstract": "Large-scale scientific data, such as weather and climate simulations, often comprise a large number of attributes for each data sample, like temperature, pressure, humidity, and many more. Interactive visualization and analysis require filtering according to any desired combination of attributes, in particular logical AND operations, which is challenging for large data and many attributes. Many general data structures for this problem are built for and scale with a fixed number of attributes, and scalability of joint queries with arbitrary attribute subsets remains a significant problem. We propose a flexible probabilistic framework for multivariate range queries that decouples all attribute dimensions via projection, allowing any subset of attributes to be queried with full efficiency. Moreover, our approach is output-sensitive, mainly scaling with the cardinality of the query result rather than with the input data size. This is particularly important for joint attribute queries, where the query output is usually much smaller than the whole data set. Additionally, our approach can split query evaluation between user interaction and rendering, achieving much better scalability for interactive visualization than the previous state of the art. Furthermore, even when a multi-resolution strategy is used for visualization, queries are jointly evaluated at the finest data granularity, because our framework does not limit query accuracy to a fixed spatial subdivision.",
                        "uid": "v-full-1406",
                        "file_name": "v-full-1406_Ageeli_Presentation.mp4",
                        "time_stamp": "2022-10-19T20:45:00Z",
                        "time_start": "2022-10-19T20:45:00Z",
                        "time_end": "2022-10-19T20:55:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "629",
                        "paper_award": "",
                        "image_caption": "Large-scale scientific data, such as climate simulations, often comprise a large number of attributes for each data sample, like temperature, pressure, humidity, and many more. Interactive visualization and analysis require filtering according to any desired combination of attributes, which is challenging for large data and many attributes. We propose a flexible probabilistic framework for multivariate range queries that decouples all attribute dimensions via projection, allowing any subset of attributes to be queried with full efficiency. Moreover, our approach is output-sensitive, mainly scaling with the cardinality of the query result rather than with the input data size.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "https://youtu.be/8X8Ctt10oII",
                        "ff_id": "8X8Ctt10oII"
                    },
                    {
                        "slot_id": "v-full-1406-qa",
                        "session_id": "full3",
                        "type": "In Person Q+A",
                        "title": "Multivariate Probabilistic Range Queries for Scalable Interactive 3D Visualization (Q+A)",
                        "contributors": [
                            "Markus Hadwiger",
                            "Amani Ageeli"
                        ],
                        "authors": [],
                        "abstract": "Large-scale scientific data, such as weather and climate simulations, often comprise a large number of attributes for each data sample, like temperature, pressure, humidity, and many more. Interactive visualization and analysis require filtering according to any desired combination of attributes, in particular logical AND operations, which is challenging for large data and many attributes. Many general data structures for this problem are built for and scale with a fixed number of attributes, and scalability of joint queries with arbitrary attribute subsets remains a significant problem. We propose a flexible probabilistic framework for multivariate range queries that decouples all attribute dimensions via projection, allowing any subset of attributes to be queried with full efficiency. Moreover, our approach is output-sensitive, mainly scaling with the cardinality of the query result rather than with the input data size. This is particularly important for joint attribute queries, where the query output is usually much smaller than the whole data set. Additionally, our approach can split query evaluation between user interaction and rendering, achieving much better scalability for interactive visualization than the previous state of the art. Furthermore, even when a multi-resolution strategy is used for visualization, queries are jointly evaluated at the finest data granularity, because our framework does not limit query accuracy to a fixed spatial subdivision.",
                        "uid": "v-full-1406",
                        "file_name": "",
                        "time_stamp": "2022-10-19T20:55:00Z",
                        "time_start": "2022-10-19T20:55:00Z",
                        "time_end": "2022-10-19T20:57:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "629",
                        "paper_award": "",
                        "image_caption": "Large-scale scientific data, such as climate simulations, often comprise a large number of attributes for each data sample, like temperature, pressure, humidity, and many more. Interactive visualization and analysis require filtering according to any desired combination of attributes, which is challenging for large data and many attributes. We propose a flexible probabilistic framework for multivariate range queries that decouples all attribute dimensions via projection, allowing any subset of attributes to be queried with full efficiency. Moreover, our approach is output-sensitive, mainly scaling with the cardinality of the query result rather than with the input data size.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "https://youtu.be/8X8Ctt10oII",
                        "ff_id": "8X8Ctt10oII"
                    },
                    {
                        "slot_id": "v-full-1394-pres",
                        "session_id": "full3",
                        "type": "Virtual Presentation (pre-recorded)",
                        "title": "Dual Space Coupling Model Guided Overlap-free Scatterplot",
                        "contributors": [
                            "zeyu li"
                        ],
                        "authors": [
                            "zeyu li",
                            "RuiZhi Shi",
                            "Yan Liu",
                            "Shizhuo Long",
                            "Ziheng Guo",
                            "Shichao Jia",
                            "Jiawan Zhang"
                        ],
                        "abstract": "The overdraw problem of scatterplots seriously interferes with the visual tasks. Existing methods, such as data sampling, node dispersion, subspace mapping, and visual abstraction, cannot guarantee the correspondence and consistency between the data points that reflect the intrinsic original data distribution and the corresponding visual units that reveal the presented data distribution, thus failing to obtain an overlap-free scatterplot with unbiased and lossless data distribution. A dual space coupling model is proposed in this paper to represent the complex bilateral relationship between data space and visual space theoretically and analytically. Under the guidance of the model, an overlap-free scatterplot method is developed through integration of the following: a geometry-based data transformation algorithm, namely DistributionTranscriptor; an efficient spatial mutual exclusion guided view transformation algorithm, namely PolarPacking; an overlap-free oriented visual encoding configuration model and a radius adjustment tool, namely $f_{r_{draw}}$. Our method can ensure complete and accurate information transfer between the two spaces, maintaining consistency between the newly created scatterplot and the original data distribution on global and local features. Quantitative evaluation proves our remarkable progress on computational efficiency compared with the state-of-the-art methods. Three applications involving pattern enhancement, interaction improvement, and overdraw mitigation of trajectory visualization demonstrate the broad prospects of our method.",
                        "uid": "v-full-1394",
                        "file_name": "v-full-1394_Li_Presentation.mp4",
                        "time_stamp": "2022-10-19T20:57:00Z",
                        "time_start": "2022-10-19T20:57:00Z",
                        "time_end": "2022-10-19T21:07:09Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "609",
                        "paper_award": "",
                        "image_caption": "In our paper, we contributed a dual space coupling model that introduces a new design space for promising overlap removal algorithm and interaction paradigm for large-scale scatterplots. We also developed an overlap-free\nscatterplot visualization method based on the model, which shows competitive advantages compared with the state-of-the-art overlap removal methods.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "v-full-1394-qa",
                        "session_id": "full3",
                        "type": "Virtual Q+A",
                        "title": "Dual Space Coupling Model Guided Overlap-free Scatterplot (Q+A)",
                        "contributors": [
                            "zeyu li"
                        ],
                        "authors": [],
                        "abstract": "The overdraw problem of scatterplots seriously interferes with the visual tasks. Existing methods, such as data sampling, node dispersion, subspace mapping, and visual abstraction, cannot guarantee the correspondence and consistency between the data points that reflect the intrinsic original data distribution and the corresponding visual units that reveal the presented data distribution, thus failing to obtain an overlap-free scatterplot with unbiased and lossless data distribution. A dual space coupling model is proposed in this paper to represent the complex bilateral relationship between data space and visual space theoretically and analytically. Under the guidance of the model, an overlap-free scatterplot method is developed through integration of the following: a geometry-based data transformation algorithm, namely DistributionTranscriptor; an efficient spatial mutual exclusion guided view transformation algorithm, namely PolarPacking; an overlap-free oriented visual encoding configuration model and a radius adjustment tool, namely $f_{r_{draw}}$. Our method can ensure complete and accurate information transfer between the two spaces, maintaining consistency between the newly created scatterplot and the original data distribution on global and local features. Quantitative evaluation proves our remarkable progress on computational efficiency compared with the state-of-the-art methods. Three applications involving pattern enhancement, interaction improvement, and overdraw mitigation of trajectory visualization demonstrate the broad prospects of our method.",
                        "uid": "v-full-1394",
                        "file_name": "",
                        "time_stamp": "2022-10-19T21:07:09Z",
                        "time_start": "2022-10-19T21:07:09Z",
                        "time_end": "2022-10-19T21:09:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "609",
                        "paper_award": "",
                        "image_caption": "In our paper, we contributed a dual space coupling model that introduces a new design space for promising overlap removal algorithm and interaction paradigm for large-scale scatterplots. We also developed an overlap-free\nscatterplot visualization method based on the model, which shows competitive advantages compared with the state-of-the-art overlap removal methods.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "v-full-1124-pres",
                        "session_id": "full3",
                        "type": "In Person Presentation",
                        "title": "Measuring Effects of Spatial Visualization and Domain on Visualization Task Performance: A Comparative Study",
                        "contributors": [
                            "Sara Tandon"
                        ],
                        "authors": [
                            "Sara Tandon",
                            "Alfie Abdul-Rahman",
                            "Rita Borgo"
                        ],
                        "abstract": "Understanding one\u2019s audience is foundational to creating high impact visualization designs. However, individual differences and cognitive abilities influence interactions with information visualization. Different user needs and abilities suggest that an individual\u2019s background could influence cognitive performance and interactions with visuals in a systematic way. This study builds on current research in domain-specific visualization and cognition to address if domain and spatial visualization ability combine to affect performance on information visualization tasks. We measure spatial visualization and visual task performance between those with tertiary education and professional profile in business, law & political science, and math & computer science. We conducted an online study with 90 participants using an established psychometric test to assess spatial visualization ability, and bar chart layouts rotated along Cartesian and polar coordinates to assess performance on spatially rotated data. Accuracy and response times varied with domain across chart types and task difficulty. We found that accuracy and time correlate with spatial visualization level, and education in math & computer science can indicate higher spatial visualization. Additionally, we found that motivational differences between domains could contribute to increased levels of accuracy. Our findings indicate discipline not only affects user needs and interactions with data visualization, but also cognitive traits. Our results can advance inclusive practices in visualization design and add to knowledge in domain-specific visual research that can empower designers across disciplines to create effective visualizations.",
                        "uid": "v-full-1124",
                        "file_name": "v-full-1124_Tandon_Presentation.mp4",
                        "time_stamp": "2022-10-19T21:09:00Z",
                        "time_start": "2022-10-19T21:09:00Z",
                        "time_end": "2022-10-19T21:19:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "747",
                        "paper_award": "",
                        "image_caption": "This study builds on current research in domain-specific visualization and cognition to address if domain and spatial visualization ability combine to affect performance on information visualization tasks. \nWe measure spatial visualization and visual task performance between those with tertiary education and professional profile in business, law & political science, and math & computer science. \nStimuli consisted of bar chart layouts rotated along Cartesian and polar coordinates to assess performance on spatially rotated data. \nOur results can advance inclusive practices in visualization design and add to knowledge in domain-specific visual research that can empower designers across disciplines to create effective visualizations.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "https://youtu.be/9fMhNt5XZSs",
                        "ff_id": "9fMhNt5XZSs"
                    },
                    {
                        "slot_id": "v-full-1124-qa",
                        "session_id": "full3",
                        "type": "In Person Q+A",
                        "title": "Measuring Effects of Spatial Visualization and Domain on Visualization Task Performance: A Comparative Study (Q+A)",
                        "contributors": [
                            "Sara Tandon"
                        ],
                        "authors": [],
                        "abstract": "Understanding one\u2019s audience is foundational to creating high impact visualization designs. However, individual differences and cognitive abilities influence interactions with information visualization. Different user needs and abilities suggest that an individual\u2019s background could influence cognitive performance and interactions with visuals in a systematic way. This study builds on current research in domain-specific visualization and cognition to address if domain and spatial visualization ability combine to affect performance on information visualization tasks. We measure spatial visualization and visual task performance between those with tertiary education and professional profile in business, law & political science, and math & computer science. We conducted an online study with 90 participants using an established psychometric test to assess spatial visualization ability, and bar chart layouts rotated along Cartesian and polar coordinates to assess performance on spatially rotated data. Accuracy and response times varied with domain across chart types and task difficulty. We found that accuracy and time correlate with spatial visualization level, and education in math & computer science can indicate higher spatial visualization. Additionally, we found that motivational differences between domains could contribute to increased levels of accuracy. Our findings indicate discipline not only affects user needs and interactions with data visualization, but also cognitive traits. Our results can advance inclusive practices in visualization design and add to knowledge in domain-specific visual research that can empower designers across disciplines to create effective visualizations.",
                        "uid": "v-full-1124",
                        "file_name": "",
                        "time_stamp": "2022-10-19T21:19:00Z",
                        "time_start": "2022-10-19T21:19:00Z",
                        "time_end": "2022-10-19T21:21:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "747",
                        "paper_award": "",
                        "image_caption": "This study builds on current research in domain-specific visualization and cognition to address if domain and spatial visualization ability combine to affect performance on information visualization tasks. \nWe measure spatial visualization and visual task performance between those with tertiary education and professional profile in business, law & political science, and math & computer science. \nStimuli consisted of bar chart layouts rotated along Cartesian and polar coordinates to assess performance on spatially rotated data. \nOur results can advance inclusive practices in visualization design and add to knowledge in domain-specific visual research that can empower designers across disciplines to create effective visualizations.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "https://youtu.be/9fMhNt5XZSs",
                        "ff_id": "9fMhNt5XZSs"
                    },
                    {
                        "slot_id": "v-tvcg-9735308-pres",
                        "session_id": "full3",
                        "type": "In Person Presentation",
                        "title": "Local Latent Representation based on Geometric Convolution for Particle Data Feature Exploration",
                        "contributors": [
                            "Li, Haoyu",
                            "Haoyu Li"
                        ],
                        "authors": [
                            "Haoyu Li",
                            "Han-Wei Shen"
                        ],
                        "abstract": "Feature related particle data analysis plays an important role in many scientific applications such as fluid simulations, cosmology simulations and molecular dynamics. Compared to conventional methods that use hand-crafted feature descriptors, some recent studies focus on transforming the data into a new latent space, where features are easier to be identified, compared and extracted. However, it is challenging to transform particle data into latent representations, since the convolution neural networks used in prior studies require the data presented in regular grids. In this paper, we adopt Geometric Convolution, a neural network building block designed for 3D point clouds, to create latent representations for scientific particle data. These latent representations capture both the particle positions and their physical attributes in the local neighborhood so that features can be extracted by clustering in the latent space, and tracked by applying tracking algorithms such as mean-shift. We validate the extracted features and tracking results from our approach using datasets from three applications and show that they are comparable to the methods that define hand-crafted features for each specific dataset.",
                        "uid": "v-tvcg-9735308",
                        "file_name": "v-tvcg-9735308_Li_Presentation.mp4",
                        "time_stamp": "2022-10-19T21:21:00Z",
                        "time_start": "2022-10-19T21:21:00Z",
                        "time_end": "2022-10-19T21:31:00Z",
                        "paper_type": "full",
                        "keywords": [
                            "Data transformation, Particle data, Feature extraction and tracking, Deep learning"
                        ],
                        "has_image": "1",
                        "has_video": "591",
                        "paper_award": "",
                        "image_caption": "We present a particle data feature analysis method using geometric convolution based neural networks. Feature descriptor is defined using geometric convolution and automatedly learned using an autoencoder architecture. The bottom left figure shows an overview workflow. With our designed feature exploration tool (top left figure), we demonstrate our approach has comparable feature extraction quality (left figure) and does not require tedious feature descriptor crafting or costly data resampling to regular grids.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "https://youtu.be/_EmDXFy7UDs",
                        "ff_id": "_EmDXFy7UDs"
                    },
                    {
                        "slot_id": "v-tvcg-9735308-qa",
                        "session_id": "full3",
                        "type": "In Person Q+A",
                        "title": "Local Latent Representation based on Geometric Convolution for Particle Data Feature Exploration (Q+A)",
                        "contributors": [
                            "Li, Haoyu",
                            "Haoyu Li"
                        ],
                        "authors": [],
                        "abstract": "Feature related particle data analysis plays an important role in many scientific applications such as fluid simulations, cosmology simulations and molecular dynamics. Compared to conventional methods that use hand-crafted feature descriptors, some recent studies focus on transforming the data into a new latent space, where features are easier to be identified, compared and extracted. However, it is challenging to transform particle data into latent representations, since the convolution neural networks used in prior studies require the data presented in regular grids. In this paper, we adopt Geometric Convolution, a neural network building block designed for 3D point clouds, to create latent representations for scientific particle data. These latent representations capture both the particle positions and their physical attributes in the local neighborhood so that features can be extracted by clustering in the latent space, and tracked by applying tracking algorithms such as mean-shift. We validate the extracted features and tracking results from our approach using datasets from three applications and show that they are comparable to the methods that define hand-crafted features for each specific dataset.",
                        "uid": "v-tvcg-9735308",
                        "file_name": "",
                        "time_stamp": "2022-10-19T21:31:00Z",
                        "time_start": "2022-10-19T21:31:00Z",
                        "time_end": "2022-10-19T21:33:00Z",
                        "paper_type": "full",
                        "keywords": [
                            "Data transformation, Particle data, Feature extraction and tracking, Deep learning"
                        ],
                        "has_image": "1",
                        "has_video": "591",
                        "paper_award": "",
                        "image_caption": "We present a particle data feature analysis method using geometric convolution based neural networks. Feature descriptor is defined using geometric convolution and automatedly learned using an autoencoder architecture. The bottom left figure shows an overview workflow. With our designed feature exploration tool (top left figure), we demonstrate our approach has comparable feature extraction quality (left figure) and does not require tedious feature descriptor crafting or costly data resampling to regular grids.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "https://youtu.be/_EmDXFy7UDs",
                        "ff_id": "_EmDXFy7UDs"
                    },
                    {
                        "slot_id": "v-tvcg-9765327-pres",
                        "session_id": "full3",
                        "type": "Virtual Presentation (pre-recorded)",
                        "title": "Graphical Enhancements for Effective Exemplar Identification in Contextual Data Visualizations",
                        "contributors": [
                            "Shenghui Cheng"
                        ],
                        "authors": [
                            "Xinyu Zhang",
                            "Shenghui Cheng",
                            "Klaus Mueller"
                        ],
                        "abstract": "An exemplar is an entity that represents a desirable instance in a multi-attribute configuration space. It offers certain strengths in some of its attributes without unduly compromising the strengths in other attributes. Exemplars are frequently sought after in real life applications, such as systems engineering, investment banking, drug advisory, product marketing and many others. We study a specific method for the visualization of multi-attribute configuration spaces, the Data Context Map (DCM), for its capacity in enabling users to identify proper exemplars. The DCM produces a 2D embedding where users can view the data objects in the context of the data attributes. We ask whether certain graphical enhancements can aid users to gain a better understanding of the attribute-wise tradeoffs and so select better exemplar sets.  We conducted several user studies for three different graphical designs, namely iso-contour, value-shaded topographic rendering and terrain topographic rendering, and compare these with a baseline DCM display. As a benchmark we use an exemplar set generated via Pareto optimization which has similar goals but unlike humans can operate in the native high-dimensional data space. Our study finds that the two topographic maps are statistically superior to both the iso-contour and the DCM baseline display.",
                        "uid": "v-tvcg-9765327",
                        "file_name": "v-tvcg-9765327_Zhang_Presentation.mp4",
                        "time_stamp": "2022-10-19T21:33:00Z",
                        "time_start": "2022-10-19T21:33:00Z",
                        "time_end": "2022-10-19T21:44:28Z",
                        "paper_type": "full",
                        "keywords": [
                            "High-dimensional data, multivariate data, contextual displays, exemplar generation, decision support, configuration space"
                        ],
                        "has_image": "1",
                        "has_video": "688",
                        "paper_award": "",
                        "image_caption": "These are graphical enhancements on data context map tested by (a)wine dataset on iso-contour map, (b)university dataset on value-shaded map and (c)car dataset on terrain map. Points on paths are exemplars recommended by our Pareto guided searching algorithm and stacked bar charts show their attributes.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "https://youtu.be/A3HrumxydZc",
                        "ff_id": "A3HrumxydZc"
                    },
                    {
                        "slot_id": "v-tvcg-9765327-qa",
                        "session_id": "full3",
                        "type": "Virtual Q+A",
                        "title": "Graphical Enhancements for Effective Exemplar Identification in Contextual Data Visualizations (Q+A)",
                        "contributors": [
                            "Shenghui Cheng"
                        ],
                        "authors": [],
                        "abstract": "An exemplar is an entity that represents a desirable instance in a multi-attribute configuration space. It offers certain strengths in some of its attributes without unduly compromising the strengths in other attributes. Exemplars are frequently sought after in real life applications, such as systems engineering, investment banking, drug advisory, product marketing and many others. We study a specific method for the visualization of multi-attribute configuration spaces, the Data Context Map (DCM), for its capacity in enabling users to identify proper exemplars. The DCM produces a 2D embedding where users can view the data objects in the context of the data attributes. We ask whether certain graphical enhancements can aid users to gain a better understanding of the attribute-wise tradeoffs and so select better exemplar sets.  We conducted several user studies for three different graphical designs, namely iso-contour, value-shaded topographic rendering and terrain topographic rendering, and compare these with a baseline DCM display. As a benchmark we use an exemplar set generated via Pareto optimization which has similar goals but unlike humans can operate in the native high-dimensional data space. Our study finds that the two topographic maps are statistically superior to both the iso-contour and the DCM baseline display.",
                        "uid": "v-tvcg-9765327",
                        "file_name": "",
                        "time_stamp": "2022-10-19T21:44:28Z",
                        "time_start": "2022-10-19T21:44:28Z",
                        "time_end": "2022-10-19T21:45:00Z",
                        "paper_type": "full",
                        "keywords": [
                            "High-dimensional data, multivariate data, contextual displays, exemplar generation, decision support, configuration space"
                        ],
                        "has_image": "1",
                        "has_video": "688",
                        "paper_award": "",
                        "image_caption": "These are graphical enhancements on data context map tested by (a)wine dataset on iso-contour map, (b)university dataset on value-shaded map and (c)car dataset on terrain map. Points on paths are exemplars recommended by our Pareto guided searching algorithm and stacked bar charts show their attributes.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "https://youtu.be/A3HrumxydZc",
                        "ff_id": "A3HrumxydZc"
                    },
                    {
                        "slot_id": "v-tvcg-9514468-pres",
                        "session_id": "full3",
                        "type": "In Person Presentation",
                        "title": "Effectiveness Error: Measuring and Improving RadViz Visual Effectiveness",
                        "contributors": [
                            "Marco Angelini"
                        ],
                        "authors": [
                            "Marco Angelini",
                            "Graziano Blasilli",
                            "Simone Lenti",
                            "Alessia Palleschi",
                            "Giuseppe Santucci"
                        ],
                        "abstract": "RadViz contributes to multidimensional analysis by using 2D points for encoding data elements and interpreting them alongthe original data dimensions. For these characteristics it is used in different application domains, like clustering, anomaly detection, and software visualization. However, it is likely that using the dimension arrangement that comes with the data will produce a plot that leads users to make inaccurate conclusions about points values and data distribution. This paper attacks this problem without altering the original RadViz design: it defines, for both a single point and a set of points, the metric of effectiveness error, and uses it to define the objective function of a dimension arrangement strategy, arguing that minimizing it increases the overall RadViz visual quality. This paper investigated the intuition that reducing the effectiveness error is beneficial for other well-known RadViz problems, like points clumping toward the center, many-to-one plotting of non-proportional points, and cluster separation. It presents an algorithm that reduces to zero the effectiveness error for a single point and a heuristic that approximates the dimension arrangement minimizing the effectiveness error for an arbitrary set of points. A set of experiments based on 21 real datasets has been performed, with the goals of analyzing the advantages of reducing the effectiveness error, comparing the proposed dimension arrangement strategy with other related proposals, and investigating the heuristic accuracy. The Effectiveness Error metric, the algorithm, and the heuristic presented in this paper have been made available in a d3.jsplugin at  https://aware-diag-sapienza.github.io/d3-radviz.",
                        "uid": "v-tvcg-9514468",
                        "file_name": "v-tvcg-9514468_Angelini_Presentation.mp4",
                        "time_stamp": "2022-10-19T21:45:00Z",
                        "time_start": "2022-10-19T21:45:00Z",
                        "time_end": "2022-10-19T21:55:00Z",
                        "paper_type": "full",
                        "keywords": [
                            "Dimensionality reduction, RadViz, dimension arrangement, visual quality metrics."
                        ],
                        "has_image": "1",
                        "has_video": "1671",
                        "paper_award": "",
                        "image_caption": "Two RadViz charts show the CSM dataset using two different Dimension Arrangements out of about 20 million, highlighting the points' Effectiveness Error (EE).\n\nThe original dataset DA on the left shows quite a high EE for the points, suggesting inaccurate conclusions: the data distribution is led by Budget, Gross, and Screens.\n\nThe RadViz on the right uses the arrangement generated by the Effectiveness Error Minimization Heuristic proposed in this paper. Most of the points have an EE close to zero, suggesting that the current DA produces correct insights: the data distribution is leaded by Sentiment and Ratings.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "https://youtu.be/pgUopAeXnIs",
                        "ff_id": "pgUopAeXnIs"
                    },
                    {
                        "slot_id": "v-tvcg-9514468-qa",
                        "session_id": "full3",
                        "type": "In Person Q+A",
                        "title": "Effectiveness Error: Measuring and Improving RadViz Visual Effectiveness (Q+A)",
                        "contributors": [
                            "Marco Angelini"
                        ],
                        "authors": [],
                        "abstract": "RadViz contributes to multidimensional analysis by using 2D points for encoding data elements and interpreting them alongthe original data dimensions. For these characteristics it is used in different application domains, like clustering, anomaly detection, and software visualization. However, it is likely that using the dimension arrangement that comes with the data will produce a plot that leads users to make inaccurate conclusions about points values and data distribution. This paper attacks this problem without altering the original RadViz design: it defines, for both a single point and a set of points, the metric of effectiveness error, and uses it to define the objective function of a dimension arrangement strategy, arguing that minimizing it increases the overall RadViz visual quality. This paper investigated the intuition that reducing the effectiveness error is beneficial for other well-known RadViz problems, like points clumping toward the center, many-to-one plotting of non-proportional points, and cluster separation. It presents an algorithm that reduces to zero the effectiveness error for a single point and a heuristic that approximates the dimension arrangement minimizing the effectiveness error for an arbitrary set of points. A set of experiments based on 21 real datasets has been performed, with the goals of analyzing the advantages of reducing the effectiveness error, comparing the proposed dimension arrangement strategy with other related proposals, and investigating the heuristic accuracy. The Effectiveness Error metric, the algorithm, and the heuristic presented in this paper have been made available in a d3.jsplugin at  https://aware-diag-sapienza.github.io/d3-radviz.",
                        "uid": "v-tvcg-9514468",
                        "file_name": "",
                        "time_stamp": "2022-10-19T21:55:00Z",
                        "time_start": "2022-10-19T21:55:00Z",
                        "time_end": "2022-10-19T21:57:00Z",
                        "paper_type": "full",
                        "keywords": [
                            "Dimensionality reduction, RadViz, dimension arrangement, visual quality metrics."
                        ],
                        "has_image": "1",
                        "has_video": "1671",
                        "paper_award": "",
                        "image_caption": "Two RadViz charts show the CSM dataset using two different Dimension Arrangements out of about 20 million, highlighting the points' Effectiveness Error (EE).\n\nThe original dataset DA on the left shows quite a high EE for the points, suggesting inaccurate conclusions: the data distribution is led by Budget, Gross, and Screens.\n\nThe RadViz on the right uses the arrangement generated by the Effectiveness Error Minimization Heuristic proposed in this paper. Most of the points have an EE close to zero, suggesting that the current DA produces correct insights: the data distribution is leaded by Sentiment and Ratings.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "https://youtu.be/pgUopAeXnIs",
                        "ff_id": "pgUopAeXnIs"
                    }
                ]
            },
            {
                "title": "Text, Language, and Image Data",
                "session_id": "full4",
                "event_prefix": "v-full",
                "track": "ok4",
                "livestream_id": "ok4-thu",
                "session_image": "full4.png",
                "chair": [
                    "Steffen Koch"
                ],
                "organizers": [],
                "time_start": "2022-10-20T20:45:00Z",
                "time_end": "2022-10-20T22:00:00Z",
                "discord_category": "",
                "discord_channel": "oklahoma-st-4",
                "discord_channel_id": "1024600674924765264",
                "discord_link": "https://discord.com/channels/978320435554947082/1024600674924765264",
                "slido_link": "https://app.sli.do/event/i28UETZaiRWr1XwX6eKjcf?section=51658b34-0f5b-40c9-a335-1fd1468fad8d",
                "youtube_url": "https://youtu.be/0qY_NxLSGBk",
                "youtube_id": "0qY_NxLSGBk",
                "zoom_meeting": "",
                "zoom_password": "",
                "zoom_link": "",
                "ff_link": "https://youtu.be/B-PsUqvv2jg",
                "ff_playlist": "",
                "time_slots": [
                    {
                        "slot_id": "full4-opening",
                        "session_id": "full4",
                        "type": "In Person Other",
                        "title": "Session Opening",
                        "contributors": [
                            "Steffen Koch"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "",
                        "file_name": "",
                        "time_stamp": "2022-10-20T20:45:00Z",
                        "time_start": "2022-10-20T20:45:00Z",
                        "time_end": "2022-10-20T20:45:00Z",
                        "paper_type": "",
                        "keywords": [],
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "v-tvcg-9801603-pres",
                        "session_id": "full4",
                        "type": "Virtual Presentation (pre-recorded)",
                        "title": "A Unified Understanding of Deep NLP Models for Text Classification",
                        "contributors": [
                            "Weikai Yang",
                            "Zhen Li"
                        ],
                        "authors": [
                            "Zhen Li",
                            "Xiting Wang",
                            "Weikai Yang",
                            "Jing Wu",
                            "Zhengyan Zhang",
                            "Zhiyuan Liu",
                            "Maosong Sun",
                            "Hui Zhang",
                            "Shixia Liu"
                        ],
                        "abstract": "The rapid development of deep natural language processing (NLP) models for text classification has led to an urgent need for a unified understanding of these models proposed individually. Existing methods cannot meet the need for understanding different models in one framework due to the lack of a unified measure for explaining both low-level (e.g., words) and high-level (e.g., phrases) features. We have developed a visual analysis tool, DeepNLPVis, to enable a unified understanding of NLP models for text classification. The key idea is a mutual information-based measure, which provides quantitative explanations on how each layer of a model maintains the information of input words in a sample. We model the intra- and inter-word information at each layer measuring the importance of a word to the final prediction as well as the relationships between words, such as the formation of phrases. A multi-level visualization, which consists of a corpus-level, a sample-level, and a word-level visualization, supports the analysis from the overall training set to individual samples. Two case studies on classification tasks and comparison between models demonstrate that DeepNLPVis can help users effectively identify potential problems caused by samples and model architectures and then make informed improvements.",
                        "uid": "v-tvcg-9801603",
                        "file_name": "v-tvcg-9801603_Li_Presentation.mp4",
                        "time_stamp": "2022-10-20T20:45:00Z",
                        "time_start": "2022-10-20T20:45:00Z",
                        "time_end": "2022-10-20T20:54:21Z",
                        "paper_type": "full",
                        "keywords": [
                            "Explainable AI, visual debugging, visual analytics, deep NLP model, information-based interpretation"
                        ],
                        "has_image": "1",
                        "has_video": "561",
                        "paper_award": "",
                        "image_caption": "DeepNLPVis for analyzing the BERT model on news classification: (a) class view for showing the overall model performance; (b) distribution view for identifying samples and words of interest; (c) word contribution of selected samples; (d) sample list; (e) information flow for analyzing a sample by its intra- and inter-word information.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "https://youtu.be/1qOlxaqljmE",
                        "ff_id": "1qOlxaqljmE"
                    },
                    {
                        "slot_id": "v-tvcg-9801603-qa",
                        "session_id": "full4",
                        "type": "Virtual Q+A",
                        "title": "A Unified Understanding of Deep NLP Models for Text Classification (Q+A)",
                        "contributors": [
                            "Weikai Yang",
                            "Zhen Li"
                        ],
                        "authors": [],
                        "abstract": "The rapid development of deep natural language processing (NLP) models for text classification has led to an urgent need for a unified understanding of these models proposed individually. Existing methods cannot meet the need for understanding different models in one framework due to the lack of a unified measure for explaining both low-level (e.g., words) and high-level (e.g., phrases) features. We have developed a visual analysis tool, DeepNLPVis, to enable a unified understanding of NLP models for text classification. The key idea is a mutual information-based measure, which provides quantitative explanations on how each layer of a model maintains the information of input words in a sample. We model the intra- and inter-word information at each layer measuring the importance of a word to the final prediction as well as the relationships between words, such as the formation of phrases. A multi-level visualization, which consists of a corpus-level, a sample-level, and a word-level visualization, supports the analysis from the overall training set to individual samples. Two case studies on classification tasks and comparison between models demonstrate that DeepNLPVis can help users effectively identify potential problems caused by samples and model architectures and then make informed improvements.",
                        "uid": "v-tvcg-9801603",
                        "file_name": "",
                        "time_stamp": "2022-10-20T20:54:21Z",
                        "time_start": "2022-10-20T20:54:21Z",
                        "time_end": "2022-10-20T20:57:00Z",
                        "paper_type": "full",
                        "keywords": [
                            "Explainable AI, visual debugging, visual analytics, deep NLP model, information-based interpretation"
                        ],
                        "has_image": "1",
                        "has_video": "561",
                        "paper_award": "",
                        "image_caption": "DeepNLPVis for analyzing the BERT model on news classification: (a) class view for showing the overall model performance; (b) distribution view for identifying samples and words of interest; (c) word contribution of selected samples; (d) sample list; (e) information flow for analyzing a sample by its intra- and inter-word information.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "https://youtu.be/1qOlxaqljmE",
                        "ff_id": "1qOlxaqljmE"
                    },
                    {
                        "slot_id": "v-tvcg-9801527-pres",
                        "session_id": "full4",
                        "type": "In Person Presentation",
                        "title": "EBBE-Text: Explaining Neural Networks by Exploring Text Classification Decision Boundaries",
                        "contributors": [
                            "Alexis Delaforge"
                        ],
                        "authors": [
                            "Alexis Delaforge",
                            "J\u00e9r\u00f4me Az\u00e9",
                            "Sandra Bringay",
                            "Caroline Mollevi",
                            "Arnaud Sallaberry",
                            "Maximilien Servajean"
                        ],
                        "abstract": "While neural networks (NN) have been successfully applied to many NLP tasks, the way they function is often difficult to interpret. In this article, we focus on binary text classification via NNs and propose a new tool, which includes a visualization of the decision boundary and the distances of data elements to this boundary. This tool increases the interpretability of NN. Our approach uses two innovative views: (1) an overview of the text representation space and (2) a local view allowing data exploration around the decision boundary for various localities of this representation space. These views are integrated into a visual platform, EBBE-Text, which also contains state-of-the-art visualizations of NN representation spaces and several kinds of information obtained from the classification process. The various views are linked through numerous interactive functionalities that enable easy exploration of texts and classification results via the various complementary views. A user study shows the effectiveness of the visual encoding and a case study illustrates the benefits of using our tool for the analysis of the classifications obtained with several recent NNs and two datasets.",
                        "uid": "v-tvcg-9801527",
                        "file_name": "v-tvcg-9801527_Delaforge_Presentation.mp4",
                        "time_stamp": "2022-10-20T20:57:00Z",
                        "time_start": "2022-10-20T20:57:00Z",
                        "time_end": "2022-10-20T21:07:00Z",
                        "paper_type": "full",
                        "keywords": [
                            "Artificial neural networks, Data visualization, Computational modeling, Natural language processing, Predictive models, Task analysis, Deep learning, Binary text classification, decision boundary, deep learning, interpretability, neural networks, representation space, visual analytics"
                        ],
                        "has_image": "1",
                        "has_video": "554",
                        "paper_award": "",
                        "image_caption": "Locality view of EBBE-Text. From left to right and top to bottom: boundary sub-view, path text list, top 10 word list, text representation spaces resulting from dimension reduction, confusion matrix, input text form, classify command. With these features, EBBE-Text is used to explain neural network predictions for a binary text classification task.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "https://youtu.be/j3sHXXlFWZ8",
                        "ff_id": "j3sHXXlFWZ8"
                    },
                    {
                        "slot_id": "v-tvcg-9801527-qa",
                        "session_id": "full4",
                        "type": "In Person Q+A",
                        "title": "EBBE-Text: Explaining Neural Networks by Exploring Text Classification Decision Boundaries (Q+A)",
                        "contributors": [
                            "Alexis Delaforge"
                        ],
                        "authors": [],
                        "abstract": "While neural networks (NN) have been successfully applied to many NLP tasks, the way they function is often difficult to interpret. In this article, we focus on binary text classification via NNs and propose a new tool, which includes a visualization of the decision boundary and the distances of data elements to this boundary. This tool increases the interpretability of NN. Our approach uses two innovative views: (1) an overview of the text representation space and (2) a local view allowing data exploration around the decision boundary for various localities of this representation space. These views are integrated into a visual platform, EBBE-Text, which also contains state-of-the-art visualizations of NN representation spaces and several kinds of information obtained from the classification process. The various views are linked through numerous interactive functionalities that enable easy exploration of texts and classification results via the various complementary views. A user study shows the effectiveness of the visual encoding and a case study illustrates the benefits of using our tool for the analysis of the classifications obtained with several recent NNs and two datasets.",
                        "uid": "v-tvcg-9801527",
                        "file_name": "",
                        "time_stamp": "2022-10-20T21:07:00Z",
                        "time_start": "2022-10-20T21:07:00Z",
                        "time_end": "2022-10-20T21:09:00Z",
                        "paper_type": "full",
                        "keywords": [
                            "Artificial neural networks, Data visualization, Computational modeling, Natural language processing, Predictive models, Task analysis, Deep learning, Binary text classification, decision boundary, deep learning, interpretability, neural networks, representation space, visual analytics"
                        ],
                        "has_image": "1",
                        "has_video": "554",
                        "paper_award": "",
                        "image_caption": "Locality view of EBBE-Text. From left to right and top to bottom: boundary sub-view, path text list, top 10 word list, text representation spaces resulting from dimension reduction, confusion matrix, input text form, classify command. With these features, EBBE-Text is used to explain neural network predictions for a binary text classification task.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "https://youtu.be/j3sHXXlFWZ8",
                        "ff_id": "j3sHXXlFWZ8"
                    },
                    {
                        "slot_id": "v-tvcg-9716779-pres",
                        "session_id": "full4",
                        "type": "In Person Presentation",
                        "title": "LegalVis: Exploring and Inferring Precedent Citations in Legal Documents",
                        "contributors": [
                            "Jean Roberto",
                            "Lucas Resck"
                        ],
                        "authors": [
                            "Lucas E. Resck",
                            "Jean R. Ponciano",
                            "Luis Gustavo Nonato",
                            "Jorge Poco"
                        ],
                        "abstract": "To reduce the number of pending cases and conflicting rulings in the Brazilian Judiciary, the National Congress amended the Constitution, allowing the Brazilian Supreme Court (STF) to create binding precedents (BPs), i.e., a set of understandings that both Executive and lower Judiciary branches must follow. The STF's justices frequently cite the 58 existing BPs in their decisions, and it is of primary relevance that judicial experts could identify and analyze such citations. To assist in this problem, we propose LegalVis, a web-based visual analytics system designed to support the analysis of legal documents that cite or could potentially cite a BP. We model the problem of identifying potential citations (i.e., non-explicit) as a classification problem. However, a simple score is not enough to explain the results; that is why we use an interpretability machine learning method to explain the reason behind each identified citation. For a compelling visual exploration of documents and BPs, LegalVis comprises three interactive visual components: the first presents an overview of the data showing temporal patterns, the second allows filtering and grouping relevant documents by topic, and the last one shows a document's text aiming to interpret the model's output by pointing out which paragraphs are likely to mention the BP, even if not explicitly specified. We evaluated our identification model and obtained an accuracy of 96%; we also made a quantitative and qualitative analysis of the results. The usefulness and effectiveness of LegalVis were evaluated through two usage scenarios and feedback from six domain experts.",
                        "uid": "v-tvcg-9716779",
                        "file_name": "v-tvcg-9716779_Resck_Presentation.mp4",
                        "time_stamp": "2022-10-20T21:09:00Z",
                        "time_start": "2022-10-20T21:09:00Z",
                        "time_end": "2022-10-20T21:19:00Z",
                        "paper_type": "full",
                        "keywords": [
                            "Legal Documents, Visual Analytics, Brazilian Legal System, Natural Language Processing"
                        ],
                        "has_image": "1",
                        "has_video": "523",
                        "paper_award": "",
                        "image_caption": "LegalVis is a web-based visual analytic system designed to assist judicial experts in analyzing Brazilian legal documents that cite or could potentially cite binding precedents. LegalVis first identifies potential citations by implementing a machine learning model based on classification. Then, an interpretability mechanism provides reliable explanations for the model's decisions. All this information becomes accessible through the three provided linked views: (a) Global View, which presents an overview of the data showing temporal patterns; (b) Paragraph Similarities View, which allows filtering and grouping of relevant documents; and (c) Document Reader, which shows a document\u2019s text while highlighting relevant paragraphs.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "https://youtu.be/WQl874Ylajc",
                        "ff_id": "WQl874Ylajc"
                    },
                    {
                        "slot_id": "v-tvcg-9716779-qa",
                        "session_id": "full4",
                        "type": "In Person Q+A",
                        "title": "LegalVis: Exploring and Inferring Precedent Citations in Legal Documents (Q+A)",
                        "contributors": [
                            "Jean Roberto",
                            "Lucas Resck"
                        ],
                        "authors": [],
                        "abstract": "To reduce the number of pending cases and conflicting rulings in the Brazilian Judiciary, the National Congress amended the Constitution, allowing the Brazilian Supreme Court (STF) to create binding precedents (BPs), i.e., a set of understandings that both Executive and lower Judiciary branches must follow. The STF's justices frequently cite the 58 existing BPs in their decisions, and it is of primary relevance that judicial experts could identify and analyze such citations. To assist in this problem, we propose LegalVis, a web-based visual analytics system designed to support the analysis of legal documents that cite or could potentially cite a BP. We model the problem of identifying potential citations (i.e., non-explicit) as a classification problem. However, a simple score is not enough to explain the results; that is why we use an interpretability machine learning method to explain the reason behind each identified citation. For a compelling visual exploration of documents and BPs, LegalVis comprises three interactive visual components: the first presents an overview of the data showing temporal patterns, the second allows filtering and grouping relevant documents by topic, and the last one shows a document's text aiming to interpret the model's output by pointing out which paragraphs are likely to mention the BP, even if not explicitly specified. We evaluated our identification model and obtained an accuracy of 96%; we also made a quantitative and qualitative analysis of the results. The usefulness and effectiveness of LegalVis were evaluated through two usage scenarios and feedback from six domain experts.",
                        "uid": "v-tvcg-9716779",
                        "file_name": "",
                        "time_stamp": "2022-10-20T21:19:00Z",
                        "time_start": "2022-10-20T21:19:00Z",
                        "time_end": "2022-10-20T21:21:00Z",
                        "paper_type": "full",
                        "keywords": [
                            "Legal Documents, Visual Analytics, Brazilian Legal System, Natural Language Processing"
                        ],
                        "has_image": "1",
                        "has_video": "523",
                        "paper_award": "",
                        "image_caption": "LegalVis is a web-based visual analytic system designed to assist judicial experts in analyzing Brazilian legal documents that cite or could potentially cite binding precedents. LegalVis first identifies potential citations by implementing a machine learning model based on classification. Then, an interpretability mechanism provides reliable explanations for the model's decisions. All this information becomes accessible through the three provided linked views: (a) Global View, which presents an overview of the data showing temporal patterns; (b) Paragraph Similarities View, which allows filtering and grouping of relevant documents; and (c) Document Reader, which shows a document\u2019s text while highlighting relevant paragraphs.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "https://youtu.be/WQl874Ylajc",
                        "ff_id": "WQl874Ylajc"
                    },
                    {
                        "slot_id": "v-tvcg-9488285-pres",
                        "session_id": "full4",
                        "type": "In Person Presentation",
                        "title": "DeHumor: Visual Analytics for Decomposing Humor",
                        "contributors": [
                            "Xingbo Wang"
                        ],
                        "authors": [
                            "Xingbo Wang",
                            "Yao Ming",
                            "Tongshuang Wu",
                            "Haipeng Zeng",
                            "Yong Wang",
                            "Huamin Qu"
                        ],
                        "abstract": "Despite being a critical communication skill, grasping humor is challenging\u2014a successful use of humor requires a mixture of both engaging content build-up and an appropriate vocal delivery (e.g., pause). Prior studies on computational humor emphasize the textual and audio features immediately next to the punchline, yet overlooking longer-term context setup. Moreover, the theories are usually too abstract for understanding each concrete humor snippet. To fill in the gap, we develop DeHumor, a visual analytical system for analyzing humorous behaviors in public speaking. To intuitively reveal the building blocks of each concrete example, DeHumor decomposes each humorous video into multimodal features and provides inline annotations of them on the video script. In particular, to better capture the build-ups, we introduce content repetition as a complement to features introduced in theories of computational humor and visualize them in a context linking graph. To help users locate the punchlines that have the desired features to learn, we summarize the content (with keywords) and humor feature statistics on an augmented time matrix. With case studies on stand-up comedy shows and TED talks, we show that DeHumor is able to highlight various building blocks of humor examples. In addition, expert interviews with communication coaches and humor researchers demonstrate the effectiveness of DeHumor for multimodal humor analysis of speech content and vocal delivery.",
                        "uid": "v-tvcg-9488285",
                        "file_name": "v-tvcg-9488285_Wang_Presentation.mp4",
                        "time_stamp": "2022-10-20T21:21:00Z",
                        "time_start": "2022-10-20T21:21:00Z",
                        "time_end": "2022-10-20T21:31:00Z",
                        "paper_type": "full",
                        "keywords": [
                            "Humor, Context, Multimodal Features, Visualization"
                        ],
                        "has_image": "1",
                        "has_video": "649",
                        "paper_award": "",
                        "image_caption": "Humor\u2014a critical communication skill\u2014requires a mixture of both engaging content build-ups and an appropriate vocal delivery. In this paper, we present DeHumor, a visual analytics system to decompose humor into a set of quantifiable verbal and vocal features. DeHumor helps domain experts systematically explore humorous behaviors from speech level, context level, and sentence level.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "https://youtu.be/9uvUqGfxo3w",
                        "ff_id": "9uvUqGfxo3w"
                    },
                    {
                        "slot_id": "v-tvcg-9488285-qa",
                        "session_id": "full4",
                        "type": "In Person Q+A",
                        "title": "DeHumor: Visual Analytics for Decomposing Humor (Q+A)",
                        "contributors": [
                            "Xingbo Wang"
                        ],
                        "authors": [],
                        "abstract": "Despite being a critical communication skill, grasping humor is challenging\u2014a successful use of humor requires a mixture of both engaging content build-up and an appropriate vocal delivery (e.g., pause). Prior studies on computational humor emphasize the textual and audio features immediately next to the punchline, yet overlooking longer-term context setup. Moreover, the theories are usually too abstract for understanding each concrete humor snippet. To fill in the gap, we develop DeHumor, a visual analytical system for analyzing humorous behaviors in public speaking. To intuitively reveal the building blocks of each concrete example, DeHumor decomposes each humorous video into multimodal features and provides inline annotations of them on the video script. In particular, to better capture the build-ups, we introduce content repetition as a complement to features introduced in theories of computational humor and visualize them in a context linking graph. To help users locate the punchlines that have the desired features to learn, we summarize the content (with keywords) and humor feature statistics on an augmented time matrix. With case studies on stand-up comedy shows and TED talks, we show that DeHumor is able to highlight various building blocks of humor examples. In addition, expert interviews with communication coaches and humor researchers demonstrate the effectiveness of DeHumor for multimodal humor analysis of speech content and vocal delivery.",
                        "uid": "v-tvcg-9488285",
                        "file_name": "",
                        "time_stamp": "2022-10-20T21:31:00Z",
                        "time_start": "2022-10-20T21:31:00Z",
                        "time_end": "2022-10-20T21:33:00Z",
                        "paper_type": "full",
                        "keywords": [
                            "Humor, Context, Multimodal Features, Visualization"
                        ],
                        "has_image": "1",
                        "has_video": "649",
                        "paper_award": "",
                        "image_caption": "Humor\u2014a critical communication skill\u2014requires a mixture of both engaging content build-ups and an appropriate vocal delivery. In this paper, we present DeHumor, a visual analytics system to decompose humor into a set of quantifiable verbal and vocal features. DeHumor helps domain experts systematically explore humorous behaviors from speech level, context level, and sentence level.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "https://youtu.be/9uvUqGfxo3w",
                        "ff_id": "9uvUqGfxo3w"
                    },
                    {
                        "slot_id": "v-full-1651-pres",
                        "session_id": "full4",
                        "type": "In Person Presentation",
                        "title": "Interactive and visual prompt engineering for ad-hoc task adaptation with large language models",
                        "contributors": [
                            "Hendrik Strobelt"
                        ],
                        "authors": [
                            "Hendrik Strobelt",
                            "Albert Webson",
                            "Victor Sanh",
                            "Benjamin Hoover",
                            "Johanna Beyer",
                            "Hanspeter Pfister",
                            "Alexander Rush"
                        ],
                        "abstract": "State-of-the-art neural language models can now be used to solve ad-hoc language tasks through zero-shot prompting without the need for supervised training. This approach has gained popularity in recent years, and researchers have demonstrated prompts that achieve strong accuracy on specific NLP tasks. However, finding a prompt for new tasks requires experimentation. Different prompt templates with different wording choices lead to significant accuracy differences. PromptIDE allows users to experiment with prompt variations, visualize prompt performance, and iteratively optimize prompts. We developed a workflow that allows users to first focus on model feedback using small data before moving on to a large data regime that allows empirical grounding of promising prompts using quantitative measures of the task. The tool then allows easy deployment of the newly created ad-hoc models. We demonstrate the utility of PromptIDE (demo: http://prompt.vizhub.ai) and our workflow using several real-world use cases.",
                        "uid": "v-full-1651",
                        "file_name": "v-full-1651_Strobelt_Presentation.mp4",
                        "time_stamp": "2022-10-20T21:33:00Z",
                        "time_start": "2022-10-20T21:33:00Z",
                        "time_end": "2022-10-20T21:43:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "385",
                        "paper_award": "",
                        "image_caption": "Example of PromptIDE's interface to explore variations of different prompts. Each variation is tested against up to twenty data examples and represented as a template card (a). For each variation, rich detail can be tracked by using the detail stripes (b). If performance and qualitative detail are convincing, a user can collect the prompt in the prompt cart (c).",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "https://youtu.be/qegI-vrys88",
                        "ff_id": "qegI-vrys88"
                    },
                    {
                        "slot_id": "v-full-1651-qa",
                        "session_id": "full4",
                        "type": "In Person Q+A",
                        "title": "Interactive and visual prompt engineering for ad-hoc task adaptation with large language models (Q+A)",
                        "contributors": [
                            "Hendrik Strobelt"
                        ],
                        "authors": [],
                        "abstract": "State-of-the-art neural language models can now be used to solve ad-hoc language tasks through zero-shot prompting without the need for supervised training. This approach has gained popularity in recent years, and researchers have demonstrated prompts that achieve strong accuracy on specific NLP tasks. However, finding a prompt for new tasks requires experimentation. Different prompt templates with different wording choices lead to significant accuracy differences. PromptIDE allows users to experiment with prompt variations, visualize prompt performance, and iteratively optimize prompts. We developed a workflow that allows users to first focus on model feedback using small data before moving on to a large data regime that allows empirical grounding of promising prompts using quantitative measures of the task. The tool then allows easy deployment of the newly created ad-hoc models. We demonstrate the utility of PromptIDE (demo: http://prompt.vizhub.ai) and our workflow using several real-world use cases.",
                        "uid": "v-full-1651",
                        "file_name": "",
                        "time_stamp": "2022-10-20T21:43:00Z",
                        "time_start": "2022-10-20T21:43:00Z",
                        "time_end": "2022-10-20T21:45:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "385",
                        "paper_award": "",
                        "image_caption": "Example of PromptIDE's interface to explore variations of different prompts. Each variation is tested against up to twenty data examples and represented as a template card (a). For each variation, rich detail can be tracked by using the detail stripes (b). If performance and qualitative detail are convincing, a user can collect the prompt in the prompt cart (c).",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "https://youtu.be/qegI-vrys88",
                        "ff_id": "qegI-vrys88"
                    },
                    {
                        "slot_id": "v-tvcg-9765476-pres",
                        "session_id": "full4",
                        "type": "In Person Presentation",
                        "title": "A Hybrid In Situ Approach for Cost Efficient Image Database Generation",
                        "contributors": [
                            "Steffen Frey"
                        ],
                        "authors": [
                            "Valentin Bruder",
                            "Matthew Larsen",
                            "Thomas Ertl",
                            "Hank Childs",
                            "Steffen Frey"
                        ],
                        "abstract": "The visualization of results while the simulation is running is increasingly common in extreme scale computing environments. We present a novel approach for in situ generation of image databases to achieve cost savings on supercomputers. Our approach, a hybrid between traditional inline and in transit techniques, dynamically distributes visualization tasks between simulation nodes and visualization nodes, using probing as a basis to estimate rendering cost. Our hybrid design differs from previous works in that it creates opportunities to minimize idle time from four fundamental types of inefficiency: variability, limited scalability, overhead, and rightsizing. We demonstrate our results by comparing our method against both inline and in transit methods for a variety of configurations, including two simulation codes and a scaling study that goes above 19K cores. Our findings show that our approach is superior in many configurations. As in situ visualization becomes increasingly ubiquitous, we believe our technique could lead to significant amounts of reclaimed cycles on supercomputers.",
                        "uid": "v-tvcg-9765476",
                        "file_name": "v-tvcg-9765476_Bruder_Presentation.mp4",
                        "time_stamp": "2022-10-20T21:45:00Z",
                        "time_start": "2022-10-20T21:45:00Z",
                        "time_end": "2022-10-20T21:55:00Z",
                        "paper_type": "full",
                        "keywords": [
                            "Visualization, High performance computing, In situ"
                        ],
                        "has_image": "1",
                        "has_video": "527",
                        "paper_award": "",
                        "image_caption": "Our hybrid in situ approach dynamically distributes visualization tasks based on predicted render times. It combines characteristics from traditional in situ techniques: it employs dedicated visualization nodes such as in transit in situ, and simulation nodes can also take over rendering tasks like in inline in situ. Dynamic task distribution allows to address various inefficiencies (variability, scalability, rightsizing and overhead) and save precious supercomputer resources.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "https://youtu.be/js6rnMsDyqs",
                        "ff_id": "js6rnMsDyqs"
                    },
                    {
                        "slot_id": "v-tvcg-9765476-qa",
                        "session_id": "full4",
                        "type": "In Person Q+A",
                        "title": "A Hybrid In Situ Approach for Cost Efficient Image Database Generation (Q+A)",
                        "contributors": [
                            "Steffen Frey"
                        ],
                        "authors": [],
                        "abstract": "The visualization of results while the simulation is running is increasingly common in extreme scale computing environments. We present a novel approach for in situ generation of image databases to achieve cost savings on supercomputers. Our approach, a hybrid between traditional inline and in transit techniques, dynamically distributes visualization tasks between simulation nodes and visualization nodes, using probing as a basis to estimate rendering cost. Our hybrid design differs from previous works in that it creates opportunities to minimize idle time from four fundamental types of inefficiency: variability, limited scalability, overhead, and rightsizing. We demonstrate our results by comparing our method against both inline and in transit methods for a variety of configurations, including two simulation codes and a scaling study that goes above 19K cores. Our findings show that our approach is superior in many configurations. As in situ visualization becomes increasingly ubiquitous, we believe our technique could lead to significant amounts of reclaimed cycles on supercomputers.",
                        "uid": "v-tvcg-9765476",
                        "file_name": "",
                        "time_stamp": "2022-10-20T21:55:00Z",
                        "time_start": "2022-10-20T21:55:00Z",
                        "time_end": "2022-10-20T21:57:00Z",
                        "paper_type": "full",
                        "keywords": [
                            "Visualization, High performance computing, In situ"
                        ],
                        "has_image": "1",
                        "has_video": "527",
                        "paper_award": "",
                        "image_caption": "Our hybrid in situ approach dynamically distributes visualization tasks based on predicted render times. It combines characteristics from traditional in situ techniques: it employs dedicated visualization nodes such as in transit in situ, and simulation nodes can also take over rendering tasks like in inline in situ. Dynamic task distribution allows to address various inefficiencies (variability, scalability, rightsizing and overhead) and save precious supercomputer resources.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "https://youtu.be/js6rnMsDyqs",
                        "ff_id": "js6rnMsDyqs"
                    }
                ]
            },
            {
                "title": "Transforming Tabular Data and Grammars",
                "session_id": "full5",
                "event_prefix": "v-full",
                "track": "ok5",
                "livestream_id": "ok5-wed",
                "session_image": "full5.png",
                "chair": [
                    "Alexander Lex"
                ],
                "organizers": [],
                "time_start": "2022-10-19T14:00:00Z",
                "time_end": "2022-10-19T15:15:00Z",
                "discord_category": "",
                "discord_channel": "oklahoma-st-5",
                "discord_channel_id": "1024600839295356939",
                "discord_link": "https://discord.com/channels/978320435554947082/1024600839295356939",
                "slido_link": "https://app.sli.do/event/i28UETZaiRWr1XwX6eKjcf?section=d355f89b-fbd2-4abf-9958-741607905551",
                "youtube_url": "https://youtu.be/wr1l85Jo7jM",
                "youtube_id": "wr1l85Jo7jM",
                "zoom_meeting": "",
                "zoom_password": "",
                "zoom_link": "",
                "ff_link": "https://youtu.be/Vo2ggx2i-gE",
                "ff_playlist": "",
                "time_slots": [
                    {
                        "slot_id": "full5-opening",
                        "session_id": "full5",
                        "type": "In Person Other",
                        "title": "Session Opening",
                        "contributors": [
                            "Alexander Lex"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "",
                        "file_name": "",
                        "time_stamp": "2022-10-19T14:00:00Z",
                        "time_start": "2022-10-19T14:00:00Z",
                        "time_end": "2022-10-19T14:00:00Z",
                        "paper_type": "",
                        "keywords": [],
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "v-full-1346-pres",
                        "session_id": "full5",
                        "type": "Virtual Presentation (pre-recorded)",
                        "title": "Revealing the Semantics of Data Wrangling Scripts With COMANTICS",
                        "contributors": [
                            "Kai Xiong"
                        ],
                        "authors": [
                            "Kai Xiong",
                            "Zhongsu Luo",
                            "Siwei Fu",
                            "Yongheng Wang",
                            "Mingliang Xu",
                            "Yingcai Wu"
                        ],
                        "abstract": "Data workers usually seek to understand the semantics of data wrangling scripts in various scenarios, such as code debugging, reusing, and maintaining. However, the understanding is challenging for novice data workers due to the variety of programming languages, functions, and parameters. Based on the observation that differences between input and output tables highly relate to the type of data transformation, we outline a design space including 103 characteristics to describe table differences. Then, we develop COMANTICS, a three-step pipeline that automatically detects the semantics of data transformation scripts. The first step focuses on the detection of table differences for each line of wrangling code. Second, we incorporate a characteristic-based component and a Siamese convolutional neural network-based component for the detection of transformation types. Third, we derive the parameters of each data transformation by employing a \"slot filling\" strategy. We design experiments to evaluate the performance of COMANTICS. Further, we assess its flexibility using three example applications in different domains.",
                        "uid": "v-full-1346",
                        "file_name": "v-full-1346_Xiong_Presentation.mp4",
                        "time_stamp": "2022-10-19T14:00:00Z",
                        "time_start": "2022-10-19T14:00:00Z",
                        "time_end": "2022-10-19T14:10:01Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "601",
                        "paper_award": "",
                        "image_caption": "COMANTICS is a three-step pipeline that automatically detects the semantics of data wrangling scripts by inferring the types of data transformations with their parameters. COMANTICS first generates intermediate input and output tables for each line of code and detects changes between them. Then, it identifies the transformation type through characteristic-based and CNN-based components. Last, it infers parameters for the transformation by employing a \u201cslot filling\u201d strategy.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "https://youtu.be/rieZfgUGEJw",
                        "ff_id": "rieZfgUGEJw"
                    },
                    {
                        "slot_id": "v-full-1346-qa",
                        "session_id": "full5",
                        "type": "Virtual Q+A",
                        "title": "Revealing the Semantics of Data Wrangling Scripts With COMANTICS (Q+A)",
                        "contributors": [
                            "Kai Xiong"
                        ],
                        "authors": [],
                        "abstract": "Data workers usually seek to understand the semantics of data wrangling scripts in various scenarios, such as code debugging, reusing, and maintaining. However, the understanding is challenging for novice data workers due to the variety of programming languages, functions, and parameters. Based on the observation that differences between input and output tables highly relate to the type of data transformation, we outline a design space including 103 characteristics to describe table differences. Then, we develop COMANTICS, a three-step pipeline that automatically detects the semantics of data transformation scripts. The first step focuses on the detection of table differences for each line of wrangling code. Second, we incorporate a characteristic-based component and a Siamese convolutional neural network-based component for the detection of transformation types. Third, we derive the parameters of each data transformation by employing a \"slot filling\" strategy. We design experiments to evaluate the performance of COMANTICS. Further, we assess its flexibility using three example applications in different domains.",
                        "uid": "v-full-1346",
                        "file_name": "",
                        "time_stamp": "2022-10-19T14:10:01Z",
                        "time_start": "2022-10-19T14:10:01Z",
                        "time_end": "2022-10-19T14:12:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "601",
                        "paper_award": "",
                        "image_caption": "COMANTICS is a three-step pipeline that automatically detects the semantics of data wrangling scripts by inferring the types of data transformations with their parameters. COMANTICS first generates intermediate input and output tables for each line of code and detects changes between them. Then, it identifies the transformation type through characteristic-based and CNN-based components. Last, it infers parameters for the transformation by employing a \u201cslot filling\u201d strategy.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "https://youtu.be/rieZfgUGEJw",
                        "ff_id": "rieZfgUGEJw"
                    },
                    {
                        "slot_id": "v-tvcg-9693232-pres",
                        "session_id": "full5",
                        "type": "Virtual Presentation (pre-recorded)",
                        "title": "Visualizing the Scripts of Data Wrangling with SOMNUS",
                        "contributors": [
                            "Kai Xiong,"
                        ],
                        "authors": [
                            "Kai Xiong",
                            "Siwei Fu",
                            "Guoming Ding",
                            "Zhongsu Luo",
                            "Rong Yu",
                            "Wei Chen",
                            "Hujun Bao",
                            "Yingcai Wu"
                        ],
                        "abstract": "Data workers use various scripting languages for data transformation, such as SAS, R, and Python. However, understanding intricate code pieces requires advanced programming skills, which hinders data workers from grasping the idea of data transformation at ease. Program visualization is beneficial for debugging and education and has the potential to illustrate transformations intuitively and interactively. In this paper, we explore visualization design for demonstrating the semantics of code pieces in the context of data transformation. First, to depict individual data transformations, we structure a design space by two primary dimensions, i.e., key parameters to encode and possible visual channels to be mapped. Then, we derive a collection of 23 glyphs that visualize the semantics of transformations. Next, we design a pipeline, named Somnus, that provides an overview of the creation and evolution of data tables using a provenance graph. At the same time, it allows detailed investigation of individual transformations. User feedback on Somnus is positive. Our study participants achieved better accuracy with less time using Somnus, and preferred it over carefully-crafted textual description. Further, we provide two example applications to demonstrate the utility and versatility of Somnus.",
                        "uid": "v-tvcg-9693232",
                        "file_name": "v-tvcg-9693232_Xiong_Presentation.mp4",
                        "time_stamp": "2022-10-19T14:12:00Z",
                        "time_start": "2022-10-19T14:12:00Z",
                        "time_end": "2022-10-19T14:22:00Z",
                        "paper_type": "full",
                        "keywords": [
                            "Program understanding, data transformation, visualization design"
                        ],
                        "has_image": "1",
                        "has_video": "600",
                        "paper_award": "",
                        "image_caption": "SOMNUS is a pipeline to visualize the semantics of code pieces in the context of data transformation. SOMNUS accepts a data wrangling script with its source tables as input and results in a glyph-based provenance graph where nodes represent tables and edges denote data transformations. We implement SOMNUS as a web-based system, which can help data scientists validate the procedure of data transformation. We also apply SOMNUS to explain scripts generated by MORPHEUS to reveal intermediate data transformations given source and target tables.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "https://youtu.be/6wd1KjLObqM",
                        "ff_id": "6wd1KjLObqM"
                    },
                    {
                        "slot_id": "v-tvcg-9693232-qa",
                        "session_id": "full5",
                        "type": "Virtual Q+A",
                        "title": "Visualizing the Scripts of Data Wrangling with SOMNUS (Q+A)",
                        "contributors": [
                            "Kai Xiong,"
                        ],
                        "authors": [],
                        "abstract": "Data workers use various scripting languages for data transformation, such as SAS, R, and Python. However, understanding intricate code pieces requires advanced programming skills, which hinders data workers from grasping the idea of data transformation at ease. Program visualization is beneficial for debugging and education and has the potential to illustrate transformations intuitively and interactively. In this paper, we explore visualization design for demonstrating the semantics of code pieces in the context of data transformation. First, to depict individual data transformations, we structure a design space by two primary dimensions, i.e., key parameters to encode and possible visual channels to be mapped. Then, we derive a collection of 23 glyphs that visualize the semantics of transformations. Next, we design a pipeline, named Somnus, that provides an overview of the creation and evolution of data tables using a provenance graph. At the same time, it allows detailed investigation of individual transformations. User feedback on Somnus is positive. Our study participants achieved better accuracy with less time using Somnus, and preferred it over carefully-crafted textual description. Further, we provide two example applications to demonstrate the utility and versatility of Somnus.",
                        "uid": "v-tvcg-9693232",
                        "file_name": "",
                        "time_stamp": "2022-10-19T14:22:00Z",
                        "time_start": "2022-10-19T14:22:00Z",
                        "time_end": "2022-10-19T14:24:00Z",
                        "paper_type": "full",
                        "keywords": [
                            "Program understanding, data transformation, visualization design"
                        ],
                        "has_image": "1",
                        "has_video": "600",
                        "paper_award": "",
                        "image_caption": "SOMNUS is a pipeline to visualize the semantics of code pieces in the context of data transformation. SOMNUS accepts a data wrangling script with its source tables as input and results in a glyph-based provenance graph where nodes represent tables and edges denote data transformations. We implement SOMNUS as a web-based system, which can help data scientists validate the procedure of data transformation. We also apply SOMNUS to explain scripts generated by MORPHEUS to reveal intermediate data transformations given source and target tables.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "https://youtu.be/6wd1KjLObqM",
                        "ff_id": "6wd1KjLObqM"
                    },
                    {
                        "slot_id": "v-full-1171-pres",
                        "session_id": "full5",
                        "type": "In Person Presentation",
                        "title": "Rigel: Transforming Tabular Data By Declarative Mapping",
                        "contributors": [
                            "Ran Chen",
                            "Xinhuan Shu"
                        ],
                        "authors": [
                            "Ran Chen",
                            "Di Weng",
                            "Yanwei Huang",
                            "Xinhuan Shu",
                            "Jiayi Zhou",
                            "Guodao Sun",
                            "Yingcai Wu"
                        ],
                        "abstract": "We present Rigel, an interactive system for rapid transformation of tabular data. Rigel implements a new declarative mapping approach that formulates the data transformation procedure as direct mappings from data to the row, column, and cell channels of the target table. To construct such mappings, Rigel allows users to directly drag data attributes from input data to these three channels and indirectly drag or type data values in a spreadsheet, and possible mappings that do not contradict these interactions are recommended to achieve efficient and straightforward data transformation. The recommended mappings are generated by enumerating and composing data variables based on the row, column, and cell channels, thereby revealing the possibility of alternative tabular forms and facilitating open-ended exploration in many data transformation scenarios, such as designing tables for presentation. In contrast to existing systems that transform data by composing operations (like transposing and pivoting), Rigel requires less prior knowledge on these operations, and constructing tables from the channels is more efficient and results in less ambiguity than generating operation sequences as done by the traditional by-example approaches. User study results demonstrated that Rigel is significantly less demanding in terms of time and interactions and suits more scenarios compared to the state-of-the-art by-example approach. A gallery of diverse transformation cases is also presented to show the potential of Rigel's expressiveness.",
                        "uid": "v-full-1171",
                        "file_name": "v-full-1171_Chen_Presentation.mp4",
                        "time_stamp": "2022-10-19T14:24:00Z",
                        "time_start": "2022-10-19T14:24:00Z",
                        "time_end": "2022-10-19T14:34:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "593",
                        "paper_award": "",
                        "image_caption": "Rigel\u2019s user interface: Rigel allows users to construct declarative mappings by dragging or typing values from input data into a spreadsheet. Rigel recommends possible mappings from data to row, column, and cell channels to declaratively compose user-desired tables.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "https://youtu.be/8h2nv0JdyS0",
                        "ff_id": "8h2nv0JdyS0"
                    },
                    {
                        "slot_id": "v-full-1171-qa",
                        "session_id": "full5",
                        "type": "In Person Q+A",
                        "title": "Rigel: Transforming Tabular Data By Declarative Mapping (Q+A)",
                        "contributors": [
                            "Ran Chen",
                            "Xinhuan Shu"
                        ],
                        "authors": [],
                        "abstract": "We present Rigel, an interactive system for rapid transformation of tabular data. Rigel implements a new declarative mapping approach that formulates the data transformation procedure as direct mappings from data to the row, column, and cell channels of the target table. To construct such mappings, Rigel allows users to directly drag data attributes from input data to these three channels and indirectly drag or type data values in a spreadsheet, and possible mappings that do not contradict these interactions are recommended to achieve efficient and straightforward data transformation. The recommended mappings are generated by enumerating and composing data variables based on the row, column, and cell channels, thereby revealing the possibility of alternative tabular forms and facilitating open-ended exploration in many data transformation scenarios, such as designing tables for presentation. In contrast to existing systems that transform data by composing operations (like transposing and pivoting), Rigel requires less prior knowledge on these operations, and constructing tables from the channels is more efficient and results in less ambiguity than generating operation sequences as done by the traditional by-example approaches. User study results demonstrated that Rigel is significantly less demanding in terms of time and interactions and suits more scenarios compared to the state-of-the-art by-example approach. A gallery of diverse transformation cases is also presented to show the potential of Rigel's expressiveness.",
                        "uid": "v-full-1171",
                        "file_name": "",
                        "time_stamp": "2022-10-19T14:34:00Z",
                        "time_start": "2022-10-19T14:34:00Z",
                        "time_end": "2022-10-19T14:36:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "593",
                        "paper_award": "",
                        "image_caption": "Rigel\u2019s user interface: Rigel allows users to construct declarative mappings by dragging or typing values from input data into a spreadsheet. Rigel recommends possible mappings from data to row, column, and cell channels to declaratively compose user-desired tables.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "https://youtu.be/8h2nv0JdyS0",
                        "ff_id": "8h2nv0JdyS0"
                    },
                    {
                        "slot_id": "v-full-1413-pres",
                        "session_id": "full5",
                        "type": "Virtual Presentation (pre-recorded)",
                        "title": "HiTailor: Interactive Transformation and Visualization for Hierarchical Tabular Data",
                        "contributors": [
                            "Guozheng Li",
                            "Runfei Li"
                        ],
                        "authors": [
                            "Guozheng Li",
                            "Runfei Li",
                            "Zicheng Wang",
                            "Chi Harold Liu",
                            "Min Lu",
                            "Guoren Wang"
                        ],
                        "abstract": "Tabular visualization techniques integrate visual representations with tabular data to avoid additional cognitive load caused by splitting users' attention. However, most of the existing studies focus on simple flat tables instead of hierarchical tables, whose complex structure limits the expressiveness of visualization results and affects users' efficiency in visualization construction. We present HiTailor, a technique for presenting and exploring hierarchical tables. HiTailor constructs an abstract model, which defines row/column headings as biclustering and hierarchical structures. Based on our abstract model, we identify three pairs of operators, Swap/Transpose, ToStacked/ToLinear, Fold/Unfold, for transformations of hierarchical tables to support users' comprehensive explorations. After transformation, users can specify a cell or block of interest in hierarchical tables as a TableUnit for visualization, and HiTailor recommends other related TableUnits according to the abstract model using different mechanisms. We demonstrate the usability of the HiTailor system through a comparative study and a case study with domain experts, showing that HiTailor can present and explore hierarchical tables from different viewpoints. HiTailor is available at https://github.com/bitvis2021/HiTailor.",
                        "uid": "v-full-1413",
                        "file_name": "v-full-1413_Li_Presentation.mp4",
                        "time_stamp": "2022-10-19T14:36:00Z",
                        "time_start": "2022-10-19T14:36:00Z",
                        "time_end": "2022-10-19T14:45:10Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "550",
                        "paper_award": "",
                        "image_caption": "The user interface of the HiTailor prototype system. (a) Transformation operator panel; (b) Tabular visualization panel; (c) Visualization template panel; (d) Visualization configuration panel.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "https://youtu.be/nrSju2-lqCc",
                        "ff_id": "nrSju2-lqCc"
                    },
                    {
                        "slot_id": "v-full-1413-qa",
                        "session_id": "full5",
                        "type": "Virtual Q+A",
                        "title": "HiTailor: Interactive Transformation and Visualization for Hierarchical Tabular Data (Q+A)",
                        "contributors": [
                            "Guozheng Li",
                            "Runfei Li"
                        ],
                        "authors": [],
                        "abstract": "Tabular visualization techniques integrate visual representations with tabular data to avoid additional cognitive load caused by splitting users' attention. However, most of the existing studies focus on simple flat tables instead of hierarchical tables, whose complex structure limits the expressiveness of visualization results and affects users' efficiency in visualization construction. We present HiTailor, a technique for presenting and exploring hierarchical tables. HiTailor constructs an abstract model, which defines row/column headings as biclustering and hierarchical structures. Based on our abstract model, we identify three pairs of operators, Swap/Transpose, ToStacked/ToLinear, Fold/Unfold, for transformations of hierarchical tables to support users' comprehensive explorations. After transformation, users can specify a cell or block of interest in hierarchical tables as a TableUnit for visualization, and HiTailor recommends other related TableUnits according to the abstract model using different mechanisms. We demonstrate the usability of the HiTailor system through a comparative study and a case study with domain experts, showing that HiTailor can present and explore hierarchical tables from different viewpoints. HiTailor is available at https://github.com/bitvis2021/HiTailor.",
                        "uid": "v-full-1413",
                        "file_name": "",
                        "time_stamp": "2022-10-19T14:45:10Z",
                        "time_start": "2022-10-19T14:45:10Z",
                        "time_end": "2022-10-19T14:48:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "550",
                        "paper_award": "",
                        "image_caption": "The user interface of the HiTailor prototype system. (a) Transformation operator panel; (b) Tabular visualization panel; (c) Visualization template panel; (d) Visualization configuration panel.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "https://youtu.be/nrSju2-lqCc",
                        "ff_id": "nrSju2-lqCc"
                    },
                    {
                        "slot_id": "v-full-1138-pres",
                        "session_id": "full5",
                        "type": "In Person Presentation",
                        "title": "Animated Vega-Lite: Unifying Animation with a Grammar of Interactive Graphics",
                        "contributors": [
                            "Jonathan Zong"
                        ],
                        "authors": [
                            "Jonathan Zong",
                            "Josh M. Pollock",
                            "Dylan Wootton",
                            "Arvind Satyanarayan"
                        ],
                        "abstract": "We present Animated Vega-Lite, a set of extensions to Vega-Lite that model animated visualizations as time-varying data queries. In contrast to alternate approaches for specifying animated visualizations, which prize a highly expressive design space, Animated Vega-Lite prioritizes unifying animation with the language\u2019s existing abstractions for static and interactive visualizations to enable authors to smoothly move between or combine these modalities. Thus, to compose animation with static visualizations, we represent time as an encoding channel. Time encodings map a data field to animation keyframes, providing a lightweight specification for animations without interaction. To compose animation and interaction, we also represent time as an event stream; Vega-Lite selections, which provide dynamic data queries, are now driven not only by input events but by timer ticks as well. We evaluate the expressiveness of our approach through a gallery of diverse examples that demonstrate coverage over taxonomies of both interaction and animation. We also critically reflect on the conceptual affordances and limitations of our contribution by interviewing five expert developers of existing animation grammars. These reflections highlight the key motivating role of in-the-wild examples, and identify three central tradeoffs: the language design process, the types of animated transitions supported, and how the systems model keyframes.",
                        "uid": "v-full-1138",
                        "file_name": "v-full-1138_Zong_Presentation.mp4",
                        "time_stamp": "2022-10-19T14:48:00Z",
                        "time_start": "2022-10-19T14:48:00Z",
                        "time_end": "2022-10-19T14:58:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "734",
                        "paper_award": "",
                        "image_caption": "An analyst\u2019s workflow with Animated Vega-Lite. A) Static visualization of bird migrations. B) Adding interaction to hover over a migration path and view a tooltip. C) Switching from static lines to animated circle marks. D) Adding animated path trails for the previous 5 days. E) Adding an interactive slider to scrub through the animation.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "https://youtu.be/Qe3Foy2h3ag",
                        "ff_id": "Qe3Foy2h3ag"
                    },
                    {
                        "slot_id": "v-full-1138-qa",
                        "session_id": "full5",
                        "type": "In Person Q+A",
                        "title": "Animated Vega-Lite: Unifying Animation with a Grammar of Interactive Graphics (Q+A)",
                        "contributors": [
                            "Jonathan Zong"
                        ],
                        "authors": [],
                        "abstract": "We present Animated Vega-Lite, a set of extensions to Vega-Lite that model animated visualizations as time-varying data queries. In contrast to alternate approaches for specifying animated visualizations, which prize a highly expressive design space, Animated Vega-Lite prioritizes unifying animation with the language\u2019s existing abstractions for static and interactive visualizations to enable authors to smoothly move between or combine these modalities. Thus, to compose animation with static visualizations, we represent time as an encoding channel. Time encodings map a data field to animation keyframes, providing a lightweight specification for animations without interaction. To compose animation and interaction, we also represent time as an event stream; Vega-Lite selections, which provide dynamic data queries, are now driven not only by input events but by timer ticks as well. We evaluate the expressiveness of our approach through a gallery of diverse examples that demonstrate coverage over taxonomies of both interaction and animation. We also critically reflect on the conceptual affordances and limitations of our contribution by interviewing five expert developers of existing animation grammars. These reflections highlight the key motivating role of in-the-wild examples, and identify three central tradeoffs: the language design process, the types of animated transitions supported, and how the systems model keyframes.",
                        "uid": "v-full-1138",
                        "file_name": "",
                        "time_stamp": "2022-10-19T14:58:00Z",
                        "time_start": "2022-10-19T14:58:00Z",
                        "time_end": "2022-10-19T15:00:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "734",
                        "paper_award": "",
                        "image_caption": "An analyst\u2019s workflow with Animated Vega-Lite. A) Static visualization of bird migrations. B) Adding interaction to hover over a migration path and view a tooltip. C) Switching from static lines to animated circle marks. D) Adding animated path trails for the previous 5 days. E) Adding an interactive slider to scrub through the animation.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "https://youtu.be/Qe3Foy2h3ag",
                        "ff_id": "Qe3Foy2h3ag"
                    },
                    {
                        "slot_id": "v-full-1045-pres",
                        "session_id": "full5",
                        "type": "In Person Presentation",
                        "title": "No Grammar to Rule Them All: A Survey of JSON-style DSLs for Visualization",
                        "contributors": [
                            "Andrew M McNutt"
                        ],
                        "authors": [
                            "Andrew M McNutt"
                        ],
                        "abstract": "There has been substantial growth in the use of JSON-based grammars, as well as other standard data serialization languages, to create visualizations. Each of these grammars serves a purpose: some focus on particular computational tasks (such as animation), some are concerned with certain chart types (such as maps), and some target specific data domains (such as ML). Despite the prominence of this interface form, there has been little detailed analysis of the characteristics of these languages. In this study, we survey and analyze the design and implementation of 57 JSON-style DSLs for visualization. We analyze these languages supported by a collected corpus of examples for each DSL (consisting of 4395 instances) across a variety of axes organized into concerns related to domain, conceptual model, language relationships, affordances, and general practicalities. We identify tensions throughout these areas, such as between formal and colloquial specifications, among types of users, and within the composition of languages. Through this work, we seek to support language implementers by elucidating the choices, opportunities, and tradeoffs in visualization DSL design.",
                        "uid": "v-full-1045",
                        "file_name": "v-full-1045_Mcnutt_Presentation.mp4",
                        "time_stamp": "2022-10-19T15:00:00Z",
                        "time_start": "2022-10-19T15:00:00Z",
                        "time_end": "2022-10-19T15:10:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "574",
                        "paper_award": "",
                        "image_caption": "How are JSON-style domain specific languages for visualization designed and implemented? In this paper we answer that question by analyzing 57 different languages. We identify five key concerns relating to the targeted domain, the underlying model, the composition of languages, affordances relating to the targeted end user, and other implementation practicalities.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "https://youtu.be/1GTqeZ4nKpk",
                        "ff_id": "1GTqeZ4nKpk"
                    },
                    {
                        "slot_id": "v-full-1045-qa",
                        "session_id": "full5",
                        "type": "In Person Q+A",
                        "title": "No Grammar to Rule Them All: A Survey of JSON-style DSLs for Visualization (Q+A)",
                        "contributors": [
                            "Andrew M McNutt"
                        ],
                        "authors": [],
                        "abstract": "There has been substantial growth in the use of JSON-based grammars, as well as other standard data serialization languages, to create visualizations. Each of these grammars serves a purpose: some focus on particular computational tasks (such as animation), some are concerned with certain chart types (such as maps), and some target specific data domains (such as ML). Despite the prominence of this interface form, there has been little detailed analysis of the characteristics of these languages. In this study, we survey and analyze the design and implementation of 57 JSON-style DSLs for visualization. We analyze these languages supported by a collected corpus of examples for each DSL (consisting of 4395 instances) across a variety of axes organized into concerns related to domain, conceptual model, language relationships, affordances, and general practicalities. We identify tensions throughout these areas, such as between formal and colloquial specifications, among types of users, and within the composition of languages. Through this work, we seek to support language implementers by elucidating the choices, opportunities, and tradeoffs in visualization DSL design.",
                        "uid": "v-full-1045",
                        "file_name": "",
                        "time_stamp": "2022-10-19T15:10:00Z",
                        "time_start": "2022-10-19T15:10:00Z",
                        "time_end": "2022-10-19T15:12:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "574",
                        "paper_award": "",
                        "image_caption": "How are JSON-style domain specific languages for visualization designed and implemented? In this paper we answer that question by analyzing 57 different languages. We identify five key concerns relating to the targeted domain, the underlying model, the composition of languages, affordances relating to the targeted end user, and other implementation practicalities.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "https://youtu.be/1GTqeZ4nKpk",
                        "ff_id": "1GTqeZ4nKpk"
                    }
                ]
            },
            {
                "title": "Spatial Data",
                "session_id": "full6",
                "event_prefix": "v-full",
                "track": "ok6",
                "livestream_id": "ok6-fri",
                "session_image": "full6.png",
                "chair": [
                    "Holger Theisel"
                ],
                "organizers": [],
                "time_start": "2022-10-21T14:00:00Z",
                "time_end": "2022-10-21T15:15:00Z",
                "discord_category": "",
                "discord_channel": "oklahoma-st-6",
                "discord_channel_id": "1024600906689421372",
                "discord_link": "https://discord.com/channels/978320435554947082/1024600906689421372",
                "slido_link": "https://app.sli.do/event/i28UETZaiRWr1XwX6eKjcf?section=29ba1f79-04cd-4568-a83c-ab81aa3c28b8",
                "youtube_url": "https://youtu.be/59F5KS-aNtQ",
                "youtube_id": "59F5KS-aNtQ",
                "zoom_meeting": "",
                "zoom_password": "",
                "zoom_link": "",
                "ff_link": "https://youtu.be/ypQo_YZCw-g",
                "ff_playlist": "",
                "time_slots": [
                    {
                        "slot_id": "full6-opening",
                        "session_id": "full6",
                        "type": "In Person Other",
                        "title": "Session Opening",
                        "contributors": [
                            "Holger Theisel"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "",
                        "file_name": "",
                        "time_stamp": "2022-10-21T14:00:00Z",
                        "time_start": "2022-10-21T14:00:00Z",
                        "time_end": "2022-10-21T14:00:00Z",
                        "paper_type": "",
                        "keywords": [],
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "v-tvcg-9576578-pres",
                        "session_id": "full6",
                        "type": "In Person Presentation",
                        "title": "Real-Time Visualization of Large-Scale Geological Models with Nonlinear Feature-Preserving  Levels of Detail",
                        "contributors": [
                            "Ronell Sicat"
                        ],
                        "authors": [
                            "Ronell Sicat",
                            "Mohamed Ibrahim",
                            "Amani Ageeli",
                            "Florian Mannuss",
                            "Peter Rautek",
                            "Markus Hadwiger"
                        ],
                        "abstract": "The rapidly growing size and complexity of 3D geological models has increased the need for level-of-detail techniques and compact encodings to facilitate interactive visualization. For large-scale hexahedral meshes, state-of-the-art approaches often employ wavelet schemes for level of detail as well as for data compression. Here, wavelet transforms serve two  purposes: (1) they achieve substantial compression for data reduction; and (2) the multiresolution encoding provides levels of detail for visualization. However, in coarser detail levels, important geometric features, such as geological faults, often get too smoothed out or lost, due to linear translation invariant filtering. The same is true for attribute features, such as discontinuities in porosity or permeability.We present a novel, integrated approach addressing both purposes above, while preserving critical data features of both model geometry and its attributes. Our first major contribution is that we completely decouple the computation of levels of detail from data compression, and perform nonlinear filtering in a high-dimensional data space jointly representing the geological model geometry with its attributes. Computing detail levels in this space enables us to jointly preserve features in both geometry and attributes. While designed in a general way, our framework specifically employs joint bilateral filters, computed efficiently on a high-dimensional permutohedral grid. For data compression, after the computation of all detail levels, each level is separately encoded with a standard wavelet transform. Our second major contribution is a compact GPU data structure for the encoded mesh and attributes that enables direct real-time GPU visualization without prior decoding.",
                        "uid": "v-tvcg-9576578",
                        "file_name": "v-tvcg-9576578_Sicat_Presentation.mp4",
                        "time_stamp": "2022-10-21T14:00:00Z",
                        "time_start": "2022-10-21T14:00:00Z",
                        "time_end": "2022-10-21T14:10:00Z",
                        "paper_type": "full",
                        "keywords": [
                            "Geological model visualization, Structured hexahedral meshes, Multiresolution representations and visualization, GPU data structures and rendering"
                        ],
                        "has_image": "1",
                        "has_video": "528",
                        "paper_award": "",
                        "image_caption": "Coarse resolution levels of hexahedral meshes with attributes computed using subsampling (b,c) can suffer from degradation of small details and faults (magenta annotations: faults disappear and turn into slanted cells). HexaShrink (e,f) is able to maintain faults but attribute edges can bleed out (red annotations) and the mesh and attribute spatial positions are misaligned (orange annotations). Our approach (a,d) considers the mesh and attribute features jointly so that even regions with homogenous or flat attributes but with non-homogenous mesh (cyan boxes) or other way around (green boxes) will be rendered with adaptively fine-detailed geometry.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "https://youtu.be/1CP3sHYxybE",
                        "ff_id": "1CP3sHYxybE"
                    },
                    {
                        "slot_id": "v-tvcg-9576578-qa",
                        "session_id": "full6",
                        "type": "In Person Q+A",
                        "title": "Real-Time Visualization of Large-Scale Geological Models with Nonlinear Feature-Preserving  Levels of Detail (Q+A)",
                        "contributors": [
                            "Ronell Sicat"
                        ],
                        "authors": [],
                        "abstract": "The rapidly growing size and complexity of 3D geological models has increased the need for level-of-detail techniques and compact encodings to facilitate interactive visualization. For large-scale hexahedral meshes, state-of-the-art approaches often employ wavelet schemes for level of detail as well as for data compression. Here, wavelet transforms serve two  purposes: (1) they achieve substantial compression for data reduction; and (2) the multiresolution encoding provides levels of detail for visualization. However, in coarser detail levels, important geometric features, such as geological faults, often get too smoothed out or lost, due to linear translation invariant filtering. The same is true for attribute features, such as discontinuities in porosity or permeability.We present a novel, integrated approach addressing both purposes above, while preserving critical data features of both model geometry and its attributes. Our first major contribution is that we completely decouple the computation of levels of detail from data compression, and perform nonlinear filtering in a high-dimensional data space jointly representing the geological model geometry with its attributes. Computing detail levels in this space enables us to jointly preserve features in both geometry and attributes. While designed in a general way, our framework specifically employs joint bilateral filters, computed efficiently on a high-dimensional permutohedral grid. For data compression, after the computation of all detail levels, each level is separately encoded with a standard wavelet transform. Our second major contribution is a compact GPU data structure for the encoded mesh and attributes that enables direct real-time GPU visualization without prior decoding.",
                        "uid": "v-tvcg-9576578",
                        "file_name": "",
                        "time_stamp": "2022-10-21T14:10:00Z",
                        "time_start": "2022-10-21T14:10:00Z",
                        "time_end": "2022-10-21T14:12:00Z",
                        "paper_type": "full",
                        "keywords": [
                            "Geological model visualization, Structured hexahedral meshes, Multiresolution representations and visualization, GPU data structures and rendering"
                        ],
                        "has_image": "1",
                        "has_video": "528",
                        "paper_award": "",
                        "image_caption": "Coarse resolution levels of hexahedral meshes with attributes computed using subsampling (b,c) can suffer from degradation of small details and faults (magenta annotations: faults disappear and turn into slanted cells). HexaShrink (e,f) is able to maintain faults but attribute edges can bleed out (red annotations) and the mesh and attribute spatial positions are misaligned (orange annotations). Our approach (a,d) considers the mesh and attribute features jointly so that even regions with homogenous or flat attributes but with non-homogenous mesh (cyan boxes) or other way around (green boxes) will be rendered with adaptively fine-detailed geometry.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "https://youtu.be/1CP3sHYxybE",
                        "ff_id": "1CP3sHYxybE"
                    },
                    {
                        "slot_id": "v-tvcg-9736650-pres",
                        "session_id": "full6",
                        "type": "In Person Presentation",
                        "title": "CosmoVis: An Interactive Visual Analysis Tool for Exploring Hydrodynamic Cosmological Simulations",
                        "contributors": [
                            "Angus G. Forbes",
                            "David Abramov"
                        ],
                        "authors": [
                            "David Abramov",
                            "Joseph N. Burchett",
                            "Oskar Elek",
                            "Cameron Hummels",
                            "J. Xavier Prochaska",
                            "Angus G. Forbes"
                        ],
                        "abstract": "We introduce CosmoVis, an open source web-based visualization tool for the interactive analysis of massive hydrodynamic cosmological simulation data. CosmoVis was designed in close collaboration with astrophysicists to enable researchers and citizen scientists to share and explore these datasets, and to use them to investigate a range of scientific questions. CosmoVis visualizes many key gas, dark matter, and stellar attributes extracted from the source simulations, which typically consist of complex data structures multiple terabytes in size, often requiring extensive data wrangling. CosmoVis introduces a range of features to facilitate real-time analysis of these simulations, including the use of \"virtual skewers,\" simulated analogues of absorption line spectroscopy that act as spectral probes piercing the volume of gaseous cosmic medium. We explain how such synthetic spectra can be used to gain insight into the source datasets and to make functional comparisons with observational data. Furthermore, we identify the main analysis tasks that CosmoVis enables and present implementation details of the software interface and the client-server architecture. We conclude by providing details of three contemporary scientific use cases that were conducted by domain experts using the software and by documenting expert feedback from astrophysicists at different career levels.",
                        "uid": "v-tvcg-9736650",
                        "file_name": "v-tvcg-9736650_Abramov_Presentation.mp4",
                        "time_stamp": "2022-10-21T14:12:00Z",
                        "time_start": "2022-10-21T14:12:00Z",
                        "time_end": "2022-10-21T14:22:00Z",
                        "paper_type": "full",
                        "keywords": [
                            "Astrovis, astrographics, cosmological simulations, astronomy, astrophysics, virtual spectrography"
                        ],
                        "has_image": "1",
                        "has_video": "139",
                        "paper_award": "",
                        "image_caption": "Cosmological simulations are rendered volumetrically within the CosmoVis software, illustrating the various types of properties within them. Here, a user interactively places a \u201cvirtual skewer\u201d within the EAGLE 12 Mpc simulation, as shown in the leftmost panel (temperature). Skewers are used to generate absorption line spectra, ion column densities, and other properties. The remaining three panels display gas density, temperature, and metallicity fields within the cross-section from the first panel, where the same skewer runs through each panel. Plots displaying thermodynamical properties of the gas intercepted by the skewer are shown in the lower-right corner of the last three panels.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "https://youtu.be/XN4DOKEEOFo",
                        "ff_id": "XN4DOKEEOFo"
                    },
                    {
                        "slot_id": "v-tvcg-9736650-qa",
                        "session_id": "full6",
                        "type": "In Person Q+A",
                        "title": "CosmoVis: An Interactive Visual Analysis Tool for Exploring Hydrodynamic Cosmological Simulations (Q+A)",
                        "contributors": [
                            "Angus G. Forbes",
                            "David Abramov"
                        ],
                        "authors": [],
                        "abstract": "We introduce CosmoVis, an open source web-based visualization tool for the interactive analysis of massive hydrodynamic cosmological simulation data. CosmoVis was designed in close collaboration with astrophysicists to enable researchers and citizen scientists to share and explore these datasets, and to use them to investigate a range of scientific questions. CosmoVis visualizes many key gas, dark matter, and stellar attributes extracted from the source simulations, which typically consist of complex data structures multiple terabytes in size, often requiring extensive data wrangling. CosmoVis introduces a range of features to facilitate real-time analysis of these simulations, including the use of \"virtual skewers,\" simulated analogues of absorption line spectroscopy that act as spectral probes piercing the volume of gaseous cosmic medium. We explain how such synthetic spectra can be used to gain insight into the source datasets and to make functional comparisons with observational data. Furthermore, we identify the main analysis tasks that CosmoVis enables and present implementation details of the software interface and the client-server architecture. We conclude by providing details of three contemporary scientific use cases that were conducted by domain experts using the software and by documenting expert feedback from astrophysicists at different career levels.",
                        "uid": "v-tvcg-9736650",
                        "file_name": "",
                        "time_stamp": "2022-10-21T14:22:00Z",
                        "time_start": "2022-10-21T14:22:00Z",
                        "time_end": "2022-10-21T14:24:00Z",
                        "paper_type": "full",
                        "keywords": [
                            "Astrovis, astrographics, cosmological simulations, astronomy, astrophysics, virtual spectrography"
                        ],
                        "has_image": "1",
                        "has_video": "139",
                        "paper_award": "",
                        "image_caption": "Cosmological simulations are rendered volumetrically within the CosmoVis software, illustrating the various types of properties within them. Here, a user interactively places a \u201cvirtual skewer\u201d within the EAGLE 12 Mpc simulation, as shown in the leftmost panel (temperature). Skewers are used to generate absorption line spectra, ion column densities, and other properties. The remaining three panels display gas density, temperature, and metallicity fields within the cross-section from the first panel, where the same skewer runs through each panel. Plots displaying thermodynamical properties of the gas intercepted by the skewer are shown in the lower-right corner of the last three panels.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "https://youtu.be/XN4DOKEEOFo",
                        "ff_id": "XN4DOKEEOFo"
                    },
                    {
                        "slot_id": "v-tvcg-9599597-pres",
                        "session_id": "full6",
                        "type": "Virtual Presentation (pre-recorded)",
                        "title": "Dynamic Mode Decomposition for Large-Scale Coherent Structure Extraction in Shear Flows",
                        "contributors": [
                            "Duong Nguyen"
                        ],
                        "authors": [
                            "Duong Nguyen",
                            "Panruo Wu",
                            "Rodolfo Ostilla Monico",
                            "Guoning Chen"
                        ],
                        "abstract": "Large-scale structures have been observed in many shear flows which are the fluid generated between two surfaces moving with different velocity. A better understanding of the physics of the structures (especially large-scale structures) in shear flows will help explain a diverse range of physical phenomena and improve our capability of modeling more complex turbulence flows. Many efforts have been made in order to capture such structures; however, conventional methods have their limitations, such as arbitrariness in parameter choice or specificity to certain setups. To address this challenge, we propose to use Multi-Resolution Dynamic Mode Decomposition (mrDMD), for large-scale structure extraction in shear flows. In particular, we show that the slow-motion DMD modes are able to reveal large-scale structures in shear flows that also have slow dynamics. In most cases, we find that the slowest DMD mode and its reconstructed flow can sufficiently capture the large-scale dynamics in the shear flows, which leads to a parameter-free strategy for large-scale structure extraction. Effective visualization of the large-scale structures can then be produced with the aid of the slowest DMD mode. To speed up the computation of mrDMD, we provide a fast GPU-based implementation. We also apply our method to some non-shear flows that need not behave quasi-linearly to demonstrate the limitation of our strategy of using the slowest DMD mode. For non-shear flows, we show that multiple modes from different levels of mrDMD may be needed to sufficiently characterize the flow behavior.",
                        "uid": "v-tvcg-9599597",
                        "file_name": "v-tvcg-9599597_Nguyen_Presentation.mp4",
                        "time_stamp": "2022-10-21T14:24:00Z",
                        "time_start": "2022-10-21T14:24:00Z",
                        "time_end": "2022-10-21T14:35:30Z",
                        "paper_type": "full",
                        "keywords": [
                            "Flow visualization, Shear Flows, Dynamic Mode Decomposition"
                        ],
                        "has_image": "1",
                        "has_video": "690",
                        "paper_award": "",
                        "image_caption": "Large-scale coherent structures extraction for shear flows based on Dynamic Mode Decomposition",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "https://youtu.be/da8vB12WalE",
                        "ff_id": "da8vB12WalE"
                    },
                    {
                        "slot_id": "v-tvcg-9599597-qa",
                        "session_id": "full6",
                        "type": "Virtual Q+A",
                        "title": "Dynamic Mode Decomposition for Large-Scale Coherent Structure Extraction in Shear Flows (Q+A)",
                        "contributors": [
                            "Duong Nguyen"
                        ],
                        "authors": [],
                        "abstract": "Large-scale structures have been observed in many shear flows which are the fluid generated between two surfaces moving with different velocity. A better understanding of the physics of the structures (especially large-scale structures) in shear flows will help explain a diverse range of physical phenomena and improve our capability of modeling more complex turbulence flows. Many efforts have been made in order to capture such structures; however, conventional methods have their limitations, such as arbitrariness in parameter choice or specificity to certain setups. To address this challenge, we propose to use Multi-Resolution Dynamic Mode Decomposition (mrDMD), for large-scale structure extraction in shear flows. In particular, we show that the slow-motion DMD modes are able to reveal large-scale structures in shear flows that also have slow dynamics. In most cases, we find that the slowest DMD mode and its reconstructed flow can sufficiently capture the large-scale dynamics in the shear flows, which leads to a parameter-free strategy for large-scale structure extraction. Effective visualization of the large-scale structures can then be produced with the aid of the slowest DMD mode. To speed up the computation of mrDMD, we provide a fast GPU-based implementation. We also apply our method to some non-shear flows that need not behave quasi-linearly to demonstrate the limitation of our strategy of using the slowest DMD mode. For non-shear flows, we show that multiple modes from different levels of mrDMD may be needed to sufficiently characterize the flow behavior.",
                        "uid": "v-tvcg-9599597",
                        "file_name": "",
                        "time_stamp": "2022-10-21T14:35:30Z",
                        "time_start": "2022-10-21T14:35:30Z",
                        "time_end": "2022-10-21T14:36:00Z",
                        "paper_type": "full",
                        "keywords": [
                            "Flow visualization, Shear Flows, Dynamic Mode Decomposition"
                        ],
                        "has_image": "1",
                        "has_video": "690",
                        "paper_award": "",
                        "image_caption": "Large-scale coherent structures extraction for shear flows based on Dynamic Mode Decomposition",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "https://youtu.be/da8vB12WalE",
                        "ff_id": "da8vB12WalE"
                    },
                    {
                        "slot_id": "v-full-1239-pres",
                        "session_id": "full6",
                        "type": "Virtual Presentation (pre-recorded)",
                        "title": "A Comparison of Spatiotemporal Visualizations for 3D Urban Analytics",
                        "contributors": [
                            "Roberta Mota",
                            "Roberta Cabral Ramos Mota"
                        ],
                        "authors": [
                            "Roberta Mota",
                            "Fabio Miranda",
                            "Julio Daniel Silva",
                            "Marius Horga",
                            "Marcos Lage",
                            "Luis Ceferino",
                            "Usman Raza Alim",
                            "Ehud Sharlin",
                            "Nivan Ferreira"
                        ],
                        "abstract": "Recent technological innovations have led to an increase in the availability of 3D urban data, such as shadow, noise, solar potential, and earthquake simulations. These spatiotemporal datasets create opportunities for new visualizations to engage experts from different domains to study the dynamic behavior of urban spaces in this under explored dimension. However, designing 3D spatiotemporal urban visualizations is challenging, as it requires visual strategies to support analysis of time-varying data referent to the city geometry. Although different visual strategies have been used in 3D urban visual analytics, the question of how effective these visual designs are at supporting spatiotemporal analysis on building surfaces remains open. To investigate this, in this paper we first contribute a series of analytical tasks elicited after interviews with practitioners from three urban domains. We also contribute a quantitative user study comparing the effectiveness of four representative visual designs used to visualize 3D spatiotemporal urban data: spatial juxtaposition, temporal juxtaposition, linked view, and embedded view. Participants performed a series of tasks that required them to identify extreme values on building surfaces over time. Tasks varied in granularity for both space and time dimensions. Our results demonstrate that participants were more accurate using plot-based visualizations (linked view, embedded view) but faster using color-coded visualizations (spatial juxtaposition, temporal juxtaposition). Our results also show that, with increasing task complexity, plot-based visualizations perform better in preserving efficiency (time, accuracy) compared to color-coded visualizations. Based on our findings, we present a set of takeaways with design recommendations for 3D spatiotemporal urban visualizations for researchers and practitioners. Lastly, we report on a series of interviews with four practitioners, and their feedback and suggestions for further work on the visualizations to support 3D spatiotemporal urban data analysis.",
                        "uid": "v-full-1239",
                        "file_name": "v-full-1239_Mota_Presentation.mp4",
                        "time_stamp": "2022-10-21T14:36:00Z",
                        "time_start": "2022-10-21T14:36:00Z",
                        "time_end": "2022-10-21T14:46:17Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "617",
                        "paper_award": "",
                        "image_caption": "The four spatiotemporal visualizations compared in our study. From left to right: Embedded View, Linked View, Temporal Juxtaposition, and Spatial Juxtaposition. In this example, the visualizations encode the values of an artificially-created temporal attribute of the building facade with four time steps. In each visualization, three rectangular regions with colored outlines are placed along the building surface. When comparing their attribute values, the blue region has the minimum average attribute value between [t1,t2] time steps, whereas the red and black regions have the maximum average values in t3 and t4, respectively.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "https://youtu.be/TyrUZWjKRw0",
                        "ff_id": "TyrUZWjKRw0"
                    },
                    {
                        "slot_id": "v-full-1239-qa",
                        "session_id": "full6",
                        "type": "Virtual Q+A",
                        "title": "A Comparison of Spatiotemporal Visualizations for 3D Urban Analytics (Q+A)",
                        "contributors": [
                            "Roberta Mota",
                            "Roberta Cabral Ramos Mota"
                        ],
                        "authors": [],
                        "abstract": "Recent technological innovations have led to an increase in the availability of 3D urban data, such as shadow, noise, solar potential, and earthquake simulations. These spatiotemporal datasets create opportunities for new visualizations to engage experts from different domains to study the dynamic behavior of urban spaces in this under explored dimension. However, designing 3D spatiotemporal urban visualizations is challenging, as it requires visual strategies to support analysis of time-varying data referent to the city geometry. Although different visual strategies have been used in 3D urban visual analytics, the question of how effective these visual designs are at supporting spatiotemporal analysis on building surfaces remains open. To investigate this, in this paper we first contribute a series of analytical tasks elicited after interviews with practitioners from three urban domains. We also contribute a quantitative user study comparing the effectiveness of four representative visual designs used to visualize 3D spatiotemporal urban data: spatial juxtaposition, temporal juxtaposition, linked view, and embedded view. Participants performed a series of tasks that required them to identify extreme values on building surfaces over time. Tasks varied in granularity for both space and time dimensions. Our results demonstrate that participants were more accurate using plot-based visualizations (linked view, embedded view) but faster using color-coded visualizations (spatial juxtaposition, temporal juxtaposition). Our results also show that, with increasing task complexity, plot-based visualizations perform better in preserving efficiency (time, accuracy) compared to color-coded visualizations. Based on our findings, we present a set of takeaways with design recommendations for 3D spatiotemporal urban visualizations for researchers and practitioners. Lastly, we report on a series of interviews with four practitioners, and their feedback and suggestions for further work on the visualizations to support 3D spatiotemporal urban data analysis.",
                        "uid": "v-full-1239",
                        "file_name": "",
                        "time_stamp": "2022-10-21T14:46:17Z",
                        "time_start": "2022-10-21T14:46:17Z",
                        "time_end": "2022-10-21T14:48:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "617",
                        "paper_award": "",
                        "image_caption": "The four spatiotemporal visualizations compared in our study. From left to right: Embedded View, Linked View, Temporal Juxtaposition, and Spatial Juxtaposition. In this example, the visualizations encode the values of an artificially-created temporal attribute of the building facade with four time steps. In each visualization, three rectangular regions with colored outlines are placed along the building surface. When comparing their attribute values, the blue region has the minimum average attribute value between [t1,t2] time steps, whereas the red and black regions have the maximum average values in t3 and t4, respectively.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "https://youtu.be/TyrUZWjKRw0",
                        "ff_id": "TyrUZWjKRw0"
                    },
                    {
                        "slot_id": "v-tvcg-9750868-pres",
                        "session_id": "full6",
                        "type": "In Person Presentation",
                        "title": "EpiMob: Interactive Visual Analytics of Citywide Human Mobility Restrictions for Epidemic Control",
                        "contributors": [
                            "Chuang Yang"
                        ],
                        "authors": [
                            "Chuang Yang",
                            "Zhiwen Zhang",
                            "Zipei Fan",
                            "Renhe Jiang",
                            "Quanjun Chen",
                            "Xuan Song",
                            "Ryosuke Shibasaki."
                        ],
                        "abstract": "The outbreak of coronavirus disease (COVID-19) has swept across more than 180 countries and territories since late January 2020. As a worldwide emergency response, governments have implemented various measures and policies, such as self-quarantine, travel restrictions, work from home, and regional lockdown, to control the spread of the epidemic. These countermeasures seek to restrict human mobility because COVID-19 is a highly contagious disease that is spread by human-to-human transmission. Medical experts and policymakers have expressed the urgency to effectively evaluate the outcome of human restriction policies with the aid of big data and information technology. Thus, based on big human mobility data and city POI data, an interactive visual analytics system called Epidemic Mobility (EpiMob) was designed in this study. The system interactively simulates the changes in human mobility and infection status in response to the implementation of a certain restriction policy or a combination of policies (e.g., regional lockdown, telecommuting, screening). Users can conveniently designate the spatial and temporal ranges for different mobility restriction policies. Then, the results reflecting the infection situation under different policies are dynamically displayed and can be flexibly compared and analyzed in depth. Multiple case studies consisting of interviews with domain experts were conducted in the largest metropolitan area of Japan (i.e., Greater Tokyo Area) to demonstrate that the system can provide insight into the effects of different human mobility restriction policies for epidemic control, through measurements and comparisons.",
                        "uid": "v-tvcg-9750868",
                        "file_name": "v-tvcg-9750868_Yang_Presentation.mp4",
                        "time_stamp": "2022-10-21T14:48:00Z",
                        "time_start": "2022-10-21T14:48:00Z",
                        "time_end": "2022-10-21T14:58:00Z",
                        "paper_type": "full",
                        "keywords": [
                            "Human mobility simulation, epidemic control, visual analytics, interactive system, big trajectory data"
                        ],
                        "has_image": "1",
                        "has_video": "603",
                        "paper_award": "",
                        "image_caption": "EpiMob\u2013an interactive visual analytics system for simulating and evaluating the effects of mobility restriction policies for epidemic control. In Panel A the user is enabled to specify the mobility restriction policies, including A1\u2014regional lockdown, A2\u2014screening, and A3\u2014telecommuting, and can also set the essential epidemic parameters to adapt to different diseases and local environments (A4, A5). The simulation results are listed in the overview panel B. By clicking the in-depth analysis button of a simulation result, users can further analyze its spatial propagation features (panel D). The user can also perform a comparative analysis (panel C).",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "https://youtu.be/n0cMlkMMtjs",
                        "ff_id": "n0cMlkMMtjs"
                    },
                    {
                        "slot_id": "v-tvcg-9750868-qa",
                        "session_id": "full6",
                        "type": "In Person Q+A",
                        "title": "EpiMob: Interactive Visual Analytics of Citywide Human Mobility Restrictions for Epidemic Control (Q+A)",
                        "contributors": [
                            "Chuang Yang"
                        ],
                        "authors": [],
                        "abstract": "The outbreak of coronavirus disease (COVID-19) has swept across more than 180 countries and territories since late January 2020. As a worldwide emergency response, governments have implemented various measures and policies, such as self-quarantine, travel restrictions, work from home, and regional lockdown, to control the spread of the epidemic. These countermeasures seek to restrict human mobility because COVID-19 is a highly contagious disease that is spread by human-to-human transmission. Medical experts and policymakers have expressed the urgency to effectively evaluate the outcome of human restriction policies with the aid of big data and information technology. Thus, based on big human mobility data and city POI data, an interactive visual analytics system called Epidemic Mobility (EpiMob) was designed in this study. The system interactively simulates the changes in human mobility and infection status in response to the implementation of a certain restriction policy or a combination of policies (e.g., regional lockdown, telecommuting, screening). Users can conveniently designate the spatial and temporal ranges for different mobility restriction policies. Then, the results reflecting the infection situation under different policies are dynamically displayed and can be flexibly compared and analyzed in depth. Multiple case studies consisting of interviews with domain experts were conducted in the largest metropolitan area of Japan (i.e., Greater Tokyo Area) to demonstrate that the system can provide insight into the effects of different human mobility restriction policies for epidemic control, through measurements and comparisons.",
                        "uid": "v-tvcg-9750868",
                        "file_name": "",
                        "time_stamp": "2022-10-21T14:58:00Z",
                        "time_start": "2022-10-21T14:58:00Z",
                        "time_end": "2022-10-21T15:00:00Z",
                        "paper_type": "full",
                        "keywords": [
                            "Human mobility simulation, epidemic control, visual analytics, interactive system, big trajectory data"
                        ],
                        "has_image": "1",
                        "has_video": "603",
                        "paper_award": "",
                        "image_caption": "EpiMob\u2013an interactive visual analytics system for simulating and evaluating the effects of mobility restriction policies for epidemic control. In Panel A the user is enabled to specify the mobility restriction policies, including A1\u2014regional lockdown, A2\u2014screening, and A3\u2014telecommuting, and can also set the essential epidemic parameters to adapt to different diseases and local environments (A4, A5). The simulation results are listed in the overview panel B. By clicking the in-depth analysis button of a simulation result, users can further analyze its spatial propagation features (panel D). The user can also perform a comparative analysis (panel C).",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "https://youtu.be/n0cMlkMMtjs",
                        "ff_id": "n0cMlkMMtjs"
                    },
                    {
                        "slot_id": "v-full-1291-pres",
                        "session_id": "full6",
                        "type": "Virtual Presentation (pre-recorded)",
                        "title": "On-Tube Attribute Visualization for Multivariate Trajectory Data",
                        "contributors": [
                            "Benjamin Russig"
                        ],
                        "authors": [
                            "Benjamin Russig",
                            "David Gro\u00df",
                            "Raimund Dachselt",
                            "Stefan Gumhold"
                        ],
                        "abstract": "Stylized tubes are an established visualization primitive for line data as encountered in many scientific fields, ranging from characteristic lines in flow fields, fiber tracks reconstructed from diffusion tensor imaging, to trajectories of moving objects as they arise from cyber-physical systems in many engineering disciplines. Typical challenges include large data set sizes demanding for efficient rendering techniques as well as a large number of attributes that cannot be mapped simultaneously to the basic visual attributes provided by a tube-based visualization. In this work, we tackle both challenges with a new on-tube visualization approach. We improve recent work on high-quality GPU ray casting of Hermite spline tubes supporting ambient occlusion and extend it by a new layered procedural texturing technique. In the proposed framework, a large number of data set attributes can be mapped simultaneously to a variety of glyphs and plots that are embedded in texture space and organized in layers. Efficient rendering with minimal data transfer is achieved by generating the glyphs procedurally and drawing them in a deferred shading pass. We integrated these techniques in a prototype visualization tool that facilitates flexible mapping of data set attributes to visual tube and glyph attributes. We studied our approach on a variety of example data from different fields and found it to provide a highly adaptable and extensible toolbox to quickly craft tailor-made tube-based trajectory visualizations.",
                        "uid": "v-full-1291",
                        "file_name": "v-full-1291_Russig_Presentation.mp4",
                        "time_stamp": "2022-10-21T15:00:00Z",
                        "time_start": "2022-10-21T15:00:00Z",
                        "time_end": "2022-10-21T15:10:55Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "655",
                        "paper_award": "",
                        "image_caption": "Various on-tube visualizations of vehicle GPS trajectories (top row) and stream lines (bottom row), visualizing many additional attributes right at the spatial locations they were sampled at. Procedural visualization primitives visible in these examples include circle, triangle, rectangle and sign glyphs, line plots and star coordinates, as well as a normal-mapped grid aiding perception of relative sizes.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "https://youtu.be/no-xqx4VRQ8",
                        "ff_id": "no-xqx4VRQ8"
                    },
                    {
                        "slot_id": "v-full-1291-qa",
                        "session_id": "full6",
                        "type": "Virtual Q+A",
                        "title": "On-Tube Attribute Visualization for Multivariate Trajectory Data (Q+A)",
                        "contributors": [
                            "Benjamin Russig"
                        ],
                        "authors": [],
                        "abstract": "Stylized tubes are an established visualization primitive for line data as encountered in many scientific fields, ranging from characteristic lines in flow fields, fiber tracks reconstructed from diffusion tensor imaging, to trajectories of moving objects as they arise from cyber-physical systems in many engineering disciplines. Typical challenges include large data set sizes demanding for efficient rendering techniques as well as a large number of attributes that cannot be mapped simultaneously to the basic visual attributes provided by a tube-based visualization. In this work, we tackle both challenges with a new on-tube visualization approach. We improve recent work on high-quality GPU ray casting of Hermite spline tubes supporting ambient occlusion and extend it by a new layered procedural texturing technique. In the proposed framework, a large number of data set attributes can be mapped simultaneously to a variety of glyphs and plots that are embedded in texture space and organized in layers. Efficient rendering with minimal data transfer is achieved by generating the glyphs procedurally and drawing them in a deferred shading pass. We integrated these techniques in a prototype visualization tool that facilitates flexible mapping of data set attributes to visual tube and glyph attributes. We studied our approach on a variety of example data from different fields and found it to provide a highly adaptable and extensible toolbox to quickly craft tailor-made tube-based trajectory visualizations.",
                        "uid": "v-full-1291",
                        "file_name": "",
                        "time_stamp": "2022-10-21T15:10:55Z",
                        "time_start": "2022-10-21T15:10:55Z",
                        "time_end": "2022-10-21T15:12:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "655",
                        "paper_award": "",
                        "image_caption": "Various on-tube visualizations of vehicle GPS trajectories (top row) and stream lines (bottom row), visualizing many additional attributes right at the spatial locations they were sampled at. Procedural visualization primitives visible in these examples include circle, triangle, rectangle and sign glyphs, line plots and star coordinates, as well as a normal-mapped grid aiding perception of relative sizes.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "https://youtu.be/no-xqx4VRQ8",
                        "ff_id": "no-xqx4VRQ8"
                    }
                ]
            },
            {
                "title": "Uncertainty",
                "session_id": "full7",
                "event_prefix": "v-full",
                "track": "ok5",
                "livestream_id": "ok5-wed",
                "session_image": "full7.png",
                "chair": [
                    "Kristi Potter"
                ],
                "organizers": [],
                "time_start": "2022-10-19T20:45:00Z",
                "time_end": "2022-10-19T22:00:00Z",
                "discord_category": "",
                "discord_channel": "oklahoma-st-5",
                "discord_channel_id": "1024600839295356939",
                "discord_link": "https://discord.com/channels/978320435554947082/1024600839295356939",
                "slido_link": "https://app.sli.do/event/i28UETZaiRWr1XwX6eKjcf?section=d355f89b-fbd2-4abf-9958-741607905551",
                "youtube_url": "https://youtu.be/wr1l85Jo7jM",
                "youtube_id": "wr1l85Jo7jM",
                "zoom_meeting": "",
                "zoom_password": "",
                "zoom_link": "",
                "ff_link": "https://youtu.be/fSlkA9B9_1g",
                "ff_playlist": "",
                "time_slots": [
                    {
                        "slot_id": "full7-opening",
                        "session_id": "full7",
                        "type": "In Person Other",
                        "title": "Session Opening",
                        "contributors": [
                            "Kristi Potter"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "",
                        "file_name": "",
                        "time_stamp": "2022-10-19T20:45:00Z",
                        "time_start": "2022-10-19T20:45:00Z",
                        "time_end": "2022-10-19T20:45:00Z",
                        "paper_type": "",
                        "keywords": [],
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "v-full-1307-pres",
                        "session_id": "full7",
                        "type": "Virtual Presentation (pre-recorded)",
                        "title": "Fiber Uncertainty Visualization for Bivariate Data With Parametric and Nonparametric Noise Models",
                        "contributors": [
                            "Dr. Tushar M. Athawale"
                        ],
                        "authors": [
                            "Tushar M. Athawale",
                            "Chris R. Johnson",
                            "Sudhanshu Sane",
                            "David Pugmire"
                        ],
                        "abstract": "Visualization and analysis of multivariate data and their uncertainty are top research challenges in data visualization. Constructing fiber surfaces is a popular technique for multivariate data visualization that generalizes the idea of level-set visualization for univariate data to multivariate data. In this paper, we present a statistical framework to quantify positional probabilities of fibers extracted from uncertain bivariate fields. Specifically, we extend the state-of-the-art Gaussian models of uncertainty for bivariate data to other parametric distributions (e.g., uniform and Epanechnikov) and more general nonparametric probability distributions (e.g., histograms and kernel density estimation) and derive corresponding spatial probabilities of fibers. In our proposed framework, we leverage Green\u2019s theorem for closed-form computation of fiber probabilities when bivariate data are assumed to have independent parametric and nonparametric noise. Additionally, we present a nonparametric approach combined with numerical integration to study the positional probability of fibers when bivariate data are assumed to have correlated noise. For uncertainty analysis, we visualize the derived probability volumes for fibers via volume rendering and extracting level sets based on probability thresholds. We present the utility of our proposed techniques via experiments on synthetic and simulation datasets.",
                        "uid": "v-full-1307",
                        "file_name": "v-full-1307_Athawale_Presentation.mp4",
                        "time_stamp": "2022-10-19T20:45:00Z",
                        "time_start": "2022-10-19T20:45:00Z",
                        "time_end": "2022-10-19T20:56:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "660",
                        "paper_award": "",
                        "image_caption": "Mean, parametric, and nonparametric noise models for uncertainty analysis of vortical features of the Red Sea ensemble dataset. Fiber positions are visualized for a trait corresponding to anticyclonic (negative Z component of curl) vortices, as indicated by the cyan polygon in image (a). The parametric noise model (c) suggests multiple high-probability vortices inside the magenta box, whereas the mean (b) and nonparametric (d) statistical models suggest a relatively low number of vortices inside the magenta box. Such inconsistency inside the magenta box across the three statistical models indicates the need for further eddy analysis in the same region.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "https://youtu.be/X2qxav8zXsk",
                        "ff_id": "X2qxav8zXsk"
                    },
                    {
                        "slot_id": "v-full-1307-qa",
                        "session_id": "full7",
                        "type": "Virtual Q+A",
                        "title": "Fiber Uncertainty Visualization for Bivariate Data With Parametric and Nonparametric Noise Models (Q+A)",
                        "contributors": [
                            "Dr. Tushar M. Athawale"
                        ],
                        "authors": [],
                        "abstract": "Visualization and analysis of multivariate data and their uncertainty are top research challenges in data visualization. Constructing fiber surfaces is a popular technique for multivariate data visualization that generalizes the idea of level-set visualization for univariate data to multivariate data. In this paper, we present a statistical framework to quantify positional probabilities of fibers extracted from uncertain bivariate fields. Specifically, we extend the state-of-the-art Gaussian models of uncertainty for bivariate data to other parametric distributions (e.g., uniform and Epanechnikov) and more general nonparametric probability distributions (e.g., histograms and kernel density estimation) and derive corresponding spatial probabilities of fibers. In our proposed framework, we leverage Green\u2019s theorem for closed-form computation of fiber probabilities when bivariate data are assumed to have independent parametric and nonparametric noise. Additionally, we present a nonparametric approach combined with numerical integration to study the positional probability of fibers when bivariate data are assumed to have correlated noise. For uncertainty analysis, we visualize the derived probability volumes for fibers via volume rendering and extracting level sets based on probability thresholds. We present the utility of our proposed techniques via experiments on synthetic and simulation datasets.",
                        "uid": "v-full-1307",
                        "file_name": "",
                        "time_stamp": "2022-10-19T20:56:00Z",
                        "time_start": "2022-10-19T20:56:00Z",
                        "time_end": "2022-10-19T20:57:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "660",
                        "paper_award": "",
                        "image_caption": "Mean, parametric, and nonparametric noise models for uncertainty analysis of vortical features of the Red Sea ensemble dataset. Fiber positions are visualized for a trait corresponding to anticyclonic (negative Z component of curl) vortices, as indicated by the cyan polygon in image (a). The parametric noise model (c) suggests multiple high-probability vortices inside the magenta box, whereas the mean (b) and nonparametric (d) statistical models suggest a relatively low number of vortices inside the magenta box. Such inconsistency inside the magenta box across the three statistical models indicates the need for further eddy analysis in the same region.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "https://youtu.be/X2qxav8zXsk",
                        "ff_id": "X2qxav8zXsk"
                    },
                    {
                        "slot_id": "v-full-1121-pres",
                        "session_id": "full7",
                        "type": "Virtual Presentation (pre-recorded)",
                        "title": "Evaluating the Use of Uncertainty Visualisations for Imputations of Data Missing At Random in Scatterplots",
                        "contributors": [
                            "Abhraneel Sarma"
                        ],
                        "authors": [
                            "Abhraneel Sarma",
                            "Shunan Guo",
                            "Jane Hoffswell",
                            "Ryan Rossi",
                            "Fan Du",
                            "Eunyee Koh",
                            "Matthew Kay"
                        ],
                        "abstract": "Most real-world datasets contain missing values yet most exploratory data analysis (EDA) systems only support visualising data points with complete cases. This omission may potentially lead the user to biased analyses and insights. Imputation techniques can help estimate the value of a missing data point, but introduces additional uncertainty. In this work, we investigate the effects of visualising imputed values in charts using different ways of representing data imputations and imputation uncertainty\u2014no imputation, mean, 95% confidence intervals, probability density plots, gradient intervals, and hypothetical outcome plots. We focus on scatterplots, which is a commonly used chart type, and conduct a crowdsourced study with 202 participants. We measure users\u2019 bias and precision in performing two tasks\u2014estimating average and detecting trend\u2014and their self-reported confidence in performing these tasks. Our results suggest that, when estimating averages, uncertainty representations may reduce bias but at the cost of decreasing precision. When estimating trend, only hypothetical outcome plots may lead to a small probability of reducing bias while increasing precision. Participants in every uncertainty representation were less certain about their response when compared to the baseline. The findings point towards potential trade-offs in using uncertainty encodings for datasets with a large number of missing values. This paper and the associated analysis materials are available at: https://osf.io/q4y5r/",
                        "uid": "v-full-1121",
                        "file_name": "v-full-1121_Sarma_Presentation.mp4",
                        "time_stamp": "2022-10-19T20:57:00Z",
                        "time_start": "2022-10-19T20:57:00Z",
                        "time_end": "2022-10-19T21:06:02Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "542",
                        "paper_award": "",
                        "image_caption": "Datasets can often have missing values (A). Not accounting for missing values of a dataset can lead to incorrect conclusions (B). For example, the figure on the middle-left shows how the trend line can vary when considering only observed data compared to the actual ground truth trend line (which is unknowable). However, imputing missing values can provide the necessary information required for correct inference (C), as the estimate with the imputed dataset is close to the ground truth estimate. In this study, we explore how different ways of representing imputations of a dataset with missing values (D) impact analysts performance on two visual analysis tasks (E)?",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "https://youtu.be/xS7TURfPCdQ",
                        "ff_id": "xS7TURfPCdQ"
                    },
                    {
                        "slot_id": "v-full-1121-qa",
                        "session_id": "full7",
                        "type": "Virtual Q+A",
                        "title": "Evaluating the Use of Uncertainty Visualisations for Imputations of Data Missing At Random in Scatterplots (Q+A)",
                        "contributors": [
                            "Abhraneel Sarma"
                        ],
                        "authors": [],
                        "abstract": "Most real-world datasets contain missing values yet most exploratory data analysis (EDA) systems only support visualising data points with complete cases. This omission may potentially lead the user to biased analyses and insights. Imputation techniques can help estimate the value of a missing data point, but introduces additional uncertainty. In this work, we investigate the effects of visualising imputed values in charts using different ways of representing data imputations and imputation uncertainty\u2014no imputation, mean, 95% confidence intervals, probability density plots, gradient intervals, and hypothetical outcome plots. We focus on scatterplots, which is a commonly used chart type, and conduct a crowdsourced study with 202 participants. We measure users\u2019 bias and precision in performing two tasks\u2014estimating average and detecting trend\u2014and their self-reported confidence in performing these tasks. Our results suggest that, when estimating averages, uncertainty representations may reduce bias but at the cost of decreasing precision. When estimating trend, only hypothetical outcome plots may lead to a small probability of reducing bias while increasing precision. Participants in every uncertainty representation were less certain about their response when compared to the baseline. The findings point towards potential trade-offs in using uncertainty encodings for datasets with a large number of missing values. This paper and the associated analysis materials are available at: https://osf.io/q4y5r/",
                        "uid": "v-full-1121",
                        "file_name": "",
                        "time_stamp": "2022-10-19T21:06:02Z",
                        "time_start": "2022-10-19T21:06:02Z",
                        "time_end": "2022-10-19T21:09:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "542",
                        "paper_award": "",
                        "image_caption": "Datasets can often have missing values (A). Not accounting for missing values of a dataset can lead to incorrect conclusions (B). For example, the figure on the middle-left shows how the trend line can vary when considering only observed data compared to the actual ground truth trend line (which is unknowable). However, imputing missing values can provide the necessary information required for correct inference (C), as the estimate with the imputed dataset is close to the ground truth estimate. In this study, we explore how different ways of representing imputations of a dataset with missing values (D) impact analysts performance on two visual analysis tasks (E)?",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "https://youtu.be/xS7TURfPCdQ",
                        "ff_id": "xS7TURfPCdQ"
                    },
                    {
                        "slot_id": "v-full-1062-pres",
                        "session_id": "full7",
                        "type": "In Person Presentation",
                        "title": "Dispersion vs Disparity: Hiding Variability Can Encourage Stereotyping When Visualizing Social Outcomes",
                        "contributors": [
                            "Cindy Xiong",
                            "Eli Holder"
                        ],
                        "authors": [
                            "Eli Holder",
                            "Cindy Xiong"
                        ],
                        "abstract": "Visualization research often focuses on perceptual accuracy or helping readers interpret key messages. However, we know very little about how chart designs might influence readers' perceptions of the people behind the data. Specifically, could designs interact with readers' social cognitive biases in ways that perpetuate harmful stereotypes? For example, when analyzing social inequality, bar charts are a popular choice to present outcome disparities between race, gender, or other groups. But bar charts may encourage deficit thinking, the perception that outcome disparities are caused by groups\u2019 personal strengths or deficiencies, rather than external factors. These faulty personal attributions can then reinforce stereotypes about the groups being visualized. We conducted four experiments examining design choices that influence attribution biases (and therefore deficit thinking). Crowdworkers viewed visualizations depicting social outcomes that either mask variability in data, such as bar charts or dot plots, or emphasize variability in data, such as jitter plots or prediction intervals. They reported their agreement with both personal and external explanations for the visualized disparities. Overall, when participants saw visualizations that hide within-group variability, they agreed more with personal explanations. When they saw visualizations that emphasize within-group variability, they agreed less with personal explanations. These results demonstrate that data visualizations about social inequity can be misinterpreted in harmful ways and lead to stereotyping. Design choices can influence these biases: Hiding variability tends to increase stereotyping while emphasizing variability reduces it.",
                        "uid": "v-full-1062",
                        "file_name": "v-full-1062_Holder_Presentation.mp4",
                        "time_stamp": "2022-10-19T21:09:00Z",
                        "time_start": "2022-10-19T21:09:00Z",
                        "time_end": "2022-10-19T21:19:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "675",
                        "paper_award": "",
                        "image_caption": "In the United States, social outcomes like health, wealth, and education vary widely between\u00a0race, gender, and other groups.\nVisualizing disparities can help raise awareness.\nBut conventional chart design\u00a0choices can actually reinforce harmful stereotypes about\u00a0the people being visualized.\nThrough a series of experiments, we\u00a0find that charts that hide variability increase stereotyping, whereas charts that\u00a0emphasize variability decrease stereotyping.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "https://youtu.be/qak_QLRIiqQ",
                        "ff_id": "qak_QLRIiqQ"
                    },
                    {
                        "slot_id": "v-full-1062-qa",
                        "session_id": "full7",
                        "type": "In Person Q+A",
                        "title": "Dispersion vs Disparity: Hiding Variability Can Encourage Stereotyping When Visualizing Social Outcomes (Q+A)",
                        "contributors": [
                            "Cindy Xiong",
                            "Eli Holder"
                        ],
                        "authors": [],
                        "abstract": "Visualization research often focuses on perceptual accuracy or helping readers interpret key messages. However, we know very little about how chart designs might influence readers' perceptions of the people behind the data. Specifically, could designs interact with readers' social cognitive biases in ways that perpetuate harmful stereotypes? For example, when analyzing social inequality, bar charts are a popular choice to present outcome disparities between race, gender, or other groups. But bar charts may encourage deficit thinking, the perception that outcome disparities are caused by groups\u2019 personal strengths or deficiencies, rather than external factors. These faulty personal attributions can then reinforce stereotypes about the groups being visualized. We conducted four experiments examining design choices that influence attribution biases (and therefore deficit thinking). Crowdworkers viewed visualizations depicting social outcomes that either mask variability in data, such as bar charts or dot plots, or emphasize variability in data, such as jitter plots or prediction intervals. They reported their agreement with both personal and external explanations for the visualized disparities. Overall, when participants saw visualizations that hide within-group variability, they agreed more with personal explanations. When they saw visualizations that emphasize within-group variability, they agreed less with personal explanations. These results demonstrate that data visualizations about social inequity can be misinterpreted in harmful ways and lead to stereotyping. Design choices can influence these biases: Hiding variability tends to increase stereotyping while emphasizing variability reduces it.",
                        "uid": "v-full-1062",
                        "file_name": "",
                        "time_stamp": "2022-10-19T21:19:00Z",
                        "time_start": "2022-10-19T21:19:00Z",
                        "time_end": "2022-10-19T21:21:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "675",
                        "paper_award": "",
                        "image_caption": "In the United States, social outcomes like health, wealth, and education vary widely between\u00a0race, gender, and other groups.\nVisualizing disparities can help raise awareness.\nBut conventional chart design\u00a0choices can actually reinforce harmful stereotypes about\u00a0the people being visualized.\nThrough a series of experiments, we\u00a0find that charts that hide variability increase stereotyping, whereas charts that\u00a0emphasize variability decrease stereotyping.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "https://youtu.be/qak_QLRIiqQ",
                        "ff_id": "qak_QLRIiqQ"
                    },
                    {
                        "slot_id": "v-full-1269-pres",
                        "session_id": "full7",
                        "type": "In Person Presentation",
                        "title": "Communicating Uncertainty in Digital Humanities Visualization Research",
                        "contributors": [
                            "Georgia Panagiotidou"
                        ],
                        "authors": [
                            "Georgia Panagiotidou",
                            "Houda Lamqaddam",
                            "Jeroen Poblome",
                            "Koenraad Brosens",
                            "Katrien Verbert",
                            "Andrew Vande Moere"
                        ],
                        "abstract": "Due to their historical nature, humanistic data encompass multiple sources of uncertainty. While humanists are accustomed to handling such uncertainty with their established methods, they are cautious of visualizations that appear overly objective and fail to communicate this uncertainty. To design more trustworthy visualizations for humanistic research, therefore, a deeper understanding of its relation to uncertainty is needed. We systematically reviewed 126 publications from digital humanities literature that use visualization as part of their research process, and examined how uncertainty was handled and represented in their visualizations. Crossing these dimensions with the visualization type and use, we identified that uncertainty originated from multiple steps in the research process from the source artifacts to their datafication. We also noted how besides known uncertainty coping strategies, such as excluding data and evaluating its effects, humanists also embraced uncertainty as a separate dimension important to retain. By mapping how the visualizations encoded uncertainty, we identified four approaches that varied in terms of explicitness and customization. This work thus contributes with two empirical taxonomies of uncertainty and it\u2019s corresponding coping strategies, as well as with the foundation of a research agenda for uncertainty visualization in the digital humanities. Our findings further the synergy among humanists and visualization researchers, and ultimately contribute to the development of more trustworthy, uncertainty-aware visualizations.",
                        "uid": "v-full-1269",
                        "file_name": "v-full-1269_Panagiotidou_Presentation.mp4",
                        "time_stamp": "2022-10-19T21:21:00Z",
                        "time_start": "2022-10-19T21:21:00Z",
                        "time_end": "2022-10-19T21:31:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "524",
                        "paper_award": "",
                        "image_caption": "By mapping how visualizations in the digital humanities encoded uncertainty, we identified four approaches that varied in terms of explicitness and customization.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "https://youtu.be/YlUu-_7EItI",
                        "ff_id": "YlUu-_7EItI"
                    },
                    {
                        "slot_id": "v-full-1269-qa",
                        "session_id": "full7",
                        "type": "In Person Q+A",
                        "title": "Communicating Uncertainty in Digital Humanities Visualization Research (Q+A)",
                        "contributors": [
                            "Georgia Panagiotidou"
                        ],
                        "authors": [],
                        "abstract": "Due to their historical nature, humanistic data encompass multiple sources of uncertainty. While humanists are accustomed to handling such uncertainty with their established methods, they are cautious of visualizations that appear overly objective and fail to communicate this uncertainty. To design more trustworthy visualizations for humanistic research, therefore, a deeper understanding of its relation to uncertainty is needed. We systematically reviewed 126 publications from digital humanities literature that use visualization as part of their research process, and examined how uncertainty was handled and represented in their visualizations. Crossing these dimensions with the visualization type and use, we identified that uncertainty originated from multiple steps in the research process from the source artifacts to their datafication. We also noted how besides known uncertainty coping strategies, such as excluding data and evaluating its effects, humanists also embraced uncertainty as a separate dimension important to retain. By mapping how the visualizations encoded uncertainty, we identified four approaches that varied in terms of explicitness and customization. This work thus contributes with two empirical taxonomies of uncertainty and it\u2019s corresponding coping strategies, as well as with the foundation of a research agenda for uncertainty visualization in the digital humanities. Our findings further the synergy among humanists and visualization researchers, and ultimately contribute to the development of more trustworthy, uncertainty-aware visualizations.",
                        "uid": "v-full-1269",
                        "file_name": "",
                        "time_stamp": "2022-10-19T21:31:00Z",
                        "time_start": "2022-10-19T21:31:00Z",
                        "time_end": "2022-10-19T21:33:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "524",
                        "paper_award": "",
                        "image_caption": "By mapping how visualizations in the digital humanities encoded uncertainty, we identified four approaches that varied in terms of explicitness and customization.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "https://youtu.be/YlUu-_7EItI",
                        "ff_id": "YlUu-_7EItI"
                    },
                    {
                        "slot_id": "v-tvcg-9566799-pres",
                        "session_id": "full7",
                        "type": "In Person Presentation",
                        "title": "Fuzzy Spreadsheet: Understanding and Exploring Uncertainties in Tabular Calculations",
                        "contributors": [
                            "Vaishali Dhanoa"
                        ],
                        "authors": [
                            "Vaishali Dhanoa",
                            "Conny Walchshofer",
                            "Andreas Hinterreiter",
                            "Eduard Gr\u00f6ller",
                            "Marc Streit"
                        ],
                        "abstract": "Spreadsheet-based tools provide a simple yet effective way of calculating values, which makes them the number-one choice for building and formalizing simple models for budget planning and many other applications. A cell in a spreadsheet holds one specific value and gives a discrete, over precise view of the underlying model. Therefore, spreadsheets are of limited use when investigating the inherent uncertainties of such models and answering what-if questions. Existing extensions typically require a complex modeling process that cannot easily be embedded in a tabular layout. In Fuzzy Spreadsheet, a cell can hold and display a distribution of values. This integrated uncertainty-handling immediately conveys sensitivity and robustness information. The fuzzification of the cells enables calculations not only with precise values but also with distributions, and probabilities. We conservatively added and carefully crafted visuals to maintain the look and feel of a traditional spreadsheet while facilitating what-if analyses. Given a user-specified reference cell, Fuzzy Spreadsheet automatically extracts and visualizes contextually relevant information, such as impact, uncertainty, and degree of neighborhood, for the selected and related cells. To evaluate its usability and the perceived mental effort required, we conducted a user study. The results show that our approach outperforms traditional spreadsheets in terms of answer correctness, response time, and perceived mental effort in almost all tasks tested.",
                        "uid": "v-tvcg-9566799",
                        "file_name": "v-tvcg-9566799_Dhanoa_Presentation.mp4",
                        "time_stamp": "2022-10-19T21:33:00Z",
                        "time_start": "2022-10-19T21:33:00Z",
                        "time_end": "2022-10-19T21:43:00Z",
                        "paper_type": "full",
                        "keywords": [
                            "Uncertainty visualization, tabular data, spreadsheet augmentation"
                        ],
                        "has_image": "1",
                        "has_video": "533",
                        "paper_award": "",
                        "image_caption": "Conference  planning  usage  scenario  with  Fuzzy  Spreadsheet  encodings.  The  user  selects  as  a  reference  cell  (A)  and  controls  thevisualization using the side panel. In the what-if analysis mode, they reduce the value of Catering 2021 to zero (B) and analyze its effect onthe focus cell (C). More detailed information for the focus cell is shown in the side panel (right)",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "https://youtu.be/HEBsxTh2pg4",
                        "ff_id": "HEBsxTh2pg4"
                    },
                    {
                        "slot_id": "v-tvcg-9566799-qa",
                        "session_id": "full7",
                        "type": "In Person Q+A",
                        "title": "Fuzzy Spreadsheet: Understanding and Exploring Uncertainties in Tabular Calculations (Q+A)",
                        "contributors": [
                            "Vaishali Dhanoa"
                        ],
                        "authors": [],
                        "abstract": "Spreadsheet-based tools provide a simple yet effective way of calculating values, which makes them the number-one choice for building and formalizing simple models for budget planning and many other applications. A cell in a spreadsheet holds one specific value and gives a discrete, over precise view of the underlying model. Therefore, spreadsheets are of limited use when investigating the inherent uncertainties of such models and answering what-if questions. Existing extensions typically require a complex modeling process that cannot easily be embedded in a tabular layout. In Fuzzy Spreadsheet, a cell can hold and display a distribution of values. This integrated uncertainty-handling immediately conveys sensitivity and robustness information. The fuzzification of the cells enables calculations not only with precise values but also with distributions, and probabilities. We conservatively added and carefully crafted visuals to maintain the look and feel of a traditional spreadsheet while facilitating what-if analyses. Given a user-specified reference cell, Fuzzy Spreadsheet automatically extracts and visualizes contextually relevant information, such as impact, uncertainty, and degree of neighborhood, for the selected and related cells. To evaluate its usability and the perceived mental effort required, we conducted a user study. The results show that our approach outperforms traditional spreadsheets in terms of answer correctness, response time, and perceived mental effort in almost all tasks tested.",
                        "uid": "v-tvcg-9566799",
                        "file_name": "",
                        "time_stamp": "2022-10-19T21:43:00Z",
                        "time_start": "2022-10-19T21:43:00Z",
                        "time_end": "2022-10-19T21:45:00Z",
                        "paper_type": "full",
                        "keywords": [
                            "Uncertainty visualization, tabular data, spreadsheet augmentation"
                        ],
                        "has_image": "1",
                        "has_video": "533",
                        "paper_award": "",
                        "image_caption": "Conference  planning  usage  scenario  with  Fuzzy  Spreadsheet  encodings.  The  user  selects  as  a  reference  cell  (A)  and  controls  thevisualization using the side panel. In the what-if analysis mode, they reduce the value of Catering 2021 to zero (B) and analyze its effect onthe focus cell (C). More detailed information for the focus cell is shown in the side panel (right)",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "https://youtu.be/HEBsxTh2pg4",
                        "ff_id": "HEBsxTh2pg4"
                    }
                ]
            },
            {
                "title": "Understanding and Modeling How People Respond to Visualizations",
                "session_id": "full8",
                "event_prefix": "v-full",
                "track": "ok6",
                "livestream_id": "ok6-wed",
                "session_image": "full8.png",
                "chair": [
                    "Carolina Nobre"
                ],
                "organizers": [],
                "time_start": "2022-10-19T15:45:00Z",
                "time_end": "2022-10-19T17:00:00Z",
                "discord_category": "",
                "discord_channel": "oklahoma-st-6",
                "discord_channel_id": "1024600906689421372",
                "discord_link": "https://discord.com/channels/978320435554947082/1024600906689421372",
                "slido_link": "https://app.sli.do/event/i28UETZaiRWr1XwX6eKjcf?section=29ba1f79-04cd-4568-a83c-ab81aa3c28b8",
                "youtube_url": "https://youtu.be/L523gBLIM5c",
                "youtube_id": "L523gBLIM5c",
                "zoom_meeting": "",
                "zoom_password": "",
                "zoom_link": "",
                "ff_link": "https://youtu.be/O_AKLOawNdQ",
                "ff_playlist": "",
                "time_slots": [
                    {
                        "slot_id": "full8-opening",
                        "session_id": "full8",
                        "type": "In Person Other",
                        "title": "Session Opening",
                        "contributors": [
                            "Carolina Nobre"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "",
                        "file_name": "",
                        "time_stamp": "2022-10-19T15:45:00Z",
                        "time_start": "2022-10-19T15:45:00Z",
                        "time_end": "2022-10-19T15:45:00Z",
                        "paper_type": "",
                        "keywords": [],
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "v-tvcg-9492011-pres",
                        "session_id": "full8",
                        "type": "In Person Presentation",
                        "title": "A Survey of Perception-Based Visualization Studies by Task",
                        "contributors": [
                            "Ghulam Jilani Quadri"
                        ],
                        "authors": [
                            "Ghulam Jilani Quadri",
                            "Paul Rosen"
                        ],
                        "abstract": "Knowledge of human perception has long been incorporated into visualizations to enhance their quality and effectiveness. The last decade, in particular, has shown an increase in perception-based visualization research studies. With all of this recent progress, the visualization community lacks a comprehensive guide to contextualize their results. In this report, we provide a systematic and comprehensive review of research studies on perception related to visualization. This survey reviews perception-focused visualization studies since 1980 and summarizes their research developments focusing on low-level tasks, further breaking techniques down by visual encoding and visualization type. In particular, we focus on how perception is used to evaluate the effectiveness of visualizations, to help readers understand and apply the principles of perception of their visualization designs through a task-optimized approach. We concluded our report with a summary of the weaknesses and open research questions in the area.",
                        "uid": "v-tvcg-9492011",
                        "file_name": "v-tvcg-9492011_Quadri_Presentation.mp4",
                        "time_stamp": "2022-10-19T15:45:00Z",
                        "time_start": "2022-10-19T15:45:00Z",
                        "time_end": "2022-10-19T15:55:00Z",
                        "paper_type": "full",
                        "keywords": [
                            "Visualization, perception, graphical perception, visual analytics tasks, evaluation, survey."
                        ],
                        "has_image": "1",
                        "has_video": "530",
                        "paper_award": "",
                        "image_caption": "The image shows a web resource as an interactive list of the papers we have reviewed and identified in our survey. The interactive web resources can be utilized to identify and filter the selected perception-based studies on visualization types, visual encoding, low-level visual tasks, authors, and venue. Please check our paper for more detail.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "https://youtu.be/HPY28tCwZK4",
                        "ff_id": "HPY28tCwZK4"
                    },
                    {
                        "slot_id": "v-tvcg-9492011-qa",
                        "session_id": "full8",
                        "type": "In Person Q+A",
                        "title": "A Survey of Perception-Based Visualization Studies by Task (Q+A)",
                        "contributors": [
                            "Ghulam Jilani Quadri"
                        ],
                        "authors": [],
                        "abstract": "Knowledge of human perception has long been incorporated into visualizations to enhance their quality and effectiveness. The last decade, in particular, has shown an increase in perception-based visualization research studies. With all of this recent progress, the visualization community lacks a comprehensive guide to contextualize their results. In this report, we provide a systematic and comprehensive review of research studies on perception related to visualization. This survey reviews perception-focused visualization studies since 1980 and summarizes their research developments focusing on low-level tasks, further breaking techniques down by visual encoding and visualization type. In particular, we focus on how perception is used to evaluate the effectiveness of visualizations, to help readers understand and apply the principles of perception of their visualization designs through a task-optimized approach. We concluded our report with a summary of the weaknesses and open research questions in the area.",
                        "uid": "v-tvcg-9492011",
                        "file_name": "",
                        "time_stamp": "2022-10-19T15:55:00Z",
                        "time_start": "2022-10-19T15:55:00Z",
                        "time_end": "2022-10-19T15:57:00Z",
                        "paper_type": "full",
                        "keywords": [
                            "Visualization, perception, graphical perception, visual analytics tasks, evaluation, survey."
                        ],
                        "has_image": "1",
                        "has_video": "530",
                        "paper_award": "",
                        "image_caption": "The image shows a web resource as an interactive list of the papers we have reviewed and identified in our survey. The interactive web resources can be utilized to identify and filter the selected perception-based studies on visualization types, visual encoding, low-level visual tasks, authors, and venue. Please check our paper for more detail.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "https://youtu.be/HPY28tCwZK4",
                        "ff_id": "HPY28tCwZK4"
                    },
                    {
                        "slot_id": "v-full-1219-pres",
                        "session_id": "full8",
                        "type": "In Person Presentation",
                        "title": "BeauVis: A Validated Scale for Measuring the Aesthetic Pleasure of Visual Representations",
                        "contributors": [
                            "Tingying He"
                        ],
                        "authors": [
                            "Tingying He",
                            "Petra Isenberg",
                            "Raimund Dachselt",
                            "Tobias Isenberg"
                        ],
                        "abstract": "We developed and validated a rating scale to assess the aesthetic pleasure (or beauty) of a visual data representation: the BeauVis scale. With our work we offer researchers and practitioners a simple instrument to compare the visual appearance of different visualizations, unrelated to data or context of use. Our rating scale can, for example, be used to accompany results from controlled experiments or be used as informative data points during in-depth qualitative studies. Given the lack of an aesthetic pleasure scale dedicated to visualizations, researchers have mostly chosen their own terms to study or compare the aesthetic pleasure of visualizations. Yet, many terms are possible and currently no clear guidance on their effectiveness regarding the judgment of aesthetic pleasure exists. To solve this problem, we engaged in a multi-step research process to develop the first validated rating scale specifically for judging the aesthetic pleasure of a visualization (osf.io/fxs76). Our final BeauVis scale consists of five items, \u201cenjoyable,\u201d \u201clikable,\u201d \u201cpleasing,\u201d \u201cnice,\u201d and \u201cappealing.\u201d Beyond this scale itself, we contribute (a) a systematic review of the terms used in past research to capture aesthetics, (b) an investigation with visualization experts who suggested terms to use for judging the aesthetic pleasure of a visualization, and (c) a confirmatory survey in which we used our terms to study the aesthetic pleasure of a set of 3 visualizations.",
                        "uid": "v-full-1219",
                        "file_name": "v-full-1219_He_Presentation.mp4",
                        "time_stamp": "2022-10-19T15:57:00Z",
                        "time_start": "2022-10-19T15:57:00Z",
                        "time_end": "2022-10-19T16:07:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "530",
                        "paper_award": "",
                        "image_caption": "One participant\u2019s data on our BeauVis scale in its recommended version.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "https://youtu.be/QjHI0eHLhRU",
                        "ff_id": "QjHI0eHLhRU"
                    },
                    {
                        "slot_id": "v-full-1219-qa",
                        "session_id": "full8",
                        "type": "In Person Q+A",
                        "title": "BeauVis: A Validated Scale for Measuring the Aesthetic Pleasure of Visual Representations (Q+A)",
                        "contributors": [
                            "Tingying He"
                        ],
                        "authors": [],
                        "abstract": "We developed and validated a rating scale to assess the aesthetic pleasure (or beauty) of a visual data representation: the BeauVis scale. With our work we offer researchers and practitioners a simple instrument to compare the visual appearance of different visualizations, unrelated to data or context of use. Our rating scale can, for example, be used to accompany results from controlled experiments or be used as informative data points during in-depth qualitative studies. Given the lack of an aesthetic pleasure scale dedicated to visualizations, researchers have mostly chosen their own terms to study or compare the aesthetic pleasure of visualizations. Yet, many terms are possible and currently no clear guidance on their effectiveness regarding the judgment of aesthetic pleasure exists. To solve this problem, we engaged in a multi-step research process to develop the first validated rating scale specifically for judging the aesthetic pleasure of a visualization (osf.io/fxs76). Our final BeauVis scale consists of five items, \u201cenjoyable,\u201d \u201clikable,\u201d \u201cpleasing,\u201d \u201cnice,\u201d and \u201cappealing.\u201d Beyond this scale itself, we contribute (a) a systematic review of the terms used in past research to capture aesthetics, (b) an investigation with visualization experts who suggested terms to use for judging the aesthetic pleasure of a visualization, and (c) a confirmatory survey in which we used our terms to study the aesthetic pleasure of a set of 3 visualizations.",
                        "uid": "v-full-1219",
                        "file_name": "",
                        "time_stamp": "2022-10-19T16:07:00Z",
                        "time_start": "2022-10-19T16:07:00Z",
                        "time_end": "2022-10-19T16:09:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "530",
                        "paper_award": "",
                        "image_caption": "One participant\u2019s data on our BeauVis scale in its recommended version.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "https://youtu.be/QjHI0eHLhRU",
                        "ff_id": "QjHI0eHLhRU"
                    },
                    {
                        "slot_id": "v-full-1217-pres",
                        "session_id": "full8",
                        "type": "In Person Presentation",
                        "title": "Photosensitive Accessibility for Interactive Data Visualizations",
                        "contributors": [
                            "Laura South"
                        ],
                        "authors": [
                            "Laura South",
                            "Michelle A. Borkin"
                        ],
                        "abstract": "Accessibility guidelines place restrictions on the use of animations and interactivity on webpages to lessen the likelihood of webpages inadvertently producing sequences with flashes, patterns, or color changes that may trigger seizures for individuals with photosensitive epilepsy. Online data visualizations often incorporate elements of animation and interactivity to create a narrative, engage users, or encourage exploration. These design guidelines have been empirically validated by perceptual studies in visualization literature, but the impact of animation and interaction in visualizations on users with photosensitivity, who may experience seizures in response to certain visual stimuli, has not been considered. We systematically gathered and tested 1,132 interactive and animated visualizations for seizure-inducing risk using established methods and found that currently available methods for determining photosensitive risk are not reliable when evaluating interactive visualizations, as risk scores varied significantly based on the individual interacting with the visualization. To address this issue, we introduce a theoretical model defining the degree of control visualization designers have over three determinants of photosensitive risk in potentially seizure-inducing sequences: the size, frequency, and color of flashing content. Using an analysis of 375 visualizations hosted on bl.ocks.org, we created a theoretical model of photosensitive risk in visualizations by arranging the photosensitive risk determinants according to the degree of control visualization authors have over whether content exceeds photosensitive accessibility thresholds. We then use this model to propose a new method of testing for photosensitive risk that focuses on elements of visualizations that are subject to greater authorial control \u2013 and are therefore more robust to variations in the individual user \u2013 producing more reliable risk assessments than existing methods when applied to interactive visualizations. A full copy of this paper and all study materials are available at https://osf.io/8kzmg/.",
                        "uid": "v-full-1217",
                        "file_name": "v-full-1217_South_Presentation.mp4",
                        "time_stamp": "2022-10-19T16:09:00Z",
                        "time_start": "2022-10-19T16:09:00Z",
                        "time_end": "2022-10-19T16:19:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "565",
                        "paper_award": "",
                        "image_caption": "Three visualizations with navigation (A), filtering (B), and selection (C) interaction mechanisms from our dataset of 375 online D3 visualizations annotated for photosensitive accessibility. Each of the three visualizations in this figure are inaccessible because they are capable of producing flickering sequences that could induce seizures when viewed by people with photosensitive epilepsy.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "https://youtu.be/O9PXUJM2ocQ",
                        "ff_id": "O9PXUJM2ocQ"
                    },
                    {
                        "slot_id": "v-full-1217-qa",
                        "session_id": "full8",
                        "type": "In Person Q+A",
                        "title": "Photosensitive Accessibility for Interactive Data Visualizations (Q+A)",
                        "contributors": [
                            "Laura South"
                        ],
                        "authors": [],
                        "abstract": "Accessibility guidelines place restrictions on the use of animations and interactivity on webpages to lessen the likelihood of webpages inadvertently producing sequences with flashes, patterns, or color changes that may trigger seizures for individuals with photosensitive epilepsy. Online data visualizations often incorporate elements of animation and interactivity to create a narrative, engage users, or encourage exploration. These design guidelines have been empirically validated by perceptual studies in visualization literature, but the impact of animation and interaction in visualizations on users with photosensitivity, who may experience seizures in response to certain visual stimuli, has not been considered. We systematically gathered and tested 1,132 interactive and animated visualizations for seizure-inducing risk using established methods and found that currently available methods for determining photosensitive risk are not reliable when evaluating interactive visualizations, as risk scores varied significantly based on the individual interacting with the visualization. To address this issue, we introduce a theoretical model defining the degree of control visualization designers have over three determinants of photosensitive risk in potentially seizure-inducing sequences: the size, frequency, and color of flashing content. Using an analysis of 375 visualizations hosted on bl.ocks.org, we created a theoretical model of photosensitive risk in visualizations by arranging the photosensitive risk determinants according to the degree of control visualization authors have over whether content exceeds photosensitive accessibility thresholds. We then use this model to propose a new method of testing for photosensitive risk that focuses on elements of visualizations that are subject to greater authorial control \u2013 and are therefore more robust to variations in the individual user \u2013 producing more reliable risk assessments than existing methods when applied to interactive visualizations. A full copy of this paper and all study materials are available at https://osf.io/8kzmg/.",
                        "uid": "v-full-1217",
                        "file_name": "",
                        "time_stamp": "2022-10-19T16:19:00Z",
                        "time_start": "2022-10-19T16:19:00Z",
                        "time_end": "2022-10-19T16:21:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "565",
                        "paper_award": "",
                        "image_caption": "Three visualizations with navigation (A), filtering (B), and selection (C) interaction mechanisms from our dataset of 375 online D3 visualizations annotated for photosensitive accessibility. Each of the three visualizations in this figure are inaccessible because they are capable of producing flickering sequences that could induce seizures when viewed by people with photosensitive epilepsy.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "https://youtu.be/O9PXUJM2ocQ",
                        "ff_id": "O9PXUJM2ocQ"
                    },
                    {
                        "slot_id": "v-full-1115-pres",
                        "session_id": "full8",
                        "type": "In Person Presentation",
                        "title": "Unifying Effects of Direct and Relational Associations for Visual Communication",
                        "contributors": [
                            "Melissa A. Schoenlein"
                        ],
                        "authors": [
                            "Melissa A. Schoenlein",
                            "Johnny Campos",
                            "Kevin Lande",
                            "Laurent Lessard",
                            "Karen Schloss"
                        ],
                        "abstract": "People have expectations about how colors map to concepts in visualizations, and they are better at interpreting visualizations that match their expectations. Traditionally, studies on these expectations (inferred mappings) distinguished distinct factors relevant for visualizations of categorical vs. continuous information. Studies on categorical information focused on direct associations (e.g., mangos are associated with yellows) whereas studies on continuous information focused on relational associations (e.g., darker colors map to larger quantities; dark-is-more bias). We unite these two areas within a single framework of assignment inference. Assignment inference is the process by which people infer mappings between perceptual features and concepts represented in encoding systems. Observers infer globally optimal assignments by maximizing the \u201cmerit,\u201d or \u201cgoodness,\u201d of each possible assignment. Previous work on assignment inference focused on visualizations of categorical information. We extend this approach to visualizations of continuous data by (a) broadening the notion of merit to include relational associations and (b) developing a method for combining multiple (sometimes conflicting) sources of merit to predict people\u2019s inferred mappings. We developed and tested our model on data from experiments in which participants interpreted colormap data visualizations, representing fictitious data about environmental concepts (sunshine, shade, wild fire, ocean water, glacial ice). We found both direct and relational associations contribute independently to inferred mappings. These results can be used to optimize visualization design to facilitate visual communication.",
                        "uid": "v-full-1115",
                        "file_name": "v-full-1115_Schoenlein_Presentation.mp4",
                        "time_stamp": "2022-10-19T16:21:00Z",
                        "time_start": "2022-10-19T16:21:00Z",
                        "time_end": "2022-10-19T16:31:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "654",
                        "paper_award": "",
                        "image_caption": "In this study, participants inferred which region of colormaps (left/right) represented more of a domain concept (e.g., ocean water). Inferences can be predicted by simulating assignment inference using a weighted combination of multiple (sometimes competing) sources of \u201cmerit\u201d: direct associations and relational associations (dark-is-more bias). This study bridges our understanding how direct and relational associations combine to influence inferences about the meanings of colors in visualizations. Our findings can be translated directly to design visualizations that align with people\u2019s expectations about the meanings of colors, thereby facilitating visual communication.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "https://youtu.be/U-YfpqfbUfg",
                        "ff_id": "U-YfpqfbUfg"
                    },
                    {
                        "slot_id": "v-full-1115-qa",
                        "session_id": "full8",
                        "type": "In Person Q+A",
                        "title": "Unifying Effects of Direct and Relational Associations for Visual Communication (Q+A)",
                        "contributors": [
                            "Melissa A. Schoenlein"
                        ],
                        "authors": [],
                        "abstract": "People have expectations about how colors map to concepts in visualizations, and they are better at interpreting visualizations that match their expectations. Traditionally, studies on these expectations (inferred mappings) distinguished distinct factors relevant for visualizations of categorical vs. continuous information. Studies on categorical information focused on direct associations (e.g., mangos are associated with yellows) whereas studies on continuous information focused on relational associations (e.g., darker colors map to larger quantities; dark-is-more bias). We unite these two areas within a single framework of assignment inference. Assignment inference is the process by which people infer mappings between perceptual features and concepts represented in encoding systems. Observers infer globally optimal assignments by maximizing the \u201cmerit,\u201d or \u201cgoodness,\u201d of each possible assignment. Previous work on assignment inference focused on visualizations of categorical information. We extend this approach to visualizations of continuous data by (a) broadening the notion of merit to include relational associations and (b) developing a method for combining multiple (sometimes conflicting) sources of merit to predict people\u2019s inferred mappings. We developed and tested our model on data from experiments in which participants interpreted colormap data visualizations, representing fictitious data about environmental concepts (sunshine, shade, wild fire, ocean water, glacial ice). We found both direct and relational associations contribute independently to inferred mappings. These results can be used to optimize visualization design to facilitate visual communication.",
                        "uid": "v-full-1115",
                        "file_name": "",
                        "time_stamp": "2022-10-19T16:31:00Z",
                        "time_start": "2022-10-19T16:31:00Z",
                        "time_end": "2022-10-19T16:33:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "654",
                        "paper_award": "",
                        "image_caption": "In this study, participants inferred which region of colormaps (left/right) represented more of a domain concept (e.g., ocean water). Inferences can be predicted by simulating assignment inference using a weighted combination of multiple (sometimes competing) sources of \u201cmerit\u201d: direct associations and relational associations (dark-is-more bias). This study bridges our understanding how direct and relational associations combine to influence inferences about the meanings of colors in visualizations. Our findings can be translated directly to design visualizations that align with people\u2019s expectations about the meanings of colors, thereby facilitating visual communication.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "https://youtu.be/U-YfpqfbUfg",
                        "ff_id": "U-YfpqfbUfg"
                    },
                    {
                        "slot_id": "v-full-1448-pres",
                        "session_id": "full8",
                        "type": "In Person Presentation",
                        "title": "A Scanner Deeply: Predicting Gaze Heatmaps on Visualizations Using Crowdsourced Eye Movement Data",
                        "contributors": [
                            "Sungbok Shin"
                        ],
                        "authors": [
                            "Sungbok Shin",
                            "Sunghyo Chung",
                            "Sanghyun Hong",
                            "Niklas Elmqvist"
                        ],
                        "abstract": "Visual perception is a key component of data visualization. Much prior empirical work uses eye movement as a proxy to understand human visual perception. Diverse apparatus and techniques have been proposed to collect eye movements, but there is still no optimal approach. In this paper, we review 30 prior works for collecting eye movements based on three axes: (1) the tracker technology used to measure eye movements; (2) the image stimulus shown to participants; and (3) the collection methodology used to gather the data. Based on this taxonomy, we employ a webcam-based eyetracking approach using task-specific visualizations as the stimulus. The low technology requirement means that virtually anyone can participate, thus enabling us to collect data at large scale using crowdsourcing: approximately 12,000 samples in total. Choosing visualization images as stimulus means that the eye movements will be specific to perceptual tasks associated with visualization. We use these data to propose a SCANNER DEEPLY, a virtual eyetracker model that, given an image of a visualization, generates a gaze heatmap for that image. We employ a computationally efficient, yet powerful convolutional neural network for our model. We compare the results of our work with results from the DVS model and a neural network trained on the Salicon dataset. The analysis of our gaze patterns enables us to understand how users grasp the structure of visualized data. We also make our stimulus dataset of visualization images available as part of this paper\u2019s contribution.",
                        "uid": "v-full-1448",
                        "file_name": "v-full-1448_Shin_Presentation.mp4",
                        "time_stamp": "2022-10-19T16:33:00Z",
                        "time_start": "2022-10-19T16:33:00Z",
                        "time_end": "2022-10-19T16:43:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "501",
                        "paper_award": "",
                        "image_caption": "Sungbok Shin is a Ph.D. Student at the dept. of Computer in the University of Maryland, College Park, United States. He gained his B.S. as a dual degree of Computer Science and Engineering and Mathematics and his M.Eng. in Computer Science and Engineering in Korea University, Seoul, South Korea. Sungbok is interested in understanding how existing machine learning models (e.g., neural networks) learn humans' perceptions while looking at visualizations. He is also interested in developing applications that would facilitate and can improve chart designs.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "https://youtu.be/kZyUfaertD8",
                        "ff_id": "kZyUfaertD8"
                    },
                    {
                        "slot_id": "v-full-1448-qa",
                        "session_id": "full8",
                        "type": "In Person Q+A",
                        "title": "A Scanner Deeply: Predicting Gaze Heatmaps on Visualizations Using Crowdsourced Eye Movement Data (Q+A)",
                        "contributors": [
                            "Sungbok Shin"
                        ],
                        "authors": [],
                        "abstract": "Visual perception is a key component of data visualization. Much prior empirical work uses eye movement as a proxy to understand human visual perception. Diverse apparatus and techniques have been proposed to collect eye movements, but there is still no optimal approach. In this paper, we review 30 prior works for collecting eye movements based on three axes: (1) the tracker technology used to measure eye movements; (2) the image stimulus shown to participants; and (3) the collection methodology used to gather the data. Based on this taxonomy, we employ a webcam-based eyetracking approach using task-specific visualizations as the stimulus. The low technology requirement means that virtually anyone can participate, thus enabling us to collect data at large scale using crowdsourcing: approximately 12,000 samples in total. Choosing visualization images as stimulus means that the eye movements will be specific to perceptual tasks associated with visualization. We use these data to propose a SCANNER DEEPLY, a virtual eyetracker model that, given an image of a visualization, generates a gaze heatmap for that image. We employ a computationally efficient, yet powerful convolutional neural network for our model. We compare the results of our work with results from the DVS model and a neural network trained on the Salicon dataset. The analysis of our gaze patterns enables us to understand how users grasp the structure of visualized data. We also make our stimulus dataset of visualization images available as part of this paper\u2019s contribution.",
                        "uid": "v-full-1448",
                        "file_name": "",
                        "time_stamp": "2022-10-19T16:43:00Z",
                        "time_start": "2022-10-19T16:43:00Z",
                        "time_end": "2022-10-19T16:45:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "501",
                        "paper_award": "",
                        "image_caption": "Sungbok Shin is a Ph.D. Student at the dept. of Computer in the University of Maryland, College Park, United States. He gained his B.S. as a dual degree of Computer Science and Engineering and Mathematics and his M.Eng. in Computer Science and Engineering in Korea University, Seoul, South Korea. Sungbok is interested in understanding how existing machine learning models (e.g., neural networks) learn humans' perceptions while looking at visualizations. He is also interested in developing applications that would facilitate and can improve chart designs.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "https://youtu.be/kZyUfaertD8",
                        "ff_id": "kZyUfaertD8"
                    },
                    {
                        "slot_id": "v-full-1254-pres",
                        "session_id": "full8",
                        "type": "In Person Presentation",
                        "title": "Studying Early Decision Making with Progressive Bar Charts",
                        "contributors": [
                            "Ameya B Patil"
                        ],
                        "authors": [
                            "Ameya B Patil",
                            "Ga\u00eblle Richer",
                            "Dominik Moritz",
                            "Christopher Jermaine",
                            "Jean-Daniel Fekete"
                        ],
                        "abstract": "We conduct a user study to quantify and compare user performance for a value comparison task using four bar chart designs, where the bars show the mean values of data loaded progressively and updated every second (progressive bar charts). Progressive visualization divides different stages of the visualization pipeline\u2014data loading, processing, and visualization\u2014into iterative animated steps to limit the latency when loading large amounts of data. An animated visualization appearing quickly, unfolding, and getting more accurate with time, enables users to make early decisions. However, intermediate mean estimates are computed only on partial data and may not have time to converge to the true means, potentially misleading users and resulting in incorrect decisions. To address this issue, we propose two new designs visualizing the history of values in progressive bar charts, in addition to the use of confidence intervals. We comparatively study four progressive bar chart designs: with/without confidence intervals, and using near-history representation with/without confidence intervals, on three realistic data distributions. We evaluate user performance based on the percentage of correct answers (accuracy), response time, and user confidence. Our results show that, overall, users can make early and accurate decisions with 92% accuracy using only 18% of the data, regardless of the design. We find that our proposed bar chart design with only near-history is comparable to bar charts with only confidence intervals in performance, and the qualitative feedback we received indicates a preference for designs with history.",
                        "uid": "v-full-1254",
                        "file_name": "v-full-1254_Patil_Presentation.mp4",
                        "time_stamp": "2022-10-19T16:45:00Z",
                        "time_start": "2022-10-19T16:45:00Z",
                        "time_end": "2022-10-19T16:55:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "447",
                        "paper_award": "",
                        "image_caption": "Four progressive bar chart designs showing the means for four columns of a data table loaded progressively, updated every second: (BASELINE) baseline bar chart, (CI) bar chart with confidence intervals (95%), (HISTORY) bar chart with near-history, and (HISTORY+CI) bar chart with near-history and confidence intervals. All the bar charts represent the same data at one step of the progression. For our proposed new designs (HISTORY & HISTORY+CI), opacity and position along X-axis encode the recency of updates; opaque bars on the right represent more recent updates, and transparent bars on the left represent older updates.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "https://youtu.be/ygpu92JMhA0",
                        "ff_id": "ygpu92JMhA0"
                    },
                    {
                        "slot_id": "v-full-1254-qa",
                        "session_id": "full8",
                        "type": "In Person Q+A",
                        "title": "Studying Early Decision Making with Progressive Bar Charts (Q+A)",
                        "contributors": [
                            "Ameya B Patil"
                        ],
                        "authors": [],
                        "abstract": "We conduct a user study to quantify and compare user performance for a value comparison task using four bar chart designs, where the bars show the mean values of data loaded progressively and updated every second (progressive bar charts). Progressive visualization divides different stages of the visualization pipeline\u2014data loading, processing, and visualization\u2014into iterative animated steps to limit the latency when loading large amounts of data. An animated visualization appearing quickly, unfolding, and getting more accurate with time, enables users to make early decisions. However, intermediate mean estimates are computed only on partial data and may not have time to converge to the true means, potentially misleading users and resulting in incorrect decisions. To address this issue, we propose two new designs visualizing the history of values in progressive bar charts, in addition to the use of confidence intervals. We comparatively study four progressive bar chart designs: with/without confidence intervals, and using near-history representation with/without confidence intervals, on three realistic data distributions. We evaluate user performance based on the percentage of correct answers (accuracy), response time, and user confidence. Our results show that, overall, users can make early and accurate decisions with 92% accuracy using only 18% of the data, regardless of the design. We find that our proposed bar chart design with only near-history is comparable to bar charts with only confidence intervals in performance, and the qualitative feedback we received indicates a preference for designs with history.",
                        "uid": "v-full-1254",
                        "file_name": "",
                        "time_stamp": "2022-10-19T16:55:00Z",
                        "time_start": "2022-10-19T16:55:00Z",
                        "time_end": "2022-10-19T16:57:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "447",
                        "paper_award": "",
                        "image_caption": "Four progressive bar chart designs showing the means for four columns of a data table loaded progressively, updated every second: (BASELINE) baseline bar chart, (CI) bar chart with confidence intervals (95%), (HISTORY) bar chart with near-history, and (HISTORY+CI) bar chart with near-history and confidence intervals. All the bar charts represent the same data at one step of the progression. For our proposed new designs (HISTORY & HISTORY+CI), opacity and position along X-axis encode the recency of updates; opaque bars on the right represent more recent updates, and transparent bars on the left represent older updates.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "https://youtu.be/ygpu92JMhA0",
                        "ff_id": "ygpu92JMhA0"
                    }
                ]
            },
            {
                "title": "Interpreting Machine Learning",
                "session_id": "full9",
                "event_prefix": "v-full",
                "track": "ok4",
                "livestream_id": "ok4-thu",
                "session_image": "full9.png",
                "chair": [
                    "Adam Perer"
                ],
                "organizers": [],
                "time_start": "2022-10-20T15:45:00Z",
                "time_end": "2022-10-20T17:00:00Z",
                "discord_category": "",
                "discord_channel": "oklahoma-st-4",
                "discord_channel_id": "1024600674924765264",
                "discord_link": "https://discord.com/channels/978320435554947082/1024600674924765264",
                "slido_link": "https://app.sli.do/event/i28UETZaiRWr1XwX6eKjcf?section=51658b34-0f5b-40c9-a335-1fd1468fad8d",
                "youtube_url": "https://youtu.be/0qY_NxLSGBk",
                "youtube_id": "0qY_NxLSGBk",
                "zoom_meeting": "",
                "zoom_password": "",
                "zoom_link": "",
                "ff_link": "https://youtu.be/9oWGVewOZeM",
                "ff_playlist": "",
                "time_slots": [
                    {
                        "slot_id": "full9-opening",
                        "session_id": "full9",
                        "type": "In Person Other",
                        "title": "Session Opening",
                        "contributors": [
                            "Adam Perer"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "",
                        "file_name": "",
                        "time_stamp": "2022-10-20T15:45:00Z",
                        "time_start": "2022-10-20T15:45:00Z",
                        "time_end": "2022-10-20T15:45:00Z",
                        "paper_type": "",
                        "keywords": [],
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "v-tvcg-9695246-pres",
                        "session_id": "full9",
                        "type": "In Person Presentation",
                        "title": "StrategyAtlas: Strategy Analysis for Machine Learning Interpretability",
                        "contributors": [
                            "Collaris, Dennis"
                        ],
                        "authors": [
                            "Dennis Collaris",
                            "Jarke J. van Wijk"
                        ],
                        "abstract": "Businesses in high-risk environments have been reluctant to adopt modern machine learning approaches due to their complex and uninterpretable nature. Most current solutions provide local, instance-level explanations, but this is insufficient for understanding the model as a whole. In this work, we show that strategy clusters (i.e., groups of data instances that are treated distinctly by the model) can be used to understand the global behavior of a complex ML model. To support effective exploration and understanding of these clusters, we introduce StrategyAtlas, a system designed to analyze and explain model strategies. Furthermore, it supports multiple ways to utilize these strategies for simplifying and improving the reference model. In collaboration with a large insurance company, we present a use case in automatic insurance acceptance, and show how professional data scientists were enabled to understand a complex model and improve the production model based on these insights.",
                        "uid": "v-tvcg-9695246",
                        "file_name": "v-tvcg-9695246_Collaris_Presentation.mp4",
                        "time_stamp": "2022-10-20T15:45:00Z",
                        "time_start": "2022-10-20T15:45:00Z",
                        "time_end": "2022-10-20T15:55:00Z",
                        "paper_type": "full",
                        "keywords": [
                            "Machine learning, Visual analytics, Explainable AI"
                        ],
                        "has_image": "1",
                        "has_video": "640",
                        "paper_award": "",
                        "image_caption": "StrategyAtlas: a visual analytics system designed to identify and interpret different model strategies, to help data scientists understand complex machine learning models.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "https://youtu.be/5tPk2jh3fgM",
                        "ff_id": "5tPk2jh3fgM"
                    },
                    {
                        "slot_id": "v-tvcg-9695246-qa",
                        "session_id": "full9",
                        "type": "In Person Q+A",
                        "title": "StrategyAtlas: Strategy Analysis for Machine Learning Interpretability (Q+A)",
                        "contributors": [
                            "Collaris, Dennis"
                        ],
                        "authors": [],
                        "abstract": "Businesses in high-risk environments have been reluctant to adopt modern machine learning approaches due to their complex and uninterpretable nature. Most current solutions provide local, instance-level explanations, but this is insufficient for understanding the model as a whole. In this work, we show that strategy clusters (i.e., groups of data instances that are treated distinctly by the model) can be used to understand the global behavior of a complex ML model. To support effective exploration and understanding of these clusters, we introduce StrategyAtlas, a system designed to analyze and explain model strategies. Furthermore, it supports multiple ways to utilize these strategies for simplifying and improving the reference model. In collaboration with a large insurance company, we present a use case in automatic insurance acceptance, and show how professional data scientists were enabled to understand a complex model and improve the production model based on these insights.",
                        "uid": "v-tvcg-9695246",
                        "file_name": "",
                        "time_stamp": "2022-10-20T15:55:00Z",
                        "time_start": "2022-10-20T15:55:00Z",
                        "time_end": "2022-10-20T15:57:00Z",
                        "paper_type": "full",
                        "keywords": [
                            "Machine learning, Visual analytics, Explainable AI"
                        ],
                        "has_image": "1",
                        "has_video": "640",
                        "paper_award": "",
                        "image_caption": "StrategyAtlas: a visual analytics system designed to identify and interpret different model strategies, to help data scientists understand complex machine learning models.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "https://youtu.be/5tPk2jh3fgM",
                        "ff_id": "5tPk2jh3fgM"
                    },
                    {
                        "slot_id": "v-full-1696-pres",
                        "session_id": "full9",
                        "type": "In Person Presentation",
                        "title": "VDL-Surrogate: A View-Dependent Latent-based Model for Parameter Space Exploration of Ensemble Simulations",
                        "contributors": [
                            "Neng Shi"
                        ],
                        "authors": [
                            "Neng Shi",
                            "Jiayi Xu",
                            "Haoyu Li",
                            "Hanqi Guo",
                            "Jonathan Woodring",
                            "Han-Wei Shen"
                        ],
                        "abstract": "We propose VDL-Surrogate, a view-dependent neural-network-latent-based surrogate model for parameter space exploration of ensemble simulations that allows high-resolution visualizations and user-specified visual mappings. Surrogate-enabled parameter space exploration allows domain scientists to preview simulation results without having to run a large number of computationally costly simulations. Limited by computational resources, however, existing surrogate models may not produce previews with sufficient resolution for visualization and analysis. To improve the efficient use of computational resources and support high-resolution exploration, we perform ray casting from different viewpoints to collect samples and produce compact latent representations. This latent encoding process reduces the cost of surrogate model training while maintaining the output quality. In the model training stage, we select viewpoints to cover the whole viewing sphere and train corresponding VDL-Surrogate models for the selected viewpoints. In the model inference stage, we predict the latent representations at previously selected viewpoints and decode the latent representations to data space. For any given viewpoint, we make interpolations over decoded data at selected viewpoints and generate visualizations with user-specified visual mappings. We show the effectiveness and efficiency of VDL-Surrogate in cosmological and ocean simulations with quantitative and qualitative evaluations. Source code is publicly available at \\url{https://github.com/trainsn/VDL-Surrogate}.",
                        "uid": "v-full-1696",
                        "file_name": "v-full-1696_Shi_Presentation.mp4",
                        "time_stamp": "2022-10-20T15:57:00Z",
                        "time_start": "2022-10-20T15:57:00Z",
                        "time_end": "2022-10-20T16:07:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "599",
                        "paper_award": "",
                        "image_caption": "We propose VDL-Surrogate, a view-dependent neural-network-latent-based surrogate model for parameter space exploration of ensemble simulations that allows high-resolution visualizations and user-specified visual mappings. We use view-dependent latent representations to replace the raw data as the input and output of the visualization surrogate to train the surrogate with limited GPU memory. In the inference stage, given a new input simulation parameter, we first predict the latent representation and then decode the representation to data space. The image compares the visualization results generated using VDL-Surrogate with the ground truth images on cosmology (Nyx) and ocean (MPAS-Ocean) simulations.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "https://youtu.be/nP3i5XfMmCQ",
                        "ff_id": "nP3i5XfMmCQ"
                    },
                    {
                        "slot_id": "v-full-1696-qa",
                        "session_id": "full9",
                        "type": "In Person Q+A",
                        "title": "VDL-Surrogate: A View-Dependent Latent-based Model for Parameter Space Exploration of Ensemble Simulations (Q+A)",
                        "contributors": [
                            "Neng Shi"
                        ],
                        "authors": [],
                        "abstract": "We propose VDL-Surrogate, a view-dependent neural-network-latent-based surrogate model for parameter space exploration of ensemble simulations that allows high-resolution visualizations and user-specified visual mappings. Surrogate-enabled parameter space exploration allows domain scientists to preview simulation results without having to run a large number of computationally costly simulations. Limited by computational resources, however, existing surrogate models may not produce previews with sufficient resolution for visualization and analysis. To improve the efficient use of computational resources and support high-resolution exploration, we perform ray casting from different viewpoints to collect samples and produce compact latent representations. This latent encoding process reduces the cost of surrogate model training while maintaining the output quality. In the model training stage, we select viewpoints to cover the whole viewing sphere and train corresponding VDL-Surrogate models for the selected viewpoints. In the model inference stage, we predict the latent representations at previously selected viewpoints and decode the latent representations to data space. For any given viewpoint, we make interpolations over decoded data at selected viewpoints and generate visualizations with user-specified visual mappings. We show the effectiveness and efficiency of VDL-Surrogate in cosmological and ocean simulations with quantitative and qualitative evaluations. Source code is publicly available at \\url{https://github.com/trainsn/VDL-Surrogate}.",
                        "uid": "v-full-1696",
                        "file_name": "",
                        "time_stamp": "2022-10-20T16:07:00Z",
                        "time_start": "2022-10-20T16:07:00Z",
                        "time_end": "2022-10-20T16:09:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "599",
                        "paper_award": "",
                        "image_caption": "We propose VDL-Surrogate, a view-dependent neural-network-latent-based surrogate model for parameter space exploration of ensemble simulations that allows high-resolution visualizations and user-specified visual mappings. We use view-dependent latent representations to replace the raw data as the input and output of the visualization surrogate to train the surrogate with limited GPU memory. In the inference stage, given a new input simulation parameter, we first predict the latent representation and then decode the representation to data space. The image compares the visualization results generated using VDL-Surrogate with the ground truth images on cosmology (Nyx) and ocean (MPAS-Ocean) simulations.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "https://youtu.be/nP3i5XfMmCQ",
                        "ff_id": "nP3i5XfMmCQ"
                    },
                    {
                        "slot_id": "v-full-1074-pres",
                        "session_id": "full9",
                        "type": "In Person Presentation",
                        "title": "ConceptExplainer: Interactive Explanation for Deep Neural Networks from a Concept Perspective",
                        "contributors": [
                            "Jinbin Huang"
                        ],
                        "authors": [
                            "Jinbin Huang",
                            "Aditi Mishra",
                            "Bum Chul Kwon",
                            "Chris Bryan"
                        ],
                        "abstract": "Traditional deep learning interpretability methods which are suitable for model users cannot explain network behaviors at the global level and are inflexible at providing fine-grained explanations. As a solution, concept-based explanations are gaining attention due to their human intuitiveness and their flexibility to describe both global and local model behaviors. Concepts are groups of similarly meaningful pixels that express a notion, embedded within the network\u2019s latent space and have commonly been hand-generated, but have recently been discovered by automated approaches. Unfortunately, the magnitude and diversity of discovered concepts makes it difficult to navigate and make sense of the concept space. Visual analytics can serve a valuable role in bridging these gaps by enabling structured navigation and exploration of the concept space to provide concept-based insights of model behavior to users. To this end, we design, develop, and validate ConceptExplainer, a visual analytics system that enables people to interactively probe and explore the concept space to explain model behavior at the instance/class/global level. The system was developed via iterative prototyping to address a number of design challenges that model users face in interpreting the behavior of deep learning models. Via a rigorous user study, we validate how ConceptExplainer supports these challenges. Likewise, we conduct a series of usage scenarios to demonstrate how the system supports the interactive analysis of model behavior across a variety of tasks and explanation granularities, such as identifying concepts that are important to classification, identifying bias in training data, and understanding how concepts can be shared across diverse and seemingly dissimilar classes.",
                        "uid": "v-full-1074",
                        "file_name": "v-full-1074_Huang_Presentation.mp4",
                        "time_stamp": "2022-10-20T16:09:00Z",
                        "time_start": "2022-10-20T16:09:00Z",
                        "time_end": "2022-10-20T16:19:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "667",
                        "paper_award": "",
                        "image_caption": "Traditional deep learning interpretability methods which are suitable for model users cannot explain network behaviors at the global level and are inflexible at providing fine-grained explanations. As a solution, concept-based explanations are gaining attention due to their human intuitiveness and their flexibility to describe both global and local model behaviors. Concepts are groups of similarly meaningful pixels that express a notion, embedded within the network\u2019s latent space and have commonly been hand-generated, but have recently been discovered by automated approaches. Unfortunately, the magnitude and diversity of discovered concepts makes it difficult to navigate and make sense of the concept space. Visual analytics can serve a valuable role in bridging these gaps by enabling structured navigation and exploration of the concept space to provide concept-based insights of model behavior to users. To this end, we design, develop, and validate ConceptExplainer, a visual analytics system that enables people to interactively probe and explore the concept space to explain model behavior at the instance/class/global level. The system was developed via iterative prototyping to address a number of design challenges that model users face in interpreting the behavior of deep learning models. Via a rigorous user study, we validate how ConceptExplainer supports these challenges. Likewise, we conduct a series of usage scenarios to demonstrate how the system supports the interactive analysis of model behavior across a variety of tasks and explanation granularities, such as identifying concepts that are important to classification, identifying bias in training data, and understanding how concepts can be shared across diverse and seemingly dissimilar classes.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "https://youtu.be/gltneexyhYs",
                        "ff_id": "gltneexyhYs"
                    },
                    {
                        "slot_id": "v-full-1074-qa",
                        "session_id": "full9",
                        "type": "In Person Q+A",
                        "title": "ConceptExplainer: Interactive Explanation for Deep Neural Networks from a Concept Perspective (Q+A)",
                        "contributors": [
                            "Jinbin Huang"
                        ],
                        "authors": [],
                        "abstract": "Traditional deep learning interpretability methods which are suitable for model users cannot explain network behaviors at the global level and are inflexible at providing fine-grained explanations. As a solution, concept-based explanations are gaining attention due to their human intuitiveness and their flexibility to describe both global and local model behaviors. Concepts are groups of similarly meaningful pixels that express a notion, embedded within the network\u2019s latent space and have commonly been hand-generated, but have recently been discovered by automated approaches. Unfortunately, the magnitude and diversity of discovered concepts makes it difficult to navigate and make sense of the concept space. Visual analytics can serve a valuable role in bridging these gaps by enabling structured navigation and exploration of the concept space to provide concept-based insights of model behavior to users. To this end, we design, develop, and validate ConceptExplainer, a visual analytics system that enables people to interactively probe and explore the concept space to explain model behavior at the instance/class/global level. The system was developed via iterative prototyping to address a number of design challenges that model users face in interpreting the behavior of deep learning models. Via a rigorous user study, we validate how ConceptExplainer supports these challenges. Likewise, we conduct a series of usage scenarios to demonstrate how the system supports the interactive analysis of model behavior across a variety of tasks and explanation granularities, such as identifying concepts that are important to classification, identifying bias in training data, and understanding how concepts can be shared across diverse and seemingly dissimilar classes.",
                        "uid": "v-full-1074",
                        "file_name": "",
                        "time_stamp": "2022-10-20T16:19:00Z",
                        "time_start": "2022-10-20T16:19:00Z",
                        "time_end": "2022-10-20T16:21:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "667",
                        "paper_award": "",
                        "image_caption": "Traditional deep learning interpretability methods which are suitable for model users cannot explain network behaviors at the global level and are inflexible at providing fine-grained explanations. As a solution, concept-based explanations are gaining attention due to their human intuitiveness and their flexibility to describe both global and local model behaviors. Concepts are groups of similarly meaningful pixels that express a notion, embedded within the network\u2019s latent space and have commonly been hand-generated, but have recently been discovered by automated approaches. Unfortunately, the magnitude and diversity of discovered concepts makes it difficult to navigate and make sense of the concept space. Visual analytics can serve a valuable role in bridging these gaps by enabling structured navigation and exploration of the concept space to provide concept-based insights of model behavior to users. To this end, we design, develop, and validate ConceptExplainer, a visual analytics system that enables people to interactively probe and explore the concept space to explain model behavior at the instance/class/global level. The system was developed via iterative prototyping to address a number of design challenges that model users face in interpreting the behavior of deep learning models. Via a rigorous user study, we validate how ConceptExplainer supports these challenges. Likewise, we conduct a series of usage scenarios to demonstrate how the system supports the interactive analysis of model behavior across a variety of tasks and explanation granularities, such as identifying concepts that are important to classification, identifying bias in training data, and understanding how concepts can be shared across diverse and seemingly dissimilar classes.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "https://youtu.be/gltneexyhYs",
                        "ff_id": "gltneexyhYs"
                    },
                    {
                        "slot_id": "v-full-1320-pres",
                        "session_id": "full9",
                        "type": "In Person Presentation",
                        "title": "SliceTeller : A Data Slice-Driven Approach for Machine Learning Model Validation",
                        "contributors": [
                            "Jorge H Piazentin Ono",
                            "Xiaoyu Zhang"
                        ],
                        "authors": [
                            "Xiaoyu Zhang",
                            "Jorge H Piazentin Ono",
                            "Huan Song",
                            "Liang Gou",
                            "Kwan-Liu Ma",
                            "Liu Ren"
                        ],
                        "abstract": "Real-world machine learning applications need to be thoroughly evaluated to meet critical product requirements for model release, to ensure fairness for different groups or individuals, and to achieve a consistent performance in various scenarios. For example, in autonomous driving, an object classification model should achieve high detection rates under different conditions of weather, distance, etc. Similarly, in the financial setting, credit-scoring models must not discriminate against minority groups. These conditions or groups are called as \u201cData Slices\u201d. In product MLOps cycles, product developers must identify such critical data slices and adapt models to mitigate data slice problems. Discovering where models fail, understanding why they fail, and mitigating these problems, are therefore essential tasks in the MLOps life-cycle. In this paper, we present SliceTeller, a novel tool that allows users to debug, compare and improve machine learning models driven by critical data slices. SliceTeller automatically discovers problematic slices in the data, helps the user understand why models fail. More importantly, we present an efficient algorithm, SliceBoosting, to estimate trade-offs when prioritizing the optimization over certain slices. Furthermore, our system empowers model developers to compare and analyze different model versions during model iterations, allowing them to choose the model version best suitable for their applications. We evaluate our system with three use cases, including two real-world use cases of product development, to demonstrate the power of SliceTeller in the debugging and improvement of product-quality ML models.",
                        "uid": "v-full-1320",
                        "file_name": "v-full-1320_Zhang_Presentation.mp4",
                        "time_stamp": "2022-10-20T16:21:00Z",
                        "time_start": "2022-10-20T16:21:00Z",
                        "time_end": "2022-10-20T16:31:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "590",
                        "paper_award": "",
                        "image_caption": "SliceTeller applied to the comparison of two machine learning models (ResNet50 and GroupDRO) for hair color classification (gray hair, not gray hair), trained on the CelebFaces Attributes Dataset (CelebA). (A) Slice Matrix: The data slices (represented as rows), slice descriptions (encoded as columns), and slice metrics (Support and Accuracy). Slices are sorted by model accuracy. (A - Tooltip) Confusion matrix for Slice 1. (B) Estimated effects of optimizing the model for two data slices (Slices 1 and 2, highlighted in blue). (C) Accuracy comparison between the two models, ResNet50 and GroupDRO. (D) Slice Detail View containing image samples from a data slice. (E) Slice Detail View containing the comparison of two data slices using the MatrixScape visualization. (F) System menu, containing options for model selection, effect estimation of focusing on a slice during model training, and data slice summarization.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "https://youtu.be/NOcHFj87uWI",
                        "ff_id": "NOcHFj87uWI"
                    },
                    {
                        "slot_id": "v-full-1320-qa",
                        "session_id": "full9",
                        "type": "In Person Q+A",
                        "title": "SliceTeller : A Data Slice-Driven Approach for Machine Learning Model Validation (Q+A)",
                        "contributors": [
                            "Jorge H Piazentin Ono",
                            "Xiaoyu Zhang"
                        ],
                        "authors": [],
                        "abstract": "Real-world machine learning applications need to be thoroughly evaluated to meet critical product requirements for model release, to ensure fairness for different groups or individuals, and to achieve a consistent performance in various scenarios. For example, in autonomous driving, an object classification model should achieve high detection rates under different conditions of weather, distance, etc. Similarly, in the financial setting, credit-scoring models must not discriminate against minority groups. These conditions or groups are called as \u201cData Slices\u201d. In product MLOps cycles, product developers must identify such critical data slices and adapt models to mitigate data slice problems. Discovering where models fail, understanding why they fail, and mitigating these problems, are therefore essential tasks in the MLOps life-cycle. In this paper, we present SliceTeller, a novel tool that allows users to debug, compare and improve machine learning models driven by critical data slices. SliceTeller automatically discovers problematic slices in the data, helps the user understand why models fail. More importantly, we present an efficient algorithm, SliceBoosting, to estimate trade-offs when prioritizing the optimization over certain slices. Furthermore, our system empowers model developers to compare and analyze different model versions during model iterations, allowing them to choose the model version best suitable for their applications. We evaluate our system with three use cases, including two real-world use cases of product development, to demonstrate the power of SliceTeller in the debugging and improvement of product-quality ML models.",
                        "uid": "v-full-1320",
                        "file_name": "",
                        "time_stamp": "2022-10-20T16:31:00Z",
                        "time_start": "2022-10-20T16:31:00Z",
                        "time_end": "2022-10-20T16:33:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "590",
                        "paper_award": "",
                        "image_caption": "SliceTeller applied to the comparison of two machine learning models (ResNet50 and GroupDRO) for hair color classification (gray hair, not gray hair), trained on the CelebFaces Attributes Dataset (CelebA). (A) Slice Matrix: The data slices (represented as rows), slice descriptions (encoded as columns), and slice metrics (Support and Accuracy). Slices are sorted by model accuracy. (A - Tooltip) Confusion matrix for Slice 1. (B) Estimated effects of optimizing the model for two data slices (Slices 1 and 2, highlighted in blue). (C) Accuracy comparison between the two models, ResNet50 and GroupDRO. (D) Slice Detail View containing image samples from a data slice. (E) Slice Detail View containing the comparison of two data slices using the MatrixScape visualization. (F) System menu, containing options for model selection, effect estimation of focusing on a slice during model training, and data slice summarization.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "https://youtu.be/NOcHFj87uWI",
                        "ff_id": "NOcHFj87uWI"
                    },
                    {
                        "slot_id": "v-full-1657-pres",
                        "session_id": "full9",
                        "type": "In Person Presentation",
                        "title": "Calibrate: Interactive Analysis of Probabilistic Model Output",
                        "contributors": [
                            "Peter Xenopoulos"
                        ],
                        "authors": [
                            "Peter Xenopoulos",
                            "Jo\u00e3o Rulff",
                            "Luis Gustavo Nonato",
                            "Brian Barr",
                            "Claudio Silva"
                        ],
                        "abstract": "Analyzing classification model performance is a crucial task for machine learning practitioners. While practitioners often use count-based metrics derived from confusion matrices, like accuracy, many applications, such as weather prediction, sports betting, or patient risk prediction, rely on a classifier's predicted probabilities rather than predicted labels. In these instances, practitioners are concerned with producing a calibrated model, that is, one which outputs probabilities that reflect those of the true distribution. Model calibration is often analyzed visually, through static reliability diagrams, however, the traditional calibration visualization may suffer from a variety of drawbacks due to the strong aggregations it necessitates. Furthermore, count-based approaches are unable to sufficiently analyze model calibration. We present Calibrate, an interactive reliability diagram that addresses the aforementioned issues. Calibrate constructs a reliability diagram that is resistant to drawbacks in traditional approaches, and allows for interactive subgroup analysis and instance-level inspection. We demonstrate the utility of Calibrate through use cases on both real-world and synthetic data. We further validate Calibrate by presenting the results of a think-aloud experiment with data scientists who routinely analyze model calibration.",
                        "uid": "v-full-1657",
                        "file_name": "v-full-1657_Xenopoulos_Presentation.mp4",
                        "time_stamp": "2022-10-20T16:33:00Z",
                        "time_start": "2022-10-20T16:33:00Z",
                        "time_end": "2022-10-20T16:43:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "504",
                        "paper_award": "",
                        "image_caption": "Calibrate allows users to interactively analyze model calibration in Jupyter Notebooks. The system allows for easy creation of conventional and learned reliability diagrams. The reliability diagrams are coupled with a prediction histogram to provide better context for the users. The diagrams also allow for brushing to analyze prediction regions. When a selection is made, the instance view, shown below the reliability diagram, is updated. Users can also create and analyze subsets of the data by brushing on feature distributions.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "https://youtu.be/IXfUiI3Lybg",
                        "ff_id": "IXfUiI3Lybg"
                    },
                    {
                        "slot_id": "v-full-1657-qa",
                        "session_id": "full9",
                        "type": "In Person Q+A",
                        "title": "Calibrate: Interactive Analysis of Probabilistic Model Output (Q+A)",
                        "contributors": [
                            "Peter Xenopoulos"
                        ],
                        "authors": [],
                        "abstract": "Analyzing classification model performance is a crucial task for machine learning practitioners. While practitioners often use count-based metrics derived from confusion matrices, like accuracy, many applications, such as weather prediction, sports betting, or patient risk prediction, rely on a classifier's predicted probabilities rather than predicted labels. In these instances, practitioners are concerned with producing a calibrated model, that is, one which outputs probabilities that reflect those of the true distribution. Model calibration is often analyzed visually, through static reliability diagrams, however, the traditional calibration visualization may suffer from a variety of drawbacks due to the strong aggregations it necessitates. Furthermore, count-based approaches are unable to sufficiently analyze model calibration. We present Calibrate, an interactive reliability diagram that addresses the aforementioned issues. Calibrate constructs a reliability diagram that is resistant to drawbacks in traditional approaches, and allows for interactive subgroup analysis and instance-level inspection. We demonstrate the utility of Calibrate through use cases on both real-world and synthetic data. We further validate Calibrate by presenting the results of a think-aloud experiment with data scientists who routinely analyze model calibration.",
                        "uid": "v-full-1657",
                        "file_name": "",
                        "time_stamp": "2022-10-20T16:43:00Z",
                        "time_start": "2022-10-20T16:43:00Z",
                        "time_end": "2022-10-20T16:45:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "504",
                        "paper_award": "",
                        "image_caption": "Calibrate allows users to interactively analyze model calibration in Jupyter Notebooks. The system allows for easy creation of conventional and learned reliability diagrams. The reliability diagrams are coupled with a prediction histogram to provide better context for the users. The diagrams also allow for brushing to analyze prediction regions. When a selection is made, the instance view, shown below the reliability diagram, is updated. Users can also create and analyze subsets of the data by brushing on feature distributions.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "https://youtu.be/IXfUiI3Lybg",
                        "ff_id": "IXfUiI3Lybg"
                    },
                    {
                        "slot_id": "v-full-1319-pres",
                        "session_id": "full9",
                        "type": "In Person Presentation",
                        "title": "Visualizing Ensemble Predictions of Music Mood",
                        "contributors": [
                            "Mr Zelin Ye"
                        ],
                        "authors": [
                            "Zelin Ye",
                            "Min Chen"
                        ],
                        "abstract": "Music mood classification has been a challenging problem in comparison with other music classification problems (e.g., genre, composer, or period). One solution for addressing this challenge is to use an ensemble of machine learning models. In this paper, we show that visualization techniques can effectively convey the popular prediction as well as uncertainty at different music sections along the temporal axis while enabling the analysis of individual ML models in conjunction with their application to different musical data. In addition to the traditional visual designs, such as stacked line graph, ThemeRiver, and pixel-based visualization, we introduce a new variant of ThemeRiver, called ``dual-flux ThemeRiver'', which allows viewers to observe and measure the most popular prediction more easily than stacked line graph and ThemeRiver. Together with pixel-based visualization, dual-flux ThemeRiver plots can also assist in model-development workflows, in addition to annotating music using ensemble model predictions.",
                        "uid": "v-full-1319",
                        "file_name": "v-full-1319_Ye_Presentation.mp4",
                        "time_stamp": "2022-10-20T16:45:00Z",
                        "time_start": "2022-10-20T16:45:00Z",
                        "time_end": "2022-10-20T16:55:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "597",
                        "paper_award": "",
                        "image_caption": "An ensemble of 210 machine learning (ML) models are visualized by the dual-flux ThemeRiver, \naccompanied with the music score for bar 30~34 of Bach: Paritita V in G, BWV829: Gigue.\nThe upper, lower flux and the dashed line depict the dominant mood, the other moods in the descending\norder, and the 50% threshold over time.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "https://youtu.be/-nVLuVKEpe4",
                        "ff_id": "-nVLuVKEpe4"
                    },
                    {
                        "slot_id": "v-full-1319-qa",
                        "session_id": "full9",
                        "type": "In Person Q+A",
                        "title": "Visualizing Ensemble Predictions of Music Mood (Q+A)",
                        "contributors": [
                            "Mr Zelin Ye"
                        ],
                        "authors": [],
                        "abstract": "Music mood classification has been a challenging problem in comparison with other music classification problems (e.g., genre, composer, or period). One solution for addressing this challenge is to use an ensemble of machine learning models. In this paper, we show that visualization techniques can effectively convey the popular prediction as well as uncertainty at different music sections along the temporal axis while enabling the analysis of individual ML models in conjunction with their application to different musical data. In addition to the traditional visual designs, such as stacked line graph, ThemeRiver, and pixel-based visualization, we introduce a new variant of ThemeRiver, called ``dual-flux ThemeRiver'', which allows viewers to observe and measure the most popular prediction more easily than stacked line graph and ThemeRiver. Together with pixel-based visualization, dual-flux ThemeRiver plots can also assist in model-development workflows, in addition to annotating music using ensemble model predictions.",
                        "uid": "v-full-1319",
                        "file_name": "",
                        "time_stamp": "2022-10-20T16:55:00Z",
                        "time_start": "2022-10-20T16:55:00Z",
                        "time_end": "2022-10-20T16:57:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "597",
                        "paper_award": "",
                        "image_caption": "An ensemble of 210 machine learning (ML) models are visualized by the dual-flux ThemeRiver, \naccompanied with the music score for bar 30~34 of Bach: Paritita V in G, BWV829: Gigue.\nThe upper, lower flux and the dashed line depict the dominant mood, the other moods in the descending\norder, and the 50% threshold over time.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "https://youtu.be/-nVLuVKEpe4",
                        "ff_id": "-nVLuVKEpe4"
                    }
                ]
            },
            {
                "title": "Storytelling",
                "session_id": "full10",
                "event_prefix": "v-full",
                "track": "ok4",
                "livestream_id": "ok4-thu",
                "session_image": "full10.png",
                "chair": [
                    "Robert Laramee"
                ],
                "organizers": [],
                "time_start": "2022-10-20T19:00:00Z",
                "time_end": "2022-10-20T20:15:00Z",
                "discord_category": "",
                "discord_channel": "oklahoma-st-4",
                "discord_channel_id": "1024600674924765264",
                "discord_link": "https://discord.com/channels/978320435554947082/1024600674924765264",
                "slido_link": "https://app.sli.do/event/i28UETZaiRWr1XwX6eKjcf?section=51658b34-0f5b-40c9-a335-1fd1468fad8d",
                "youtube_url": "https://youtu.be/0qY_NxLSGBk",
                "youtube_id": "0qY_NxLSGBk",
                "zoom_meeting": "",
                "zoom_password": "",
                "zoom_link": "",
                "ff_link": "https://youtu.be/LK9tMkr9qOI",
                "ff_playlist": "",
                "time_slots": [
                    {
                        "slot_id": "full10-opening",
                        "session_id": "full10",
                        "type": "In Person Other",
                        "title": "Session Opening",
                        "contributors": [
                            "Robert Laramee"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "",
                        "file_name": "",
                        "time_stamp": "2022-10-20T19:00:00Z",
                        "time_start": "2022-10-20T19:00:00Z",
                        "time_end": "2022-10-20T19:00:00Z",
                        "paper_type": "",
                        "keywords": [],
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "v-full-1198-pres",
                        "session_id": "full10",
                        "type": "Virtual Presentation (pre-recorded)",
                        "title": "Breaking the Fourth Wall of Data Stories through Interaction",
                        "contributors": [
                            "Yang Shi",
                            "Tian Gao"
                        ],
                        "authors": [
                            "Yang Shi",
                            "Tian Gao",
                            "Xiaohan Jiao",
                            "Nan Cao"
                        ],
                        "abstract": "Interaction is increasingly integrating into data stories to support data exploration and explanation. Interaction can also be combined with the narrative device, breaking the fourth wall (BTFW), to build a deeper connection between readers and data stories. BTFW interaction directly addresses readers by requiring their input. Such user input is then integrated into the narrative or visuals of data stories to encourage readers to inspect the stories more closely. In this work, we explore the design patterns of BTFW interaction commonly used in data stories. Six design patterns were identified through the analysis of 58 high-quality data stories collected from a range of online sources. Specifically, the data stories were categorized using a coding framework, including the input of BTFW interaction provided by readers and the output of BTFW interaction generated by data stories to respond to the input. To explore the benefits as well as concerns of using BTFW interaction, we conducted a three-session user study including the reading, interview, and recall sessions. The results of our user study suggested that BTFW interaction has a positive impact on self-story connection, user engagement, and information recall. We also discussed design implications to address the possible negative effects on the interactivity-comprehensibility balance, information privacy, and the learning curve of interaction brought by BTFW interaction.",
                        "uid": "v-full-1198",
                        "file_name": "v-full-1198_Shi_Presentation.mp4",
                        "time_stamp": "2022-10-20T19:00:00Z",
                        "time_start": "2022-10-20T19:00:00Z",
                        "time_end": "2022-10-20T19:07:58Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "478",
                        "paper_award": "",
                        "image_caption": "We explored data stories that are combined with the narrative device, breaking the fourth wall (BTFW). We collected 58 high-quality data stories from a range of online sources and coded them from two perspectives, including the input of BTFW interaction provided by readers and the output of BTFW interaction generated by data stories to respond to the input. As a result, six design patterns were identified: (C1) Golden Hook, (C2) Kaleidoscope, (C3) Simulator, (C4) Spotlight, (C5) Magic Mirror, and (C6) Touchstone.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "https://youtu.be/m1MwgbOWVxg",
                        "ff_id": "m1MwgbOWVxg"
                    },
                    {
                        "slot_id": "v-full-1198-qa",
                        "session_id": "full10",
                        "type": "Virtual Q+A",
                        "title": "Breaking the Fourth Wall of Data Stories through Interaction (Q+A)",
                        "contributors": [
                            "Yang Shi",
                            "Tian Gao"
                        ],
                        "authors": [],
                        "abstract": "Interaction is increasingly integrating into data stories to support data exploration and explanation. Interaction can also be combined with the narrative device, breaking the fourth wall (BTFW), to build a deeper connection between readers and data stories. BTFW interaction directly addresses readers by requiring their input. Such user input is then integrated into the narrative or visuals of data stories to encourage readers to inspect the stories more closely. In this work, we explore the design patterns of BTFW interaction commonly used in data stories. Six design patterns were identified through the analysis of 58 high-quality data stories collected from a range of online sources. Specifically, the data stories were categorized using a coding framework, including the input of BTFW interaction provided by readers and the output of BTFW interaction generated by data stories to respond to the input. To explore the benefits as well as concerns of using BTFW interaction, we conducted a three-session user study including the reading, interview, and recall sessions. The results of our user study suggested that BTFW interaction has a positive impact on self-story connection, user engagement, and information recall. We also discussed design implications to address the possible negative effects on the interactivity-comprehensibility balance, information privacy, and the learning curve of interaction brought by BTFW interaction.",
                        "uid": "v-full-1198",
                        "file_name": "",
                        "time_stamp": "2022-10-20T19:07:58Z",
                        "time_start": "2022-10-20T19:07:58Z",
                        "time_end": "2022-10-20T19:15:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "478",
                        "paper_award": "",
                        "image_caption": "We explored data stories that are combined with the narrative device, breaking the fourth wall (BTFW). We collected 58 high-quality data stories from a range of online sources and coded them from two perspectives, including the input of BTFW interaction provided by readers and the output of BTFW interaction generated by data stories to respond to the input. As a result, six design patterns were identified: (C1) Golden Hook, (C2) Kaleidoscope, (C3) Simulator, (C4) Spotlight, (C5) Magic Mirror, and (C6) Touchstone.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "https://youtu.be/m1MwgbOWVxg",
                        "ff_id": "m1MwgbOWVxg"
                    },
                    {
                        "slot_id": "v-full-1495-pres",
                        "session_id": "full10",
                        "type": "Virtual Presentation (pre-recorded)",
                        "title": "Erato: Cooperative Data Story Editing via Fact Interpolation",
                        "contributors": [
                            "Prof. Nan Cao",
                            "Mengdi Sun"
                        ],
                        "authors": [
                            "Mengdi Sun",
                            "Ligan Cai",
                            "Weiwei Cui",
                            "Yanqiu Wu",
                            "Yang Shi",
                            "Nan Cao"
                        ],
                        "abstract": "As an effective form of narrative visualization, visual data stories are widely used in data-driven storytelling to communicate complex insights and support data understanding. Although important, they are difficult to create, as a variety of interdisciplinary skills, such as data analysis and design, are required. In this work, we introduce Erato, a human-machine cooperative data story editing system, which allows users to generate insightful and fluent data stories together with the computer. Specifically, Erato only requires a number of keyframes provided by the user to briefly describe the topic and structure of a data story. Meanwhile, our system leverages a novel interpolation algorithm to help users insert intermediate frames between the keyframes to smooth the transition. We evaluated the effectiveness and usefulness of the Erato system via a series of evaluations including a Turing test, a controlled user study, a performance validation, and interviews with three expert users. The evaluation results showed that the proposed interpolation technique was able to generate coherent story content and help users create data stories more efficiently.",
                        "uid": "v-full-1495",
                        "file_name": "v-full-1495_Sun_Presentation.mp4",
                        "time_stamp": "2022-10-20T19:15:00Z",
                        "time_start": "2022-10-20T19:15:00Z",
                        "time_end": "2022-10-20T19:24:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "540",
                        "paper_award": "",
                        "image_caption": "A data story about natural disasters. The data facts in black were created as keyframes of the story, while the facts in red were generated based on our interpolation algorithm to fill the content gap. The story first illustrates an overall situation of global natural disasters and gradually focuses on the situation in China. Finally, it reveals that floods have the most pernicious impact on China. The corresponding interpolation process is also shown under the data story. The searching path is marked in red and the nodes with yellow borders are the final selected interpolation results.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "https://youtu.be/Qv6AlPPfZhg",
                        "ff_id": "Qv6AlPPfZhg"
                    },
                    {
                        "slot_id": "v-full-1495-qa",
                        "session_id": "full10",
                        "type": "Virtual Q+A",
                        "title": "Erato: Cooperative Data Story Editing via Fact Interpolation (Q+A)",
                        "contributors": [
                            "Prof. Nan Cao",
                            "Mengdi Sun"
                        ],
                        "authors": [],
                        "abstract": "As an effective form of narrative visualization, visual data stories are widely used in data-driven storytelling to communicate complex insights and support data understanding. Although important, they are difficult to create, as a variety of interdisciplinary skills, such as data analysis and design, are required. In this work, we introduce Erato, a human-machine cooperative data story editing system, which allows users to generate insightful and fluent data stories together with the computer. Specifically, Erato only requires a number of keyframes provided by the user to briefly describe the topic and structure of a data story. Meanwhile, our system leverages a novel interpolation algorithm to help users insert intermediate frames between the keyframes to smooth the transition. We evaluated the effectiveness and usefulness of the Erato system via a series of evaluations including a Turing test, a controlled user study, a performance validation, and interviews with three expert users. The evaluation results showed that the proposed interpolation technique was able to generate coherent story content and help users create data stories more efficiently.",
                        "uid": "v-full-1495",
                        "file_name": "",
                        "time_stamp": "2022-10-20T19:24:00Z",
                        "time_start": "2022-10-20T19:24:00Z",
                        "time_end": "2022-10-20T19:30:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "540",
                        "paper_award": "",
                        "image_caption": "A data story about natural disasters. The data facts in black were created as keyframes of the story, while the facts in red were generated based on our interpolation algorithm to fill the content gap. The story first illustrates an overall situation of global natural disasters and gradually focuses on the situation in China. Finally, it reveals that floods have the most pernicious impact on China. The corresponding interpolation process is also shown under the data story. The searching path is marked in red and the nodes with yellow borders are the final selected interpolation results.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "https://youtu.be/Qv6AlPPfZhg",
                        "ff_id": "Qv6AlPPfZhg"
                    },
                    {
                        "slot_id": "v-full-1180-pres",
                        "session_id": "full10",
                        "type": "Virtual Presentation (pre-recorded)",
                        "title": "Geo-Storylines: Integrating Maps into Storyline Visualizations",
                        "contributors": [
                            "Vanessa Pe\u00f1a-Araya"
                        ],
                        "authors": [
                            "Golina Hulstein",
                            "Vanessa Pe\u00f1a-Araya",
                            "Anastasia Bezerianos"
                        ],
                        "abstract": "Storyline visualizations are a powerful way to compactly visualize how the relationships between people evolve over time. Real-world relationships often also involve space, for example the cities that two political rivals visited together or alone over the years. By default, Storyline visualizations only show implicitly geospatial co-occurrence between people (drawn as lines), by bringing their lines together. Even the few designs that do explicitly show geographic locations only do so in abstract ways (e.g. annotations) and do not communicate geospatial information, such as the direction or extent of their political campains. We introduce Geo-Storylines, a collection of visualisation designs that integrate geospatial context into Storyline visualizations, using different strategies for compositing time and space. Our contribution is twofold. First, we present the results of a sketching workshop with 11 participants, that we used to derive a design space for integrating maps into Storylines. Second, by analyzing the strengths and weaknesses of the potential designs of the design space in terms of legibility and ability to scale to multiple relationships, we extract the three most promising: Time Glyphs, Coordinated Views, and Map Glyphs. We compare these three techniques first in a controlled study with 18 participants, under five different geospatial tasks and two maps of different complexity. We additionally collected informal feedback about their usefulness from domain experts in data journalism. Our results indicate that, as expected, detailed performance depends on the task. Nevertheless, Coordinated Views remain a highly effective and preferred technique across the board.",
                        "uid": "v-full-1180",
                        "file_name": "v-full-1180_Hulstein_Presentation.mp4",
                        "time_stamp": "2022-10-20T19:30:00Z",
                        "time_start": "2022-10-20T19:30:00Z",
                        "time_end": "2022-10-20T19:38:18Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "498",
                        "paper_award": "",
                        "image_caption": "Three Geo-Storyline designs showing the geo-temporal evolution of the relationships between people. (A) Coordinated Views include a map on the left and a unique Storyline timeline on the right. While scrolling, links appear between the relationship nearest to the map and the associated locations. (B) In Map Glyphs each relationship is represented by a map with the associated locations drawn in orange. (C) Time Glyphs are composed by a map on the left and a scrollable list of Storyline glyphs on the right. Each Storyline glyph contains all the relationships associated with one location linked by a gray line.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "https://youtu.be/sPzsQqHDSGo",
                        "ff_id": "sPzsQqHDSGo"
                    },
                    {
                        "slot_id": "v-full-1180-qa",
                        "session_id": "full10",
                        "type": "Virtual Q+A",
                        "title": "Geo-Storylines: Integrating Maps into Storyline Visualizations (Q+A)",
                        "contributors": [
                            "Vanessa Pe\u00f1a-Araya"
                        ],
                        "authors": [],
                        "abstract": "Storyline visualizations are a powerful way to compactly visualize how the relationships between people evolve over time. Real-world relationships often also involve space, for example the cities that two political rivals visited together or alone over the years. By default, Storyline visualizations only show implicitly geospatial co-occurrence between people (drawn as lines), by bringing their lines together. Even the few designs that do explicitly show geographic locations only do so in abstract ways (e.g. annotations) and do not communicate geospatial information, such as the direction or extent of their political campains. We introduce Geo-Storylines, a collection of visualisation designs that integrate geospatial context into Storyline visualizations, using different strategies for compositing time and space. Our contribution is twofold. First, we present the results of a sketching workshop with 11 participants, that we used to derive a design space for integrating maps into Storylines. Second, by analyzing the strengths and weaknesses of the potential designs of the design space in terms of legibility and ability to scale to multiple relationships, we extract the three most promising: Time Glyphs, Coordinated Views, and Map Glyphs. We compare these three techniques first in a controlled study with 18 participants, under five different geospatial tasks and two maps of different complexity. We additionally collected informal feedback about their usefulness from domain experts in data journalism. Our results indicate that, as expected, detailed performance depends on the task. Nevertheless, Coordinated Views remain a highly effective and preferred technique across the board.",
                        "uid": "v-full-1180",
                        "file_name": "",
                        "time_stamp": "2022-10-20T19:38:18Z",
                        "time_start": "2022-10-20T19:38:18Z",
                        "time_end": "2022-10-20T19:45:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "498",
                        "paper_award": "",
                        "image_caption": "Three Geo-Storyline designs showing the geo-temporal evolution of the relationships between people. (A) Coordinated Views include a map on the left and a unique Storyline timeline on the right. While scrolling, links appear between the relationship nearest to the map and the associated locations. (B) In Map Glyphs each relationship is represented by a map with the associated locations drawn in orange. (C) Time Glyphs are composed by a map on the left and a scrollable list of Storyline glyphs on the right. Each Storyline glyph contains all the relationships associated with one location linked by a gray line.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "https://youtu.be/sPzsQqHDSGo",
                        "ff_id": "sPzsQqHDSGo"
                    },
                    {
                        "slot_id": "v-tvcg-9695173-pres",
                        "session_id": "full10",
                        "type": "In Person Presentation",
                        "title": "Roslingifier: Semi-Automated Storytelling for Animated Scatterplots",
                        "contributors": [
                            "Sungahn Ko"
                        ],
                        "authors": [
                            "Minjeong Shin",
                            "Joohee Kim",
                            "Yunha Han",
                            "Lexing Xie",
                            "Mitchell Whitelaw",
                            "Bum Chul Kwon",
                            "Sungahn Ko",
                            "Niklas Elmqvist"
                        ],
                        "abstract": "We present Roslingifier, a data-driven storytelling method for animated scatterplots. Like its namesake, Hans Rosling (1948--2017), a professor of public health and a spellbinding public speaker, Roslingifier turns a sequence of entities changing over time---such as countries and continents with their demographic data---into an engaging narrative telling the story of the data. This data-driven storytelling method with an in-person presenter is a new genre of storytelling technique and has never been studied before. In this paper, we aim to define a design space for this new genre---data presentation---and provide a semi-automated authoring tool for helping presenters create quality presentations. From an in-depth analysis of video clips of presentations using interactive visualizations, we derive three specific techniques to achieve this: natural language narratives, visual effects that highlight events, and temporal branching that changes playback time of the animation. Our implementation of the Roslingifier method is capable of identifying and clustering significant movements, automatically generating visual highlighting and a narrative for playback, and enabling the user to customize. From two user studies, we show that Roslingifier allows users to effectively create engaging data stories and the system features help both presenters and viewers find diverse insights.",
                        "uid": "v-tvcg-9695173",
                        "file_name": "v-tvcg-9695173_Shin_Presentation.mp4",
                        "time_stamp": "2022-10-20T19:45:00Z",
                        "time_start": "2022-10-20T19:45:00Z",
                        "time_end": "2022-10-20T19:58:00Z",
                        "paper_type": "full",
                        "keywords": [
                            "Data-driven storytelling, narrative visualization, Hans Rosling, Gapminder, Trendalyzer"
                        ],
                        "has_image": "1",
                        "has_video": "548",
                        "paper_award": "",
                        "image_caption": "In this paper, we define a design space for the new genre, data presentation, as the use of interactive data visualization to support in-person presentations. From an in-depth analysis of video clips of presentations using interactive visualizations, we derive specific presentation techniques and implement a semi-automated authoring tool for helping presenters create quality presentations. Our implementation of the Roslingifier method is capable of, identifying and clustering significant movements, automatically generating visual highlighting and a narrative for playback, and enabling the user to customize.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "https://youtu.be/336cbwg5jQY",
                        "ff_id": "336cbwg5jQY"
                    },
                    {
                        "slot_id": "v-tvcg-9695173-qa",
                        "session_id": "full10",
                        "type": "In Person Q+A",
                        "title": "Roslingifier: Semi-Automated Storytelling for Animated Scatterplots (Q+A)",
                        "contributors": [
                            "Sungahn Ko"
                        ],
                        "authors": [],
                        "abstract": "We present Roslingifier, a data-driven storytelling method for animated scatterplots. Like its namesake, Hans Rosling (1948--2017), a professor of public health and a spellbinding public speaker, Roslingifier turns a sequence of entities changing over time---such as countries and continents with their demographic data---into an engaging narrative telling the story of the data. This data-driven storytelling method with an in-person presenter is a new genre of storytelling technique and has never been studied before. In this paper, we aim to define a design space for this new genre---data presentation---and provide a semi-automated authoring tool for helping presenters create quality presentations. From an in-depth analysis of video clips of presentations using interactive visualizations, we derive three specific techniques to achieve this: natural language narratives, visual effects that highlight events, and temporal branching that changes playback time of the animation. Our implementation of the Roslingifier method is capable of identifying and clustering significant movements, automatically generating visual highlighting and a narrative for playback, and enabling the user to customize. From two user studies, we show that Roslingifier allows users to effectively create engaging data stories and the system features help both presenters and viewers find diverse insights.",
                        "uid": "v-tvcg-9695173",
                        "file_name": "",
                        "time_stamp": "2022-10-20T19:58:00Z",
                        "time_start": "2022-10-20T19:58:00Z",
                        "time_end": "2022-10-20T20:00:00Z",
                        "paper_type": "full",
                        "keywords": [
                            "Data-driven storytelling, narrative visualization, Hans Rosling, Gapminder, Trendalyzer"
                        ],
                        "has_image": "1",
                        "has_video": "548",
                        "paper_award": "",
                        "image_caption": "In this paper, we define a design space for the new genre, data presentation, as the use of interactive data visualization to support in-person presentations. From an in-depth analysis of video clips of presentations using interactive visualizations, we derive specific presentation techniques and implement a semi-automated authoring tool for helping presenters create quality presentations. Our implementation of the Roslingifier method is capable of, identifying and clustering significant movements, automatically generating visual highlighting and a narrative for playback, and enabling the user to customize.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "https://youtu.be/336cbwg5jQY",
                        "ff_id": "336cbwg5jQY"
                    },
                    {
                        "slot_id": "v-tvcg-9645360-pres",
                        "session_id": "full10",
                        "type": "In Person Presentation",
                        "title": "Nanotilus: Generator of Immersive Guided-Tours in Crowded 3D Environments",
                        "contributors": [
                            "Ruwayda Alharbi"
                        ],
                        "authors": [
                            "Ruwayda Alharbi",
                            "Ond\u02c7rej Strnad",
                            "Laura R. Luidolt",
                            "Manuela Waldner",
                            "David Kou\u02c7ril",
                            "Ciril Bohak",
                            "Tobias Klein",
                            "Eduard Groller",
                            "Ivan Viola"
                        ],
                        "abstract": "Immersive virtual reality environments are gaining popularity for studying and exploring crowded three-dimensional structures. When reaching very high structural densities, the natural depiction of the scene produces impenetrable clutter and requires visibility and occlusion management strategies for exploration and orientation. Strategies developed to address the crowdedness in desktop applications, however, inhibit the feeling of immersion. They result in nonimmersive, desktop-style outside-in viewing in virtual reality. This paper proposes Nanotilus---a new visibility and guidance approach for very dense environments that generates an endoscopic inside-out experience instead of outside-in viewing, preserving the immersive aspect of virtual reality. The approach consists of two novel, tightly coupled mechanisms that control scene sparsification simultaneously with camera path planning. The sparsification strategy is localized around the camera and is realized as a multi-scale, multi-shell, variety-preserving technique. When Nanotilus dives into the structures to capture internal details residing on multiple scales, it guides the camera using depth-based path planning. In addition to sparsification and path planning, we complete the tour generation with an animation controller, textual annotation, and text-to-visualization conversion. We demonstrate the generated guided tours on mesoscopic biological models -- SARS-CoV-2 and HIV. We evaluate the Nanotilus experience with a baseline outside-in sparsification and navigational technique in a formal user study with 29 participants. While users can maintain a better overview using the outside-in sparsification, the study confirms our hypothesis that Nanotilus leads to stronger engagement and immersion.",
                        "uid": "v-tvcg-9645360",
                        "file_name": "v-tvcg-9645360_Alharbi_Presentation.mp4",
                        "time_stamp": "2022-10-20T20:00:00Z",
                        "time_start": "2022-10-20T20:00:00Z",
                        "time_end": "2022-10-20T20:13:00Z",
                        "paper_type": "full",
                        "keywords": [
                            "VR immersive, Visibility management, Path planning, Storytelling, Visualization"
                        ],
                        "has_image": "1",
                        "has_video": "498",
                        "paper_award": "",
                        "image_caption": "Nanotilus shells enters the center of the HIV model and sparsifies several protein instances to reveal the internal part of the model. A grayscale model with highlighted proteins inside the Nanotilus shells is combined with the model\u2019s final depiction.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "https://youtu.be/1nAWadkhWoc",
                        "ff_id": "1nAWadkhWoc"
                    },
                    {
                        "slot_id": "v-tvcg-9645360-qa",
                        "session_id": "full10",
                        "type": "In Person Q+A",
                        "title": "Nanotilus: Generator of Immersive Guided-Tours in Crowded 3D Environments (Q+A)",
                        "contributors": [
                            "Ruwayda Alharbi"
                        ],
                        "authors": [],
                        "abstract": "Immersive virtual reality environments are gaining popularity for studying and exploring crowded three-dimensional structures. When reaching very high structural densities, the natural depiction of the scene produces impenetrable clutter and requires visibility and occlusion management strategies for exploration and orientation. Strategies developed to address the crowdedness in desktop applications, however, inhibit the feeling of immersion. They result in nonimmersive, desktop-style outside-in viewing in virtual reality. This paper proposes Nanotilus---a new visibility and guidance approach for very dense environments that generates an endoscopic inside-out experience instead of outside-in viewing, preserving the immersive aspect of virtual reality. The approach consists of two novel, tightly coupled mechanisms that control scene sparsification simultaneously with camera path planning. The sparsification strategy is localized around the camera and is realized as a multi-scale, multi-shell, variety-preserving technique. When Nanotilus dives into the structures to capture internal details residing on multiple scales, it guides the camera using depth-based path planning. In addition to sparsification and path planning, we complete the tour generation with an animation controller, textual annotation, and text-to-visualization conversion. We demonstrate the generated guided tours on mesoscopic biological models -- SARS-CoV-2 and HIV. We evaluate the Nanotilus experience with a baseline outside-in sparsification and navigational technique in a formal user study with 29 participants. While users can maintain a better overview using the outside-in sparsification, the study confirms our hypothesis that Nanotilus leads to stronger engagement and immersion.",
                        "uid": "v-tvcg-9645360",
                        "file_name": "",
                        "time_stamp": "2022-10-20T20:13:00Z",
                        "time_start": "2022-10-20T20:13:00Z",
                        "time_end": "2022-10-20T20:15:00Z",
                        "paper_type": "full",
                        "keywords": [
                            "VR immersive, Visibility management, Path planning, Storytelling, Visualization"
                        ],
                        "has_image": "1",
                        "has_video": "498",
                        "paper_award": "",
                        "image_caption": "Nanotilus shells enters the center of the HIV model and sparsifies several protein instances to reveal the internal part of the model. A grayscale model with highlighted proteins inside the Nanotilus shells is combined with the model\u2019s final depiction.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "https://youtu.be/1nAWadkhWoc",
                        "ff_id": "1nAWadkhWoc"
                    },
                    {
                        "slot_id": "v-full-1103-pres",
                        "session_id": "full10",
                        "type": "In Person Presentation",
                        "title": "How Do Viewers Synthesize Conflicting Information from Data Visualizations?",
                        "contributors": [
                            "Mr. Prateek Mantri",
                            "Cindy Xiong"
                        ],
                        "authors": [
                            "Prateek Mantri",
                            "Hariharan Subramonyam",
                            "Audrey Michal",
                            "Cindy Xiong"
                        ],
                        "abstract": "Scientific knowledge develops through cumulative discoveries that build on, contradict, contextualize, or correct prior findings. Scientists and journalists often communicate these incremental findings to lay people through visualizations and text (e.g., the positive and negative effects of caffeine intake). Consequently, readers need to integrate diverse and contrasting evidence from multiple sources to form opinions or make decisions. However, the underlying mechanism for synthesizing information from multiple visualizations remains under-explored. To address this knowledge gap, we conducted a series of four experiments (N = 1166) in which participants synthesized empirical evidence from a pair of line charts presented sequentially. In Experiment 1, we administered a baseline condition with charts depicting no specific context where participants held no strong belief. To test for the generalizability, we introduced real-world scenarios to our visualizations in Experiment 2 and added accompanying text descriptions similar to online news articles or blog posts in Experiment 3. In all three experiments, we varied the relative direction and magnitude of line slopes within the chart pairs. We found that participants tended to weigh the positive slope more when the two charts depicted relationships in the opposite direction (e.g., one positive slope and one negative slope). Participants tended to weigh the less steep slope more when the two charts depicted relationships in the same direction (e.g., both positive). Through these experiments, we characterize participants' synthesis behaviors depending on the relationship between the information they viewed, contribute to theories describing underlying cognitive mechanisms in information synthesis, and describe design implications for data storytelling.",
                        "uid": "v-full-1103",
                        "file_name": "v-full-1103_Mantri_Presentation.mp4",
                        "time_stamp": "2022-10-20T20:15:00Z",
                        "time_start": "2022-10-20T20:15:00Z",
                        "time_end": "2022-10-20T20:28:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "623",
                        "paper_award": "",
                        "image_caption": "How do visualization readers synthesize conflicting information from multiple visualizations? We conducted a series of experiments in which participants synthesized empirical evidence from a pair of line charts presented sequeentially. We found that participants tended to weigh the positive slope more when the two charts depicted relationships in the opposite direction (e.g., one positive slope and one negative slope).",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "https://youtu.be/nudl0CYXZBU",
                        "ff_id": "nudl0CYXZBU"
                    },
                    {
                        "slot_id": "v-full-1103-qa",
                        "session_id": "full10",
                        "type": "In Person Q+A",
                        "title": "How Do Viewers Synthesize Conflicting Information from Data Visualizations? (Q+A)",
                        "contributors": [
                            "Mr. Prateek Mantri",
                            "Cindy Xiong"
                        ],
                        "authors": [],
                        "abstract": "Scientific knowledge develops through cumulative discoveries that build on, contradict, contextualize, or correct prior findings. Scientists and journalists often communicate these incremental findings to lay people through visualizations and text (e.g., the positive and negative effects of caffeine intake). Consequently, readers need to integrate diverse and contrasting evidence from multiple sources to form opinions or make decisions. However, the underlying mechanism for synthesizing information from multiple visualizations remains under-explored. To address this knowledge gap, we conducted a series of four experiments (N = 1166) in which participants synthesized empirical evidence from a pair of line charts presented sequentially. In Experiment 1, we administered a baseline condition with charts depicting no specific context where participants held no strong belief. To test for the generalizability, we introduced real-world scenarios to our visualizations in Experiment 2 and added accompanying text descriptions similar to online news articles or blog posts in Experiment 3. In all three experiments, we varied the relative direction and magnitude of line slopes within the chart pairs. We found that participants tended to weigh the positive slope more when the two charts depicted relationships in the opposite direction (e.g., one positive slope and one negative slope). Participants tended to weigh the less steep slope more when the two charts depicted relationships in the same direction (e.g., both positive). Through these experiments, we characterize participants' synthesis behaviors depending on the relationship between the information they viewed, contribute to theories describing underlying cognitive mechanisms in information synthesis, and describe design implications for data storytelling.",
                        "uid": "v-full-1103",
                        "file_name": "",
                        "time_stamp": "2022-10-20T20:28:00Z",
                        "time_start": "2022-10-20T20:28:00Z",
                        "time_end": "2022-10-20T20:30:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "623",
                        "paper_award": "",
                        "image_caption": "How do visualization readers synthesize conflicting information from multiple visualizations? We conducted a series of experiments in which participants synthesized empirical evidence from a pair of line charts presented sequeentially. We found that participants tended to weigh the positive slope more when the two charts depicted relationships in the opposite direction (e.g., one positive slope and one negative slope).",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "https://youtu.be/nudl0CYXZBU",
                        "ff_id": "nudl0CYXZBU"
                    }
                ]
            },
            {
                "title": "Interactive Dimensionality (High Dimensional Data)",
                "session_id": "full11",
                "event_prefix": "v-full",
                "track": "ok5",
                "livestream_id": "ok5-thu",
                "session_image": "full11.png",
                "chair": [
                    "Lars Linsen"
                ],
                "organizers": [],
                "time_start": "2022-10-20T14:00:00Z",
                "time_end": "2022-10-20T15:15:00Z",
                "discord_category": "",
                "discord_channel": "oklahoma-st-5",
                "discord_channel_id": "1024600839295356939",
                "discord_link": "https://discord.com/channels/978320435554947082/1024600839295356939",
                "slido_link": "https://app.sli.do/event/i28UETZaiRWr1XwX6eKjcf?section=d355f89b-fbd2-4abf-9958-741607905551",
                "youtube_url": "https://youtu.be/JCJlogloJH8",
                "youtube_id": "JCJlogloJH8",
                "zoom_meeting": "",
                "zoom_password": "",
                "zoom_link": "",
                "ff_link": "https://youtu.be/C48KlidqBrY",
                "ff_playlist": "",
                "time_slots": [
                    {
                        "slot_id": "full11-opening",
                        "session_id": "full11",
                        "type": "In Person Other",
                        "title": "Session Opening",
                        "contributors": [
                            "Lars Linsen"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "",
                        "file_name": "",
                        "time_stamp": "2022-10-20T14:00:00Z",
                        "time_start": "2022-10-20T14:00:00Z",
                        "time_end": "2022-10-20T14:00:00Z",
                        "paper_type": "",
                        "keywords": [],
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "v-full-1525-pres",
                        "session_id": "full11",
                        "type": "Virtual Presentation (pre-recorded)",
                        "title": "RankAxis: Towards a Systematic Combination of Projection and Ranking in Multi-Attribute Data Exploration",
                        "contributors": [
                            "Quan Li"
                        ],
                        "authors": [
                            "Qiangqiang Liu",
                            "Yukun Ren",
                            "Zhihua Zhu",
                            "Dai Li",
                            "Xiaojuan Ma",
                            "Quan Li"
                        ],
                        "abstract": "Projection and ranking are frequently used analysis techniques in multi-attribute data exploration. Both families of techniques help analysts with tasks such as identifying similarities between observations and determining ordered subgroups, and have shown good performances in multi-attribute data exploration. However, they often exhibit problems such as distorted projection layouts, obscure semantic interpretations, and non-intuitive effects produced by selecting a subset of (weighted) attributes. Moreover, few studies have attempted to combine projection and ranking into the same exploration space to complement each other's strengths and weaknesses. For this reason, we propose RankAxis, a visual analytics system that systematically combines projection and ranking to facilitate the mutual interpretation of these two techniques and jointly support multi-attribute data exploration. A real-world case study, expert feedback, and a user study demonstrate the efficacy of RankAxis.",
                        "uid": "v-full-1525",
                        "file_name": "v-full-1525_Qiangqiang_Presentation.mp4",
                        "time_stamp": "2022-10-20T14:00:00Z",
                        "time_start": "2022-10-20T14:00:00Z",
                        "time_end": "2022-10-20T14:09:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "544",
                        "paper_award": "",
                        "image_caption": "(A) The data loader facilitates data selection; (B) The interactive projection view shows the projection distribution and guides analysts to explore the projection layout and directional semantics; (C1 \u2013 C5) The ranking tabular view summarizes the attribute contributions to the ranking and supports the deduction of the attribute weights based on user interaction, as well as compares different ranking schemes; (D) The comparative projection view analyzes the distribution of observations generated by different ranking schemes; (E) The ranking projection axis view compares the results of projection and ranking in the same context.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "https://youtu.be/NnsJSS34pPI",
                        "ff_id": "NnsJSS34pPI"
                    },
                    {
                        "slot_id": "v-full-1525-qa",
                        "session_id": "full11",
                        "type": "Virtual Q+A",
                        "title": "RankAxis: Towards a Systematic Combination of Projection and Ranking in Multi-Attribute Data Exploration (Q+A)",
                        "contributors": [
                            "Quan Li"
                        ],
                        "authors": [],
                        "abstract": "Projection and ranking are frequently used analysis techniques in multi-attribute data exploration. Both families of techniques help analysts with tasks such as identifying similarities between observations and determining ordered subgroups, and have shown good performances in multi-attribute data exploration. However, they often exhibit problems such as distorted projection layouts, obscure semantic interpretations, and non-intuitive effects produced by selecting a subset of (weighted) attributes. Moreover, few studies have attempted to combine projection and ranking into the same exploration space to complement each other's strengths and weaknesses. For this reason, we propose RankAxis, a visual analytics system that systematically combines projection and ranking to facilitate the mutual interpretation of these two techniques and jointly support multi-attribute data exploration. A real-world case study, expert feedback, and a user study demonstrate the efficacy of RankAxis.",
                        "uid": "v-full-1525",
                        "file_name": "",
                        "time_stamp": "2022-10-20T14:09:00Z",
                        "time_start": "2022-10-20T14:09:00Z",
                        "time_end": "2022-10-20T14:12:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "544",
                        "paper_award": "",
                        "image_caption": "(A) The data loader facilitates data selection; (B) The interactive projection view shows the projection distribution and guides analysts to explore the projection layout and directional semantics; (C1 \u2013 C5) The ranking tabular view summarizes the attribute contributions to the ranking and supports the deduction of the attribute weights based on user interaction, as well as compares different ranking schemes; (D) The comparative projection view analyzes the distribution of observations generated by different ranking schemes; (E) The ranking projection axis view compares the results of projection and ranking in the same context.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "https://youtu.be/NnsJSS34pPI",
                        "ff_id": "NnsJSS34pPI"
                    },
                    {
                        "slot_id": "v-full-1631-pres",
                        "session_id": "full11",
                        "type": "In Person Presentation",
                        "title": "PC-Expo: A Metrics-Based Interactive Axes Reordering Method for Parallel Coordinate Displays",
                        "contributors": [
                            "Anjul Kumar Tyagi"
                        ],
                        "authors": [
                            "Anjul Kumar Tyagi",
                            "Tyler Estro",
                            "Geoff Kuenning",
                            "Erez Zadok",
                            "Klaus Mueller"
                        ],
                        "abstract": "Parallel coordinate plots (PCPs) have been widely used for high-dimensional (HD) data storytelling because they allow presenting a large number of dimensions without distortions. The axes ordering in PCP presents a particular story from the data based\n on the user perception of PCP polylines. Existing works focus on directly optimizing for PCP axes ordering based on some common analysis tasks like clustering, neighborhood, and correlation. However, direct optimization for PCP axes based on these common properties is restrictive because it does not account for multiple properties occurring between the axes, and for local properties that occur in small regions in the data. Also, many of these techniques do not support the human-in-the-loop (HIL) paradigm, which is crucial (i) for explainability and (ii) in cases where no single reordering scheme fits the users\u2019 goals. To alleviate these problems, we\n present PC-Expo, a real-time visual analytics framework for all-in-one PCP line pattern detection, and axes reordering. We studied the connection of line patterns in PCPs with different data analysis tasks and datasets. PC-Expo expands prior work on PCP axes\n reordering by developing real-time, local detection schemes for the 12 most common analysis tasks (properties). Users can choose the story they want to present with PCPs by optimizing directly over their choice of properties. These properties can be ranked, or\n combined using individual weights, creating a custom optimization scheme for axes reordering. Users can control the granularity at which they want to work with their detection scheme in the data, allowing exploration of local regions. PC-Expo also supports HIL axes reordering via local-property visualization, which shows the regions of granular activity for every axis pair. Local-property visualization is helpful for PCP axes reordering based on multiple properties, when no single reordering scheme fits the user goals. A comprehensive evaluation done with real users and diverse datasets confirms the efficacy of PC-Expo in data storytelling with PCPs.",
                        "uid": "v-full-1631",
                        "file_name": "v-full-1631_Tyagi_Presentation.mp4",
                        "time_stamp": "2022-10-20T14:12:00Z",
                        "time_start": "2022-10-20T14:12:00Z",
                        "time_end": "2022-10-20T14:21:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "265",
                        "paper_award": "",
                        "image_caption": "PC-Expo is a parallel coordinate axis reordering system supporting real-time local line patter detections. Users can reorder the axis based on many different line properties to represent any story in the data.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "https://youtu.be/TjxnMIOaJo4",
                        "ff_id": "TjxnMIOaJo4"
                    },
                    {
                        "slot_id": "v-full-1631-qa",
                        "session_id": "full11",
                        "type": "In Person Q+A",
                        "title": "PC-Expo: A Metrics-Based Interactive Axes Reordering Method for Parallel Coordinate Displays (Q+A)",
                        "contributors": [
                            "Anjul Kumar Tyagi"
                        ],
                        "authors": [],
                        "abstract": "Parallel coordinate plots (PCPs) have been widely used for high-dimensional (HD) data storytelling because they allow presenting a large number of dimensions without distortions. The axes ordering in PCP presents a particular story from the data based\n on the user perception of PCP polylines. Existing works focus on directly optimizing for PCP axes ordering based on some common analysis tasks like clustering, neighborhood, and correlation. However, direct optimization for PCP axes based on these common properties is restrictive because it does not account for multiple properties occurring between the axes, and for local properties that occur in small regions in the data. Also, many of these techniques do not support the human-in-the-loop (HIL) paradigm, which is crucial (i) for explainability and (ii) in cases where no single reordering scheme fits the users\u2019 goals. To alleviate these problems, we\n present PC-Expo, a real-time visual analytics framework for all-in-one PCP line pattern detection, and axes reordering. We studied the connection of line patterns in PCPs with different data analysis tasks and datasets. PC-Expo expands prior work on PCP axes\n reordering by developing real-time, local detection schemes for the 12 most common analysis tasks (properties). Users can choose the story they want to present with PCPs by optimizing directly over their choice of properties. These properties can be ranked, or\n combined using individual weights, creating a custom optimization scheme for axes reordering. Users can control the granularity at which they want to work with their detection scheme in the data, allowing exploration of local regions. PC-Expo also supports HIL axes reordering via local-property visualization, which shows the regions of granular activity for every axis pair. Local-property visualization is helpful for PCP axes reordering based on multiple properties, when no single reordering scheme fits the user goals. A comprehensive evaluation done with real users and diverse datasets confirms the efficacy of PC-Expo in data storytelling with PCPs.",
                        "uid": "v-full-1631",
                        "file_name": "",
                        "time_stamp": "2022-10-20T14:21:00Z",
                        "time_start": "2022-10-20T14:21:00Z",
                        "time_end": "2022-10-20T14:24:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "265",
                        "paper_award": "",
                        "image_caption": "PC-Expo is a parallel coordinate axis reordering system supporting real-time local line patter detections. Users can reorder the axis based on many different line properties to represent any story in the data.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "https://youtu.be/TjxnMIOaJo4",
                        "ff_id": "TjxnMIOaJo4"
                    },
                    {
                        "slot_id": "v-full-1359-pres",
                        "session_id": "full11",
                        "type": "Virtual Presentation (pre-recorded)",
                        "title": "Incorporation of Human Knowledge into Data Embeddings to Improve Pattern Significance and Interpretability",
                        "contributors": [
                            "Jie Li"
                        ],
                        "authors": [
                            "Jie Li",
                            "Chunqi Zhou"
                        ],
                        "abstract": "Embedding is a common technique for analyzing multi-dimensional data. However, the embedding projection cannot always form significant and interpretable visual structures that foreshadow underlying data patterns. We propose an approach that incorporates human knowledge into data embeddings to improve pattern significance and interpretability. The core idea is (1) externalizing tacit human knowledge as explicit sample labels and (2) adding a classification loss in the embedding network to encode samples\u2019 classes. The approach pulls samples of the same class with similar data features closer in the projection, leading to more compact (significant) and class-consistent (interpretable) visual structures. We give an embedding network with a customized classification loss to implement the idea and integrate the network into a visualization system to form a workflow that supports flexible class creation and pattern exploration. Patterns found on open datasets in case studies, subjects\u2019 performance in a user study, and quantitative experiment results illustrate the general usability and effectiveness of the approach.",
                        "uid": "v-full-1359",
                        "file_name": "v-full-1359_Li_Presentation.mp4",
                        "time_stamp": "2022-10-20T14:24:00Z",
                        "time_start": "2022-10-20T14:24:00Z",
                        "time_end": "2022-10-20T14:33:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "571",
                        "paper_award": "",
                        "image_caption": "The user is using the knowledge-based visualization system to analyze the Covid-19 dataset. Visual structures (outliers and clusters) become distinguishable by incorporating human knowledge, making pattern discovery and understanding easy.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "https://youtu.be/LUmmzKTx-rM",
                        "ff_id": "LUmmzKTx-rM"
                    },
                    {
                        "slot_id": "v-full-1359-qa",
                        "session_id": "full11",
                        "type": "Virtual Q+A",
                        "title": "Incorporation of Human Knowledge into Data Embeddings to Improve Pattern Significance and Interpretability (Q+A)",
                        "contributors": [
                            "Jie Li"
                        ],
                        "authors": [],
                        "abstract": "Embedding is a common technique for analyzing multi-dimensional data. However, the embedding projection cannot always form significant and interpretable visual structures that foreshadow underlying data patterns. We propose an approach that incorporates human knowledge into data embeddings to improve pattern significance and interpretability. The core idea is (1) externalizing tacit human knowledge as explicit sample labels and (2) adding a classification loss in the embedding network to encode samples\u2019 classes. The approach pulls samples of the same class with similar data features closer in the projection, leading to more compact (significant) and class-consistent (interpretable) visual structures. We give an embedding network with a customized classification loss to implement the idea and integrate the network into a visualization system to form a workflow that supports flexible class creation and pattern exploration. Patterns found on open datasets in case studies, subjects\u2019 performance in a user study, and quantitative experiment results illustrate the general usability and effectiveness of the approach.",
                        "uid": "v-full-1359",
                        "file_name": "",
                        "time_stamp": "2022-10-20T14:33:00Z",
                        "time_start": "2022-10-20T14:33:00Z",
                        "time_end": "2022-10-20T14:36:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "571",
                        "paper_award": "",
                        "image_caption": "The user is using the knowledge-based visualization system to analyze the Covid-19 dataset. Visual structures (outliers and clusters) become distinguishable by incorporating human knowledge, making pattern discovery and understanding easy.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "https://youtu.be/LUmmzKTx-rM",
                        "ff_id": "LUmmzKTx-rM"
                    },
                    {
                        "slot_id": "v-full-1350-pres",
                        "session_id": "full11",
                        "type": "Virtual Presentation (pre-recorded)",
                        "title": "Interactive Visual Cluster Analysis by Contrastive Dimensionality Reduction",
                        "contributors": [
                            "Jiazhi Xia",
                            "Linquan Huang"
                        ],
                        "authors": [
                            "Jiazhi Xia",
                            "Linquan Huang",
                            "Weixing Lin",
                            "Xin Zhao",
                            "Jing Wu",
                            "Yang Chen",
                            "Ying Zhao",
                            "Wei Chen"
                        ],
                        "abstract": "We propose a contrastive dimensionality reduction approach (CDR) for interactive visual cluster analysis. Although dimensionality reduction of high-dimensional data is widely used in visual cluster analysis in conjunction with scatterplots, there are several limitations on effective visual cluster analysis. First, it is non-trivial for an embedding to present clear visual cluster separation when keeping neighborhood structures. Second, as cluster analysis is a subjective task, user steering is required. However, it is also non-trivial to enable interactions in dimensionality reduction. To tackle these problems, we introduce contrastive learning into dimensionality reduction for high-quality embedding. We then redefine the gradient of the loss function to the negative pairs to enhance the visual cluster separation of embedding results. Based on the contrastive learning scheme, we employ link-based interactions to steer embeddings. After that, we implement a prototype visual interface that integrates the proposed algorithms and a set of visualizations. Quantitative experiments demonstrate that CDR outperforms existing techniques in terms of preserving correct neighborhood structures and improving visual cluster separation. The ablation experiment demonstrates the effectiveness of gradient redefinition. The user study verifies that CDR outperforms t-SNE and UMAP in the task of cluster identification. We also showcase two use cases on real-world datasets to present the effectiveness of link-based interactions.",
                        "uid": "v-full-1350",
                        "file_name": "v-full-1350_Xia_Presentation.mp4",
                        "time_stamp": "2022-10-20T14:36:00Z",
                        "time_start": "2022-10-20T14:36:00Z",
                        "time_end": "2022-10-20T14:45:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "642",
                        "paper_award": "",
                        "image_caption": "The embedding results by dimensionality reduction techniques. Top: the embedding results of the Indian Food dataset by (a) ISOMAP, (b) t-SNE, (c) UMAP, and (d) CDR (the Contrastive Dimensionality Reduction), respectively. The data points are color-encoded by class labels. Bottom: the interactive analysis of the Animals dataset by CDR. (e) The initial embedding result. (f) A must link is added to merge the butterfly clusters.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "https://youtu.be/jCzKrn_Sins",
                        "ff_id": "jCzKrn_Sins"
                    },
                    {
                        "slot_id": "v-full-1350-qa",
                        "session_id": "full11",
                        "type": "Virtual Q+A",
                        "title": "Interactive Visual Cluster Analysis by Contrastive Dimensionality Reduction (Q+A)",
                        "contributors": [
                            "Jiazhi Xia",
                            "Linquan Huang"
                        ],
                        "authors": [],
                        "abstract": "We propose a contrastive dimensionality reduction approach (CDR) for interactive visual cluster analysis. Although dimensionality reduction of high-dimensional data is widely used in visual cluster analysis in conjunction with scatterplots, there are several limitations on effective visual cluster analysis. First, it is non-trivial for an embedding to present clear visual cluster separation when keeping neighborhood structures. Second, as cluster analysis is a subjective task, user steering is required. However, it is also non-trivial to enable interactions in dimensionality reduction. To tackle these problems, we introduce contrastive learning into dimensionality reduction for high-quality embedding. We then redefine the gradient of the loss function to the negative pairs to enhance the visual cluster separation of embedding results. Based on the contrastive learning scheme, we employ link-based interactions to steer embeddings. After that, we implement a prototype visual interface that integrates the proposed algorithms and a set of visualizations. Quantitative experiments demonstrate that CDR outperforms existing techniques in terms of preserving correct neighborhood structures and improving visual cluster separation. The ablation experiment demonstrates the effectiveness of gradient redefinition. The user study verifies that CDR outperforms t-SNE and UMAP in the task of cluster identification. We also showcase two use cases on real-world datasets to present the effectiveness of link-based interactions.",
                        "uid": "v-full-1350",
                        "file_name": "",
                        "time_stamp": "2022-10-20T14:45:00Z",
                        "time_start": "2022-10-20T14:45:00Z",
                        "time_end": "2022-10-20T14:48:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "642",
                        "paper_award": "",
                        "image_caption": "The embedding results by dimensionality reduction techniques. Top: the embedding results of the Indian Food dataset by (a) ISOMAP, (b) t-SNE, (c) UMAP, and (d) CDR (the Contrastive Dimensionality Reduction), respectively. The data points are color-encoded by class labels. Bottom: the interactive analysis of the Animals dataset by CDR. (e) The initial embedding result. (f) A must link is added to merge the butterfly clusters.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "https://youtu.be/jCzKrn_Sins",
                        "ff_id": "jCzKrn_Sins"
                    },
                    {
                        "slot_id": "v-full-1447-pres",
                        "session_id": "full11",
                        "type": "In Person Presentation",
                        "title": "Predicting User Preferences of Dimensionality Reduction Embedding Quality",
                        "contributors": [
                            "Cristina Morariu"
                        ],
                        "authors": [
                            "Cristina Morariu",
                            "Adrien Bibal",
                            "Rene Cutura",
                            "Benoit Frenay",
                            "Michael Sedlmair"
                        ],
                        "abstract": "A plethora of dimensionality reduction techniques have emerged over the past decades, leaving researchers and analysts with a wide variety of choices for reducing their data, all the more so given some techniques come with additional hyper-parametrization (e.g., t-SNE, UMAP, etc.). Recent studies are showing that people often use dimensionality reduction as a black-box regardless of the specific properties the method itself preserves. Hence, evaluating and comparing 2D embeddings is usually qualitatively decided, by setting embeddings side-by-side and letting human judgment decide which embedding is the best. In this work, we propose a quantitative way of evaluating embeddings, that nonetheless places human perception at the center. We run a comparative study, where we ask people to select \"good'' and \"misleading'' views between scatterplots of low-dimensional embeddings of image datasets, simulating the way people usually select embeddings. We use the study data as labels for a set of quality metrics for a supervised machine learning model whose purpose is to discover and quantify what exactly people are looking for when deciding between embeddings. With the model as a proxy for human judgments, we use it to rank embeddings on new datasets, explain why they are relevant, and quantify the degree of subjectivity when people select preferred embeddings.",
                        "uid": "v-full-1447",
                        "file_name": "v-full-1447_Morariu_Presentation.mp4",
                        "time_stamp": "2022-10-20T14:48:00Z",
                        "time_start": "2022-10-20T14:48:00Z",
                        "time_end": "2022-10-20T14:57:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "583",
                        "paper_award": "",
                        "image_caption": "Overview of the best embeddings, as predicted by our method for the \"Photography of Flowers\" Dataset. All embeddings we generated are visualized on the metamap on the left hand side, with very good ones displayed against a blue background, while the very good ones displayed against red background. On the left, the top 3 highest rated embeddings are visualized as scatterplots.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "https://youtu.be/x6qjUoEUpIc",
                        "ff_id": "x6qjUoEUpIc"
                    },
                    {
                        "slot_id": "v-full-1447-qa",
                        "session_id": "full11",
                        "type": "In Person Q+A",
                        "title": "Predicting User Preferences of Dimensionality Reduction Embedding Quality (Q+A)",
                        "contributors": [
                            "Cristina Morariu"
                        ],
                        "authors": [],
                        "abstract": "A plethora of dimensionality reduction techniques have emerged over the past decades, leaving researchers and analysts with a wide variety of choices for reducing their data, all the more so given some techniques come with additional hyper-parametrization (e.g., t-SNE, UMAP, etc.). Recent studies are showing that people often use dimensionality reduction as a black-box regardless of the specific properties the method itself preserves. Hence, evaluating and comparing 2D embeddings is usually qualitatively decided, by setting embeddings side-by-side and letting human judgment decide which embedding is the best. In this work, we propose a quantitative way of evaluating embeddings, that nonetheless places human perception at the center. We run a comparative study, where we ask people to select \"good'' and \"misleading'' views between scatterplots of low-dimensional embeddings of image datasets, simulating the way people usually select embeddings. We use the study data as labels for a set of quality metrics for a supervised machine learning model whose purpose is to discover and quantify what exactly people are looking for when deciding between embeddings. With the model as a proxy for human judgments, we use it to rank embeddings on new datasets, explain why they are relevant, and quantify the degree of subjectivity when people select preferred embeddings.",
                        "uid": "v-full-1447",
                        "file_name": "",
                        "time_stamp": "2022-10-20T14:57:00Z",
                        "time_start": "2022-10-20T14:57:00Z",
                        "time_end": "2022-10-20T15:00:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "583",
                        "paper_award": "",
                        "image_caption": "Overview of the best embeddings, as predicted by our method for the \"Photography of Flowers\" Dataset. All embeddings we generated are visualized on the metamap on the left hand side, with very good ones displayed against a blue background, while the very good ones displayed against red background. On the left, the top 3 highest rated embeddings are visualized as scatterplots.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "https://youtu.be/x6qjUoEUpIc",
                        "ff_id": "x6qjUoEUpIc"
                    },
                    {
                        "slot_id": "v-tvcg-9382912-pres",
                        "session_id": "full11",
                        "type": "In Person Presentation",
                        "title": "VERTIGo: A Visual Platform for Querying and Exploring Large Multilayer Networks",
                        "contributors": [
                            "Erick Cuenca"
                        ],
                        "authors": [
                            "Erick Cuenca",
                            "Arnaud Sallaberry",
                            "Dino Ienco",
                            "Pascal Poncelet"
                        ],
                        "abstract": "Many real world data can be modeled by a graph with a set of nodes interconnected to each other by multiple relationships. Such a rich graph is called multilayer graph or network. Providing useful visualization tools to support the query process for such graphs is challenging. Although many approaches have addressed the visual query construction, few efforts have been done to provide a contextualized exploration of query results and suggestion strategies to refine the original query. This is due to several issues such as i) the size of the graphs ii) the large number of retrieved results and iii) the way they can be organized to facilitate their exploration. In this paper, we present VERTIGo, a novel visual platform to query, explore and support the analysis of large multilayer graphs. VERTIGo provides coordinated views to navigate and explore the large set of retrieved results at different granularity levels. In addition, the proposed system supports the refinement of the query by visual suggestions to guide the user through the exploration process. Two examples and a user study demonstrate how VERTIGo can be used to perform visual analysis query, exploration, and suggestion) on real world multilayer networks.",
                        "uid": "v-tvcg-9382912",
                        "file_name": "v-tvcg-9382912_Cuenca_Presentation.mp4",
                        "time_stamp": "2022-10-20T15:00:00Z",
                        "time_start": "2022-10-20T15:00:00Z",
                        "time_end": "2022-10-20T15:09:00Z",
                        "paper_type": "full",
                        "keywords": [
                            "Visual Querying System, Visual Pattern Suggestion, Multilayer Networks"
                        ],
                        "has_image": "1",
                        "has_video": "547",
                        "paper_award": "",
                        "image_caption": "The VERTIGo\u2019s user interface being used to explore a co-authorship network. (a) The Query view allows the users to build the query and start the process to retrieve the results, called embeddings. This view also supports query suggestions visualized as pie charts. (b)(c) The Graph view showing either an overview of the embedding locations with a heatmap representation (b) or the embedding relations with Kelp-based diagrams (c). (d) The Embeddings view shows a list of embeddings for a set of selected entities. (e) A histogram allows filtering embeddings by their Minimum Bounding Rectangle values.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "https://youtu.be/dMsaPw3_tgE",
                        "ff_id": "dMsaPw3_tgE"
                    },
                    {
                        "slot_id": "v-tvcg-9382912-qa",
                        "session_id": "full11",
                        "type": "In Person Q+A",
                        "title": "VERTIGo: A Visual Platform for Querying and Exploring Large Multilayer Networks (Q+A)",
                        "contributors": [
                            "Erick Cuenca"
                        ],
                        "authors": [],
                        "abstract": "Many real world data can be modeled by a graph with a set of nodes interconnected to each other by multiple relationships. Such a rich graph is called multilayer graph or network. Providing useful visualization tools to support the query process for such graphs is challenging. Although many approaches have addressed the visual query construction, few efforts have been done to provide a contextualized exploration of query results and suggestion strategies to refine the original query. This is due to several issues such as i) the size of the graphs ii) the large number of retrieved results and iii) the way they can be organized to facilitate their exploration. In this paper, we present VERTIGo, a novel visual platform to query, explore and support the analysis of large multilayer graphs. VERTIGo provides coordinated views to navigate and explore the large set of retrieved results at different granularity levels. In addition, the proposed system supports the refinement of the query by visual suggestions to guide the user through the exploration process. Two examples and a user study demonstrate how VERTIGo can be used to perform visual analysis query, exploration, and suggestion) on real world multilayer networks.",
                        "uid": "v-tvcg-9382912",
                        "file_name": "",
                        "time_stamp": "2022-10-20T15:09:00Z",
                        "time_start": "2022-10-20T15:09:00Z",
                        "time_end": "2022-10-20T15:12:00Z",
                        "paper_type": "full",
                        "keywords": [
                            "Visual Querying System, Visual Pattern Suggestion, Multilayer Networks"
                        ],
                        "has_image": "1",
                        "has_video": "547",
                        "paper_award": "",
                        "image_caption": "The VERTIGo\u2019s user interface being used to explore a co-authorship network. (a) The Query view allows the users to build the query and start the process to retrieve the results, called embeddings. This view also supports query suggestions visualized as pie charts. (b)(c) The Graph view showing either an overview of the embedding locations with a heatmap representation (b) or the embedding relations with Kelp-based diagrams (c). (d) The Embeddings view shows a list of embeddings for a set of selected entities. (e) A histogram allows filtering embeddings by their Minimum Bounding Rectangle values.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "https://youtu.be/dMsaPw3_tgE",
                        "ff_id": "dMsaPw3_tgE"
                    }
                ]
            },
            {
                "title": "Natural Language Interaction",
                "session_id": "full12",
                "event_prefix": "v-full",
                "track": "ok4",
                "livestream_id": "ok4-fri",
                "session_image": "full12.png",
                "chair": [
                    "John Stasko"
                ],
                "organizers": [],
                "time_start": "2022-10-21T14:00:00Z",
                "time_end": "2022-10-21T15:15:00Z",
                "discord_category": "",
                "discord_channel": "oklahoma-st-4",
                "discord_channel_id": "1024600674924765264",
                "discord_link": "https://discord.com/channels/978320435554947082/1024600674924765264",
                "slido_link": "https://app.sli.do/event/i28UETZaiRWr1XwX6eKjcf?section=51658b34-0f5b-40c9-a335-1fd1468fad8d",
                "youtube_url": "https://youtu.be/XhPpTVMgmhU",
                "youtube_id": "XhPpTVMgmhU",
                "zoom_meeting": "",
                "zoom_password": "",
                "zoom_link": "",
                "ff_link": "https://youtu.be/zwl0LEc_I6A",
                "ff_playlist": "",
                "time_slots": [
                    {
                        "slot_id": "full12-opening",
                        "session_id": "full12",
                        "type": "In Person Other",
                        "title": "Session Opening",
                        "contributors": [
                            "John Stasko"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "",
                        "file_name": "",
                        "time_stamp": "2022-10-21T14:00:00Z",
                        "time_start": "2022-10-21T14:00:00Z",
                        "time_end": "2022-10-21T14:00:00Z",
                        "paper_type": "",
                        "keywords": [],
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "v-full-1653-pres",
                        "session_id": "full12",
                        "type": "In Person Presentation",
                        "title": "Probablement, Wahrscheinlich, Likely ? A Cross-Language Study of How People Verbalize Probabilities in Icon Array Visualizations",
                        "contributors": [
                            "No\u00eblle Rakotondravony"
                        ],
                        "authors": [
                            "No\u00eblle Rakotondravony",
                            "Yiren Ding",
                            "Lane Harrison"
                        ],
                        "abstract": "Visualizations today are used across a wide range of languages and cultures. Yet the extent to which language impacts how we reason about data and visualizations remains unclear. In this paper, we explore the intersection of visualization and language through a cross-language study on estimative probability tasks with icon-array visualizations. Across Arabic, English, French, German, and Mandarin, n = 50 participants per language both chose probability expressions \u2014 e.g. likely, probable \u2014 to describe icon-array visualizations (Vis-to-Expression), and drew icon-array visualizations to match a given expression (Expression-to-Vis). Results suggest that there is no clear one-to-one mapping of probability expressions and associated visual ranges between languages. Several translated expressions fell significantly above or below the range of the corresponding English expressions. Compared to other languages, French and German respondents appear to exhibit high levels of consistency between the visualizations they drew and the words they chose. Participants across languages used similar words when describing scenarios above 80% chance, with more variance in expressions targeting mid-range and lower values. We discuss how these results suggest potential differences in the expressiveness of language as it relates to visualization interpretation and design goals, as well as practical implications for translation efforts and future studies at the intersection of languages, culture, and visualization. Experiment data, source code, and analysis scripts are available at the following repository: https://osf.io/g5d4r/",
                        "uid": "v-full-1653",
                        "file_name": "v-full-1653_Rakotondravony_Presentation.mp4",
                        "time_stamp": "2022-10-21T14:00:00Z",
                        "time_start": "2022-10-21T14:00:00Z",
                        "time_end": "2022-10-21T14:09:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "504",
                        "paper_award": "",
                        "image_caption": "Two charts showing results from two experiments. \nLeft: Expression-to-Vis experiment. A user icon, an icon array with 100 gray icons, an arrow pointing to a second icon array with 67 icons in orange are above a graph that shows the 95% confidence interval of the values drawn on the icon arrays on the x-axis. The list of probability expressions is on the y-axis. Five 95% confidence intervals representing the range of drawn values in the five languages are displayed for each probability expression.\nRight: Vis-to-Expression experiment. A user icon, an icon array with 35 orange icons, and a list of probability expressions with the expression 'likely' highlighted are above a confidence interval graph that shows the value appearing on the icon array on the x-axis. The list of probability expressions is on the y-axis. Five 95% confidence intervals representing the range of drawn values in the five languages are displayed for each probability expression.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "https://youtu.be/UDqqAiqRhXI",
                        "ff_id": "UDqqAiqRhXI"
                    },
                    {
                        "slot_id": "v-full-1653-qa",
                        "session_id": "full12",
                        "type": "In Person Q+A",
                        "title": "Probablement, Wahrscheinlich, Likely ? A Cross-Language Study of How People Verbalize Probabilities in Icon Array Visualizations (Q+A)",
                        "contributors": [
                            "No\u00eblle Rakotondravony"
                        ],
                        "authors": [],
                        "abstract": "Visualizations today are used across a wide range of languages and cultures. Yet the extent to which language impacts how we reason about data and visualizations remains unclear. In this paper, we explore the intersection of visualization and language through a cross-language study on estimative probability tasks with icon-array visualizations. Across Arabic, English, French, German, and Mandarin, n = 50 participants per language both chose probability expressions \u2014 e.g. likely, probable \u2014 to describe icon-array visualizations (Vis-to-Expression), and drew icon-array visualizations to match a given expression (Expression-to-Vis). Results suggest that there is no clear one-to-one mapping of probability expressions and associated visual ranges between languages. Several translated expressions fell significantly above or below the range of the corresponding English expressions. Compared to other languages, French and German respondents appear to exhibit high levels of consistency between the visualizations they drew and the words they chose. Participants across languages used similar words when describing scenarios above 80% chance, with more variance in expressions targeting mid-range and lower values. We discuss how these results suggest potential differences in the expressiveness of language as it relates to visualization interpretation and design goals, as well as practical implications for translation efforts and future studies at the intersection of languages, culture, and visualization. Experiment data, source code, and analysis scripts are available at the following repository: https://osf.io/g5d4r/",
                        "uid": "v-full-1653",
                        "file_name": "",
                        "time_stamp": "2022-10-21T14:09:00Z",
                        "time_start": "2022-10-21T14:09:00Z",
                        "time_end": "2022-10-21T14:12:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "504",
                        "paper_award": "",
                        "image_caption": "Two charts showing results from two experiments. \nLeft: Expression-to-Vis experiment. A user icon, an icon array with 100 gray icons, an arrow pointing to a second icon array with 67 icons in orange are above a graph that shows the 95% confidence interval of the values drawn on the icon arrays on the x-axis. The list of probability expressions is on the y-axis. Five 95% confidence intervals representing the range of drawn values in the five languages are displayed for each probability expression.\nRight: Vis-to-Expression experiment. A user icon, an icon array with 35 orange icons, and a list of probability expressions with the expression 'likely' highlighted are above a confidence interval graph that shows the value appearing on the icon array on the x-axis. The list of probability expressions is on the y-axis. Five 95% confidence intervals representing the range of drawn values in the five languages are displayed for each probability expression.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "https://youtu.be/UDqqAiqRhXI",
                        "ff_id": "UDqqAiqRhXI"
                    },
                    {
                        "slot_id": "v-full-1577-pres",
                        "session_id": "full12",
                        "type": "Virtual Presentation (pre-recorded)",
                        "title": "FlowNL: Asking the Flow Data in Natural Languages",
                        "contributors": [
                            "Jun Tao"
                        ],
                        "authors": [
                            "Jieying Huang",
                            "Yang Xi",
                            "Junnan Hu",
                            "Jun Tao"
                        ],
                        "abstract": "Flow visualization is essentially a tool to answer domain experts' questions about flow fields using rendered images. Static flow visualization approaches require domain experts to raise their questions to visualization experts, who develop specific techniques to extract and visualize the flow structures of interest. Interactive visualization approaches allow domain experts to ask the system directly through the visual analytic interface, which provides flexibility to support various tasks. However, in practice, the visual analytic interface may require extra learning effort, which often discourages domain experts and limits its usage in real-world scenarios. In this paper, we propose FlowNL, a novel interactive system with a natural language interface. FlowNL allows users to manipulate the flow visualization system using plain English, which greatly reduces the learning effort. We develop a natural language parser to interpret user intention and translate textual input into a declarative language. We design the declarative language as an intermediate layer between the natural language and the programming language specifically for flow visualization. The declarative language provides selection and composition rules to derive relatively complicated flow structures from primitive objects that encode various kinds of information about scalar fields, flow patterns, regions of interest, connectivities, etc. We demonstrate the effectiveness of FlowNL using multiple usage scenarios and an empirical evaluation.",
                        "uid": "v-full-1577",
                        "file_name": "v-full-1577_Huang_Presentation.mp4",
                        "time_stamp": "2022-10-21T14:12:00Z",
                        "time_start": "2022-10-21T14:12:00Z",
                        "time_end": "2022-10-21T14:21:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "659",
                        "paper_award": "",
                        "image_caption": "Using FLowNL to explore the crayfish dataset. Users can query flow structure in natural languages using the input box (a). They can resolve unknown terms through conversation in the dialog box (b) based on the existing objects (d). FlowNL system will suggest queries based on templates (e) and display the resulting query formula (c). FlowNL visualizes \u201ctiny spiral flow\u201d in orange, \u201cspiral flow\u201d in green, and \u201cupward flow\u201d in purple (f), respectively, based on user queries.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "https://youtu.be/54yeCnVaTdE",
                        "ff_id": "54yeCnVaTdE"
                    },
                    {
                        "slot_id": "v-full-1577-qa",
                        "session_id": "full12",
                        "type": "Virtual Q+A",
                        "title": "FlowNL: Asking the Flow Data in Natural Languages (Q+A)",
                        "contributors": [
                            "Jun Tao"
                        ],
                        "authors": [],
                        "abstract": "Flow visualization is essentially a tool to answer domain experts' questions about flow fields using rendered images. Static flow visualization approaches require domain experts to raise their questions to visualization experts, who develop specific techniques to extract and visualize the flow structures of interest. Interactive visualization approaches allow domain experts to ask the system directly through the visual analytic interface, which provides flexibility to support various tasks. However, in practice, the visual analytic interface may require extra learning effort, which often discourages domain experts and limits its usage in real-world scenarios. In this paper, we propose FlowNL, a novel interactive system with a natural language interface. FlowNL allows users to manipulate the flow visualization system using plain English, which greatly reduces the learning effort. We develop a natural language parser to interpret user intention and translate textual input into a declarative language. We design the declarative language as an intermediate layer between the natural language and the programming language specifically for flow visualization. The declarative language provides selection and composition rules to derive relatively complicated flow structures from primitive objects that encode various kinds of information about scalar fields, flow patterns, regions of interest, connectivities, etc. We demonstrate the effectiveness of FlowNL using multiple usage scenarios and an empirical evaluation.",
                        "uid": "v-full-1577",
                        "file_name": "",
                        "time_stamp": "2022-10-21T14:21:00Z",
                        "time_start": "2022-10-21T14:21:00Z",
                        "time_end": "2022-10-21T14:24:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "659",
                        "paper_award": "",
                        "image_caption": "Using FLowNL to explore the crayfish dataset. Users can query flow structure in natural languages using the input box (a). They can resolve unknown terms through conversation in the dialog box (b) based on the existing objects (d). FlowNL system will suggest queries based on templates (e) and display the resulting query formula (c). FlowNL visualizes \u201ctiny spiral flow\u201d in orange, \u201cspiral flow\u201d in green, and \u201cupward flow\u201d in purple (f), respectively, based on user queries.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "https://youtu.be/54yeCnVaTdE",
                        "ff_id": "54yeCnVaTdE"
                    },
                    {
                        "slot_id": "v-full-1056-pres",
                        "session_id": "full12",
                        "type": "In Person Presentation",
                        "title": "Comparison Conundrum and the Chamber of Visualizations: An Exploration of How Language Influences Visual Design",
                        "contributors": [
                            "Aimen Gaba"
                        ],
                        "authors": [
                            "Aimen Gaba",
                            "Vidya Setlur",
                            "Arjun Srinivasan",
                            "Jane Hoffswell",
                            "Cindy Xiong"
                        ],
                        "abstract": "The language for expressing comparisons is often complex and nuanced, making supporting natural language-based visual comparison a non-trivial task. To better understand how people reason about comparisons in natural language, we explore a design space of utterances for comparing data entities. We identified different parameters of comparison utterances that indicate what is being compared (i.e., data variables and attributes) as well as how these parameters are specified (i.e., explicitly or implicitly). We conducted a user study with sixteen data visualization experts and non-experts to investigate how they designed visualizations for comparisons in our design space. Based on the rich set of visualization techniques observed, we extracted key design features from the visualizations and synthesized them into a subset of sixteen representative visualization designs. We then conducted a follow-up study to validate user preferences for the sixteen representative visualizations corresponding to utterances in our design space. Findings from these studies suggest guidelines and future directions for designing natural language interfaces and recommendation tools to better support natural language comparisons in visual analytics.",
                        "uid": "v-full-1056",
                        "file_name": "v-full-1056_Gaba_Presentation.mp4",
                        "time_stamp": "2022-10-21T14:24:00Z",
                        "time_start": "2022-10-21T14:24:00Z",
                        "time_end": "2022-10-21T14:33:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "512",
                        "paper_award": "",
                        "image_caption": "Four comparison utterances from our design space with varying cardinalities for the comparison entities (1 - 1, 1 - N, N - M, N) and different levels of concreteness (explicit and implicit). Each of these comparison utterances was included in our online survey in which participants ranked their preference for the different visualization types; the most preferred visualization(s) have a colored border.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "https://youtu.be/XYBfY4IU-CI",
                        "ff_id": "XYBfY4IU-CI"
                    },
                    {
                        "slot_id": "v-full-1056-qa",
                        "session_id": "full12",
                        "type": "In Person Q+A",
                        "title": "Comparison Conundrum and the Chamber of Visualizations: An Exploration of How Language Influences Visual Design (Q+A)",
                        "contributors": [
                            "Aimen Gaba"
                        ],
                        "authors": [],
                        "abstract": "The language for expressing comparisons is often complex and nuanced, making supporting natural language-based visual comparison a non-trivial task. To better understand how people reason about comparisons in natural language, we explore a design space of utterances for comparing data entities. We identified different parameters of comparison utterances that indicate what is being compared (i.e., data variables and attributes) as well as how these parameters are specified (i.e., explicitly or implicitly). We conducted a user study with sixteen data visualization experts and non-experts to investigate how they designed visualizations for comparisons in our design space. Based on the rich set of visualization techniques observed, we extracted key design features from the visualizations and synthesized them into a subset of sixteen representative visualization designs. We then conducted a follow-up study to validate user preferences for the sixteen representative visualizations corresponding to utterances in our design space. Findings from these studies suggest guidelines and future directions for designing natural language interfaces and recommendation tools to better support natural language comparisons in visual analytics.",
                        "uid": "v-full-1056",
                        "file_name": "",
                        "time_stamp": "2022-10-21T14:33:00Z",
                        "time_start": "2022-10-21T14:33:00Z",
                        "time_end": "2022-10-21T14:36:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "512",
                        "paper_award": "",
                        "image_caption": "Four comparison utterances from our design space with varying cardinalities for the comparison entities (1 - 1, 1 - N, N - M, N) and different levels of concreteness (explicit and implicit). Each of these comparison utterances was included in our online survey in which participants ranked their preference for the different visualization types; the most preferred visualization(s) have a colored border.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "https://youtu.be/XYBfY4IU-CI",
                        "ff_id": "XYBfY4IU-CI"
                    },
                    {
                        "slot_id": "v-tvcg-9615008-pres",
                        "session_id": "full12",
                        "type": "Virtual Presentation (pre-recorded)",
                        "title": "Explaining with Examples: Lessons Learned from Crowdsourced Introductory Description of Information Visualizations",
                        "contributors": [
                            "Leni YANG"
                        ],
                        "authors": [
                            "Leni Yang; Cindy Xiong; Jason K. Wong; Aoyu Wu; Huamin Qu"
                        ],
                        "abstract": "Data visualizations have been increasingly used in oral presentations to communicate data patterns to the general public. Clear verbal introductions of visualizations to explain how to interpret the visually encoded information are essential to convey the takeaways and avoid misunderstandings. We contribute a series of studies to investigate how to effectively introduce visualizations to the audience with varying degrees of visualization literacy. We begin with understanding how people are introducing visualizations. We crowdsource 110 introductions of visualizations and categorize them based on their content and structures. From these crowdsourced introductions, we identify different introduction strategies and generate a set of introductions for evaluation. We conduct experiments to systematically compare the effectiveness of different introduction strategies across four visualizations with 1,080 participants. We find that introductions explaining visual encodings with concrete examples are the most effective. Our study provides both qualitative and quantitative insights into how to construct effective verbal introductions of visualizations in presentations, inspiring further research in data storytelling.",
                        "uid": "v-tvcg-9615008",
                        "file_name": "v-tvcg-9615008_Yang_Presentation.mp4",
                        "time_stamp": "2022-10-21T14:36:00Z",
                        "time_start": "2022-10-21T14:36:00Z",
                        "time_end": "2022-10-21T14:45:00Z",
                        "paper_type": "full",
                        "keywords": [
                            "narrative visualization, oral presentation, introduction"
                        ],
                        "has_image": "1",
                        "has_video": "582",
                        "paper_award": "",
                        "image_caption": "The figure shows a design space concluded from introductions of data charts by 110 participants.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "https://youtu.be/yy7xau_McYk",
                        "ff_id": "yy7xau_McYk"
                    },
                    {
                        "slot_id": "v-tvcg-9615008-qa",
                        "session_id": "full12",
                        "type": "Virtual Q+A",
                        "title": "Explaining with Examples: Lessons Learned from Crowdsourced Introductory Description of Information Visualizations (Q+A)",
                        "contributors": [
                            "Leni YANG"
                        ],
                        "authors": [],
                        "abstract": "Data visualizations have been increasingly used in oral presentations to communicate data patterns to the general public. Clear verbal introductions of visualizations to explain how to interpret the visually encoded information are essential to convey the takeaways and avoid misunderstandings. We contribute a series of studies to investigate how to effectively introduce visualizations to the audience with varying degrees of visualization literacy. We begin with understanding how people are introducing visualizations. We crowdsource 110 introductions of visualizations and categorize them based on their content and structures. From these crowdsourced introductions, we identify different introduction strategies and generate a set of introductions for evaluation. We conduct experiments to systematically compare the effectiveness of different introduction strategies across four visualizations with 1,080 participants. We find that introductions explaining visual encodings with concrete examples are the most effective. Our study provides both qualitative and quantitative insights into how to construct effective verbal introductions of visualizations in presentations, inspiring further research in data storytelling.",
                        "uid": "v-tvcg-9615008",
                        "file_name": "",
                        "time_stamp": "2022-10-21T14:45:00Z",
                        "time_start": "2022-10-21T14:45:00Z",
                        "time_end": "2022-10-21T14:48:00Z",
                        "paper_type": "full",
                        "keywords": [
                            "narrative visualization, oral presentation, introduction"
                        ],
                        "has_image": "1",
                        "has_video": "582",
                        "paper_award": "",
                        "image_caption": "The figure shows a design space concluded from introductions of data charts by 110 participants.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "https://youtu.be/yy7xau_McYk",
                        "ff_id": "yy7xau_McYk"
                    },
                    {
                        "slot_id": "v-full-1260-pres",
                        "session_id": "full12",
                        "type": "Virtual Presentation (pre-recorded)",
                        "title": "Towards Natural Language-Based Visualization Authoring",
                        "contributors": [
                            "Yun Wang"
                        ],
                        "authors": [
                            "Yun Wang",
                            "Zhitao Hou",
                            "Jiaqi Wang",
                            "Tongshuang Wu",
                            "Leixian Shen",
                            "He Huang",
                            "Haidong Zhang",
                            "Dongmei Zhang"
                        ],
                        "abstract": "A key challenge to visualization authoring is the process of getting familiar with the complex user interfaces of authoring tools. Natural Language Interface (NLI) presents promising benefits due to its learnability and usability. However, supporting NLIs for authoring tools requires expertise in natural language processing, while existing NLIs are mostly designed for visual analytic workflow. In this paper, we propose an authoring-oriented NLI pipeline by introducing a structured representation of users\u2019 visualization editing intents, called editing actions, based on a formative study and an extensive survey on visualization construction tools. The editing actions are executable, and thus decouple natural language interpretation and visualization applications as an intermediate layer. We implement a deep learning-based NL interpreter to translate NL utterances into editing actions. The interpreter is reusable and extensible across authoring tools. The authoring tools only need to map the editing actions into tool-specific operations. To illustrate the usages of the NL interpreter, we implement an Excel chart editor and a proof-of-concept authoring tool, VisTalk. We conduct a user study with VisTalk to understand the usage patterns of NL-based authoring systems. Finally, we discuss observations on how users author charts with natural language, as well as implications for future research.",
                        "uid": "v-full-1260",
                        "file_name": "v-full-1260_Wang_Presentation.mp4",
                        "time_stamp": "2022-10-21T14:48:00Z",
                        "time_start": "2022-10-21T14:48:00Z",
                        "time_end": "2022-10-21T14:57:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "489",
                        "paper_award": "",
                        "image_caption": "In our authoring-oriented NLI pipeline, we model users\u2019 editing intents as editing actions, which decouple natural language interpreter and visualization applications as an intermediate layer.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "https://youtu.be/fgCDJ2s9k-M",
                        "ff_id": "fgCDJ2s9k-M"
                    },
                    {
                        "slot_id": "v-full-1260-qa",
                        "session_id": "full12",
                        "type": "Virtual Q+A",
                        "title": "Towards Natural Language-Based Visualization Authoring (Q+A)",
                        "contributors": [
                            "Yun Wang"
                        ],
                        "authors": [],
                        "abstract": "A key challenge to visualization authoring is the process of getting familiar with the complex user interfaces of authoring tools. Natural Language Interface (NLI) presents promising benefits due to its learnability and usability. However, supporting NLIs for authoring tools requires expertise in natural language processing, while existing NLIs are mostly designed for visual analytic workflow. In this paper, we propose an authoring-oriented NLI pipeline by introducing a structured representation of users\u2019 visualization editing intents, called editing actions, based on a formative study and an extensive survey on visualization construction tools. The editing actions are executable, and thus decouple natural language interpretation and visualization applications as an intermediate layer. We implement a deep learning-based NL interpreter to translate NL utterances into editing actions. The interpreter is reusable and extensible across authoring tools. The authoring tools only need to map the editing actions into tool-specific operations. To illustrate the usages of the NL interpreter, we implement an Excel chart editor and a proof-of-concept authoring tool, VisTalk. We conduct a user study with VisTalk to understand the usage patterns of NL-based authoring systems. Finally, we discuss observations on how users author charts with natural language, as well as implications for future research.",
                        "uid": "v-full-1260",
                        "file_name": "",
                        "time_stamp": "2022-10-21T14:57:00Z",
                        "time_start": "2022-10-21T14:57:00Z",
                        "time_end": "2022-10-21T15:00:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "489",
                        "paper_award": "",
                        "image_caption": "In our authoring-oriented NLI pipeline, we model users\u2019 editing intents as editing actions, which decouple natural language interpreter and visualization applications as an intermediate layer.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "https://youtu.be/fgCDJ2s9k-M",
                        "ff_id": "fgCDJ2s9k-M"
                    },
                    {
                        "slot_id": "v-full-1024-pres",
                        "session_id": "full12",
                        "type": "In Person Presentation",
                        "title": "Striking a Balance: Reader Takeaways and Preferences when Integrating Text and Charts",
                        "contributors": [
                            "Chase Stokes"
                        ],
                        "authors": [
                            "Chase Stokes",
                            "Vidya Setlur",
                            "Bridget Cogley",
                            "Arvind Satyanarayan",
                            "Marti Hearst"
                        ],
                        "abstract": "While visualizations are an effective way to represent insights about information, they rarely stand alone. When designing a visualization, text is often added to provide additional context and guidance for the reader. However, there is little experimental evidence to guide designers as to what is the right amount of text to show within a chart, what its qualitative properties should be, and where it should be placed. Prior work also shows variation in personal preferences for charts versus textual representations. In this paper, we explore several research questions about the relative value of textual components of visualizations. 302 participants ranked univariate line charts containing varying amounts of text, ranging from no text (except for the axes) to a written paragraph with no visuals. Participants also described what information they could take away from line charts with text having varying semantic content. We find that heavily annotated charts were not penalized. In fact, participants preferred the charts with the largest number of textual annotations over charts with fewer annotations or text alone. We also find effects of semantic content. For instance, the text that describes statistical or relational components of a chart leads to more takeaways regarding statistics or relational comparisons than text describing elemental or encoded components. Finally, we found different effects for the semantic levels based on the placement of the text on the chart; some kinds of information are best placed in the title, while others should be placed closer to the data. These results have important implications for chart design guidelines and future work pertaining to the combination of text and charts.",
                        "uid": "v-full-1024",
                        "file_name": "v-full-1024_Stokes_Presentation.mp4",
                        "time_stamp": "2022-10-21T15:00:00Z",
                        "time_start": "2022-10-21T15:00:00Z",
                        "time_end": "2022-10-21T15:09:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "522",
                        "paper_award": "",
                        "image_caption": "A set of information moves from an entirely visual representation to an entirely textual representation. These forms of communication differ importantly in reader preference, as readers most prefer the third set in this sequence: the chart with the most annotations. Readers least preferred the entirely visual and entirely textual representations, although a substantial minority expressed a strong preference for textual representations. Furthermore, the specifics of the text added to the visualizations may influence the readers' takeaways, with certain content leading to similar takeaways and certain positions leading to a higher likelihood for the reader to use the text in their takeaway.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "https://youtu.be/Mpf7EiAlZ08",
                        "ff_id": "Mpf7EiAlZ08"
                    },
                    {
                        "slot_id": "v-full-1024-qa",
                        "session_id": "full12",
                        "type": "In Person Q+A",
                        "title": "Striking a Balance: Reader Takeaways and Preferences when Integrating Text and Charts (Q+A)",
                        "contributors": [
                            "Chase Stokes"
                        ],
                        "authors": [],
                        "abstract": "While visualizations are an effective way to represent insights about information, they rarely stand alone. When designing a visualization, text is often added to provide additional context and guidance for the reader. However, there is little experimental evidence to guide designers as to what is the right amount of text to show within a chart, what its qualitative properties should be, and where it should be placed. Prior work also shows variation in personal preferences for charts versus textual representations. In this paper, we explore several research questions about the relative value of textual components of visualizations. 302 participants ranked univariate line charts containing varying amounts of text, ranging from no text (except for the axes) to a written paragraph with no visuals. Participants also described what information they could take away from line charts with text having varying semantic content. We find that heavily annotated charts were not penalized. In fact, participants preferred the charts with the largest number of textual annotations over charts with fewer annotations or text alone. We also find effects of semantic content. For instance, the text that describes statistical or relational components of a chart leads to more takeaways regarding statistics or relational comparisons than text describing elemental or encoded components. Finally, we found different effects for the semantic levels based on the placement of the text on the chart; some kinds of information are best placed in the title, while others should be placed closer to the data. These results have important implications for chart design guidelines and future work pertaining to the combination of text and charts.",
                        "uid": "v-full-1024",
                        "file_name": "",
                        "time_stamp": "2022-10-21T15:09:00Z",
                        "time_start": "2022-10-21T15:09:00Z",
                        "time_end": "2022-10-21T15:12:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "522",
                        "paper_award": "",
                        "image_caption": "A set of information moves from an entirely visual representation to an entirely textual representation. These forms of communication differ importantly in reader preference, as readers most prefer the third set in this sequence: the chart with the most annotations. Readers least preferred the entirely visual and entirely textual representations, although a substantial minority expressed a strong preference for textual representations. Furthermore, the specifics of the text added to the visualizations may influence the readers' takeaways, with certain content leading to similar takeaways and certain positions leading to a higher likelihood for the reader to use the text in their takeaway.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "https://youtu.be/Mpf7EiAlZ08",
                        "ff_id": "Mpf7EiAlZ08"
                    }
                ]
            },
            {
                "title": "Reflecting on Academia and our Field",
                "session_id": "full13",
                "event_prefix": "v-full",
                "track": "ok5",
                "livestream_id": "ok5-thu",
                "session_image": "full13.png",
                "chair": [
                    "Petra Isenberg"
                ],
                "organizers": [],
                "time_start": "2022-10-20T19:00:00Z",
                "time_end": "2022-10-20T20:15:00Z",
                "discord_category": "",
                "discord_channel": "oklahoma-st-5",
                "discord_channel_id": "1024600839295356939",
                "discord_link": "https://discord.com/channels/978320435554947082/1024600839295356939",
                "slido_link": "https://app.sli.do/event/i28UETZaiRWr1XwX6eKjcf?section=d355f89b-fbd2-4abf-9958-741607905551",
                "youtube_url": "https://youtu.be/JCJlogloJH8",
                "youtube_id": "JCJlogloJH8",
                "zoom_meeting": "",
                "zoom_password": "",
                "zoom_link": "",
                "ff_link": "https://youtu.be/ZqWDmM_R0cE",
                "ff_playlist": "",
                "time_slots": [
                    {
                        "slot_id": "full13-opening",
                        "session_id": "full13",
                        "type": "In Person Other",
                        "title": "Session Opening",
                        "contributors": [
                            "Petra Isenberg"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "",
                        "file_name": "",
                        "time_stamp": "2022-10-20T19:00:00Z",
                        "time_start": "2022-10-20T19:00:00Z",
                        "time_end": "2022-10-20T19:00:00Z",
                        "paper_type": "",
                        "keywords": [],
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "v-tvcg-9733942-pres",
                        "session_id": "full13",
                        "type": "In Person Presentation",
                        "title": "Scientometric Analysis of Interdisciplinary Collaboration and Gender Trends in 30 Years of IEEE VIS Publications",
                        "contributors": [
                            "Ali Sarvghad"
                        ],
                        "authors": [
                            "Ali Sarvghad",
                            "Rolando Franqui-Nadal",
                            "Rebecca Reznik-Zellen",
                            "Ria Chawla",
                            "Narges Mahyar"
                        ],
                        "abstract": "We present the results of a scientometric analysis of 30 years of IEEE VIS publications between 1990-2020, in which we conducted a multifaceted analysis of interdisciplinary collaboration and gender composition among authors. To this end, we curated BiblioVIS, a bibliometric dataset that contains rich metadata about IEEE VIS publications, including 3032 papers and 6113 authors. One of the main factors differentiating BiblioVIS from similar datasets is the authors' gender and discipline data, which we inferred through iterative rounds of computational and manual processes. Our analysis shows that, by and large, inter-institutional and interdisciplinary collaboration has been steadily growing over the past 30 years. However, interdisciplinary research was mainly between a few fields, including Computer Science, Engineering and Technology, and Medicine and Health disciplines. Our analysis of gender shows steady growth in women's authorship. Despite this growth, the gender distribution is still highly skewed, with men dominating (~75%) of this space. Our predictive analysis of gender balance shows that if the current trends continue, gender parity in the visualization field will not be reached before the third quarter of the century (~2070). Our primary goal in this work is to call the visualization community's attention to the critical topics of collaboration, diversity, and gender. Our research offers critical insights through the lens of diversity and gender to help accelerate progress towards a more diverse and representative research community.",
                        "uid": "v-tvcg-9733942",
                        "file_name": "",
                        "time_stamp": "2022-10-20T19:00:00Z",
                        "time_start": "2022-10-20T19:00:00Z",
                        "time_end": "2022-10-20T19:09:00Z",
                        "paper_type": "full",
                        "keywords": [
                            "Scientometric, IEEE VIS Publications, Gender, Co-authorship, Collaboration, Interdisciplinary, Inter-institutional"
                        ],
                        "has_image": "0",
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "https://youtu.be/v_v6xlV9m-0",
                        "ff_id": "v_v6xlV9m-0"
                    },
                    {
                        "slot_id": "v-tvcg-9733942-qa",
                        "session_id": "full13",
                        "type": "In Person Q+A",
                        "title": "Scientometric Analysis of Interdisciplinary Collaboration and Gender Trends in 30 Years of IEEE VIS Publications (Q+A)",
                        "contributors": [
                            "Ali Sarvghad"
                        ],
                        "authors": [],
                        "abstract": "We present the results of a scientometric analysis of 30 years of IEEE VIS publications between 1990-2020, in which we conducted a multifaceted analysis of interdisciplinary collaboration and gender composition among authors. To this end, we curated BiblioVIS, a bibliometric dataset that contains rich metadata about IEEE VIS publications, including 3032 papers and 6113 authors. One of the main factors differentiating BiblioVIS from similar datasets is the authors' gender and discipline data, which we inferred through iterative rounds of computational and manual processes. Our analysis shows that, by and large, inter-institutional and interdisciplinary collaboration has been steadily growing over the past 30 years. However, interdisciplinary research was mainly between a few fields, including Computer Science, Engineering and Technology, and Medicine and Health disciplines. Our analysis of gender shows steady growth in women's authorship. Despite this growth, the gender distribution is still highly skewed, with men dominating (~75%) of this space. Our predictive analysis of gender balance shows that if the current trends continue, gender parity in the visualization field will not be reached before the third quarter of the century (~2070). Our primary goal in this work is to call the visualization community's attention to the critical topics of collaboration, diversity, and gender. Our research offers critical insights through the lens of diversity and gender to help accelerate progress towards a more diverse and representative research community.",
                        "uid": "v-tvcg-9733942",
                        "file_name": "",
                        "time_stamp": "2022-10-20T19:09:00Z",
                        "time_start": "2022-10-20T19:09:00Z",
                        "time_end": "2022-10-20T19:12:00Z",
                        "paper_type": "full",
                        "keywords": [
                            "Scientometric, IEEE VIS Publications, Gender, Co-authorship, Collaboration, Interdisciplinary, Inter-institutional"
                        ],
                        "has_image": "0",
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "https://youtu.be/v_v6xlV9m-0",
                        "ff_id": "v_v6xlV9m-0"
                    },
                    {
                        "slot_id": "v-full-1364-pres",
                        "session_id": "full13",
                        "type": "In Person Presentation",
                        "title": "Thirty-Two Years of IEEE VIS: Authors, Fields of Study and Citations",
                        "contributors": [
                            "Hongtao Hao"
                        ],
                        "authors": [
                            "Hongtao Hao",
                            "Yumian Cui",
                            "Zhengxiang Wang",
                            "Yea-Seul Kim"
                        ],
                        "abstract": "The IEEE VIS Conference (VIS) recently rebranded itself as a unified conference and officially positioned itself within the discipline of Data Science. Driven by this movement, we investigated (1) who contributed to VIS, and (2) where VIS stands in the scientific world. We examined the authors and fields of study of 3,240 VIS publications in the past 32 years based on data collected from OpenAlex and IEEE Xplore, among other sources. We also examined the citation flows from referenced papers (i.e., those referenced in VIS) to VIS, and from VIS to citing papers (i.e., those citing VIS). We found that VIS has been becoming increasingly popular and collaborative. The number of publications, of unique authors, and of participating countries have been steadily growing. Both cross-country collaborations, and collaborations between educational and non-educational affiliations, namely \u201ccross-type collaborations'', are increasing. The dominance of the US is decreasing, and authors from China are now an important part of VIS. In terms of author affiliation types, VIS is increasingly dominated by authors from universities. We found that the topics, inspirations, and influences of VIS research is limited such that (1) VIS, and their referenced and citing papers largely fall into the Computer Science domain, and (2) citations flow mostly between the same set of subfields within Computer Science. Our citation analyses showed that award-winning VIS papers had higher citations. Interactive visualizations, replication data, source code and supplementary material are available at https://32vis.hongtaoh.com and https://osf.io/zkvjm.",
                        "uid": "v-full-1364",
                        "file_name": "v-full-1364_Hao_Presentation.mp4",
                        "time_stamp": "2022-10-20T19:12:00Z",
                        "time_start": "2022-10-20T19:12:00Z",
                        "time_end": "2022-10-20T19:21:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "470",
                        "paper_award": "",
                        "image_caption": "Fields of study at different levels in 3,240 VIS papers across the past 32 years. From L0 to L3, granularity increases. \u201cTrend\u201d indicates the proportion of papers falling into a field of study against the total number of papers published in that year. The highest proportion for each field of study is highlighted. \u201cTotal\u201d indicates the total number of VIS publications falling into a field of study. One paper may contain more than one field of study at the same level, and one field of study may have multiple parent fields. For example, Pattern Recognition belongs to both Computer Science and Psychology.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "https://youtu.be/myKlYDJzk-Q",
                        "ff_id": "myKlYDJzk-Q"
                    },
                    {
                        "slot_id": "v-full-1364-qa",
                        "session_id": "full13",
                        "type": "In Person Q+A",
                        "title": "Thirty-Two Years of IEEE VIS: Authors, Fields of Study and Citations (Q+A)",
                        "contributors": [
                            "Hongtao Hao"
                        ],
                        "authors": [],
                        "abstract": "The IEEE VIS Conference (VIS) recently rebranded itself as a unified conference and officially positioned itself within the discipline of Data Science. Driven by this movement, we investigated (1) who contributed to VIS, and (2) where VIS stands in the scientific world. We examined the authors and fields of study of 3,240 VIS publications in the past 32 years based on data collected from OpenAlex and IEEE Xplore, among other sources. We also examined the citation flows from referenced papers (i.e., those referenced in VIS) to VIS, and from VIS to citing papers (i.e., those citing VIS). We found that VIS has been becoming increasingly popular and collaborative. The number of publications, of unique authors, and of participating countries have been steadily growing. Both cross-country collaborations, and collaborations between educational and non-educational affiliations, namely \u201ccross-type collaborations'', are increasing. The dominance of the US is decreasing, and authors from China are now an important part of VIS. In terms of author affiliation types, VIS is increasingly dominated by authors from universities. We found that the topics, inspirations, and influences of VIS research is limited such that (1) VIS, and their referenced and citing papers largely fall into the Computer Science domain, and (2) citations flow mostly between the same set of subfields within Computer Science. Our citation analyses showed that award-winning VIS papers had higher citations. Interactive visualizations, replication data, source code and supplementary material are available at https://32vis.hongtaoh.com and https://osf.io/zkvjm.",
                        "uid": "v-full-1364",
                        "file_name": "",
                        "time_stamp": "2022-10-20T19:21:00Z",
                        "time_start": "2022-10-20T19:21:00Z",
                        "time_end": "2022-10-20T19:24:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "470",
                        "paper_award": "",
                        "image_caption": "Fields of study at different levels in 3,240 VIS papers across the past 32 years. From L0 to L3, granularity increases. \u201cTrend\u201d indicates the proportion of papers falling into a field of study against the total number of papers published in that year. The highest proportion for each field of study is highlighted. \u201cTotal\u201d indicates the total number of VIS publications falling into a field of study. One paper may contain more than one field of study at the same level, and one field of study may have multiple parent fields. For example, Pattern Recognition belongs to both Computer Science and Psychology.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "https://youtu.be/myKlYDJzk-Q",
                        "ff_id": "myKlYDJzk-Q"
                    },
                    {
                        "slot_id": "v-tvcg-9747941-pres",
                        "session_id": "full13",
                        "type": "In Person Presentation",
                        "title": "SD^2: Slicing and Dicing Scholarly Data for Interactive Evaluation of Academic Performance",
                        "contributors": [
                            "Zhichun Guo"
                        ],
                        "authors": [
                            "Zhichun Guo",
                            "Jun Tao",
                            "Siming Chen",
                            "Nitesh V. Chawla",
                            "Chaoli Wang"
                        ],
                        "abstract": "Comprehensively evaluating and comparing researchers' academic performance is complicated due to the intrinsic complexity of scholarly data. Different scholarly evaluation tasks often require the publication and citation data to be investigated in various manners. In this paper, we present an interactive visualization framework, SD^2, to enable flexible data partition and composition to support various analysis requirements within a single system. SD^2 features the hierarchical histogram, a novel visual representation for flexibly slicing and dicing the data, allowing different aspects of scholarly performance to be studied and compared. We also leverage the state-of-the-art set visualization technique to select individual researchers or combine multiple scholars for comprehensive visual comparison. We conduct multiple rounds of expert evaluation to study the effectiveness and usability of SD^2 and revise the design and system implementation accordingly. The effectiveness of SD^2 is demonstrated via multiple usage scenarios with each aiming to answer a specific, commonly raised question.",
                        "uid": "v-tvcg-9747941",
                        "file_name": "v-tvcg-9747941_Guo_Presentation.mp4",
                        "time_stamp": "2022-10-20T19:24:00Z",
                        "time_start": "2022-10-20T19:24:00Z",
                        "time_end": "2022-10-20T19:33:00Z",
                        "paper_type": "full",
                        "keywords": [
                            "Scholarly performance, publication, citation, hierarchical histogram, visual analytics."
                        ],
                        "has_image": "1",
                        "has_video": "700",
                        "paper_award": "",
                        "image_caption": "SD^2 consists of three coordinated views: (a) scholar view, (b) publication view, and (c) hierarchical histogram view. The example shows a 2019 ACM Turing Award winner Yoshua Bengio\ufffds papers with his co-authors Ian Goodfellow or Aaron Courville. In (c), the upper histogram shows papers authored by Goodfellow or Courville and the lower histogram shows papers authored by Bengio and Courville.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "https://youtu.be/8yuz-fI7ENM",
                        "ff_id": "8yuz-fI7ENM"
                    },
                    {
                        "slot_id": "v-tvcg-9747941-qa",
                        "session_id": "full13",
                        "type": "In Person Q+A",
                        "title": "SD^2: Slicing and Dicing Scholarly Data for Interactive Evaluation of Academic Performance (Q+A)",
                        "contributors": [
                            "Zhichun Guo"
                        ],
                        "authors": [],
                        "abstract": "Comprehensively evaluating and comparing researchers' academic performance is complicated due to the intrinsic complexity of scholarly data. Different scholarly evaluation tasks often require the publication and citation data to be investigated in various manners. In this paper, we present an interactive visualization framework, SD^2, to enable flexible data partition and composition to support various analysis requirements within a single system. SD^2 features the hierarchical histogram, a novel visual representation for flexibly slicing and dicing the data, allowing different aspects of scholarly performance to be studied and compared. We also leverage the state-of-the-art set visualization technique to select individual researchers or combine multiple scholars for comprehensive visual comparison. We conduct multiple rounds of expert evaluation to study the effectiveness and usability of SD^2 and revise the design and system implementation accordingly. The effectiveness of SD^2 is demonstrated via multiple usage scenarios with each aiming to answer a specific, commonly raised question.",
                        "uid": "v-tvcg-9747941",
                        "file_name": "",
                        "time_stamp": "2022-10-20T19:33:00Z",
                        "time_start": "2022-10-20T19:33:00Z",
                        "time_end": "2022-10-20T19:36:00Z",
                        "paper_type": "full",
                        "keywords": [
                            "Scholarly performance, publication, citation, hierarchical histogram, visual analytics."
                        ],
                        "has_image": "1",
                        "has_video": "700",
                        "paper_award": "",
                        "image_caption": "SD^2 consists of three coordinated views: (a) scholar view, (b) publication view, and (c) hierarchical histogram view. The example shows a 2019 ACM Turing Award winner Yoshua Bengio\ufffds papers with his co-authors Ian Goodfellow or Aaron Courville. In (c), the upper histogram shows papers authored by Goodfellow or Courville and the lower histogram shows papers authored by Bengio and Courville.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "https://youtu.be/8yuz-fI7ENM",
                        "ff_id": "8yuz-fI7ENM"
                    },
                    {
                        "slot_id": "v-full-1344-pres",
                        "session_id": "full13",
                        "type": "In Person Presentation",
                        "title": "In Defence of Visual Analytics Systems: Replies to Critics",
                        "contributors": [
                            "Aoyu Wu"
                        ],
                        "authors": [
                            "Aoyu Wu",
                            "Dazhen Deng",
                            "Furui Cheng",
                            "Yingcai Wu",
                            "Shixia Liu",
                            "Huamin Qu"
                        ],
                        "abstract": "The last decade has witnessed many visual analytics (VA) systems that make successful applications to wide-ranging domains like urban analytics and explainable AI. However, their research rigor and contributions have been extensively challenged within the visualization community. We come in defence of VA systems by contributing two interview studies for gathering critics and responses to those criticisms. First, we interview 24 researchers to collect criticisms the review comments on their VA work. Through an iterative coding and refinement process, the interview feedback is summarized into a list of 36 common criticisms. Second, we interview 17 researchers to validate our list and collect their responses, thereby discussing implications for defending and improving the scientific values and rigor of VA systems. We highlight that the presented knowledge is deep, extensive, but also imperfect, provocative, and controversial, and thus recommend reading with an inclusive and critical eye. We hope our work can provide thoughts and foundations for conducting VA research and spark discussions to promote the research field forward more rigorously and vibrantly.",
                        "uid": "v-full-1344",
                        "file_name": "v-full-1344_Wu_Presentation.mp4",
                        "time_stamp": "2022-10-20T19:36:00Z",
                        "time_start": "2022-10-20T19:36:00Z",
                        "time_end": "2022-10-20T19:45:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "551",
                        "paper_award": "",
                        "image_caption": "We contribute a research on visualization research to collect data from researchers to study the underly doing of research. Specifically, we contribute two interview studies. In the first study, we asked researchers \u201cWhat are the criticisms you have received during peer-reviewing?\u201d. In the second interview, we asked researchers how to respond to those criticisms throught the research progress.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "https://youtu.be/i5bp51QSFCQ",
                        "ff_id": "i5bp51QSFCQ"
                    },
                    {
                        "slot_id": "v-full-1344-qa",
                        "session_id": "full13",
                        "type": "In Person Q+A",
                        "title": "In Defence of Visual Analytics Systems: Replies to Critics (Q+A)",
                        "contributors": [
                            "Aoyu Wu"
                        ],
                        "authors": [],
                        "abstract": "The last decade has witnessed many visual analytics (VA) systems that make successful applications to wide-ranging domains like urban analytics and explainable AI. However, their research rigor and contributions have been extensively challenged within the visualization community. We come in defence of VA systems by contributing two interview studies for gathering critics and responses to those criticisms. First, we interview 24 researchers to collect criticisms the review comments on their VA work. Through an iterative coding and refinement process, the interview feedback is summarized into a list of 36 common criticisms. Second, we interview 17 researchers to validate our list and collect their responses, thereby discussing implications for defending and improving the scientific values and rigor of VA systems. We highlight that the presented knowledge is deep, extensive, but also imperfect, provocative, and controversial, and thus recommend reading with an inclusive and critical eye. We hope our work can provide thoughts and foundations for conducting VA research and spark discussions to promote the research field forward more rigorously and vibrantly.",
                        "uid": "v-full-1344",
                        "file_name": "",
                        "time_stamp": "2022-10-20T19:45:00Z",
                        "time_start": "2022-10-20T19:45:00Z",
                        "time_end": "2022-10-20T19:48:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "551",
                        "paper_award": "",
                        "image_caption": "We contribute a research on visualization research to collect data from researchers to study the underly doing of research. Specifically, we contribute two interview studies. In the first study, we asked researchers \u201cWhat are the criticisms you have received during peer-reviewing?\u201d. In the second interview, we asked researchers how to respond to those criticisms throught the research progress.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "https://youtu.be/i5bp51QSFCQ",
                        "ff_id": "i5bp51QSFCQ"
                    },
                    {
                        "slot_id": "v-full-1152-pres",
                        "session_id": "full13",
                        "type": "In Person Presentation",
                        "title": "Visualization Design Practices in a Crisis: Behind the Scenes with COVID-19 Dashboard Creators",
                        "contributors": [
                            "Yixuan Zhang"
                        ],
                        "authors": [
                            "Yixuan Zhang",
                            "Joseph D Gaggiano",
                            "Yifan Sun",
                            "Neha Kumar",
                            "Clio Andris",
                            "Andrea G Parker"
                        ],
                        "abstract": "During the COVID-19 pandemic, a number of data visualizations were created to inform the public about the rapidly evolving crisis. Data dashboards, a form of information dissemination used during the pandemic, have facilitated this process by visualizing statistics regarding the number of COVID-19 cases over time. Prior work on COVID-19 visualizations has primarily focused on the design and evaluation of specific visualization systems from technology-centered perspectives. However, little is known about what occurs behind the scenes during the visualization creation processes, given the complex sociotechnical contexts in which they are embedded. Yet, such ecological knowledge is necessary to help characterize the nuances and trajectories of visualization design practices in the wild, as well as generate insights into how creators come to understand and approach visualization design on their own terms and for their own situated purposes. In this research, we conducted a qualitative interview study among dashboard creators from federal agencies, state health departments, mainstream news media outlets, and other organizations that created (often widely-used) COVID-19 dashboards to answer the following questions: how did visualization creators engage in COVID-19 dashboard design, and what tensions, conflicts, and challenges arose during this process? Our findings detail the trajectory of design practices---from creation to expansion, maintenance, and termination---that are shaped by the complex interplay between design goals, tools and technologies, labor, emerging crisis contexts, and public engagement. We particularly examined the tensions between designers and the general public involved in these processes. These conflicts, which often materialized due to a divergence between public demands and standing policies, centered around the type and amount of information to be visualized, how public perceptions shape and are shaped by visualization design, and the strategies utilized to deal with (potential) misinterpretations and misuse of visualizations. Our findings and lessons learned shed light on new ways of thinking in visualization design, focusing on the bundled activities that are invariably involved in human and nonhuman participation throughout the entire trajectory of design practice.",
                        "uid": "v-full-1152",
                        "file_name": "v-full-1152_Zhang_Presentation.mp4",
                        "time_stamp": "2022-10-20T19:48:00Z",
                        "time_start": "2022-10-20T19:48:00Z",
                        "time_end": "2022-10-20T19:57:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "591",
                        "paper_award": "",
                        "image_caption": "{\\rtf1\\ansi\\ansicpg1252\\cocoartf2638\n\\cocoatextscaling0\\cocoaplatform0{\\fonttbl\\f0\\fswiss\\fcharset0 Helvetica;}\n{\\colortbl;\\red255\\green255\\blue255;}\n{\\*\\expandedcolortbl;;}\n\\margl1440\\margr1440\\vieww11520\\viewh8400\\viewkind0\n\\pard\\tx720\\tx1440\\tx2160\\tx2880\\tx3600\\tx4320\\tx5040\\tx5760\\tx6480\\tx7200\\tx7920\\tx8640\\pardirnatural\\partightenfactor0\n\n\\f0\\fs24 \\cf0 Conceptualize visualization as \\'93boundary objects\\'94 to understand design and design guidelines.}",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "https://youtu.be/YAIVmmIm2KI",
                        "ff_id": "YAIVmmIm2KI"
                    },
                    {
                        "slot_id": "v-full-1152-qa",
                        "session_id": "full13",
                        "type": "In Person Q+A",
                        "title": "Visualization Design Practices in a Crisis: Behind the Scenes with COVID-19 Dashboard Creators (Q+A)",
                        "contributors": [
                            "Yixuan Zhang"
                        ],
                        "authors": [],
                        "abstract": "During the COVID-19 pandemic, a number of data visualizations were created to inform the public about the rapidly evolving crisis. Data dashboards, a form of information dissemination used during the pandemic, have facilitated this process by visualizing statistics regarding the number of COVID-19 cases over time. Prior work on COVID-19 visualizations has primarily focused on the design and evaluation of specific visualization systems from technology-centered perspectives. However, little is known about what occurs behind the scenes during the visualization creation processes, given the complex sociotechnical contexts in which they are embedded. Yet, such ecological knowledge is necessary to help characterize the nuances and trajectories of visualization design practices in the wild, as well as generate insights into how creators come to understand and approach visualization design on their own terms and for their own situated purposes. In this research, we conducted a qualitative interview study among dashboard creators from federal agencies, state health departments, mainstream news media outlets, and other organizations that created (often widely-used) COVID-19 dashboards to answer the following questions: how did visualization creators engage in COVID-19 dashboard design, and what tensions, conflicts, and challenges arose during this process? Our findings detail the trajectory of design practices---from creation to expansion, maintenance, and termination---that are shaped by the complex interplay between design goals, tools and technologies, labor, emerging crisis contexts, and public engagement. We particularly examined the tensions between designers and the general public involved in these processes. These conflicts, which often materialized due to a divergence between public demands and standing policies, centered around the type and amount of information to be visualized, how public perceptions shape and are shaped by visualization design, and the strategies utilized to deal with (potential) misinterpretations and misuse of visualizations. Our findings and lessons learned shed light on new ways of thinking in visualization design, focusing on the bundled activities that are invariably involved in human and nonhuman participation throughout the entire trajectory of design practice.",
                        "uid": "v-full-1152",
                        "file_name": "",
                        "time_stamp": "2022-10-20T19:57:00Z",
                        "time_start": "2022-10-20T19:57:00Z",
                        "time_end": "2022-10-20T20:00:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "591",
                        "paper_award": "",
                        "image_caption": "{\\rtf1\\ansi\\ansicpg1252\\cocoartf2638\n\\cocoatextscaling0\\cocoaplatform0{\\fonttbl\\f0\\fswiss\\fcharset0 Helvetica;}\n{\\colortbl;\\red255\\green255\\blue255;}\n{\\*\\expandedcolortbl;;}\n\\margl1440\\margr1440\\vieww11520\\viewh8400\\viewkind0\n\\pard\\tx720\\tx1440\\tx2160\\tx2880\\tx3600\\tx4320\\tx5040\\tx5760\\tx6480\\tx7200\\tx7920\\tx8640\\pardirnatural\\partightenfactor0\n\n\\f0\\fs24 \\cf0 Conceptualize visualization as \\'93boundary objects\\'94 to understand design and design guidelines.}",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "https://youtu.be/YAIVmmIm2KI",
                        "ff_id": "YAIVmmIm2KI"
                    },
                    {
                        "slot_id": "v-full-1456-pres",
                        "session_id": "full13",
                        "type": "In Person Presentation",
                        "title": "Understanding how Designers Find and Use Data Visualization Examples",
                        "contributors": [
                            "Hannah K. Bako"
                        ],
                        "authors": [
                            "Hannah K. Bako",
                            "Xinyi Liu",
                            "Leilani Battle",
                            "Zhicheng Liu"
                        ],
                        "abstract": "Examples are useful for inspiring ideas and facilitating implementation in visualization design. However, there is little understanding of how visualization designers use examples, and how computational tools may support such activities. In this paper, we contribute an exploratory study of current practices in incorporating visualization examples. We conducted semi-structured interviews with 15 university students and 15 professional designers. Our analysis focus on two core design activities: searching for examples and utilizing examples. We characterize observed strategies and tools for performing these activities, as well as major challenges that hinder designers\u2019 current workflows. In addition, we identify themes that cut across these two activities: criteria for determining example usefulness, curation practices, and design fixation. Given our findings, we discuss the implications for visualization design and authoring tools and highlight critical areas for future research.",
                        "uid": "v-full-1456",
                        "file_name": "v-full-1456_Bako_Presentation.mp4",
                        "time_stamp": "2022-10-20T20:00:00Z",
                        "time_start": "2022-10-20T20:00:00Z",
                        "time_end": "2022-10-20T20:09:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "600",
                        "paper_award": "",
                        "image_caption": "A graphic showing key themes for a paper titled \"Understanding how designers find and use data visualization examples\". At the top right is a graphic of a young man and lady depicting the two key groups of visualization designers in this study. Below is a collage of images with a magnifying glass depicting the search for examples with the caption \"exploratory vs targeted search\". In the middle is a graphic of a puzzled person trying to evaluate the criteria for selecting examples with the caption \"effectiveness vs aesthetics\". On the far right is a graphic depicting ideas being transferred between examples and visualization designs captioned \" strategies for using examples, Select & Merge, Replicate & Modify, Trial & Error\".",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "https://youtu.be/gwx8P5rMt2s",
                        "ff_id": "gwx8P5rMt2s"
                    },
                    {
                        "slot_id": "v-full-1456-qa",
                        "session_id": "full13",
                        "type": "In Person Q+A",
                        "title": "Understanding how Designers Find and Use Data Visualization Examples (Q+A)",
                        "contributors": [
                            "Hannah K. Bako"
                        ],
                        "authors": [],
                        "abstract": "Examples are useful for inspiring ideas and facilitating implementation in visualization design. However, there is little understanding of how visualization designers use examples, and how computational tools may support such activities. In this paper, we contribute an exploratory study of current practices in incorporating visualization examples. We conducted semi-structured interviews with 15 university students and 15 professional designers. Our analysis focus on two core design activities: searching for examples and utilizing examples. We characterize observed strategies and tools for performing these activities, as well as major challenges that hinder designers\u2019 current workflows. In addition, we identify themes that cut across these two activities: criteria for determining example usefulness, curation practices, and design fixation. Given our findings, we discuss the implications for visualization design and authoring tools and highlight critical areas for future research.",
                        "uid": "v-full-1456",
                        "file_name": "",
                        "time_stamp": "2022-10-20T20:09:00Z",
                        "time_start": "2022-10-20T20:09:00Z",
                        "time_end": "2022-10-20T20:12:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "600",
                        "paper_award": "",
                        "image_caption": "A graphic showing key themes for a paper titled \"Understanding how designers find and use data visualization examples\". At the top right is a graphic of a young man and lady depicting the two key groups of visualization designers in this study. Below is a collage of images with a magnifying glass depicting the search for examples with the caption \"exploratory vs targeted search\". In the middle is a graphic of a puzzled person trying to evaluate the criteria for selecting examples with the caption \"effectiveness vs aesthetics\". On the far right is a graphic depicting ideas being transferred between examples and visualization designs captioned \" strategies for using examples, Select & Merge, Replicate & Modify, Trial & Error\".",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "https://youtu.be/gwx8P5rMt2s",
                        "ff_id": "gwx8P5rMt2s"
                    }
                ]
            },
            {
                "title": "Infrastructure Management",
                "session_id": "full14",
                "event_prefix": "v-full",
                "track": "mistletoe",
                "livestream_id": "mistletoe-thu",
                "session_image": "full14.png",
                "chair": [
                    "Ross Maciejewski"
                ],
                "organizers": [],
                "time_start": "2022-10-20T20:45:00Z",
                "time_end": "2022-10-20T22:00:00Z",
                "discord_category": "",
                "discord_channel": "mistletoe",
                "discord_channel_id": "1024601055700471839",
                "discord_link": "https://discord.com/channels/978320435554947082/1024601055700471839",
                "slido_link": "https://app.sli.do/event/i28UETZaiRWr1XwX6eKjcf?section=05d57c7a-397b-4e16-a41d-7f06e8e64c37",
                "youtube_url": "https://youtu.be/QUC4mL-YR40",
                "youtube_id": "QUC4mL-YR40",
                "zoom_meeting": "",
                "zoom_password": "",
                "zoom_link": "",
                "ff_link": "https://youtu.be/5QQWbONfugY",
                "ff_playlist": "",
                "time_slots": [
                    {
                        "slot_id": "full14-opening",
                        "session_id": "full14",
                        "type": "In Person Other",
                        "title": "Session Opening",
                        "contributors": [
                            "Ross Maciejewski"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "",
                        "file_name": "",
                        "time_stamp": "2022-10-20T20:45:00Z",
                        "time_start": "2022-10-20T20:45:00Z",
                        "time_end": "2022-10-20T20:45:00Z",
                        "paper_type": "",
                        "keywords": [],
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "v-full-1006-pres",
                        "session_id": "full14",
                        "type": "In Person Presentation",
                        "title": "The State of the Art in BGP Visualization Tools: A Mapping of Visualization Techniques to Cyberattack Types",
                        "contributors": [
                            "Justin Raynor"
                        ],
                        "authors": [
                            "Justin Raynor",
                            "Tarik Crnovrsanin",
                            "Sara Di Bartolomeo",
                            "Laura South",
                            "David Saffo",
                            "Cody Dunne"
                        ],
                        "abstract": "Internet routing is largely dependent on Border Gateway Protocol (BGP). However, BGP does not have any inherent authentication or integrity mechanisms that help make it secure. Effective security is challenging or infeasible to implement due to high costs, policy employment in these distributed systems, and unique routing behavior. Visualization tools provide an attractive alternative in lieu of traditional security approaches. Several BGP security visualization tools have been developed as a stop-gap in the face of ever-present BGP attacks. Even though the target users, tasks, and domain remain largely consistent across such tools, many diverse visualization designs have been proposed. The purpose of this study is to provide an initial formalization of methods and visualization techniques for BGP cybersecurity analysis. Using PRISMA guidelines, we provide a systematic review and survey of 29 BGP visualization tools with their tasks, implementation techniques, and attacks and anomalies that they were intended for. We focused on BGP visualization tools as the main inclusion criteria to best capture the visualization techniques used in this domain while excluding solely algorithmic solutions and other detection tools that do not involve user interaction or interpretation. We take the unique approach of connecting (1) the actual BGP attacks and anomalies used to validate existing tools with (2) the techniques employed to detect them. In this way, we contribute an analysis of which techniques can be used for each attack type. Furthermore, we can see the evolution of visualization solutions in this domain as new attack types are discovered. This systematic review provides the groundwork for future designers and researchers building visualization tools for providing BGP cybersecurity, including an understanding of the state-of-the-art in this space and an analysis of what techniques are appropriate for each attack type. Our novel security visualization survey methodology---connecting visualization techniques with appropriate attack types---may also assist future researchers conducting systematic reviews of security visualizations. All supplemental materials are available at https://osf.io/tupz6/.",
                        "uid": "v-full-1006",
                        "file_name": "v-full-1006_Raynor_Presentation.mp4",
                        "time_stamp": "2022-10-20T20:45:00Z",
                        "time_start": "2022-10-20T20:45:00Z",
                        "time_end": "2022-10-20T20:54:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "645",
                        "paper_award": "",
                        "image_caption": "Parallel timeline of attacks on the BGP system (bottom) and the tools that have been proposed to visualize them (top). The bottom nodes and the corresponding edges are colored categorically by the type of attack. Each tool is connected to the specific attacks that were used to demonstrate or validate the tool design and are colored to show the number of connected attacks. We have also encoded the edges that connect the specific tools to the attacks with color indicating the attack type to help show patterns in how tool development is changing over time and to answer the question of whether or not the tools are evolving with the threat landscape.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "https://youtu.be/EN4msZICZcg",
                        "ff_id": "EN4msZICZcg"
                    },
                    {
                        "slot_id": "v-full-1006-qa",
                        "session_id": "full14",
                        "type": "In Person Q+A",
                        "title": "The State of the Art in BGP Visualization Tools: A Mapping of Visualization Techniques to Cyberattack Types (Q+A)",
                        "contributors": [
                            "Justin Raynor"
                        ],
                        "authors": [],
                        "abstract": "Internet routing is largely dependent on Border Gateway Protocol (BGP). However, BGP does not have any inherent authentication or integrity mechanisms that help make it secure. Effective security is challenging or infeasible to implement due to high costs, policy employment in these distributed systems, and unique routing behavior. Visualization tools provide an attractive alternative in lieu of traditional security approaches. Several BGP security visualization tools have been developed as a stop-gap in the face of ever-present BGP attacks. Even though the target users, tasks, and domain remain largely consistent across such tools, many diverse visualization designs have been proposed. The purpose of this study is to provide an initial formalization of methods and visualization techniques for BGP cybersecurity analysis. Using PRISMA guidelines, we provide a systematic review and survey of 29 BGP visualization tools with their tasks, implementation techniques, and attacks and anomalies that they were intended for. We focused on BGP visualization tools as the main inclusion criteria to best capture the visualization techniques used in this domain while excluding solely algorithmic solutions and other detection tools that do not involve user interaction or interpretation. We take the unique approach of connecting (1) the actual BGP attacks and anomalies used to validate existing tools with (2) the techniques employed to detect them. In this way, we contribute an analysis of which techniques can be used for each attack type. Furthermore, we can see the evolution of visualization solutions in this domain as new attack types are discovered. This systematic review provides the groundwork for future designers and researchers building visualization tools for providing BGP cybersecurity, including an understanding of the state-of-the-art in this space and an analysis of what techniques are appropriate for each attack type. Our novel security visualization survey methodology---connecting visualization techniques with appropriate attack types---may also assist future researchers conducting systematic reviews of security visualizations. All supplemental materials are available at https://osf.io/tupz6/.",
                        "uid": "v-full-1006",
                        "file_name": "",
                        "time_stamp": "2022-10-20T20:54:00Z",
                        "time_start": "2022-10-20T20:54:00Z",
                        "time_end": "2022-10-20T20:57:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "645",
                        "paper_award": "",
                        "image_caption": "Parallel timeline of attacks on the BGP system (bottom) and the tools that have been proposed to visualize them (top). The bottom nodes and the corresponding edges are colored categorically by the type of attack. Each tool is connected to the specific attacks that were used to demonstrate or validate the tool design and are colored to show the number of connected attacks. We have also encoded the edges that connect the specific tools to the attacks with color indicating the attack type to help show patterns in how tool development is changing over time and to answer the question of whether or not the tools are evolving with the threat landscape.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "https://youtu.be/EN4msZICZcg",
                        "ff_id": "EN4msZICZcg"
                    },
                    {
                        "slot_id": "v-full-1329-pres",
                        "session_id": "full14",
                        "type": "Virtual Presentation (pre-recorded)",
                        "title": "RISeer: Inspecting the Status and Dynamics of Regional Industrial Structure via Visual Analytics",
                        "contributors": [
                            "Longfei Chen"
                        ],
                        "authors": [
                            "Longfei Chen",
                            "Yang Ouyang",
                            "Haipeng Zhang",
                            "Suting Hong",
                            "Quan Li"
                        ],
                        "abstract": "Restructuring the regional industrial structure (RIS) has the potential to halt economic recession and achieve revitalization. Understanding the current status and dynamics of RIS will greatly assist in studying and evaluating the current industrial structure. Previous studies have focused on qualitative and quantitative research to rationalize RIS from a macroscopic perspective. Although recent studies have traced information at the industrial enterprise level to complement existing research from a micro perspective, the ambiguity of the underlying variables contributing to the industrial sector and its composition, the dynamic nature, and the large number of multivariant features of RIS records have obscured a deep and fine-grained understanding of RIS. To this end, we propose an interactive visualization system, RISeer, which is based on interpretable machine learning models and enhanced visualizations designed to identify the evolutionary patterns of the RIS and facilitate inter-regional inspection and comparison. Two case studies confirm the effectiveness of our approach, and feedback from experts indicates that RISeer helps them to gain a fine-grained understanding of the dynamics and evolution of the RIS.",
                        "uid": "v-full-1329",
                        "file_name": "v-full-1329_Chen_Presentation.mp4",
                        "time_stamp": "2022-10-20T20:57:00Z",
                        "time_start": "2022-10-20T20:57:00Z",
                        "time_end": "2022-10-20T21:06:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "657",
                        "paper_award": "",
                        "image_caption": "Regional industrial structure (RIS) refers to the proportional relationship between industrial sectors with different development functions in a specific region, and it is the result of the combination of the spatial layout of the national economy in a specific region. To explore the evolutionary patterns of the RIS, we present RISeer, an interactive visualization system based on interpretable machine learning models and enhanced visualizations. Our work allows users to track evolution of RIS dynamically and supports inspection and comparison between inter-regional enterprise clusters.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "https://youtu.be/Cx81AWaK_oU",
                        "ff_id": "Cx81AWaK_oU"
                    },
                    {
                        "slot_id": "v-full-1329-qa",
                        "session_id": "full14",
                        "type": "Virtual Q+A",
                        "title": "RISeer: Inspecting the Status and Dynamics of Regional Industrial Structure via Visual Analytics (Q+A)",
                        "contributors": [
                            "Longfei Chen"
                        ],
                        "authors": [],
                        "abstract": "Restructuring the regional industrial structure (RIS) has the potential to halt economic recession and achieve revitalization. Understanding the current status and dynamics of RIS will greatly assist in studying and evaluating the current industrial structure. Previous studies have focused on qualitative and quantitative research to rationalize RIS from a macroscopic perspective. Although recent studies have traced information at the industrial enterprise level to complement existing research from a micro perspective, the ambiguity of the underlying variables contributing to the industrial sector and its composition, the dynamic nature, and the large number of multivariant features of RIS records have obscured a deep and fine-grained understanding of RIS. To this end, we propose an interactive visualization system, RISeer, which is based on interpretable machine learning models and enhanced visualizations designed to identify the evolutionary patterns of the RIS and facilitate inter-regional inspection and comparison. Two case studies confirm the effectiveness of our approach, and feedback from experts indicates that RISeer helps them to gain a fine-grained understanding of the dynamics and evolution of the RIS.",
                        "uid": "v-full-1329",
                        "file_name": "",
                        "time_stamp": "2022-10-20T21:06:00Z",
                        "time_start": "2022-10-20T21:06:00Z",
                        "time_end": "2022-10-20T21:09:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "657",
                        "paper_award": "",
                        "image_caption": "Regional industrial structure (RIS) refers to the proportional relationship between industrial sectors with different development functions in a specific region, and it is the result of the combination of the spatial layout of the national economy in a specific region. To explore the evolutionary patterns of the RIS, we present RISeer, an interactive visualization system based on interpretable machine learning models and enhanced visualizations. Our work allows users to track evolution of RIS dynamically and supports inspection and comparison between inter-regional enterprise clusters.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "https://youtu.be/Cx81AWaK_oU",
                        "ff_id": "Cx81AWaK_oU"
                    },
                    {
                        "slot_id": "v-full-1161-pres",
                        "session_id": "full14",
                        "type": "In Person Presentation",
                        "title": "PMU Tracker: A Visualization Platform for Epicentric Event Propagation Analysis in the Power Grid",
                        "contributors": [
                            "Anjana Arunkumar"
                        ],
                        "authors": [
                            "Anjana Arunkumar",
                            "Andrea Pinceti",
                            "Lalitha Sankar",
                            "Chris Bryan"
                        ],
                        "abstract": "The electrical power grid is a critical infrastructure, with disruptions in transmission having severe repercussions on daily activities, across multiple sectors. To identify, prevent, and mitigate such events, power grids are being refurbished as \u2018smart\u2019 systems that include the widespread deployment of GPS-enabled phasor measurement units (PMUs). PMUs provide fast, precise, and time-synchronized measurements of voltage and current, enabling real-time wide-area monitoring and control. However, the potential benefits of PMUs, for analyzing grid events like abnormal power oscillations and load fluctuations, are hindered by the fact that these sensors produce large, concurrent volumes of noisy data. In this paper, we describe working with power grid engineers to investigate how this problem can be addressed from a visual analytics perspective. As a result, we have developed PMU Tracker, an event localization tool that supports power grid operators in visually analyzing and identifying power grid events and tracking their propagation through the power grid\u2019s network. As a part of the PMU Tracker interface, we develop a novel visualization technique which we term an epicentric cluster dendrogram, which allows operators to analyze the effects of an event as it propagates outwards from a source location. We robustly validate PMU Tracker with: (1) a usage scenario demonstrating how PMU Tracker can be used to analyze anomalous grid events, and (2) case studies with power grid operators using a real-world interconnection dataset. Our results indicate that PMU Tracker effectively supports the analysis of power grid events; we also demonstrate and discuss how PMU Tracker\u2019s visual analytics approach can be generalized to other domains composed of time-varying networks with epicentric event characteristics.",
                        "uid": "v-full-1161",
                        "file_name": "v-full-1161_Arunkumar_Presentation.mp4",
                        "time_stamp": "2022-10-20T21:09:00Z",
                        "time_start": "2022-10-20T21:09:00Z",
                        "time_end": "2022-10-20T21:18:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "584",
                        "paper_award": "",
                        "image_caption": "Phasor Measurement Units (PMUs) provide high volume time-synchronized measurements of voltage and current, enabling real-time monitoring and control in the electric power grid. PMU Tracker is an end-to-end visual analysis and event localization tool that affords streamlined, flexible, and scalable analysis of PMU data. Based on an identified grid event, users can apply spectral analysis for anomaly detection across a set of coordinated visualizations. This is enabled by our development of a novel cluster dendrogram visualization, which affords users the ability to interactively track event propagation from the epicentric PMU(s), i.e., those at or near the center of the event.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "https://youtu.be/Q0-SJVxudh8",
                        "ff_id": "Q0-SJVxudh8"
                    },
                    {
                        "slot_id": "v-full-1161-qa",
                        "session_id": "full14",
                        "type": "In Person Q+A",
                        "title": "PMU Tracker: A Visualization Platform for Epicentric Event Propagation Analysis in the Power Grid (Q+A)",
                        "contributors": [
                            "Anjana Arunkumar"
                        ],
                        "authors": [],
                        "abstract": "The electrical power grid is a critical infrastructure, with disruptions in transmission having severe repercussions on daily activities, across multiple sectors. To identify, prevent, and mitigate such events, power grids are being refurbished as \u2018smart\u2019 systems that include the widespread deployment of GPS-enabled phasor measurement units (PMUs). PMUs provide fast, precise, and time-synchronized measurements of voltage and current, enabling real-time wide-area monitoring and control. However, the potential benefits of PMUs, for analyzing grid events like abnormal power oscillations and load fluctuations, are hindered by the fact that these sensors produce large, concurrent volumes of noisy data. In this paper, we describe working with power grid engineers to investigate how this problem can be addressed from a visual analytics perspective. As a result, we have developed PMU Tracker, an event localization tool that supports power grid operators in visually analyzing and identifying power grid events and tracking their propagation through the power grid\u2019s network. As a part of the PMU Tracker interface, we develop a novel visualization technique which we term an epicentric cluster dendrogram, which allows operators to analyze the effects of an event as it propagates outwards from a source location. We robustly validate PMU Tracker with: (1) a usage scenario demonstrating how PMU Tracker can be used to analyze anomalous grid events, and (2) case studies with power grid operators using a real-world interconnection dataset. Our results indicate that PMU Tracker effectively supports the analysis of power grid events; we also demonstrate and discuss how PMU Tracker\u2019s visual analytics approach can be generalized to other domains composed of time-varying networks with epicentric event characteristics.",
                        "uid": "v-full-1161",
                        "file_name": "",
                        "time_stamp": "2022-10-20T21:18:00Z",
                        "time_start": "2022-10-20T21:18:00Z",
                        "time_end": "2022-10-20T21:21:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "584",
                        "paper_award": "",
                        "image_caption": "Phasor Measurement Units (PMUs) provide high volume time-synchronized measurements of voltage and current, enabling real-time monitoring and control in the electric power grid. PMU Tracker is an end-to-end visual analysis and event localization tool that affords streamlined, flexible, and scalable analysis of PMU data. Based on an identified grid event, users can apply spectral analysis for anomaly detection across a set of coordinated visualizations. This is enabled by our development of a novel cluster dendrogram visualization, which affords users the ability to interactively track event propagation from the epicentric PMU(s), i.e., those at or near the center of the event.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "https://youtu.be/Q0-SJVxudh8",
                        "ff_id": "Q0-SJVxudh8"
                    },
                    {
                        "slot_id": "v-full-1143-pres",
                        "session_id": "full14",
                        "type": "Virtual Presentation (pre-recorded)",
                        "title": "ECoalVis: Visual Analysis of Control Strategies in Coal-fired Power Plants",
                        "contributors": [
                            "Shuhan Liu"
                        ],
                        "authors": [
                            "Shuhan Liu",
                            "Di Weng",
                            "Yuan Tian",
                            "Zikun Deng",
                            "Haoran Xu",
                            "Xiangyu Zhu",
                            "Honglei Yin",
                            "Xianyuan Zhan",
                            "Yingcai Wu"
                        ],
                        "abstract": "Improving the efficiency of coal-fired power plants has numerous benefits. The control strategy is one of the major factors affecting such efficiency. However, due to the complex and dynamic environment inside the power plants, it is hard to extract and evaluate control strategies and their cascading impact across massive sensors. Existing manual and data-driven approaches cannot well support the analysis of control strategies because these approaches are time-consuming and do not scale with the complexity of the power plant systems. Three challenges were identified: a) interactive extraction of control strategies from large-scale dynamic sensor data, b) intuitive visual representation of cascading impact among the sensors in a complex power plant system, and c) time-lag-aware analysis of the impact of control strategies on electricity generation efficiency. By collaborating with energy domain experts, we addressed these challenges with ECoalVis, a novel interactive system for experts to visually analyze the control strategies of coal-fired power plants extracted from historical sensor data. The effectiveness of the proposed system is evaluated with two usage scenarios on a real-world historical dataset and received positive feedback from experts.",
                        "uid": "v-full-1143",
                        "file_name": "v-full-1143_Liu_Presentation.mp4",
                        "time_stamp": "2022-10-20T21:21:00Z",
                        "time_start": "2022-10-20T21:21:00Z",
                        "time_end": "2022-10-20T21:30:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "580",
                        "paper_award": "",
                        "image_caption": "The user interface of ECoalVis. (A) The filter view reveals the time series of key sensors and allows users to fuzzy query control strategies. (B) The graph view reveals the spatial propagation of control strategy impact across components, units and sensors. (C) The strategy view depicts the temporal cascading of control strategy impact, visualizing the topology of the strategy and the time-lag-aligned time series. (D) The detail view allows users to search for sensors and perform time series operations to find insights from the raw data.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "https://youtu.be/F0UEbxpkC0o",
                        "ff_id": "F0UEbxpkC0o"
                    },
                    {
                        "slot_id": "v-full-1143-qa",
                        "session_id": "full14",
                        "type": "Virtual Q+A",
                        "title": "ECoalVis: Visual Analysis of Control Strategies in Coal-fired Power Plants (Q+A)",
                        "contributors": [
                            "Shuhan Liu"
                        ],
                        "authors": [],
                        "abstract": "Improving the efficiency of coal-fired power plants has numerous benefits. The control strategy is one of the major factors affecting such efficiency. However, due to the complex and dynamic environment inside the power plants, it is hard to extract and evaluate control strategies and their cascading impact across massive sensors. Existing manual and data-driven approaches cannot well support the analysis of control strategies because these approaches are time-consuming and do not scale with the complexity of the power plant systems. Three challenges were identified: a) interactive extraction of control strategies from large-scale dynamic sensor data, b) intuitive visual representation of cascading impact among the sensors in a complex power plant system, and c) time-lag-aware analysis of the impact of control strategies on electricity generation efficiency. By collaborating with energy domain experts, we addressed these challenges with ECoalVis, a novel interactive system for experts to visually analyze the control strategies of coal-fired power plants extracted from historical sensor data. The effectiveness of the proposed system is evaluated with two usage scenarios on a real-world historical dataset and received positive feedback from experts.",
                        "uid": "v-full-1143",
                        "file_name": "",
                        "time_stamp": "2022-10-20T21:30:00Z",
                        "time_start": "2022-10-20T21:30:00Z",
                        "time_end": "2022-10-20T21:33:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "580",
                        "paper_award": "",
                        "image_caption": "The user interface of ECoalVis. (A) The filter view reveals the time series of key sensors and allows users to fuzzy query control strategies. (B) The graph view reveals the spatial propagation of control strategy impact across components, units and sensors. (C) The strategy view depicts the temporal cascading of control strategy impact, visualizing the topology of the strategy and the time-lag-aligned time series. (D) The detail view allows users to search for sensors and perform time series operations to find insights from the raw data.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "https://youtu.be/F0UEbxpkC0o",
                        "ff_id": "F0UEbxpkC0o"
                    },
                    {
                        "slot_id": "v-full-1491-pres",
                        "session_id": "full14",
                        "type": "In Person Presentation",
                        "title": "A Visual Analytics System for Improving Attention-based Traffic Forecasting Models",
                        "contributors": [
                            "Seungmin Jin"
                        ],
                        "authors": [
                            "Seungmin Jin",
                            "Hyunwook Lee",
                            "Cheonbok Park",
                            "Hyeshin Chu",
                            "Yunwon Tae",
                            "Jaegul Choo",
                            "Sungahn Ko"
                        ],
                        "abstract": "With deep learning (DL) outperforming conventional methods for different tasks, much effort has been devoted to utilizing DL in various domains. Researchers and developers in the traffic domain have also designed and improved DL models for forecasting tasks such as estimation of traffic speed and time of arrival. However, there exist many challenges in analyzing DL models due to the black-box property of DL models and complexity of traffic data (i.e., spatio-temporal dependencies). Collaborating with domain experts, we design a visual analytics system, AttnAnalyzer, that enables users to explore how DL models make predictions by allowing effective spatio-temporal dependency analysis. The system incorporates dynamic time warping (DTW) and Granger causality tests for computational spatio-temporal dependency analysis while providing map, table, line chart, and pixel views to assist user to perform dependency and model behavior analysis. For the evaluation, we present three case studies showing how AttnAnalyzer can effectively explore model behaviors and improve model performance in two different road networks. We also provide domain expert feedback.",
                        "uid": "v-full-1491",
                        "file_name": "v-full-1491_Jin_Presentation.mp4",
                        "time_stamp": "2022-10-20T21:33:00Z",
                        "time_start": "2022-10-20T21:33:00Z",
                        "time_end": "2022-10-20T21:42:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "655",
                        "paper_award": "",
                        "image_caption": "(a) filter view, (b) table view, (c) ground-truth&prediction result comparison view, (d) a map with attention curves and clusters, and (e) attention heatmap view. There are 8 matrices in the head-cluster view to show how much each attention head refers to reference roads for making predictions for target roads in a cluster point of view. In each matrix, the column represents the clusters of reference roads, while the row means clusters of target roads in ascending order. The color of each cluster cell represents the intensity of the attention.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "https://youtu.be/vv-eZ1lHGoo",
                        "ff_id": "vv-eZ1lHGoo"
                    },
                    {
                        "slot_id": "v-full-1491-qa",
                        "session_id": "full14",
                        "type": "In Person Q+A",
                        "title": "A Visual Analytics System for Improving Attention-based Traffic Forecasting Models (Q+A)",
                        "contributors": [
                            "Seungmin Jin"
                        ],
                        "authors": [],
                        "abstract": "With deep learning (DL) outperforming conventional methods for different tasks, much effort has been devoted to utilizing DL in various domains. Researchers and developers in the traffic domain have also designed and improved DL models for forecasting tasks such as estimation of traffic speed and time of arrival. However, there exist many challenges in analyzing DL models due to the black-box property of DL models and complexity of traffic data (i.e., spatio-temporal dependencies). Collaborating with domain experts, we design a visual analytics system, AttnAnalyzer, that enables users to explore how DL models make predictions by allowing effective spatio-temporal dependency analysis. The system incorporates dynamic time warping (DTW) and Granger causality tests for computational spatio-temporal dependency analysis while providing map, table, line chart, and pixel views to assist user to perform dependency and model behavior analysis. For the evaluation, we present three case studies showing how AttnAnalyzer can effectively explore model behaviors and improve model performance in two different road networks. We also provide domain expert feedback.",
                        "uid": "v-full-1491",
                        "file_name": "",
                        "time_stamp": "2022-10-20T21:42:00Z",
                        "time_start": "2022-10-20T21:42:00Z",
                        "time_end": "2022-10-20T21:45:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "655",
                        "paper_award": "",
                        "image_caption": "(a) filter view, (b) table view, (c) ground-truth&prediction result comparison view, (d) a map with attention curves and clusters, and (e) attention heatmap view. There are 8 matrices in the head-cluster view to show how much each attention head refers to reference roads for making predictions for target roads in a cluster point of view. In each matrix, the column represents the clusters of reference roads, while the row means clusters of target roads in ascending order. The color of each cluster cell represents the intensity of the attention.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "https://youtu.be/vv-eZ1lHGoo",
                        "ff_id": "vv-eZ1lHGoo"
                    },
                    {
                        "slot_id": "v-tvcg-9632413-pres",
                        "session_id": "full14",
                        "type": "Virtual Presentation (pre-recorded)",
                        "title": "RCMVis: A Visual Analytics System for Route Choice Modeling",
                        "contributors": [
                            "DongHwa Shin"
                        ],
                        "authors": [
                            "DongHwa Shin",
                            "Jaemin Jo",
                            "Bohyoung Kim",
                            "Hyunjoo Song",
                            "Shin-Hyung Cho",
                            "Jinwook Seo"
                        ],
                        "abstract": "We present RCMVis, a visual analytics system to support interactive Route Choice Modeling analysis. It aims to model which characteristics of routes, such as distance and the number of traffic lights, affect travelers' route choice behaviors and how much they affect the choice during their trips. Through close collaboration with domain experts, we designed a visual analytics framework for Route Choice Modeling. The framework supports three interactive analysis stages: exploration, modeling, and reasoning. In the exploration stage, we help analysts interactively explore trip data from multiple origin-destination (OD) pairs and choose a subset of data they want to focus on. To this end, we provide coordinated multiple OD views with different foci that allow analysts to inspect, rank, and compare OD pairs in terms of their multidimensional attributes. In the modeling stage, we integrate a k-medoids clustering method and a path-size logit model into our system to enable analysts to model route choice behaviors from trips with support for feature selection, hyperparameter tuning, and model comparison. Finally, in the reasoning stage, we help analysts rationalize and refine the model by selectively inspecting the trips that strongly support the modeling result. For evaluation, we conducted a case study and interviews with domain experts. The domain experts discovered unexpected insights from numerous modeling results, allowing them to explore the hyperparameter space more effectively to gain better results. In addition, they gained OD- and road-level insights into which data mainly supported the modeling result, enabling further discussion of the model.",
                        "uid": "v-tvcg-9632413",
                        "file_name": "v-tvcg-9632413_Shin_Presentation.mp4",
                        "time_stamp": "2022-10-20T21:45:00Z",
                        "time_start": "2022-10-20T21:45:00Z",
                        "time_end": "2022-10-20T21:54:00Z",
                        "paper_type": "full",
                        "keywords": [
                            "route choice modeling, urban planning, trajectory data, origin-destination, visual analytics"
                        ],
                        "has_image": "1",
                        "has_video": "535",
                        "paper_award": "",
                        "image_caption": "We present RCMVis, a visual analytics system to support interactive Route Choice Modeling analysis. It aims to model which characteristics of routes, such as distance and the number of traffic lights, affect travelers' route choice behaviors and how much they affect the choice during their trips. Through close collaboration with domain experts, they could make meaningful discoveries about the data and the models they developed, including geographical distributions of traffic, the hyperparameter space of the models, and data-level insights to help interpret models.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "https://youtu.be/MOT0_Z3XHTo",
                        "ff_id": "MOT0_Z3XHTo"
                    },
                    {
                        "slot_id": "v-tvcg-9632413-qa",
                        "session_id": "full14",
                        "type": "Virtual Q+A",
                        "title": "RCMVis: A Visual Analytics System for Route Choice Modeling (Q+A)",
                        "contributors": [
                            "DongHwa Shin"
                        ],
                        "authors": [],
                        "abstract": "We present RCMVis, a visual analytics system to support interactive Route Choice Modeling analysis. It aims to model which characteristics of routes, such as distance and the number of traffic lights, affect travelers' route choice behaviors and how much they affect the choice during their trips. Through close collaboration with domain experts, we designed a visual analytics framework for Route Choice Modeling. The framework supports three interactive analysis stages: exploration, modeling, and reasoning. In the exploration stage, we help analysts interactively explore trip data from multiple origin-destination (OD) pairs and choose a subset of data they want to focus on. To this end, we provide coordinated multiple OD views with different foci that allow analysts to inspect, rank, and compare OD pairs in terms of their multidimensional attributes. In the modeling stage, we integrate a k-medoids clustering method and a path-size logit model into our system to enable analysts to model route choice behaviors from trips with support for feature selection, hyperparameter tuning, and model comparison. Finally, in the reasoning stage, we help analysts rationalize and refine the model by selectively inspecting the trips that strongly support the modeling result. For evaluation, we conducted a case study and interviews with domain experts. The domain experts discovered unexpected insights from numerous modeling results, allowing them to explore the hyperparameter space more effectively to gain better results. In addition, they gained OD- and road-level insights into which data mainly supported the modeling result, enabling further discussion of the model.",
                        "uid": "v-tvcg-9632413",
                        "file_name": "",
                        "time_stamp": "2022-10-20T21:54:00Z",
                        "time_start": "2022-10-20T21:54:00Z",
                        "time_end": "2022-10-20T21:57:00Z",
                        "paper_type": "full",
                        "keywords": [
                            "route choice modeling, urban planning, trajectory data, origin-destination, visual analytics"
                        ],
                        "has_image": "1",
                        "has_video": "535",
                        "paper_award": "",
                        "image_caption": "We present RCMVis, a visual analytics system to support interactive Route Choice Modeling analysis. It aims to model which characteristics of routes, such as distance and the number of traffic lights, affect travelers' route choice behaviors and how much they affect the choice during their trips. Through close collaboration with domain experts, they could make meaningful discoveries about the data and the models they developed, including geographical distributions of traffic, the hyperparameter space of the models, and data-level insights to help interpret models.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "https://youtu.be/MOT0_Z3XHTo",
                        "ff_id": "MOT0_Z3XHTo"
                    }
                ]
            },
            {
                "title": "Sports Vis",
                "session_id": "full15",
                "event_prefix": "v-full",
                "track": "ok6",
                "livestream_id": "ok6-thu",
                "session_image": "full15.png",
                "chair": [
                    "Johanna Schmidt"
                ],
                "organizers": [],
                "time_start": "2022-10-20T15:45:00Z",
                "time_end": "2022-10-20T17:00:00Z",
                "discord_category": "",
                "discord_channel": "oklahoma-st-6",
                "discord_channel_id": "1024600906689421372",
                "discord_link": "https://discord.com/channels/978320435554947082/1024600906689421372",
                "slido_link": "https://app.sli.do/event/i28UETZaiRWr1XwX6eKjcf?section=29ba1f79-04cd-4568-a83c-ab81aa3c28b8",
                "youtube_url": "https://youtu.be/KIIASF6gEIE",
                "youtube_id": "KIIASF6gEIE",
                "zoom_meeting": "",
                "zoom_password": "",
                "zoom_link": "",
                "ff_link": "https://youtu.be/GMpdqB7BYs4",
                "ff_playlist": "",
                "time_slots": [
                    {
                        "slot_id": "full15-opening",
                        "session_id": "full15",
                        "type": "In Person Other",
                        "title": "Session Opening",
                        "contributors": [
                            "Johanna Schmidt"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "",
                        "file_name": "",
                        "time_stamp": "2022-10-20T15:45:00Z",
                        "time_start": "2022-10-20T15:45:00Z",
                        "time_end": "2022-10-20T15:45:00Z",
                        "paper_type": "",
                        "keywords": [],
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "v-full-1238-pres",
                        "session_id": "full15",
                        "type": "In Person Presentation",
                        "title": "Sporthesia: Augmenting Sports Videos Using Natural Language",
                        "contributors": [
                            "Zhutian Chen"
                        ],
                        "authors": [
                            "Zhutian Chen",
                            "Qisen Yang",
                            "Xiao Xie",
                            "Johanna Beyer",
                            "Haijun Xia",
                            "Yingcai Wu",
                            "Hanspeter Pfister"
                        ],
                        "abstract": "Augmented sports videos, which combine visualizations and video effects to present data in actual scenes, can communicate insights engagingly and thus have been increasingly popular for sports enthusiasts around the world. Yet, creating augmented sports videos remains a challenging task, requiring considerable time and video editing skills. On the other hand, sports insights are often communicated using natural languages, such as in commentaries, oral presentations, and articles, but usually lack visual cues. Thus, this work aims to facilitate the creation of augmented sports videos by enabling analysts to directly create visualizations embedded in videos using insights expressed in natural language. To achieve this goal, we proposed a three-step approach \u2013 1) detecting visualizable entities in the text, 2) mapping these entities into visualizations, and 3) scheduling these visualizations to play with the video \u2013 and analyzed 155 sports video clips and the accompanying commentaries for accomplishing these steps. Informed by our analysis, we have designed and implemented Sporthesia, a proof-of-concept system that takes racket-based sports videos and textual commentaries as the input and outputs augmented videos. We demonstrate Sporthesia\u2019s applicability in two exemplar scenarios, i.e., authoring augmented sports videos using text and augmenting historical sports videos based on auditory comments. A technical evaluation shows that Sporthesia achieves high accuracy (F1-score of 0.9) in detecting visualizable entities in the text. An expert evaluation with eight sports analysts suggests high utility, effectiveness, and satisfaction with our language-driven authoring method and provides insights for future improvement and opportunities.",
                        "uid": "v-full-1238",
                        "file_name": "v-full-1238_Chen_Presentation.mp4",
                        "time_stamp": "2022-10-20T15:45:00Z",
                        "time_start": "2022-10-20T15:45:00Z",
                        "time_end": "2022-10-20T15:54:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "488",
                        "paper_award": "",
                        "image_caption": "Sporthesia takes raw video footage and commentary text of racket-based sports as input, and outputs an augmented video. To achieve this, three key steps are taken: 1) detecting the visualizable entities in the text, 2) mapping the entities to visualizations, and 3) scheduling the visualizations to play with the raw video.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "https://youtu.be/Uoi1FlLWk6I",
                        "ff_id": "Uoi1FlLWk6I"
                    },
                    {
                        "slot_id": "v-full-1238-qa",
                        "session_id": "full15",
                        "type": "In Person Q+A",
                        "title": "Sporthesia: Augmenting Sports Videos Using Natural Language (Q+A)",
                        "contributors": [
                            "Zhutian Chen"
                        ],
                        "authors": [],
                        "abstract": "Augmented sports videos, which combine visualizations and video effects to present data in actual scenes, can communicate insights engagingly and thus have been increasingly popular for sports enthusiasts around the world. Yet, creating augmented sports videos remains a challenging task, requiring considerable time and video editing skills. On the other hand, sports insights are often communicated using natural languages, such as in commentaries, oral presentations, and articles, but usually lack visual cues. Thus, this work aims to facilitate the creation of augmented sports videos by enabling analysts to directly create visualizations embedded in videos using insights expressed in natural language. To achieve this goal, we proposed a three-step approach \u2013 1) detecting visualizable entities in the text, 2) mapping these entities into visualizations, and 3) scheduling these visualizations to play with the video \u2013 and analyzed 155 sports video clips and the accompanying commentaries for accomplishing these steps. Informed by our analysis, we have designed and implemented Sporthesia, a proof-of-concept system that takes racket-based sports videos and textual commentaries as the input and outputs augmented videos. We demonstrate Sporthesia\u2019s applicability in two exemplar scenarios, i.e., authoring augmented sports videos using text and augmenting historical sports videos based on auditory comments. A technical evaluation shows that Sporthesia achieves high accuracy (F1-score of 0.9) in detecting visualizable entities in the text. An expert evaluation with eight sports analysts suggests high utility, effectiveness, and satisfaction with our language-driven authoring method and provides insights for future improvement and opportunities.",
                        "uid": "v-full-1238",
                        "file_name": "",
                        "time_stamp": "2022-10-20T15:54:00Z",
                        "time_start": "2022-10-20T15:54:00Z",
                        "time_end": "2022-10-20T15:57:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "488",
                        "paper_award": "",
                        "image_caption": "Sporthesia takes raw video footage and commentary text of racket-based sports as input, and outputs an augmented video. To achieve this, three key steps are taken: 1) detecting the visualizable entities in the text, 2) mapping the entities to visualizations, and 3) scheduling the visualizations to play with the raw video.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "https://youtu.be/Uoi1FlLWk6I",
                        "ff_id": "Uoi1FlLWk6I"
                    },
                    {
                        "slot_id": "v-full-1083-pres",
                        "session_id": "full15",
                        "type": "Virtual Presentation (pre-recorded)",
                        "title": "OBTracker: Visual Analytics of Off-ball Movements in Basketball",
                        "contributors": [
                            "Yihong Wu"
                        ],
                        "authors": [
                            "Yihong Wu",
                            "Dazhen Deng",
                            "Xiao Xie",
                            "Moqi He",
                            "Jie Xu",
                            "Hongzeng Zhang",
                            "Hui Zhang",
                            "Yingcai Wu"
                        ],
                        "abstract": "In a basketball play, players who are not in possession of the ball (i.e., off-ball players) can still effectively contribute to the team\u2019s offense, such as making a sudden move to create scoring opportunities. Analyzing the movements of off-ball players can thus facilitate the development of effective strategies for coaches. However, common basketball statistics (e.g., points and assists) primarily focus on what happens around the ball and are mostly result-oriented, making it challenging to objectively assess and fully understand the contributions of off-ball movements. To address these challenges, we collaborate closely with domain experts and summarize the multi-level requirements for off-ball movement analysis in basketball. We first establish an assessment model to quantitatively evaluate the offensive contribution of an off-ball movement considering both the position of players and the team cooperation. Based on the model, we design and develop a visual analytics system called OBTracker to support the multifaceted analysis of off-ball movements. OBTracker enables users to identify the frequency and effectiveness of off-ball movement patterns and learn the performance of different off-ball players. A tailored visualization based on the Voronoi diagram is proposed to help users interpret the contribution of off-ball movements from a temporal perspective. We conduct two case studies based on the tracking data from NBA games and demonstrate the effectiveness and usability of OBTracker through expert feedback.",
                        "uid": "v-full-1083",
                        "file_name": "v-full-1083_Wu_Presentation.mp4",
                        "time_stamp": "2022-10-20T15:57:00Z",
                        "time_start": "2022-10-20T15:57:00Z",
                        "time_end": "2022-10-20T16:06:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "580",
                        "paper_award": "",
                        "image_caption": "The image shows the system interface of OBTracker. (A) The summary view provides navigation of basketball teams and presents a team\u2019s typical off-ball movement patterns. (B) The player view shows a list of player combinations and their performance when executing a specific type of off-ball movement. (C) The explanation view illustrates why an off-ball movement is effective from the aspects of player positioning and team cooperation.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "https://youtu.be/AGYqHPqQ8_4",
                        "ff_id": "AGYqHPqQ8_4"
                    },
                    {
                        "slot_id": "v-full-1083-qa",
                        "session_id": "full15",
                        "type": "Virtual Q+A",
                        "title": "OBTracker: Visual Analytics of Off-ball Movements in Basketball (Q+A)",
                        "contributors": [
                            "Yihong Wu"
                        ],
                        "authors": [],
                        "abstract": "In a basketball play, players who are not in possession of the ball (i.e., off-ball players) can still effectively contribute to the team\u2019s offense, such as making a sudden move to create scoring opportunities. Analyzing the movements of off-ball players can thus facilitate the development of effective strategies for coaches. However, common basketball statistics (e.g., points and assists) primarily focus on what happens around the ball and are mostly result-oriented, making it challenging to objectively assess and fully understand the contributions of off-ball movements. To address these challenges, we collaborate closely with domain experts and summarize the multi-level requirements for off-ball movement analysis in basketball. We first establish an assessment model to quantitatively evaluate the offensive contribution of an off-ball movement considering both the position of players and the team cooperation. Based on the model, we design and develop a visual analytics system called OBTracker to support the multifaceted analysis of off-ball movements. OBTracker enables users to identify the frequency and effectiveness of off-ball movement patterns and learn the performance of different off-ball players. A tailored visualization based on the Voronoi diagram is proposed to help users interpret the contribution of off-ball movements from a temporal perspective. We conduct two case studies based on the tracking data from NBA games and demonstrate the effectiveness and usability of OBTracker through expert feedback.",
                        "uid": "v-full-1083",
                        "file_name": "",
                        "time_stamp": "2022-10-20T16:06:00Z",
                        "time_start": "2022-10-20T16:06:00Z",
                        "time_end": "2022-10-20T16:09:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "580",
                        "paper_award": "",
                        "image_caption": "The image shows the system interface of OBTracker. (A) The summary view provides navigation of basketball teams and presents a team\u2019s typical off-ball movement patterns. (B) The player view shows a list of player combinations and their performance when executing a specific type of off-ball movement. (C) The explanation view illustrates why an off-ball movement is effective from the aspects of player positioning and team cooperation.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "https://youtu.be/AGYqHPqQ8_4",
                        "ff_id": "AGYqHPqQ8_4"
                    },
                    {
                        "slot_id": "v-tvcg-9802784-pres",
                        "session_id": "full15",
                        "type": "Virtual Presentation (pre-recorded)",
                        "title": "Visualization in Motion: A Research Agenda and Two Evaluations",
                        "contributors": [
                            "Lijie Yao"
                        ],
                        "authors": [
                            "Lijie Yao",
                            "Anastasia Bezerianos",
                            "Romain Vuillemot",
                            "Petra Isenberg"
                        ],
                        "abstract": "We contribute a research agenda for visualization in motion and two experiments to understand how well viewers can read data from moving visualizations. We define visualizations in motion as visual data representations that are used in contexts that exhibit relative motion between a viewer and an entire visualization. Sports analytics, video games, wearable devices, or data physicalizations are example contexts that involve different types of relative motion between a viewer and a visualization. To analyze the opportunities and challenges for designing visualization in motion, we show example scenarios and outline a first research agenda. Motivated primarily by the prevalence of and opportunities for visualizations in sports and video games we started to investigate a small aspect of our research agenda: the impact of two important characteristics of motion---speed and trajectory on a stationary viewer's ability to read data from moving donut and bar charts. We found that increasing speed and trajectory complexity did negatively affect the accuracy of reading values from the charts and that bar charts were more negatively impacted. In practice, however, this impact was small: both charts were still read fairly accurately.",
                        "uid": "v-tvcg-9802784",
                        "file_name": "v-tvcg-9802784_Yao_Presentation.mp4",
                        "time_stamp": "2022-10-20T16:09:00Z",
                        "time_start": "2022-10-20T16:09:00Z",
                        "time_end": "2022-10-20T16:18:00Z",
                        "paper_type": "full",
                        "keywords": [
                            "Situated Visualization, Visualization in Motion, Research Agenda, Empirical Study"
                        ],
                        "has_image": "1",
                        "has_video": "536",
                        "paper_award": "",
                        "image_caption": "This is the logo of \"Visualization in motion\", as a entire new research direction.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "https://youtu.be/qsvFMjYbuL0",
                        "ff_id": "qsvFMjYbuL0"
                    },
                    {
                        "slot_id": "v-tvcg-9802784-qa",
                        "session_id": "full15",
                        "type": "Virtual Q+A",
                        "title": "Visualization in Motion: A Research Agenda and Two Evaluations (Q+A)",
                        "contributors": [
                            "Lijie Yao"
                        ],
                        "authors": [],
                        "abstract": "We contribute a research agenda for visualization in motion and two experiments to understand how well viewers can read data from moving visualizations. We define visualizations in motion as visual data representations that are used in contexts that exhibit relative motion between a viewer and an entire visualization. Sports analytics, video games, wearable devices, or data physicalizations are example contexts that involve different types of relative motion between a viewer and a visualization. To analyze the opportunities and challenges for designing visualization in motion, we show example scenarios and outline a first research agenda. Motivated primarily by the prevalence of and opportunities for visualizations in sports and video games we started to investigate a small aspect of our research agenda: the impact of two important characteristics of motion---speed and trajectory on a stationary viewer's ability to read data from moving donut and bar charts. We found that increasing speed and trajectory complexity did negatively affect the accuracy of reading values from the charts and that bar charts were more negatively impacted. In practice, however, this impact was small: both charts were still read fairly accurately.",
                        "uid": "v-tvcg-9802784",
                        "file_name": "",
                        "time_stamp": "2022-10-20T16:18:00Z",
                        "time_start": "2022-10-20T16:18:00Z",
                        "time_end": "2022-10-20T16:21:00Z",
                        "paper_type": "full",
                        "keywords": [
                            "Situated Visualization, Visualization in Motion, Research Agenda, Empirical Study"
                        ],
                        "has_image": "1",
                        "has_video": "536",
                        "paper_award": "",
                        "image_caption": "This is the logo of \"Visualization in motion\", as a entire new research direction.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "https://youtu.be/qsvFMjYbuL0",
                        "ff_id": "qsvFMjYbuL0"
                    },
                    {
                        "slot_id": "v-full-1041-pres",
                        "session_id": "full15",
                        "type": "In Person Presentation",
                        "title": "RASIPAM: Interactive Pattern Mining of Multivariate Event Sequences in Racket Sports",
                        "contributors": [
                            "Jiang Wu",
                            "Ziyang Guo"
                        ],
                        "authors": [
                            "Jiang Wu",
                            "Dongyu Liu",
                            "Ziyang Guo",
                            "Yingcai Wu"
                        ],
                        "abstract": "Experts in racket sports like tennis and badminton use tactical analysis to gain insight into competitors' playing styles. Many data-driven methods apply pattern mining to racket sports data \u2014 which is often recorded as multivariate event sequences \u2014 to uncover sports tactics. However, tactics obtained in this way are often inconsistent with those deduced by experts through their domain knowledge, which can be confusing to those experts. This work introduces RASIPAM, a RAcket-Sports Interactive PAttern Mining system, which allows experts to incorporate their knowledge into data mining algorithms to discover meaningful tactics interactively. RASIPAM consists of a constraint-based pattern mining algorithm that responds to the analysis demands of experts: Experts provide suggestions for finding tactics in intuitive written language, and these suggestions are translated into constraints to run the algorithm. RASIPAM further introduces a tailored visual interface that allows experts to compare the new tactics with the original ones and decide whether to apply a given adjustment. This interactive workflow iteratively progresses until experts are satisfied with all tactics. We conduct a quantitative experiment to show that our algorithm supports real-time interaction. Two case studies in tennis and in badminton respectively, each involving two domain experts, are conducted to show the effectiveness and usefulness of the system.",
                        "uid": "v-full-1041",
                        "file_name": "v-full-1041_Wu_Presentation.mp4",
                        "time_stamp": "2022-10-20T16:21:00Z",
                        "time_start": "2022-10-20T16:21:00Z",
                        "time_end": "2022-10-20T16:30:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "555",
                        "paper_award": "",
                        "image_caption": "A screenshot of our system for interactive tactic mining. After domain experts select a player of interest on the control bar (A), the system mines an initial set of tactics and visualizes them in the Projection View (E) and the Tactic View (C). Experts can give suggestions to improve the tactics based on their domain knowledge in the Suggestion Panel (B). The system will refine the tactic set and compare the new tactics with the original ones (C1 and E1). The Rally View (D) shows more details about a chosen tactic.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "https://youtu.be/CL8HxPjKHuI",
                        "ff_id": "CL8HxPjKHuI"
                    },
                    {
                        "slot_id": "v-full-1041-qa",
                        "session_id": "full15",
                        "type": "In Person Q+A",
                        "title": "RASIPAM: Interactive Pattern Mining of Multivariate Event Sequences in Racket Sports (Q+A)",
                        "contributors": [
                            "Jiang Wu",
                            "Ziyang Guo"
                        ],
                        "authors": [],
                        "abstract": "Experts in racket sports like tennis and badminton use tactical analysis to gain insight into competitors' playing styles. Many data-driven methods apply pattern mining to racket sports data \u2014 which is often recorded as multivariate event sequences \u2014 to uncover sports tactics. However, tactics obtained in this way are often inconsistent with those deduced by experts through their domain knowledge, which can be confusing to those experts. This work introduces RASIPAM, a RAcket-Sports Interactive PAttern Mining system, which allows experts to incorporate their knowledge into data mining algorithms to discover meaningful tactics interactively. RASIPAM consists of a constraint-based pattern mining algorithm that responds to the analysis demands of experts: Experts provide suggestions for finding tactics in intuitive written language, and these suggestions are translated into constraints to run the algorithm. RASIPAM further introduces a tailored visual interface that allows experts to compare the new tactics with the original ones and decide whether to apply a given adjustment. This interactive workflow iteratively progresses until experts are satisfied with all tactics. We conduct a quantitative experiment to show that our algorithm supports real-time interaction. Two case studies in tennis and in badminton respectively, each involving two domain experts, are conducted to show the effectiveness and usefulness of the system.",
                        "uid": "v-full-1041",
                        "file_name": "",
                        "time_stamp": "2022-10-20T16:30:00Z",
                        "time_start": "2022-10-20T16:30:00Z",
                        "time_end": "2022-10-20T16:33:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "555",
                        "paper_award": "",
                        "image_caption": "A screenshot of our system for interactive tactic mining. After domain experts select a player of interest on the control bar (A), the system mines an initial set of tactics and visualizes them in the Projection View (E) and the Tactic View (C). Experts can give suggestions to improve the tactics based on their domain knowledge in the Suggestion Panel (B). The system will refine the tactic set and compare the new tactics with the original ones (C1 and E1). The Rally View (D) shows more details about a chosen tactic.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "https://youtu.be/CL8HxPjKHuI",
                        "ff_id": "CL8HxPjKHuI"
                    },
                    {
                        "slot_id": "v-full-1031-pres",
                        "session_id": "full15",
                        "type": "Virtual Presentation (pre-recorded)",
                        "title": "Tac-Trainer: A Visual Analytics System for IoT-based Racket Sports Training",
                        "contributors": [
                            "Jiachen Wang"
                        ],
                        "authors": [
                            "Jiachen Wang",
                            "Ji Ma",
                            "Kangping Hu",
                            "Zheng Zhou",
                            "Hui Zhang",
                            "Xiao Xie",
                            "Yingcai Wu"
                        ],
                        "abstract": "Conventional racket sports training highly relies on coaches' knowledge and experience, leading to biases in the guidance. To solve this problem, smart wearable devices based on Internet of Things technology (IoT) have been extensively investigated to support data-driven training. Considerable studies introduced methods to extract valuable information from the sensor data collected by IoT devices. However, the information cannot provide actionable insights for coaches due to the large data volume and high data dimensions. We proposed an IoT + VA framework, Tac-Trainer, to integrate the sensor data, the information, and coaches' knowledge to facilitate racket sports training. Tac-Trainer consists of four components: device configuration, data interpretation, training optimization, and result visualization. These components collect trainees' kinematic data through IoT devices, transform the data into attributes and indicators, generate training suggestions, and provide an interactive visualization interface for exploration, respectively. We further discuss new research opportunities and challenges inspired by our work from two perspectives, VA for IoT and IoT for VA.",
                        "uid": "v-full-1031",
                        "file_name": "v-full-1031_Wang_Presentation.mp4",
                        "time_stamp": "2022-10-20T16:33:00Z",
                        "time_start": "2022-10-20T16:33:00Z",
                        "time_end": "2022-10-20T16:42:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "540",
                        "paper_award": "",
                        "image_caption": "Tac-Trainer contains a training view and a suggestion view. The training view visualizes the strokes in a training session through a customized flow (C) consisting of a metadata panel (A), a control panel (B), and a stroke flow (F). The suggestion view provides a list of optimization suggestions (J) for a poorly-performed stroke (G). Each suggestion can be explored in a 3-D coordinate (K). The interface presents the details of Case 1. E is the first training session and D is the second. G is the stroke chosen for optimization. L is the optimization suggestion chosen by the coach.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "https://youtu.be/Da-OM9bMbC8",
                        "ff_id": "Da-OM9bMbC8"
                    },
                    {
                        "slot_id": "v-full-1031-qa",
                        "session_id": "full15",
                        "type": "Virtual Q+A",
                        "title": "Tac-Trainer: A Visual Analytics System for IoT-based Racket Sports Training (Q+A)",
                        "contributors": [
                            "Jiachen Wang"
                        ],
                        "authors": [],
                        "abstract": "Conventional racket sports training highly relies on coaches' knowledge and experience, leading to biases in the guidance. To solve this problem, smart wearable devices based on Internet of Things technology (IoT) have been extensively investigated to support data-driven training. Considerable studies introduced methods to extract valuable information from the sensor data collected by IoT devices. However, the information cannot provide actionable insights for coaches due to the large data volume and high data dimensions. We proposed an IoT + VA framework, Tac-Trainer, to integrate the sensor data, the information, and coaches' knowledge to facilitate racket sports training. Tac-Trainer consists of four components: device configuration, data interpretation, training optimization, and result visualization. These components collect trainees' kinematic data through IoT devices, transform the data into attributes and indicators, generate training suggestions, and provide an interactive visualization interface for exploration, respectively. We further discuss new research opportunities and challenges inspired by our work from two perspectives, VA for IoT and IoT for VA.",
                        "uid": "v-full-1031",
                        "file_name": "",
                        "time_stamp": "2022-10-20T16:42:00Z",
                        "time_start": "2022-10-20T16:42:00Z",
                        "time_end": "2022-10-20T16:45:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "540",
                        "paper_award": "",
                        "image_caption": "Tac-Trainer contains a training view and a suggestion view. The training view visualizes the strokes in a training session through a customized flow (C) consisting of a metadata panel (A), a control panel (B), and a stroke flow (F). The suggestion view provides a list of optimization suggestions (J) for a poorly-performed stroke (G). Each suggestion can be explored in a 3-D coordinate (K). The interface presents the details of Case 1. E is the first training session and D is the second. G is the stroke chosen for optimization. L is the optimization suggestion chosen by the coach.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "https://youtu.be/Da-OM9bMbC8",
                        "ff_id": "Da-OM9bMbC8"
                    },
                    {
                        "slot_id": "v-full-1223-pres",
                        "session_id": "full15",
                        "type": "In Person Presentation",
                        "title": "The Quest for Omnioculars: Embedded Visualization for Augmenting Basketball Game Viewing Experiences",
                        "contributors": [
                            "Tica Lin"
                        ],
                        "authors": [
                            "Tica Lin",
                            "Zhutian Chen",
                            "Yalong Yang",
                            "Daniele Chiappalupi",
                            "Johanna Beyer",
                            "Hanspeter Pfister"
                        ],
                        "abstract": "Sports game data is becoming increasingly complex, often consisting of multivariate data such as player performance stats, historical team records, and athletes\u2019 positional tracking information. While numerous visual analytics systems have been developed for sports analysts to derive insights, few tools target fans to improve their understanding and engagement of sports data during live games. By presenting extra data in the actual game views, embedded visualization has the potential to enhance fans\u2019 game-viewing experience. However, little is known about how to design such kinds of visualizations embedded into live games. In this work, we present a user-centered design study of developing interactive embedded visualizations for basketball fans to improve their live game-watching experiences. We first conducted a formative study to characterize basketball fans\u2019 in-game analysis behaviors and tasks. Based on our findings, we propose a design framework to inform the design of embedded visualizations based on specific data-seeking contexts. Following the design framework, we present five novel embedded visualization designs targeting five representative contexts identified by the fans, including shooting, offense, defense, player evaluation, and team comparison. We then developed Omnioculars, an interactive basketball game-viewing prototype that features the proposed embedded visualizations for fans\u2019 in-game data analysis. We evaluated Omnioculars in a simulated basketball game with basketball fans. The study results suggest that our design supports personalized in-game data analysis and enhances game understanding and engagement.",
                        "uid": "v-full-1223",
                        "file_name": "v-full-1223_Lin_Presentation.mp4",
                        "time_stamp": "2022-10-20T16:45:00Z",
                        "time_start": "2022-10-20T16:45:00Z",
                        "time_end": "2022-10-20T16:54:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "626",
                        "paper_award": "",
                        "image_caption": "Our Omnioculars prototype supports a personalized and interactive game viewing experience with embedded visualizations for in-game analysis. We created a simulated basketball game environment to support our design probe of embedded visualizations.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "https://youtu.be/1hr1t5ZjZ9g",
                        "ff_id": "1hr1t5ZjZ9g"
                    },
                    {
                        "slot_id": "v-full-1223-qa",
                        "session_id": "full15",
                        "type": "In Person Q+A",
                        "title": "The Quest for Omnioculars: Embedded Visualization for Augmenting Basketball Game Viewing Experiences (Q+A)",
                        "contributors": [
                            "Tica Lin"
                        ],
                        "authors": [],
                        "abstract": "Sports game data is becoming increasingly complex, often consisting of multivariate data such as player performance stats, historical team records, and athletes\u2019 positional tracking information. While numerous visual analytics systems have been developed for sports analysts to derive insights, few tools target fans to improve their understanding and engagement of sports data during live games. By presenting extra data in the actual game views, embedded visualization has the potential to enhance fans\u2019 game-viewing experience. However, little is known about how to design such kinds of visualizations embedded into live games. In this work, we present a user-centered design study of developing interactive embedded visualizations for basketball fans to improve their live game-watching experiences. We first conducted a formative study to characterize basketball fans\u2019 in-game analysis behaviors and tasks. Based on our findings, we propose a design framework to inform the design of embedded visualizations based on specific data-seeking contexts. Following the design framework, we present five novel embedded visualization designs targeting five representative contexts identified by the fans, including shooting, offense, defense, player evaluation, and team comparison. We then developed Omnioculars, an interactive basketball game-viewing prototype that features the proposed embedded visualizations for fans\u2019 in-game data analysis. We evaluated Omnioculars in a simulated basketball game with basketball fans. The study results suggest that our design supports personalized in-game data analysis and enhances game understanding and engagement.",
                        "uid": "v-full-1223",
                        "file_name": "",
                        "time_stamp": "2022-10-20T16:54:00Z",
                        "time_start": "2022-10-20T16:54:00Z",
                        "time_end": "2022-10-20T16:57:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "626",
                        "paper_award": "",
                        "image_caption": "Our Omnioculars prototype supports a personalized and interactive game viewing experience with embedded visualizations for in-game analysis. We created a simulated basketball game environment to support our design probe of embedded visualizations.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "https://youtu.be/1hr1t5ZjZ9g",
                        "ff_id": "1hr1t5ZjZ9g"
                    }
                ]
            },
            {
                "title": "Visual Analytics of Health Data",
                "session_id": "full16",
                "event_prefix": "v-full",
                "track": "ok5",
                "livestream_id": "ok5-fri",
                "session_image": "full16.png",
                "chair": [
                    "Bum Chul Kwon"
                ],
                "organizers": [],
                "time_start": "2022-10-21T14:00:00Z",
                "time_end": "2022-10-21T15:15:00Z",
                "discord_category": "",
                "discord_channel": "oklahoma-st-5",
                "discord_channel_id": "1024600839295356939",
                "discord_link": "https://discord.com/channels/978320435554947082/1024600839295356939",
                "slido_link": "https://app.sli.do/event/i28UETZaiRWr1XwX6eKjcf?section=d355f89b-fbd2-4abf-9958-741607905551",
                "youtube_url": "https://youtu.be/wdfkYM2dw0Y",
                "youtube_id": "wdfkYM2dw0Y",
                "zoom_meeting": "",
                "zoom_password": "",
                "zoom_link": "",
                "ff_link": "https://youtu.be/MBhfw849tKs",
                "ff_playlist": "",
                "time_slots": [
                    {
                        "slot_id": "full16-opening",
                        "session_id": "full16",
                        "type": "In Person Other",
                        "title": "Session Opening",
                        "contributors": [
                            "Bum Chul Kwon"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "",
                        "file_name": "",
                        "time_stamp": "2022-10-21T14:00:00Z",
                        "time_start": "2022-10-21T14:00:00Z",
                        "time_end": "2022-10-21T14:00:00Z",
                        "paper_type": "",
                        "keywords": [],
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "v-tvcg-9779066-pres",
                        "session_id": "full16",
                        "type": "In Person Presentation",
                        "title": "ClinicalPath: a Visualization tool to Improve the Evaluation of Electronic Health Records in Clinical Decision-Making",
                        "contributors": [
                            "Claudio Linhares"
                        ],
                        "authors": [
                            "Claudio D. G. Linhares",
                            "Daniel M. Lima",
                            "Jean R. Ponciano",
                            "Mauro M. Olivatto",
                            "Marco A. Gutierrez",
                            "Jorge Poco",
                            "Caetano Traina Jr.",
                            "Agma J. M. Traina"
                        ],
                        "abstract": "Physicians work at a very tight schedule and need decision-making support tools to help on improving and doing their work in a timely and dependable manner. Examining piles of sheets with test results and using systems with little visualization support to provide diagnostics is daunting, but that is still the usual way for the physicians' daily procedure, especially in developing countries. Electronic Health Records systems have been designed to keep the patients' history and reduce the time spent analyzing the patient's data. However, better tools to support decision-making are still needed. In this paper, we propose \\systemname, a visualization tool for users to track a patient's clinical path through a series of tests and data, which can aid in treatments and diagnoses. Our proposal is focused on patient's data analysis, presenting the test results and clinical history longitudinally. Both the visualization design and the system functionality were developed in close collaboration with experts in the medical domain to ensure a right fit of the technical solutions and the real needs of the professionals. We validated the proposed visualization based on case studies and user assessments through tasks based on the physician's daily activities. Our results show that our proposed system improves the physicians' experience in decision-making tasks, made with more confidence and better usage of the physicians' time, allowing them to take other needed care for the patients.",
                        "uid": "v-tvcg-9779066",
                        "file_name": "v-tvcg-9779066_Linhares_Presentation.mp4",
                        "time_stamp": "2022-10-21T14:00:00Z",
                        "time_start": "2022-10-21T14:00:00Z",
                        "time_end": "2022-10-21T14:09:00Z",
                        "paper_type": "full",
                        "keywords": [
                            "Information Visualization, Interactive Visualizations, Human-Computer Interaction, Electronic Health Records"
                        ],
                        "has_image": "1",
                        "has_video": "522",
                        "paper_award": "",
                        "image_caption": "Example of the Clinical path visualization divided into the main features: (A) Categorization of the tests; (B) Timestamp information with color codding, note that the dates follow the date format (dd/MM/yyyy); (C) Color and shape codding (symbols) for the test results; and (D) Line chart with the test result variation over time; (E)  Line chart combining different test results over time; (F) Automatic highlighting relevant changes in tests results over time; (G) Patient global and local information over time; (H) Line chart showing patient information over time.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "https://youtu.be/xdwHQ43zBbk",
                        "ff_id": "xdwHQ43zBbk"
                    },
                    {
                        "slot_id": "v-tvcg-9779066-qa",
                        "session_id": "full16",
                        "type": "In Person Q+A",
                        "title": "ClinicalPath: a Visualization tool to Improve the Evaluation of Electronic Health Records in Clinical Decision-Making (Q+A)",
                        "contributors": [
                            "Claudio Linhares"
                        ],
                        "authors": [],
                        "abstract": "Physicians work at a very tight schedule and need decision-making support tools to help on improving and doing their work in a timely and dependable manner. Examining piles of sheets with test results and using systems with little visualization support to provide diagnostics is daunting, but that is still the usual way for the physicians' daily procedure, especially in developing countries. Electronic Health Records systems have been designed to keep the patients' history and reduce the time spent analyzing the patient's data. However, better tools to support decision-making are still needed. In this paper, we propose \\systemname, a visualization tool for users to track a patient's clinical path through a series of tests and data, which can aid in treatments and diagnoses. Our proposal is focused on patient's data analysis, presenting the test results and clinical history longitudinally. Both the visualization design and the system functionality were developed in close collaboration with experts in the medical domain to ensure a right fit of the technical solutions and the real needs of the professionals. We validated the proposed visualization based on case studies and user assessments through tasks based on the physician's daily activities. Our results show that our proposed system improves the physicians' experience in decision-making tasks, made with more confidence and better usage of the physicians' time, allowing them to take other needed care for the patients.",
                        "uid": "v-tvcg-9779066",
                        "file_name": "",
                        "time_stamp": "2022-10-21T14:09:00Z",
                        "time_start": "2022-10-21T14:09:00Z",
                        "time_end": "2022-10-21T14:12:00Z",
                        "paper_type": "full",
                        "keywords": [
                            "Information Visualization, Interactive Visualizations, Human-Computer Interaction, Electronic Health Records"
                        ],
                        "has_image": "1",
                        "has_video": "522",
                        "paper_award": "",
                        "image_caption": "Example of the Clinical path visualization divided into the main features: (A) Categorization of the tests; (B) Timestamp information with color codding, note that the dates follow the date format (dd/MM/yyyy); (C) Color and shape codding (symbols) for the test results; and (D) Line chart with the test result variation over time; (E)  Line chart combining different test results over time; (F) Automatic highlighting relevant changes in tests results over time; (G) Patient global and local information over time; (H) Line chart showing patient information over time.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "https://youtu.be/xdwHQ43zBbk",
                        "ff_id": "xdwHQ43zBbk"
                    },
                    {
                        "slot_id": "v-tvcg-9754243-pres",
                        "session_id": "full16",
                        "type": "Virtual Presentation (pre-recorded)",
                        "title": "Visual Assistance in Development and Validation of Bayesian Networks for Clinical Decision Support",
                        "contributors": [
                            "Juliane M\u00fcller-Sielaff,",
                            "Juliane M\u00fcller-Sielaff"
                        ],
                        "authors": [
                            "Juliane M\u00fcller-Sielaff",
                            "Seyed Behnam Beladi",
                            "Stephanie W. Vrede",
                            "Monique Meuschke",
                            "Peter J.F. Lucas",
                            "Johanna M.A. Pijnenborg",
                            "Steffen Oeltze-Jafra"
                        ],
                        "abstract": "The development and validation of Clinical Decision Support Models (CDSM) based on Bayesian networks (BN) is commonly done in a collaborative work between medical researchers providing the domain expertise and computer scientists developing the decision support model. Although modern tools provide facilities for data-driven model generation, domain experts are required to validate the accuracy of the learned model and to provide expert knowledge for fine-tuning it while computer scientists are needed to integrate this knowledge in the learned model (hybrid modeling approach). This generally time-expensive procedure hampers CDSM generation and updating. To address this problem, we developed a novel interactive visual approach allowing medical researchers with less knowledge in CDSM to develop and validate BNs based on domain specific data mainly independently and thus, diminishing the need for an additional computer scientist. In this context, we abstracted and simplified the common workflow in BN development as well as adjusted the workflow to medical experts' needs. We demonstrate our visual approach with data of endometrial cancer patients and evaluated it with six medical researchers who are domain experts in the gynecological field.",
                        "uid": "v-tvcg-9754243",
                        "file_name": "v-tvcg-9754243_Sielaff_Presentation.mp4",
                        "time_stamp": "2022-10-21T14:12:00Z",
                        "time_start": "2022-10-21T14:12:00Z",
                        "time_end": "2022-10-21T14:21:04Z",
                        "paper_type": "full",
                        "keywords": [
                            "Bayesian networks, Visual Analysis, Clinical Decision Support, Causal Model Development"
                        ],
                        "has_image": "1",
                        "has_video": "604",
                        "paper_award": "",
                        "image_caption": "The common development process of Clinical Decision Support Models (CDSM) based on Bayesian networks (BN) requires medical researchers providing the domain expertise and a modeling expert developing the model. This collaborative but generally time-expensive procedure, however, hampers in model generation and updating. Furthermore, because the modelling expert might make design decisions on the experts\u2019 domain, at some stage the medical expert may be confronted with a lack of understanding of the resulting BN model. To address these problems, we gathered the requirements on a visual approach allowing medical researchers to generate and validate CDSMs based on BNs mainly independently.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "https://youtu.be/97uAxjQkBug",
                        "ff_id": "97uAxjQkBug"
                    },
                    {
                        "slot_id": "v-tvcg-9754243-qa",
                        "session_id": "full16",
                        "type": "Virtual Q+A",
                        "title": "Visual Assistance in Development and Validation of Bayesian Networks for Clinical Decision Support (Q+A)",
                        "contributors": [
                            "Juliane M\u00fcller-Sielaff,",
                            "Juliane M\u00fcller-Sielaff"
                        ],
                        "authors": [],
                        "abstract": "The development and validation of Clinical Decision Support Models (CDSM) based on Bayesian networks (BN) is commonly done in a collaborative work between medical researchers providing the domain expertise and computer scientists developing the decision support model. Although modern tools provide facilities for data-driven model generation, domain experts are required to validate the accuracy of the learned model and to provide expert knowledge for fine-tuning it while computer scientists are needed to integrate this knowledge in the learned model (hybrid modeling approach). This generally time-expensive procedure hampers CDSM generation and updating. To address this problem, we developed a novel interactive visual approach allowing medical researchers with less knowledge in CDSM to develop and validate BNs based on domain specific data mainly independently and thus, diminishing the need for an additional computer scientist. In this context, we abstracted and simplified the common workflow in BN development as well as adjusted the workflow to medical experts' needs. We demonstrate our visual approach with data of endometrial cancer patients and evaluated it with six medical researchers who are domain experts in the gynecological field.",
                        "uid": "v-tvcg-9754243",
                        "file_name": "",
                        "time_stamp": "2022-10-21T14:21:04Z",
                        "time_start": "2022-10-21T14:21:04Z",
                        "time_end": "2022-10-21T14:24:00Z",
                        "paper_type": "full",
                        "keywords": [
                            "Bayesian networks, Visual Analysis, Clinical Decision Support, Causal Model Development"
                        ],
                        "has_image": "1",
                        "has_video": "604",
                        "paper_award": "",
                        "image_caption": "The common development process of Clinical Decision Support Models (CDSM) based on Bayesian networks (BN) requires medical researchers providing the domain expertise and a modeling expert developing the model. This collaborative but generally time-expensive procedure, however, hampers in model generation and updating. Furthermore, because the modelling expert might make design decisions on the experts\u2019 domain, at some stage the medical expert may be confronted with a lack of understanding of the resulting BN model. To address these problems, we gathered the requirements on a visual approach allowing medical researchers to generate and validate CDSMs based on BNs mainly independently.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "https://youtu.be/97uAxjQkBug",
                        "ff_id": "97uAxjQkBug"
                    },
                    {
                        "slot_id": "v-full-1228-pres",
                        "session_id": "full16",
                        "type": "In Person Presentation",
                        "title": "ChartWalk: Navigating Large Collections of Text Notes in Electronic Health Records for Clinical Chart Review",
                        "contributors": [
                            "Nicole Sultanum"
                        ],
                        "authors": [
                            "Nicole Sultanum",
                            "Farooq Naeem",
                            "Michael Brudno",
                            "Fanny Chevalier"
                        ],
                        "abstract": "Before seeing a patient for the first time, healthcare workers will typically conduct a comprehensive clinical chart review of the patient's electronic health record (EHR). Within the diverse documentation pieces included there, text notes are among the most important and thoroughly perused segments for this task; and yet they are among the least supported medium in terms of content navigation and overview. In this work, we delve deeper into the task of clinical chart review from a data visualization perspective and propose a hybrid graphics+text approach via ChartWalk, an interactive tool to support the review of text notes in EHRs. We report on our iterative design process grounded in input provided by a diverse range of healthcare professionals, with steps including: (a) initial requirements distilled from interviews and the literature, (b) an interim evaluation to validate design decisions, and (c) a task-based qualitative evaluation of our final design. We contribute lessons learned to better support the design of tools not only for clinical chart reviews but also other healthcare-related tasks around medical text analysis.",
                        "uid": "v-full-1228",
                        "file_name": "v-full-1228_Sultanum_Presentation.mp4",
                        "time_stamp": "2022-10-21T14:24:00Z",
                        "time_start": "2022-10-21T14:24:00Z",
                        "time_end": "2022-10-21T14:33:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "646",
                        "paper_award": "",
                        "image_caption": "Interface Design for ChartWalk, a text visualization tool to support healthcare workers navigate text collections in patient records for clinical overview.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "https://youtu.be/aZg3ufXrqeo",
                        "ff_id": "aZg3ufXrqeo"
                    },
                    {
                        "slot_id": "v-full-1228-qa",
                        "session_id": "full16",
                        "type": "In Person Q+A",
                        "title": "ChartWalk: Navigating Large Collections of Text Notes in Electronic Health Records for Clinical Chart Review (Q+A)",
                        "contributors": [
                            "Nicole Sultanum"
                        ],
                        "authors": [],
                        "abstract": "Before seeing a patient for the first time, healthcare workers will typically conduct a comprehensive clinical chart review of the patient's electronic health record (EHR). Within the diverse documentation pieces included there, text notes are among the most important and thoroughly perused segments for this task; and yet they are among the least supported medium in terms of content navigation and overview. In this work, we delve deeper into the task of clinical chart review from a data visualization perspective and propose a hybrid graphics+text approach via ChartWalk, an interactive tool to support the review of text notes in EHRs. We report on our iterative design process grounded in input provided by a diverse range of healthcare professionals, with steps including: (a) initial requirements distilled from interviews and the literature, (b) an interim evaluation to validate design decisions, and (c) a task-based qualitative evaluation of our final design. We contribute lessons learned to better support the design of tools not only for clinical chart reviews but also other healthcare-related tasks around medical text analysis.",
                        "uid": "v-full-1228",
                        "file_name": "",
                        "time_stamp": "2022-10-21T14:33:00Z",
                        "time_start": "2022-10-21T14:33:00Z",
                        "time_end": "2022-10-21T14:36:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "646",
                        "paper_award": "",
                        "image_caption": "Interface Design for ChartWalk, a text visualization tool to support healthcare workers navigate text collections in patient records for clinical overview.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "https://youtu.be/aZg3ufXrqeo",
                        "ff_id": "aZg3ufXrqeo"
                    },
                    {
                        "slot_id": "v-full-1111-pres",
                        "session_id": "full16",
                        "type": "In Person Presentation",
                        "title": "Development and Evaluation of Two Approaches of Visual Sensitivity Analysis to Support Epidemiological Modeling",
                        "contributors": [
                            "Erik Rydow"
                        ],
                        "authors": [
                            "Erik Rydow",
                            "Rita Borgo",
                            "Hui Fang",
                            "Thomas Torsney-Weir",
                            "Ben Swallow",
                            "Thibaud Porphyre",
                            "Cagatay Turkay",
                            "Min Chen"
                        ],
                        "abstract": "Computational modeling is a commonly used technology in many scientific disciplines and has played a noticeable role in combating the COVID-19 pandemic. Modeling scientists conduct sensitivity analysis frequently to observe and monitor the behavior of a model during its development and deployment. The traditional algorithmic ranking of sensitivity of different parameters usually does not provide modeling scientists with sufficient information to understand the interactions between different parameters and model outputs, while modeling scientists need to observe a large number of model runs in order to gain actionable information for parameter optimization. To address the above challenge, we developed and compared two visual analytics approaches, namely: algorithm-centric and visualization-assisted, and visualization-centric and algorithm-assisted. We evaluated the two approaches based on a structured analysis of different tasks in visual sensitivity analysis as well as the feedback of domain experts. While the work was carried out in the context of epidemiological modeling, the two approaches developed in this work are directly applicable to a variety of modeling processes featuring time series outputs, and can be extended to work with models with other types of outputs.",
                        "uid": "v-full-1111",
                        "file_name": "v-full-1111_Rydow_Presentation.mp4",
                        "time_stamp": "2022-10-21T14:36:00Z",
                        "time_start": "2022-10-21T14:36:00Z",
                        "time_end": "2022-10-21T14:45:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "485",
                        "paper_award": "",
                        "image_caption": "An algorithmic approach to sensitivity analysis typically takes the data of ensemble runs, such as the parameter sets (a) and outputs (b), and estimates the sensitivity of each parameter (c). This algorithmic-centric approach can be assisted by visualization (d) that enriches the basis numerical measures, and be complemented by a visualization-centric and algorithm-assisted approach (e).",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "https://youtu.be/g_GonL3WuTs",
                        "ff_id": "g_GonL3WuTs"
                    },
                    {
                        "slot_id": "v-full-1111-qa",
                        "session_id": "full16",
                        "type": "In Person Q+A",
                        "title": "Development and Evaluation of Two Approaches of Visual Sensitivity Analysis to Support Epidemiological Modeling (Q+A)",
                        "contributors": [
                            "Erik Rydow"
                        ],
                        "authors": [],
                        "abstract": "Computational modeling is a commonly used technology in many scientific disciplines and has played a noticeable role in combating the COVID-19 pandemic. Modeling scientists conduct sensitivity analysis frequently to observe and monitor the behavior of a model during its development and deployment. The traditional algorithmic ranking of sensitivity of different parameters usually does not provide modeling scientists with sufficient information to understand the interactions between different parameters and model outputs, while modeling scientists need to observe a large number of model runs in order to gain actionable information for parameter optimization. To address the above challenge, we developed and compared two visual analytics approaches, namely: algorithm-centric and visualization-assisted, and visualization-centric and algorithm-assisted. We evaluated the two approaches based on a structured analysis of different tasks in visual sensitivity analysis as well as the feedback of domain experts. While the work was carried out in the context of epidemiological modeling, the two approaches developed in this work are directly applicable to a variety of modeling processes featuring time series outputs, and can be extended to work with models with other types of outputs.",
                        "uid": "v-full-1111",
                        "file_name": "",
                        "time_stamp": "2022-10-21T14:45:00Z",
                        "time_start": "2022-10-21T14:45:00Z",
                        "time_end": "2022-10-21T14:48:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "485",
                        "paper_award": "",
                        "image_caption": "An algorithmic approach to sensitivity analysis typically takes the data of ensemble runs, such as the parameter sets (a) and outputs (b), and estimates the sensitivity of each parameter (c). This algorithmic-centric approach can be assisted by visualization (d) that enriches the basis numerical measures, and be complemented by a visualization-centric and algorithm-assisted approach (e).",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "https://youtu.be/g_GonL3WuTs",
                        "ff_id": "g_GonL3WuTs"
                    },
                    {
                        "slot_id": "v-tvcg-9721816-pres",
                        "session_id": "full16",
                        "type": "Virtual Presentation (pre-recorded)",
                        "title": "A framework for evaluating dashboards in healthcare",
                        "contributors": [
                            "Mengdie Zhuang"
                        ],
                        "authors": [
                            "Mengdie Zhuang",
                            "David Concannon",
                            "Ed Manley"
                        ],
                        "abstract": "In the era of \u2018information overload\u2019, effective information provision is essential for enabling rapid response and critical decision making. In making sense of diverse information sources, dashboards have become an indispensable tool, providing fast, effective, adaptable, and personalized access to information for professionals and the general public alike. However, these objectives place heavy requirements on dashboards as information systems in usability and effective design. Understanding these issues is challenging given the absence of consistent and comprehensive approaches to dashboard evaluation. In this paper we systematically review literature on dashboard implementation in healthcare, where dashboards have been employed widely, and where there is widespread interest for improving the current state of the art, and subsequently analyse approaches taken towards evaluation. We draw upon consolidated dashboard literature and our own observations to introduce a general definition of dashboards which is more relevant to current trends, together with seven evaluation scenarios - task performance, behaviour change, interaction workflow,  perceived engagement, potential utility, algorithm performance and system implementation. These scenarios distinguish different evaluation purposes which we illustrate through measurements, example studies, and common challenges in evaluation study design. We provide a breakdown of each evaluation scenario, and highlight some of the more subtle questions. We demonstrate the use of the proposed framework by a design study guided by this framework. We conclude by comparing this framework with existing literature,  outlining a number of active discussion points and a set of dashboard evaluation best practices for the academic, clinical and software development communities alike.",
                        "uid": "v-tvcg-9721816",
                        "file_name": "v-tvcg-9721816_Zhuang_Presentation.mp4",
                        "time_stamp": "2022-10-21T14:48:00Z",
                        "time_start": "2022-10-21T14:48:00Z",
                        "time_end": "2022-10-21T14:57:00Z",
                        "paper_type": "full",
                        "keywords": [
                            "Visualization, Dashboard, Evaluation, Healthcare."
                        ],
                        "has_image": "1",
                        "has_video": "641",
                        "paper_award": "",
                        "image_caption": "A Framework: Seven Scenarios for Evaluating Dashboards",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "https://youtu.be/Ivz_RB0rd3w",
                        "ff_id": "Ivz_RB0rd3w"
                    },
                    {
                        "slot_id": "v-tvcg-9721816-qa",
                        "session_id": "full16",
                        "type": "Virtual Q+A",
                        "title": "A framework for evaluating dashboards in healthcare (Q+A)",
                        "contributors": [
                            "Mengdie Zhuang"
                        ],
                        "authors": [],
                        "abstract": "In the era of \u2018information overload\u2019, effective information provision is essential for enabling rapid response and critical decision making. In making sense of diverse information sources, dashboards have become an indispensable tool, providing fast, effective, adaptable, and personalized access to information for professionals and the general public alike. However, these objectives place heavy requirements on dashboards as information systems in usability and effective design. Understanding these issues is challenging given the absence of consistent and comprehensive approaches to dashboard evaluation. In this paper we systematically review literature on dashboard implementation in healthcare, where dashboards have been employed widely, and where there is widespread interest for improving the current state of the art, and subsequently analyse approaches taken towards evaluation. We draw upon consolidated dashboard literature and our own observations to introduce a general definition of dashboards which is more relevant to current trends, together with seven evaluation scenarios - task performance, behaviour change, interaction workflow,  perceived engagement, potential utility, algorithm performance and system implementation. These scenarios distinguish different evaluation purposes which we illustrate through measurements, example studies, and common challenges in evaluation study design. We provide a breakdown of each evaluation scenario, and highlight some of the more subtle questions. We demonstrate the use of the proposed framework by a design study guided by this framework. We conclude by comparing this framework with existing literature,  outlining a number of active discussion points and a set of dashboard evaluation best practices for the academic, clinical and software development communities alike.",
                        "uid": "v-tvcg-9721816",
                        "file_name": "",
                        "time_stamp": "2022-10-21T14:57:00Z",
                        "time_start": "2022-10-21T14:57:00Z",
                        "time_end": "2022-10-21T15:00:00Z",
                        "paper_type": "full",
                        "keywords": [
                            "Visualization, Dashboard, Evaluation, Healthcare."
                        ],
                        "has_image": "1",
                        "has_video": "641",
                        "paper_award": "",
                        "image_caption": "A Framework: Seven Scenarios for Evaluating Dashboards",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "https://youtu.be/Ivz_RB0rd3w",
                        "ff_id": "Ivz_RB0rd3w"
                    },
                    {
                        "slot_id": "v-full-1584-pres",
                        "session_id": "full16",
                        "type": "In Person Presentation",
                        "title": "Extending the Nested Model for User-Centric XAI: A Design Study on GNN-based Drug Repurposing",
                        "contributors": [
                            "Qianwen Wang"
                        ],
                        "authors": [
                            "Qianwen Wang",
                            "Kexin Huang",
                            "Payal Chandak",
                            "Marinka Zitnik",
                            "Nils Gehlenborg"
                        ],
                        "abstract": "Whether AI explanations can help users achieve specific tasks efficiently (i.e., usable explanations) is significantly influenced by their visual presentation. While many techniques exist to generate explanations, it remains unclear how to select and visually present AI explanations based on the characteristics of domain users. This paper aims to understand this question through a multidisciplinary design study for a specific problem: explaining graph neural network (GNN) predictions to domain experts in drug repurposing, i.e., reuse of existing drugs for new diseases. Building on the nested design model of visualization, we incorporate XAI design considerations from a literature review and from our collaborators\u2019 feedback into the design process. Specifically, we discuss XAI-related design considerations for usable visual explanations at each design layer: target user, usage context, domain explanation, and XAI goal at the domain layer; format, granularity, and operation of explanations at the abstraction layer; encodings and interactions at the visualization layer; and XAI and rendering algorithm at the algorithm layer. We present how the extended nested model motivates and informs the design of DrugExplorer, an XAI tool for drug repurposing. Based on our domain characterization, DrugExplorer provides path-based explanations and presents them both as individual paths and meta-paths for two key XAI operations, why and what else. DrugExplorer offers a novel visualization design called MetaMatrix with a set of interactions to help domain users organize and compare explanation paths at different levels of granularity to generate domain meaningful insights. We demonstrate the effectiveness of the selected visual presentation and DrugExplorer as a whole via a usage scenario, a user study, and expert interviews. From these evaluations, we derive insightful observations and reflections that can inform the design of XAI visualizations for other scientific applications.",
                        "uid": "v-full-1584",
                        "file_name": "v-full-1584_Wang_Presentation.mp4",
                        "time_stamp": "2022-10-21T15:00:00Z",
                        "time_start": "2022-10-21T15:00:00Z",
                        "time_end": "2022-10-21T15:09:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "542",
                        "paper_award": "",
                        "image_caption": "Previous AI visualization tools usually select one specific explanation before the design study based on its popularity in the ML community without considering how the domain characteristics and user needs may influence the selection and visualization of explanations.\nThis paper presents a design study where we investigated how to select and visualize AI explanations for domain users.\nBuilding on the nested design model of visualization, we incorporate XAI design considerations from a literature review and our collaborators' feedback into the design process.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "https://youtu.be/r1QpPuw3zbw",
                        "ff_id": "r1QpPuw3zbw"
                    },
                    {
                        "slot_id": "v-full-1584-qa",
                        "session_id": "full16",
                        "type": "In Person Q+A",
                        "title": "Extending the Nested Model for User-Centric XAI: A Design Study on GNN-based Drug Repurposing (Q+A)",
                        "contributors": [
                            "Qianwen Wang"
                        ],
                        "authors": [],
                        "abstract": "Whether AI explanations can help users achieve specific tasks efficiently (i.e., usable explanations) is significantly influenced by their visual presentation. While many techniques exist to generate explanations, it remains unclear how to select and visually present AI explanations based on the characteristics of domain users. This paper aims to understand this question through a multidisciplinary design study for a specific problem: explaining graph neural network (GNN) predictions to domain experts in drug repurposing, i.e., reuse of existing drugs for new diseases. Building on the nested design model of visualization, we incorporate XAI design considerations from a literature review and from our collaborators\u2019 feedback into the design process. Specifically, we discuss XAI-related design considerations for usable visual explanations at each design layer: target user, usage context, domain explanation, and XAI goal at the domain layer; format, granularity, and operation of explanations at the abstraction layer; encodings and interactions at the visualization layer; and XAI and rendering algorithm at the algorithm layer. We present how the extended nested model motivates and informs the design of DrugExplorer, an XAI tool for drug repurposing. Based on our domain characterization, DrugExplorer provides path-based explanations and presents them both as individual paths and meta-paths for two key XAI operations, why and what else. DrugExplorer offers a novel visualization design called MetaMatrix with a set of interactions to help domain users organize and compare explanation paths at different levels of granularity to generate domain meaningful insights. We demonstrate the effectiveness of the selected visual presentation and DrugExplorer as a whole via a usage scenario, a user study, and expert interviews. From these evaluations, we derive insightful observations and reflections that can inform the design of XAI visualizations for other scientific applications.",
                        "uid": "v-full-1584",
                        "file_name": "",
                        "time_stamp": "2022-10-21T15:09:00Z",
                        "time_start": "2022-10-21T15:09:00Z",
                        "time_end": "2022-10-21T15:12:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "542",
                        "paper_award": "",
                        "image_caption": "Previous AI visualization tools usually select one specific explanation before the design study based on its popularity in the ML community without considering how the domain characteristics and user needs may influence the selection and visualization of explanations.\nThis paper presents a design study where we investigated how to select and visualize AI explanations for domain users.\nBuilding on the nested design model of visualization, we incorporate XAI design considerations from a literature review and our collaborators' feedback into the design process.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "https://youtu.be/r1QpPuw3zbw",
                        "ff_id": "r1QpPuw3zbw"
                    }
                ]
            },
            {
                "title": "ML for VIS",
                "session_id": "full17",
                "event_prefix": "v-full",
                "track": "ok4",
                "livestream_id": "ok4-thu",
                "session_image": "full17.png",
                "chair": [
                    "Daniel J\u00f6nsson"
                ],
                "organizers": [],
                "time_start": "2022-10-20T14:00:00Z",
                "time_end": "2022-10-20T15:15:00Z",
                "discord_category": "",
                "discord_channel": "oklahoma-st-4",
                "discord_channel_id": "1024600674924765264",
                "discord_link": "https://discord.com/channels/978320435554947082/1024600674924765264",
                "slido_link": "https://app.sli.do/event/i28UETZaiRWr1XwX6eKjcf?section=51658b34-0f5b-40c9-a335-1fd1468fad8d",
                "youtube_url": "https://youtu.be/0qY_NxLSGBk",
                "youtube_id": "0qY_NxLSGBk",
                "zoom_meeting": "",
                "zoom_password": "",
                "zoom_link": "",
                "ff_link": "https://youtu.be/1-bJtg59MBw",
                "ff_playlist": "",
                "time_slots": [
                    {
                        "slot_id": "full17-opening",
                        "session_id": "full17",
                        "type": "In Person Other",
                        "title": "Session Opening",
                        "contributors": [
                            "Daniel J\u00f6nsson"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "",
                        "file_name": "",
                        "time_stamp": "2022-10-20T14:00:00Z",
                        "time_start": "2022-10-20T14:00:00Z",
                        "time_end": "2022-10-20T14:00:00Z",
                        "paper_type": "",
                        "keywords": [],
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "v-tvcg-9495259-pres",
                        "session_id": "full17",
                        "type": "In Person Presentation",
                        "title": "AI4VIS: Survey on Artificial Intelligence Approaches for Data Visualization",
                        "contributors": [
                            "Aoyu Wu"
                        ],
                        "authors": [
                            "Aoyu Wu",
                            "Yun Wang",
                            "Xinhuan Shu",
                            "Dominik Moritz",
                            "Weiwei Cui",
                            "Haidong Zhang",
                            "Dongmei Zhang",
                            "Huamin Qu"
                        ],
                        "abstract": "Visualizations themselves have become a data format. Akin to other data formats such as text and images, visualizations are increasingly created, stored, shared, and (re-)used with artificial intelligence (AI) techniques. In this survey, we probe the underlying vision of formalizing visualizations as an emerging data format and review the recent advance in applying AI techniques to visualization data (AI4VIS). We define visualization data as the digital representations of visualizations in computers and focus on data visualization (e.g., charts and infographics). We build our survey upon a corpus spanning ten different fields in computer science with an eye toward identifying important common interests. Our resulting taxonomy is organized around WHAT is visualization data and its representation, WHY and HOW to apply AI to visualization data. We highlight a set of common tasks that researchers apply to the visualization data and present a detailed discussion of AI approaches developed to accomplish those tasks. Drawing upon our literature review, we discuss several important research questions surrounding the management and exploitation of visualization data, as well as the role of AI in support of those processes.",
                        "uid": "v-tvcg-9495259",
                        "file_name": "v-tvcg-9495259_Wu_Presentation.mp4",
                        "time_stamp": "2022-10-20T14:00:00Z",
                        "time_start": "2022-10-20T14:00:00Z",
                        "time_end": "2022-10-20T14:09:00Z",
                        "paper_type": "full",
                        "keywords": [
                            "Survey; Data Visualization; Artificial Intelligence; Data Format; Machine Learning"
                        ],
                        "has_image": "1",
                        "has_video": "607",
                        "paper_award": "",
                        "image_caption": "We present a survey on AI for visualizations from a corpus of 98 papers spanning 10 different fields in computer science. We argue that visualizations are becoming a new data format (visualization data) and categorize the surveyed papers from three aspects: (A) What is the visualization data and its representation; (B) Why apply AI to visualization data; and (C) How to apply AI to visualization data, where we outline seven tasks and separately discuss corresponding AI approaches.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "https://youtu.be/uL0MJYw4s9E",
                        "ff_id": "uL0MJYw4s9E"
                    },
                    {
                        "slot_id": "v-tvcg-9495259-qa",
                        "session_id": "full17",
                        "type": "In Person Q+A",
                        "title": "AI4VIS: Survey on Artificial Intelligence Approaches for Data Visualization (Q+A)",
                        "contributors": [
                            "Aoyu Wu"
                        ],
                        "authors": [],
                        "abstract": "Visualizations themselves have become a data format. Akin to other data formats such as text and images, visualizations are increasingly created, stored, shared, and (re-)used with artificial intelligence (AI) techniques. In this survey, we probe the underlying vision of formalizing visualizations as an emerging data format and review the recent advance in applying AI techniques to visualization data (AI4VIS). We define visualization data as the digital representations of visualizations in computers and focus on data visualization (e.g., charts and infographics). We build our survey upon a corpus spanning ten different fields in computer science with an eye toward identifying important common interests. Our resulting taxonomy is organized around WHAT is visualization data and its representation, WHY and HOW to apply AI to visualization data. We highlight a set of common tasks that researchers apply to the visualization data and present a detailed discussion of AI approaches developed to accomplish those tasks. Drawing upon our literature review, we discuss several important research questions surrounding the management and exploitation of visualization data, as well as the role of AI in support of those processes.",
                        "uid": "v-tvcg-9495259",
                        "file_name": "",
                        "time_stamp": "2022-10-20T14:09:00Z",
                        "time_start": "2022-10-20T14:09:00Z",
                        "time_end": "2022-10-20T14:12:00Z",
                        "paper_type": "full",
                        "keywords": [
                            "Survey; Data Visualization; Artificial Intelligence; Data Format; Machine Learning"
                        ],
                        "has_image": "1",
                        "has_video": "607",
                        "paper_award": "",
                        "image_caption": "We present a survey on AI for visualizations from a corpus of 98 papers spanning 10 different fields in computer science. We argue that visualizations are becoming a new data format (visualization data) and categorize the surveyed papers from three aspects: (A) What is the visualization data and its representation; (B) Why apply AI to visualization data; and (C) How to apply AI to visualization data, where we outline seven tasks and separately discuss corresponding AI approaches.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "https://youtu.be/uL0MJYw4s9E",
                        "ff_id": "uL0MJYw4s9E"
                    },
                    {
                        "slot_id": "v-tvcg-9760126-pres",
                        "session_id": "full17",
                        "type": "In Person Presentation",
                        "title": "DL4SciVis: A State-of-the-Art Survey on Deep Learning for Scientific Visualization",
                        "contributors": [
                            "Chaoli Wang"
                        ],
                        "authors": [
                            "Chaoli Wang",
                            "Jun Han"
                        ],
                        "abstract": "Since 2016, we have witnessed the tremendous growth of artificial intelligence+visualization (AI+VIS) research. However, existing survey papers on AI+VIS focus on visual analytics and information visualization, not scientific visualization (SciVis). In this paper, we survey related deep learning (DL) works in SciVis, specifically in the direction of DL4SciVis: designing DL solutions for solving SciVis problems. To stay focused, we primarily consider works that handle scalar and vector field data but exclude mesh data. We classify and discuss these works along six dimensions: domain setting, research task, learning type, network architecture, loss function, and evaluation metric. The paper concludes with a discussion of the remaining gaps to fill along the discussed dimensions and the grand challenges we need to tackle as a community. This state-of-the-art survey guides SciVis researchers in gaining an overview of this emerging topic and points out future directions to grow this research.",
                        "uid": "v-tvcg-9760126",
                        "file_name": "v-tvcg-9760126_Wang_Presentation.mp4",
                        "time_stamp": "2022-10-20T14:12:00Z",
                        "time_start": "2022-10-20T14:12:00Z",
                        "time_end": "2022-10-20T14:21:00Z",
                        "paper_type": "full",
                        "keywords": [
                            "Scientific visualization, deep learning, survey"
                        ],
                        "has_image": "1",
                        "has_video": "568",
                        "paper_award": "",
                        "image_caption": "Selected visualization results generated from the authors' DL4SciVis works. From left to right: reconstruction of vector field data from representative streamlines (CG&A 2019), learning surface representation via a graph convolutional network (SurfNet, EuroVis 2022), generating super-resolution for vector field data (SSR-VFD, PacificVis 2020), and clustering and selecting stream surfaces via an autoencoder (FlowNet, TVCG 2020).",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "https://youtu.be/vamEeME-GdI",
                        "ff_id": "vamEeME-GdI"
                    },
                    {
                        "slot_id": "v-tvcg-9760126-qa",
                        "session_id": "full17",
                        "type": "In Person Q+A",
                        "title": "DL4SciVis: A State-of-the-Art Survey on Deep Learning for Scientific Visualization (Q+A)",
                        "contributors": [
                            "Chaoli Wang"
                        ],
                        "authors": [],
                        "abstract": "Since 2016, we have witnessed the tremendous growth of artificial intelligence+visualization (AI+VIS) research. However, existing survey papers on AI+VIS focus on visual analytics and information visualization, not scientific visualization (SciVis). In this paper, we survey related deep learning (DL) works in SciVis, specifically in the direction of DL4SciVis: designing DL solutions for solving SciVis problems. To stay focused, we primarily consider works that handle scalar and vector field data but exclude mesh data. We classify and discuss these works along six dimensions: domain setting, research task, learning type, network architecture, loss function, and evaluation metric. The paper concludes with a discussion of the remaining gaps to fill along the discussed dimensions and the grand challenges we need to tackle as a community. This state-of-the-art survey guides SciVis researchers in gaining an overview of this emerging topic and points out future directions to grow this research.",
                        "uid": "v-tvcg-9760126",
                        "file_name": "",
                        "time_stamp": "2022-10-20T14:21:00Z",
                        "time_start": "2022-10-20T14:21:00Z",
                        "time_end": "2022-10-20T14:24:00Z",
                        "paper_type": "full",
                        "keywords": [
                            "Scientific visualization, deep learning, survey"
                        ],
                        "has_image": "1",
                        "has_video": "568",
                        "paper_award": "",
                        "image_caption": "Selected visualization results generated from the authors' DL4SciVis works. From left to right: reconstruction of vector field data from representative streamlines (CG&A 2019), learning surface representation via a graph convolutional network (SurfNet, EuroVis 2022), generating super-resolution for vector field data (SSR-VFD, PacificVis 2020), and clustering and selecting stream surfaces via an autoencoder (FlowNet, TVCG 2020).",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "https://youtu.be/vamEeME-GdI",
                        "ff_id": "vamEeME-GdI"
                    },
                    {
                        "slot_id": "v-tvcg-9523770-pres",
                        "session_id": "full17",
                        "type": "In Person Presentation",
                        "title": "A Survey on ML4VIS: Applying Machine Learning Advances to Data Visualization",
                        "contributors": [
                            "Wang, Qianwen"
                        ],
                        "authors": [
                            "Qianwen Wang",
                            "Zhutian Chen",
                            "Yong Wang",
                            "Huamin Qu"
                        ],
                        "abstract": "Inspired by the great success of machine learning (ML), researchers have applied ML techniques to visualizations to achieve a better design, development, and evaluation of visualizations. This branch of studies, known as ML4VIS, is gaining increasing research attention in recent years. To successfully adapt ML techniques for visualizations, a structured understanding of the integration of ML4VISis needed. In this paper, we systematically survey ML4VIS studies, aiming to answer two motivating questions: \"what visualization processes can be assisted by ML?\" and \"how ML techniques can be used to solve visualization problems?\" This survey reveals seven main processes where the employment of ML techniques can benefit visualizations: Data Processing4VIS, Data-VIS Mapping, Insight Communication, Style Imitation, VIS Interaction, VIS Reading, and User Profiling. The seven processes are related to existing visualization theoretical models in an ML4VIS pipeline, aiming to illuminate the role of ML-assisted visualization in general visualizations. Meanwhile, the seven processes are mapped into main learning tasks in ML to align the capabilities of ML with the needs in visualization. Current practices and future opportunities of ML4VIS are discussed in the context of the ML4VIS pipeline and the ML-VIS mapping. While more studies are still needed in the area of ML4VIS, we hope this paper can provide a stepping-stone for future exploration. A web-based interactive browser of this survey is available at https://ml4vis.github.io",
                        "uid": "v-tvcg-9523770",
                        "file_name": "v-tvcg-9523770_Wang_Presentation.mp4",
                        "time_stamp": "2022-10-20T14:24:00Z",
                        "time_start": "2022-10-20T14:24:00Z",
                        "time_end": "2022-10-20T14:33:00Z",
                        "paper_type": "full",
                        "keywords": [
                            "ML4VIS, Machine Learning, Data Visualization, Survey"
                        ],
                        "has_image": "1",
                        "has_video": "623",
                        "paper_award": "",
                        "image_caption": "To successfully adapt ML techniques for visualizations, a structured understanding of the integration of ML4VISis needed. In this paper, we systematically survey 88 ML4VIS studies, aiming to answer two motivating questions: \"what visualization processes can be assisted by ML?\" and \"how ML techniques can be used to solve visualization problems?\"",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "https://youtu.be/TCpVEzgcErQ",
                        "ff_id": "TCpVEzgcErQ"
                    },
                    {
                        "slot_id": "v-tvcg-9523770-qa",
                        "session_id": "full17",
                        "type": "In Person Q+A",
                        "title": "A Survey on ML4VIS: Applying Machine Learning Advances to Data Visualization (Q+A)",
                        "contributors": [
                            "Wang, Qianwen"
                        ],
                        "authors": [],
                        "abstract": "Inspired by the great success of machine learning (ML), researchers have applied ML techniques to visualizations to achieve a better design, development, and evaluation of visualizations. This branch of studies, known as ML4VIS, is gaining increasing research attention in recent years. To successfully adapt ML techniques for visualizations, a structured understanding of the integration of ML4VISis needed. In this paper, we systematically survey ML4VIS studies, aiming to answer two motivating questions: \"what visualization processes can be assisted by ML?\" and \"how ML techniques can be used to solve visualization problems?\" This survey reveals seven main processes where the employment of ML techniques can benefit visualizations: Data Processing4VIS, Data-VIS Mapping, Insight Communication, Style Imitation, VIS Interaction, VIS Reading, and User Profiling. The seven processes are related to existing visualization theoretical models in an ML4VIS pipeline, aiming to illuminate the role of ML-assisted visualization in general visualizations. Meanwhile, the seven processes are mapped into main learning tasks in ML to align the capabilities of ML with the needs in visualization. Current practices and future opportunities of ML4VIS are discussed in the context of the ML4VIS pipeline and the ML-VIS mapping. While more studies are still needed in the area of ML4VIS, we hope this paper can provide a stepping-stone for future exploration. A web-based interactive browser of this survey is available at https://ml4vis.github.io",
                        "uid": "v-tvcg-9523770",
                        "file_name": "",
                        "time_stamp": "2022-10-20T14:33:00Z",
                        "time_start": "2022-10-20T14:33:00Z",
                        "time_end": "2022-10-20T14:36:00Z",
                        "paper_type": "full",
                        "keywords": [
                            "ML4VIS, Machine Learning, Data Visualization, Survey"
                        ],
                        "has_image": "1",
                        "has_video": "623",
                        "paper_award": "",
                        "image_caption": "To successfully adapt ML techniques for visualizations, a structured understanding of the integration of ML4VISis needed. In this paper, we systematically survey 88 ML4VIS studies, aiming to answer two motivating questions: \"what visualization processes can be assisted by ML?\" and \"how ML techniques can be used to solve visualization problems?\"",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "https://youtu.be/TCpVEzgcErQ",
                        "ff_id": "TCpVEzgcErQ"
                    },
                    {
                        "slot_id": "v-tvcg-9706326-pres",
                        "session_id": "full17",
                        "type": "Virtual Presentation (pre-recorded)",
                        "title": "Reinforcement Learning for Load-balanced Parallel Particle Tracing",
                        "contributors": [
                            "Jiayi Xu"
                        ],
                        "authors": [
                            "Jiayi Xu",
                            "Hanqi Guo",
                            "Han-Wei Shen",
                            "Mukund Raj",
                            "Skylar W. Wurster",
                            "Tom Peterka"
                        ],
                        "abstract": "We explore an online reinforcement learning (RL) paradigm to dynamically optimize parallel particle tracing performance in distributed-memory systems. Our method combines three novel components: (1) a work donation algorithm, (2) a high-order workload estimation model, and (3) a communication cost model. First, we design an RL-based work donation algorithm. Our algorithm monitors workloads of processes and creates RL agents to donate data blocks and particles from high-workload processes to low-workload processes to minimize program execution time. The agents learn the donation strategy on the fly based on reward and cost functions designed to consider processes' workload changes and data transfer costs of donation actions. Second, we propose a workload estimation model, helping RL agents estimate the workload distribution of processes in future computations. Third, we design a communication cost model that considers both block and particle data exchange costs, helping RL agents make effective decisions with minimized communication costs. We demonstrate that our algorithm adapts to different flow behaviors in large-scale fluid dynamics, ocean, and weather simulation data. Our algorithm improves parallel particle tracing performance in terms of parallel efficiency, load balance, and costs of I/O and communication for evaluations with up to 16,384 processors.",
                        "uid": "v-tvcg-9706326",
                        "file_name": "v-tvcg-9706326_Xu_Presentation.mp4",
                        "time_stamp": "2022-10-20T14:36:00Z",
                        "time_start": "2022-10-20T14:36:00Z",
                        "time_end": "2022-10-20T14:45:04Z",
                        "paper_type": "full",
                        "keywords": [
                            "Distributed and parallel particle tracing, dynamic load balancing, reinforcement learning."
                        ],
                        "has_image": "1",
                        "has_video": "604",
                        "paper_award": "",
                        "image_caption": "Reinforcement Learning for Load-balanced Parallel Particle Tracing",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "https://youtu.be/kELpgKT3p1U",
                        "ff_id": "kELpgKT3p1U"
                    },
                    {
                        "slot_id": "v-tvcg-9706326-qa",
                        "session_id": "full17",
                        "type": "Virtual Q+A",
                        "title": "Reinforcement Learning for Load-balanced Parallel Particle Tracing (Q+A)",
                        "contributors": [
                            "Jiayi Xu"
                        ],
                        "authors": [],
                        "abstract": "We explore an online reinforcement learning (RL) paradigm to dynamically optimize parallel particle tracing performance in distributed-memory systems. Our method combines three novel components: (1) a work donation algorithm, (2) a high-order workload estimation model, and (3) a communication cost model. First, we design an RL-based work donation algorithm. Our algorithm monitors workloads of processes and creates RL agents to donate data blocks and particles from high-workload processes to low-workload processes to minimize program execution time. The agents learn the donation strategy on the fly based on reward and cost functions designed to consider processes' workload changes and data transfer costs of donation actions. Second, we propose a workload estimation model, helping RL agents estimate the workload distribution of processes in future computations. Third, we design a communication cost model that considers both block and particle data exchange costs, helping RL agents make effective decisions with minimized communication costs. We demonstrate that our algorithm adapts to different flow behaviors in large-scale fluid dynamics, ocean, and weather simulation data. Our algorithm improves parallel particle tracing performance in terms of parallel efficiency, load balance, and costs of I/O and communication for evaluations with up to 16,384 processors.",
                        "uid": "v-tvcg-9706326",
                        "file_name": "",
                        "time_stamp": "2022-10-20T14:45:04Z",
                        "time_start": "2022-10-20T14:45:04Z",
                        "time_end": "2022-10-20T14:48:00Z",
                        "paper_type": "full",
                        "keywords": [
                            "Distributed and parallel particle tracing, dynamic load balancing, reinforcement learning."
                        ],
                        "has_image": "1",
                        "has_video": "604",
                        "paper_award": "",
                        "image_caption": "Reinforcement Learning for Load-balanced Parallel Particle Tracing",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "https://youtu.be/kELpgKT3p1U",
                        "ff_id": "kELpgKT3p1U"
                    },
                    {
                        "slot_id": "v-full-1018-pres",
                        "session_id": "full17",
                        "type": "In Person Presentation",
                        "title": "IDLat: An Importance-Driven Latent Generation Method for Scientific Data",
                        "contributors": [
                            "JINGYI SHEN"
                        ],
                        "authors": [
                            "JINGYI SHEN",
                            "Haoyu Li",
                            "Jiayi Xu",
                            "Ayan Biswas",
                            "Han-Wei Shen"
                        ],
                        "abstract": "Deep learning based latent representations have been widely used for numerous scientific visualization applications such as isosurface similarity analysis, volume rendering, flow field synthesis, and data reduction, just to name a few. However, existing latent representations are mostly generated from raw data in an unsupervised manner, which makes it difficult to incorporate domain interest to control the size of the latent representations and the quality of the reconstructed data. In this paper, we present a novel importance-driven latent representation to facilitate domain-interest-guided scientific data visualization and analysis. We utilize spatial importance maps to represent various scientific interests and take them as the input to a feature transformation network to guide latent generation. We further reduced the latent size by a lossless entropy encoding algorithm trained together with the autoencoder, improving the storage and memory efficiency. We qualitatively and quantitatively evaluate the effectiveness and efficiency of latent representations generated by our method with data from multiple scientific visualization applications.",
                        "uid": "v-full-1018",
                        "file_name": "v-full-1018_Shen_Presentation.mp4",
                        "time_stamp": "2022-10-20T14:48:00Z",
                        "time_start": "2022-10-20T14:48:00Z",
                        "time_end": "2022-10-20T14:57:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "554",
                        "paper_award": "",
                        "image_caption": "We present an importance-driven latent generation method (IDLat) based on an autoencoder model which tightly relates latent representations to specific data of interest, such as salient regions or features of interest. With a trained model, scientists can flexibly define various importance criteria and obtain different latent representations. We further reduce the latent size through a lossless entropy coding model. In addition, we develop a visual exploration tool for latent space analysis and demonstrate the efficiency of identifying and analyzing feature regions with importance-driven latent representations.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "https://youtu.be/0rCVIDzQG6Y",
                        "ff_id": "0rCVIDzQG6Y"
                    },
                    {
                        "slot_id": "v-full-1018-qa",
                        "session_id": "full17",
                        "type": "In Person Q+A",
                        "title": "IDLat: An Importance-Driven Latent Generation Method for Scientific Data (Q+A)",
                        "contributors": [
                            "JINGYI SHEN"
                        ],
                        "authors": [],
                        "abstract": "Deep learning based latent representations have been widely used for numerous scientific visualization applications such as isosurface similarity analysis, volume rendering, flow field synthesis, and data reduction, just to name a few. However, existing latent representations are mostly generated from raw data in an unsupervised manner, which makes it difficult to incorporate domain interest to control the size of the latent representations and the quality of the reconstructed data. In this paper, we present a novel importance-driven latent representation to facilitate domain-interest-guided scientific data visualization and analysis. We utilize spatial importance maps to represent various scientific interests and take them as the input to a feature transformation network to guide latent generation. We further reduced the latent size by a lossless entropy encoding algorithm trained together with the autoencoder, improving the storage and memory efficiency. We qualitatively and quantitatively evaluate the effectiveness and efficiency of latent representations generated by our method with data from multiple scientific visualization applications.",
                        "uid": "v-full-1018",
                        "file_name": "",
                        "time_stamp": "2022-10-20T14:57:00Z",
                        "time_start": "2022-10-20T14:57:00Z",
                        "time_end": "2022-10-20T15:00:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "554",
                        "paper_award": "",
                        "image_caption": "We present an importance-driven latent generation method (IDLat) based on an autoencoder model which tightly relates latent representations to specific data of interest, such as salient regions or features of interest. With a trained model, scientists can flexibly define various importance criteria and obtain different latent representations. We further reduce the latent size through a lossless entropy coding model. In addition, we develop a visual exploration tool for latent space analysis and demonstrate the efficiency of identifying and analyzing feature regions with importance-driven latent representations.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "https://youtu.be/0rCVIDzQG6Y",
                        "ff_id": "0rCVIDzQG6Y"
                    },
                    {
                        "slot_id": "v-full-1033-pres",
                        "session_id": "full17",
                        "type": "Virtual Presentation (pre-recorded)",
                        "title": "DashBot: Insight-Driven Dashboard Generation Based on Deep Reinforcement Learning",
                        "contributors": [
                            "Dazhen Deng"
                        ],
                        "authors": [
                            "Dazhen Deng",
                            "Aoyu Wu",
                            "Huamin Qu",
                            "Yingcai Wu"
                        ],
                        "abstract": "Analytical dashboards are popular in business intelligence to facilitate insight discovery with multiple charts. However, creating an effective dashboard is highly demanding, which requires users to have adequate data analysis background and be familiar with professional tools, such as Power BI. To create a dashboard, users have to configure charts by selecting data columns and exploring different chart combinations to optimize the communication of insights, which is trial-and-error. Recent research has started to use deep learning methods for dashboard generation to lower the burden of visualization creation. However, such efforts are greatly hindered by the lack of large-scale and high-quality datasets of dashboards. In this work, we propose using deep reinforcement learning to generate analytical dashboards that can use well-established visualization knowledge and the estimation capacity of reinforcement learning. Specifically, we use visualization knowledge to construct a training environment and rewards for agents to explore and imitate human exploration behavior with a well-designed agent network. The usefulness of the deep reinforcement learning model is demonstrated through ablation studies and user studies. In conclusion, our work opens up new opportunities to develop effective ML-based visualization recommenders without beforehand training datasets.",
                        "uid": "v-full-1033",
                        "file_name": "v-full-1033_Deng_Presentation.mp4",
                        "time_stamp": "2022-10-20T15:00:00Z",
                        "time_start": "2022-10-20T15:00:00Z",
                        "time_end": "2022-10-20T15:09:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "579",
                        "paper_award": "",
                        "image_caption": "Screenshot of the DashBot Interface showing the generation of a dashboard named \u201cInsights about wind in seattle-weather\u201d. The interface consists of a table view (A), a topic list (B), a chart editor (C), a canvas view (D), and a recommendation view (E).",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "https://youtu.be/Nb22kJIVT7Q",
                        "ff_id": "Nb22kJIVT7Q"
                    },
                    {
                        "slot_id": "v-full-1033-qa",
                        "session_id": "full17",
                        "type": "Virtual Q+A",
                        "title": "DashBot: Insight-Driven Dashboard Generation Based on Deep Reinforcement Learning (Q+A)",
                        "contributors": [
                            "Dazhen Deng"
                        ],
                        "authors": [],
                        "abstract": "Analytical dashboards are popular in business intelligence to facilitate insight discovery with multiple charts. However, creating an effective dashboard is highly demanding, which requires users to have adequate data analysis background and be familiar with professional tools, such as Power BI. To create a dashboard, users have to configure charts by selecting data columns and exploring different chart combinations to optimize the communication of insights, which is trial-and-error. Recent research has started to use deep learning methods for dashboard generation to lower the burden of visualization creation. However, such efforts are greatly hindered by the lack of large-scale and high-quality datasets of dashboards. In this work, we propose using deep reinforcement learning to generate analytical dashboards that can use well-established visualization knowledge and the estimation capacity of reinforcement learning. Specifically, we use visualization knowledge to construct a training environment and rewards for agents to explore and imitate human exploration behavior with a well-designed agent network. The usefulness of the deep reinforcement learning model is demonstrated through ablation studies and user studies. In conclusion, our work opens up new opportunities to develop effective ML-based visualization recommenders without beforehand training datasets.",
                        "uid": "v-full-1033",
                        "file_name": "",
                        "time_stamp": "2022-10-20T15:09:00Z",
                        "time_start": "2022-10-20T15:09:00Z",
                        "time_end": "2022-10-20T15:12:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "579",
                        "paper_award": "",
                        "image_caption": "Screenshot of the DashBot Interface showing the generation of a dashboard named \u201cInsights about wind in seattle-weather\u201d. The interface consists of a table view (A), a topic list (B), a chart editor (C), a canvas view (D), and a recommendation view (E).",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "https://youtu.be/Nb22kJIVT7Q",
                        "ff_id": "Nb22kJIVT7Q"
                    }
                ]
            },
            {
                "title": "VA and ML",
                "session_id": "full18",
                "event_prefix": "v-full",
                "track": "ok4",
                "livestream_id": "ok4-wed",
                "session_image": "full18.png",
                "chair": [
                    "Cagatay Turkay"
                ],
                "organizers": [],
                "time_start": "2022-10-19T14:00:00Z",
                "time_end": "2022-10-19T15:15:00Z",
                "discord_category": "",
                "discord_channel": "oklahoma-st-4",
                "discord_channel_id": "1024600674924765264",
                "discord_link": "https://discord.com/channels/978320435554947082/1024600674924765264",
                "slido_link": "https://app.sli.do/event/i28UETZaiRWr1XwX6eKjcf?section=51658b34-0f5b-40c9-a335-1fd1468fad8d",
                "youtube_url": "https://youtu.be/BKQz_UXnWMA",
                "youtube_id": "BKQz_UXnWMA",
                "zoom_meeting": "",
                "zoom_password": "",
                "zoom_link": "",
                "ff_link": "https://youtu.be/gJT8MxuB0Us",
                "ff_playlist": "",
                "time_slots": [
                    {
                        "slot_id": "full18-opening",
                        "session_id": "full18",
                        "type": "In Person Other",
                        "title": "Session Opening",
                        "contributors": [
                            "Cagatay Turkay"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "",
                        "file_name": "",
                        "time_stamp": "2022-10-19T14:00:00Z",
                        "time_start": "2022-10-19T14:00:00Z",
                        "time_end": "2022-10-19T14:00:00Z",
                        "paper_type": "",
                        "keywords": [],
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "v-tvcg-9497654-pres",
                        "session_id": "full18",
                        "type": "Virtual Presentation (pre-recorded)",
                        "title": "Survey on Visual Analysis of Event Sequence Data",
                        "contributors": [
                            "Yi Guo"
                        ],
                        "authors": [
                            "Yi Guo",
                            "Shunan Guo",
                            "Zhuochen Jin",
                            "Smiti Kaul",
                            "David Gotz",
                            "Nan Cao"
                        ],
                        "abstract": "Event sequence data record series of discrete events in the time order of occurrence. They are commonly observed in a variety of applications ranging from electronic health records to network logs, with the characteristics of large-scale, high-dimensional and heterogeneous. This high complexity of event sequence data makes it difficult for analysts to manually explore and find patterns, resulting in ever-increasing needs for computational and perceptual aids from visual analytics techniques to extract and communicate insights from event sequence datasets. In this paper, we review the state-of-the-art visual analytics approaches, characterize them with our proposed design space, and categorize them based on analytical tasks and applications. From our review of relevant literature, we have also identified several remaining research challenges and future research opportunities.",
                        "uid": "v-tvcg-9497654",
                        "file_name": "v-tvcg-9497654_Guo_Presentation.mp4",
                        "time_stamp": "2022-10-19T14:00:00Z",
                        "time_start": "2022-10-19T14:00:00Z",
                        "time_end": "2022-10-19T14:09:00Z",
                        "paper_type": "full",
                        "keywords": [
                            "Visual Analysis, Event Sequence Data, Visualization"
                        ],
                        "has_image": "1",
                        "has_video": "528",
                        "paper_award": "",
                        "image_caption": "The design spaces of visual analytics techniques for event sequence data include four dimensions: data scale, automated sequence analysis, visual representation, interaction technique.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "https://youtu.be/XS3BWEpySY8",
                        "ff_id": "XS3BWEpySY8"
                    },
                    {
                        "slot_id": "v-tvcg-9497654-qa",
                        "session_id": "full18",
                        "type": "Virtual Q+A",
                        "title": "Survey on Visual Analysis of Event Sequence Data (Q+A)",
                        "contributors": [
                            "Yi Guo"
                        ],
                        "authors": [],
                        "abstract": "Event sequence data record series of discrete events in the time order of occurrence. They are commonly observed in a variety of applications ranging from electronic health records to network logs, with the characteristics of large-scale, high-dimensional and heterogeneous. This high complexity of event sequence data makes it difficult for analysts to manually explore and find patterns, resulting in ever-increasing needs for computational and perceptual aids from visual analytics techniques to extract and communicate insights from event sequence datasets. In this paper, we review the state-of-the-art visual analytics approaches, characterize them with our proposed design space, and categorize them based on analytical tasks and applications. From our review of relevant literature, we have also identified several remaining research challenges and future research opportunities.",
                        "uid": "v-tvcg-9497654",
                        "file_name": "",
                        "time_stamp": "2022-10-19T14:09:00Z",
                        "time_start": "2022-10-19T14:09:00Z",
                        "time_end": "2022-10-19T14:12:00Z",
                        "paper_type": "full",
                        "keywords": [
                            "Visual Analysis, Event Sequence Data, Visualization"
                        ],
                        "has_image": "1",
                        "has_video": "528",
                        "paper_award": "",
                        "image_caption": "The design spaces of visual analytics techniques for event sequence data include four dimensions: data scale, automated sequence analysis, visual representation, interaction technique.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "https://youtu.be/XS3BWEpySY8",
                        "ff_id": "XS3BWEpySY8"
                    },
                    {
                        "slot_id": "v-tvcg-9664269-pres",
                        "session_id": "full18",
                        "type": "Virtual Presentation (pre-recorded)",
                        "title": "Towards Better Caption Supervision for Object Detection",
                        "contributors": [
                            "Shixia Liu"
                        ],
                        "authors": [
                            "Changjian Chen",
                            "Jing Wu",
                            "Xiaohan Wang",
                            "Shouxing Xiang",
                            "Song-Hai Zhang",
                            "Qifeng Tang",
                            "Shixia Liu"
                        ],
                        "abstract": "As training high-performance object detectors requires expensive bounding box annotations, recent methods resort to free available image captions. However, detectors trained on caption supervision perform poorly because captions are usually noisy and cannot provide precise location information. To tackle this issue, we present a visual analysis method, which tightly integrates caption supervision with object detection to mutually enhance each other. In particular, object labels are first extracted from captions, which are utilized to train the detectors. Then, the label information from images is fed into caption supervision for further improvement. To effectively loop users into the object detection process, a node-link-based set visualization supported by a multi-type relational co-clustering algorithm is developed to explain the relationships between the extracted labels and the images with detected objects. The co-clustering algorithm clusters labels and images simultaneously by utilizing both their representations and their relationships. Quantitative evaluations and a case study are conducted to demonstrate the efficiency and effectiveness of the developed method in improving the performance of object detectors.",
                        "uid": "v-tvcg-9664269",
                        "file_name": "v-tvcg-9664269_Chen_Presentation.mp4",
                        "time_stamp": "2022-10-19T14:12:00Z",
                        "time_start": "2022-10-19T14:12:00Z",
                        "time_end": "2022-10-19T14:21:00Z",
                        "paper_type": "full",
                        "keywords": [
                            "Machine learning, interactive visualization, object detection, caption supervision, co-clustering."
                        ],
                        "has_image": "1",
                        "has_video": "488",
                        "paper_award": "",
                        "image_caption": "MutualDetector: (a) a node-link-based set visualization consists of a tree of labels (1), the relationships between the labels and image clusters (2), and a matrix (3) to show the representative images with the detected objects for each cluster; (b) an information panel to show important words, captions, and selected images.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "https://youtu.be/QtSYyzChamk",
                        "ff_id": "QtSYyzChamk"
                    },
                    {
                        "slot_id": "v-tvcg-9664269-qa",
                        "session_id": "full18",
                        "type": "Virtual Q+A",
                        "title": "Towards Better Caption Supervision for Object Detection (Q+A)",
                        "contributors": [
                            "Shixia Liu"
                        ],
                        "authors": [],
                        "abstract": "As training high-performance object detectors requires expensive bounding box annotations, recent methods resort to free available image captions. However, detectors trained on caption supervision perform poorly because captions are usually noisy and cannot provide precise location information. To tackle this issue, we present a visual analysis method, which tightly integrates caption supervision with object detection to mutually enhance each other. In particular, object labels are first extracted from captions, which are utilized to train the detectors. Then, the label information from images is fed into caption supervision for further improvement. To effectively loop users into the object detection process, a node-link-based set visualization supported by a multi-type relational co-clustering algorithm is developed to explain the relationships between the extracted labels and the images with detected objects. The co-clustering algorithm clusters labels and images simultaneously by utilizing both their representations and their relationships. Quantitative evaluations and a case study are conducted to demonstrate the efficiency and effectiveness of the developed method in improving the performance of object detectors.",
                        "uid": "v-tvcg-9664269",
                        "file_name": "",
                        "time_stamp": "2022-10-19T14:21:00Z",
                        "time_start": "2022-10-19T14:21:00Z",
                        "time_end": "2022-10-19T14:24:00Z",
                        "paper_type": "full",
                        "keywords": [
                            "Machine learning, interactive visualization, object detection, caption supervision, co-clustering."
                        ],
                        "has_image": "1",
                        "has_video": "488",
                        "paper_award": "",
                        "image_caption": "MutualDetector: (a) a node-link-based set visualization consists of a tree of labels (1), the relationships between the labels and image clusters (2), and a matrix (3) to show the representative images with the detected objects for each cluster; (b) an information panel to show important words, captions, and selected images.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "https://youtu.be/QtSYyzChamk",
                        "ff_id": "QtSYyzChamk"
                    },
                    {
                        "slot_id": "v-tvcg-9672706-pres",
                        "session_id": "full18",
                        "type": "In Person Presentation",
                        "title": "FeatureEnVi: Visual Analytics for Feature Engineering Using Stepwise Selection and Semi-Automatic Extraction Approaches",
                        "contributors": [
                            "Angelos Chatzimparmpas"
                        ],
                        "authors": [
                            "Angelos Chatzimparmpas",
                            "Rafael M. Martins",
                            "Kostiantyn Kucher",
                            "Andreas Kerren"
                        ],
                        "abstract": "The machine learning (ML) life cycle involves a series of iterative steps, from the effective gathering and preparation of the data\u2014including complex feature engineering processes\u2014to the presentation and improvement of results, with various algorithms to choose from in every step. Feature engineering in particular can be very beneficial for ML, leading to numerous improvements such as boosting the predictive results, decreasing computational times, reducing excessive noise, and increasing the transparency behind the decisions taken during the training. Despite that, while several visual analytics tools exist to monitor and control the different stages of the ML life cycle (especially those related to data and algorithms), feature engineering support remains inadequate. In this paper, we present FeatureEnVi, a visual analytics system specifically designed to assist with the feature engineering process. Our proposed system helps users to choose the most important feature, to transform the original features into powerful alternatives, and to experiment with different feature generation combinations. Additionally, data space slicing allows users to explore the impact of features on both local and global scales. FeatureEnVi utilizes multiple automatic feature selection techniques; furthermore, it visually guides users with statistical evidence about the influence of each feature (or subsets of features). The final outcome is the extraction of heavily engineered features, evaluated by multiple validation metrics. The usefulness and applicability of FeatureEnVi are demonstrated with two use cases and a case study. We also report feedback from interviews with two ML experts and a visualization researcher who assessed the effectiveness of our system.",
                        "uid": "v-tvcg-9672706",
                        "file_name": "v-tvcg-9672706_Chatzimparmpas_Presentation.mp4",
                        "time_stamp": "2022-10-19T14:24:00Z",
                        "time_start": "2022-10-19T14:24:00Z",
                        "time_end": "2022-10-19T14:33:00Z",
                        "paper_type": "full",
                        "keywords": [
                            "Feature selection, feature extraction, feature engineering, machine learning, visual analytics, visualization"
                        ],
                        "has_image": "1",
                        "has_video": "651",
                        "paper_award": "",
                        "image_caption": "Selecting important features, transforming them, and generating new features with FeatureEnVi: (a) the plot for manually slicing the data space and continuously checking the migration of instances; (b) the view for the selection of features according to multiple feature importances; (c) provides an overview of the features with statistical measures for the different groups of instances; (d) the visualization for the detailed exploration of features, their transformation, and comparison between features for feature generation purposes; and (e) the punchcard for tracking the steps of the process and the grouped bar chart for comparing the current versus the best predictive performance.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "https://youtu.be/2ZrRys1q8ss",
                        "ff_id": "2ZrRys1q8ss"
                    },
                    {
                        "slot_id": "v-tvcg-9672706-qa",
                        "session_id": "full18",
                        "type": "In Person Q+A",
                        "title": "FeatureEnVi: Visual Analytics for Feature Engineering Using Stepwise Selection and Semi-Automatic Extraction Approaches (Q+A)",
                        "contributors": [
                            "Angelos Chatzimparmpas"
                        ],
                        "authors": [],
                        "abstract": "The machine learning (ML) life cycle involves a series of iterative steps, from the effective gathering and preparation of the data\u2014including complex feature engineering processes\u2014to the presentation and improvement of results, with various algorithms to choose from in every step. Feature engineering in particular can be very beneficial for ML, leading to numerous improvements such as boosting the predictive results, decreasing computational times, reducing excessive noise, and increasing the transparency behind the decisions taken during the training. Despite that, while several visual analytics tools exist to monitor and control the different stages of the ML life cycle (especially those related to data and algorithms), feature engineering support remains inadequate. In this paper, we present FeatureEnVi, a visual analytics system specifically designed to assist with the feature engineering process. Our proposed system helps users to choose the most important feature, to transform the original features into powerful alternatives, and to experiment with different feature generation combinations. Additionally, data space slicing allows users to explore the impact of features on both local and global scales. FeatureEnVi utilizes multiple automatic feature selection techniques; furthermore, it visually guides users with statistical evidence about the influence of each feature (or subsets of features). The final outcome is the extraction of heavily engineered features, evaluated by multiple validation metrics. The usefulness and applicability of FeatureEnVi are demonstrated with two use cases and a case study. We also report feedback from interviews with two ML experts and a visualization researcher who assessed the effectiveness of our system.",
                        "uid": "v-tvcg-9672706",
                        "file_name": "",
                        "time_stamp": "2022-10-19T14:33:00Z",
                        "time_start": "2022-10-19T14:33:00Z",
                        "time_end": "2022-10-19T14:36:00Z",
                        "paper_type": "full",
                        "keywords": [
                            "Feature selection, feature extraction, feature engineering, machine learning, visual analytics, visualization"
                        ],
                        "has_image": "1",
                        "has_video": "651",
                        "paper_award": "",
                        "image_caption": "Selecting important features, transforming them, and generating new features with FeatureEnVi: (a) the plot for manually slicing the data space and continuously checking the migration of instances; (b) the view for the selection of features according to multiple feature importances; (c) provides an overview of the features with statistical measures for the different groups of instances; (d) the visualization for the detailed exploration of features, their transformation, and comparison between features for feature generation purposes; and (e) the punchcard for tracking the steps of the process and the grouped bar chart for comparing the current versus the best predictive performance.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "https://youtu.be/2ZrRys1q8ss",
                        "ff_id": "2ZrRys1q8ss"
                    },
                    {
                        "slot_id": "v-full-1569-pres",
                        "session_id": "full18",
                        "type": "In Person Presentation",
                        "title": "A Design Space for Surfacing Content Recommendations in Visual Analytic Platforms",
                        "contributors": [
                            "David Gotz",
                            "Zhilan Zhou"
                        ],
                        "authors": [
                            "Zhilan Zhou",
                            "Wenyuan Wang",
                            "Mengtian Guo",
                            "Yue Wang",
                            "David Gotz"
                        ],
                        "abstract": "Recommendation algorithms have been leveraged in various ways within visualization systems to assist users as they perform of a range of information tasks. One common focus for these techniques has been the recommendation of content, rather than visual form, as a means to assist users in the identification of information that is relevant to their task context. A wide variety of techniques have been proposed to address this general problem, with a range of design choices in how these solutions surface relevant information to users. This paper reviews the state-of-the-art in how visualization systems surface recommended content to users during users' visual analysis; introduces a four-dimensional design space for visual content recommendation based on a characterization of prior work; and discusses key observations regarding common patterns and future research opportunities.",
                        "uid": "v-full-1569",
                        "file_name": "v-full-1569_Zhou_Presentation.mp4",
                        "time_stamp": "2022-10-19T14:36:00Z",
                        "time_start": "2022-10-19T14:36:00Z",
                        "time_end": "2022-10-19T14:45:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "482",
                        "paper_award": "",
                        "image_caption": "A hypothetical visual analytics platform with an interface includes two tabs: one with visualizations and another with a text-based list. A pop-up window is on top of the interface with content recommendations.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "https://youtu.be/xzaOTSubu4Y",
                        "ff_id": "xzaOTSubu4Y"
                    },
                    {
                        "slot_id": "v-full-1569-qa",
                        "session_id": "full18",
                        "type": "In Person Q+A",
                        "title": "A Design Space for Surfacing Content Recommendations in Visual Analytic Platforms (Q+A)",
                        "contributors": [
                            "David Gotz",
                            "Zhilan Zhou"
                        ],
                        "authors": [],
                        "abstract": "Recommendation algorithms have been leveraged in various ways within visualization systems to assist users as they perform of a range of information tasks. One common focus for these techniques has been the recommendation of content, rather than visual form, as a means to assist users in the identification of information that is relevant to their task context. A wide variety of techniques have been proposed to address this general problem, with a range of design choices in how these solutions surface relevant information to users. This paper reviews the state-of-the-art in how visualization systems surface recommended content to users during users' visual analysis; introduces a four-dimensional design space for visual content recommendation based on a characterization of prior work; and discusses key observations regarding common patterns and future research opportunities.",
                        "uid": "v-full-1569",
                        "file_name": "",
                        "time_stamp": "2022-10-19T14:45:00Z",
                        "time_start": "2022-10-19T14:45:00Z",
                        "time_end": "2022-10-19T14:48:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "482",
                        "paper_award": "",
                        "image_caption": "A hypothetical visual analytics platform with an interface includes two tabs: one with visualizations and another with a text-based list. A pop-up window is on top of the interface with content recommendations.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "https://youtu.be/xzaOTSubu4Y",
                        "ff_id": "xzaOTSubu4Y"
                    },
                    {
                        "slot_id": "v-full-1452-pres",
                        "session_id": "full18",
                        "type": "In Person Presentation",
                        "title": "Diverse Interaction Recommendation for Public Users Exploring Multi-view Visualization using Deep Learning",
                        "contributors": [
                            "Yixuan Li"
                        ],
                        "authors": [
                            "Yixuan Li",
                            "Yusheng Qi",
                            "Yang Shi",
                            "Qing Chen",
                            "Nan Cao",
                            "Siming Chen"
                        ],
                        "abstract": "Interaction is an important channel to offer users insights in interactive visualization systems. However, which interaction to operate and which part of data to explore are hard questions for public users facing a multi-view visualization for the first time. Making these decisions largely relies on professional experience and analytic abilities, which is a huge challenge for non-professionals. To solve the problem, we propose a method aiming to provide diverse, insightful, and real-time interaction recommendations for novice users. Building on the Long-Short Term Memory Model (LSTM) structure, our model captures users' interactions and visual states and encodes them in numerical vectors to make further recommendations. Through an illustrative example of a visualization system about Chinese poets in the museum scenario, the model is proven to be workable in systems with multi-views and multiple interaction types. A further user study demonstrates the method's capability to help public users conduct more insightful and diverse interactive explorations and gain more accurate data insights.",
                        "uid": "v-full-1452",
                        "file_name": "v-full-1452_Li_Presentation.mp4",
                        "time_stamp": "2022-10-19T14:48:00Z",
                        "time_start": "2022-10-19T14:48:00Z",
                        "time_end": "2022-10-19T14:57:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "700",
                        "paper_award": "",
                        "image_caption": "In this work, we introduced a method for making real-time, insightful, and diverse interaction recommendations in visualization systems with multiple views and interaction types. The model performs as an end-to-end pipeline with users\u2019 Previous Interaction logs and Visual States as the input and Recommendations of the next step interaction as the output.\nIn the user's workflow, each time the user make one interaction, the system collects the interaction and visual state and then transfers them into numerical vectors. In the predicting workflow, the pre-trained models receive the data and output predicted interaction vectors. After being converted to JSON data, the recommended interactions will be sent back.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "https://youtu.be/Ut5l987xayw",
                        "ff_id": "Ut5l987xayw"
                    },
                    {
                        "slot_id": "v-full-1452-qa",
                        "session_id": "full18",
                        "type": "In Person Q+A",
                        "title": "Diverse Interaction Recommendation for Public Users Exploring Multi-view Visualization using Deep Learning (Q+A)",
                        "contributors": [
                            "Yixuan Li"
                        ],
                        "authors": [],
                        "abstract": "Interaction is an important channel to offer users insights in interactive visualization systems. However, which interaction to operate and which part of data to explore are hard questions for public users facing a multi-view visualization for the first time. Making these decisions largely relies on professional experience and analytic abilities, which is a huge challenge for non-professionals. To solve the problem, we propose a method aiming to provide diverse, insightful, and real-time interaction recommendations for novice users. Building on the Long-Short Term Memory Model (LSTM) structure, our model captures users' interactions and visual states and encodes them in numerical vectors to make further recommendations. Through an illustrative example of a visualization system about Chinese poets in the museum scenario, the model is proven to be workable in systems with multi-views and multiple interaction types. A further user study demonstrates the method's capability to help public users conduct more insightful and diverse interactive explorations and gain more accurate data insights.",
                        "uid": "v-full-1452",
                        "file_name": "",
                        "time_stamp": "2022-10-19T14:57:00Z",
                        "time_start": "2022-10-19T14:57:00Z",
                        "time_end": "2022-10-19T15:00:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "700",
                        "paper_award": "",
                        "image_caption": "In this work, we introduced a method for making real-time, insightful, and diverse interaction recommendations in visualization systems with multiple views and interaction types. The model performs as an end-to-end pipeline with users\u2019 Previous Interaction logs and Visual States as the input and Recommendations of the next step interaction as the output.\nIn the user's workflow, each time the user make one interaction, the system collects the interaction and visual state and then transfers them into numerical vectors. In the predicting workflow, the pre-trained models receive the data and output predicted interaction vectors. After being converted to JSON data, the recommended interactions will be sent back.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "https://youtu.be/Ut5l987xayw",
                        "ff_id": "Ut5l987xayw"
                    },
                    {
                        "slot_id": "v-full-1674-pres",
                        "session_id": "full18",
                        "type": "In Person Presentation",
                        "title": "Visinity: Visual Spatial Neighborhood Analysis for Multiplexed Tissue Imaging Data",
                        "contributors": [
                            "Simon Alexander Warchol"
                        ],
                        "authors": [
                            "Simon Alexander Warchol",
                            "Robert Kr\u00fcger",
                            "Ajit Johnson Nirmal",
                            "Giorgio Gaglia",
                            "Jared Jessup Jessup",
                            "Cecily C. Ritch",
                            "John Hoffer",
                            "Jeremy Muhlich",
                            "Megan L Burger",
                            "Tyler Jacks",
                            "Sandro Santagata Santagata",
                            "Peter Sorger",
                            "Hanspeter Pfister"
                        ],
                        "abstract": "New highly-multiplexed imaging technologies have enabled the study of tissues in unprecedented detail. These methods are increasingly being applied to understand how cancer cells and immune response change during tumor development, progression, and metastasis, as well as following treatment. Yet, existing analysis approaches focus on investigating small tissue samples on a per-cell basis, not taking into account the spatial proximity of cells, which indicates cell-cell interaction and specific biological processes in the larger cancer microenvironment. We present Visinity, a scalable visual analytics system to analyze cell interaction patterns across cohorts of whole-slide multiplexed tissue images. Our approach is based on a fast regional neighborhood computation, leveraging unsupervised learning to quantify, compare, and group cells by their surrounding cellular neighborhood. These neighborhoods can be visually analyzed in an exploratory and confirmatory workflow. Users can explore spatial patterns present across tissues through a scalable image viewer and coordinated views highlighting the neighborhood composition and spatial arrangements of cells. To verify or refine existing hypotheses, users can query for specific patterns to determine their presence and statistical significance. Findings can be interactively annotated, ranked, and compared in the form of small multiples. In two case studies with biomedical experts, we demonstrate that Visinity can identify common biological processes within a human tonsil and uncover novel white-blood cell networks and immune-tumor interactions.",
                        "uid": "v-full-1674",
                        "file_name": "v-full-1674_Warchol_Presentation.mp4",
                        "time_stamp": "2022-10-19T15:00:00Z",
                        "time_start": "2022-10-19T15:00:00Z",
                        "time_end": "2022-10-19T15:09:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "989",
                        "paper_award": "",
                        "image_caption": "Visinity is a visual analytics system for spatial neighborhood analysis across cohorts of gigapixel whole-slide tissue images, developed in direct collaboration with pathologists and cell biologists. Our approach is based on a fast and scalable spatial neighborhood computation and leverages unsupervised learning to quantify, compare, and group cells by their surrounding cellular neighborhood. To test, verify, and refine hypotheses, experts can visually query for spatial neighborhood patterns, determine their presence and statistical significance across specimens, and annotate and visually compare findings. We demonstrate that Visinity can be used to identify biologically relevant patterns in two case studies with biomedical experts.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "https://youtu.be/g3xy000UsFo",
                        "ff_id": "g3xy000UsFo"
                    },
                    {
                        "slot_id": "v-full-1674-qa",
                        "session_id": "full18",
                        "type": "In Person Q+A",
                        "title": "Visinity: Visual Spatial Neighborhood Analysis for Multiplexed Tissue Imaging Data (Q+A)",
                        "contributors": [
                            "Simon Alexander Warchol"
                        ],
                        "authors": [],
                        "abstract": "New highly-multiplexed imaging technologies have enabled the study of tissues in unprecedented detail. These methods are increasingly being applied to understand how cancer cells and immune response change during tumor development, progression, and metastasis, as well as following treatment. Yet, existing analysis approaches focus on investigating small tissue samples on a per-cell basis, not taking into account the spatial proximity of cells, which indicates cell-cell interaction and specific biological processes in the larger cancer microenvironment. We present Visinity, a scalable visual analytics system to analyze cell interaction patterns across cohorts of whole-slide multiplexed tissue images. Our approach is based on a fast regional neighborhood computation, leveraging unsupervised learning to quantify, compare, and group cells by their surrounding cellular neighborhood. These neighborhoods can be visually analyzed in an exploratory and confirmatory workflow. Users can explore spatial patterns present across tissues through a scalable image viewer and coordinated views highlighting the neighborhood composition and spatial arrangements of cells. To verify or refine existing hypotheses, users can query for specific patterns to determine their presence and statistical significance. Findings can be interactively annotated, ranked, and compared in the form of small multiples. In two case studies with biomedical experts, we demonstrate that Visinity can identify common biological processes within a human tonsil and uncover novel white-blood cell networks and immune-tumor interactions.",
                        "uid": "v-full-1674",
                        "file_name": "",
                        "time_stamp": "2022-10-19T15:09:00Z",
                        "time_start": "2022-10-19T15:09:00Z",
                        "time_end": "2022-10-19T15:12:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "989",
                        "paper_award": "",
                        "image_caption": "Visinity is a visual analytics system for spatial neighborhood analysis across cohorts of gigapixel whole-slide tissue images, developed in direct collaboration with pathologists and cell biologists. Our approach is based on a fast and scalable spatial neighborhood computation and leverages unsupervised learning to quantify, compare, and group cells by their surrounding cellular neighborhood. To test, verify, and refine hypotheses, experts can visually query for spatial neighborhood patterns, determine their presence and statistical significance across specimens, and annotate and visually compare findings. We demonstrate that Visinity can be used to identify biologically relevant patterns in two case studies with biomedical experts.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "https://youtu.be/g3xy000UsFo",
                        "ff_id": "g3xy000UsFo"
                    }
                ]
            },
            {
                "title": "VA for ML",
                "session_id": "full19",
                "event_prefix": "v-full",
                "track": "ok4",
                "livestream_id": "ok4-wed",
                "session_image": "full19.png",
                "chair": [
                    "Remco Chang"
                ],
                "organizers": [],
                "time_start": "2022-10-19T15:45:00Z",
                "time_end": "2022-10-19T17:00:00Z",
                "discord_category": "",
                "discord_channel": "oklahoma-st-4",
                "discord_channel_id": "1024600674924765264",
                "discord_link": "https://discord.com/channels/978320435554947082/1024600674924765264",
                "slido_link": "https://app.sli.do/event/i28UETZaiRWr1XwX6eKjcf?section=51658b34-0f5b-40c9-a335-1fd1468fad8d",
                "youtube_url": "https://youtu.be/BKQz_UXnWMA",
                "youtube_id": "BKQz_UXnWMA",
                "zoom_meeting": "",
                "zoom_password": "",
                "zoom_link": "",
                "ff_link": "https://youtu.be/2Nl23ApDdSQ",
                "ff_playlist": "",
                "time_slots": [
                    {
                        "slot_id": "full19-opening",
                        "session_id": "full19",
                        "type": "In Person Other",
                        "title": "Session Opening",
                        "contributors": [
                            "Remco Chang"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "",
                        "file_name": "",
                        "time_stamp": "2022-10-19T15:45:00Z",
                        "time_start": "2022-10-19T15:45:00Z",
                        "time_end": "2022-10-19T15:45:00Z",
                        "paper_type": "",
                        "keywords": [],
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "v-tvcg-9705076-pres",
                        "session_id": "full19",
                        "type": "In Person Presentation",
                        "title": "GNNLens: A Visual Analytics Approach for Prediction Error Diagnosis of Graph Neural Networks",
                        "contributors": [
                            "Zhihua JIN"
                        ],
                        "authors": [
                            "Zhihua Jin",
                            "Yong Wang",
                            "Qianwen Wang",
                            "Yao Ming",
                            "Tengfei Ma",
                            "Huamin Qu"
                        ],
                        "abstract": "Graph Neural Networks (GNNs) aim to extend deep learning techniques to graph data and have achieved significant progress in graph analysis tasks (e.g., node classification) in recent years. However, similar to other deep neural networks like Convolutional Neural Networks (CNNs) and Recurrent Neural Networks (RNNs), GNNs behave like a black box with their details hidden from model developers and users. It is therefore difficult to diagnose possible errors of GNNs. Despite many visual analytics studies being done on CNNs and RNNs, little research has addressed the challenges for GNNs. This paper fills the research gap with an interactive visual analysis tool, GNNLens, to assist model developers and users in understanding and analyzing GNNs. Specifically, Parallel Sets View and Projection View enable users to quickly identify and validate error patterns in the set of wrong predictions; Graph View and Feature Matrix View offer a detailed analysis of individual nodes to assist users in forming hypotheses about the error patterns. Since GNNs jointly model the graph structure and the node features, we reveal the relative influences of the two types of information by comparing the predictions of three models: GNN, Multi-Layer Perceptron (MLP), and GNN Without Using Features (GNNWUF). Two case studies and interviews with domain experts demonstrate the effectiveness of GNNLens in facilitating the understanding of GNN models and their errors.",
                        "uid": "v-tvcg-9705076",
                        "file_name": "v-tvcg-9705076_Jin_Presentation.mp4",
                        "time_stamp": "2022-10-19T15:45:00Z",
                        "time_start": "2022-10-19T15:45:00Z",
                        "time_end": "2022-10-19T15:54:00Z",
                        "paper_type": "full",
                        "keywords": [
                            "Graph Neural Networks, Error Diagnosis, Visualization"
                        ],
                        "has_image": "1",
                        "has_video": "643",
                        "paper_award": "",
                        "image_caption": "GNNLens is a visual analytics tool that helps model developers and users understand and diagnose GNNs. GNNLens consists of four visualization components. Parallel Sets View (b) and Projection View (c) enable users to quickly identify and validate error patterns in the set of wrong predictions. Graph View (d) and Feature Matrix View (e) offer a detailed analysis of individual nodes to assist users in forming hypotheses about the error patterns. They are linked together to support users to analyze GNN models simultaneously from multiple angles and extract general error patterns in GNN prediction results.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "https://youtu.be/eUl_RlC2Q24",
                        "ff_id": "eUl_RlC2Q24"
                    },
                    {
                        "slot_id": "v-tvcg-9705076-qa",
                        "session_id": "full19",
                        "type": "In Person Q+A",
                        "title": "GNNLens: A Visual Analytics Approach for Prediction Error Diagnosis of Graph Neural Networks (Q+A)",
                        "contributors": [
                            "Zhihua JIN"
                        ],
                        "authors": [],
                        "abstract": "Graph Neural Networks (GNNs) aim to extend deep learning techniques to graph data and have achieved significant progress in graph analysis tasks (e.g., node classification) in recent years. However, similar to other deep neural networks like Convolutional Neural Networks (CNNs) and Recurrent Neural Networks (RNNs), GNNs behave like a black box with their details hidden from model developers and users. It is therefore difficult to diagnose possible errors of GNNs. Despite many visual analytics studies being done on CNNs and RNNs, little research has addressed the challenges for GNNs. This paper fills the research gap with an interactive visual analysis tool, GNNLens, to assist model developers and users in understanding and analyzing GNNs. Specifically, Parallel Sets View and Projection View enable users to quickly identify and validate error patterns in the set of wrong predictions; Graph View and Feature Matrix View offer a detailed analysis of individual nodes to assist users in forming hypotheses about the error patterns. Since GNNs jointly model the graph structure and the node features, we reveal the relative influences of the two types of information by comparing the predictions of three models: GNN, Multi-Layer Perceptron (MLP), and GNN Without Using Features (GNNWUF). Two case studies and interviews with domain experts demonstrate the effectiveness of GNNLens in facilitating the understanding of GNN models and their errors.",
                        "uid": "v-tvcg-9705076",
                        "file_name": "",
                        "time_stamp": "2022-10-19T15:54:00Z",
                        "time_start": "2022-10-19T15:54:00Z",
                        "time_end": "2022-10-19T15:57:00Z",
                        "paper_type": "full",
                        "keywords": [
                            "Graph Neural Networks, Error Diagnosis, Visualization"
                        ],
                        "has_image": "1",
                        "has_video": "643",
                        "paper_award": "",
                        "image_caption": "GNNLens is a visual analytics tool that helps model developers and users understand and diagnose GNNs. GNNLens consists of four visualization components. Parallel Sets View (b) and Projection View (c) enable users to quickly identify and validate error patterns in the set of wrong predictions. Graph View (d) and Feature Matrix View (e) offer a detailed analysis of individual nodes to assist users in forming hypotheses about the error patterns. They are linked together to support users to analyze GNN models simultaneously from multiple angles and extract general error patterns in GNN prediction results.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "https://youtu.be/eUl_RlC2Q24",
                        "ff_id": "eUl_RlC2Q24"
                    },
                    {
                        "slot_id": "v-full-1119-pres",
                        "session_id": "full19",
                        "type": "Virtual Presentation (pre-recorded)",
                        "title": "Visual Analysis of Neural Architecture Spaces for Summarizing Design Principles",
                        "contributors": [
                            "Jun Yuan"
                        ],
                        "authors": [
                            "Jun Yuan",
                            "Mengchen Liu",
                            "Fengyuan Tian",
                            "Shixia Liu"
                        ],
                        "abstract": "Recent advances in artificial intelligence largely benefit from better neural network architectures. These architectures are a product of a costly process of trial-and-error. To ease this process, we develop ArchExplorer, a visual analysis method for understanding a neural architecture space and summarizing design principles. The key idea behind our method is to make the architecture space explainable by exploiting structural distances between architectures. We formulate the calculation of the pairwise distances as solving an all-pairs shortest path problem. To improve efficiency, we decompose this problem into a set of single-source shortest path problems. The time complexity is reduced from O(kn^2N) to O(knN). Architectures are hierarchically clustered according to the distances between them. A circle-packing-based architecture visualization has been developed to convey both the global relationships between clusters and local neighborhoods of the architectures in each cluster. Two case studies and a post-analysis are presented to demonstrate the effectiveness of ArchExplorer in summarizing design principles and selecting better-performing architectures.",
                        "uid": "v-full-1119",
                        "file_name": "v-full-1119_Yuan_Presentation.mp4",
                        "time_stamp": "2022-10-19T15:57:00Z",
                        "time_start": "2022-10-19T15:57:00Z",
                        "time_end": "2022-10-19T16:06:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "510",
                        "paper_award": "",
                        "image_caption": "ArchExplorer: (a) the architecture visualization to show the architecture clusters; (b) three selected sub-clusters after zooming into cluster A; (c) a detailed comparison of the selected architectures.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "https://youtu.be/ThkSP326kJ8",
                        "ff_id": "ThkSP326kJ8"
                    },
                    {
                        "slot_id": "v-full-1119-qa",
                        "session_id": "full19",
                        "type": "Virtual Q+A",
                        "title": "Visual Analysis of Neural Architecture Spaces for Summarizing Design Principles (Q+A)",
                        "contributors": [
                            "Jun Yuan"
                        ],
                        "authors": [],
                        "abstract": "Recent advances in artificial intelligence largely benefit from better neural network architectures. These architectures are a product of a costly process of trial-and-error. To ease this process, we develop ArchExplorer, a visual analysis method for understanding a neural architecture space and summarizing design principles. The key idea behind our method is to make the architecture space explainable by exploiting structural distances between architectures. We formulate the calculation of the pairwise distances as solving an all-pairs shortest path problem. To improve efficiency, we decompose this problem into a set of single-source shortest path problems. The time complexity is reduced from O(kn^2N) to O(knN). Architectures are hierarchically clustered according to the distances between them. A circle-packing-based architecture visualization has been developed to convey both the global relationships between clusters and local neighborhoods of the architectures in each cluster. Two case studies and a post-analysis are presented to demonstrate the effectiveness of ArchExplorer in summarizing design principles and selecting better-performing architectures.",
                        "uid": "v-full-1119",
                        "file_name": "",
                        "time_stamp": "2022-10-19T16:06:00Z",
                        "time_start": "2022-10-19T16:06:00Z",
                        "time_end": "2022-10-19T16:09:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "510",
                        "paper_award": "",
                        "image_caption": "ArchExplorer: (a) the architecture visualization to show the architecture clusters; (b) three selected sub-clusters after zooming into cluster A; (c) a detailed comparison of the selected architectures.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "https://youtu.be/ThkSP326kJ8",
                        "ff_id": "ThkSP326kJ8"
                    },
                    {
                        "slot_id": "v-full-1626-pres",
                        "session_id": "full19",
                        "type": "In Person Presentation",
                        "title": "NAS-Navigator: Visual Steering for Explainable One-Shot Deep Neural Network Synthesis",
                        "contributors": [
                            "Anjul Kumar Tyagi"
                        ],
                        "authors": [
                            "Anjul Kumar Tyagi",
                            "Cong Xie",
                            "Klaus Mueller"
                        ],
                        "abstract": "Recent advancements in deep learning have shown the effectiveness of deep neural networks in several applications. The success of deep learning can be attributed to hours of parameter and architecture tuning by human experts. Neural Architecture Search (NAS) techniques aim to solve this problem by automating the search procedure for deep neural network architectures making it possible for non-experts to work with deep learning. Specifically, One-shot NAS techniques have recently gained popularity as they are known to reduce the search time for NAS techniques. One-Shot NAS works by training a large template network through parameter sharing which includes all the candidate neural networks. This is followed by applying a procedure to rank its components through evaluating the possible candidate architectures chosen randomly. However, as these search models become increasingly powerful and diverse, they become harder to understand. Consequently, even though the search results work well, it is hard to identify search biases and control the search progression, hence a need for explainability and human-in-the-loop One-Shot NAS. To alleviate these problems, we present NAS-Navigator, a visual analytics system aiming to solve three problems with One-Shot NAS; explainability, human-in-the-loop design, and performance improvements compared to existing state-of-the-art techniques. NAS-Navigator gives full control of NAS back in the hands of the users while still keeping the perks of automated search, thus assisting non-expert users. Analysts can use their domain knowledge aided by cues from the interface to guide the search. Evaluation results confirm the performance of our improved One-Shot NAS algorithm is comparable to other state-of-the-art techniques. While adding visual analytics using NAS-Navigator shows further improvements in search time and performance. We designed our interface in collaboration with several deep learning researchers and evaluated NAS-Navigator through a control experiment and expert interviews.",
                        "uid": "v-full-1626",
                        "file_name": "v-full-1626_Tyagi_Presentation.mp4",
                        "time_stamp": "2022-10-19T16:09:00Z",
                        "time_start": "2022-10-19T16:09:00Z",
                        "time_end": "2022-10-19T16:18:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "314",
                        "paper_award": "",
                        "image_caption": "NAS-Navigator is a human-in-the-loop, full-explainable system for Neural Network Architecture Search. Compared to full automated NAS techniques, NAS-Navigator allows users to control the NAS search and visualize the network architecture search space.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "https://youtu.be/DwCxfleJStg",
                        "ff_id": "DwCxfleJStg"
                    },
                    {
                        "slot_id": "v-full-1626-qa",
                        "session_id": "full19",
                        "type": "In Person Q+A",
                        "title": "NAS-Navigator: Visual Steering for Explainable One-Shot Deep Neural Network Synthesis (Q+A)",
                        "contributors": [
                            "Anjul Kumar Tyagi"
                        ],
                        "authors": [],
                        "abstract": "Recent advancements in deep learning have shown the effectiveness of deep neural networks in several applications. The success of deep learning can be attributed to hours of parameter and architecture tuning by human experts. Neural Architecture Search (NAS) techniques aim to solve this problem by automating the search procedure for deep neural network architectures making it possible for non-experts to work with deep learning. Specifically, One-shot NAS techniques have recently gained popularity as they are known to reduce the search time for NAS techniques. One-Shot NAS works by training a large template network through parameter sharing which includes all the candidate neural networks. This is followed by applying a procedure to rank its components through evaluating the possible candidate architectures chosen randomly. However, as these search models become increasingly powerful and diverse, they become harder to understand. Consequently, even though the search results work well, it is hard to identify search biases and control the search progression, hence a need for explainability and human-in-the-loop One-Shot NAS. To alleviate these problems, we present NAS-Navigator, a visual analytics system aiming to solve three problems with One-Shot NAS; explainability, human-in-the-loop design, and performance improvements compared to existing state-of-the-art techniques. NAS-Navigator gives full control of NAS back in the hands of the users while still keeping the perks of automated search, thus assisting non-expert users. Analysts can use their domain knowledge aided by cues from the interface to guide the search. Evaluation results confirm the performance of our improved One-Shot NAS algorithm is comparable to other state-of-the-art techniques. While adding visual analytics using NAS-Navigator shows further improvements in search time and performance. We designed our interface in collaboration with several deep learning researchers and evaluated NAS-Navigator through a control experiment and expert interviews.",
                        "uid": "v-full-1626",
                        "file_name": "",
                        "time_stamp": "2022-10-19T16:18:00Z",
                        "time_start": "2022-10-19T16:18:00Z",
                        "time_end": "2022-10-19T16:21:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "314",
                        "paper_award": "",
                        "image_caption": "NAS-Navigator is a human-in-the-loop, full-explainable system for Neural Network Architecture Search. Compared to full automated NAS techniques, NAS-Navigator allows users to control the NAS search and visualize the network architecture search space.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "https://youtu.be/DwCxfleJStg",
                        "ff_id": "DwCxfleJStg"
                    },
                    {
                        "slot_id": "v-full-1338-pres",
                        "session_id": "full19",
                        "type": "Virtual Presentation (pre-recorded)",
                        "title": "HetVis: A Visual Analysis Approach for Identifying Data Heterogeneity in Horizontal Federated Learning",
                        "contributors": [
                            "Xumeng Wang"
                        ],
                        "authors": [
                            "Xumeng Wang",
                            "Wei Chen",
                            "Jiazhi Xia",
                            "Zhen Wen",
                            "Rongchen Zhu",
                            "Tobias Schreck"
                        ],
                        "abstract": "Horizontal federated learning (HFL) enables distributed clients to train a shared model and keep their data privacy. In training high-quality HFL models, the data heterogeneity among clients is one of the major concerns. However, due to the security issue and the complexity of deep learning models, it is challenging to investigate data heterogeneity across different clients. To address this issue, based on a requirement analysis we developed a visual analytics tool, HetVis, for participating clients to explore data heterogeneity. We identify data heterogeneity through comparing prediction behaviors of the global federated model and the stand-alone model trained with local data. Then, a context-aware clustering of the inconsistent records is done, to provide a summary of data heterogeneity. Combining with the proposed comparison techniques, we develop a novel set of visualizations to identify heterogeneity issues in HFL. We designed three case studies to introduce how HetVis can assist client analysts in understanding different types of heterogeneity issues. Expert reviews and a comparative study demonstrate the effectiveness of HetVis.",
                        "uid": "v-full-1338",
                        "file_name": "v-full-1338_Wang_Presentation.mp4",
                        "time_stamp": "2022-10-19T16:21:00Z",
                        "time_start": "2022-10-19T16:21:00Z",
                        "time_end": "2022-10-19T16:30:09Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "609",
                        "paper_award": "",
                        "image_caption": "We developed a visual analytics tool, HetVis, for participating clients in federated learning to explore data heterogeneity. We identify data heterogeneity by comparing prediction behaviors of the global federated model and the stand-alone model trained with local data. Then, a context-aware clustering of the inconsistent records is done, to provide a summary of data heterogeneity. To further explore the extracted heterogeneity issues and corresponding impacts, we leverage a suite of novel charts, as shown in the image.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "https://youtu.be/bboSdZ294x0",
                        "ff_id": "bboSdZ294x0"
                    },
                    {
                        "slot_id": "v-full-1338-qa",
                        "session_id": "full19",
                        "type": "Virtual Q+A",
                        "title": "HetVis: A Visual Analysis Approach for Identifying Data Heterogeneity in Horizontal Federated Learning (Q+A)",
                        "contributors": [
                            "Xumeng Wang"
                        ],
                        "authors": [],
                        "abstract": "Horizontal federated learning (HFL) enables distributed clients to train a shared model and keep their data privacy. In training high-quality HFL models, the data heterogeneity among clients is one of the major concerns. However, due to the security issue and the complexity of deep learning models, it is challenging to investigate data heterogeneity across different clients. To address this issue, based on a requirement analysis we developed a visual analytics tool, HetVis, for participating clients to explore data heterogeneity. We identify data heterogeneity through comparing prediction behaviors of the global federated model and the stand-alone model trained with local data. Then, a context-aware clustering of the inconsistent records is done, to provide a summary of data heterogeneity. Combining with the proposed comparison techniques, we develop a novel set of visualizations to identify heterogeneity issues in HFL. We designed three case studies to introduce how HetVis can assist client analysts in understanding different types of heterogeneity issues. Expert reviews and a comparative study demonstrate the effectiveness of HetVis.",
                        "uid": "v-full-1338",
                        "file_name": "",
                        "time_stamp": "2022-10-19T16:30:09Z",
                        "time_start": "2022-10-19T16:30:09Z",
                        "time_end": "2022-10-19T16:33:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "609",
                        "paper_award": "",
                        "image_caption": "We developed a visual analytics tool, HetVis, for participating clients in federated learning to explore data heterogeneity. We identify data heterogeneity by comparing prediction behaviors of the global federated model and the stand-alone model trained with local data. Then, a context-aware clustering of the inconsistent records is done, to provide a summary of data heterogeneity. To further explore the extracted heterogeneity issues and corresponding impacts, we leverage a suite of novel charts, as shown in the image.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "https://youtu.be/bboSdZ294x0",
                        "ff_id": "bboSdZ294x0"
                    },
                    {
                        "slot_id": "v-full-1205-pres",
                        "session_id": "full19",
                        "type": "In Person Presentation",
                        "title": "DendroMap: Visual Exploration of Large-Scale Image Datasets for Machine Learning with Treemaps",
                        "contributors": [
                            "Minsuk Kahng",
                            "Donald Bertucci"
                        ],
                        "authors": [
                            "Donald R Bertucci",
                            "Md Montaser Hamid",
                            "Yashwanthi Anand",
                            "Anita Ruangrotsakun",
                            "Delyar Tabatabai",
                            "Melissa Perez",
                            "Minsuk Kahng"
                        ],
                        "abstract": "In this paper, we present DendroMap, a novel approach to interactively exploring large-scale image datasets for machine learning (ML). ML practitioners often explore image datasets by generating a grid of images or projecting high-dimensional representations of images into 2-D using dimensionality reduction techniques (e.g., t-SNE). However, neither approach effectively scales to large datasets because images are ineffectively organized and interactions are insufficiently supported. To address these challenges, we develop DendroMap by adapting Treemaps, a well-known visualization technique. DendroMap effectively organizes images by extracting hierarchical cluster structures from high-dimensional representations of images. It enables users to make sense of the overall distributions of datasets and interactively zoom into specific areas of interests at multiple levels of abstraction. Our case studies with widely-used image datasets for deep learning demonstrate that users can discover insights about datasets and trained models by examining the diversity of images, identifying underperforming subgroups, and analyzing classification errors. We conducted a user study that evaluates the effectiveness of DendroMap in grouping and searching tasks by comparing it with a gridified version of t-SNE and found that participants preferred DendroMap. DendroMap is available at https://div-lab.github.io/dendromap/.",
                        "uid": "v-full-1205",
                        "file_name": "v-full-1205_Bertucci_Presentation.mp4",
                        "time_stamp": "2022-10-19T16:33:00Z",
                        "time_start": "2022-10-19T16:33:00Z",
                        "time_end": "2022-10-19T16:42:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "331",
                        "paper_award": "",
                        "image_caption": "DendroMap is an interactive visualization system for exploring large-scale image datasets used in machine learning. The initial view of DendroMap shown on the left side of the figure presents an image dataset as nested rectangles of similar images, like treemaps, providing an overview of the entire dataset. A user clicks on a rectangle with plants, insects, and animals to zoom down into the hierarchy of those images. Then, DendroMap expands this portion to take up the space of the entire screen and shows new similar image groups.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "https://youtu.be/2Fq7Z4Y-cbI",
                        "ff_id": "2Fq7Z4Y-cbI"
                    },
                    {
                        "slot_id": "v-full-1205-qa",
                        "session_id": "full19",
                        "type": "In Person Q+A",
                        "title": "DendroMap: Visual Exploration of Large-Scale Image Datasets for Machine Learning with Treemaps (Q+A)",
                        "contributors": [
                            "Minsuk Kahng",
                            "Donald Bertucci"
                        ],
                        "authors": [],
                        "abstract": "In this paper, we present DendroMap, a novel approach to interactively exploring large-scale image datasets for machine learning (ML). ML practitioners often explore image datasets by generating a grid of images or projecting high-dimensional representations of images into 2-D using dimensionality reduction techniques (e.g., t-SNE). However, neither approach effectively scales to large datasets because images are ineffectively organized and interactions are insufficiently supported. To address these challenges, we develop DendroMap by adapting Treemaps, a well-known visualization technique. DendroMap effectively organizes images by extracting hierarchical cluster structures from high-dimensional representations of images. It enables users to make sense of the overall distributions of datasets and interactively zoom into specific areas of interests at multiple levels of abstraction. Our case studies with widely-used image datasets for deep learning demonstrate that users can discover insights about datasets and trained models by examining the diversity of images, identifying underperforming subgroups, and analyzing classification errors. We conducted a user study that evaluates the effectiveness of DendroMap in grouping and searching tasks by comparing it with a gridified version of t-SNE and found that participants preferred DendroMap. DendroMap is available at https://div-lab.github.io/dendromap/.",
                        "uid": "v-full-1205",
                        "file_name": "",
                        "time_stamp": "2022-10-19T16:42:00Z",
                        "time_start": "2022-10-19T16:42:00Z",
                        "time_end": "2022-10-19T16:45:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "331",
                        "paper_award": "",
                        "image_caption": "DendroMap is an interactive visualization system for exploring large-scale image datasets used in machine learning. The initial view of DendroMap shown on the left side of the figure presents an image dataset as nested rectangles of similar images, like treemaps, providing an overview of the entire dataset. A user clicks on a rectangle with plants, insects, and animals to zoom down into the hierarchy of those images. Then, DendroMap expands this portion to take up the space of the entire screen and shows new similar image groups.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "https://youtu.be/2Fq7Z4Y-cbI",
                        "ff_id": "2Fq7Z4Y-cbI"
                    },
                    {
                        "slot_id": "v-tvcg-9795241-pres",
                        "session_id": "full19",
                        "type": "Virtual Presentation (pre-recorded)",
                        "title": "Diagnosing Ensemble Few-Shot Classifiers",
                        "contributors": [
                            "Weikai Yang"
                        ],
                        "authors": [
                            "Weikai Yang",
                            "Xi Ye",
                            "Xingxing Zhang",
                            "Lanxi Xiao",
                            "Jiazhi Xia",
                            "Zhongyuan Wang",
                            "Jun Zhu",
                            "Hanspeter Pfister",
                            "Shixia Liu"
                        ],
                        "abstract": "The base learners and labeled samples (shots) in an ensemble few-shot classifier greatly affect the model performance. When the performance is not satisfactory, it is usually difficult to understand the underlying causes and make improvements. To tackle this issue, we propose a visual analysis method, FSLDiagnotor. Given a set of base learners and a collection of samples with a few shots, we consider two problems: 1) finding a subset of base learners that well predict the sample collections; and 2) replacing the low-quality shots with more representative ones to adequately represent the sample collections. We formulate both problems as sparse subset selection and develop two selection algorithms to recommend appropriate learners and shots, respectively. A matrix visualization and a scatterplot are combined to explain the recommended learners and shots in context and facilitate users in adjusting them. Based on the adjustment, the algorithm updates the recommendation results for another round of improvement. Two case studies are conducted to demonstrate that FSLDiagnotor helps build a few-shot classifier efficiently and increases the accuracy by 12% and 21%, respectively.",
                        "uid": "v-tvcg-9795241",
                        "file_name": "v-tvcg-9795241_Yang_Presentation.mp4",
                        "time_stamp": "2022-10-19T16:45:00Z",
                        "time_start": "2022-10-19T16:45:00Z",
                        "time_end": "2022-10-19T16:54:00Z",
                        "paper_type": "full",
                        "keywords": [
                            "Few-shot learning, ensemble model, subset selection, matrix visualization, scatterplot"
                        ],
                        "has_image": "1",
                        "has_video": "557",
                        "paper_award": "",
                        "image_caption": "FSLDiagnotor: the learner view (left) compares base learners (rows) with the ensemble model, including the overall difference (circles in the first column) and detailed difference (stacked bars in the other columns); the sample view (right) visualizes the shots and unlabeled samples in context. The image content and label distributions of the samples of interest are displayed below.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "https://youtu.be/jQnSJt2pKhs",
                        "ff_id": "jQnSJt2pKhs"
                    },
                    {
                        "slot_id": "v-tvcg-9795241-qa",
                        "session_id": "full19",
                        "type": "Virtual Q+A",
                        "title": "Diagnosing Ensemble Few-Shot Classifiers (Q+A)",
                        "contributors": [
                            "Weikai Yang"
                        ],
                        "authors": [],
                        "abstract": "The base learners and labeled samples (shots) in an ensemble few-shot classifier greatly affect the model performance. When the performance is not satisfactory, it is usually difficult to understand the underlying causes and make improvements. To tackle this issue, we propose a visual analysis method, FSLDiagnotor. Given a set of base learners and a collection of samples with a few shots, we consider two problems: 1) finding a subset of base learners that well predict the sample collections; and 2) replacing the low-quality shots with more representative ones to adequately represent the sample collections. We formulate both problems as sparse subset selection and develop two selection algorithms to recommend appropriate learners and shots, respectively. A matrix visualization and a scatterplot are combined to explain the recommended learners and shots in context and facilitate users in adjusting them. Based on the adjustment, the algorithm updates the recommendation results for another round of improvement. Two case studies are conducted to demonstrate that FSLDiagnotor helps build a few-shot classifier efficiently and increases the accuracy by 12% and 21%, respectively.",
                        "uid": "v-tvcg-9795241",
                        "file_name": "",
                        "time_stamp": "2022-10-19T16:54:00Z",
                        "time_start": "2022-10-19T16:54:00Z",
                        "time_end": "2022-10-19T16:57:00Z",
                        "paper_type": "full",
                        "keywords": [
                            "Few-shot learning, ensemble model, subset selection, matrix visualization, scatterplot"
                        ],
                        "has_image": "1",
                        "has_video": "557",
                        "paper_award": "",
                        "image_caption": "FSLDiagnotor: the learner view (left) compares base learners (rows) with the ensemble model, including the overall difference (circles in the first column) and detailed difference (stacked bars in the other columns); the sample view (right) visualizes the shots and unlabeled samples in context. The image content and label distributions of the samples of interest are displayed below.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "https://youtu.be/jQnSJt2pKhs",
                        "ff_id": "jQnSJt2pKhs"
                    }
                ]
            },
            {
                "title": "Neuro/Brain/Medical Data",
                "session_id": "full20",
                "event_prefix": "v-full",
                "track": "ok6",
                "livestream_id": "ok6-thu",
                "session_image": "full20.png",
                "chair": [
                    "Johanna Beyer"
                ],
                "organizers": [],
                "time_start": "2022-10-20T14:00:00Z",
                "time_end": "2022-10-20T15:15:00Z",
                "discord_category": "",
                "discord_channel": "oklahoma-st-6",
                "discord_channel_id": "1024600906689421372",
                "discord_link": "https://discord.com/channels/978320435554947082/1024600906689421372",
                "slido_link": "https://app.sli.do/event/i28UETZaiRWr1XwX6eKjcf?section=29ba1f79-04cd-4568-a83c-ab81aa3c28b8",
                "youtube_url": "https://youtu.be/KIIASF6gEIE",
                "youtube_id": "KIIASF6gEIE",
                "zoom_meeting": "",
                "zoom_password": "",
                "zoom_link": "",
                "ff_link": "https://youtu.be/UknH_D1Hjt4",
                "ff_playlist": "",
                "time_slots": [
                    {
                        "slot_id": "full20-opening",
                        "session_id": "full20",
                        "type": "In Person Other",
                        "title": "Session Opening",
                        "contributors": [
                            "Johanna Beyer"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "",
                        "file_name": "",
                        "time_stamp": "2022-10-20T14:00:00Z",
                        "time_start": "2022-10-20T14:00:00Z",
                        "time_end": "2022-10-20T14:00:00Z",
                        "paper_type": "",
                        "keywords": [],
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "v-tvcg-9492002-pres",
                        "session_id": "full20",
                        "type": "Virtual Presentation (pre-recorded)",
                        "title": "MV2Net: Multi-Variate Multi-View Brain Network Comparison over Uncertain Data",
                        "contributors": [
                            "Lei Shi"
                        ],
                        "authors": [
                            "Lei Shi",
                            "Junnan Hu",
                            "Zhihao Tan",
                            "Jun Tao",
                            "Jiayan Ding",
                            "Yan Jin",
                            "Yanjun Wu",
                            "Paul M. Thompson"
                        ],
                        "abstract": "Visually identifying effective bio-markers from human brain networks poses non-trivial challenges to the field of data visualization and analysis. Existing methods in the literature and neuroscience practice are generally limited to the study of individual connectivity features in the brain (e.g., the strength of neural connection among brain regions). Pairwise comparisons between contrasting subject groups (e.g., the diseased and the healthy controls) are normally performed. The underlying neuroimaging and brain network construction process is assumed to have 100% fidelity. Yet, real-world user requirements on brain network visual comparison lean against these assumptions. In this work, we present MV^2Net, a visual analytics system that tightly integrates multi-variate multi-view visualization for brain network comparison with an interactive wrangling mechanism to deal with data uncertainty. On the analysis side, the system integrates multiple extraction methods on diffusion and geometric connectivity features of brain networks, an anomaly detection algorithm for data quality assessment, single- and multi-connection feature selection methods for bio-marker detection. On the visualization side, novel designs are introduced which optimize network comparisons among contrasting subject groups and related connectivity features. Our design provides level-of-detail comparisons, from juxtaposed and explicit-coding views for subject group comparisons, to high-order composite view for correlation of network comparisons, and to fiber tract detail view for voxel-level comparisons. The proposed techniques are inspired and evaluated in expert studies, as well as through case analyses on diffusion and geometric bio-markers of certain neurology diseases. Results in these experiments demonstrate the effectiveness and superiority of MV^2Net over state-of-the-art approaches.",
                        "uid": "v-tvcg-9492002",
                        "file_name": "v-tvcg-9492002_Shi_Presentation.mp4",
                        "time_stamp": "2022-10-20T14:00:00Z",
                        "time_start": "2022-10-20T14:00:00Z",
                        "time_end": "2022-10-20T14:10:13Z",
                        "paper_type": "full",
                        "keywords": [
                            "brain network, visual comparison, multivariate analysis"
                        ],
                        "has_image": "1",
                        "has_video": "613",
                        "paper_award": "",
                        "image_caption": "The visualization interface of MV2Net: (a) selection panel for two subject groups under comparison; (b) heatmaps showing data quality in the subject by feature matrices which helps to select high-quality features; (c) brain network view for group-based comparison; (d) high-order composite of multiple comparisons; (e) 3D view of fiber tract details between two selected ROIs.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "https://youtu.be/aG_QEf203sk",
                        "ff_id": "aG_QEf203sk"
                    },
                    {
                        "slot_id": "v-tvcg-9492002-qa",
                        "session_id": "full20",
                        "type": "Virtual Q+A",
                        "title": "MV2Net: Multi-Variate Multi-View Brain Network Comparison over Uncertain Data (Q+A)",
                        "contributors": [
                            "Lei Shi"
                        ],
                        "authors": [],
                        "abstract": "Visually identifying effective bio-markers from human brain networks poses non-trivial challenges to the field of data visualization and analysis. Existing methods in the literature and neuroscience practice are generally limited to the study of individual connectivity features in the brain (e.g., the strength of neural connection among brain regions). Pairwise comparisons between contrasting subject groups (e.g., the diseased and the healthy controls) are normally performed. The underlying neuroimaging and brain network construction process is assumed to have 100% fidelity. Yet, real-world user requirements on brain network visual comparison lean against these assumptions. In this work, we present MV^2Net, a visual analytics system that tightly integrates multi-variate multi-view visualization for brain network comparison with an interactive wrangling mechanism to deal with data uncertainty. On the analysis side, the system integrates multiple extraction methods on diffusion and geometric connectivity features of brain networks, an anomaly detection algorithm for data quality assessment, single- and multi-connection feature selection methods for bio-marker detection. On the visualization side, novel designs are introduced which optimize network comparisons among contrasting subject groups and related connectivity features. Our design provides level-of-detail comparisons, from juxtaposed and explicit-coding views for subject group comparisons, to high-order composite view for correlation of network comparisons, and to fiber tract detail view for voxel-level comparisons. The proposed techniques are inspired and evaluated in expert studies, as well as through case analyses on diffusion and geometric bio-markers of certain neurology diseases. Results in these experiments demonstrate the effectiveness and superiority of MV^2Net over state-of-the-art approaches.",
                        "uid": "v-tvcg-9492002",
                        "file_name": "",
                        "time_stamp": "2022-10-20T14:10:13Z",
                        "time_start": "2022-10-20T14:10:13Z",
                        "time_end": "2022-10-20T14:12:00Z",
                        "paper_type": "full",
                        "keywords": [
                            "brain network, visual comparison, multivariate analysis"
                        ],
                        "has_image": "1",
                        "has_video": "613",
                        "paper_award": "",
                        "image_caption": "The visualization interface of MV2Net: (a) selection panel for two subject groups under comparison; (b) heatmaps showing data quality in the subject by feature matrices which helps to select high-quality features; (c) brain network view for group-based comparison; (d) high-order composite of multiple comparisons; (e) 3D view of fiber tract details between two selected ROIs.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "https://youtu.be/aG_QEf203sk",
                        "ff_id": "aG_QEf203sk"
                    },
                    {
                        "slot_id": "v-tvcg-9529035-pres",
                        "session_id": "full20",
                        "type": "In Person Presentation",
                        "title": "NeuroConstruct: 3D Reconstruction and Visualization of Neurites in Optical Microscopy Brain Images",
                        "contributors": [
                            "Parmida Ghahremani",
                            "Saeed Boorboor"
                        ],
                        "authors": [
                            "Parmida Ghahremani",
                            "Saeed Boorboor",
                            "Pooya Mirhosseini",
                            "Chetan Gudisagar",
                            "Mala Ananth",
                            "David Talmage",
                            "Lorna W. Role",
                            "Arie E. Kaufman"
                        ],
                        "abstract": "We introduce NeuroConstruct, a novel end-to-end application for the segmentation, registration, and visualization of brain volumes imaged using wide-field microscopy. NeuroConstruct offers a Segmentation Toolbox with various annotation helper functions that aid experts to effectively and precisely annotate micrometer resolution neurites. It also offers an automatic neurites segmentation using convolutional neuronal networks (CNN) trained by the Toolbox annotations and somas segmentation using thresholding. To visualize neurites in a given volume, NeuroConstruct offers a hybrid rendering by combining iso-surface rendering of high-confidence classified neurites, along with real-time rendering of raw volume using a 2D transfer function for voxel classification score vs. voxel intensity value. For a complete reconstruction of the 3D neurites, we introduce a Registration Toolbox that provides automatic coarse-to-fine alignment of serially sectioned samples. The quantitative and qualitative analysis show that NeuroConstruct outperforms the state-of-the-art in all design aspects. NeuroConstruct was developed as a collaboration between computer scientists and neuroscientists, with an application to the study of cholinergic neurons, which are severely affected in Alzheimer\u2019s disease.",
                        "uid": "v-tvcg-9529035",
                        "file_name": "v-tvcg-9529035_Ghahremani_Presentation.mp4",
                        "time_stamp": "2022-10-20T14:12:00Z",
                        "time_start": "2022-10-20T14:12:00Z",
                        "time_end": "2022-10-20T14:22:00Z",
                        "paper_type": "full",
                        "keywords": [
                            "Wide-field Microscopy,Neuron Morphology,Segmentation,Image Processing and Computer Vision,Computing Methodologies,Registration,Hybrid Volume Rendering,CNN"
                        ],
                        "has_image": "0",
                        "has_video": "430",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "v-tvcg-9529035-qa",
                        "session_id": "full20",
                        "type": "In Person Q+A",
                        "title": "NeuroConstruct: 3D Reconstruction and Visualization of Neurites in Optical Microscopy Brain Images (Q+A)",
                        "contributors": [
                            "Parmida Ghahremani",
                            "Saeed Boorboor"
                        ],
                        "authors": [],
                        "abstract": "We introduce NeuroConstruct, a novel end-to-end application for the segmentation, registration, and visualization of brain volumes imaged using wide-field microscopy. NeuroConstruct offers a Segmentation Toolbox with various annotation helper functions that aid experts to effectively and precisely annotate micrometer resolution neurites. It also offers an automatic neurites segmentation using convolutional neuronal networks (CNN) trained by the Toolbox annotations and somas segmentation using thresholding. To visualize neurites in a given volume, NeuroConstruct offers a hybrid rendering by combining iso-surface rendering of high-confidence classified neurites, along with real-time rendering of raw volume using a 2D transfer function for voxel classification score vs. voxel intensity value. For a complete reconstruction of the 3D neurites, we introduce a Registration Toolbox that provides automatic coarse-to-fine alignment of serially sectioned samples. The quantitative and qualitative analysis show that NeuroConstruct outperforms the state-of-the-art in all design aspects. NeuroConstruct was developed as a collaboration between computer scientists and neuroscientists, with an application to the study of cholinergic neurons, which are severely affected in Alzheimer\u2019s disease.",
                        "uid": "v-tvcg-9529035",
                        "file_name": "",
                        "time_stamp": "2022-10-20T14:22:00Z",
                        "time_start": "2022-10-20T14:22:00Z",
                        "time_end": "2022-10-20T14:24:00Z",
                        "paper_type": "full",
                        "keywords": [
                            "Wide-field Microscopy,Neuron Morphology,Segmentation,Image Processing and Computer Vision,Computing Methodologies,Registration,Hybrid Volume Rendering,CNN"
                        ],
                        "has_image": "0",
                        "has_video": "430",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "v-tvcg-9555234-pres",
                        "session_id": "full20",
                        "type": "In Person Presentation",
                        "title": "DXplorer: A Unified Visualization Framework for Interactive Dendritic Spine Analysis using 3D morphological Features",
                        "contributors": [
                            "Won-Ki Jeong",
                            "JunYoung Choi"
                        ],
                        "authors": [
                            "JunYoung Choi",
                            "Sang-Eun Lee",
                            "YeIn Lee",
                            "Eunji Cho",
                            "Sunghoe Chang",
                            "Won-Ki Jeong"
                        ],
                        "abstract": "Dendritic spines are dynamic, submicron-scale protrusions on neuronal dendrites that receive neuronal inputs. Morphological changes in the dendritic spine often reflect alterations in physiological conditions and are indicators of various neuropsychiatric conditions. However, owing to the highly dynamic and heterogeneous nature of spines, accurate measurement and objective analysis of spine morphology are major challenges in neuroscience research. Most conventional approaches for analyzing dendritic spines are based on two-dimensional (2D) images, which barely reflect the actual three-dimensional (3D) shapes. Although some recent studies have attempted to analyze spines with various 3D-based features, it is still difficult to objectively categorize and analyze spines based on 3D morphology. Here, we propose a unified visualization framework for an interactive 3D dendritic spine analysis system, DXplorer, that displays 3D rendering of spines and plots the high-dimensional features extracted from the 3D mesh of spines. With this system, users can perform the clustering of spines interactively and explore and analyze dendritic spines based on high-dimensional features. We propose a series of high-dimensional morphological features extracted from a 3D mesh of dendritic spines. In addition, an interactive machine learning classifier with visual exploration and user feedback using an interactive 3D mesh grid view ensures a more precise classification based on the spine phenotype. A user study and two case studies were conducted to quantitatively verify the performance and usability of the DXplorer. We demonstrate that the system performs the entire analytic process effectively and provides high-quality, accurate, and objective analysis.",
                        "uid": "v-tvcg-9555234",
                        "file_name": "v-tvcg-9555234_Choi_Presentation.mp4",
                        "time_stamp": "2022-10-20T14:24:00Z",
                        "time_start": "2022-10-20T14:24:00Z",
                        "time_end": "2022-10-20T14:34:00Z",
                        "paper_type": "full",
                        "keywords": [
                            "Biomedical and Medical Visualization, Machine Learning, Task and Requirements Analysis, User Interfaces, Intelligence Analysis"
                        ],
                        "has_image": "1",
                        "has_video": "485",
                        "paper_award": "",
                        "image_caption": "Example of dendritic spine\u2019s 3D mesh with morphological features (upper row) and DXplorer\u2019s user interfaces (bottom row). DXplorer is a unified visual analysis framework for the interactive analysis of dendritic spines using 3D meshes and high-dimensional features. The aim of the proposed system is to provide a unified data processing and visualization workflow for various spine analysis tasks, which will eventually provide insight into the correlation between neuronal functions and the shape of the spine.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "https://youtu.be/Up6eOWWjegI",
                        "ff_id": "Up6eOWWjegI"
                    },
                    {
                        "slot_id": "v-tvcg-9555234-qa",
                        "session_id": "full20",
                        "type": "In Person Q+A",
                        "title": "DXplorer: A Unified Visualization Framework for Interactive Dendritic Spine Analysis using 3D morphological Features (Q+A)",
                        "contributors": [
                            "Won-Ki Jeong",
                            "JunYoung Choi"
                        ],
                        "authors": [],
                        "abstract": "Dendritic spines are dynamic, submicron-scale protrusions on neuronal dendrites that receive neuronal inputs. Morphological changes in the dendritic spine often reflect alterations in physiological conditions and are indicators of various neuropsychiatric conditions. However, owing to the highly dynamic and heterogeneous nature of spines, accurate measurement and objective analysis of spine morphology are major challenges in neuroscience research. Most conventional approaches for analyzing dendritic spines are based on two-dimensional (2D) images, which barely reflect the actual three-dimensional (3D) shapes. Although some recent studies have attempted to analyze spines with various 3D-based features, it is still difficult to objectively categorize and analyze spines based on 3D morphology. Here, we propose a unified visualization framework for an interactive 3D dendritic spine analysis system, DXplorer, that displays 3D rendering of spines and plots the high-dimensional features extracted from the 3D mesh of spines. With this system, users can perform the clustering of spines interactively and explore and analyze dendritic spines based on high-dimensional features. We propose a series of high-dimensional morphological features extracted from a 3D mesh of dendritic spines. In addition, an interactive machine learning classifier with visual exploration and user feedback using an interactive 3D mesh grid view ensures a more precise classification based on the spine phenotype. A user study and two case studies were conducted to quantitatively verify the performance and usability of the DXplorer. We demonstrate that the system performs the entire analytic process effectively and provides high-quality, accurate, and objective analysis.",
                        "uid": "v-tvcg-9555234",
                        "file_name": "",
                        "time_stamp": "2022-10-20T14:34:00Z",
                        "time_start": "2022-10-20T14:34:00Z",
                        "time_end": "2022-10-20T14:36:00Z",
                        "paper_type": "full",
                        "keywords": [
                            "Biomedical and Medical Visualization, Machine Learning, Task and Requirements Analysis, User Interfaces, Intelligence Analysis"
                        ],
                        "has_image": "1",
                        "has_video": "485",
                        "paper_award": "",
                        "image_caption": "Example of dendritic spine\u2019s 3D mesh with morphological features (upper row) and DXplorer\u2019s user interfaces (bottom row). DXplorer is a unified visual analysis framework for the interactive analysis of dendritic spines using 3D meshes and high-dimensional features. The aim of the proposed system is to provide a unified data processing and visualization workflow for various spine analysis tasks, which will eventually provide insight into the correlation between neuronal functions and the shape of the spine.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "https://youtu.be/Up6eOWWjegI",
                        "ff_id": "Up6eOWWjegI"
                    },
                    {
                        "slot_id": "v-tvcg-9665344-pres",
                        "session_id": "full20",
                        "type": "Virtual Presentation (pre-recorded)",
                        "title": "A Predictive Visual Analytics System for Studying Neurodegenerative Disease Based on DTI Fiber Tracts",
                        "contributors": [
                            "Chaoqing Xu"
                        ],
                        "authors": [
                            "Chaoqing Xu",
                            "Tyson Neuroth",
                            "Takanori Fujiwara",
                            "Ronghua Liang",
                            "Kwan-Liu Ma"
                        ],
                        "abstract": "Diffusion tensor imaging (DTI) has been used to study the effects of neurodegenerative diseases on neural pathways, which may lead to more reliable and early diagnosis of these diseases as well as a better understanding of how they affect the brain. We introduce a predictive visual analytics system for studying patient groups based on their labeled DTI fiber tract data and corresponding statistics. The system\u2019s machine-learning-augmented interface guides the user through an organized and holistic analysis space, including the statistical feature space, the physical space, and the space of patients over different groups. We use a custom machine learning pipeline to help narrow down this large analysis space and then explore it pragmatically through a range of linked visualizations. We conduct several case studies using DTI and T1-weighted images from the research database of Parkinson\u2019s Progression Markers Initiative.",
                        "uid": "v-tvcg-9665344",
                        "file_name": "v-tvcg-9665344_Xu_Presentation.mp4",
                        "time_stamp": "2022-10-20T14:36:00Z",
                        "time_start": "2022-10-20T14:36:00Z",
                        "time_end": "2022-10-20T14:45:25Z",
                        "paper_type": "full",
                        "keywords": [
                            "Brain fiber tracts, neurodegenerative disease, machine learning, predictive visual analytics, visualization."
                        ],
                        "has_image": "1",
                        "has_video": "565",
                        "paper_award": "",
                        "image_caption": "The interface of the brain fiber visualization system. Right-Top is the cohort selection module and machine learning module. Right-Bot shows the outputs of machine learning, which includes three exploration modules: the feature, region, and subject modules. Left-Bot is the information visualization module that includes a range of views for comparative analysis. Left-Mid is the 3D fiber rendering module that shows selected subjects\u2019 fibers for physiological analysis. The selected feature is mapped to the fibers through color. Left-Top is the timeline view of subjects, which can be clicked to render the fibers corresponding to the clicked time.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "https://youtu.be/Ni7g27CvEM0",
                        "ff_id": "Ni7g27CvEM0"
                    },
                    {
                        "slot_id": "v-tvcg-9665344-qa",
                        "session_id": "full20",
                        "type": "Virtual Q+A",
                        "title": "A Predictive Visual Analytics System for Studying Neurodegenerative Disease Based on DTI Fiber Tracts (Q+A)",
                        "contributors": [
                            "Chaoqing Xu"
                        ],
                        "authors": [],
                        "abstract": "Diffusion tensor imaging (DTI) has been used to study the effects of neurodegenerative diseases on neural pathways, which may lead to more reliable and early diagnosis of these diseases as well as a better understanding of how they affect the brain. We introduce a predictive visual analytics system for studying patient groups based on their labeled DTI fiber tract data and corresponding statistics. The system\u2019s machine-learning-augmented interface guides the user through an organized and holistic analysis space, including the statistical feature space, the physical space, and the space of patients over different groups. We use a custom machine learning pipeline to help narrow down this large analysis space and then explore it pragmatically through a range of linked visualizations. We conduct several case studies using DTI and T1-weighted images from the research database of Parkinson\u2019s Progression Markers Initiative.",
                        "uid": "v-tvcg-9665344",
                        "file_name": "",
                        "time_stamp": "2022-10-20T14:45:25Z",
                        "time_start": "2022-10-20T14:45:25Z",
                        "time_end": "2022-10-20T14:48:00Z",
                        "paper_type": "full",
                        "keywords": [
                            "Brain fiber tracts, neurodegenerative disease, machine learning, predictive visual analytics, visualization."
                        ],
                        "has_image": "1",
                        "has_video": "565",
                        "paper_award": "",
                        "image_caption": "The interface of the brain fiber visualization system. Right-Top is the cohort selection module and machine learning module. Right-Bot shows the outputs of machine learning, which includes three exploration modules: the feature, region, and subject modules. Left-Bot is the information visualization module that includes a range of views for comparative analysis. Left-Mid is the 3D fiber rendering module that shows selected subjects\u2019 fibers for physiological analysis. The selected feature is mapped to the fibers through color. Left-Top is the timeline view of subjects, which can be clicked to render the fibers corresponding to the clicked time.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "https://youtu.be/Ni7g27CvEM0",
                        "ff_id": "Ni7g27CvEM0"
                    },
                    {
                        "slot_id": "v-tvcg-9610985-pres",
                        "session_id": "full20",
                        "type": "In Person Presentation",
                        "title": "NeuRegenerate: A Framework for Visualizing Neurodegeneration",
                        "contributors": [
                            "Saeed Boorboor"
                        ],
                        "authors": [
                            "Saeed Boorboor",
                            "Shawn Mathew",
                            "Mala Ananth",
                            "David Talmage",
                            "Lorna W. Role",
                            "Arie E. Kaufman."
                        ],
                        "abstract": "Recent advances in high-resolution microscopy have allowed scientists to better understand the underlying brain connectivity. However, due to the limitation that biological specimens can only be imaged at a single timepoint, studying changes to neural projections over time is limited to observations gathered using population analysis. In this paper, we introduce NeuRegenerate, a novel end-to-end framework for the prediction and visualization of changes in neural fiber morphology within a subject across specified age-timepoints. To predict projections, we present  neuReGANerator, a deep-learning network based on cycle-consistent generative adversarial network (GAN) that translates features of neuronal structures across age-timepoints for large brain microscopy volumes. We improve the reconstruction quality of the predicted neuronal structures by implementing a density multiplier and a new loss function, called the hallucination loss. Moreover, to alleviate artifacts that occur due to tiling of large input volumes, we introduce a spatial-consistency module in the training pipeline of neuReGANerator. Finally, to visualize the change in projections, predicted using neuReGANerator, NeuRegenerate offers two modes: (i) neuroCompare to simultaneously visualize the difference in the structures of the neuronal projections, from two age domains (using structural view and bounded view), and (ii) em neuroMorph, a vesselness-based morphing technique to interactively visualize the transformation of the structures from one age-timepoint to the other. Our framework is designed specifically for volumes acquired using wide-field microscopy. We demonstrate our framework by visualizing the structural changes within the cholinergic system of the mouse brain between a young and old specimen.",
                        "uid": "v-tvcg-9610985",
                        "file_name": "v-tvcg-9610985_Boorboor_Presentation.mp4",
                        "time_stamp": "2022-10-20T14:48:00Z",
                        "time_start": "2022-10-20T14:48:00Z",
                        "time_end": "2022-10-20T14:58:00Z",
                        "paper_type": "full",
                        "keywords": [
                            "Neuron visualization, volume visualization, volume transformation, neuroscience, wide-field microscopy, machine learning,"
                        ],
                        "has_image": "1",
                        "has_video": "592",
                        "paper_award": "",
                        "image_caption": "NeuRegenerate allows neuroscientists to visualize structural changes that occur in an individual specimen's brain, across age: for a diseased mouse data (right-most structure), we are able to predict and reconstruct its healthy neuronal extensions at a younger age (left-most structure). The three in-between structures are generated using our neuroMorph technique that allows users to interactively visualize the neurodegeneration process. The structures in this figure are processed from an input volume imaged using a wide-field microscope, and rendered using our framework's structural mode.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "https://youtu.be/n977DY54QUs",
                        "ff_id": "n977DY54QUs"
                    },
                    {
                        "slot_id": "v-tvcg-9610985-qa",
                        "session_id": "full20",
                        "type": "In Person Q+A",
                        "title": "NeuRegenerate: A Framework for Visualizing Neurodegeneration (Q+A)",
                        "contributors": [
                            "Saeed Boorboor"
                        ],
                        "authors": [],
                        "abstract": "Recent advances in high-resolution microscopy have allowed scientists to better understand the underlying brain connectivity. However, due to the limitation that biological specimens can only be imaged at a single timepoint, studying changes to neural projections over time is limited to observations gathered using population analysis. In this paper, we introduce NeuRegenerate, a novel end-to-end framework for the prediction and visualization of changes in neural fiber morphology within a subject across specified age-timepoints. To predict projections, we present  neuReGANerator, a deep-learning network based on cycle-consistent generative adversarial network (GAN) that translates features of neuronal structures across age-timepoints for large brain microscopy volumes. We improve the reconstruction quality of the predicted neuronal structures by implementing a density multiplier and a new loss function, called the hallucination loss. Moreover, to alleviate artifacts that occur due to tiling of large input volumes, we introduce a spatial-consistency module in the training pipeline of neuReGANerator. Finally, to visualize the change in projections, predicted using neuReGANerator, NeuRegenerate offers two modes: (i) neuroCompare to simultaneously visualize the difference in the structures of the neuronal projections, from two age domains (using structural view and bounded view), and (ii) em neuroMorph, a vesselness-based morphing technique to interactively visualize the transformation of the structures from one age-timepoint to the other. Our framework is designed specifically for volumes acquired using wide-field microscopy. We demonstrate our framework by visualizing the structural changes within the cholinergic system of the mouse brain between a young and old specimen.",
                        "uid": "v-tvcg-9610985",
                        "file_name": "",
                        "time_stamp": "2022-10-20T14:58:00Z",
                        "time_start": "2022-10-20T14:58:00Z",
                        "time_end": "2022-10-20T15:00:00Z",
                        "paper_type": "full",
                        "keywords": [
                            "Neuron visualization, volume visualization, volume transformation, neuroscience, wide-field microscopy, machine learning,"
                        ],
                        "has_image": "1",
                        "has_video": "592",
                        "paper_award": "",
                        "image_caption": "NeuRegenerate allows neuroscientists to visualize structural changes that occur in an individual specimen's brain, across age: for a diseased mouse data (right-most structure), we are able to predict and reconstruct its healthy neuronal extensions at a younger age (left-most structure). The three in-between structures are generated using our neuroMorph technique that allows users to interactively visualize the neurodegeneration process. The structures in this figure are processed from an input volume imaged using a wide-field microscope, and rendered using our framework's structural mode.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "https://youtu.be/n977DY54QUs",
                        "ff_id": "n977DY54QUs"
                    },
                    {
                        "slot_id": "v-tvcg-9645173-pres",
                        "session_id": "full20",
                        "type": "Virtual Presentation (pre-recorded)",
                        "title": "GUCCI - Guided Cardiac Cohort Investigation of Blood Flow Data",
                        "contributors": [
                            "Monique Meuschke"
                        ],
                        "authors": [
                            "Monique Meuschke",
                            "Uli Niemann",
                            "Benjamin Behrendt",
                            "Matthias Gutberlet",
                            "Bernhard Preim",
                            "Kai Lawonn"
                        ],
                        "abstract": "We present the framework GUCCI (Guided Cardiac Cohort Investigation), which provides a guided visual analytics workflow to analyze cohort-based measured blood flow data in the aorta. In the past, many specialized techniques have been developed for the visual exploration of such data sets for a better understanding of the influence of morphological and hemodynamic conditions on cardiovascular diseases. However, there is a lack of dedicated techniques that allow visual comparison of multiple datasets and defined cohorts, which is essential to characterize pathologies. GUCCI offers visual analytics techniques and novel visualization methods to guide the user through the comparison of predefined cohorts, such as healthy volunteers and patients with a pathologically altered aorta.  The combination of overview and glyph-based depictions together with statistical cohort-specific information allows investigating differences and similarities of the time-dependent data. Our framework was evaluated in a qualitative user study with three radiologists specialized in cardiac imaging and two experts in medical blood flow visualization. They were able to discover cohort-specific characteristics, which supports the derivation of standard values as well as the assessment of pathology-related severity and the need for treatment.",
                        "uid": "v-tvcg-9645173",
                        "file_name": "v-tvcg-9645173_Meuschke_Presentation.mp4",
                        "time_stamp": "2022-10-20T15:00:00Z",
                        "time_start": "2022-10-20T15:00:00Z",
                        "time_end": "2022-10-20T15:09:48Z",
                        "paper_type": "full",
                        "keywords": [
                            "Medical Visualization, Cohort Analysis, Measured Blood Flow Data, Cardiac Diseases"
                        ],
                        "has_image": "1",
                        "has_video": "588",
                        "paper_award": "",
                        "image_caption": "Workflow of GUCCI for a cohort-based visual analysis of aortic flow data. First, a subset of relevant features is selected, which serves as\ninput for the cohort definition. Then, visual analysis of the cohorts is supported with different visualization techniques.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "https://youtu.be/W3loBG9YolI",
                        "ff_id": "W3loBG9YolI"
                    },
                    {
                        "slot_id": "v-tvcg-9645173-qa",
                        "session_id": "full20",
                        "type": "Virtual Q+A",
                        "title": "GUCCI - Guided Cardiac Cohort Investigation of Blood Flow Data (Q+A)",
                        "contributors": [
                            "Monique Meuschke"
                        ],
                        "authors": [],
                        "abstract": "We present the framework GUCCI (Guided Cardiac Cohort Investigation), which provides a guided visual analytics workflow to analyze cohort-based measured blood flow data in the aorta. In the past, many specialized techniques have been developed for the visual exploration of such data sets for a better understanding of the influence of morphological and hemodynamic conditions on cardiovascular diseases. However, there is a lack of dedicated techniques that allow visual comparison of multiple datasets and defined cohorts, which is essential to characterize pathologies. GUCCI offers visual analytics techniques and novel visualization methods to guide the user through the comparison of predefined cohorts, such as healthy volunteers and patients with a pathologically altered aorta.  The combination of overview and glyph-based depictions together with statistical cohort-specific information allows investigating differences and similarities of the time-dependent data. Our framework was evaluated in a qualitative user study with three radiologists specialized in cardiac imaging and two experts in medical blood flow visualization. They were able to discover cohort-specific characteristics, which supports the derivation of standard values as well as the assessment of pathology-related severity and the need for treatment.",
                        "uid": "v-tvcg-9645173",
                        "file_name": "",
                        "time_stamp": "2022-10-20T15:09:48Z",
                        "time_start": "2022-10-20T15:09:48Z",
                        "time_end": "2022-10-20T15:12:00Z",
                        "paper_type": "full",
                        "keywords": [
                            "Medical Visualization, Cohort Analysis, Measured Blood Flow Data, Cardiac Diseases"
                        ],
                        "has_image": "1",
                        "has_video": "588",
                        "paper_award": "",
                        "image_caption": "Workflow of GUCCI for a cohort-based visual analysis of aortic flow data. First, a subset of relevant features is selected, which serves as\ninput for the cohort definition. Then, visual analysis of the cohorts is supported with different visualization techniques.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "https://youtu.be/W3loBG9YolI",
                        "ff_id": "W3loBG9YolI"
                    }
                ]
            },
            {
                "title": "Temporal Data",
                "session_id": "full21",
                "event_prefix": "v-full",
                "track": "ok6",
                "livestream_id": "ok6-wed",
                "session_image": "full21.png",
                "chair": [
                    "Wolfgang Aigner"
                ],
                "organizers": [],
                "time_start": "2022-10-19T14:00:00Z",
                "time_end": "2022-10-19T15:15:00Z",
                "discord_category": "",
                "discord_channel": "oklahoma-st-6",
                "discord_channel_id": "1024600906689421372",
                "discord_link": "https://discord.com/channels/978320435554947082/1024600906689421372",
                "slido_link": "https://app.sli.do/event/i28UETZaiRWr1XwX6eKjcf?section=29ba1f79-04cd-4568-a83c-ab81aa3c28b8",
                "youtube_url": "https://youtu.be/L523gBLIM5c",
                "youtube_id": "L523gBLIM5c",
                "zoom_meeting": "",
                "zoom_password": "",
                "zoom_link": "",
                "ff_link": "https://youtu.be/7oSZnEBT5I8",
                "ff_playlist": "",
                "time_slots": [
                    {
                        "slot_id": "full21-opening",
                        "session_id": "full21",
                        "type": "In Person Other",
                        "title": "Session Opening",
                        "contributors": [
                            "Wolfgang Aigner"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "",
                        "file_name": "",
                        "time_stamp": "2022-10-19T14:00:00Z",
                        "time_start": "2022-10-19T14:00:00Z",
                        "time_end": "2022-10-19T14:00:00Z",
                        "paper_type": "",
                        "keywords": [],
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "v-full-1380-pres",
                        "session_id": "full21",
                        "type": "Virtual Presentation (pre-recorded)",
                        "title": "Visualizing the Passage of Time with Video Temporal Pyramids",
                        "contributors": [
                            "Melissa E Swift",
                            "Melissa Swift"
                        ],
                        "authors": [
                            "Melissa E Swift",
                            "Wyatt Ayers",
                            "Sophie Pallanck",
                            "Scott Wehrwein"
                        ],
                        "abstract": "What can we learn about a scene by watching it for months or years? A video recorded over a long timespan will depict interesting phenomena at multiple timescales, but identifying and viewing them presents a challenge. The video is too long to watch in full, and some things are too slow to experience in real-time, such as glacial retreat or the gradual shift from summer to fall. Timelapse videography is a common approach to summarizing long videos and visualizing slow timescales. However, a timelapse is limited to a single chosen temporal frequency, and often appears flickery due to aliasing. Also, the length of the timelapse video is directly tied to its temporal resolution, which necessitates tradeoffs between those two facets. In this paper, we propose Video Temporal Pyramids, a technique that addresses these limitations and expands the possibilities for visualizing the passage of time. Inspired by spatial image pyramids from computer vision, we developed an algorithm that builds video pyramids in the temporal domain. Each level of a Video Temporal Pyramid visualizes a different timescale; for instance, videos from the monthly timescale are usually good for visualizing seasonal changes, while videos from the one-minute timescale are best for visualizing sunrise or the movement of clouds across the sky. To help explore the different pyramid levels, we also propose a Video Spectrogram to visualize the amount of activity across the entire pyramid, providing a holistic overview of the scene dynamics and the ability to explore and discover phenomena across time and timescales. To demonstrate our approach, we have built Video Temporal Pyramids from ten outdoor scenes, each containing months or years of data. We compare Video Temporal Pyramid layers to naive timelapse and find that our pyramids enable alias-free viewing of longer-term changes. We also demonstrate that the Video Spectrogram facilitates exploration and discovery of phenomena across pyramid levels, by enabling both overview and detail-focused perspectives.",
                        "uid": "v-full-1380",
                        "file_name": "v-full-1380_Swift_Presentation.mp4",
                        "time_stamp": "2022-10-19T14:00:00Z",
                        "time_start": "2022-10-19T14:00:00Z",
                        "time_end": "2022-10-19T14:08:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "480",
                        "paper_award": "",
                        "image_caption": "Given months or years of video footage from a fixed scene (such as a recorded webcam), our approach builds a Video Temporal Pyramid consisting of different length shorter videos, each distilling the events from a particular timescale such as seasonal changes at the 90-day timescale and human activity at the shorter 1-second timescale. Our videos are an improvement over standard timelapse videos because they avoid aliasing and temporal discontinuities and can be upsampled for a smooth viewing experience. Additionally, our Video Spectrogram tool visualizes the whole pyramid in order to help with navigation and discovery of patterns and anomalies.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "https://youtu.be/0YdE0CU2cEw",
                        "ff_id": "0YdE0CU2cEw"
                    },
                    {
                        "slot_id": "v-full-1380-qa",
                        "session_id": "full21",
                        "type": "Virtual Q+A",
                        "title": "Visualizing the Passage of Time with Video Temporal Pyramids (Q+A)",
                        "contributors": [
                            "Melissa E Swift",
                            "Melissa Swift"
                        ],
                        "authors": [],
                        "abstract": "What can we learn about a scene by watching it for months or years? A video recorded over a long timespan will depict interesting phenomena at multiple timescales, but identifying and viewing them presents a challenge. The video is too long to watch in full, and some things are too slow to experience in real-time, such as glacial retreat or the gradual shift from summer to fall. Timelapse videography is a common approach to summarizing long videos and visualizing slow timescales. However, a timelapse is limited to a single chosen temporal frequency, and often appears flickery due to aliasing. Also, the length of the timelapse video is directly tied to its temporal resolution, which necessitates tradeoffs between those two facets. In this paper, we propose Video Temporal Pyramids, a technique that addresses these limitations and expands the possibilities for visualizing the passage of time. Inspired by spatial image pyramids from computer vision, we developed an algorithm that builds video pyramids in the temporal domain. Each level of a Video Temporal Pyramid visualizes a different timescale; for instance, videos from the monthly timescale are usually good for visualizing seasonal changes, while videos from the one-minute timescale are best for visualizing sunrise or the movement of clouds across the sky. To help explore the different pyramid levels, we also propose a Video Spectrogram to visualize the amount of activity across the entire pyramid, providing a holistic overview of the scene dynamics and the ability to explore and discover phenomena across time and timescales. To demonstrate our approach, we have built Video Temporal Pyramids from ten outdoor scenes, each containing months or years of data. We compare Video Temporal Pyramid layers to naive timelapse and find that our pyramids enable alias-free viewing of longer-term changes. We also demonstrate that the Video Spectrogram facilitates exploration and discovery of phenomena across pyramid levels, by enabling both overview and detail-focused perspectives.",
                        "uid": "v-full-1380",
                        "file_name": "",
                        "time_stamp": "2022-10-19T14:08:00Z",
                        "time_start": "2022-10-19T14:08:00Z",
                        "time_end": "2022-10-19T14:12:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "480",
                        "paper_award": "",
                        "image_caption": "Given months or years of video footage from a fixed scene (such as a recorded webcam), our approach builds a Video Temporal Pyramid consisting of different length shorter videos, each distilling the events from a particular timescale such as seasonal changes at the 90-day timescale and human activity at the shorter 1-second timescale. Our videos are an improvement over standard timelapse videos because they avoid aliasing and temporal discontinuities and can be upsampled for a smooth viewing experience. Additionally, our Video Spectrogram tool visualizes the whole pyramid in order to help with navigation and discovery of patterns and anomalies.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "https://youtu.be/0YdE0CU2cEw",
                        "ff_id": "0YdE0CU2cEw"
                    },
                    {
                        "slot_id": "v-full-1289-pres",
                        "session_id": "full21",
                        "type": "In Person Presentation",
                        "title": "Constrained Dynamic Mode Decomposition",
                        "contributors": [
                            "Tim Krake"
                        ],
                        "authors": [
                            "Tim Krake",
                            "Daniel Kl\u00f6tzl",
                            "Bernhard Eberhardt",
                            "Daniel Weiskopf"
                        ],
                        "abstract": "Frequency-based decomposition of time series data is used in many visualization applications. Most of these decomposition methods (such as Fourier transform or singular spectrum analysis) only provide interaction via pre- and post-processing, but no means to influence the core algorithm. A method that also belongs to this class is Dynamic Mode Decomposition (DMD), a spectral decomposition method that extracts spatio-temporal patterns from data. In this paper, we incorporate frequency-based constraints into DMD for an adaptive decomposition that leads to user-controllable visualizations, allowing analysts to include their knowledge into the process. To accomplish this, we derive an equivalent reformulation of DMD that implicitly provides access to the eigenvalues (and therefore to the frequencies) identified by DMD. By utilizing a constrained minimization problem customized to DMD, we can guarantee the existence of desired frequencies by minimal changes to DMD. We complement this core approach by additional techniques for constrained DMD to facilitate explorative visualization and investigation of time series data. With several examples, we demonstrate the usefulness of constrained DMD and compare it to conventional frequency-based decomposition methods.",
                        "uid": "v-full-1289",
                        "file_name": "v-full-1289_Krake_Presentation.mp4",
                        "time_stamp": "2022-10-19T14:12:00Z",
                        "time_start": "2022-10-19T14:12:00Z",
                        "time_end": "2022-10-19T14:22:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "480",
                        "paper_award": "",
                        "image_caption": "The decomposition of time series into seasonal and trend patterns can be performed with Dynamic Mode Decomposition. The extracted DMD components capture these patterns only in an approximate fashion, although knowledge about time scales or specific periodicities is often present. Our technique, constrained Dynamic Mode Decomposition, enables incorporation of user-defined frequencies into DMD, which makes an explorative investigation of time series possible.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "https://youtu.be/vqahHdOPkzM",
                        "ff_id": "vqahHdOPkzM"
                    },
                    {
                        "slot_id": "v-full-1289-qa",
                        "session_id": "full21",
                        "type": "In Person Q+A",
                        "title": "Constrained Dynamic Mode Decomposition (Q+A)",
                        "contributors": [
                            "Tim Krake"
                        ],
                        "authors": [],
                        "abstract": "Frequency-based decomposition of time series data is used in many visualization applications. Most of these decomposition methods (such as Fourier transform or singular spectrum analysis) only provide interaction via pre- and post-processing, but no means to influence the core algorithm. A method that also belongs to this class is Dynamic Mode Decomposition (DMD), a spectral decomposition method that extracts spatio-temporal patterns from data. In this paper, we incorporate frequency-based constraints into DMD for an adaptive decomposition that leads to user-controllable visualizations, allowing analysts to include their knowledge into the process. To accomplish this, we derive an equivalent reformulation of DMD that implicitly provides access to the eigenvalues (and therefore to the frequencies) identified by DMD. By utilizing a constrained minimization problem customized to DMD, we can guarantee the existence of desired frequencies by minimal changes to DMD. We complement this core approach by additional techniques for constrained DMD to facilitate explorative visualization and investigation of time series data. With several examples, we demonstrate the usefulness of constrained DMD and compare it to conventional frequency-based decomposition methods.",
                        "uid": "v-full-1289",
                        "file_name": "",
                        "time_stamp": "2022-10-19T14:22:00Z",
                        "time_start": "2022-10-19T14:22:00Z",
                        "time_end": "2022-10-19T14:24:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "480",
                        "paper_award": "",
                        "image_caption": "The decomposition of time series into seasonal and trend patterns can be performed with Dynamic Mode Decomposition. The extracted DMD components capture these patterns only in an approximate fashion, although knowledge about time scales or specific periodicities is often present. Our technique, constrained Dynamic Mode Decomposition, enables incorporation of user-defined frequencies into DMD, which makes an explorative investigation of time series possible.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "https://youtu.be/vqahHdOPkzM",
                        "ff_id": "vqahHdOPkzM"
                    },
                    {
                        "slot_id": "v-full-1163-pres",
                        "session_id": "full21",
                        "type": "Virtual Presentation (pre-recorded)",
                        "title": "SizePairs: Achieving Stable and Balanced Temporal Treemaps using Hierarchical Size-based Pairing",
                        "contributors": [
                            "Chang Han"
                        ],
                        "authors": [
                            "Chang Han",
                            "Anyi Li",
                            "Jaemin Jo",
                            "Bongshin Lee",
                            "Oliver Deussen",
                            "Yunhai Wang"
                        ],
                        "abstract": "We present SizePairs, a new technique to create stable and balanced treemap layouts that visualize values changing over time in hierarchical data. To achieve an overall high-quality result across all time steps in terms of stability and aspect ratio, SizePairs employs a new hierarchical size-based pairing algorithm that recursively pairs two nodes that complement their size changes over time and have similar sizes. SizePairs maximizes the visual quality and stability by optimizing the splitting orientation of each internal node and flipping leaf nodes, if necessary. We also present a comprehensive comparison of SizePairs against the state-of-the-art treemaps developed for visualizing time-dependent data. SizePairs outperforms existing techniques in both visual quality and stability, while being faster than the local moves technique.",
                        "uid": "v-full-1163",
                        "file_name": "v-full-1163_Han_Presentation.mp4",
                        "time_stamp": "2022-10-19T14:24:00Z",
                        "time_start": "2022-10-19T14:24:00Z",
                        "time_end": "2022-10-19T14:33:33Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "573",
                        "paper_award": "",
                        "image_caption": "Comparing different temporal treemap methods using three time steps (t = 0, t = 10, and t = 20) of the WorldBankHIV data: (a) Hilbert treemap method; (b) Local moves method; (c) Greedy insertion treemaps. (d) SizePairs; and (e) SizePairs with local moves. To reflect the stability of each method, we assign colors to each rectangle at the first time step and then apply the color scheme to the other time steps. SizePairs is more stable and maintains better aspect ratios, and SizePairs in combination with local moves (e) has even better aspect ratios than SizePairs alone (d).",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "https://youtu.be/UZyjwbsri6c",
                        "ff_id": "UZyjwbsri6c"
                    },
                    {
                        "slot_id": "v-full-1163-qa",
                        "session_id": "full21",
                        "type": "Virtual Q+A",
                        "title": "SizePairs: Achieving Stable and Balanced Temporal Treemaps using Hierarchical Size-based Pairing (Q+A)",
                        "contributors": [
                            "Chang Han"
                        ],
                        "authors": [],
                        "abstract": "We present SizePairs, a new technique to create stable and balanced treemap layouts that visualize values changing over time in hierarchical data. To achieve an overall high-quality result across all time steps in terms of stability and aspect ratio, SizePairs employs a new hierarchical size-based pairing algorithm that recursively pairs two nodes that complement their size changes over time and have similar sizes. SizePairs maximizes the visual quality and stability by optimizing the splitting orientation of each internal node and flipping leaf nodes, if necessary. We also present a comprehensive comparison of SizePairs against the state-of-the-art treemaps developed for visualizing time-dependent data. SizePairs outperforms existing techniques in both visual quality and stability, while being faster than the local moves technique.",
                        "uid": "v-full-1163",
                        "file_name": "",
                        "time_stamp": "2022-10-19T14:33:33Z",
                        "time_start": "2022-10-19T14:33:33Z",
                        "time_end": "2022-10-19T14:36:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "573",
                        "paper_award": "",
                        "image_caption": "Comparing different temporal treemap methods using three time steps (t = 0, t = 10, and t = 20) of the WorldBankHIV data: (a) Hilbert treemap method; (b) Local moves method; (c) Greedy insertion treemaps. (d) SizePairs; and (e) SizePairs with local moves. To reflect the stability of each method, we assign colors to each rectangle at the first time step and then apply the color scheme to the other time steps. SizePairs is more stable and maintains better aspect ratios, and SizePairs in combination with local moves (e) has even better aspect ratios than SizePairs alone (d).",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "https://youtu.be/UZyjwbsri6c",
                        "ff_id": "UZyjwbsri6c"
                    },
                    {
                        "slot_id": "v-full-1230-pres",
                        "session_id": "full21",
                        "type": "In Person Presentation",
                        "title": "LargeNetVis: visual exploration of large temporal networks based on community taxonomies",
                        "contributors": [
                            "Claudio Linhares",
                            "Jean Ponciano"
                        ],
                        "authors": [
                            "Claudio Linhares",
                            "Jean Roberto Ponciano",
                            "Diogenes Pedro",
                            "Luis Rocha",
                            "Agma Traina",
                            "Jorge Poco"
                        ],
                        "abstract": "Temporal (or time-evolving) networks are commonly used to model complex systems and the evolution of their components throughout time.\n Although these networks can be analyzed by different means, visual analytics stands out as an effective way for a pre-analysis before doing quantitative/statistical analyses to identify patterns, anomalies, and other behaviors in the data, thus leading to new insights and better decision-making.\n However, the large number of nodes, edges, and/or timestamps in many real-world networks may lead to polluted layouts that make the analysis inefficient or even infeasible.\n In this paper, we propose LargeNetVis, a web-based visual analytics system designed to assist in analyzing small and large temporal networks. It successfully achieves this goal by leveraging three taxonomies focused on network communities to guide the visual exploration process.\n The system is composed of four interactive visual components: the first (Taxonomy Matrix) presents a summary of the network characteristics, the second (Global View) gives an overview of the network evolution, the third (a node-link diagram) enables community- and node-level structural analysis, and the fourth (a Temporal Activity Map -- TAM) shows the community- and node-level activity under a temporal perspective.\n We demonstrate the usefulness and effectiveness of LargeNetVis through two usage scenarios and a user study with 14 participants.",
                        "uid": "v-full-1230",
                        "file_name": "v-full-1230_Linhares_Presentation.mp4",
                        "time_stamp": "2022-10-19T14:36:00Z",
                        "time_start": "2022-10-19T14:36:00Z",
                        "time_end": "2022-10-19T14:46:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "576",
                        "paper_award": "",
                        "image_caption": "LargeNetVis is a web-based visual analytics system designed to support the visual exploration of temporal networks with up to a few thousand nodes and timestamps. It enhances the analysis by leveraging the network community structure and three taxonomies. LargeNetVis contains four linked views. While the Taxonomy Matrix (A) and Global View (B) enable global-level analysis, the node-link diagram (C) and the Temporal Activity Map (D) enable structural and temporal local-level analysis, respectively. The system also provides a panel with the network, community, or node numerical information (E). We demonstrate usefulness through usage scenarios and a user study with 14 participants.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "https://youtu.be/TTkGjBBTOwE",
                        "ff_id": "TTkGjBBTOwE"
                    },
                    {
                        "slot_id": "v-full-1230-qa",
                        "session_id": "full21",
                        "type": "In Person Q+A",
                        "title": "LargeNetVis: visual exploration of large temporal networks based on community taxonomies (Q+A)",
                        "contributors": [
                            "Claudio Linhares",
                            "Jean Ponciano"
                        ],
                        "authors": [],
                        "abstract": "Temporal (or time-evolving) networks are commonly used to model complex systems and the evolution of their components throughout time.\n Although these networks can be analyzed by different means, visual analytics stands out as an effective way for a pre-analysis before doing quantitative/statistical analyses to identify patterns, anomalies, and other behaviors in the data, thus leading to new insights and better decision-making.\n However, the large number of nodes, edges, and/or timestamps in many real-world networks may lead to polluted layouts that make the analysis inefficient or even infeasible.\n In this paper, we propose LargeNetVis, a web-based visual analytics system designed to assist in analyzing small and large temporal networks. It successfully achieves this goal by leveraging three taxonomies focused on network communities to guide the visual exploration process.\n The system is composed of four interactive visual components: the first (Taxonomy Matrix) presents a summary of the network characteristics, the second (Global View) gives an overview of the network evolution, the third (a node-link diagram) enables community- and node-level structural analysis, and the fourth (a Temporal Activity Map -- TAM) shows the community- and node-level activity under a temporal perspective.\n We demonstrate the usefulness and effectiveness of LargeNetVis through two usage scenarios and a user study with 14 participants.",
                        "uid": "v-full-1230",
                        "file_name": "",
                        "time_stamp": "2022-10-19T14:46:00Z",
                        "time_start": "2022-10-19T14:46:00Z",
                        "time_end": "2022-10-19T14:48:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "576",
                        "paper_award": "",
                        "image_caption": "LargeNetVis is a web-based visual analytics system designed to support the visual exploration of temporal networks with up to a few thousand nodes and timestamps. It enhances the analysis by leveraging the network community structure and three taxonomies. LargeNetVis contains four linked views. While the Taxonomy Matrix (A) and Global View (B) enable global-level analysis, the node-link diagram (C) and the Temporal Activity Map (D) enable structural and temporal local-level analysis, respectively. The system also provides a panel with the network, community, or node numerical information (E). We demonstrate usefulness through usage scenarios and a user study with 14 participants.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "https://youtu.be/TTkGjBBTOwE",
                        "ff_id": "TTkGjBBTOwE"
                    },
                    {
                        "slot_id": "v-full-1069-pres",
                        "session_id": "full21",
                        "type": "Virtual Presentation (pre-recorded)",
                        "title": "ASTF: Visual Abstractions of Time-Varying Patterns in Radio Signals",
                        "contributors": [
                            "Ying Zhao"
                        ],
                        "authors": [
                            "Ying Zhao",
                            "Luhao Ge",
                            "Huixuan Xie",
                            "Genghuai Bai",
                            "Zhao Zhang",
                            "Qiang Wei",
                            "Yun Lin",
                            "Yuchao Liu",
                            "Fangfang Zhou"
                        ],
                        "abstract": "A time-frequency diagram is a commonly used visualization for observing the time-frequency distribution of radio signals and analyzing their time-varying patterns of communication states in radio monitoring and management. While it excels when performing short-term signal analyses, it becomes inadaptable for long-term signal analyses because it cannot adequately depict signal time-varying patterns in a large time span on a space-limited screen. This research thus presents an abstract signal time-frequency (ASTF) diagram to address this problem. In the diagram design, a visual abstraction method is proposed to visually encode signal communication state changes in time slices. A time segmentation algorithm is proposed to divide a large time span into time slices. Three new quantified metrics and a loss function are defined to ensure the preservation of important time-varying information in the time segmentation. An algorithm performance experiment and a user study are conducted to evaluate the effectiveness of the diagram for long-term signal analyses.",
                        "uid": "v-full-1069",
                        "file_name": "v-full-1069_Zhao_Presentation.mp4",
                        "time_stamp": "2022-10-19T14:48:00Z",
                        "time_start": "2022-10-19T14:48:00Z",
                        "time_end": "2022-10-19T14:58:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "600",
                        "paper_award": "",
                        "image_caption": "Visual abstractions of time-varying patterns in radio signals using abstracted signal time-frequency (ASTF) diagram.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "https://youtu.be/z16ClvSo8lQ",
                        "ff_id": "z16ClvSo8lQ"
                    },
                    {
                        "slot_id": "v-full-1069-qa",
                        "session_id": "full21",
                        "type": "Virtual Q+A",
                        "title": "ASTF: Visual Abstractions of Time-Varying Patterns in Radio Signals (Q+A)",
                        "contributors": [
                            "Ying Zhao"
                        ],
                        "authors": [],
                        "abstract": "A time-frequency diagram is a commonly used visualization for observing the time-frequency distribution of radio signals and analyzing their time-varying patterns of communication states in radio monitoring and management. While it excels when performing short-term signal analyses, it becomes inadaptable for long-term signal analyses because it cannot adequately depict signal time-varying patterns in a large time span on a space-limited screen. This research thus presents an abstract signal time-frequency (ASTF) diagram to address this problem. In the diagram design, a visual abstraction method is proposed to visually encode signal communication state changes in time slices. A time segmentation algorithm is proposed to divide a large time span into time slices. Three new quantified metrics and a loss function are defined to ensure the preservation of important time-varying information in the time segmentation. An algorithm performance experiment and a user study are conducted to evaluate the effectiveness of the diagram for long-term signal analyses.",
                        "uid": "v-full-1069",
                        "file_name": "",
                        "time_stamp": "2022-10-19T14:58:00Z",
                        "time_start": "2022-10-19T14:58:00Z",
                        "time_end": "2022-10-19T15:00:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "600",
                        "paper_award": "",
                        "image_caption": "Visual abstractions of time-varying patterns in radio signals using abstracted signal time-frequency (ASTF) diagram.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "https://youtu.be/z16ClvSo8lQ",
                        "ff_id": "z16ClvSo8lQ"
                    }
                ]
            },
            {
                "title": "Comparisons",
                "session_id": "full22",
                "event_prefix": "v-full",
                "track": "ok1",
                "livestream_id": "ok1-fri",
                "session_image": "full22.png",
                "chair": [
                    "Peter Lindstrom"
                ],
                "organizers": [],
                "time_start": "2022-10-21T14:00:00Z",
                "time_end": "2022-10-21T15:15:00Z",
                "discord_category": "",
                "discord_channel": "oklahoma-st-1",
                "discord_channel_id": "1024600861340606484",
                "discord_link": "https://discord.com/channels/978320435554947082/1024600861340606484",
                "slido_link": "https://app.sli.do/event/i28UETZaiRWr1XwX6eKjcf?section=740d9835-fe47-4cb4-b260-353262a7f8dd",
                "youtube_url": "https://youtu.be/5A_zrS9dpWk",
                "youtube_id": "5A_zrS9dpWk",
                "zoom_meeting": "",
                "zoom_password": "",
                "zoom_link": "",
                "ff_link": "https://youtu.be/UMSbz4zYlV0",
                "ff_playlist": "",
                "time_slots": [
                    {
                        "slot_id": "full22-opening",
                        "session_id": "full22",
                        "type": "In Person Other",
                        "title": "Session Opening",
                        "contributors": [
                            "Peter Lindstrom"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "",
                        "file_name": "",
                        "time_stamp": "2022-10-21T14:00:00Z",
                        "time_start": "2022-10-21T14:00:00Z",
                        "time_end": "2022-10-21T14:00:00Z",
                        "paper_type": "",
                        "keywords": [],
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "v-tvcg-9744472-pres",
                        "session_id": "full22",
                        "type": "In Person Presentation",
                        "title": "Geometry-Aware Merge Tree Comparisons for Time-Varying Data with Interleaving Distances",
                        "contributors": [
                            "Lin Yan"
                        ],
                        "authors": [
                            "Lin Yan",
                            "Talha Bin Masood",
                            "Farhan Rasheed",
                            "Ingrid Hotz",
                            "Bei Wang"
                        ],
                        "abstract": "Merge trees, a type of topological descriptor, serve to identify and summarize the topological characteristics associated with scalar fields. They have great potential for analyzing and visualizing time-varying data. First, they give compressed and topology-preserving representations of data instances. Second, their comparisons provide a basis for studying the relations among data instances, such as their distributions, clusters, outliers, and periodicities. A number of comparative measures have been developed for merge trees. However, these measures are often computationally expensive since they implicitly consider all possible correspondences between critical points of the merge trees. In this paper, we perform geometry-aware comparisons of merge trees using labeled interleaving distances. The main idea is to decouple the computation of a comparative measure into two steps: a labeling step that generates a correspondence between the critical points of two merge trees, and a comparison step that computes distances between a pair of labeled merge trees by encoding them as matrices. We show that our approach is general, computationally efficient, and practically useful. Our framework makes it possible to integrate geometric information of the data domain in the labeling process. At the same time, the framework reduces the computational complexity since not all possible correspondences have to be considered. We demonstrate via experiments that such geometry-aware merge tree comparisons help to detect transitions, clusters, and periodicities of time-varying datasets, as well as to diagnose and highlight the topological changes between adjacent data instances.",
                        "uid": "v-tvcg-9744472",
                        "file_name": "v-tvcg-9744472_Yan_Presentation.mp4",
                        "time_stamp": "2022-10-21T14:00:00Z",
                        "time_start": "2022-10-21T14:00:00Z",
                        "time_end": "2022-10-21T14:10:00Z",
                        "paper_type": "full",
                        "keywords": [
                            "Merge trees, merge tree metrics, topological data analysis, topology in visualization"
                        ],
                        "has_image": "1",
                        "has_video": "585",
                        "paper_award": "",
                        "image_caption": "In this paper, we perform geometry-aware comparisons of merge trees using labeled interleaving distances. The main idea is to decouple the computation of a comparative measure into two steps: a labeling step that generates a correspondence between the critical points of two merge trees, and a comparison step that computes distances between a pair of labeled merge trees by encoding them as matrices. We demonstrate via experiments that such geometry-aware merge tree comparisons help to detect transitions, clusters, and periodicities of time-varying datasets.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "https://youtu.be/YFvFBQ42VjU",
                        "ff_id": "YFvFBQ42VjU"
                    },
                    {
                        "slot_id": "v-tvcg-9744472-qa",
                        "session_id": "full22",
                        "type": "In Person Q+A",
                        "title": "Geometry-Aware Merge Tree Comparisons for Time-Varying Data with Interleaving Distances (Q+A)",
                        "contributors": [
                            "Lin Yan"
                        ],
                        "authors": [],
                        "abstract": "Merge trees, a type of topological descriptor, serve to identify and summarize the topological characteristics associated with scalar fields. They have great potential for analyzing and visualizing time-varying data. First, they give compressed and topology-preserving representations of data instances. Second, their comparisons provide a basis for studying the relations among data instances, such as their distributions, clusters, outliers, and periodicities. A number of comparative measures have been developed for merge trees. However, these measures are often computationally expensive since they implicitly consider all possible correspondences between critical points of the merge trees. In this paper, we perform geometry-aware comparisons of merge trees using labeled interleaving distances. The main idea is to decouple the computation of a comparative measure into two steps: a labeling step that generates a correspondence between the critical points of two merge trees, and a comparison step that computes distances between a pair of labeled merge trees by encoding them as matrices. We show that our approach is general, computationally efficient, and practically useful. Our framework makes it possible to integrate geometric information of the data domain in the labeling process. At the same time, the framework reduces the computational complexity since not all possible correspondences have to be considered. We demonstrate via experiments that such geometry-aware merge tree comparisons help to detect transitions, clusters, and periodicities of time-varying datasets, as well as to diagnose and highlight the topological changes between adjacent data instances.",
                        "uid": "v-tvcg-9744472",
                        "file_name": "",
                        "time_stamp": "2022-10-21T14:10:00Z",
                        "time_start": "2022-10-21T14:10:00Z",
                        "time_end": "2022-10-21T14:12:00Z",
                        "paper_type": "full",
                        "keywords": [
                            "Merge trees, merge tree metrics, topological data analysis, topology in visualization"
                        ],
                        "has_image": "1",
                        "has_video": "585",
                        "paper_award": "",
                        "image_caption": "In this paper, we perform geometry-aware comparisons of merge trees using labeled interleaving distances. The main idea is to decouple the computation of a comparative measure into two steps: a labeling step that generates a correspondence between the critical points of two merge trees, and a comparison step that computes distances between a pair of labeled merge trees by encoding them as matrices. We demonstrate via experiments that such geometry-aware merge tree comparisons help to detect transitions, clusters, and periodicities of time-varying datasets.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "https://youtu.be/YFvFBQ42VjU",
                        "ff_id": "YFvFBQ42VjU"
                    },
                    {
                        "slot_id": "v-tvcg-9468958-pres",
                        "session_id": "full22",
                        "type": "In Person Presentation",
                        "title": "Interpretable Anomaly Detection in Event Sequences via Sequence Matching and Visual Comparison",
                        "contributors": [
                            "Shunan Guo"
                        ],
                        "authors": [
                            "Shunan Guo",
                            "Zhuochen Jin",
                            "Qing Chen",
                            "David Gotz",
                            "Hongyuan Zha",
                            "Nan Cao"
                        ],
                        "abstract": "Anomaly detection is a common analytical task that aims to identify rare cases that differ from the typical cases that make up the majority of a dataset. When analyzing event sequence data, the task of anomaly detection can be complex because the sequential and temporal nature of such data results in diverse definitions and flexible forms of anomalies. This, in turn, increases the difficulty in interpreting detected anomalies. In this paper, we propose a visual analytic approach for detecting anomalous sequences in an event sequence dataset via an unsupervised anomaly detection algorithm based on Variational AutoEncoders. We further compare the anomalous sequences with their reconstructions and with the normal sequences through a sequence matching algorithm to identify event anomalies. A visual analytics system is developed to support interactive exploration and interpretations of anomalies through novel visualization designs that facilitate the comparison between anomalous sequences and normal sequences. Finally, we quantitatively evaluate the performance of our anomaly detection algorithm, demonstrate the effectiveness of our system through case studies, and report feedback collected from study participants.",
                        "uid": "v-tvcg-9468958",
                        "file_name": "v-tvcg-9468958_Guo_Presentation.mp4",
                        "time_stamp": "2022-10-21T14:12:00Z",
                        "time_start": "2022-10-21T14:12:00Z",
                        "time_end": "2022-10-21T14:22:00Z",
                        "paper_type": "full",
                        "keywords": [
                            "Anomaly detection, Data models, Data visualizations, Task analysis, Sequences, Heart, Diabetes"
                        ],
                        "has_image": "1",
                        "has_video": "670",
                        "paper_award": "",
                        "image_caption": "The user interface of the visual analytics system, which consists of seven key views to support comparison-based visual anomaly detection: (1) anomaly overview, (2) similarity view, (3) reconstruction view, (4) anomalous sequence view, (5) normal sequence view with two variants(5A, 5B) in cluster mode and sequence mode, respectively, (6) anomalous record view and (7) similar record list.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "https://youtu.be/_xqMGqbVy78",
                        "ff_id": "_xqMGqbVy78"
                    },
                    {
                        "slot_id": "v-tvcg-9468958-qa",
                        "session_id": "full22",
                        "type": "In Person Q+A",
                        "title": "Interpretable Anomaly Detection in Event Sequences via Sequence Matching and Visual Comparison (Q+A)",
                        "contributors": [
                            "Shunan Guo"
                        ],
                        "authors": [],
                        "abstract": "Anomaly detection is a common analytical task that aims to identify rare cases that differ from the typical cases that make up the majority of a dataset. When analyzing event sequence data, the task of anomaly detection can be complex because the sequential and temporal nature of such data results in diverse definitions and flexible forms of anomalies. This, in turn, increases the difficulty in interpreting detected anomalies. In this paper, we propose a visual analytic approach for detecting anomalous sequences in an event sequence dataset via an unsupervised anomaly detection algorithm based on Variational AutoEncoders. We further compare the anomalous sequences with their reconstructions and with the normal sequences through a sequence matching algorithm to identify event anomalies. A visual analytics system is developed to support interactive exploration and interpretations of anomalies through novel visualization designs that facilitate the comparison between anomalous sequences and normal sequences. Finally, we quantitatively evaluate the performance of our anomaly detection algorithm, demonstrate the effectiveness of our system through case studies, and report feedback collected from study participants.",
                        "uid": "v-tvcg-9468958",
                        "file_name": "",
                        "time_stamp": "2022-10-21T14:22:00Z",
                        "time_start": "2022-10-21T14:22:00Z",
                        "time_end": "2022-10-21T14:24:00Z",
                        "paper_type": "full",
                        "keywords": [
                            "Anomaly detection, Data models, Data visualizations, Task analysis, Sequences, Heart, Diabetes"
                        ],
                        "has_image": "1",
                        "has_video": "670",
                        "paper_award": "",
                        "image_caption": "The user interface of the visual analytics system, which consists of seven key views to support comparison-based visual anomaly detection: (1) anomaly overview, (2) similarity view, (3) reconstruction view, (4) anomalous sequence view, (5) normal sequence view with two variants(5A, 5B) in cluster mode and sequence mode, respectively, (6) anomalous record view and (7) similar record list.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "https://youtu.be/_xqMGqbVy78",
                        "ff_id": "_xqMGqbVy78"
                    },
                    {
                        "slot_id": "v-tvcg-9585392-pres",
                        "session_id": "full22",
                        "type": "In Person Presentation",
                        "title": "Comparative Analysis of Merge Trees using Local Tree Edit Distance",
                        "contributors": [
                            "Raghavendra Sridharamurthy"
                        ],
                        "authors": [
                            "Raghavendra Sridharamurthy",
                            "Vijay Natarajan"
                        ],
                        "abstract": "Comparative analysis of scalar fields is an important problem with various applications including feature-directed visualization and feature tracking in time-varying data. Comparing topological structures that are abstract and succinct representations of the scalar fields lead to faster and meaningful comparison. While there are many distance or similarity measures to compare topological structures in a global context, there are no known measures for comparing topological structures locally. While the global measures have many applications, they do not directly lend themselves to fine-grained analysis across multiple scales. We define a local variant of the tree edit distance and apply it towards local comparative analysis of merge trees with support for finer analysis. We also present experimental results on time-varying scalar fields, 3D cryo-electron microscopy data, and other synthetic data sets to show the utility of this approach in applications like symmetry detection and feature tracking.",
                        "uid": "v-tvcg-9585392",
                        "file_name": "v-tvcg-9585392_Sridharamurthy_Presentation.mp4",
                        "time_stamp": "2022-10-21T14:24:00Z",
                        "time_start": "2022-10-21T14:24:00Z",
                        "time_end": "2022-10-21T14:34:00Z",
                        "paper_type": "full",
                        "keywords": [
                            "Merge tree, scalar field, local distance measure, persistence, edit distance, symmetry detection, feature tracking."
                        ],
                        "has_image": "1",
                        "has_video": "454",
                        "paper_award": "",
                        "image_caption": "Local Merge Tree Edit Distance (LMTED) helps in local comparison of scalar fields and facilitates lot of applications like symmetry detection, feature tracking, analysis of effects of topological compression.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "https://youtu.be/Sht6_xpNfLo",
                        "ff_id": "Sht6_xpNfLo"
                    },
                    {
                        "slot_id": "v-tvcg-9585392-qa",
                        "session_id": "full22",
                        "type": "In Person Q+A",
                        "title": "Comparative Analysis of Merge Trees using Local Tree Edit Distance (Q+A)",
                        "contributors": [
                            "Raghavendra Sridharamurthy"
                        ],
                        "authors": [],
                        "abstract": "Comparative analysis of scalar fields is an important problem with various applications including feature-directed visualization and feature tracking in time-varying data. Comparing topological structures that are abstract and succinct representations of the scalar fields lead to faster and meaningful comparison. While there are many distance or similarity measures to compare topological structures in a global context, there are no known measures for comparing topological structures locally. While the global measures have many applications, they do not directly lend themselves to fine-grained analysis across multiple scales. We define a local variant of the tree edit distance and apply it towards local comparative analysis of merge trees with support for finer analysis. We also present experimental results on time-varying scalar fields, 3D cryo-electron microscopy data, and other synthetic data sets to show the utility of this approach in applications like symmetry detection and feature tracking.",
                        "uid": "v-tvcg-9585392",
                        "file_name": "",
                        "time_stamp": "2022-10-21T14:34:00Z",
                        "time_start": "2022-10-21T14:34:00Z",
                        "time_end": "2022-10-21T14:36:00Z",
                        "paper_type": "full",
                        "keywords": [
                            "Merge tree, scalar field, local distance measure, persistence, edit distance, symmetry detection, feature tracking."
                        ],
                        "has_image": "1",
                        "has_video": "454",
                        "paper_award": "",
                        "image_caption": "Local Merge Tree Edit Distance (LMTED) helps in local comparison of scalar fields and facilitates lot of applications like symmetry detection, feature tracking, analysis of effects of topological compression.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "https://youtu.be/Sht6_xpNfLo",
                        "ff_id": "Sht6_xpNfLo"
                    },
                    {
                        "slot_id": "v-tvcg-9729550-pres",
                        "session_id": "full22",
                        "type": "In Person Presentation",
                        "title": "Visual Exploration of Relationships and Structure in Low-Dimensional Embeddings",
                        "contributors": [
                            "Klaus Eckelt"
                        ],
                        "authors": [
                            "Klaus Eckelt",
                            "Andreas Hinterreiter",
                            "Patrick Adelberger",
                            "Conny Walchshofer",
                            "Vaishali Dhanoa",
                            "Christina Humer",
                            "Moritz Heckmann",
                            "Christian A. Steinparz",
                            "Marc Streit"
                        ],
                        "abstract": "In this work, we propose an interactive visual approach for the exploration and formation of structural relationships in embeddings of high-dimensional data. These structural relationships, such as item sequences, associations of items with groups, and hierarchies between groups of items, are defining properties of many real-world datasets. Nevertheless, most existing methods for the visual exploration of embeddings treat these structures as second-class citizens or do not take them into account at all. In our proposed analysis workflow, users explore enriched scatterplots of the embedding, in which relationships between items and\\slash or groups are visually highlighted. The original high-dimensional data for single items, groups of items, or differences between connected items and groups are accessible through additional summary visualizations. We carefully tailored these summary and difference visualizations to the various data types and semantic contexts. During their exploratory analysis, users can externalize their insights by setting up additional groups and relationships between items and\\slash or groups. We demonstrate the utility and potential impact of our approach by means of two use cases and multiple examples from various domains.",
                        "uid": "v-tvcg-9729550",
                        "file_name": "v-tvcg-9729550_Eckelt_Presentation.mp4",
                        "time_stamp": "2022-10-21T14:36:00Z",
                        "time_start": "2022-10-21T14:36:00Z",
                        "time_end": "2022-10-21T14:46:00Z",
                        "paper_type": "full",
                        "keywords": [
                            "Dimensionality reduction, projection, visual analytics, layout enrichment, aggregation, comparison"
                        ],
                        "has_image": "1",
                        "has_video": "556",
                        "paper_award": "",
                        "image_caption": "The figure shows an embedding of 450 chess games. Each game consists of multiple states; one for each player move. The states of a game are connected in order to follow the course of the game. Groups of game states are represented by diamond symbols. Group A represents the start of all chess games, which splits up into groups for three different openings: Zukertort (B, green), English (C, pink), and Queen's Pawn (D, orange) opening. Groups E-I contain middlegame moves, and group J endgame moves. In the comparison pane on the right, summaries and differences of middlegame groups are shown.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "https://youtu.be/N5VoiUb9Vs0",
                        "ff_id": "N5VoiUb9Vs0"
                    },
                    {
                        "slot_id": "v-tvcg-9729550-qa",
                        "session_id": "full22",
                        "type": "In Person Q+A",
                        "title": "Visual Exploration of Relationships and Structure in Low-Dimensional Embeddings (Q+A)",
                        "contributors": [
                            "Klaus Eckelt"
                        ],
                        "authors": [],
                        "abstract": "In this work, we propose an interactive visual approach for the exploration and formation of structural relationships in embeddings of high-dimensional data. These structural relationships, such as item sequences, associations of items with groups, and hierarchies between groups of items, are defining properties of many real-world datasets. Nevertheless, most existing methods for the visual exploration of embeddings treat these structures as second-class citizens or do not take them into account at all. In our proposed analysis workflow, users explore enriched scatterplots of the embedding, in which relationships between items and\\slash or groups are visually highlighted. The original high-dimensional data for single items, groups of items, or differences between connected items and groups are accessible through additional summary visualizations. We carefully tailored these summary and difference visualizations to the various data types and semantic contexts. During their exploratory analysis, users can externalize their insights by setting up additional groups and relationships between items and\\slash or groups. We demonstrate the utility and potential impact of our approach by means of two use cases and multiple examples from various domains.",
                        "uid": "v-tvcg-9729550",
                        "file_name": "",
                        "time_stamp": "2022-10-21T14:46:00Z",
                        "time_start": "2022-10-21T14:46:00Z",
                        "time_end": "2022-10-21T14:48:00Z",
                        "paper_type": "full",
                        "keywords": [
                            "Dimensionality reduction, projection, visual analytics, layout enrichment, aggregation, comparison"
                        ],
                        "has_image": "1",
                        "has_video": "556",
                        "paper_award": "",
                        "image_caption": "The figure shows an embedding of 450 chess games. Each game consists of multiple states; one for each player move. The states of a game are connected in order to follow the course of the game. Groups of game states are represented by diamond symbols. Group A represents the start of all chess games, which splits up into groups for three different openings: Zukertort (B, green), English (C, pink), and Queen's Pawn (D, orange) opening. Groups E-I contain middlegame moves, and group J endgame moves. In the comparison pane on the right, summaries and differences of middlegame groups are shown.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "https://youtu.be/N5VoiUb9Vs0",
                        "ff_id": "N5VoiUb9Vs0"
                    },
                    {
                        "slot_id": "v-tvcg-9716867-pres",
                        "session_id": "full22",
                        "type": "In Person Presentation",
                        "title": "View Composition Algebra for Ad Hoc Comparison",
                        "contributors": [
                            "Eugene Wu"
                        ],
                        "authors": [
                            "Eugene Wu"
                        ],
                        "abstract": "Comparison is a core task in visual analysis.  Although there are numerous guidelines to help users design effective visualizations to aid known comparison tasks, there are few techniques available when users want to make ad hoc comparisons between marks, trends, or charts during data exploration and visual analysis.  For instance, to compare voting count maps from different years, two stock trends in a line chart, or a scatterplot of country GDPs with a textual summary of the average GDP.  Ideally, users can directly select the comparison targets and compare them, however what elements of a visualization should be candidate targets, which combinations of targets are safe to compare, and what comparison operations make sense?  This paper proposes a conceptual model that lets users compose combinations of values, marks, legend elements, and charts using a set of composition operators that summarize, compute differences, merge, and model their operands.  We further define a View Composition Algebra (VCA) that is compatible with datacube-based visualizations, derive an interaction design based on this algebra that supports ad hoc visual comparisons, and illustrate its utility through several use cases.",
                        "uid": "v-tvcg-9716867",
                        "file_name": "v-tvcg-9716867_Wu_Presentation.mp4",
                        "time_stamp": "2022-10-21T14:48:00Z",
                        "time_start": "2022-10-21T14:48:00Z",
                        "time_end": "2022-10-21T14:57:33Z",
                        "paper_type": "full",
                        "keywords": [
                            "Visualization, Algebra, Comparison, Databases"
                        ],
                        "has_image": "1",
                        "has_video": "573",
                        "paper_award": "",
                        "image_caption": "View Composition Algebra for Ad-hoc Comparisons",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "https://youtu.be/F8tQ3WzDeBE",
                        "ff_id": "F8tQ3WzDeBE"
                    },
                    {
                        "slot_id": "v-tvcg-9716867-qa",
                        "session_id": "full22",
                        "type": "Virtual Q+A",
                        "title": "View Composition Algebra for Ad Hoc Comparison (Q+A)",
                        "contributors": [
                            "Eugene Wu"
                        ],
                        "authors": [],
                        "abstract": "Comparison is a core task in visual analysis.  Although there are numerous guidelines to help users design effective visualizations to aid known comparison tasks, there are few techniques available when users want to make ad hoc comparisons between marks, trends, or charts during data exploration and visual analysis.  For instance, to compare voting count maps from different years, two stock trends in a line chart, or a scatterplot of country GDPs with a textual summary of the average GDP.  Ideally, users can directly select the comparison targets and compare them, however what elements of a visualization should be candidate targets, which combinations of targets are safe to compare, and what comparison operations make sense?  This paper proposes a conceptual model that lets users compose combinations of values, marks, legend elements, and charts using a set of composition operators that summarize, compute differences, merge, and model their operands.  We further define a View Composition Algebra (VCA) that is compatible with datacube-based visualizations, derive an interaction design based on this algebra that supports ad hoc visual comparisons, and illustrate its utility through several use cases.",
                        "uid": "v-tvcg-9716867",
                        "file_name": "",
                        "time_stamp": "2022-10-21T14:57:33Z",
                        "time_start": "2022-10-21T14:57:33Z",
                        "time_end": "2022-10-21T15:00:00Z",
                        "paper_type": "full",
                        "keywords": [
                            "Visualization, Algebra, Comparison, Databases"
                        ],
                        "has_image": "1",
                        "has_video": "573",
                        "paper_award": "",
                        "image_caption": "View Composition Algebra for Ad-hoc Comparisons",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "https://youtu.be/F8tQ3WzDeBE",
                        "ff_id": "F8tQ3WzDeBE"
                    },
                    {
                        "slot_id": "v-full-1607-pres",
                        "session_id": "full22",
                        "type": "Virtual Presentation (pre-recorded)",
                        "title": "Visual Comparison of Language Model Adaptation",
                        "contributors": [
                            "Rita Sevastjanova"
                        ],
                        "authors": [
                            "Rita Sevastjanova",
                            "Eren Cakmak",
                            "Shauli Ravfogel",
                            "Ryan Cotterell",
                            "Mennatallah El-Assady"
                        ],
                        "abstract": "Neural language models are widely used; however, their model parameters often need to be adapted to the specific domains and tasks of an application, which is time- and resource-consuming. Thus, adapters have recently been introduced as a lightweight alternative for model adaptation. They consist of a small set of task-specific parameters with a reduced training time and simple parameter composition. The simplicity of adapter training and composition comes along with new challenges, such as maintaining an overview of adapter properties and effectively comparing their produced embedding spaces. To help developers overcome these challenges, we provide a twofold contribution. First, in close collaboration with NLP researchers, we conducted a requirement analysis for an approach supporting adapter evaluation and detected, among others, the need for both intrinsic (i.e., embedding similarity-based) and extrinsic (i.e., prediction-based) explanation methods. Second, motivated by the gathered requirements, we designed a flexible visual analytics workspace that enables the comparison of adapter properties. In this paper, we discuss several design iterations and alternatives for interactive, comparative visual explanation methods. Our comparative visualizations show the differences in the adapted embedding vectors and prediction outcomes for diverse human-interpretable concepts (e.g., person names, human qualities). We evaluate our workspace through case studies and show that, for instance, an adapter trained on the language debiasing task according to context-0 (decontextualized) embeddings introduces a new type of bias where words (even gender-independent words such as countries) become more similar to female- than male pronouns. We demonstrate that these are artifacts of context-0 embeddings, and the adapter effectively eliminates the gender information from the contextualized word representations.",
                        "uid": "v-full-1607",
                        "file_name": "v-full-1607_Sevastjanova_Presentation.mp4",
                        "time_stamp": "2022-10-21T15:00:00Z",
                        "time_start": "2022-10-21T15:00:00Z",
                        "time_end": "2022-10-21T15:09:59Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "599",
                        "paper_award": "",
                        "image_caption": "We present a workspace that enables the evaluation and comparison of adapters \u2013 lightweight alternatives for language model fine-tuning. After data pre-processing (e.g., embedding extraction), users can select pre-trained adapters, create explanations, and explore model differences through three types of visualizations: Concept Embedding Similarity, Concept Embedding Projection, and Concept Prediction Similarity. The explanations are provided for single models as well as model comparisons. Here: contrary to\nthe rotten-tomatoes sentiment classifier, the context-0 embeddings of the sst-2 sentiment classifier strongly encode the two polarities of human qualities.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "https://youtu.be/xChs1wpCnmk",
                        "ff_id": "xChs1wpCnmk"
                    },
                    {
                        "slot_id": "v-full-1607-qa",
                        "session_id": "full22",
                        "type": "Virtual Q+A",
                        "title": "Visual Comparison of Language Model Adaptation (Q+A)",
                        "contributors": [
                            "Rita Sevastjanova"
                        ],
                        "authors": [],
                        "abstract": "Neural language models are widely used; however, their model parameters often need to be adapted to the specific domains and tasks of an application, which is time- and resource-consuming. Thus, adapters have recently been introduced as a lightweight alternative for model adaptation. They consist of a small set of task-specific parameters with a reduced training time and simple parameter composition. The simplicity of adapter training and composition comes along with new challenges, such as maintaining an overview of adapter properties and effectively comparing their produced embedding spaces. To help developers overcome these challenges, we provide a twofold contribution. First, in close collaboration with NLP researchers, we conducted a requirement analysis for an approach supporting adapter evaluation and detected, among others, the need for both intrinsic (i.e., embedding similarity-based) and extrinsic (i.e., prediction-based) explanation methods. Second, motivated by the gathered requirements, we designed a flexible visual analytics workspace that enables the comparison of adapter properties. In this paper, we discuss several design iterations and alternatives for interactive, comparative visual explanation methods. Our comparative visualizations show the differences in the adapted embedding vectors and prediction outcomes for diverse human-interpretable concepts (e.g., person names, human qualities). We evaluate our workspace through case studies and show that, for instance, an adapter trained on the language debiasing task according to context-0 (decontextualized) embeddings introduces a new type of bias where words (even gender-independent words such as countries) become more similar to female- than male pronouns. We demonstrate that these are artifacts of context-0 embeddings, and the adapter effectively eliminates the gender information from the contextualized word representations.",
                        "uid": "v-full-1607",
                        "file_name": "",
                        "time_stamp": "2022-10-21T15:09:59Z",
                        "time_start": "2022-10-21T15:09:59Z",
                        "time_end": "2022-10-21T15:12:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "599",
                        "paper_award": "",
                        "image_caption": "We present a workspace that enables the evaluation and comparison of adapters \u2013 lightweight alternatives for language model fine-tuning. After data pre-processing (e.g., embedding extraction), users can select pre-trained adapters, create explanations, and explore model differences through three types of visualizations: Concept Embedding Similarity, Concept Embedding Projection, and Concept Prediction Similarity. The explanations are provided for single models as well as model comparisons. Here: contrary to\nthe rotten-tomatoes sentiment classifier, the context-0 embeddings of the sst-2 sentiment classifier strongly encode the two polarities of human qualities.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "https://youtu.be/xChs1wpCnmk",
                        "ff_id": "xChs1wpCnmk"
                    }
                ]
            },
            {
                "title": "Topology",
                "session_id": "full23",
                "event_prefix": "v-full",
                "track": "ok5",
                "livestream_id": "ok5-thu",
                "session_image": "full23.png",
                "chair": [
                    "Julien Tierny"
                ],
                "organizers": [],
                "time_start": "2022-10-20T20:45:00Z",
                "time_end": "2022-10-20T22:00:00Z",
                "discord_category": "",
                "discord_channel": "oklahoma-st-5",
                "discord_channel_id": "1024600839295356939",
                "discord_link": "https://discord.com/channels/978320435554947082/1024600839295356939",
                "slido_link": "https://app.sli.do/event/i28UETZaiRWr1XwX6eKjcf?section=d355f89b-fbd2-4abf-9958-741607905551",
                "youtube_url": "https://youtu.be/JCJlogloJH8",
                "youtube_id": "JCJlogloJH8",
                "zoom_meeting": "",
                "zoom_password": "",
                "zoom_link": "",
                "ff_link": "https://youtu.be/GKBudx4iKOc",
                "ff_playlist": "",
                "time_slots": [
                    {
                        "slot_id": "full23-opening",
                        "session_id": "full23",
                        "type": "In Person Other",
                        "title": "Session Opening",
                        "contributors": [
                            "Julien Tierny"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "",
                        "file_name": "",
                        "time_stamp": "2022-10-20T20:45:00Z",
                        "time_start": "2022-10-20T20:45:00Z",
                        "time_end": "2022-10-20T20:45:00Z",
                        "paper_type": "",
                        "keywords": [],
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "v-full-1051-pres",
                        "session_id": "full23",
                        "type": "In Person Presentation",
                        "title": "Temporal Merge Tree Maps: A Topology-Based Static Visualization for Temporal Scalar Data",
                        "contributors": [
                            "Wiebke K\u00f6pp"
                        ],
                        "authors": [
                            "Wiebke K\u00f6pp",
                            "Tino Weinkauf"
                        ],
                        "abstract": "Creating a static visualization for a time-dependent scalar field is a non-trivial task, yet very insightful as it shows the dynamics in one picture. Existing approaches are based on a linearization of the domain or on feature tracking. Domain linearizations use space-filling curves to place all sample points into a 1D domain, thereby breaking up individual features. Feature tracking methods explicitly respect feature continuity in space and time, but generally neglect the data context in which those features live. We present a feature-based linearization of the spatial domain that keeps features together and preserves their context by involving all data samples. We use augmented merge trees to linearize the domain and show that our linearized function has the same merge tree as the original data. A greedy optimization scheme aligns the trees over time providing temporal continuity. This leads to a static 2D visualization with one temporal dimension, and all spatial dimensions compressed into one. We compare our method against other domain linearizations as well as feature-tracking approaches, and apply it to several real-world data sets.",
                        "uid": "v-full-1051",
                        "file_name": "v-full-1051_Koepp_Presentation.mp4",
                        "time_stamp": "2022-10-20T20:45:00Z",
                        "time_start": "2022-10-20T20:45:00Z",
                        "time_end": "2022-10-20T20:55:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "1325",
                        "paper_award": "",
                        "image_caption": "A temporal merge tree map (top) is a static visualization of a time-dependent scalar field (bottom). Our method uses augmented merge trees to map the data samples of each time step to a vertical slice. An optimization scheme is employed to achieve a temporally coherent mapping. The shown Storms data set represents storm activity over Europe in December 1999 and contains 744 time steps, arranged from left to right. Cyclones can easily be identified as dark blue lines and compared with each other while still being shown in the context of the entire data.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "https://youtu.be/JWDj-gwVuv0",
                        "ff_id": "JWDj-gwVuv0"
                    },
                    {
                        "slot_id": "v-full-1051-qa",
                        "session_id": "full23",
                        "type": "In Person Q+A",
                        "title": "Temporal Merge Tree Maps: A Topology-Based Static Visualization for Temporal Scalar Data (Q+A)",
                        "contributors": [
                            "Wiebke K\u00f6pp"
                        ],
                        "authors": [],
                        "abstract": "Creating a static visualization for a time-dependent scalar field is a non-trivial task, yet very insightful as it shows the dynamics in one picture. Existing approaches are based on a linearization of the domain or on feature tracking. Domain linearizations use space-filling curves to place all sample points into a 1D domain, thereby breaking up individual features. Feature tracking methods explicitly respect feature continuity in space and time, but generally neglect the data context in which those features live. We present a feature-based linearization of the spatial domain that keeps features together and preserves their context by involving all data samples. We use augmented merge trees to linearize the domain and show that our linearized function has the same merge tree as the original data. A greedy optimization scheme aligns the trees over time providing temporal continuity. This leads to a static 2D visualization with one temporal dimension, and all spatial dimensions compressed into one. We compare our method against other domain linearizations as well as feature-tracking approaches, and apply it to several real-world data sets.",
                        "uid": "v-full-1051",
                        "file_name": "",
                        "time_stamp": "2022-10-20T20:55:00Z",
                        "time_start": "2022-10-20T20:55:00Z",
                        "time_end": "2022-10-20T20:57:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "1325",
                        "paper_award": "",
                        "image_caption": "A temporal merge tree map (top) is a static visualization of a time-dependent scalar field (bottom). Our method uses augmented merge trees to map the data samples of each time step to a vertical slice. An optimization scheme is employed to achieve a temporally coherent mapping. The shown Storms data set represents storm activity over Europe in December 1999 and contains 744 time steps, arranged from left to right. Cyclones can easily be identified as dark blue lines and compared with each other while still being shown in the context of the entire data.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "https://youtu.be/JWDj-gwVuv0",
                        "ff_id": "JWDj-gwVuv0"
                    },
                    {
                        "slot_id": "v-tvcg-9583888-pres",
                        "session_id": "full23",
                        "type": "In Person Presentation",
                        "title": "TopoCluster: A Localized Data Structure for Topology-based Visualization",
                        "contributors": [
                            "Guoxi Liu"
                        ],
                        "authors": [
                            "Guoxi Liu",
                            "Federico Iuricich",
                            "Riccardo Fellegara",
                            "Leila De Floriani"
                        ],
                        "abstract": "Unstructured data are collections of points with irregular topology, often represented through simplicial meshes, such as triangle and tetrahedral meshes. Whenever possible such representations are avoided in visualization since they are computationally demanding if compared with regular grids. In this work, we aim at simplifying the encoding and processing of simplicial meshes. The paper proposes TopoCluster, a new localized data structure for tetrahedral meshes. TopoCluster provides efficient computation of the connectivity of the mesh elements with a low memory footprint. The key idea of TopoCluster is to subdivide the simplicial mesh into clusters. Then, the connectivity information is computed locally for each cluster and discarded when it is no longer needed. We define two instances of TopoCluster. The first instance prioritizes time efficiency and provides only modest savings in memory, while the second instance drastically reduces memory consumption up to an order of magnitude with respect to comparable data structures. Thanks to the simple interface provided by TopoCluster, we have been able to integrate both data structures into the existing Topological Toolkit (TTK) framework. As a result, users can run any plugin of TTK using TopoCluster without changing a single line of code.",
                        "uid": "v-tvcg-9583888",
                        "file_name": "v-tvcg-9583888_Liu_Presentation.mp4",
                        "time_stamp": "2022-10-20T20:57:00Z",
                        "time_start": "2022-10-20T20:57:00Z",
                        "time_end": "2022-10-20T21:07:00Z",
                        "paper_type": "full",
                        "keywords": [
                            "Data visualization, data structures, topological data analysis, simplicial meshes, tetrahedral meshes"
                        ],
                        "has_image": "1",
                        "has_video": "493",
                        "paper_award": "",
                        "image_caption": "An example of the TopoCluster data structure showing the localized computation of topological relations on the dragon dataset and the performance comparison with other data structures.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "https://youtu.be/CwHfk_2BYkI",
                        "ff_id": "CwHfk_2BYkI"
                    },
                    {
                        "slot_id": "v-tvcg-9583888-qa",
                        "session_id": "full23",
                        "type": "In Person Q+A",
                        "title": "TopoCluster: A Localized Data Structure for Topology-based Visualization (Q+A)",
                        "contributors": [
                            "Guoxi Liu"
                        ],
                        "authors": [],
                        "abstract": "Unstructured data are collections of points with irregular topology, often represented through simplicial meshes, such as triangle and tetrahedral meshes. Whenever possible such representations are avoided in visualization since they are computationally demanding if compared with regular grids. In this work, we aim at simplifying the encoding and processing of simplicial meshes. The paper proposes TopoCluster, a new localized data structure for tetrahedral meshes. TopoCluster provides efficient computation of the connectivity of the mesh elements with a low memory footprint. The key idea of TopoCluster is to subdivide the simplicial mesh into clusters. Then, the connectivity information is computed locally for each cluster and discarded when it is no longer needed. We define two instances of TopoCluster. The first instance prioritizes time efficiency and provides only modest savings in memory, while the second instance drastically reduces memory consumption up to an order of magnitude with respect to comparable data structures. Thanks to the simple interface provided by TopoCluster, we have been able to integrate both data structures into the existing Topological Toolkit (TTK) framework. As a result, users can run any plugin of TTK using TopoCluster without changing a single line of code.",
                        "uid": "v-tvcg-9583888",
                        "file_name": "",
                        "time_stamp": "2022-10-20T21:07:00Z",
                        "time_start": "2022-10-20T21:07:00Z",
                        "time_end": "2022-10-20T21:09:00Z",
                        "paper_type": "full",
                        "keywords": [
                            "Data visualization, data structures, topological data analysis, simplicial meshes, tetrahedral meshes"
                        ],
                        "has_image": "1",
                        "has_video": "493",
                        "paper_award": "",
                        "image_caption": "An example of the TopoCluster data structure showing the localized computation of topological relations on the dragon dataset and the performance comparison with other data structures.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "https://youtu.be/CwHfk_2BYkI",
                        "ff_id": "CwHfk_2BYkI"
                    },
                    {
                        "slot_id": "v-full-1233-pres",
                        "session_id": "full23",
                        "type": "Virtual Presentation (pre-recorded)",
                        "title": "Computing a Stable Distance on Merge Trees",
                        "contributors": [
                            "Brian C Bollen",
                            "Brian Bollen"
                        ],
                        "authors": [
                            "Brian C Bollen",
                            "Joshua A Levine",
                            "Pasindu P. Tennakoon"
                        ],
                        "abstract": "Distances on merge trees facilitate visual comparison of collections of scalar fields. Two desirable properties for these distances to exhibit are 1) the ability to discern between scalar fields which other, less complex topological summaries cannot and 2) to still be robust to perturbations in the dataset. The combination of these two properties, known respectively as stability and discriminativity, has led to theoretical distances which are either thought to be or shown to be computationally complex and thus their implementations have been scarce. In order to design similarity measures on merge trees which are computationally feasible for more complex merge trees, many researchers have elected to loosen the restrictions on at least one of these two properties. The question still remains, however, if there are practical situations where trading these desirable properties is necessary. Here we construct a distance between merge trees which is designed to retain both discriminativity and stability. While our approach can be expensive for large merge trees, we illustrate its use in a setting where the number of nodes is small. This setting can be made more practical since we also provide a proof that persistence simplification increases the outputted distance by at most half of the simplified value. We demonstrate our distance measure on applications in shape comparison and on detection of periodicity in the von K\u00e1rm\u00e1n vortex street.",
                        "uid": "v-full-1233",
                        "file_name": "v-full-1233_Bollen_Presentation.mp4",
                        "time_stamp": "2022-10-20T21:09:00Z",
                        "time_start": "2022-10-20T21:09:00Z",
                        "time_end": "2022-10-20T21:18:41Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "581",
                        "paper_award": "",
                        "image_caption": "We construct a novel distance between merge tree which exhibit two properties that are desirable for the analysis of scalar field data -- stability and discriminativity. We provide an accompanying computation to this distance and an accompanying proof that persistence simplification can provide an approximation to this distance while reducing computation time drastically. We achieve this by drawing inspriation from the well-known graph-edit distance problem, the definition and construction of the bottleneck distance, and the stability handling of the universal distance on merge trees.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "https://youtu.be/TWudEI4tBlQ",
                        "ff_id": "TWudEI4tBlQ"
                    },
                    {
                        "slot_id": "v-full-1233-qa",
                        "session_id": "full23",
                        "type": "Virtual Q+A",
                        "title": "Computing a Stable Distance on Merge Trees (Q+A)",
                        "contributors": [
                            "Brian C Bollen",
                            "Brian Bollen"
                        ],
                        "authors": [],
                        "abstract": "Distances on merge trees facilitate visual comparison of collections of scalar fields. Two desirable properties for these distances to exhibit are 1) the ability to discern between scalar fields which other, less complex topological summaries cannot and 2) to still be robust to perturbations in the dataset. The combination of these two properties, known respectively as stability and discriminativity, has led to theoretical distances which are either thought to be or shown to be computationally complex and thus their implementations have been scarce. In order to design similarity measures on merge trees which are computationally feasible for more complex merge trees, many researchers have elected to loosen the restrictions on at least one of these two properties. The question still remains, however, if there are practical situations where trading these desirable properties is necessary. Here we construct a distance between merge trees which is designed to retain both discriminativity and stability. While our approach can be expensive for large merge trees, we illustrate its use in a setting where the number of nodes is small. This setting can be made more practical since we also provide a proof that persistence simplification increases the outputted distance by at most half of the simplified value. We demonstrate our distance measure on applications in shape comparison and on detection of periodicity in the von K\u00e1rm\u00e1n vortex street.",
                        "uid": "v-full-1233",
                        "file_name": "",
                        "time_stamp": "2022-10-20T21:18:41Z",
                        "time_start": "2022-10-20T21:18:41Z",
                        "time_end": "2022-10-20T21:21:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "581",
                        "paper_award": "",
                        "image_caption": "We construct a novel distance between merge tree which exhibit two properties that are desirable for the analysis of scalar field data -- stability and discriminativity. We provide an accompanying computation to this distance and an accompanying proof that persistence simplification can provide an approximation to this distance while reducing computation time drastically. We achieve this by drawing inspriation from the well-known graph-edit distance problem, the definition and construction of the bottleneck distance, and the stability handling of the universal distance on merge trees.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "https://youtu.be/TWudEI4tBlQ",
                        "ff_id": "TWudEI4tBlQ"
                    },
                    {
                        "slot_id": "v-tvcg-9721603-pres",
                        "session_id": "full23",
                        "type": "In Person Presentation",
                        "title": "Topological Simplifications of Hypergraphs",
                        "contributors": [
                            "Youjia Zhou"
                        ],
                        "authors": [
                            "Youjia Zhou",
                            "Archit Rathore",
                            "Emilie Purvine",
                            "Bei Wang"
                        ],
                        "abstract": "We study hypergraph visualization via its topological simplification. We explore both vertex simplification and hyperedge simplification of hypergraphs using tools from topological data analysis. In particular, we transform a hypergraph into its graph representations known as the line graph and clique expansion. A topological simplification of such a graph representation induces a simplification of the hypergraph. In simplifying a hypergraph, we allow vertices to be combined if they belong to almost the same set of hyperedges, and hyperedges to be merged if they share almost the same set of vertices. Our proposed approaches are general, mathematically justifiable, and put vertex simplification and hyperedge simplification in a unifying framework.",
                        "uid": "v-tvcg-9721603",
                        "file_name": "v-tvcg-9721603_Zhou_Presentation.mp4",
                        "time_stamp": "2022-10-20T21:21:00Z",
                        "time_start": "2022-10-20T21:21:00Z",
                        "time_end": "2022-10-20T21:31:00Z",
                        "paper_type": "full",
                        "keywords": [
                            "Hypergraph simplification, hypergraph visualization, graph simplification, topological data analysis"
                        ],
                        "has_image": "1",
                        "has_video": "881",
                        "paper_award": "",
                        "image_caption": "We propose a topological simplification framework for hypergraphs, and provide an interactive tool that allows users to explore the simplification framework and apply vertex and hyperedge simplifications to gain insights from their own datasets. The image demonstrates the original hypergraph (a) from an active Domain Name System (DNS) dataset and the simplified hypergraph (c) after hyperedge simplification.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "https://youtu.be/fNVA7OM74mU",
                        "ff_id": "fNVA7OM74mU"
                    },
                    {
                        "slot_id": "v-tvcg-9721603-qa",
                        "session_id": "full23",
                        "type": "In Person Q+A",
                        "title": "Topological Simplifications of Hypergraphs (Q+A)",
                        "contributors": [
                            "Youjia Zhou"
                        ],
                        "authors": [],
                        "abstract": "We study hypergraph visualization via its topological simplification. We explore both vertex simplification and hyperedge simplification of hypergraphs using tools from topological data analysis. In particular, we transform a hypergraph into its graph representations known as the line graph and clique expansion. A topological simplification of such a graph representation induces a simplification of the hypergraph. In simplifying a hypergraph, we allow vertices to be combined if they belong to almost the same set of hyperedges, and hyperedges to be merged if they share almost the same set of vertices. Our proposed approaches are general, mathematically justifiable, and put vertex simplification and hyperedge simplification in a unifying framework.",
                        "uid": "v-tvcg-9721603",
                        "file_name": "",
                        "time_stamp": "2022-10-20T21:31:00Z",
                        "time_start": "2022-10-20T21:31:00Z",
                        "time_end": "2022-10-20T21:33:00Z",
                        "paper_type": "full",
                        "keywords": [
                            "Hypergraph simplification, hypergraph visualization, graph simplification, topological data analysis"
                        ],
                        "has_image": "1",
                        "has_video": "881",
                        "paper_award": "",
                        "image_caption": "We propose a topological simplification framework for hypergraphs, and provide an interactive tool that allows users to explore the simplification framework and apply vertex and hyperedge simplifications to gain insights from their own datasets. The image demonstrates the original hypergraph (a) from an active Domain Name System (DNS) dataset and the simplified hypergraph (c) after hyperedge simplification.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "https://youtu.be/fNVA7OM74mU",
                        "ff_id": "fNVA7OM74mU"
                    },
                    {
                        "slot_id": "v-tvcg-9531544-pres",
                        "session_id": "full23",
                        "type": "In Person Presentation",
                        "title": "Persistence cycles for visual exploration of persistent homology",
                        "contributors": [
                            "Federico Iuricich"
                        ],
                        "authors": [
                            "Federico Iuricich"
                        ],
                        "abstract": "Persistent homology is a fundamental tool in topological data analysis used for the most diverse applications. Information captured by persistent homology is commonly visualized using scatter plots representations. Despite being widely adopted, such a visualization technique limits user understanding and is prone to misinterpretation. This paper proposes a new approach for the efficient computation of persistence cycles, a geometric representation of the features captured by persistent homology. We illustrate the importance of rendering persistence cycles when analyzing scalar fields, and we discuss the advantages that our approach provides compared to other techniques in topology-based visualization. We provide an efficient implementation of our approach based on discrete Morse theory, as a new module for the Topology Toolkit. We show that our implementation has comparable performance with respect to state-of-the-art toolboxes while providing a better framework for visually analyzing persistent homology information.",
                        "uid": "v-tvcg-9531544",
                        "file_name": "v-tvcg-9531544_Iuricich_Presentation.mp4",
                        "time_stamp": "2022-10-20T21:33:00Z",
                        "time_start": "2022-10-20T21:33:00Z",
                        "time_end": "2022-10-20T21:43:00Z",
                        "paper_type": "full",
                        "keywords": [
                            "Persistent homology, Topological Data Analysis, Scalar fields"
                        ],
                        "has_image": "1",
                        "has_video": "669",
                        "paper_award": "",
                        "image_caption": "Example of persistence 1-cycles computed on the Silicium dataset. Each persistence 1-cycle can be found in the correspondence of a pair of 1-saddle 2-saddle critical points and identifies a tunnel/handle in the dataset.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "https://youtu.be/3nIARIEnj04",
                        "ff_id": "3nIARIEnj04"
                    },
                    {
                        "slot_id": "v-tvcg-9531544-qa",
                        "session_id": "full23",
                        "type": "In Person Q+A",
                        "title": "Persistence cycles for visual exploration of persistent homology (Q+A)",
                        "contributors": [
                            "Federico Iuricich"
                        ],
                        "authors": [],
                        "abstract": "Persistent homology is a fundamental tool in topological data analysis used for the most diverse applications. Information captured by persistent homology is commonly visualized using scatter plots representations. Despite being widely adopted, such a visualization technique limits user understanding and is prone to misinterpretation. This paper proposes a new approach for the efficient computation of persistence cycles, a geometric representation of the features captured by persistent homology. We illustrate the importance of rendering persistence cycles when analyzing scalar fields, and we discuss the advantages that our approach provides compared to other techniques in topology-based visualization. We provide an efficient implementation of our approach based on discrete Morse theory, as a new module for the Topology Toolkit. We show that our implementation has comparable performance with respect to state-of-the-art toolboxes while providing a better framework for visually analyzing persistent homology information.",
                        "uid": "v-tvcg-9531544",
                        "file_name": "",
                        "time_stamp": "2022-10-20T21:43:00Z",
                        "time_start": "2022-10-20T21:43:00Z",
                        "time_end": "2022-10-20T21:45:00Z",
                        "paper_type": "full",
                        "keywords": [
                            "Persistent homology, Topological Data Analysis, Scalar fields"
                        ],
                        "has_image": "1",
                        "has_video": "669",
                        "paper_award": "",
                        "image_caption": "Example of persistence 1-cycles computed on the Silicium dataset. Each persistence 1-cycle can be found in the correspondence of a pair of 1-saddle 2-saddle critical points and identifies a tunnel/handle in the dataset.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "https://youtu.be/3nIARIEnj04",
                        "ff_id": "3nIARIEnj04"
                    },
                    {
                        "slot_id": "v-tvcg-9721643-pres",
                        "session_id": "full23",
                        "type": "Virtual Presentation (pre-recorded)",
                        "title": "Geometry-Aware Planar Embedding of Treelike Structures",
                        "contributors": [
                            "Ping Hu"
                        ],
                        "authors": [
                            "Ping Hu",
                            "Saeed Boorboor",
                            "Joseph Marino",
                            "Arie E. Kaufman"
                        ],
                        "abstract": "The growing complexity of spatial and structural information in 3D data makes data inspection and visualization a challenging task. We describe a method to create a planar embedding of 3D treelike structures using their skeleton representations. Our method maintains the original geometry, without overlaps, to the best extent possible, allowing exploration of the topology within a single view. We present a novel camera view generation method which maximizes the visible geometric attributes (segment shape and relative placement between segments). Camera views are created for individual segments and are used to determine local bending angles at each node by projecting them to 2D. The final embedding is generated by minimizing an energy function (the weights of which are user adjustable) based on branch length and the 2D angles, while avoiding intersections. The user can also interactively modify segment placement within the 2D embedding, and the overall embedding will update accordingly. A global to local interactive exploration is provided using hierarchical camera views that are created for subtrees within the structure. We evaluate our method both qualitatively and quantitatively and demonstrate our results by constructing planar visualizations of line data (traced neurons) and volume data (CT vascular and bronchial data).",
                        "uid": "v-tvcg-9721643",
                        "file_name": "v-tvcg-9721643_Hu_Presentation.mp4",
                        "time_stamp": "2022-10-20T21:45:00Z",
                        "time_start": "2022-10-20T21:45:00Z",
                        "time_end": "2022-10-20T21:56:01Z",
                        "paper_type": "full",
                        "keywords": [
                            "I.3.5 Computational Geometry and Object Modeling < I.3 Computer Graphics < I Computing Methodologies, I.6 Simulation, Modeling, and Visualization < I Computing Methodologies"
                        ],
                        "has_image": "1",
                        "has_video": "661",
                        "paper_award": "",
                        "image_caption": "Our planar embedding method for the cranial blood vessels. Left: Illustration of three camera view fields focusing on a certain subtree structure. The blue curves represent a navigation example which connects the three hierarchical views. Right: Planar embedding result using our energy function that preserves the global shape of the object along with its local morphology, while avoiding intersections. The color represents the blood vessel radius of the segment with respect to the average vessel radius of the entire structure.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "https://youtu.be/bb3Tw4a11lQ",
                        "ff_id": "bb3Tw4a11lQ"
                    },
                    {
                        "slot_id": "v-tvcg-9721643-qa",
                        "session_id": "full23",
                        "type": "Virtual Q+A",
                        "title": "Geometry-Aware Planar Embedding of Treelike Structures (Q+A)",
                        "contributors": [
                            "Ping Hu"
                        ],
                        "authors": [],
                        "abstract": "The growing complexity of spatial and structural information in 3D data makes data inspection and visualization a challenging task. We describe a method to create a planar embedding of 3D treelike structures using their skeleton representations. Our method maintains the original geometry, without overlaps, to the best extent possible, allowing exploration of the topology within a single view. We present a novel camera view generation method which maximizes the visible geometric attributes (segment shape and relative placement between segments). Camera views are created for individual segments and are used to determine local bending angles at each node by projecting them to 2D. The final embedding is generated by minimizing an energy function (the weights of which are user adjustable) based on branch length and the 2D angles, while avoiding intersections. The user can also interactively modify segment placement within the 2D embedding, and the overall embedding will update accordingly. A global to local interactive exploration is provided using hierarchical camera views that are created for subtrees within the structure. We evaluate our method both qualitatively and quantitatively and demonstrate our results by constructing planar visualizations of line data (traced neurons) and volume data (CT vascular and bronchial data).",
                        "uid": "v-tvcg-9721643",
                        "file_name": "",
                        "time_stamp": "2022-10-20T21:56:01Z",
                        "time_start": "2022-10-20T21:56:01Z",
                        "time_end": "2022-10-20T21:57:00Z",
                        "paper_type": "full",
                        "keywords": [
                            "I.3.5 Computational Geometry and Object Modeling < I.3 Computer Graphics < I Computing Methodologies, I.6 Simulation, Modeling, and Visualization < I Computing Methodologies"
                        ],
                        "has_image": "1",
                        "has_video": "661",
                        "paper_award": "",
                        "image_caption": "Our planar embedding method for the cranial blood vessels. Left: Illustration of three camera view fields focusing on a certain subtree structure. The blue curves represent a navigation example which connects the three hierarchical views. Right: Planar embedding result using our energy function that preserves the global shape of the object along with its local morphology, while avoiding intersections. The color represents the blood vessel radius of the segment with respect to the average vessel radius of the entire structure.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "https://youtu.be/bb3Tw4a11lQ",
                        "ff_id": "bb3Tw4a11lQ"
                    }
                ]
            },
            {
                "title": "Graphs and Networks",
                "session_id": "full24",
                "event_prefix": "v-full",
                "track": "ok5",
                "livestream_id": "ok5-thu",
                "session_image": "full24.png",
                "chair": [
                    "Andreas Kerren"
                ],
                "organizers": [],
                "time_start": "2022-10-20T15:45:00Z",
                "time_end": "2022-10-20T17:00:00Z",
                "discord_category": "",
                "discord_channel": "oklahoma-st-5",
                "discord_channel_id": "1024600839295356939",
                "discord_link": "https://discord.com/channels/978320435554947082/1024600839295356939",
                "slido_link": "https://app.sli.do/event/i28UETZaiRWr1XwX6eKjcf?section=d355f89b-fbd2-4abf-9958-741607905551",
                "youtube_url": "https://youtu.be/JCJlogloJH8",
                "youtube_id": "JCJlogloJH8",
                "zoom_meeting": "",
                "zoom_password": "",
                "zoom_link": "",
                "ff_link": "https://youtu.be/hJbnpWT_l2I",
                "ff_playlist": "",
                "time_slots": [
                    {
                        "slot_id": "full24-opening",
                        "session_id": "full24",
                        "type": "In Person Other",
                        "title": "Session Opening",
                        "contributors": [
                            "Andreas Kerren"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "",
                        "file_name": "",
                        "time_stamp": "2022-10-20T15:45:00Z",
                        "time_start": "2022-10-20T15:45:00Z",
                        "time_end": "2022-10-20T15:45:00Z",
                        "paper_type": "",
                        "keywords": [],
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "v-full-1551-pres",
                        "session_id": "full24",
                        "type": "In Person Presentation",
                        "title": "MosaicSets: Embedding Set Systems into Grid Graphs",
                        "contributors": [
                            "Markus Wallinger",
                            "Peter Rottmann"
                        ],
                        "authors": [
                            "Peter Rottmann",
                            "Markus Wallinger",
                            "Annika Bonerath",
                            "Sven Gedicke",
                            "Martin N\u00f6llenburg",
                            "Jan-Henrik Haunert"
                        ],
                        "abstract": "Visualizing sets of elements and their relations is an important research area in information visualization. In this paper, we present MosaicSets: a novel approach to create Euler-like diagrams from non-spatial set systems such that each element occupies one cell of a regular hexagonal or square grid. The main challenge is to find an assignment of the elements to the grid cells such that each set constitutes a contiguous region. As use case, we consider the research groups of a university faculty as elements, and the departments and joint research projects as sets. We aim at finding a suitable mapping between the research groups and the grid cells such that the department structure forms a base map layout. Our objectives are to optimize both the compactness of the entirety of all cells and of each set by itself. We show that computing the mapping is NP-hard. However, using integer linear programming we can solve real-world instances optimally within a few seconds. Moreover, we propose a relaxation of the contiguity requirement to visualize otherwise non-embeddable set systems. \n We present and discuss different rendering styles for the set overlays. Based on a case study with real-world data, our evaluation comprises quantitative measures as well as expert interviews.",
                        "uid": "v-full-1551",
                        "file_name": "v-full-1551_Rottmann_Presentation.mp4",
                        "time_stamp": "2022-10-20T15:45:00Z",
                        "time_start": "2022-10-20T15:45:00Z",
                        "time_end": "2022-10-20T15:55:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "474",
                        "paper_award": "",
                        "image_caption": "We present MosaicSets: a novel approach to create Euler-like diagrams from non-spatial set systems such that each element occupies one cell of a regular hexagonal or square grid. The main challenge is to find an assignment of the elements to the grid cells such that each set constitutes a contiguous region. In fact, this task is NP-hard. Thus, we provide an approach using integer linear programming. In the given figure, we visualize the research groups of the Agricultural Faculty of the University of Bonn with MosaicSets once with a hexagonal grid and once with a square grid.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "https://youtu.be/kvvDm_5661Q",
                        "ff_id": "kvvDm_5661Q"
                    },
                    {
                        "slot_id": "v-full-1551-qa",
                        "session_id": "full24",
                        "type": "In Person Q+A",
                        "title": "MosaicSets: Embedding Set Systems into Grid Graphs (Q+A)",
                        "contributors": [
                            "Markus Wallinger",
                            "Peter Rottmann"
                        ],
                        "authors": [],
                        "abstract": "Visualizing sets of elements and their relations is an important research area in information visualization. In this paper, we present MosaicSets: a novel approach to create Euler-like diagrams from non-spatial set systems such that each element occupies one cell of a regular hexagonal or square grid. The main challenge is to find an assignment of the elements to the grid cells such that each set constitutes a contiguous region. As use case, we consider the research groups of a university faculty as elements, and the departments and joint research projects as sets. We aim at finding a suitable mapping between the research groups and the grid cells such that the department structure forms a base map layout. Our objectives are to optimize both the compactness of the entirety of all cells and of each set by itself. We show that computing the mapping is NP-hard. However, using integer linear programming we can solve real-world instances optimally within a few seconds. Moreover, we propose a relaxation of the contiguity requirement to visualize otherwise non-embeddable set systems. \n We present and discuss different rendering styles for the set overlays. Based on a case study with real-world data, our evaluation comprises quantitative measures as well as expert interviews.",
                        "uid": "v-full-1551",
                        "file_name": "",
                        "time_stamp": "2022-10-20T15:55:00Z",
                        "time_start": "2022-10-20T15:55:00Z",
                        "time_end": "2022-10-20T15:57:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "474",
                        "paper_award": "",
                        "image_caption": "We present MosaicSets: a novel approach to create Euler-like diagrams from non-spatial set systems such that each element occupies one cell of a regular hexagonal or square grid. The main challenge is to find an assignment of the elements to the grid cells such that each set constitutes a contiguous region. In fact, this task is NP-hard. Thus, we provide an approach using integer linear programming. In the given figure, we visualize the research groups of the Agricultural Faculty of the University of Bonn with MosaicSets once with a hexagonal grid and once with a square grid.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "https://youtu.be/kvvDm_5661Q",
                        "ff_id": "kvvDm_5661Q"
                    },
                    {
                        "slot_id": "v-full-1165-pres",
                        "session_id": "full24",
                        "type": "Virtual Presentation (pre-recorded)",
                        "title": "Taurus: Towards A Unified Force Representation and Universal Solver for Graph Layout",
                        "contributors": [
                            "Yunhai Wang",
                            "Mingliang Xue"
                        ],
                        "authors": [
                            "Mingliang Xue",
                            "Zhi Wang",
                            "Fahai Zhong",
                            "Yong Wang",
                            "Mingliang Xu",
                            "Oliver Deussen",
                            "Yunhai Wang"
                        ],
                        "abstract": "Over the past few decades, a large number of graph layout techniques have been proposed for visualizing graphs from various domains. In this paper, we present a general framework, Taurus, for unifying popular techniques such as the spring-electrical model, stress model, and maxent-stress model. It is based on a unified force representation, which formulates most existing techniques as a combination of quotient-based forces that combine power functions of graph-theoretical and Euclidean distances. This representation enables us to compare the strengths and weaknesses of existing techniques, while facilitating the development of new methods. Based on this, we propose a new balanced stress model (BSM) that is able to layout graphs in superior quality. In addition, we introduce a universal augmented stochastic gradient descent (SGD) optimizer that efficiently finds proper solutions for all layout techniques. To demonstrate the power of our framework, we conduct a comprehensive evaluation of existing techniques on a large number of synthetic and real graphs. We release an open-source package, which facilitates easy comparison of different graph layout methods for any graph input as well as effectively creating customized graph layout techniques.",
                        "uid": "v-full-1165",
                        "file_name": "v-full-1165_Xue_Presentation.mp4",
                        "time_stamp": "2022-10-20T15:57:00Z",
                        "time_start": "2022-10-20T15:57:00Z",
                        "time_end": "2022-10-20T16:07:57Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "657",
                        "paper_award": "",
                        "image_caption": "In this paper, we present a general framework, which we call Taurus, Towards A Unified force Representation and Universal Solver for graph layout, that offers a unified view for understanding and comparing most of the popular graph layout algorithms. (A) It is based on a unified force representation, which formulates most existing techniques as a combination of quotient-based forces that combine power functions of graph-theoretical and Euclidean distances. (B) This representation enables us to compare the strengths and weaknesses of existing techniques, while facilitating the development of new methods. Based on this, we propose a new balanced stress model (BSM) that is able to layout graphs in superior quality. (C) To demonstrate the effectiveness of our framework, we compare the implementation of each layout method under our framework with its original or existing implementation. The results show that all methods implemented in Taurus (bottom) are similar to or even better than the original implementations (top), with a largely reduced runtime. (D) We release an open-source package, which facilitates easy comparison of different graph layout methods for any graph input as well as effectively creating customized graph layout techniques.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "https://youtu.be/DU4edAM92P8",
                        "ff_id": "DU4edAM92P8"
                    },
                    {
                        "slot_id": "v-full-1165-qa",
                        "session_id": "full24",
                        "type": "Virtual Q+A",
                        "title": "Taurus: Towards A Unified Force Representation and Universal Solver for Graph Layout (Q+A)",
                        "contributors": [
                            "Yunhai Wang",
                            "Mingliang Xue"
                        ],
                        "authors": [],
                        "abstract": "Over the past few decades, a large number of graph layout techniques have been proposed for visualizing graphs from various domains. In this paper, we present a general framework, Taurus, for unifying popular techniques such as the spring-electrical model, stress model, and maxent-stress model. It is based on a unified force representation, which formulates most existing techniques as a combination of quotient-based forces that combine power functions of graph-theoretical and Euclidean distances. This representation enables us to compare the strengths and weaknesses of existing techniques, while facilitating the development of new methods. Based on this, we propose a new balanced stress model (BSM) that is able to layout graphs in superior quality. In addition, we introduce a universal augmented stochastic gradient descent (SGD) optimizer that efficiently finds proper solutions for all layout techniques. To demonstrate the power of our framework, we conduct a comprehensive evaluation of existing techniques on a large number of synthetic and real graphs. We release an open-source package, which facilitates easy comparison of different graph layout methods for any graph input as well as effectively creating customized graph layout techniques.",
                        "uid": "v-full-1165",
                        "file_name": "",
                        "time_stamp": "2022-10-20T16:07:57Z",
                        "time_start": "2022-10-20T16:07:57Z",
                        "time_end": "2022-10-20T16:09:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "657",
                        "paper_award": "",
                        "image_caption": "In this paper, we present a general framework, which we call Taurus, Towards A Unified force Representation and Universal Solver for graph layout, that offers a unified view for understanding and comparing most of the popular graph layout algorithms. (A) It is based on a unified force representation, which formulates most existing techniques as a combination of quotient-based forces that combine power functions of graph-theoretical and Euclidean distances. (B) This representation enables us to compare the strengths and weaknesses of existing techniques, while facilitating the development of new methods. Based on this, we propose a new balanced stress model (BSM) that is able to layout graphs in superior quality. (C) To demonstrate the effectiveness of our framework, we compare the implementation of each layout method under our framework with its original or existing implementation. The results show that all methods implemented in Taurus (bottom) are similar to or even better than the original implementations (top), with a largely reduced runtime. (D) We release an open-source package, which facilitates easy comparison of different graph layout methods for any graph input as well as effectively creating customized graph layout techniques.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "https://youtu.be/DU4edAM92P8",
                        "ff_id": "DU4edAM92P8"
                    },
                    {
                        "slot_id": "v-tvcg-9705082-pres",
                        "session_id": "full24",
                        "type": "Virtual Presentation (pre-recorded)",
                        "title": "Visualizing Graph Neural Networks with CorGIE: Corresponding a Graph to Its Embedding",
                        "contributors": [
                            "Zipeng Liu"
                        ],
                        "authors": [
                            "Zipeng Liu",
                            "Yang Wang",
                            "J\u00fcrgen Bernard",
                            "Tamara Munzner"
                        ],
                        "abstract": "Graph neural networks (GNNs) are a class of powerful machine learning tools that model node relations for making predictions of nodes or links. GNN developers rely on quantitative metrics of the predictions to evaluate a GNN, but similar to many other neural networks, it is difficult for them to understand if the GNN truly learns characteristics of a graph as expected. We propose an approach to corresponding an input graph to its node embedding (aka latent space), a common component of GNNs that is later used for prediction. We abstract the data and tasks, and develop an interactive multi-view interface called CorGIE to instantiate the abstraction. As the key function in CorGIE, we propose the K-hop graph layout to show topological neighbors in hops and their clustering structure. To evaluate the functionality and usability of CorGIE, we present how to use CorGIE in two usage scenarios, and conduct a case study with five GNN experts.",
                        "uid": "v-tvcg-9705082",
                        "file_name": "v-tvcg-9705082_Liu_Presentation.mp4",
                        "time_stamp": "2022-10-20T16:09:00Z",
                        "time_start": "2022-10-20T16:09:00Z",
                        "time_end": "2022-10-20T16:19:00Z",
                        "paper_type": "full",
                        "keywords": [
                            "Visualization for machine learning, graph neural network, graph layout"
                        ],
                        "has_image": "1",
                        "has_video": "600",
                        "paper_award": "",
                        "image_caption": "Full screenshot of CorGIE interface on the Movie dataset, with two focal groups of user nodes.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "https://youtu.be/bcUKFNtc-Yw",
                        "ff_id": "bcUKFNtc-Yw"
                    },
                    {
                        "slot_id": "v-tvcg-9705082-qa",
                        "session_id": "full24",
                        "type": "Virtual Q+A",
                        "title": "Visualizing Graph Neural Networks with CorGIE: Corresponding a Graph to Its Embedding (Q+A)",
                        "contributors": [
                            "Zipeng Liu"
                        ],
                        "authors": [],
                        "abstract": "Graph neural networks (GNNs) are a class of powerful machine learning tools that model node relations for making predictions of nodes or links. GNN developers rely on quantitative metrics of the predictions to evaluate a GNN, but similar to many other neural networks, it is difficult for them to understand if the GNN truly learns characteristics of a graph as expected. We propose an approach to corresponding an input graph to its node embedding (aka latent space), a common component of GNNs that is later used for prediction. We abstract the data and tasks, and develop an interactive multi-view interface called CorGIE to instantiate the abstraction. As the key function in CorGIE, we propose the K-hop graph layout to show topological neighbors in hops and their clustering structure. To evaluate the functionality and usability of CorGIE, we present how to use CorGIE in two usage scenarios, and conduct a case study with five GNN experts.",
                        "uid": "v-tvcg-9705082",
                        "file_name": "",
                        "time_stamp": "2022-10-20T16:19:00Z",
                        "time_start": "2022-10-20T16:19:00Z",
                        "time_end": "2022-10-20T16:21:00Z",
                        "paper_type": "full",
                        "keywords": [
                            "Visualization for machine learning, graph neural network, graph layout"
                        ],
                        "has_image": "1",
                        "has_video": "600",
                        "paper_award": "",
                        "image_caption": "Full screenshot of CorGIE interface on the Movie dataset, with two focal groups of user nodes.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "https://youtu.be/bcUKFNtc-Yw",
                        "ff_id": "bcUKFNtc-Yw"
                    },
                    {
                        "slot_id": "v-full-1541-pres",
                        "session_id": "full24",
                        "type": "In Person Presentation",
                        "title": "Comparative Evaluation of Bipartite, Node-Link, and Matrix-Based Network Representations",
                        "contributors": [
                            "Moataz Abdelaal"
                        ],
                        "authors": [
                            "Moataz Abdelaal",
                            "Nathan D Schiele",
                            "Katrin Angerbauer",
                            "Kuno Kurzhals",
                            "Michael Sedlmair",
                            "Daniel Weiskopf"
                        ],
                        "abstract": "This work investigates and compares the performance of node-link diagrams, adjacency matrices, and bipartite layouts for visualizing networks. In a crowd-sourced user study (n = 150), we measure the task accuracy and completion time of the three representations for different network classes and properties. In contrast to the literature, which covers mostly topology-based tasks (e.g., path finding) in small datasets, we mainly focus on overview tasks for large and directed networks. We consider three overview tasks on networks with 500 nodes: (T1) network class identification, (T2) cluster detection, and (T3) network density estimation, and two detailed tasks: (T4) node in-degree vs. out-degree and (T5) representation mapping, on networks with 50 and 20 nodes, respectively. Our results show that bipartite layouts are beneficial for revealing the overall network structure, while adjacency matrices are most reliable across the different tasks.",
                        "uid": "v-full-1541",
                        "file_name": "v-full-1541_Abdelaal_Presentation.mp4",
                        "time_stamp": "2022-10-20T16:21:00Z",
                        "time_start": "2022-10-20T16:21:00Z",
                        "time_end": "2022-10-20T16:31:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "1347",
                        "paper_award": "",
                        "image_caption": "Three visualization techniques for the representation of large networks. Node-link diagrams and adjacency matrices are\ncommon in visualization. Bipartite layouts have been proposed as an alternative for solving different tasks. We compare all three\ntechniques with respect to different network properties and tasks.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "https://youtu.be/QzNWtkJhnzI",
                        "ff_id": "QzNWtkJhnzI"
                    },
                    {
                        "slot_id": "v-full-1541-qa",
                        "session_id": "full24",
                        "type": "In Person Q+A",
                        "title": "Comparative Evaluation of Bipartite, Node-Link, and Matrix-Based Network Representations (Q+A)",
                        "contributors": [
                            "Moataz Abdelaal"
                        ],
                        "authors": [],
                        "abstract": "This work investigates and compares the performance of node-link diagrams, adjacency matrices, and bipartite layouts for visualizing networks. In a crowd-sourced user study (n = 150), we measure the task accuracy and completion time of the three representations for different network classes and properties. In contrast to the literature, which covers mostly topology-based tasks (e.g., path finding) in small datasets, we mainly focus on overview tasks for large and directed networks. We consider three overview tasks on networks with 500 nodes: (T1) network class identification, (T2) cluster detection, and (T3) network density estimation, and two detailed tasks: (T4) node in-degree vs. out-degree and (T5) representation mapping, on networks with 50 and 20 nodes, respectively. Our results show that bipartite layouts are beneficial for revealing the overall network structure, while adjacency matrices are most reliable across the different tasks.",
                        "uid": "v-full-1541",
                        "file_name": "",
                        "time_stamp": "2022-10-20T16:31:00Z",
                        "time_start": "2022-10-20T16:31:00Z",
                        "time_end": "2022-10-20T16:33:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "1347",
                        "paper_award": "",
                        "image_caption": "Three visualization techniques for the representation of large networks. Node-link diagrams and adjacency matrices are\ncommon in visualization. Bipartite layouts have been proposed as an alternative for solving different tasks. We compare all three\ntechniques with respect to different network properties and tasks.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "https://youtu.be/QzNWtkJhnzI",
                        "ff_id": "QzNWtkJhnzI"
                    },
                    {
                        "slot_id": "v-full-1154-pres",
                        "session_id": "full24",
                        "type": "In Person Presentation",
                        "title": "Understanding Barriers to Network Exploration with Visualization: A Report from the Trenches",
                        "contributors": [
                            "Mashael AlKadi"
                        ],
                        "authors": [
                            "Mashael AlKadi",
                            "Vanessa Serrano",
                            "James Scott-Brown",
                            "Uta Hinrichs",
                            "Catherine Plaisant",
                            "Jean-Daniel Fekete",
                            "Benjamin Bach"
                        ],
                        "abstract": "This article reports on an in-depth study that investigates barriers to network exploration with visualizations. Network visualization tools are becoming increasingly popular, but little is known about how analysts plan and engage in the visual exploration of network data\u2014which exploration strategies they employ, and how they prepare their data, define questions, and decide on visual mappings. Our study involved a series of workshops, interaction logging, and observations from a 6-week network exploration course. Our findings shed light on the stages that define analysts\u2019 approaches to network visualization and barriers experienced by some analysts during their network visualization processes. These barriers mainly appear before using a specific tool and include defining exploration goals, identifying relevant network structures and abstractions, or creating appropriate visual mappings for their network data. Our findings inform future work in visualization education and analyst-centered network visualization tool design.",
                        "uid": "v-full-1154",
                        "file_name": "v-full-1154_Alkadi_Presentation.mp4",
                        "time_stamp": "2022-10-20T16:33:00Z",
                        "time_start": "2022-10-20T16:33:00Z",
                        "time_end": "2022-10-20T16:43:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "582",
                        "paper_award": "",
                        "image_caption": "Visual Network Exploration Model",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "https://youtu.be/Epml77ggwoQ",
                        "ff_id": "Epml77ggwoQ"
                    },
                    {
                        "slot_id": "v-full-1154-qa",
                        "session_id": "full24",
                        "type": "In Person Q+A",
                        "title": "Understanding Barriers to Network Exploration with Visualization: A Report from the Trenches (Q+A)",
                        "contributors": [
                            "Mashael AlKadi"
                        ],
                        "authors": [],
                        "abstract": "This article reports on an in-depth study that investigates barriers to network exploration with visualizations. Network visualization tools are becoming increasingly popular, but little is known about how analysts plan and engage in the visual exploration of network data\u2014which exploration strategies they employ, and how they prepare their data, define questions, and decide on visual mappings. Our study involved a series of workshops, interaction logging, and observations from a 6-week network exploration course. Our findings shed light on the stages that define analysts\u2019 approaches to network visualization and barriers experienced by some analysts during their network visualization processes. These barriers mainly appear before using a specific tool and include defining exploration goals, identifying relevant network structures and abstractions, or creating appropriate visual mappings for their network data. Our findings inform future work in visualization education and analyst-centered network visualization tool design.",
                        "uid": "v-full-1154",
                        "file_name": "",
                        "time_stamp": "2022-10-20T16:43:00Z",
                        "time_start": "2022-10-20T16:43:00Z",
                        "time_end": "2022-10-20T16:45:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "582",
                        "paper_award": "",
                        "image_caption": "Visual Network Exploration Model",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "https://youtu.be/Epml77ggwoQ",
                        "ff_id": "Epml77ggwoQ"
                    },
                    {
                        "slot_id": "v-tvcg-9720180-pres",
                        "session_id": "full24",
                        "type": "Virtual Presentation (pre-recorded)",
                        "title": "VividGraph: Learning to Extract and Redesign Network Graphs from Visualization Images",
                        "contributors": [
                            "Sicheng Song"
                        ],
                        "authors": [
                            "Sicheng Song",
                            "Chenhui Li",
                            "Yujing Sun",
                            "Changbo Wang"
                        ],
                        "abstract": "Network graphs are common visualization charts. They often appear in the form of bitmaps in papers, web pages, magazine prints, and designer sketches. People often want to modify network graphs because of their poor design, but it is difficult to obtain their underlying data. In this paper, we present VividGraph, a pipeline for automatically extracting and redesigning network graphs from static images. We propose using convolutional neural networks to solve the problem of network graph data extraction. Our method is robust to hand-drawn graphs, blurred graph images, and large graph images. We also present a network graph classification module to make it effective for directed graphs. We propose two evaluation methods to demonstrate the effectiveness of our approach. It can be used to quickly transform designer sketches, extract underlying data from existing network graphs, and interactively redesign poorly designed network graphs.",
                        "uid": "v-tvcg-9720180",
                        "file_name": "v-tvcg-9720180_Song_Presentation.mp4",
                        "time_stamp": "2022-10-20T16:45:00Z",
                        "time_start": "2022-10-20T16:45:00Z",
                        "time_end": "2022-10-20T16:54:07Z",
                        "paper_type": "full",
                        "keywords": [
                            "Information visualization , Network graph , Data extraction , Chart recognition , Semantic segmentation , Redesign"
                        ],
                        "has_image": "1",
                        "has_video": "547",
                        "paper_award": "",
                        "image_caption": "VividGraph can be used in many practical applications. The input is a bitmap. Through our semantic segmentation and connection algorithm, we can obtain its underlying data. Using the extracted data, we can reconstruct the vector of the graph and redesign the chart, such as recoloring, re-layout, and data modification.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "https://youtu.be/t_6ax4cFjfg",
                        "ff_id": "t_6ax4cFjfg"
                    },
                    {
                        "slot_id": "v-tvcg-9720180-qa",
                        "session_id": "full24",
                        "type": "Virtual Q+A",
                        "title": "VividGraph: Learning to Extract and Redesign Network Graphs from Visualization Images (Q+A)",
                        "contributors": [
                            "Sicheng Song"
                        ],
                        "authors": [],
                        "abstract": "Network graphs are common visualization charts. They often appear in the form of bitmaps in papers, web pages, magazine prints, and designer sketches. People often want to modify network graphs because of their poor design, but it is difficult to obtain their underlying data. In this paper, we present VividGraph, a pipeline for automatically extracting and redesigning network graphs from static images. We propose using convolutional neural networks to solve the problem of network graph data extraction. Our method is robust to hand-drawn graphs, blurred graph images, and large graph images. We also present a network graph classification module to make it effective for directed graphs. We propose two evaluation methods to demonstrate the effectiveness of our approach. It can be used to quickly transform designer sketches, extract underlying data from existing network graphs, and interactively redesign poorly designed network graphs.",
                        "uid": "v-tvcg-9720180",
                        "file_name": "",
                        "time_stamp": "2022-10-20T16:54:07Z",
                        "time_start": "2022-10-20T16:54:07Z",
                        "time_end": "2022-10-20T16:57:00Z",
                        "paper_type": "full",
                        "keywords": [
                            "Information visualization , Network graph , Data extraction , Chart recognition , Semantic segmentation , Redesign"
                        ],
                        "has_image": "1",
                        "has_video": "547",
                        "paper_award": "",
                        "image_caption": "VividGraph can be used in many practical applications. The input is a bitmap. Through our semantic segmentation and connection algorithm, we can obtain its underlying data. Using the extracted data, we can reconstruct the vector of the graph and redesign the chart, such as recoloring, re-layout, and data modification.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "https://youtu.be/t_6ax4cFjfg",
                        "ff_id": "t_6ax4cFjfg"
                    }
                ]
            },
            {
                "title": "Visualization Design",
                "session_id": "full25",
                "event_prefix": "v-full",
                "track": "ok5",
                "livestream_id": "ok5-wed",
                "session_image": "full25.png",
                "chair": [
                    "Miriah Meyer"
                ],
                "organizers": [],
                "time_start": "2022-10-19T15:45:00Z",
                "time_end": "2022-10-19T17:00:00Z",
                "discord_category": "",
                "discord_channel": "oklahoma-st-5",
                "discord_channel_id": "1024600839295356939",
                "discord_link": "https://discord.com/channels/978320435554947082/1024600839295356939",
                "slido_link": "https://app.sli.do/event/i28UETZaiRWr1XwX6eKjcf?section=d355f89b-fbd2-4abf-9958-741607905551",
                "youtube_url": "https://youtu.be/wr1l85Jo7jM",
                "youtube_id": "wr1l85Jo7jM",
                "zoom_meeting": "",
                "zoom_password": "",
                "zoom_link": "",
                "ff_link": "https://youtu.be/0XuCHQ7c_e4",
                "ff_playlist": "",
                "time_slots": [
                    {
                        "slot_id": "full25-opening",
                        "session_id": "full25",
                        "type": "In Person Other",
                        "title": "Session Opening",
                        "contributors": [
                            "Miriah Meyer"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "",
                        "file_name": "",
                        "time_stamp": "2022-10-19T15:45:00Z",
                        "time_start": "2022-10-19T15:45:00Z",
                        "time_end": "2022-10-19T15:45:00Z",
                        "paper_type": "",
                        "keywords": [],
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "v-tvcg-9508898-pres",
                        "session_id": "full25",
                        "type": "Virtual Presentation (pre-recorded)",
                        "title": "Towards Systematic Design Considerations for Visualizing Cross-View Data Relationships",
                        "contributors": [
                            "Maoyuan Sun"
                        ],
                        "authors": [
                            "Maoyuan Sun",
                            "Akhil Namburi",
                            "David Koop",
                            "Jian Zhao",
                            "Tianyi Li",
                            "Haeyong Chung"
                        ],
                        "abstract": "Due to the scale of data and the complexity of analysis tasks, insight discovery often requires coordinating multiple visualizations (views), with each view displaying different parts of data or the same data from different perspectives. For example, to analyze car sales records, a marketing analyst uses a line chart to visualize the trend of car sales, a scatterplot to inspect the price and horsepower of different cars, and a matrix to compare the transaction amounts in types of deals. To explore related information across multiple views, current visual analysis tools heavily rely on brushing and linking techniques, which may require a significant amount of user effort (e.g., many trial-and-error attempts). There may be other efficient and effective ways of displaying cross-view data relationships to support data analysis with multiple views, but currently there are no guidelines to address this design challenge. In this paper, we present systematic design considerations for visualizing cross-view data relationships, which leverages descriptive aspects of relationships and usable visual context of multi-view visualizations. We discuss pros and cons of different designs for showing cross-view data relationships, and provide a set of recommendations for helping practitioners make design decisions.",
                        "uid": "v-tvcg-9508898",
                        "file_name": "v-tvcg-9508898_Sun_Presentation.mp4",
                        "time_stamp": "2022-10-19T15:45:00Z",
                        "time_start": "2022-10-19T15:45:00Z",
                        "time_end": "2022-10-19T15:54:00Z",
                        "paper_type": "full",
                        "keywords": [
                            "Visualization, Data visualization, Task analysis, Organizations, Systematics, Periodic structures, Computer science"
                        ],
                        "has_image": "1",
                        "has_video": "540",
                        "paper_award": "",
                        "image_caption": "A descriptive framework of data relationships with four aspects.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "https://youtu.be/PwyNIT-aM18",
                        "ff_id": "PwyNIT-aM18"
                    },
                    {
                        "slot_id": "v-tvcg-9508898-qa",
                        "session_id": "full25",
                        "type": "Virtual Q+A",
                        "title": "Towards Systematic Design Considerations for Visualizing Cross-View Data Relationships (Q+A)",
                        "contributors": [
                            "Maoyuan Sun"
                        ],
                        "authors": [],
                        "abstract": "Due to the scale of data and the complexity of analysis tasks, insight discovery often requires coordinating multiple visualizations (views), with each view displaying different parts of data or the same data from different perspectives. For example, to analyze car sales records, a marketing analyst uses a line chart to visualize the trend of car sales, a scatterplot to inspect the price and horsepower of different cars, and a matrix to compare the transaction amounts in types of deals. To explore related information across multiple views, current visual analysis tools heavily rely on brushing and linking techniques, which may require a significant amount of user effort (e.g., many trial-and-error attempts). There may be other efficient and effective ways of displaying cross-view data relationships to support data analysis with multiple views, but currently there are no guidelines to address this design challenge. In this paper, we present systematic design considerations for visualizing cross-view data relationships, which leverages descriptive aspects of relationships and usable visual context of multi-view visualizations. We discuss pros and cons of different designs for showing cross-view data relationships, and provide a set of recommendations for helping practitioners make design decisions.",
                        "uid": "v-tvcg-9508898",
                        "file_name": "",
                        "time_stamp": "2022-10-19T15:54:00Z",
                        "time_start": "2022-10-19T15:54:00Z",
                        "time_end": "2022-10-19T15:57:00Z",
                        "paper_type": "full",
                        "keywords": [
                            "Visualization, Data visualization, Task analysis, Organizations, Systematics, Periodic structures, Computer science"
                        ],
                        "has_image": "1",
                        "has_video": "540",
                        "paper_award": "",
                        "image_caption": "A descriptive framework of data relationships with four aspects.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "https://youtu.be/PwyNIT-aM18",
                        "ff_id": "PwyNIT-aM18"
                    },
                    {
                        "slot_id": "v-tvcg-9465643-pres",
                        "session_id": "full25",
                        "type": "Virtual Presentation (pre-recorded)",
                        "title": "Designing with Pictographs: Envision Topics without Sacrificing Understanding",
                        "contributors": [
                            "Alyx Burns"
                        ],
                        "authors": [
                            "Alyxander Burns",
                            "Cindy Xiong",
                            "Steven Franconeri",
                            "Alberto Cairo",
                            "Narges Mahyar"
                        ],
                        "abstract": "Past studies have shown that when a visualization uses pictographs to encode data, they have a positive effect on memory, engagement, and assessment of risk. However, little is known about how pictographs affect one's ability to understand a visualization, beyond memory for values and trends. We conducted two crowdsourced experiments to compare the effectiveness of using pictographs when showing part-to-whole relationships. In Experiment 1, we compared pictograph arrays to more traditional bar and pie charts. We tested participants' ability to generate high-level insights following Bloom's taxonomy of educational objectives via 6 free-response questions. We found that accuracy for extracting information and generating insights did not differ overall between the two versions. To explore the motivating differences between the designs, we conducted a second experiment where participants compared charts containing pictograph arrays to more traditional charts on 5 metrics and explained their reasoning. We found that some participants preferred the way that pictographs allowed them to envision the topic more easily, while others preferred traditional bar and pie charts because they seem less cluttered and faster to read. These results suggest that, at least in simple visualizations depicting part-to-whole relationships, the choice of using pictographs has little influence on sensemaking and insight extraction. When deciding whether to use pictograph arrays, designers should consider visual appeal, perceived comprehension time, ease of envisioning the topic, and clutteredness.",
                        "uid": "v-tvcg-9465643",
                        "file_name": "v-tvcg-9465643_Burns_Presentation.mp4",
                        "time_stamp": "2022-10-19T15:57:00Z",
                        "time_start": "2022-10-19T15:57:00Z",
                        "time_end": "2022-10-19T16:05:16Z",
                        "paper_type": "full",
                        "keywords": [
                            "Infographics, pictographs, design, graph comprehension, understanding, casual sensemaking"
                        ],
                        "has_image": "1",
                        "has_video": "496",
                        "paper_award": "",
                        "image_caption": "What impact do pictographs have on the ways that the general public makes sense of visualizations? In this paper, we conducted two crowdsourced experiments with 6 pairs of real-world visualizations to compare the effectiveness of using pictographs instead of more traditional abstract shapes when showing part-to-whole relationships.  Our results suggest that pictographs have little influence on sensemaking and insight extraction, but influence perceived visual appeal, comprehension time, ease of envisioning the topic, and clutteredness.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "https://youtu.be/kDgKwhn-74E",
                        "ff_id": "kDgKwhn-74E"
                    },
                    {
                        "slot_id": "v-tvcg-9465643-qa",
                        "session_id": "full25",
                        "type": "Virtual Q+A",
                        "title": "Designing with Pictographs: Envision Topics without Sacrificing Understanding (Q+A)",
                        "contributors": [
                            "Alyx Burns"
                        ],
                        "authors": [],
                        "abstract": "Past studies have shown that when a visualization uses pictographs to encode data, they have a positive effect on memory, engagement, and assessment of risk. However, little is known about how pictographs affect one's ability to understand a visualization, beyond memory for values and trends. We conducted two crowdsourced experiments to compare the effectiveness of using pictographs when showing part-to-whole relationships. In Experiment 1, we compared pictograph arrays to more traditional bar and pie charts. We tested participants' ability to generate high-level insights following Bloom's taxonomy of educational objectives via 6 free-response questions. We found that accuracy for extracting information and generating insights did not differ overall between the two versions. To explore the motivating differences between the designs, we conducted a second experiment where participants compared charts containing pictograph arrays to more traditional charts on 5 metrics and explained their reasoning. We found that some participants preferred the way that pictographs allowed them to envision the topic more easily, while others preferred traditional bar and pie charts because they seem less cluttered and faster to read. These results suggest that, at least in simple visualizations depicting part-to-whole relationships, the choice of using pictographs has little influence on sensemaking and insight extraction. When deciding whether to use pictograph arrays, designers should consider visual appeal, perceived comprehension time, ease of envisioning the topic, and clutteredness.",
                        "uid": "v-tvcg-9465643",
                        "file_name": "",
                        "time_stamp": "2022-10-19T16:05:16Z",
                        "time_start": "2022-10-19T16:05:16Z",
                        "time_end": "2022-10-19T16:09:00Z",
                        "paper_type": "full",
                        "keywords": [
                            "Infographics, pictographs, design, graph comprehension, understanding, casual sensemaking"
                        ],
                        "has_image": "1",
                        "has_video": "496",
                        "paper_award": "",
                        "image_caption": "What impact do pictographs have on the ways that the general public makes sense of visualizations? In this paper, we conducted two crowdsourced experiments with 6 pairs of real-world visualizations to compare the effectiveness of using pictographs instead of more traditional abstract shapes when showing part-to-whole relationships.  Our results suggest that pictographs have little influence on sensemaking and insight extraction, but influence perceived visual appeal, comprehension time, ease of envisioning the topic, and clutteredness.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "https://youtu.be/kDgKwhn-74E",
                        "ff_id": "kDgKwhn-74E"
                    },
                    {
                        "slot_id": "v-tvcg-9737134-pres",
                        "session_id": "full25",
                        "type": "In Person Presentation",
                        "title": "Visualizing Higher-Order 3D Tensors by Multipole Lines",
                        "contributors": [
                            "Chiara Hergl"
                        ],
                        "authors": [
                            "Chiara Hergl",
                            "Thomas Nagel",
                            "Gerik Scheuermann"
                        ],
                        "abstract": "Physics, medicine, earth sciences, mechanical engineering, geo-engineering, bio-engineering and many more application areas use tensorial data. For example, tensors are used in formulating the balance equations of charge, mass, momentum, or energy as well as the constitutive relations that complement them. Some of these tensors (i.e. stiffness tensor, strain gradient, photo-elastic tensor) are of order higher than two. Currently, there are nearly no visualization techniques for such data beyond glyphs. An important reason for this is the limit of currently used tensor decomposition techniques. In this article, we propose to use the deviatoric decomposition to draw lines describing tensors of arbitrary order in three dimensions. The deviatoric decomposition splits a three-dimensional tensor of any order with any type of index symmetry into totally symmetric, traceless tensors. These tensors, called deviators, can be described by a unique set of directions (called multipoles by J.~C. Maxwell) and scalars. These multipoles allow the definition of multipole lines which can be computed in a similar fashion to tensor lines and allow a line-based visualization of three-dimensional tensors of any order. We give examples for the visualization of symmetric, second-order tensor fields as well as fourth-order tensor fields. To allow an interpretation of the multipole lines, we analyze the connection between the multipoles and the eigenvectors/eigenvalues in the second-order case. For the fourth-order stiffness tensor, we prove relations between multipoles and important physical quantities such as shear moduli as well as the eigenvectors of the second-order right Cauchy-Green tensor.",
                        "uid": "v-tvcg-9737134",
                        "file_name": "v-tvcg-9737134_Hergl_Presentation.mp4",
                        "time_stamp": "2022-10-19T16:09:00Z",
                        "time_start": "2022-10-19T16:09:00Z",
                        "time_end": "2022-10-19T16:19:00Z",
                        "paper_type": "full",
                        "keywords": [
                            "tensor algebra, higher-order tensor, line-based, deviatoric decomposition, anisotropy"
                        ],
                        "has_image": "1",
                        "has_video": "714",
                        "paper_award": "",
                        "image_caption": "Stiffness tensor visualization of an indentation test using a soft biological material. The fourth-order three-dimensional tensor is decomposed into so called deviators. Each deviator is represented by multipole lines. The figure depicts the multipole lines corresponding to the second-order deviator describing the asymmetric part.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "https://youtu.be/zmYIJQFDnRM",
                        "ff_id": "zmYIJQFDnRM"
                    },
                    {
                        "slot_id": "v-tvcg-9737134-qa",
                        "session_id": "full25",
                        "type": "In Person Q+A",
                        "title": "Visualizing Higher-Order 3D Tensors by Multipole Lines (Q+A)",
                        "contributors": [
                            "Chiara Hergl"
                        ],
                        "authors": [],
                        "abstract": "Physics, medicine, earth sciences, mechanical engineering, geo-engineering, bio-engineering and many more application areas use tensorial data. For example, tensors are used in formulating the balance equations of charge, mass, momentum, or energy as well as the constitutive relations that complement them. Some of these tensors (i.e. stiffness tensor, strain gradient, photo-elastic tensor) are of order higher than two. Currently, there are nearly no visualization techniques for such data beyond glyphs. An important reason for this is the limit of currently used tensor decomposition techniques. In this article, we propose to use the deviatoric decomposition to draw lines describing tensors of arbitrary order in three dimensions. The deviatoric decomposition splits a three-dimensional tensor of any order with any type of index symmetry into totally symmetric, traceless tensors. These tensors, called deviators, can be described by a unique set of directions (called multipoles by J.~C. Maxwell) and scalars. These multipoles allow the definition of multipole lines which can be computed in a similar fashion to tensor lines and allow a line-based visualization of three-dimensional tensors of any order. We give examples for the visualization of symmetric, second-order tensor fields as well as fourth-order tensor fields. To allow an interpretation of the multipole lines, we analyze the connection between the multipoles and the eigenvectors/eigenvalues in the second-order case. For the fourth-order stiffness tensor, we prove relations between multipoles and important physical quantities such as shear moduli as well as the eigenvectors of the second-order right Cauchy-Green tensor.",
                        "uid": "v-tvcg-9737134",
                        "file_name": "",
                        "time_stamp": "2022-10-19T16:19:00Z",
                        "time_start": "2022-10-19T16:19:00Z",
                        "time_end": "2022-10-19T16:21:00Z",
                        "paper_type": "full",
                        "keywords": [
                            "tensor algebra, higher-order tensor, line-based, deviatoric decomposition, anisotropy"
                        ],
                        "has_image": "1",
                        "has_video": "714",
                        "paper_award": "",
                        "image_caption": "Stiffness tensor visualization of an indentation test using a soft biological material. The fourth-order three-dimensional tensor is decomposed into so called deviators. Each deviator is represented by multipole lines. The figure depicts the multipole lines corresponding to the second-order deviator describing the asymmetric part.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "https://youtu.be/zmYIJQFDnRM",
                        "ff_id": "zmYIJQFDnRM"
                    },
                    {
                        "slot_id": "v-full-1335-pres",
                        "session_id": "full25",
                        "type": "In Person Presentation",
                        "title": "MetaGlyph: Automatic Generation of Metaphoric Glyph-based Visualization",
                        "contributors": [
                            "Lu Ying",
                            "Xinhuan Shu"
                        ],
                        "authors": [
                            "Lu Ying",
                            "Yuchen Yang",
                            "Xinhuan Shu",
                            "Dazhen Deng",
                            "Tan Tang",
                            "Lingyun Yu",
                            "Yingcai Wu"
                        ],
                        "abstract": "Glyph-based visualization achieves an impressive graphic design when associated with comprehensive visual metaphors, which help audiences effectively grasp the conveyed information through revealing data semantics. However, creating such metaphoric glyph-based visualization (MGV) is not an easy task, as it requires not only a deep understanding of data but also professional design skills. This paper proposes MetaGlyph, an automatic system for generating MGVs from a spreadsheet. To develop MetaGlyph, we first conduct a qualitative analysis to understand the design of current MGVs from the perspectives of metaphor embodiment and glyph design. Based on the results, we introduce a novel framework for generating MGVs by metaphoric image selection and an MGV construction. Specifically, MetaGlyph automatically selects metaphors with corresponding images from online resources based on the input data semantics. We then integrate a Monte Carlo tree search algorithm that explores the design of an MGV by associating visual elements with data dimensions given the data importance, semantic relevance, and glyph non-overlap. The system also provides editing feedback that allows users to customize the MGVs according to their design preferences. We demonstrate the use of MetaGlyph through a set of examples, one usage scenario, and validate its effectiveness through a series of expert interviews.",
                        "uid": "v-full-1335",
                        "file_name": "v-full-1335_Ying_Presentation.mp4",
                        "time_stamp": "2022-10-19T16:21:00Z",
                        "time_start": "2022-10-19T16:21:00Z",
                        "time_end": "2022-10-19T16:31:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "546",
                        "paper_award": "",
                        "image_caption": "Example MGVs generated by MetaGlyph: (a) several attributes of different pokemons; (b) forest area changes in different countries from 1995 to 2020; (c) display of various CDs; (e) multiple hotels\u2019 information arranged by price (x) and rate (y); (f) chocolates in different ratings placed on a map; (g) disparate mushrooms. The legends in (a1)(b1)(f1) are also generated by MetaGlyph. The center boxes with yellow backgrounds illustrate the data mappings for each glyph. Encoding channels are represented in icons and (d) shows the annotation. Dashed arrows are used to associate visual elements with the resulting visualizations.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "https://youtu.be/7VAaN2HhsJw",
                        "ff_id": "7VAaN2HhsJw"
                    },
                    {
                        "slot_id": "v-full-1335-qa",
                        "session_id": "full25",
                        "type": "In Person Q+A",
                        "title": "MetaGlyph: Automatic Generation of Metaphoric Glyph-based Visualization (Q+A)",
                        "contributors": [
                            "Lu Ying",
                            "Xinhuan Shu"
                        ],
                        "authors": [],
                        "abstract": "Glyph-based visualization achieves an impressive graphic design when associated with comprehensive visual metaphors, which help audiences effectively grasp the conveyed information through revealing data semantics. However, creating such metaphoric glyph-based visualization (MGV) is not an easy task, as it requires not only a deep understanding of data but also professional design skills. This paper proposes MetaGlyph, an automatic system for generating MGVs from a spreadsheet. To develop MetaGlyph, we first conduct a qualitative analysis to understand the design of current MGVs from the perspectives of metaphor embodiment and glyph design. Based on the results, we introduce a novel framework for generating MGVs by metaphoric image selection and an MGV construction. Specifically, MetaGlyph automatically selects metaphors with corresponding images from online resources based on the input data semantics. We then integrate a Monte Carlo tree search algorithm that explores the design of an MGV by associating visual elements with data dimensions given the data importance, semantic relevance, and glyph non-overlap. The system also provides editing feedback that allows users to customize the MGVs according to their design preferences. We demonstrate the use of MetaGlyph through a set of examples, one usage scenario, and validate its effectiveness through a series of expert interviews.",
                        "uid": "v-full-1335",
                        "file_name": "",
                        "time_stamp": "2022-10-19T16:31:00Z",
                        "time_start": "2022-10-19T16:31:00Z",
                        "time_end": "2022-10-19T16:33:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "546",
                        "paper_award": "",
                        "image_caption": "Example MGVs generated by MetaGlyph: (a) several attributes of different pokemons; (b) forest area changes in different countries from 1995 to 2020; (c) display of various CDs; (e) multiple hotels\u2019 information arranged by price (x) and rate (y); (f) chocolates in different ratings placed on a map; (g) disparate mushrooms. The legends in (a1)(b1)(f1) are also generated by MetaGlyph. The center boxes with yellow backgrounds illustrate the data mappings for each glyph. Encoding channels are represented in icons and (d) shows the annotation. Dashed arrows are used to associate visual elements with the resulting visualizations.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "https://youtu.be/7VAaN2HhsJw",
                        "ff_id": "7VAaN2HhsJw"
                    },
                    {
                        "slot_id": "v-full-1554-pres",
                        "session_id": "full25",
                        "type": "In Person Presentation",
                        "title": "Dashboard Design Patterns",
                        "contributors": [
                            "Benjamin Bach"
                        ],
                        "authors": [
                            "Benjamin Bach",
                            "Euan Freeman",
                            "Alfie Abdul-Rahman",
                            "Cagatay Turkay",
                            "Saiful Khan",
                            "Yulei Fan",
                            "Min Chen"
                        ],
                        "abstract": "This paper introduces design patterns for dashboards to inform dashboard design processes. Despite a growing number of public examples, case studies, and general guidelines there is surprisingly little design guidance for dashboards. Such guidance is necessary to inspire designs and discuss tradeoffs in, e.g., screenspace, interaction, or information shown. Based on a systematic review of 144 dashboards, we report on eight groups of design patterns that provide common solutions in dashboard design. We discuss combinations of these patterns in \u201cdashboard genres\u201d such as narrative, analytical, or embedded dashboard. We ran a 2-week dashboard design workshop with 23 participants of varying expertise working on their own data and dashboards. We discuss the application of patterns for the dashboard design processes, as well as general design tradeoffs and common challenges. Our work complements previous surveys and aims to support dashboard designers and researchers in co-creation, structured design decisions, as well as future user evaluations about dashboard design guidelines. Detailed pattern descriptions and workshop material can be found online: https://dashboarddesignpatterns.github.io.",
                        "uid": "v-full-1554",
                        "file_name": "v-full-1554_Bach_Presentation.mp4",
                        "time_stamp": "2022-10-19T16:33:00Z",
                        "time_start": "2022-10-19T16:33:00Z",
                        "time_end": "2022-10-19T16:43:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "853",
                        "paper_award": "",
                        "image_caption": "Dashboard design patterns grouped into eight categories. More info on our website https://dashboarddesignpatterns.github.io.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "https://youtu.be/igHTCf93aa8",
                        "ff_id": "igHTCf93aa8"
                    },
                    {
                        "slot_id": "v-full-1554-qa",
                        "session_id": "full25",
                        "type": "In Person Q+A",
                        "title": "Dashboard Design Patterns (Q+A)",
                        "contributors": [
                            "Benjamin Bach"
                        ],
                        "authors": [],
                        "abstract": "This paper introduces design patterns for dashboards to inform dashboard design processes. Despite a growing number of public examples, case studies, and general guidelines there is surprisingly little design guidance for dashboards. Such guidance is necessary to inspire designs and discuss tradeoffs in, e.g., screenspace, interaction, or information shown. Based on a systematic review of 144 dashboards, we report on eight groups of design patterns that provide common solutions in dashboard design. We discuss combinations of these patterns in \u201cdashboard genres\u201d such as narrative, analytical, or embedded dashboard. We ran a 2-week dashboard design workshop with 23 participants of varying expertise working on their own data and dashboards. We discuss the application of patterns for the dashboard design processes, as well as general design tradeoffs and common challenges. Our work complements previous surveys and aims to support dashboard designers and researchers in co-creation, structured design decisions, as well as future user evaluations about dashboard design guidelines. Detailed pattern descriptions and workshop material can be found online: https://dashboarddesignpatterns.github.io.",
                        "uid": "v-full-1554",
                        "file_name": "",
                        "time_stamp": "2022-10-19T16:43:00Z",
                        "time_start": "2022-10-19T16:43:00Z",
                        "time_end": "2022-10-19T16:45:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "853",
                        "paper_award": "",
                        "image_caption": "Dashboard design patterns grouped into eight categories. More info on our website https://dashboarddesignpatterns.github.io.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "https://youtu.be/igHTCf93aa8",
                        "ff_id": "igHTCf93aa8"
                    },
                    {
                        "slot_id": "v-full-1206-pres",
                        "session_id": "full25",
                        "type": "Virtual Presentation (pre-recorded)",
                        "title": "A Framework for Multiclass Contour Visualization",
                        "contributors": [
                            "Sihang Li"
                        ],
                        "authors": [
                            "Sihang Li",
                            "Jiacheng Yu",
                            "Mingxuan Li",
                            "Le Liu",
                            "Xiaolong (Luke) Zhang",
                            "Xiaoru Yuan"
                        ],
                        "abstract": "Multiclass contour visualization is often used to interpret complex data attributes in such fields as weather forecasting, computational fluid dynamics, and artificial intelligence. However, effective and accurate representations of underlying data patterns and correlations can be challenging in multiclass contour visualization, primarily due to the inevitable visual cluttering and occlusions when the number of classes is significant. To address this issue, visualization design must carefully choose design parameters to make visualization more comprehensible. With this goal in mind, we proposed a framework for multiclass contour visualization. The framework has two components: a set of four visualization design parameters, which are developed based on an extensive review of literature on contour visualization, and a declarative domain-specific language (DSL) for creating multiclass contour rendering, which enables a fast exploration of those design parameters. A task-oriented user study was conducted to assess how those design parameters affect users' interpretations of real-world data. The study results offered some suggestions on the value choices of design parameters in multiclass contour visualization.",
                        "uid": "v-full-1206",
                        "file_name": "v-full-1206_Li_Presentation.mp4",
                        "time_stamp": "2022-10-19T16:45:00Z",
                        "time_start": "2022-10-19T16:45:00Z",
                        "time_end": "2022-10-19T16:54:22Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "562",
                        "paper_award": "",
                        "image_caption": "Variations of three design parameters under our proposed multiclass contour visualization framework. The contours are drawn using our declarative DSL. Through a user study, it can be learned that different choices of these three parameters affect the user's ability to complete different tasks.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "https://youtu.be/ZUFIRLy3McE",
                        "ff_id": "ZUFIRLy3McE"
                    },
                    {
                        "slot_id": "v-full-1206-qa",
                        "session_id": "full25",
                        "type": "Virtual Q+A",
                        "title": "A Framework for Multiclass Contour Visualization (Q+A)",
                        "contributors": [
                            "Sihang Li"
                        ],
                        "authors": [],
                        "abstract": "Multiclass contour visualization is often used to interpret complex data attributes in such fields as weather forecasting, computational fluid dynamics, and artificial intelligence. However, effective and accurate representations of underlying data patterns and correlations can be challenging in multiclass contour visualization, primarily due to the inevitable visual cluttering and occlusions when the number of classes is significant. To address this issue, visualization design must carefully choose design parameters to make visualization more comprehensible. With this goal in mind, we proposed a framework for multiclass contour visualization. The framework has two components: a set of four visualization design parameters, which are developed based on an extensive review of literature on contour visualization, and a declarative domain-specific language (DSL) for creating multiclass contour rendering, which enables a fast exploration of those design parameters. A task-oriented user study was conducted to assess how those design parameters affect users' interpretations of real-world data. The study results offered some suggestions on the value choices of design parameters in multiclass contour visualization.",
                        "uid": "v-full-1206",
                        "file_name": "",
                        "time_stamp": "2022-10-19T16:54:22Z",
                        "time_start": "2022-10-19T16:54:22Z",
                        "time_end": "2022-10-19T16:57:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "562",
                        "paper_award": "",
                        "image_caption": "Variations of three design parameters under our proposed multiclass contour visualization framework. The contours are drawn using our declarative DSL. Through a user study, it can be learned that different choices of these three parameters affect the user's ability to complete different tasks.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "https://youtu.be/ZUFIRLy3McE",
                        "ff_id": "ZUFIRLy3McE"
                    }
                ]
            },
            {
                "title": "Immersive Analytics and Situated Visualization",
                "session_id": "full26",
                "event_prefix": "v-full",
                "track": "ok4",
                "livestream_id": "ok4-wed",
                "session_image": "full26.png",
                "chair": [
                    "Danielle Szafir"
                ],
                "organizers": [],
                "time_start": "2022-10-19T19:00:00Z",
                "time_end": "2022-10-19T20:15:00Z",
                "discord_category": "",
                "discord_channel": "oklahoma-st-4",
                "discord_channel_id": "1024600674924765264",
                "discord_link": "https://discord.com/channels/978320435554947082/1024600674924765264",
                "slido_link": "https://app.sli.do/event/i28UETZaiRWr1XwX6eKjcf?section=51658b34-0f5b-40c9-a335-1fd1468fad8d",
                "youtube_url": "https://youtu.be/BKQz_UXnWMA",
                "youtube_id": "BKQz_UXnWMA",
                "zoom_meeting": "",
                "zoom_password": "",
                "zoom_link": "",
                "ff_link": "https://youtu.be/CMtbrV781BA",
                "ff_playlist": "",
                "time_slots": [
                    {
                        "slot_id": "full26-opening",
                        "session_id": "full26",
                        "type": "In Person Other",
                        "title": "Session Opening",
                        "contributors": [
                            "Danielle Szafir"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "",
                        "file_name": "",
                        "time_stamp": "2022-10-19T19:00:00Z",
                        "time_start": "2022-10-19T19:00:00Z",
                        "time_end": "2022-10-19T19:00:00Z",
                        "paper_type": "",
                        "keywords": [],
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "v-full-1101-pres",
                        "session_id": "full26",
                        "type": "In Person Presentation",
                        "title": "Exploring Interactions with Printed Data Visualizations in Augmented Reality",
                        "contributors": [
                            "Wai Tong"
                        ],
                        "authors": [
                            "Wai Tong",
                            "Zhutian Chen",
                            "Meng Xia",
                            "Leo Yu-Ho Lo",
                            "Linping Yuan",
                            "Benjamin Bach",
                            "Huamin Qu"
                        ],
                        "abstract": "This paper presents a design space of interaction techniques to engage with visualizations that are printed on paper and augmented through Augmented Reality. Paper sheets are widely used to deploy visualizations and provide a rich set of tangible affordances for interactions, such as touch, folding, tilting, or stacking. At the same time, augmented reality can dynamically update visualization content to provide commands such as pan, zoom, filter, or detail on demand. This paper is the first to provide a structured approach to mapping possible actions with the paper to interaction commands. This design space and the findings of a controlled user study have implications for future designs of augmented reality systems involving paper sheets and visualizations. Through workshops (N=20) and ideation, we identified 81 interactions that we classify in three dimensions: 1) commands that can be supported by an interaction, 2) the specific parameters provided by an (inter)action with paper, and 3) the number of paper sheets involved in an interaction. We tested user preference and viability of 11 of these interactions with a prototype implementation in a controlled study (N=12, HoloLens 2) and found that most of the interactions are intuitive and engaging to use. We summarized interactions (e.g., tilt to pan) that have strong affordance to complement \u201cpoint\u201d for data exploration, physical limitations and properties of paper as a medium, cases requiring redundancy and shortcuts, and other implications for design.",
                        "uid": "v-full-1101",
                        "file_name": "v-full-1101_Tong_Presentation.mp4",
                        "time_stamp": "2022-10-19T19:00:00Z",
                        "time_start": "2022-10-19T19:00:00Z",
                        "time_end": "2022-10-19T19:10:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "547",
                        "paper_award": "",
                        "image_caption": "We investigate the possibility of interacting with printed visualizations in Augmented Reality. Suppose a student receives (a) a leaflet about university ranking and wants to analyze three universities' ranking histories of interest. Examples of interactions with (b) digital content overlaid: (c) tilt the paper to rescale the y-axis, (d) move (translate) to zoom (e) unfold to show two charts side by side and link them, (f) point to select elements and highlight them in the other chart.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "https://youtu.be/WYP_7ASDHEo",
                        "ff_id": "WYP_7ASDHEo"
                    },
                    {
                        "slot_id": "v-full-1101-qa",
                        "session_id": "full26",
                        "type": "In Person Q+A",
                        "title": "Exploring Interactions with Printed Data Visualizations in Augmented Reality (Q+A)",
                        "contributors": [
                            "Wai Tong"
                        ],
                        "authors": [],
                        "abstract": "This paper presents a design space of interaction techniques to engage with visualizations that are printed on paper and augmented through Augmented Reality. Paper sheets are widely used to deploy visualizations and provide a rich set of tangible affordances for interactions, such as touch, folding, tilting, or stacking. At the same time, augmented reality can dynamically update visualization content to provide commands such as pan, zoom, filter, or detail on demand. This paper is the first to provide a structured approach to mapping possible actions with the paper to interaction commands. This design space and the findings of a controlled user study have implications for future designs of augmented reality systems involving paper sheets and visualizations. Through workshops (N=20) and ideation, we identified 81 interactions that we classify in three dimensions: 1) commands that can be supported by an interaction, 2) the specific parameters provided by an (inter)action with paper, and 3) the number of paper sheets involved in an interaction. We tested user preference and viability of 11 of these interactions with a prototype implementation in a controlled study (N=12, HoloLens 2) and found that most of the interactions are intuitive and engaging to use. We summarized interactions (e.g., tilt to pan) that have strong affordance to complement \u201cpoint\u201d for data exploration, physical limitations and properties of paper as a medium, cases requiring redundancy and shortcuts, and other implications for design.",
                        "uid": "v-full-1101",
                        "file_name": "",
                        "time_stamp": "2022-10-19T19:10:00Z",
                        "time_start": "2022-10-19T19:10:00Z",
                        "time_end": "2022-10-19T19:12:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "547",
                        "paper_award": "",
                        "image_caption": "We investigate the possibility of interacting with printed visualizations in Augmented Reality. Suppose a student receives (a) a leaflet about university ranking and wants to analyze three universities' ranking histories of interest. Examples of interactions with (b) digital content overlaid: (c) tilt the paper to rescale the y-axis, (d) move (translate) to zoom (e) unfold to show two charts side by side and link them, (f) point to select elements and highlight them in the other chart.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "https://youtu.be/WYP_7ASDHEo",
                        "ff_id": "WYP_7ASDHEo"
                    },
                    {
                        "slot_id": "v-tvcg-9729627-pres",
                        "session_id": "full26",
                        "type": "In Person Presentation",
                        "title": "RagRug: A Toolkit for Situated Analytics",
                        "contributors": [
                            "Philipp Fleck"
                        ],
                        "authors": [
                            "Philipp Fleck",
                            "Aimee Sousa Calepso",
                            "Sebastian Hubenschmid,Michael Sedlmair",
                            "Dieter Schmalstieg"
                        ],
                        "abstract": "We present RagRug, an open-source toolkit for situated analytics. The  , abilities of RagRug go beyond previous immersiveanalytics toolkits by  , focusing on specific requirements emerging when using augmented  , reality (AR) rather than virtual reality. RagRugcombines state of the  , art visual encoding capabilities with a comprehensive physical-virtual  , model, which lets application developerssystematically describe the  , physical objects in the real world and their role in AR. We connect AR  , visualizations with data streams fromthe Internet of Things using  , distributed dataflow. To this end, we use reactive programming  , patterns so that visualizations becomecontext-aware, i.e., they adapt  , to events coming in from the environment. The resulting authoring  , system is low-code; it emphasisesdescribing the physical and the  , virtual world and the dataflow between the elements contained therein.  , We describe the technicaldesign and implementation of RagRug, and  , report on five example applications illustrating the toolkit\u2019s  , abilities.",
                        "uid": "v-tvcg-9729627",
                        "file_name": "v-tvcg-9729627_Fleck_Presentation.mp4",
                        "time_stamp": "2022-10-19T19:12:00Z",
                        "time_start": "2022-10-19T19:12:00Z",
                        "time_end": "2022-10-19T19:22:00Z",
                        "paper_type": "full",
                        "keywords": [
                            "Augmented Reality, Visualization, Visual Analytics, Immersive Analytics, Situated Analytics"
                        ],
                        "has_image": "1",
                        "has_video": "467",
                        "paper_award": "",
                        "image_caption": "We present RagRug, an open-source toolkit for situated analytics. The abilities of RagRug go beyond previous immersive analytics toolkits by focusing on specific requirements emerging when using augmented reality (AR) rather than virtual reality. RagRug combines state of the art visual encoding capabilities with a comprehensive physical-virtual model, which lets application developers systematically describe the physical objects in the real world and their role in AR. We connect AR visualization with data streams from the Internet of Things using distributed dataflow. Github: https://github.com/philfleck/ragrug",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "https://youtu.be/HTUKH3hywsE",
                        "ff_id": "HTUKH3hywsE"
                    },
                    {
                        "slot_id": "v-tvcg-9729627-qa",
                        "session_id": "full26",
                        "type": "In Person Q+A",
                        "title": "RagRug: A Toolkit for Situated Analytics (Q+A)",
                        "contributors": [
                            "Philipp Fleck"
                        ],
                        "authors": [],
                        "abstract": "We present RagRug, an open-source toolkit for situated analytics. The  , abilities of RagRug go beyond previous immersiveanalytics toolkits by  , focusing on specific requirements emerging when using augmented  , reality (AR) rather than virtual reality. RagRugcombines state of the  , art visual encoding capabilities with a comprehensive physical-virtual  , model, which lets application developerssystematically describe the  , physical objects in the real world and their role in AR. We connect AR  , visualizations with data streams fromthe Internet of Things using  , distributed dataflow. To this end, we use reactive programming  , patterns so that visualizations becomecontext-aware, i.e., they adapt  , to events coming in from the environment. The resulting authoring  , system is low-code; it emphasisesdescribing the physical and the  , virtual world and the dataflow between the elements contained therein.  , We describe the technicaldesign and implementation of RagRug, and  , report on five example applications illustrating the toolkit\u2019s  , abilities.",
                        "uid": "v-tvcg-9729627",
                        "file_name": "",
                        "time_stamp": "2022-10-19T19:22:00Z",
                        "time_start": "2022-10-19T19:22:00Z",
                        "time_end": "2022-10-19T19:24:00Z",
                        "paper_type": "full",
                        "keywords": [
                            "Augmented Reality, Visualization, Visual Analytics, Immersive Analytics, Situated Analytics"
                        ],
                        "has_image": "1",
                        "has_video": "467",
                        "paper_award": "",
                        "image_caption": "We present RagRug, an open-source toolkit for situated analytics. The abilities of RagRug go beyond previous immersive analytics toolkits by focusing on specific requirements emerging when using augmented reality (AR) rather than virtual reality. RagRug combines state of the art visual encoding capabilities with a comprehensive physical-virtual model, which lets application developers systematically describe the physical objects in the real world and their role in AR. We connect AR visualization with data streams from the Internet of Things using distributed dataflow. Github: https://github.com/philfleck/ragrug",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "https://youtu.be/HTUKH3hywsE",
                        "ff_id": "HTUKH3hywsE"
                    },
                    {
                        "slot_id": "v-full-1272-pres",
                        "session_id": "full26",
                        "type": "Virtual Presentation (pre-recorded)",
                        "title": "PuzzleFixer: A Visual Reassembly System for Immersive Fragments Restoration",
                        "contributors": [
                            "Shuainan Ye"
                        ],
                        "authors": [
                            "Shuainan Ye",
                            "Zhutian Chen",
                            "Xiangtong Chu",
                            "Kang Li",
                            "Juntong Luo",
                            "Yi Li",
                            "Guohua Geng",
                            "Yingcai Wu"
                        ],
                        "abstract": "We present PuzzleFixer, an immersive interactive system for experts to rectify defective reassembled 3D objects. Reassembling the fragments of a broken object to restore its original state is the prerequisite of many analytical tasks such as cultural relics analysis and forensics reasoning. While existing computer-aided methods can automatically reassemble fragments, they often derive incorrect objects due to the complex and ambiguous fragment shapes. Thus, experts usually need to refine the object manually. Prior advances in immersive technologies provide benefits for realistic perception and direct interactions to visualize and interact with 3D fragments. However, few studies have investigated the reassembled object refinement. The specific challenges include: 1) the fragment combination set is too large to determine the correct matches, and 2) the geometry of the fragments is too complex to align them properly. To tackle the first challenge, PuzzleFixer leverages dimensionality reduction and clustering techniques, allowing users to review possible match categories, select the matches with reasonable shapes, and drill down to shapes to correct the corresponding faces. For the second challenge, PuzzleFixer embeds the object with node-link networks to augment the perception of match relations. Specifically, it instantly visualizes matches with graph edges and provides force feedback to facilitate the efficiency of alignment interactions. To demonstrate the effectiveness of PuzzleFixer, we conducted an expert evaluation based on two cases on real-world artifacts and collected feedback through post-study interviews. The results suggest that our system is suitable and efficient for experts to refine incorrect reassembled objects.",
                        "uid": "v-full-1272",
                        "file_name": "v-full-1272_Ye_Presentation.mp4",
                        "time_stamp": "2022-10-19T19:24:00Z",
                        "time_start": "2022-10-19T19:24:00Z",
                        "time_end": "2022-10-19T19:33:04Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "544",
                        "paper_award": "",
                        "image_caption": "A reassembly example. (a) The skeletons and shapes show that the right part of g1 is poorly aligned. (b) The clusters reveal that other fragments can be matched at bottom or left side of the target group. (c) The candidate in the selected cluster has the same appearance but different match relations. (d) Refine the alignments between groups. (e) Visual cues are shown in real-time to indicate relations. (f) Force cue is added on f2 and f3 to help rotate the f3 to align. (g) The reassembled stele after finishing the workflow.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "https://youtu.be/52xbZtb1cmo",
                        "ff_id": "52xbZtb1cmo"
                    },
                    {
                        "slot_id": "v-full-1272-qa",
                        "session_id": "full26",
                        "type": "Virtual Q+A",
                        "title": "PuzzleFixer: A Visual Reassembly System for Immersive Fragments Restoration (Q+A)",
                        "contributors": [
                            "Shuainan Ye"
                        ],
                        "authors": [],
                        "abstract": "We present PuzzleFixer, an immersive interactive system for experts to rectify defective reassembled 3D objects. Reassembling the fragments of a broken object to restore its original state is the prerequisite of many analytical tasks such as cultural relics analysis and forensics reasoning. While existing computer-aided methods can automatically reassemble fragments, they often derive incorrect objects due to the complex and ambiguous fragment shapes. Thus, experts usually need to refine the object manually. Prior advances in immersive technologies provide benefits for realistic perception and direct interactions to visualize and interact with 3D fragments. However, few studies have investigated the reassembled object refinement. The specific challenges include: 1) the fragment combination set is too large to determine the correct matches, and 2) the geometry of the fragments is too complex to align them properly. To tackle the first challenge, PuzzleFixer leverages dimensionality reduction and clustering techniques, allowing users to review possible match categories, select the matches with reasonable shapes, and drill down to shapes to correct the corresponding faces. For the second challenge, PuzzleFixer embeds the object with node-link networks to augment the perception of match relations. Specifically, it instantly visualizes matches with graph edges and provides force feedback to facilitate the efficiency of alignment interactions. To demonstrate the effectiveness of PuzzleFixer, we conducted an expert evaluation based on two cases on real-world artifacts and collected feedback through post-study interviews. The results suggest that our system is suitable and efficient for experts to refine incorrect reassembled objects.",
                        "uid": "v-full-1272",
                        "file_name": "",
                        "time_stamp": "2022-10-19T19:33:04Z",
                        "time_start": "2022-10-19T19:33:04Z",
                        "time_end": "2022-10-19T19:36:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "544",
                        "paper_award": "",
                        "image_caption": "A reassembly example. (a) The skeletons and shapes show that the right part of g1 is poorly aligned. (b) The clusters reveal that other fragments can be matched at bottom or left side of the target group. (c) The candidate in the selected cluster has the same appearance but different match relations. (d) Refine the alignments between groups. (e) Visual cues are shown in real-time to indicate relations. (f) Force cue is added on f2 and f3 to help rotate the f3 to align. (g) The reassembled stele after finishing the workflow.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "https://youtu.be/52xbZtb1cmo",
                        "ff_id": "52xbZtb1cmo"
                    },
                    {
                        "slot_id": "v-full-1382-pres",
                        "session_id": "full26",
                        "type": "Virtual Presentation (pre-recorded)",
                        "title": "Effects of View Layout On Situated Analytics for Multiple-View Representations in Immersive Visualization",
                        "contributors": [
                            "Zhen Wen"
                        ],
                        "authors": [
                            "Zhen Wen",
                            "Wei Zeng",
                            "Luoxuan Weng",
                            "Yihan Liu",
                            "Mingliang Xu",
                            "Wei Chen"
                        ],
                        "abstract": "Multiple-view (MV) representations enabling multi-perspective exploration of large and complex data are often employed on 2D displays. The technique also shows great potential in addressing complex analytic tasks in immersive visualization. However, although useful, the design space of MV representations in immersive visualization lacks in deep exploration. In this paper, we propose a new perspective to this line of research, by examining the effects of view layout for MV representations on situated analytics. Specifically, we disentangle situated analytics in perspectives of situatedness regarding spatial relationship between visual representations and physical referents, and analytics regarding cross-view data analysis including filtering, refocusing, and connecting tasks. Through an in-depth analysis of existing layout paradigms, we summarize design trade-offs for achieving high situatedness and effective analytics simultaneously. We then distill a list of design requirements for a desired layout that balances situatedness and analytics, and develop a prototype system with an automatic layout adaptation method to fulfill the requirements. The method mainly includes a cylindrical paradigm for egocentric reference frame, and a force-directed method for proper view-view, view-user, and view-referent proximities and high view visibility. We conducted a formal user study that compares layouts by our method with linked and embedded layouts. Quantitative results show that participants finished filtering- and connecting-centered tasks significantly faster with our layouts, and user feedback confirms high usability of the prototype system.",
                        "uid": "v-full-1382",
                        "file_name": "v-full-1382_Wen_Presentation.mp4",
                        "time_stamp": "2022-10-19T19:36:00Z",
                        "time_start": "2022-10-19T19:36:00Z",
                        "time_end": "2022-10-19T19:45:06Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "546",
                        "paper_award": "",
                        "image_caption": "In this paper, we present an in-depth study on the effects of view layout on situated analytics for multiple-view representations in immersive visualization. We distill a list of design requirements for effective view layout\nthat promotes situatedness and analytics simultaneously. To fulfill the requirements, we leverage cylindrical reference frame and propose a force-directed approach to automatically position multiple-view representations in immersive visualization.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "https://youtu.be/YLS_9p324ZI",
                        "ff_id": "YLS_9p324ZI"
                    },
                    {
                        "slot_id": "v-full-1382-qa",
                        "session_id": "full26",
                        "type": "Virtual Q+A",
                        "title": "Effects of View Layout On Situated Analytics for Multiple-View Representations in Immersive Visualization (Q+A)",
                        "contributors": [
                            "Zhen Wen"
                        ],
                        "authors": [],
                        "abstract": "Multiple-view (MV) representations enabling multi-perspective exploration of large and complex data are often employed on 2D displays. The technique also shows great potential in addressing complex analytic tasks in immersive visualization. However, although useful, the design space of MV representations in immersive visualization lacks in deep exploration. In this paper, we propose a new perspective to this line of research, by examining the effects of view layout for MV representations on situated analytics. Specifically, we disentangle situated analytics in perspectives of situatedness regarding spatial relationship between visual representations and physical referents, and analytics regarding cross-view data analysis including filtering, refocusing, and connecting tasks. Through an in-depth analysis of existing layout paradigms, we summarize design trade-offs for achieving high situatedness and effective analytics simultaneously. We then distill a list of design requirements for a desired layout that balances situatedness and analytics, and develop a prototype system with an automatic layout adaptation method to fulfill the requirements. The method mainly includes a cylindrical paradigm for egocentric reference frame, and a force-directed method for proper view-view, view-user, and view-referent proximities and high view visibility. We conducted a formal user study that compares layouts by our method with linked and embedded layouts. Quantitative results show that participants finished filtering- and connecting-centered tasks significantly faster with our layouts, and user feedback confirms high usability of the prototype system.",
                        "uid": "v-full-1382",
                        "file_name": "",
                        "time_stamp": "2022-10-19T19:45:06Z",
                        "time_start": "2022-10-19T19:45:06Z",
                        "time_end": "2022-10-19T19:48:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "546",
                        "paper_award": "",
                        "image_caption": "In this paper, we present an in-depth study on the effects of view layout on situated analytics for multiple-view representations in immersive visualization. We distill a list of design requirements for effective view layout\nthat promotes situatedness and analytics simultaneously. To fulfill the requirements, we leverage cylindrical reference frame and propose a force-directed approach to automatically position multiple-view representations in immersive visualization.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "https://youtu.be/YLS_9p324ZI",
                        "ff_id": "YLS_9p324ZI"
                    },
                    {
                        "slot_id": "v-full-1523-pres",
                        "session_id": "full26",
                        "type": "Virtual Presentation (pre-recorded)",
                        "title": "RoboHapalytics: A Robot Assisted Haptic Controller for Immersive Analytics",
                        "contributors": [
                            "shaozhang dai"
                        ],
                        "authors": [
                            "Shaozhang Dai",
                            "Tim Dwyer",
                            "Barrett Ens",
                            "Jim Smiley",
                            "Lonni Besan\u00e7on"
                        ],
                        "abstract": "Immersive environments offer new possibilities for exploring three-dimensional volumetric or abstract data. However, typical mid-air interaction offers little guidance to the user in interacting with the resulting visuals. Previous work has explored the use of haptic controls to give users tangible affordances for interacting with the data, but these controls have either: been limited in their range and resolution; were spatially fixed; or required users to manually align them with the data space. We explore the use of a robot arm with hand tracking to align tangible controls under the user\u2019s fingers as they reach out to interact with data affordances. We begin with a study evaluating the effectiveness of a robot-extended slider control compared to a large fixed physical slider and a purely virtual mid-air slider. We find that the robot slider has similar accuracy to the physical slider but is significantly more accurate than mid-air interaction. Further, the robot slider can be arbitrarily reoriented, opening up many new possibilities for tangible haptic interaction with immersive visualisations. We demonstrate these possibilities through three use-cases: selection in a time-series chart; interactive slicing of CT scans; and finally exploration of a scatter plot depicting time-varying socio-economic data.",
                        "uid": "v-full-1523",
                        "file_name": "v-full-1523_Dai_Presentation.mp4",
                        "time_stamp": "2022-10-19T19:48:00Z",
                        "time_start": "2022-10-19T19:48:00Z",
                        "time_end": "2022-10-19T19:56:58Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "538",
                        "paper_award": "",
                        "image_caption": "RoboHapalytics: A Robot Assisted Haptic Controller for Immersive Analytics [2022 VIS]",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "https://youtu.be/e9ANz79Z8mw",
                        "ff_id": "e9ANz79Z8mw"
                    },
                    {
                        "slot_id": "v-full-1523-qa",
                        "session_id": "full26",
                        "type": "Virtual Q+A",
                        "title": "RoboHapalytics: A Robot Assisted Haptic Controller for Immersive Analytics (Q+A)",
                        "contributors": [
                            "shaozhang dai"
                        ],
                        "authors": [],
                        "abstract": "Immersive environments offer new possibilities for exploring three-dimensional volumetric or abstract data. However, typical mid-air interaction offers little guidance to the user in interacting with the resulting visuals. Previous work has explored the use of haptic controls to give users tangible affordances for interacting with the data, but these controls have either: been limited in their range and resolution; were spatially fixed; or required users to manually align them with the data space. We explore the use of a robot arm with hand tracking to align tangible controls under the user\u2019s fingers as they reach out to interact with data affordances. We begin with a study evaluating the effectiveness of a robot-extended slider control compared to a large fixed physical slider and a purely virtual mid-air slider. We find that the robot slider has similar accuracy to the physical slider but is significantly more accurate than mid-air interaction. Further, the robot slider can be arbitrarily reoriented, opening up many new possibilities for tangible haptic interaction with immersive visualisations. We demonstrate these possibilities through three use-cases: selection in a time-series chart; interactive slicing of CT scans; and finally exploration of a scatter plot depicting time-varying socio-economic data.",
                        "uid": "v-full-1523",
                        "file_name": "",
                        "time_stamp": "2022-10-19T19:56:58Z",
                        "time_start": "2022-10-19T19:56:58Z",
                        "time_end": "2022-10-19T20:00:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "538",
                        "paper_award": "",
                        "image_caption": "RoboHapalytics: A Robot Assisted Haptic Controller for Immersive Analytics [2022 VIS]",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "https://youtu.be/e9ANz79Z8mw",
                        "ff_id": "e9ANz79Z8mw"
                    },
                    {
                        "slot_id": "v-tvcg-9645242-pres",
                        "session_id": "full26",
                        "type": "In Person Presentation",
                        "title": "Labeling Out-of-View Objects in Immersive Analytics to Support Situated Visual Searching",
                        "contributors": [
                            "Tica Lin"
                        ],
                        "authors": [
                            "Tica Lin",
                            "Yalong Yang",
                            "Johanna Beyer",
                            "Hanspeter Pfister"
                        ],
                        "abstract": "Augmented Reality (AR) embeds digital information into objects of the physical world. Data can be shown in-situ, thereby enabling real-time visual comparisons and object search in real-life user tasks, such as comparing products and looking up scores in a sports game. While there have been studies on designing AR interfaces for situated information retrieval, there has only been limited research on AR object labeling for visual search tasks in the spatial environment. In this paper, we identify and categorize different design aspects in AR label design and report on a formal user study on labels for out-of-view objects to support visual search tasks in AR. We design three visualization techniques for out-of-view object labeling in AR, which respectively encode the relative physical position (height-encoded), the rotational direction (angle-encoded), and the label values (value-encoded) of the objects. We further implement two traditional in-view object labeling techniques, where labels are placed either next to the respective objects (situated) or at the edge of the AR FoV (boundary). We evaluate these five different label conditions in three visual search tasks for static objects. Our study shows that out-of-view object labels are beneficial when searching for objects outside the FoV, spatial orientation, and when comparing multiple spatially sparse objects. Angle-encoded labels with directional cues of the surrounding objects have the overall best performance with the highest user satisfaction. We discuss the implications of our findings for future immersive AR interface design.",
                        "uid": "v-tvcg-9645242",
                        "file_name": "v-tvcg-9645242_Lin_Presentation.mp4",
                        "time_stamp": "2022-10-19T20:00:00Z",
                        "time_start": "2022-10-19T20:00:00Z",
                        "time_end": "2022-10-19T20:10:00Z",
                        "paper_type": "full",
                        "keywords": [
                            "Object Labeling, Mixed / Augmented Reality, Immersive Analytics, Situated Analytics, Data Visualization"
                        ],
                        "has_image": "1",
                        "has_video": "262",
                        "paper_award": "",
                        "image_caption": "AR label design for objects outside the field-of-view (FOV). (a) Our user study uses a VR HMD to simulate consistent AR conditions. (b) The user is surrounded by spatially sparse objects and can see labels for in-view and out-of-view objects on the AR screen. Labels for out-of-view objects are placed on the boundary to support embodied navigation. (c) A simulated grocery shopping experience with AR labels.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "https://youtu.be/qWlUuO81wwI",
                        "ff_id": "qWlUuO81wwI"
                    },
                    {
                        "slot_id": "v-tvcg-9645242-qa",
                        "session_id": "full26",
                        "type": "In Person Q+A",
                        "title": "Labeling Out-of-View Objects in Immersive Analytics to Support Situated Visual Searching (Q+A)",
                        "contributors": [
                            "Tica Lin"
                        ],
                        "authors": [],
                        "abstract": "Augmented Reality (AR) embeds digital information into objects of the physical world. Data can be shown in-situ, thereby enabling real-time visual comparisons and object search in real-life user tasks, such as comparing products and looking up scores in a sports game. While there have been studies on designing AR interfaces for situated information retrieval, there has only been limited research on AR object labeling for visual search tasks in the spatial environment. In this paper, we identify and categorize different design aspects in AR label design and report on a formal user study on labels for out-of-view objects to support visual search tasks in AR. We design three visualization techniques for out-of-view object labeling in AR, which respectively encode the relative physical position (height-encoded), the rotational direction (angle-encoded), and the label values (value-encoded) of the objects. We further implement two traditional in-view object labeling techniques, where labels are placed either next to the respective objects (situated) or at the edge of the AR FoV (boundary). We evaluate these five different label conditions in three visual search tasks for static objects. Our study shows that out-of-view object labels are beneficial when searching for objects outside the FoV, spatial orientation, and when comparing multiple spatially sparse objects. Angle-encoded labels with directional cues of the surrounding objects have the overall best performance with the highest user satisfaction. We discuss the implications of our findings for future immersive AR interface design.",
                        "uid": "v-tvcg-9645242",
                        "file_name": "",
                        "time_stamp": "2022-10-19T20:10:00Z",
                        "time_start": "2022-10-19T20:10:00Z",
                        "time_end": "2022-10-19T20:12:00Z",
                        "paper_type": "full",
                        "keywords": [
                            "Object Labeling, Mixed / Augmented Reality, Immersive Analytics, Situated Analytics, Data Visualization"
                        ],
                        "has_image": "1",
                        "has_video": "262",
                        "paper_award": "",
                        "image_caption": "AR label design for objects outside the field-of-view (FOV). (a) Our user study uses a VR HMD to simulate consistent AR conditions. (b) The user is surrounded by spatially sparse objects and can see labels for in-view and out-of-view objects on the AR screen. Labels for out-of-view objects are placed on the boundary to support embodied navigation. (c) A simulated grocery shopping experience with AR labels.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "https://youtu.be/qWlUuO81wwI",
                        "ff_id": "qWlUuO81wwI"
                    }
                ]
            },
            {
                "title": "Questioning Data and Data Bias",
                "session_id": "full27",
                "event_prefix": "v-full",
                "track": "ok5",
                "livestream_id": "ok5-wed",
                "session_image": "full27.png",
                "chair": [
                    "Emily Wall"
                ],
                "organizers": [],
                "time_start": "2022-10-19T19:00:00Z",
                "time_end": "2022-10-19T20:15:00Z",
                "discord_category": "",
                "discord_channel": "oklahoma-st-5",
                "discord_channel_id": "1024600839295356939",
                "discord_link": "https://discord.com/channels/978320435554947082/1024600839295356939",
                "slido_link": "https://app.sli.do/event/i28UETZaiRWr1XwX6eKjcf?section=d355f89b-fbd2-4abf-9958-741607905551",
                "youtube_url": "https://youtu.be/wr1l85Jo7jM",
                "youtube_id": "wr1l85Jo7jM",
                "zoom_meeting": "",
                "zoom_password": "",
                "zoom_link": "",
                "ff_link": "https://youtu.be/o9AMPN_Diek",
                "ff_playlist": "",
                "time_slots": [
                    {
                        "slot_id": "full27-opening",
                        "session_id": "full27",
                        "type": "In Person Other",
                        "title": "Session Opening",
                        "contributors": [
                            "Emily Wall"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "",
                        "file_name": "",
                        "time_stamp": "2022-10-19T19:00:00Z",
                        "time_start": "2022-10-19T19:00:00Z",
                        "time_end": "2022-10-19T19:00:00Z",
                        "paper_type": "",
                        "keywords": [],
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "v-full-1351-pres",
                        "session_id": "full27",
                        "type": "Virtual Presentation (pre-recorded)",
                        "title": "VACSEN: A Visualization Approach for Noise Awareness in Quantum Computing",
                        "contributors": [
                            "Shaolun Ruan"
                        ],
                        "authors": [
                            "Shaolun Ruan",
                            "Yong Wang",
                            "Weiwen Jiang",
                            "Ying Mao",
                            "Qiang Guan"
                        ],
                        "abstract": "Quantum computing has attracted considerable public attention due to its exponential speedup over classical computing. Despite its advantages, today\u2019s quantum computers intrinsically suffer from noise and are error-prone. To guarantee the high fidelity of the execution result of a quantum algorithm, it is crucial to inform users of the noises of the used quantum computer and the compiled physical circuits. However, an intuitive and systematic way to make users aware of the quantum computing noise is still missing. In this paper, we fill the gap by proposing a novel visualization approach to achieve noise-aware quantum computing. It provides a holistic picture of the noise of quantum computing through multiple interactively coordinated views: a Computer Evolution View with a circuit-like design overviews the temporal evolution of the noises of different quantum computers, a Circuit Filtering View facilitates quick filtering of multiple compiled physical circuits for the same quantum algorithm, and a Circuit Comparison View with a coupled bar chart enables detailed comparison of the filtered compiled circuits. We extensively evaluate the performance of VACSEN through two case studies on quantum algorithms of different scales and in-depth interviews with 12 quantum computing users. The results demonstrate the effectiveness and usability of VACSEN in achieving noise-aware quantum computing.",
                        "uid": "v-full-1351",
                        "file_name": "v-full-1351_Ruan_Presentation.mp4",
                        "time_stamp": "2022-10-19T19:00:00Z",
                        "time_start": "2022-10-19T19:00:00Z",
                        "time_end": "2022-10-19T19:10:01Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "601",
                        "paper_award": "",
                        "image_caption": "The severe and inevitable noise issues are the key challenges to achieving the real quantum advantages in today\u2019s quantum computers. We propose VACSEN, a visualization approach for noise awareness in quantum computing. VACSEN supports a real-time noise awareness of quantum computers and compiled circuits, leading to a better circuit execution with higher fidelity.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "https://youtu.be/G3AeHSmoYYc",
                        "ff_id": "G3AeHSmoYYc"
                    },
                    {
                        "slot_id": "v-full-1351-qa",
                        "session_id": "full27",
                        "type": "Virtual Q+A",
                        "title": "VACSEN: A Visualization Approach for Noise Awareness in Quantum Computing (Q+A)",
                        "contributors": [
                            "Shaolun Ruan"
                        ],
                        "authors": [],
                        "abstract": "Quantum computing has attracted considerable public attention due to its exponential speedup over classical computing. Despite its advantages, today\u2019s quantum computers intrinsically suffer from noise and are error-prone. To guarantee the high fidelity of the execution result of a quantum algorithm, it is crucial to inform users of the noises of the used quantum computer and the compiled physical circuits. However, an intuitive and systematic way to make users aware of the quantum computing noise is still missing. In this paper, we fill the gap by proposing a novel visualization approach to achieve noise-aware quantum computing. It provides a holistic picture of the noise of quantum computing through multiple interactively coordinated views: a Computer Evolution View with a circuit-like design overviews the temporal evolution of the noises of different quantum computers, a Circuit Filtering View facilitates quick filtering of multiple compiled physical circuits for the same quantum algorithm, and a Circuit Comparison View with a coupled bar chart enables detailed comparison of the filtered compiled circuits. We extensively evaluate the performance of VACSEN through two case studies on quantum algorithms of different scales and in-depth interviews with 12 quantum computing users. The results demonstrate the effectiveness and usability of VACSEN in achieving noise-aware quantum computing.",
                        "uid": "v-full-1351",
                        "file_name": "",
                        "time_stamp": "2022-10-19T19:10:01Z",
                        "time_start": "2022-10-19T19:10:01Z",
                        "time_end": "2022-10-19T19:12:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "601",
                        "paper_award": "",
                        "image_caption": "The severe and inevitable noise issues are the key challenges to achieving the real quantum advantages in today\u2019s quantum computers. We propose VACSEN, a visualization approach for noise awareness in quantum computing. VACSEN supports a real-time noise awareness of quantum computers and compiled circuits, leading to a better circuit execution with higher fidelity.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "https://youtu.be/G3AeHSmoYYc",
                        "ff_id": "G3AeHSmoYYc"
                    },
                    {
                        "slot_id": "v-tvcg-9676662-pres",
                        "session_id": "full27",
                        "type": "In Person Presentation",
                        "title": "Do you believe your (social media) data? A personal story on location data biases, errors, and plausibility as well as their visualization",
                        "contributors": [
                            "Tobias Isenberg"
                        ],
                        "authors": [
                            "Tobias Isenberg",
                            "Zujany Salazar",
                            "Rafael Blanco",
                            "Catherine Plaisant"
                        ],
                        "abstract": "We present a case study on a journey about a personal data collection of carnivorous plant species habitats, and the resulting scientific exploration of location data biases, data errors, location hiding, and data plausibility. While initially driven by personal interest, our work led to the analysis and development of various means for visualizing threats to insight from geo-tagged social media data. In the course of this endeavor we analyzed local and global geographic distributions and their inaccuracies. We also contribute Motion Plausibility Profiles---a new means for visualizing how believable a specific contributor\u2019s location data is or if it was likely manipulated. We then compared our own repurposed social media dataset with data from a dedicated citizen science project. Compared to biases and errors in the literature on traditional citizen science data, with our visualizations we could also identify some new types or show new aspects for known ones. Moreover, we demonstrate several types of errors and biases for repurposed social media data.",
                        "uid": "v-tvcg-9676662",
                        "file_name": "v-tvcg-9676662_Isenberg_Presentation.mp4",
                        "time_stamp": "2022-10-19T19:12:00Z",
                        "time_start": "2022-10-19T19:12:00Z",
                        "time_end": "2022-10-19T19:22:00Z",
                        "paper_type": "full",
                        "keywords": [
                            "Social media data; Flickr; Panoramio; iNaturalist; data bias; data error; data plausibility; data obfuscation; citizen science"
                        ],
                        "has_image": "1",
                        "has_video": "581",
                        "paper_award": "",
                        "image_caption": "In our paper we describe a journey diven by a personal dataset, in which we discovered and then analyzed various sources of data errors and data bias, This investigation led us to new visual data representation we call Motion Plausibility Profiles, which allow us to analyze a person's history of posting geo-located images on social media. The image shows the Motion Plausibility Profile of a Flickr user, which indicates that they posted only one or two images at with plausible coordinate data in 2008, that starting from 2010 they apparently individually manipulated the geo-locations of the posted images for each image individually as evident in the implausible (color-coded) speeds between image locations (including some that would require jet airplaine travel between photo sites), and from sometime in 2013 they assigned the exact same geo-location for all of several images that they posted for a given day (indicated in blue).",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "https://youtu.be/Z05QV_ucZns",
                        "ff_id": "Z05QV_ucZns"
                    },
                    {
                        "slot_id": "v-tvcg-9676662-qa",
                        "session_id": "full27",
                        "type": "In Person Q+A",
                        "title": "Do you believe your (social media) data? A personal story on location data biases, errors, and plausibility as well as their visualization (Q+A)",
                        "contributors": [
                            "Tobias Isenberg"
                        ],
                        "authors": [],
                        "abstract": "We present a case study on a journey about a personal data collection of carnivorous plant species habitats, and the resulting scientific exploration of location data biases, data errors, location hiding, and data plausibility. While initially driven by personal interest, our work led to the analysis and development of various means for visualizing threats to insight from geo-tagged social media data. In the course of this endeavor we analyzed local and global geographic distributions and their inaccuracies. We also contribute Motion Plausibility Profiles---a new means for visualizing how believable a specific contributor\u2019s location data is or if it was likely manipulated. We then compared our own repurposed social media dataset with data from a dedicated citizen science project. Compared to biases and errors in the literature on traditional citizen science data, with our visualizations we could also identify some new types or show new aspects for known ones. Moreover, we demonstrate several types of errors and biases for repurposed social media data.",
                        "uid": "v-tvcg-9676662",
                        "file_name": "",
                        "time_stamp": "2022-10-19T19:22:00Z",
                        "time_start": "2022-10-19T19:22:00Z",
                        "time_end": "2022-10-19T19:24:00Z",
                        "paper_type": "full",
                        "keywords": [
                            "Social media data; Flickr; Panoramio; iNaturalist; data bias; data error; data plausibility; data obfuscation; citizen science"
                        ],
                        "has_image": "1",
                        "has_video": "581",
                        "paper_award": "",
                        "image_caption": "In our paper we describe a journey diven by a personal dataset, in which we discovered and then analyzed various sources of data errors and data bias, This investigation led us to new visual data representation we call Motion Plausibility Profiles, which allow us to analyze a person's history of posting geo-located images on social media. The image shows the Motion Plausibility Profile of a Flickr user, which indicates that they posted only one or two images at with plausible coordinate data in 2008, that starting from 2010 they apparently individually manipulated the geo-locations of the posted images for each image individually as evident in the implausible (color-coded) speeds between image locations (including some that would require jet airplaine travel between photo sites), and from sometime in 2013 they assigned the exact same geo-location for all of several images that they posted for a given day (indicated in blue).",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "https://youtu.be/Z05QV_ucZns",
                        "ff_id": "Z05QV_ucZns"
                    },
                    {
                        "slot_id": "v-full-1050-pres",
                        "session_id": "full27",
                        "type": "In Person Presentation",
                        "title": "D-BIAS: A Causality-Based Human-in-the-Loop System for Tackling Algorithmic Bias",
                        "contributors": [
                            "Bhavya Ghai"
                        ],
                        "authors": [
                            "Bhavya Ghai",
                            "Klaus Mueller"
                        ],
                        "abstract": "With the rise of AI, algorithms have become better at learning underlying patterns from the training data including ingrained social biases based on gender, race, etc. Deployment of such algorithms to domains such as hiring, healthcare, law enforcement, etc. has raised serious concerns about fairness, accountability, trust and interpretability in machine learning algorithms. To alleviate this problem, we propose D-BIAS, a visual interactive tool that embodies human-in-the-loop AI approach for auditing and mitigating social biases from tabular datasets. It uses a graphical causal model to represent causal relationships among different features in the dataset and as a medium to inject domain knowledge. A user can detect the presence of bias against a group, say females, or a subgroup, say black females, by identifying unfair causal relationships in the causal network and using an array of fairness metrics. Thereafter, the user can mitigate bias by refining the causal model and acting on the unfair causal edges. For each interaction, say weakening/deleting a biased causal edge, the system uses a novel method to simulate a new (debiased) dataset based on the current causal model while ensuring a minimal change from the original dataset. Users can visually assess the impact of their interactions on different fairness metrics, utility metrics, data distortion, and the underlying data distribution. Once satisfied, they can download the debiased dataset and use it for any downstream application for fairer predictions. We evaluate D-BIAS by conducting experiments on 3 datasets and also a formal user study. We found that D-BIAS helps reduce bias significantly compared to the baseline debiasing approach across different fairness metrics while incurring little data distortion and a small loss in utility. Moreover, our human-in-the-loop based approach significantly outperforms an automated approach on trust, interpretability and accountability.",
                        "uid": "v-full-1050",
                        "file_name": "v-full-1050_Ghai_Presentation.mp4",
                        "time_stamp": "2022-10-19T19:24:00Z",
                        "time_start": "2022-10-19T19:24:00Z",
                        "time_end": "2022-10-19T19:34:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "1480",
                        "paper_award": "",
                        "image_caption": "The visual interface of the D-BIAS tool. (A) The Generator panel: used to create the causal network and download the debiased dataset (B) The Causal Network view: shows the causal relations between the attributes of the data, allows user to inject their prior in the system (C) The Evaluation panel: used to choose the sensitive variable, the ML model and displays different evaluation metrics. This snapshot pertains to the Adult Income dataset post-debiasing.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "https://youtu.be/v0VY4fZfsNc",
                        "ff_id": "v0VY4fZfsNc"
                    },
                    {
                        "slot_id": "v-full-1050-qa",
                        "session_id": "full27",
                        "type": "In Person Q+A",
                        "title": "D-BIAS: A Causality-Based Human-in-the-Loop System for Tackling Algorithmic Bias (Q+A)",
                        "contributors": [
                            "Bhavya Ghai"
                        ],
                        "authors": [],
                        "abstract": "With the rise of AI, algorithms have become better at learning underlying patterns from the training data including ingrained social biases based on gender, race, etc. Deployment of such algorithms to domains such as hiring, healthcare, law enforcement, etc. has raised serious concerns about fairness, accountability, trust and interpretability in machine learning algorithms. To alleviate this problem, we propose D-BIAS, a visual interactive tool that embodies human-in-the-loop AI approach for auditing and mitigating social biases from tabular datasets. It uses a graphical causal model to represent causal relationships among different features in the dataset and as a medium to inject domain knowledge. A user can detect the presence of bias against a group, say females, or a subgroup, say black females, by identifying unfair causal relationships in the causal network and using an array of fairness metrics. Thereafter, the user can mitigate bias by refining the causal model and acting on the unfair causal edges. For each interaction, say weakening/deleting a biased causal edge, the system uses a novel method to simulate a new (debiased) dataset based on the current causal model while ensuring a minimal change from the original dataset. Users can visually assess the impact of their interactions on different fairness metrics, utility metrics, data distortion, and the underlying data distribution. Once satisfied, they can download the debiased dataset and use it for any downstream application for fairer predictions. We evaluate D-BIAS by conducting experiments on 3 datasets and also a formal user study. We found that D-BIAS helps reduce bias significantly compared to the baseline debiasing approach across different fairness metrics while incurring little data distortion and a small loss in utility. Moreover, our human-in-the-loop based approach significantly outperforms an automated approach on trust, interpretability and accountability.",
                        "uid": "v-full-1050",
                        "file_name": "",
                        "time_stamp": "2022-10-19T19:34:00Z",
                        "time_start": "2022-10-19T19:34:00Z",
                        "time_end": "2022-10-19T19:36:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "1480",
                        "paper_award": "",
                        "image_caption": "The visual interface of the D-BIAS tool. (A) The Generator panel: used to create the causal network and download the debiased dataset (B) The Causal Network view: shows the causal relations between the attributes of the data, allows user to inject their prior in the system (C) The Evaluation panel: used to choose the sensitive variable, the ML model and displays different evaluation metrics. This snapshot pertains to the Adult Income dataset post-debiasing.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "https://youtu.be/v0VY4fZfsNc",
                        "ff_id": "v0VY4fZfsNc"
                    },
                    {
                        "slot_id": "v-full-1618-pres",
                        "session_id": "full27",
                        "type": "In Person Presentation",
                        "title": "A Unified Comparison of User Modeling Techniques for Predicting Data Interaction and Detecting Exploration Bias",
                        "contributors": [
                            "Sunwoo Ha"
                        ],
                        "authors": [
                            "Sunwoo Ha",
                            "Shayan Monadjemi",
                            "Alvitta Ottley",
                            "Roman Garnett"
                        ],
                        "abstract": "The visual analytics community has proposed several user modeling algorithms to capture and analyze users' interaction behavior in order to assist users in data exploration and insight generation. For example, some can detect exploration biases while others can predict data points that the user will interact with before that interaction occurs. Researchers believe this collection of algorithms can help create more intelligent visual analytics tools. However, the community lacks a rigorous evaluation and comparison of these existing techniques. As a result, there is limited guidance on which method to use and when. Our paper seeks to fill in this missing gap by comparing and ranking eight user modeling algorithms based on their performance on a diverse set of four user study datasets. We analyze exploration bias detection, data interaction prediction, and algorithmic complexity, among other measures. Based on our findings, we highlight open challenges and new directions for analyzing user interactions and visualization provenance.",
                        "uid": "v-full-1618",
                        "file_name": "v-full-1618_Ha_Presentation.mp4",
                        "time_stamp": "2022-10-19T19:36:00Z",
                        "time_start": "2022-10-19T19:36:00Z",
                        "time_end": "2022-10-19T19:46:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "515",
                        "paper_award": "",
                        "image_caption": "In our paper, we present a unified comparison of techniques that predicts the user's next data interaction and detect their exploration bias.The performance of eight previously proposed techniques are evaluated with four unique user study interaction logs.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "https://youtu.be/ZfXk_wFENmY",
                        "ff_id": "ZfXk_wFENmY"
                    },
                    {
                        "slot_id": "v-full-1618-qa",
                        "session_id": "full27",
                        "type": "In Person Q+A",
                        "title": "A Unified Comparison of User Modeling Techniques for Predicting Data Interaction and Detecting Exploration Bias (Q+A)",
                        "contributors": [
                            "Sunwoo Ha"
                        ],
                        "authors": [],
                        "abstract": "The visual analytics community has proposed several user modeling algorithms to capture and analyze users' interaction behavior in order to assist users in data exploration and insight generation. For example, some can detect exploration biases while others can predict data points that the user will interact with before that interaction occurs. Researchers believe this collection of algorithms can help create more intelligent visual analytics tools. However, the community lacks a rigorous evaluation and comparison of these existing techniques. As a result, there is limited guidance on which method to use and when. Our paper seeks to fill in this missing gap by comparing and ranking eight user modeling algorithms based on their performance on a diverse set of four user study datasets. We analyze exploration bias detection, data interaction prediction, and algorithmic complexity, among other measures. Based on our findings, we highlight open challenges and new directions for analyzing user interactions and visualization provenance.",
                        "uid": "v-full-1618",
                        "file_name": "",
                        "time_stamp": "2022-10-19T19:46:00Z",
                        "time_start": "2022-10-19T19:46:00Z",
                        "time_end": "2022-10-19T19:48:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "515",
                        "paper_award": "",
                        "image_caption": "In our paper, we present a unified comparison of techniques that predicts the user's next data interaction and detect their exploration bias.The performance of eight previously proposed techniques are evaluated with four unique user study interaction logs.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "https://youtu.be/ZfXk_wFENmY",
                        "ff_id": "ZfXk_wFENmY"
                    },
                    {
                        "slot_id": "v-full-1151-pres",
                        "session_id": "full27",
                        "type": "In Person Presentation",
                        "title": "Seeing What You Believe or Believing What You See? Belief Biases Correlation Estimation",
                        "contributors": [
                            "Cindy Xiong"
                        ],
                        "authors": [
                            "Cindy Xiong",
                            "Chase Stokes",
                            "Yea-Seul Kim",
                            "Steven Franconeri"
                        ],
                        "abstract": "When an analyst or scientist has a belief about how the world works, their thinking can be biased in favor of that belief. Therefore, one bedrock principle of science is to minimize that bias by testing the predictions of one\u2019s belief against objective data. But interpreting visualized data is a complex perceptual and cognitive process. Through two crowdsourced experiments, we demonstrate that supposedly objective assessments of the strength of a correlational relationship can be influenced by how strongly a viewer believes in the existence of that relationship. Participants viewed scatterplots depicting a relationship between meaningful variable pairs (e.g., number of environmental regulations and air quality) and estimated their correlations. They also estimated the correlation of the same scatterplots labeled instead with generic 'X' and 'Y' axes. In a separate section, they also reported how strongly they believed there to be a correlation between the meaningful variable pairs. Participants estimated correlations more accurately when they viewed scatterplots labeled with generic axes compared to scatterplots labeled with meaningful variable pairs. Furthermore, when viewers believed that two variables should have a strong relationship, they overestimated correlations between those variables by an r-value of about 0.1. When they believed that the variables should be unrelated, they underestimated the correlations by an r-value of about 0.1. While data visualizations are typically thought to present objective truths to the viewer, these results suggest that existing personal beliefs can bias even objective statistical values people extract from data.",
                        "uid": "v-full-1151",
                        "file_name": "v-full-1151_Xiong_Presentation.mp4",
                        "time_stamp": "2022-10-19T19:48:00Z",
                        "time_start": "2022-10-19T19:48:00Z",
                        "time_end": "2022-10-19T19:58:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "671",
                        "paper_award": "",
                        "image_caption": "Your new data have arrived. \nWith the intent of showing a relationship between the strength of \nenvironmental regulations in a region and that region's air quality, \nyou plot your data values in a scatterplot. \nAt first, the correlation is barely noticeable. \nBut you then notice a small set of outliers. \nIf you ignore those, the correlation looks as strong as you expected.\nPeople might induce themselves to see a correlation as weak or strong, \ndepending on existing beliefs about data.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "https://youtu.be/7ISD9WtYChI",
                        "ff_id": "7ISD9WtYChI"
                    },
                    {
                        "slot_id": "v-full-1151-qa",
                        "session_id": "full27",
                        "type": "In Person Q+A",
                        "title": "Seeing What You Believe or Believing What You See? Belief Biases Correlation Estimation (Q+A)",
                        "contributors": [
                            "Cindy Xiong"
                        ],
                        "authors": [],
                        "abstract": "When an analyst or scientist has a belief about how the world works, their thinking can be biased in favor of that belief. Therefore, one bedrock principle of science is to minimize that bias by testing the predictions of one\u2019s belief against objective data. But interpreting visualized data is a complex perceptual and cognitive process. Through two crowdsourced experiments, we demonstrate that supposedly objective assessments of the strength of a correlational relationship can be influenced by how strongly a viewer believes in the existence of that relationship. Participants viewed scatterplots depicting a relationship between meaningful variable pairs (e.g., number of environmental regulations and air quality) and estimated their correlations. They also estimated the correlation of the same scatterplots labeled instead with generic 'X' and 'Y' axes. In a separate section, they also reported how strongly they believed there to be a correlation between the meaningful variable pairs. Participants estimated correlations more accurately when they viewed scatterplots labeled with generic axes compared to scatterplots labeled with meaningful variable pairs. Furthermore, when viewers believed that two variables should have a strong relationship, they overestimated correlations between those variables by an r-value of about 0.1. When they believed that the variables should be unrelated, they underestimated the correlations by an r-value of about 0.1. While data visualizations are typically thought to present objective truths to the viewer, these results suggest that existing personal beliefs can bias even objective statistical values people extract from data.",
                        "uid": "v-full-1151",
                        "file_name": "",
                        "time_stamp": "2022-10-19T19:58:00Z",
                        "time_start": "2022-10-19T19:58:00Z",
                        "time_end": "2022-10-19T20:00:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "671",
                        "paper_award": "",
                        "image_caption": "Your new data have arrived. \nWith the intent of showing a relationship between the strength of \nenvironmental regulations in a region and that region's air quality, \nyou plot your data values in a scatterplot. \nAt first, the correlation is barely noticeable. \nBut you then notice a small set of outliers. \nIf you ignore those, the correlation looks as strong as you expected.\nPeople might induce themselves to see a correlation as weak or strong, \ndepending on existing beliefs about data.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "https://youtu.be/7ISD9WtYChI",
                        "ff_id": "7ISD9WtYChI"
                    },
                    {
                        "slot_id": "v-full-1155-pres",
                        "session_id": "full27",
                        "type": "In Person Presentation",
                        "title": "Data Hunches: Incorporating Personal Knowledge into Visualizations",
                        "contributors": [
                            "Haihan Lin"
                        ],
                        "authors": [
                            "Haihan Lin",
                            "Derya Akbaba",
                            "Miriah Meyer",
                            "Alexander Lex"
                        ],
                        "abstract": "The trouble with data is that it frequently provides only an imperfect representation of a phenomenon of interest. Experts who are familiar with their datasets will often make implicit, mental corrections when analyzing a dataset, or will be cautious not to be overly confident about their findings if caveats are present. However, personal knowledge about the caveats of a dataset is typically not incorporated in a structured way, which is problematic if others who lack that knowledge interpret the data. In this work, we define such analysts' knowledge about datasets as data hunches. We differentiate data hunches from uncertainty and discuss types of hunches. We then explore ways of recording data hunches, and, based on a prototypical design, develop recommendations for designing visualizations that support data hunches. We conclude by discussing various challenges associated with data hunches, including the potential for harm and challenges for trust and privacy. We envision that data hunches will empower analysts to externalize their knowledge, facilitate collaboration and communication, and support the ability to learn from others' data hunches.",
                        "uid": "v-full-1155",
                        "file_name": "v-full-1155_Haihan_Presentation.mp4",
                        "time_stamp": "2022-10-19T20:00:00Z",
                        "time_start": "2022-10-19T20:00:00Z",
                        "time_end": "2022-10-19T20:10:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "647",
                        "paper_award": "",
                        "image_caption": "Three rectangle bars placed as a horizontal bar chart, with name Germany, New Zealand, and Norway. The Germany bar has five sketchy lines vertically placed. Shorter sketchy zigzag line filled bars are placed on top of New Zealand and Norway. And on the New Zealand bar, there is a thumb-up icon with number 4 next to it, and a thumb-down icon with number 1 next to it.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "https://youtu.be/Akb9_1qg-EE",
                        "ff_id": "Akb9_1qg-EE"
                    },
                    {
                        "slot_id": "v-full-1155-qa",
                        "session_id": "full27",
                        "type": "In Person Q+A",
                        "title": "Data Hunches: Incorporating Personal Knowledge into Visualizations (Q+A)",
                        "contributors": [
                            "Haihan Lin"
                        ],
                        "authors": [],
                        "abstract": "The trouble with data is that it frequently provides only an imperfect representation of a phenomenon of interest. Experts who are familiar with their datasets will often make implicit, mental corrections when analyzing a dataset, or will be cautious not to be overly confident about their findings if caveats are present. However, personal knowledge about the caveats of a dataset is typically not incorporated in a structured way, which is problematic if others who lack that knowledge interpret the data. In this work, we define such analysts' knowledge about datasets as data hunches. We differentiate data hunches from uncertainty and discuss types of hunches. We then explore ways of recording data hunches, and, based on a prototypical design, develop recommendations for designing visualizations that support data hunches. We conclude by discussing various challenges associated with data hunches, including the potential for harm and challenges for trust and privacy. We envision that data hunches will empower analysts to externalize their knowledge, facilitate collaboration and communication, and support the ability to learn from others' data hunches.",
                        "uid": "v-full-1155",
                        "file_name": "",
                        "time_stamp": "2022-10-19T20:10:00Z",
                        "time_start": "2022-10-19T20:10:00Z",
                        "time_end": "2022-10-19T20:12:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "647",
                        "paper_award": "",
                        "image_caption": "Three rectangle bars placed as a horizontal bar chart, with name Germany, New Zealand, and Norway. The Germany bar has five sketchy lines vertically placed. Shorter sketchy zigzag line filled bars are placed on top of New Zealand and Norway. And on the New Zealand bar, there is a thumb-up icon with number 4 next to it, and a thumb-down icon with number 1 next to it.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "https://youtu.be/Akb9_1qg-EE",
                        "ff_id": "Akb9_1qg-EE"
                    }
                ]
            },
            {
                "title": "DNA/Genome and Molecular Data/Vis",
                "session_id": "full28",
                "event_prefix": "v-full",
                "track": "ok1",
                "livestream_id": "ok1-wed",
                "session_image": "full28.png",
                "chair": [
                    "Michael Krone"
                ],
                "organizers": [],
                "time_start": "2022-10-19T20:45:00Z",
                "time_end": "2022-10-19T22:00:00Z",
                "discord_category": "",
                "discord_channel": "oklahoma-st-1",
                "discord_channel_id": "1024600861340606484",
                "discord_link": "https://discord.com/channels/978320435554947082/1024600861340606484",
                "slido_link": "https://app.sli.do/event/i28UETZaiRWr1XwX6eKjcf?section=740d9835-fe47-4cb4-b260-353262a7f8dd",
                "youtube_url": "https://youtu.be/EK30lilKKdM",
                "youtube_id": "EK30lilKKdM",
                "zoom_meeting": "",
                "zoom_password": "",
                "zoom_link": "",
                "ff_link": "https://youtu.be/uxF7kSKowpU",
                "ff_playlist": "",
                "time_slots": [
                    {
                        "slot_id": "full28-opening",
                        "session_id": "full28",
                        "type": "In Person Other",
                        "title": "Session Opening",
                        "contributors": [
                            "Michael Krone"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "",
                        "file_name": "",
                        "time_stamp": "2022-10-19T20:45:00Z",
                        "time_start": "2022-10-19T20:45:00Z",
                        "time_end": "2022-10-19T20:45:00Z",
                        "paper_type": "",
                        "keywords": [],
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "v-full-1193-pres",
                        "session_id": "full28",
                        "type": "In Person Presentation",
                        "title": "Multi-View Design Patterns and Responsive Visualization for Genomics Data",
                        "contributors": [
                            "Sehi L'Yi"
                        ],
                        "authors": [
                            "Sehi L'Yi",
                            "Nils Gehlenborg"
                        ],
                        "abstract": "A series of recent studies has focused on designing cross-resolution and cross-device visualizations, i.e., responsive visualization, a concept adopted from responsive web design. However, these studies mainly focused on visualizations with a single view to a small number of views, and there are still unresolved questions about how to design responsive multi-view visualizations. In this paper, we present a reusable and generalizable framework for designing responsive multi-view visualizations focused on genomics data. To gain a better understanding of existing design challenges, we review web-based genomics visualization tools in the wild. By characterizing tools based on a taxonomy of responsive designs, we find that responsiveness is rarely supported in existing tools. To distill insights from the survey results in a systematic way, we classify typical view composition patterns, such as \u201cvertically long,\u201d \u201chorizontally wide,\u201d \u201ccircular,\u201d and \u201ccross-shaped\u201d compositions. We then identify their usability issues in different resolutions that stem from the composition patterns, as well as discussing approaches to address the issues and to make genomics visualizations responsive. By extending the Gosling visualization grammar to support responsive constructs, we show how these approaches can be supported. A valuable follow-up study would be taking different input modalities into account, such as mouse and touch interactions, which was not considered in our study.",
                        "uid": "v-full-1193",
                        "file_name": "v-full-1193_L'yi_Presentation.mp4",
                        "time_stamp": "2022-10-19T20:45:00Z",
                        "time_start": "2022-10-19T20:45:00Z",
                        "time_end": "2022-10-19T20:55:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "626",
                        "paper_award": "",
                        "image_caption": "Through a survey, we classify typical view composition patterns of genomics visualizations, such as vertically long, horizontally wide, circular, and cross-shaped compositions. We then identify their usability issues in different resolutions that stem from the composition patterns, as well as approaches to address the issues and to make genomics visualizations responsive.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "https://youtu.be/vPc0sh_iVB0",
                        "ff_id": "vPc0sh_iVB0"
                    },
                    {
                        "slot_id": "v-full-1193-qa",
                        "session_id": "full28",
                        "type": "In Person Q+A",
                        "title": "Multi-View Design Patterns and Responsive Visualization for Genomics Data (Q+A)",
                        "contributors": [
                            "Sehi L'Yi"
                        ],
                        "authors": [],
                        "abstract": "A series of recent studies has focused on designing cross-resolution and cross-device visualizations, i.e., responsive visualization, a concept adopted from responsive web design. However, these studies mainly focused on visualizations with a single view to a small number of views, and there are still unresolved questions about how to design responsive multi-view visualizations. In this paper, we present a reusable and generalizable framework for designing responsive multi-view visualizations focused on genomics data. To gain a better understanding of existing design challenges, we review web-based genomics visualization tools in the wild. By characterizing tools based on a taxonomy of responsive designs, we find that responsiveness is rarely supported in existing tools. To distill insights from the survey results in a systematic way, we classify typical view composition patterns, such as \u201cvertically long,\u201d \u201chorizontally wide,\u201d \u201ccircular,\u201d and \u201ccross-shaped\u201d compositions. We then identify their usability issues in different resolutions that stem from the composition patterns, as well as discussing approaches to address the issues and to make genomics visualizations responsive. By extending the Gosling visualization grammar to support responsive constructs, we show how these approaches can be supported. A valuable follow-up study would be taking different input modalities into account, such as mouse and touch interactions, which was not considered in our study.",
                        "uid": "v-full-1193",
                        "file_name": "",
                        "time_stamp": "2022-10-19T20:55:00Z",
                        "time_start": "2022-10-19T20:55:00Z",
                        "time_end": "2022-10-19T20:57:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "626",
                        "paper_award": "",
                        "image_caption": "Through a survey, we classify typical view composition patterns of genomics visualizations, such as vertically long, horizontally wide, circular, and cross-shaped compositions. We then identify their usability issues in different resolutions that stem from the composition patterns, as well as approaches to address the issues and to make genomics visualizations responsive.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "https://youtu.be/vPc0sh_iVB0",
                        "ff_id": "vPc0sh_iVB0"
                    },
                    {
                        "slot_id": "v-tvcg-9523759-pres",
                        "session_id": "full28",
                        "type": "Virtual Presentation (pre-recorded)",
                        "title": "Vivern \u2013 A Virtual Environment for Multiscale Visualization and Modeling of DNA Nanostructures",
                        "contributors": [
                            "David Ku\u0165\u00e1k"
                        ],
                        "authors": [
                            "David Ku\u0165\u00e1k",
                            "Matias Nicol\u00e1s Selzer",
                            "Jan By\u0161ka",
                            "Mar\u00eda Luj\u00e1n Ganuza",
                            "Ivan Bari\u0161i\u0107",
                            "Barbora Kozl\u00edkov\u00e1",
                            "Haichao Miao"
                        ],
                        "abstract": "DNA nanostructures offer promising applications, particularly in the biomedical domain, as they can be used for targeted drug delivery, construction of nanorobots, or as a basis for molecular motors. One of the most prominent techniques for assembling these structures is DNA origami. Nowadays, desktop applications are used for the in silico design of such structures. However, as such structures are often spatially complex, their assembly and analysis are complicated. Since virtual reality was proven to be advantageous for such spatial-related tasks and there are no existing VR solutions focused on this domain, we propose Vivern, a VR application that allows domain experts to design and visually examine DNA origami nanostructures. Our approach presents different abstracted visual representations of the nanostructures, various color schemes, and an ability to place several DNA nanostructures and proteins in one environment, thus allowing for the detailed analysis of complex assemblies. We also present two novel examination tools, the Magic Scale Lens and the DNA Untwister, that allow the experts to visually embed different representations into local regions to preserve the context and support detailed investigation. To showcase the capabilities of our solution, prototypes of novel nanodevices conceptualized by our collaborating experts, such as DNA-protein hybrid structures and DNA origami superstructures, are presented. Finally, the results of two rounds of evaluations are summarized. They demonstrate the advantages of our solution, especially for scenarios where current desktop tools are very limited, while also presenting possible future research directions.",
                        "uid": "v-tvcg-9523759",
                        "file_name": "v-tvcg-9523759_Kutak_Presentation.mp4",
                        "time_stamp": "2022-10-19T20:57:00Z",
                        "time_start": "2022-10-19T20:57:00Z",
                        "time_end": "2022-10-19T21:07:50Z",
                        "paper_type": "full",
                        "keywords": [
                            "Virtual reality, abstraction, DNA origami, nanostructures, visualization, focus+context, interaction, in silico modeling, nanotechnology, multiscale, magic scale lens"
                        ],
                        "has_image": "1",
                        "has_video": "650",
                        "paper_award": "",
                        "image_caption": "Screenshot of the Vivern application capturing the user in action. The scene contains two DNA origami structures. One in the back is visualized in single-strand scale,\nwhile the structure in front of the user is currently being modified. This structure started in a double-strand scale and the user used the provided focus+context techniques to embed additional scales, namely single-stranded one and then nucleotide one.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "https://youtu.be/ljlX0FXnQC0",
                        "ff_id": "ljlX0FXnQC0"
                    },
                    {
                        "slot_id": "v-tvcg-9523759-qa",
                        "session_id": "full28",
                        "type": "Virtual Q+A",
                        "title": "Vivern \u2013 A Virtual Environment for Multiscale Visualization and Modeling of DNA Nanostructures (Q+A)",
                        "contributors": [
                            "David Ku\u0165\u00e1k"
                        ],
                        "authors": [],
                        "abstract": "DNA nanostructures offer promising applications, particularly in the biomedical domain, as they can be used for targeted drug delivery, construction of nanorobots, or as a basis for molecular motors. One of the most prominent techniques for assembling these structures is DNA origami. Nowadays, desktop applications are used for the in silico design of such structures. However, as such structures are often spatially complex, their assembly and analysis are complicated. Since virtual reality was proven to be advantageous for such spatial-related tasks and there are no existing VR solutions focused on this domain, we propose Vivern, a VR application that allows domain experts to design and visually examine DNA origami nanostructures. Our approach presents different abstracted visual representations of the nanostructures, various color schemes, and an ability to place several DNA nanostructures and proteins in one environment, thus allowing for the detailed analysis of complex assemblies. We also present two novel examination tools, the Magic Scale Lens and the DNA Untwister, that allow the experts to visually embed different representations into local regions to preserve the context and support detailed investigation. To showcase the capabilities of our solution, prototypes of novel nanodevices conceptualized by our collaborating experts, such as DNA-protein hybrid structures and DNA origami superstructures, are presented. Finally, the results of two rounds of evaluations are summarized. They demonstrate the advantages of our solution, especially for scenarios where current desktop tools are very limited, while also presenting possible future research directions.",
                        "uid": "v-tvcg-9523759",
                        "file_name": "",
                        "time_stamp": "2022-10-19T21:07:50Z",
                        "time_start": "2022-10-19T21:07:50Z",
                        "time_end": "2022-10-19T21:09:00Z",
                        "paper_type": "full",
                        "keywords": [
                            "Virtual reality, abstraction, DNA origami, nanostructures, visualization, focus+context, interaction, in silico modeling, nanotechnology, multiscale, magic scale lens"
                        ],
                        "has_image": "1",
                        "has_video": "650",
                        "paper_award": "",
                        "image_caption": "Screenshot of the Vivern application capturing the user in action. The scene contains two DNA origami structures. One in the back is visualized in single-strand scale,\nwhile the structure in front of the user is currently being modified. This structure started in a double-strand scale and the user used the provided focus+context techniques to embed additional scales, namely single-stranded one and then nucleotide one.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "https://youtu.be/ljlX0FXnQC0",
                        "ff_id": "ljlX0FXnQC0"
                    },
                    {
                        "slot_id": "v-full-1212-pres",
                        "session_id": "full28",
                        "type": "In Person Presentation",
                        "title": "GenoREC: A Recommendation System for Interactive Genomics Data Visualization",
                        "contributors": [
                            "Aditeya Pandey"
                        ],
                        "authors": [
                            "Aditeya Pandey",
                            "Sehi L'Yi",
                            "Qianwen Wang",
                            "Michelle A. Borkin",
                            "Nils Gehlenborg"
                        ],
                        "abstract": "Interpretation of genomics data is critically reliant on the application of a wide range of visualization tools. A large number of visualization techniques for genomics data and different analysis tasks pose a significant challenge for analysts: which visualization technique is most likely to help them generate insights into their data? Since genomics analysts typically have limited training in data visualization, their choices are often based on trial and error or guided by technical details, such as data formats that a specific tool can load. This approach prevents them from making effective visualization choices for the many combinations of data types and analysis questions they encounter in their work. Visualization recommendation systems assist non-experts in creating data visualization by recommending appropriate visualizations based on the data and task characteristics. However, existing visualization recommendation systems are not designed to handle domain-specific problems. To address these challenges, we designed GenoREC, a novel visualization recommendation system for genomics. GenoREC enables genomics analysts to select effective visualizations based on a description of their data and analysis tasks. Here, we present the recommendation model that uses a knowledge-based method for choosing appropriate visualizations and a web application that enables analysts to input their requirements, explore recommended visualizations, and export them for their usage. Furthermore, we present the results of two user studies demonstrating that GenoREC recommends visualizations that are both accepted by domain experts and suited to address the given genomics analysis problem. All supplemental materials are available at https://osf.io/y73pt/.",
                        "uid": "v-full-1212",
                        "file_name": "v-full-1212_Pandey_Presentation.mp4",
                        "time_stamp": "2022-10-19T21:09:00Z",
                        "time_start": "2022-10-19T21:09:00Z",
                        "time_end": "2022-10-19T21:19:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "929",
                        "paper_award": "",
                        "image_caption": "GenoREC\u2019s user interface depicting the user input panel and recommended visualisations  from the system.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "https://youtu.be/MK8OcbGlaCk",
                        "ff_id": "MK8OcbGlaCk"
                    },
                    {
                        "slot_id": "v-full-1212-qa",
                        "session_id": "full28",
                        "type": "In Person Q+A",
                        "title": "GenoREC: A Recommendation System for Interactive Genomics Data Visualization (Q+A)",
                        "contributors": [
                            "Aditeya Pandey"
                        ],
                        "authors": [],
                        "abstract": "Interpretation of genomics data is critically reliant on the application of a wide range of visualization tools. A large number of visualization techniques for genomics data and different analysis tasks pose a significant challenge for analysts: which visualization technique is most likely to help them generate insights into their data? Since genomics analysts typically have limited training in data visualization, their choices are often based on trial and error or guided by technical details, such as data formats that a specific tool can load. This approach prevents them from making effective visualization choices for the many combinations of data types and analysis questions they encounter in their work. Visualization recommendation systems assist non-experts in creating data visualization by recommending appropriate visualizations based on the data and task characteristics. However, existing visualization recommendation systems are not designed to handle domain-specific problems. To address these challenges, we designed GenoREC, a novel visualization recommendation system for genomics. GenoREC enables genomics analysts to select effective visualizations based on a description of their data and analysis tasks. Here, we present the recommendation model that uses a knowledge-based method for choosing appropriate visualizations and a web application that enables analysts to input their requirements, explore recommended visualizations, and export them for their usage. Furthermore, we present the results of two user studies demonstrating that GenoREC recommends visualizations that are both accepted by domain experts and suited to address the given genomics analysis problem. All supplemental materials are available at https://osf.io/y73pt/.",
                        "uid": "v-full-1212",
                        "file_name": "",
                        "time_stamp": "2022-10-19T21:19:00Z",
                        "time_start": "2022-10-19T21:19:00Z",
                        "time_end": "2022-10-19T21:21:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "929",
                        "paper_award": "",
                        "image_caption": "GenoREC\u2019s user interface depicting the user input panel and recommended visualisations  from the system.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "https://youtu.be/MK8OcbGlaCk",
                        "ff_id": "MK8OcbGlaCk"
                    },
                    {
                        "slot_id": "v-full-1175-pres",
                        "session_id": "full28",
                        "type": "In Person Presentation",
                        "title": "sMolBoxes: Dataflow Model for Molecular Dynamics Exploration",
                        "contributors": [
                            "Pavol Ulbrich"
                        ],
                        "authors": [
                            "Pavol Ulbrich",
                            "Manuela Waldner",
                            "Katar\u00edna Furmanov\u00e1",
                            "S\u00e9rgio M. Marques",
                            "David Bedn\u00e1\u0159",
                            "Barbora Kozlikova",
                            "Jan By\u0161ka"
                        ],
                        "abstract": "We present sMolBoxes, a dataflow representation for the exploration and analysis of long molecular dynamics (MD) simulations. When MD simulations reach millions of snapshots, a frame-by-frame observation is not feasible anymore. Thus, biochemists rely to a large extent only on quantitative analysis of geometric and physico-chemical properties. However, the usage of abstract methods to study inherently spatial data hinders the exploration and poses a considerable workload. sMolBoxes link quantitative analysis of a user-defined set of properties with interactive 3D visualizations. They enable visual explanations of molecular behaviors, which lead to an efficient discovery of biochemically significant parts of the MD simulation. sMolBoxes follow a node-based model for flexible definition, combination, and immediate evaluation of properties to be investigated. Progressive analytics enable fluid switching between multiple properties, which facilitates hypothesis generation. Each sMolBox provides quick insight to an observed property or function, available in more detail in the bigBox View. The case studies illustrate that even with relatively few sMolBoxes, it is possible to express complex analytical tasks, and their use in exploratory analysis is perceived as more efficient than traditional scripting-based methods.",
                        "uid": "v-full-1175",
                        "file_name": "v-full-1175_Ulbrich_Presentation.mp4",
                        "time_stamp": "2022-10-19T21:21:00Z",
                        "time_start": "2022-10-19T21:21:00Z",
                        "time_end": "2022-10-19T21:31:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "386",
                        "paper_award": "",
                        "image_caption": "An overview of the sMolBoxes interface for molecular dynamics exploration, consisting of the 3D View, the Canvas with sMolBox Nodes, and the detailed bigBox View for navigation through the 3D animation.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "https://youtu.be/F8swLXWHwmU",
                        "ff_id": "F8swLXWHwmU"
                    },
                    {
                        "slot_id": "v-full-1175-qa",
                        "session_id": "full28",
                        "type": "In Person Q+A",
                        "title": "sMolBoxes: Dataflow Model for Molecular Dynamics Exploration (Q+A)",
                        "contributors": [
                            "Pavol Ulbrich"
                        ],
                        "authors": [],
                        "abstract": "We present sMolBoxes, a dataflow representation for the exploration and analysis of long molecular dynamics (MD) simulations. When MD simulations reach millions of snapshots, a frame-by-frame observation is not feasible anymore. Thus, biochemists rely to a large extent only on quantitative analysis of geometric and physico-chemical properties. However, the usage of abstract methods to study inherently spatial data hinders the exploration and poses a considerable workload. sMolBoxes link quantitative analysis of a user-defined set of properties with interactive 3D visualizations. They enable visual explanations of molecular behaviors, which lead to an efficient discovery of biochemically significant parts of the MD simulation. sMolBoxes follow a node-based model for flexible definition, combination, and immediate evaluation of properties to be investigated. Progressive analytics enable fluid switching between multiple properties, which facilitates hypothesis generation. Each sMolBox provides quick insight to an observed property or function, available in more detail in the bigBox View. The case studies illustrate that even with relatively few sMolBoxes, it is possible to express complex analytical tasks, and their use in exploratory analysis is perceived as more efficient than traditional scripting-based methods.",
                        "uid": "v-full-1175",
                        "file_name": "",
                        "time_stamp": "2022-10-19T21:31:00Z",
                        "time_start": "2022-10-19T21:31:00Z",
                        "time_end": "2022-10-19T21:33:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "386",
                        "paper_award": "",
                        "image_caption": "An overview of the sMolBoxes interface for molecular dynamics exploration, consisting of the 3D View, the Canvas with sMolBox Nodes, and the detailed bigBox View for navigation through the 3D animation.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "https://youtu.be/F8swLXWHwmU",
                        "ff_id": "F8swLXWHwmU"
                    },
                    {
                        "slot_id": "v-tvcg-9627526-pres",
                        "session_id": "full28",
                        "type": "In Person Presentation",
                        "title": "Molecumentary: Adaptable Narrated Documentaries Using Molecular Visualization",
                        "contributors": [
                            "David Kouril",
                            "Ond\u0159ej Strnad"
                        ],
                        "authors": [
                            "David Kou\u0159il",
                            "Ond\u0159ej Strnad",
                            "Peter Mindek",
                            "Sarkis Halladjian",
                            "Tobias Isenberg",
                            "M. Eduard Gr\u00f6ller",
                            "Ivan Viola"
                        ],
                        "abstract": "We present a method for producing documentary-style content using real-time scientific visualization. We introduce molecumentaries, i.e., molecular documentaries featuring structural models from molecular biology, created through adaptable methods instead of the rigid traditional production pipeline. Our work is motivated by the rapid evolution of scientific visualization and it potential in science dissemination. Without some form of explanation or guidance, however, novices and lay-persons often find it difficult to gain insights from the visualization itself. We integrate such knowledge using the verbal channel and provide it along an engaging visual presentation. To realize the synthesis of a molecumentary, we provide technical solutions along two major production steps: (1) preparing a story structure and (2) turning the story into a concrete narrative. In the first step, we compile information about the model from heterogeneous sources into a story graph. We combine local knowledge with external sources to complete the story graph and enrich the final result. In the second step, we synthesize a narrative, i.e., story elements presented in sequence, using the story graph. We then traverse the story graph and generate a virtual tour, using automated camera and visualization transitions. We turn texts written by domain experts into verbal representations using text-to-speech functionality and provide them as a commentary. Using the described framework, we synthesize fly-throughs with descriptions: automatic ones that mimic a manually authored documentary or semi-automatic ones which guide the documentary narrative solely through curated textual input.",
                        "uid": "v-tvcg-9627526",
                        "file_name": "v-tvcg-9627526_Kouril_Presentation.mp4",
                        "time_stamp": "2022-10-19T21:33:00Z",
                        "time_start": "2022-10-19T21:33:00Z",
                        "time_end": "2022-10-19T21:43:00Z",
                        "paper_type": "full",
                        "keywords": [
                            "Virtual tour, audio, biological data, storytelling, illustrative visualization."
                        ],
                        "has_image": "1",
                        "has_video": "612",
                        "paper_award": "",
                        "image_caption": "A snapshot of a molecumentary: Our framework generates guided tours that describe complex molecular models, such as the HIV pictured here.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "https://youtu.be/ShtTPmexCa8",
                        "ff_id": "ShtTPmexCa8"
                    },
                    {
                        "slot_id": "v-tvcg-9627526-qa",
                        "session_id": "full28",
                        "type": "In Person Q+A",
                        "title": "Molecumentary: Adaptable Narrated Documentaries Using Molecular Visualization (Q+A)",
                        "contributors": [
                            "David Kouril",
                            "Ond\u0159ej Strnad"
                        ],
                        "authors": [],
                        "abstract": "We present a method for producing documentary-style content using real-time scientific visualization. We introduce molecumentaries, i.e., molecular documentaries featuring structural models from molecular biology, created through adaptable methods instead of the rigid traditional production pipeline. Our work is motivated by the rapid evolution of scientific visualization and it potential in science dissemination. Without some form of explanation or guidance, however, novices and lay-persons often find it difficult to gain insights from the visualization itself. We integrate such knowledge using the verbal channel and provide it along an engaging visual presentation. To realize the synthesis of a molecumentary, we provide technical solutions along two major production steps: (1) preparing a story structure and (2) turning the story into a concrete narrative. In the first step, we compile information about the model from heterogeneous sources into a story graph. We combine local knowledge with external sources to complete the story graph and enrich the final result. In the second step, we synthesize a narrative, i.e., story elements presented in sequence, using the story graph. We then traverse the story graph and generate a virtual tour, using automated camera and visualization transitions. We turn texts written by domain experts into verbal representations using text-to-speech functionality and provide them as a commentary. Using the described framework, we synthesize fly-throughs with descriptions: automatic ones that mimic a manually authored documentary or semi-automatic ones which guide the documentary narrative solely through curated textual input.",
                        "uid": "v-tvcg-9627526",
                        "file_name": "",
                        "time_stamp": "2022-10-19T21:43:00Z",
                        "time_start": "2022-10-19T21:43:00Z",
                        "time_end": "2022-10-19T21:45:00Z",
                        "paper_type": "full",
                        "keywords": [
                            "Virtual tour, audio, biological data, storytelling, illustrative visualization."
                        ],
                        "has_image": "1",
                        "has_video": "612",
                        "paper_award": "",
                        "image_caption": "A snapshot of a molecumentary: Our framework generates guided tours that describe complex molecular models, such as the HIV pictured here.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "https://youtu.be/ShtTPmexCa8",
                        "ff_id": "ShtTPmexCa8"
                    },
                    {
                        "slot_id": "v-full-1578-pres",
                        "session_id": "full28",
                        "type": "Virtual Presentation (pre-recorded)",
                        "title": "Polyphony: an Interactive Transfer Learning Framework for Single-Cell Data Analysis",
                        "contributors": [
                            "Furui Cheng"
                        ],
                        "authors": [
                            "Furui Cheng",
                            "Mark S Keller",
                            "Huamin Qu",
                            "Nils Gehlenborg",
                            "Qianwen Wang"
                        ],
                        "abstract": "Reference-based cell-type annotation can significantly reduce time and effort in single-cell analysis by transferring labels from a previously-annotated dataset to a new dataset. However, label transfer by end-to-end computational methods is challenging due to the entanglement of technical (e.g., from different sequencing batches or techniques) and biological (e.g., from different cellular microenvironments) variations, only the first of which must be removed. To address this issue, we propose Polyphony, an interactive transfer learning (ITL) framework, to complement biologists' knowledge with advanced computational methods. Polyphony is motivated and guided by domain experts' needs for a controllable, interactive, and algorithm-assisted annotation process, identified through interviews with seven biologists. We introduce anchors, i.e., analogous cell populations across datasets, as a paradigm to explain the computational process and collect user feedback for model improvement. We further design a set of visualizations and interactions to empower users to add, delete, or modify anchors, resulting in refined cell type annotations. The effectiveness of this approach is demonstrated through quantitative experiments, two hypothetical use cases, and interviews with two biologists. The results show that our anchor-based ITL method takes advantage of both human and machine intelligence in annotating massive single-cell datasets.",
                        "uid": "v-full-1578",
                        "file_name": "v-full-1578_Cheng_Presentation.mp4",
                        "time_stamp": "2022-10-19T21:45:00Z",
                        "time_start": "2022-10-19T21:45:00Z",
                        "time_end": "2022-10-19T21:54:48Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "588",
                        "paper_award": "",
                        "image_caption": "The interface of Polyphony contains three views: the comparison view, the anchor set view, and the marker view. The comparison view provides an overview of the joint embedding space and offers users interactions to inspect, delete, and add anchors. The anchor set view orders the anchors in a table, supporting inspection and comparing different anchors. The marker view shows the significant genes for the query and reference cells from a focal anchor.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "https://youtu.be/-_vFKtJsliQ",
                        "ff_id": "-_vFKtJsliQ"
                    },
                    {
                        "slot_id": "v-full-1578-qa",
                        "session_id": "full28",
                        "type": "Virtual Q+A",
                        "title": "Polyphony: an Interactive Transfer Learning Framework for Single-Cell Data Analysis (Q+A)",
                        "contributors": [
                            "Furui Cheng"
                        ],
                        "authors": [],
                        "abstract": "Reference-based cell-type annotation can significantly reduce time and effort in single-cell analysis by transferring labels from a previously-annotated dataset to a new dataset. However, label transfer by end-to-end computational methods is challenging due to the entanglement of technical (e.g., from different sequencing batches or techniques) and biological (e.g., from different cellular microenvironments) variations, only the first of which must be removed. To address this issue, we propose Polyphony, an interactive transfer learning (ITL) framework, to complement biologists' knowledge with advanced computational methods. Polyphony is motivated and guided by domain experts' needs for a controllable, interactive, and algorithm-assisted annotation process, identified through interviews with seven biologists. We introduce anchors, i.e., analogous cell populations across datasets, as a paradigm to explain the computational process and collect user feedback for model improvement. We further design a set of visualizations and interactions to empower users to add, delete, or modify anchors, resulting in refined cell type annotations. The effectiveness of this approach is demonstrated through quantitative experiments, two hypothetical use cases, and interviews with two biologists. The results show that our anchor-based ITL method takes advantage of both human and machine intelligence in annotating massive single-cell datasets.",
                        "uid": "v-full-1578",
                        "file_name": "",
                        "time_stamp": "2022-10-19T21:54:48Z",
                        "time_start": "2022-10-19T21:54:48Z",
                        "time_end": "2022-10-19T21:57:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "588",
                        "paper_award": "",
                        "image_caption": "The interface of Polyphony contains three views: the comparison view, the anchor set view, and the marker view. The comparison view provides an overview of the joint embedding space and offers users interactions to inspect, delete, and add anchors. The anchor set view orders the anchors in a table, supporting inspection and comparing different anchors. The marker view shows the significant genes for the query and reference cells from a focal anchor.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "https://youtu.be/-_vFKtJsliQ",
                        "ff_id": "-_vFKtJsliQ"
                    }
                ]
            },
            {
                "title": "Digital Humanities, e-Commerce, and Engineering",
                "session_id": "full29",
                "event_prefix": "v-full",
                "track": "ok1",
                "livestream_id": "ok1-thu",
                "session_image": "full29.png",
                "chair": [
                    "Enrico Bertini"
                ],
                "organizers": [],
                "time_start": "2022-10-20T15:45:00Z",
                "time_end": "2022-10-20T17:00:00Z",
                "discord_category": "",
                "discord_channel": "oklahoma-st-1",
                "discord_channel_id": "1024600861340606484",
                "discord_link": "https://discord.com/channels/978320435554947082/1024600861340606484",
                "slido_link": "https://app.sli.do/event/i28UETZaiRWr1XwX6eKjcf?section=740d9835-fe47-4cb4-b260-353262a7f8dd",
                "youtube_url": "https://youtu.be/x76ysAHpetE",
                "youtube_id": "x76ysAHpetE",
                "zoom_meeting": "",
                "zoom_password": "",
                "zoom_link": "",
                "ff_link": "https://youtu.be/v4l7ENOqJoo",
                "ff_playlist": "",
                "time_slots": [
                    {
                        "slot_id": "full29-opening",
                        "session_id": "full29",
                        "type": "In Person Other",
                        "title": "Session Opening",
                        "contributors": [
                            "Enrico Bertini"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "",
                        "file_name": "",
                        "time_stamp": "2022-10-20T15:45:00Z",
                        "time_start": "2022-10-20T15:45:00Z",
                        "time_end": "2022-10-20T15:45:00Z",
                        "paper_type": "",
                        "keywords": [],
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "v-full-1426-pres",
                        "session_id": "full29",
                        "type": "In Person Presentation",
                        "title": "CohortVA: A Visual Analytic System for Interactive Exploration of Cohorts based on Historical Data",
                        "contributors": [
                            "Wei Zhang",
                            "Jason Wong"
                        ],
                        "authors": [
                            "Wei Zhang",
                            "Jason Kamkwai Wong",
                            "Xumeng Wang",
                            "Youcheng Gong",
                            "Rongchen Zhu",
                            "Kai Liu",
                            "Zihan Yan",
                            "Siwei Tan",
                            "Huamin Qu",
                            "Siming Chen",
                            "Wei Chen"
                        ],
                        "abstract": "In history research, cohort analysis seeks to identify social structures and figure mobilities by studying the group-based behavior of historical figures. Prior works mainly employ automatic data mining approaches, lacking effective visual explanation. In this paper, we present CohortVA, an interactive visual analytic approach that enables historians to incorporate expertise and insight into the iterative exploration process. The kernel of CohortVA is a novel identification model that generates candidate cohorts and constructs cohort features by means of pre-built knowledge graphs constructed from large-scale history databases. We propose a set of coordinated views to illustrate identified cohorts and features coupled with historical events and figure profiles. Two case studies and interviews with historians demonstrate that CohortVA can greatly enhance the capabilities of cohort identifications, figure authentications, and hypothesis generation.",
                        "uid": "v-full-1426",
                        "file_name": "v-full-1426_Zhang_Presentation.mp4",
                        "time_stamp": "2022-10-20T15:45:00Z",
                        "time_start": "2022-10-20T15:45:00Z",
                        "time_end": "2022-10-20T15:55:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "540",
                        "paper_award": "",
                        "image_caption": "We present CohortVA, an interactive visual analytic system for historians to identify and explore historical cohorts. Given an initial group of figures, the cohort identification model extracts their common features and identifies potential cohorts to improve the research efficiency. CohortVA\u2019s visual interface provides various supporting information for historians to cross-check those results, fostering trust in the system and a deeper understanding of cohorts. Case studies and historian interviews demonstrate our system\u2019s usefulness and effectiveness. We summarized the learned lessons and design implications, which we believe will guide system designers in dealing with historical data and working with historians.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "https://youtu.be/MlxXPJ5FN1A",
                        "ff_id": "MlxXPJ5FN1A"
                    },
                    {
                        "slot_id": "v-full-1426-qa",
                        "session_id": "full29",
                        "type": "In Person Q+A",
                        "title": "CohortVA: A Visual Analytic System for Interactive Exploration of Cohorts based on Historical Data (Q+A)",
                        "contributors": [
                            "Wei Zhang",
                            "Jason Wong"
                        ],
                        "authors": [],
                        "abstract": "In history research, cohort analysis seeks to identify social structures and figure mobilities by studying the group-based behavior of historical figures. Prior works mainly employ automatic data mining approaches, lacking effective visual explanation. In this paper, we present CohortVA, an interactive visual analytic approach that enables historians to incorporate expertise and insight into the iterative exploration process. The kernel of CohortVA is a novel identification model that generates candidate cohorts and constructs cohort features by means of pre-built knowledge graphs constructed from large-scale history databases. We propose a set of coordinated views to illustrate identified cohorts and features coupled with historical events and figure profiles. Two case studies and interviews with historians demonstrate that CohortVA can greatly enhance the capabilities of cohort identifications, figure authentications, and hypothesis generation.",
                        "uid": "v-full-1426",
                        "file_name": "",
                        "time_stamp": "2022-10-20T15:55:00Z",
                        "time_start": "2022-10-20T15:55:00Z",
                        "time_end": "2022-10-20T15:57:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "540",
                        "paper_award": "",
                        "image_caption": "We present CohortVA, an interactive visual analytic system for historians to identify and explore historical cohorts. Given an initial group of figures, the cohort identification model extracts their common features and identifies potential cohorts to improve the research efficiency. CohortVA\u2019s visual interface provides various supporting information for historians to cross-check those results, fostering trust in the system and a deeper understanding of cohorts. Case studies and historian interviews demonstrate our system\u2019s usefulness and effectiveness. We summarized the learned lessons and design implications, which we believe will guide system designers in dealing with historical data and working with historians.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "https://youtu.be/MlxXPJ5FN1A",
                        "ff_id": "MlxXPJ5FN1A"
                    },
                    {
                        "slot_id": "v-full-1421-pres",
                        "session_id": "full29",
                        "type": "In Person Presentation",
                        "title": "PromotionLens: Inspecting Promotion Strategies of Online E-commerce via Visual Analytics",
                        "contributors": [
                            "Chenyang Zhang"
                        ],
                        "authors": [
                            "Chenyang Zhang",
                            "Xiyuan Wang",
                            "Chuyi Zhao",
                            "Yijing Ren",
                            "Tianyu Zhang",
                            "Zhenhui Peng",
                            "Xiaomeng Fan",
                            "Xiaojuan Ma",
                            "Quan Li"
                        ],
                        "abstract": "Promotions are commonly used by e-commerce merchants to boost sales. The efficacy of different promotion strategies can help sellers adapt their offering to customer demand in order to survive and thrive. Current approaches to designing promotion strategies are either based on econometrics, which may not scale to large amounts of sales data, or are spontaneous and provide little explanation of sales volume. Moreover, accurately measuring the effects of promotion designs and making bootstrappable adjustments accordingly remains a challenge due to the incompleteness and complexity of the information describing promotion strategies and their market environments. We present PromotionLens, a visual analytics system for exploring, comparing, and modeling the impact of various promotion strategies. Our approach combines representative multivariant time-series forecasting models and well-designed visualizations to demonstrate and explain the impact of sales and promotional factors, and to support \"what-if\" analysis of promotions. Two case studies, expert feedback, and a qualitative user study demonstrate the efficacy of PromotionLens.",
                        "uid": "v-full-1421",
                        "file_name": "v-full-1421_Zhang_Presentation.mp4",
                        "time_stamp": "2022-10-20T15:57:00Z",
                        "time_start": "2022-10-20T15:57:00Z",
                        "time_end": "2022-10-20T16:07:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "619",
                        "paper_award": "",
                        "image_caption": "Promotions are commonly used by e-commerce merchants to boost sales. The efficacy of different promotion strategies can help sellers adapt their offering to customer demand in order to survive and thrive. We present PromotionLens, a visual analytics system for exploring, comparing, and modeling the impact of various promotional strategies. Our approach combines representative multivariant time-series forecasting models and well-designed visualizations to demonstrate and explain the impact of sales and promotional factors, and to support \u201cwhat-if\u201d analysis of promotions. Two case studies, expert feedback, and a qualitative user study demonstrate the efficacy of PromotionLens.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "https://youtu.be/pWUqTkf0S74",
                        "ff_id": "pWUqTkf0S74"
                    },
                    {
                        "slot_id": "v-full-1421-qa",
                        "session_id": "full29",
                        "type": "In Person Q+A",
                        "title": "PromotionLens: Inspecting Promotion Strategies of Online E-commerce via Visual Analytics (Q+A)",
                        "contributors": [
                            "Chenyang Zhang"
                        ],
                        "authors": [],
                        "abstract": "Promotions are commonly used by e-commerce merchants to boost sales. The efficacy of different promotion strategies can help sellers adapt their offering to customer demand in order to survive and thrive. Current approaches to designing promotion strategies are either based on econometrics, which may not scale to large amounts of sales data, or are spontaneous and provide little explanation of sales volume. Moreover, accurately measuring the effects of promotion designs and making bootstrappable adjustments accordingly remains a challenge due to the incompleteness and complexity of the information describing promotion strategies and their market environments. We present PromotionLens, a visual analytics system for exploring, comparing, and modeling the impact of various promotion strategies. Our approach combines representative multivariant time-series forecasting models and well-designed visualizations to demonstrate and explain the impact of sales and promotional factors, and to support \"what-if\" analysis of promotions. Two case studies, expert feedback, and a qualitative user study demonstrate the efficacy of PromotionLens.",
                        "uid": "v-full-1421",
                        "file_name": "",
                        "time_stamp": "2022-10-20T16:07:00Z",
                        "time_start": "2022-10-20T16:07:00Z",
                        "time_end": "2022-10-20T16:09:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "619",
                        "paper_award": "",
                        "image_caption": "Promotions are commonly used by e-commerce merchants to boost sales. The efficacy of different promotion strategies can help sellers adapt their offering to customer demand in order to survive and thrive. We present PromotionLens, a visual analytics system for exploring, comparing, and modeling the impact of various promotional strategies. Our approach combines representative multivariant time-series forecasting models and well-designed visualizations to demonstrate and explain the impact of sales and promotional factors, and to support \u201cwhat-if\u201d analysis of promotions. Two case studies, expert feedback, and a qualitative user study demonstrate the efficacy of PromotionLens.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "https://youtu.be/pWUqTkf0S74",
                        "ff_id": "pWUqTkf0S74"
                    },
                    {
                        "slot_id": "v-full-1547-pres",
                        "session_id": "full29",
                        "type": "In Person Presentation",
                        "title": "Interactive Visual Analysis of Structure-borne Noise Data",
                        "contributors": [
                            "Kresimir Matkovic",
                            "Rainer Splechtna"
                        ],
                        "authors": [
                            "Rainer Splechtna",
                            "Denis Gracanin",
                            "Goran Todorovic",
                            "Stanislav Goja",
                            "Boris Bedic",
                            "Helwig Hauser",
                            "Kresimir Matkovic"
                        ],
                        "abstract": "Numerical simulation has become omnipresent in the automotive domain, posing new challenges such as high-dimensional parameter spaces and large as well as incomplete and multi-faceted data. In this design study, we show how interactive visual exploration and analysis of high-dimensional, spectral data from noise simulation can facilitate design improvements in the context of conflicting criteria. Here, we focus on structure-borne noise, i.e., noise from vibrating mechanical parts. Detecting problematic noise\n sources early in the design and production process is essential for reducing a product\u2019s development costs and its time to market. In a close collaboration of visualization and automotive engineering, we designed a new, interactive approach to quickly identify and\n analyze critical noise sources, also contributing to an improved understanding of the analyzed system. Several carefully designed, interactive linked views enable the exploration of noises, vibrations, and harshness at multiple levels of detail, both in the frequency and spatial domain. This enables swift and smooth changes of perspective; selections in the frequency domain are immediately reflected in the spatial domain, and vice versa. Noise sources are quickly identified and shown in the context of their neighborhood, both in the frequency and spatial domain. We propose a novel drill-down view, especially tailored to noise data analysis. Split boxplots and synchronized 3D geometry views support comparison tasks. With this solution, engineers iterate over design optimizations much faster, while maintaining a good overview at each iteration. We evaluated the new approach in the automotive industry, studying noise\n simulation data for an internal combustion engine.",
                        "uid": "v-full-1547",
                        "file_name": "v-full-1547_Splechtna_Presentation.mp4",
                        "time_stamp": "2022-10-20T16:09:00Z",
                        "time_start": "2022-10-20T16:09:00Z",
                        "time_end": "2022-10-20T16:19:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "574",
                        "paper_award": "",
                        "image_caption": "The coordinated multiple views solution for interactive visual analysis of structure-borne noise simulation data. The drill-down view consists of three panes. The overview matrix pane provides aggregated values for individual engine parts across the whole frequency range. The harmonics pane and the frequency band details pane provide more information on data in the frequency domain. The linked 3D views show the three most critical harmonics for the selected frequency band. Additional views, like the box-plot view shown here, can be added on demand during exploratory analysis sessions.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "https://youtu.be/3xxYS7aTSo4",
                        "ff_id": "3xxYS7aTSo4"
                    },
                    {
                        "slot_id": "v-full-1547-qa",
                        "session_id": "full29",
                        "type": "In Person Q+A",
                        "title": "Interactive Visual Analysis of Structure-borne Noise Data (Q+A)",
                        "contributors": [
                            "Kresimir Matkovic",
                            "Rainer Splechtna"
                        ],
                        "authors": [],
                        "abstract": "Numerical simulation has become omnipresent in the automotive domain, posing new challenges such as high-dimensional parameter spaces and large as well as incomplete and multi-faceted data. In this design study, we show how interactive visual exploration and analysis of high-dimensional, spectral data from noise simulation can facilitate design improvements in the context of conflicting criteria. Here, we focus on structure-borne noise, i.e., noise from vibrating mechanical parts. Detecting problematic noise\n sources early in the design and production process is essential for reducing a product\u2019s development costs and its time to market. In a close collaboration of visualization and automotive engineering, we designed a new, interactive approach to quickly identify and\n analyze critical noise sources, also contributing to an improved understanding of the analyzed system. Several carefully designed, interactive linked views enable the exploration of noises, vibrations, and harshness at multiple levels of detail, both in the frequency and spatial domain. This enables swift and smooth changes of perspective; selections in the frequency domain are immediately reflected in the spatial domain, and vice versa. Noise sources are quickly identified and shown in the context of their neighborhood, both in the frequency and spatial domain. We propose a novel drill-down view, especially tailored to noise data analysis. Split boxplots and synchronized 3D geometry views support comparison tasks. With this solution, engineers iterate over design optimizations much faster, while maintaining a good overview at each iteration. We evaluated the new approach in the automotive industry, studying noise\n simulation data for an internal combustion engine.",
                        "uid": "v-full-1547",
                        "file_name": "",
                        "time_stamp": "2022-10-20T16:19:00Z",
                        "time_start": "2022-10-20T16:19:00Z",
                        "time_end": "2022-10-20T16:21:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "574",
                        "paper_award": "",
                        "image_caption": "The coordinated multiple views solution for interactive visual analysis of structure-borne noise simulation data. The drill-down view consists of three panes. The overview matrix pane provides aggregated values for individual engine parts across the whole frequency range. The harmonics pane and the frequency band details pane provide more information on data in the frequency domain. The linked 3D views show the three most critical harmonics for the selected frequency band. Additional views, like the box-plot view shown here, can be added on demand during exploratory analysis sessions.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "https://youtu.be/3xxYS7aTSo4",
                        "ff_id": "3xxYS7aTSo4"
                    },
                    {
                        "slot_id": "v-full-1312-pres",
                        "session_id": "full29",
                        "type": "In Person Presentation",
                        "title": "Traveler: Navigating Task Parallel Traces for Performance Analysis",
                        "contributors": [
                            "Sayef Azad Sakin"
                        ],
                        "authors": [
                            "Sayef Azad Sakin",
                            "Alex Bigelow",
                            "Mohammad Tohid",
                            "Connor Scully-Allison",
                            "Carlos Scheidegger",
                            "Steven Robert Brandt",
                            "Christopher P. Taylor",
                            "Kevin A. Huck",
                            "Hartmut Kaiser",
                            "Katherine E. Isaacs"
                        ],
                        "abstract": "Understanding the behavior of software in execution is a key step in identifying and fixing performance issues. This is especially important in high performance computing contexts where even minor performance tweaks can translate into large savings in terms of computational resource use. To aid performance analysis, developers may collect an execution trace\u2014a chronological log of program activity during execution. As traces represent the full history, developers can discover a wide array of possibly previously unknown performance issues, making them an important artifact for exploratory performance analysis. However, interactive trace visualization is difficult due to issues of data size and complexity of meaning. Traces represent nanosecond-level events across many parallel processes, meaning the collected data is often large and difficult to explore. The rise of asynchronous task parallel programming paradigms complicates the relation between events and their probable cause. To address these challenges, we conduct a continuing design study in collaboration with high performance computing researchers. We develop diverse and hierarchical ways to navigate and represent execution trace data in support of their trace analysis tasks. Through an iterative design process, we developed Traveler, an integrated visualization platform for task parallel traces. Traveler provides multiple linked interfaces to help navigate trace data from multiple contexts. We evaluate the utility of Traveler through feedback from users and a case study, finding that integrating multiple modes of navigation in our design supported performance analysis tasks and led to the discovery of previously unknown behavior in a distributed array library.",
                        "uid": "v-full-1312",
                        "file_name": "v-full-1312_Sakin_Presentation.mp4",
                        "time_stamp": "2022-10-20T16:21:00Z",
                        "time_start": "2022-10-20T16:21:00Z",
                        "time_end": "2022-10-20T16:31:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "552",
                        "paper_award": "",
                        "image_caption": "Understanding the behavior of software in execution is a key step in fixing performance issues. Execution traces contain a historical record of per-thread events collected while the software was running. Visual analysis of traces is difficult due to the size and complexity of the data. We present Traveler, multi-view coordinated visualization for visual exploration of execution traces. Traveler provides diverse and hierarchical ways of navigating trace data to manage and interpret both the vast scale differences and relationships between asynchronously scheduled computing tasks.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "https://youtu.be/hkWhOtfkLfo",
                        "ff_id": "hkWhOtfkLfo"
                    },
                    {
                        "slot_id": "v-full-1312-qa",
                        "session_id": "full29",
                        "type": "In Person Q+A",
                        "title": "Traveler: Navigating Task Parallel Traces for Performance Analysis (Q+A)",
                        "contributors": [
                            "Sayef Azad Sakin"
                        ],
                        "authors": [],
                        "abstract": "Understanding the behavior of software in execution is a key step in identifying and fixing performance issues. This is especially important in high performance computing contexts where even minor performance tweaks can translate into large savings in terms of computational resource use. To aid performance analysis, developers may collect an execution trace\u2014a chronological log of program activity during execution. As traces represent the full history, developers can discover a wide array of possibly previously unknown performance issues, making them an important artifact for exploratory performance analysis. However, interactive trace visualization is difficult due to issues of data size and complexity of meaning. Traces represent nanosecond-level events across many parallel processes, meaning the collected data is often large and difficult to explore. The rise of asynchronous task parallel programming paradigms complicates the relation between events and their probable cause. To address these challenges, we conduct a continuing design study in collaboration with high performance computing researchers. We develop diverse and hierarchical ways to navigate and represent execution trace data in support of their trace analysis tasks. Through an iterative design process, we developed Traveler, an integrated visualization platform for task parallel traces. Traveler provides multiple linked interfaces to help navigate trace data from multiple contexts. We evaluate the utility of Traveler through feedback from users and a case study, finding that integrating multiple modes of navigation in our design supported performance analysis tasks and led to the discovery of previously unknown behavior in a distributed array library.",
                        "uid": "v-full-1312",
                        "file_name": "",
                        "time_stamp": "2022-10-20T16:31:00Z",
                        "time_start": "2022-10-20T16:31:00Z",
                        "time_end": "2022-10-20T16:33:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "552",
                        "paper_award": "",
                        "image_caption": "Understanding the behavior of software in execution is a key step in fixing performance issues. Execution traces contain a historical record of per-thread events collected while the software was running. Visual analysis of traces is difficult due to the size and complexity of the data. We present Traveler, multi-view coordinated visualization for visual exploration of execution traces. Traveler provides diverse and hierarchical ways of navigating trace data to manage and interpret both the vast scale differences and relationships between asynchronously scheduled computing tasks.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "https://youtu.be/hkWhOtfkLfo",
                        "ff_id": "hkWhOtfkLfo"
                    },
                    {
                        "slot_id": "v-full-1617-pres",
                        "session_id": "full29",
                        "type": "In Person Presentation",
                        "title": "Visual Analysis and Detection of Contrails in Aircraft Engine Simulations",
                        "contributors": [
                            "Md Nafiul Alam Nipu"
                        ],
                        "authors": [
                            "Md Nafiul Alam Nipu",
                            "Carla Gabriela Floricel",
                            "Negar Naghash Zadeh",
                            "Roberto Paoli",
                            "G. Elisabeta Marai"
                        ],
                        "abstract": "Contrails are condensation trails generated from emitted particles by aircraft engines, which perturb Earth\u2019s radiation budget. Simulation modeling is used to interpret the formation and development of contrails. These simulations are computationally intensive and rely on high-performance computing solutions, and the contrail structures are not well defined. We propose a visual computing system to assist in defining contrails and their characteristics, as well as in the analysis of parameters for computer-generated aircraft engine simulations. The back-end of our system leverages a contrail-formation criterion and clustering methods to detect contrails\u2019 shape and evolution and identify similar simulation runs. The front-end system helps analyze contrails and their parameters across multiple simulation runs. The evaluation with domain experts shows this approach successfully aids in contrail data investigation.",
                        "uid": "v-full-1617",
                        "file_name": "v-full-1617_Nipu_Presentation.mp4",
                        "time_stamp": "2022-10-20T16:33:00Z",
                        "time_start": "2022-10-20T16:33:00Z",
                        "time_end": "2022-10-20T16:43:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "481",
                        "paper_award": "",
                        "image_caption": "A visualization system for the analysis of contrails generated by airplane engines.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "https://youtu.be/hg0fEqMIwZA",
                        "ff_id": "hg0fEqMIwZA"
                    },
                    {
                        "slot_id": "v-full-1617-qa",
                        "session_id": "full29",
                        "type": "In Person Q+A",
                        "title": "Visual Analysis and Detection of Contrails in Aircraft Engine Simulations (Q+A)",
                        "contributors": [
                            "Md Nafiul Alam Nipu"
                        ],
                        "authors": [],
                        "abstract": "Contrails are condensation trails generated from emitted particles by aircraft engines, which perturb Earth\u2019s radiation budget. Simulation modeling is used to interpret the formation and development of contrails. These simulations are computationally intensive and rely on high-performance computing solutions, and the contrail structures are not well defined. We propose a visual computing system to assist in defining contrails and their characteristics, as well as in the analysis of parameters for computer-generated aircraft engine simulations. The back-end of our system leverages a contrail-formation criterion and clustering methods to detect contrails\u2019 shape and evolution and identify similar simulation runs. The front-end system helps analyze contrails and their parameters across multiple simulation runs. The evaluation with domain experts shows this approach successfully aids in contrail data investigation.",
                        "uid": "v-full-1617",
                        "file_name": "",
                        "time_stamp": "2022-10-20T16:43:00Z",
                        "time_start": "2022-10-20T16:43:00Z",
                        "time_end": "2022-10-20T16:45:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "481",
                        "paper_award": "",
                        "image_caption": "A visualization system for the analysis of contrails generated by airplane engines.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "https://youtu.be/hg0fEqMIwZA",
                        "ff_id": "hg0fEqMIwZA"
                    },
                    {
                        "slot_id": "v-full-1478-pres",
                        "session_id": "full29",
                        "type": "Virtual Presentation (pre-recorded)",
                        "title": "DPVisCreator: Incorporating Pattern Constraints to Privacy-preserving Visualizations via Differential Privacy",
                        "contributors": [
                            "Jiehui Zhou"
                        ],
                        "authors": [
                            "Jiehui Zhou",
                            "Xumeng Wang",
                            "Jason Kamkwai Wong",
                            "Huanliang Wang",
                            "Zhongwei Wang",
                            "Xiaoyu Yang",
                            "Xiaoran Yan",
                            "Haozhe Feng",
                            "Huamin Qu",
                            "Haochao Ying",
                            "Wei Chen"
                        ],
                        "abstract": "Data privacy is an essential issue in publishing data visualizations. However, it is challenging to represent multiple data patterns in privacy-preserving visualizations. The prior approaches target specific chart types or perform an anonymization model uniformly without considering the importance of data patterns in visualizations. In this paper, we propose a visual analytics approach that facilitates data custodians to generate multiple private charts while maintaining user-preferred patterns. To this end, we introduce pattern constraints to model users' preferences over data patterns in the dataset and incorporate them into the proposed Bayesian network-based Differential Privacy (DP) model PriVis. A prototype system, DPVisCreator, is developed to assist data custodians in implementing our approach. The effectiveness of our approach is demonstrated with quantitative evaluation of pattern utility under the different levels of privacy protection, case studies, and semi-structured expert interviews.",
                        "uid": "v-full-1478",
                        "file_name": "v-full-1478_Jiehui_Presentation.mp4",
                        "time_stamp": "2022-10-20T16:45:00Z",
                        "time_start": "2022-10-20T16:45:00Z",
                        "time_end": "2022-10-20T16:53:42Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "522",
                        "paper_award": "",
                        "image_caption": "The overview of our visual analytics approach for generating privacy-preserving visualizations. Users can select patterns of interest from visualizations of tabular data and specify importance weights. The PriVis model considers these pattern constraints and privacy budgets to generate privacy-preserving schemes. Users can evaluate the results based on the visualization charts and utility metrics and return for iterative adjustments.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "https://youtu.be/-cmsbm8opvg",
                        "ff_id": "-cmsbm8opvg"
                    },
                    {
                        "slot_id": "v-full-1478-qa",
                        "session_id": "full29",
                        "type": "Virtual Q+A",
                        "title": "DPVisCreator: Incorporating Pattern Constraints to Privacy-preserving Visualizations via Differential Privacy (Q+A)",
                        "contributors": [
                            "Jiehui Zhou"
                        ],
                        "authors": [],
                        "abstract": "Data privacy is an essential issue in publishing data visualizations. However, it is challenging to represent multiple data patterns in privacy-preserving visualizations. The prior approaches target specific chart types or perform an anonymization model uniformly without considering the importance of data patterns in visualizations. In this paper, we propose a visual analytics approach that facilitates data custodians to generate multiple private charts while maintaining user-preferred patterns. To this end, we introduce pattern constraints to model users' preferences over data patterns in the dataset and incorporate them into the proposed Bayesian network-based Differential Privacy (DP) model PriVis. A prototype system, DPVisCreator, is developed to assist data custodians in implementing our approach. The effectiveness of our approach is demonstrated with quantitative evaluation of pattern utility under the different levels of privacy protection, case studies, and semi-structured expert interviews.",
                        "uid": "v-full-1478",
                        "file_name": "",
                        "time_stamp": "2022-10-20T16:53:42Z",
                        "time_start": "2022-10-20T16:53:42Z",
                        "time_end": "2022-10-20T16:57:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "522",
                        "paper_award": "",
                        "image_caption": "The overview of our visual analytics approach for generating privacy-preserving visualizations. Users can select patterns of interest from visualizations of tabular data and specify importance weights. The PriVis model considers these pattern constraints and privacy budgets to generate privacy-preserving schemes. Users can evaluate the results based on the visualization charts and utility metrics and return for iterative adjustments.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "https://youtu.be/-cmsbm8opvg",
                        "ff_id": "-cmsbm8opvg"
                    }
                ]
            },
            {
                "title": "Decision Making and Reasoning",
                "session_id": "full30",
                "event_prefix": "v-full",
                "track": "ok1",
                "livestream_id": "ok1-wed",
                "session_image": "full30.png",
                "chair": [
                    "Lace Padilla"
                ],
                "organizers": [],
                "time_start": "2022-10-19T14:00:00Z",
                "time_end": "2022-10-19T15:15:00Z",
                "discord_category": "",
                "discord_channel": "oklahoma-st-1",
                "discord_channel_id": "1024600861340606484",
                "discord_link": "https://discord.com/channels/978320435554947082/1024600861340606484",
                "slido_link": "https://app.sli.do/event/i28UETZaiRWr1XwX6eKjcf?section=740d9835-fe47-4cb4-b260-353262a7f8dd",
                "youtube_url": "https://youtu.be/EK30lilKKdM",
                "youtube_id": "EK30lilKKdM",
                "zoom_meeting": "",
                "zoom_password": "",
                "zoom_link": "",
                "ff_link": "https://youtu.be/kJVNMDaRfIE",
                "ff_playlist": "",
                "time_slots": [
                    {
                        "slot_id": "full30-opening",
                        "session_id": "full30",
                        "type": "In Person Other",
                        "title": "Session Opening",
                        "contributors": [
                            "Lace Padilla"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "",
                        "file_name": "",
                        "time_stamp": "2022-10-19T14:00:00Z",
                        "time_start": "2022-10-19T14:00:00Z",
                        "time_end": "2022-10-19T14:00:00Z",
                        "paper_type": "",
                        "keywords": [],
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "v-full-1568-pres",
                        "session_id": "full30",
                        "type": "In Person Presentation",
                        "title": "ErgoExplorer: Interactive Ergonomic Risk Assessment from Video Collections",
                        "contributors": [
                            "Claudio Delrieux",
                            "Kresimir Matkovic"
                        ],
                        "authors": [
                            "Manlio Miguel Massiris Fernandez",
                            "Sanjin Rados",
                            "Eduard Gr\u00f6ller",
                            "Claudio Delrieux",
                            "Kresimir Matkovic"
                        ],
                        "abstract": "Ergonomic risk assessment is now, due to an increased awareness, carried out more often than in the past. The conventional risk assessment evaluation, based on expert-assisted observation of the workplaces and manually filling in score tables, is still predominant. Data analysis is usually done with a focus on critical moments, although without the support of contextual information and changes over time. In this paper we introduce ErgoExplorer, a system for the interactive visual analysis of risk assessment data. In contrast to the current practice, we focus on data that span across multiple actions and multiple workers while keeping all contextual information. Data is automatically extracted from video streams. Based on carefully investigated analysis tasks, we introduce new views and their corresponding interactions. These views also incorporate domain-specific score tables to guarantee an easy adoption by domain experts. All views are integrated into ErgoExplorer, which relies on coordinated multiple views to facilitate analysis through interaction.\n ErgoExplorer makes it possible for the first time to examine complex relationships between risk assessments of individual body parts over long sessions that span multiple operations. The newly introduced approach supports analysis and exploration at several levels of detail, ranging from a general overview, down to inspecting individual frames in the video stream, if necessary. We illustrate the usefulness of the newly proposed approach applying it to several datasets.",
                        "uid": "v-full-1568",
                        "file_name": "v-full-1568_Delrieux_Presentation.mp4",
                        "time_stamp": "2022-10-19T14:00:00Z",
                        "time_start": "2022-10-19T14:00:00Z",
                        "time_end": "2022-10-19T14:10:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "455",
                        "paper_award": "",
                        "image_caption": "Ergonomic risk assessment is now, due to an increased awareness, carried out more often than in the past. The conventional risk assessment evaluation is based on expert-assisted observation of the workplaces and manually filling in score tables. With ErgoExplorer it is possible to explore and analyze time dependent ergonomic scores from automatically recorded observations encompassing very long periods. A dashboard as used in one of the analysis sessions is shown here. Ergonomists can understand how to mitigate ergonomic risk by using coordinated multiple views: (a) ErgoView, (b) ErgoTimeline,(c) scatterplot matrix, and (d) parallel coordinates.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "https://youtu.be/INIZJUllKuI",
                        "ff_id": "INIZJUllKuI"
                    },
                    {
                        "slot_id": "v-full-1568-qa",
                        "session_id": "full30",
                        "type": "In Person Q+A",
                        "title": "ErgoExplorer: Interactive Ergonomic Risk Assessment from Video Collections (Q+A)",
                        "contributors": [
                            "Claudio Delrieux",
                            "Kresimir Matkovic"
                        ],
                        "authors": [],
                        "abstract": "Ergonomic risk assessment is now, due to an increased awareness, carried out more often than in the past. The conventional risk assessment evaluation, based on expert-assisted observation of the workplaces and manually filling in score tables, is still predominant. Data analysis is usually done with a focus on critical moments, although without the support of contextual information and changes over time. In this paper we introduce ErgoExplorer, a system for the interactive visual analysis of risk assessment data. In contrast to the current practice, we focus on data that span across multiple actions and multiple workers while keeping all contextual information. Data is automatically extracted from video streams. Based on carefully investigated analysis tasks, we introduce new views and their corresponding interactions. These views also incorporate domain-specific score tables to guarantee an easy adoption by domain experts. All views are integrated into ErgoExplorer, which relies on coordinated multiple views to facilitate analysis through interaction.\n ErgoExplorer makes it possible for the first time to examine complex relationships between risk assessments of individual body parts over long sessions that span multiple operations. The newly introduced approach supports analysis and exploration at several levels of detail, ranging from a general overview, down to inspecting individual frames in the video stream, if necessary. We illustrate the usefulness of the newly proposed approach applying it to several datasets.",
                        "uid": "v-full-1568",
                        "file_name": "",
                        "time_stamp": "2022-10-19T14:10:00Z",
                        "time_start": "2022-10-19T14:10:00Z",
                        "time_end": "2022-10-19T14:12:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "455",
                        "paper_award": "",
                        "image_caption": "Ergonomic risk assessment is now, due to an increased awareness, carried out more often than in the past. The conventional risk assessment evaluation is based on expert-assisted observation of the workplaces and manually filling in score tables. With ErgoExplorer it is possible to explore and analyze time dependent ergonomic scores from automatically recorded observations encompassing very long periods. A dashboard as used in one of the analysis sessions is shown here. Ergonomists can understand how to mitigate ergonomic risk by using coordinated multiple views: (a) ErgoView, (b) ErgoTimeline,(c) scatterplot matrix, and (d) parallel coordinates.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "https://youtu.be/INIZJUllKuI",
                        "ff_id": "INIZJUllKuI"
                    },
                    {
                        "slot_id": "v-full-1614-pres",
                        "session_id": "full30",
                        "type": "In Person Presentation",
                        "title": "TRAFFICVIS: Visualizing Organized Activity and Spatio-Temporal Patterns for Detecting and Labeling Human Trafficking",
                        "contributors": [
                            "Catalina Vajiac"
                        ],
                        "authors": [
                            "Catalina Vajiac",
                            "Duen Horng Chau",
                            "Andreas Olligschlaeger",
                            "Rebecca Mackenzie",
                            "Pratheeksha Nair",
                            "Meng-Chieh Lee",
                            "Yifei Li",
                            "Namyong Park",
                            "Reihaneh Rabbany",
                            "Christos Faloutsos"
                        ],
                        "abstract": "Law enforcement and domain experts can detect human trafficking (HT) in online escort websites by analyzing suspicious clusters of connected ads. How can we explain clustering results intuitively and interactively, visualizing potential evidence for experts to analyze?\n We present TrafficVis, the first interface for cluster-level HT detection and labeling. Developed through months of participatory design with domain experts, TrafficVis provides coordinated views in conjunction with carefully chosen backend algorithms to effectively show spatio-temporal and text patterns to a wide variety of anti-HT stakeholders. We build upon state-of-the-art text clustering algorithms by incorporating shared metadata as a signal of connected and possibly suspicious activity, then visualize the results. Domain experts can use TrafficVis to label clusters as HT, or other, suspicious, but non-HT activity such as spam and scam, quickly creating labeled datasets to enable further HT research.\n Through domain expert feedback and a usage scenario, we demonstrate TrafficVis's efficacy. The feedback was overwhelmingly positive, with repeated high praises for the usability and explainability of our tool, the latter being vital for indicting possible criminals.",
                        "uid": "v-full-1614",
                        "file_name": "v-full-1614_Vajiac_Presentation.mp4",
                        "time_stamp": "2022-10-19T14:12:00Z",
                        "time_start": "2022-10-19T14:12:00Z",
                        "time_end": "2022-10-19T14:22:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "535",
                        "paper_award": "",
                        "image_caption": "Analyzing online escort ads using TrafficVis: we show one meta-cluster, i.e.  micro (text) clusters connected using metadata, on real data. Some text blurred for privacy. 1. Human trafficking domain expert uses Micro-cluster panel to drill down to specific micro-cluster data and associated ads. 2-3. Expert uses Timeline panel and Map panel to investigate metadata, noticing inconsistent posting time and regional geographic spread, ruling out spam and scam. 4.  Expert uses Text panel to quickly find telling signals; differences between ads in a micro-cluster are highlighted. 5. Finally, the expert confidently labels the meta-cluster for each modus operandi (M.O.), deciding on benign (at-will sex worker), with a small chance of trafficking.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "https://youtu.be/uIQ4tZDadK4",
                        "ff_id": "uIQ4tZDadK4"
                    },
                    {
                        "slot_id": "v-full-1614-qa",
                        "session_id": "full30",
                        "type": "In Person Q+A",
                        "title": "TRAFFICVIS: Visualizing Organized Activity and Spatio-Temporal Patterns for Detecting and Labeling Human Trafficking (Q+A)",
                        "contributors": [
                            "Catalina Vajiac"
                        ],
                        "authors": [],
                        "abstract": "Law enforcement and domain experts can detect human trafficking (HT) in online escort websites by analyzing suspicious clusters of connected ads. How can we explain clustering results intuitively and interactively, visualizing potential evidence for experts to analyze?\n We present TrafficVis, the first interface for cluster-level HT detection and labeling. Developed through months of participatory design with domain experts, TrafficVis provides coordinated views in conjunction with carefully chosen backend algorithms to effectively show spatio-temporal and text patterns to a wide variety of anti-HT stakeholders. We build upon state-of-the-art text clustering algorithms by incorporating shared metadata as a signal of connected and possibly suspicious activity, then visualize the results. Domain experts can use TrafficVis to label clusters as HT, or other, suspicious, but non-HT activity such as spam and scam, quickly creating labeled datasets to enable further HT research.\n Through domain expert feedback and a usage scenario, we demonstrate TrafficVis's efficacy. The feedback was overwhelmingly positive, with repeated high praises for the usability and explainability of our tool, the latter being vital for indicting possible criminals.",
                        "uid": "v-full-1614",
                        "file_name": "",
                        "time_stamp": "2022-10-19T14:22:00Z",
                        "time_start": "2022-10-19T14:22:00Z",
                        "time_end": "2022-10-19T14:24:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "535",
                        "paper_award": "",
                        "image_caption": "Analyzing online escort ads using TrafficVis: we show one meta-cluster, i.e.  micro (text) clusters connected using metadata, on real data. Some text blurred for privacy. 1. Human trafficking domain expert uses Micro-cluster panel to drill down to specific micro-cluster data and associated ads. 2-3. Expert uses Timeline panel and Map panel to investigate metadata, noticing inconsistent posting time and regional geographic spread, ruling out spam and scam. 4.  Expert uses Text panel to quickly find telling signals; differences between ads in a micro-cluster are highlighted. 5. Finally, the expert confidently labels the meta-cluster for each modus operandi (M.O.), deciding on benign (at-will sex worker), with a small chance of trafficking.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "https://youtu.be/uIQ4tZDadK4",
                        "ff_id": "uIQ4tZDadK4"
                    },
                    {
                        "slot_id": "v-tvcg-9507307-pres",
                        "session_id": "full30",
                        "type": "In Person Presentation",
                        "title": "Outcome-Explorer: A Causality Guided Interactive Visual Interface for Interpretable Algorithmic Decision Making",
                        "contributors": [
                            "Md Naimul Hoque"
                        ],
                        "authors": [
                            "Hoque",
                            "Md Naimul\uff0cMueller",
                            "Klaus"
                        ],
                        "abstract": "The widespread adoption of algorithmic decision-making systems has brought about the necessity to interpret the reasoning behind these decisions. The majority of these systems are complex black box models, and auxiliary models are often used to approximate and then explain their behavior. However, recent research suggests that such explanations are not overly accessible to lay users with no specific expertise in machine learning and this can lead to an incorrect interpretation of the underlying model. In this paper, we show that a predictive and interactive model based on causality is inherently interpretable, does not require any auxiliary model, and allows both expert and non-expert users to understand the model comprehensively. To demonstrate our method we developed Outcome Explorer, a causality guided interactive interface, and evaluated it by conducting think-aloud sessions with three expert users and a user study with 18 non-expert users. All three expert users found our tool to be comprehensive in supporting their explanation needs while the non-expert users were able to understand the inner workings of a model easily.",
                        "uid": "v-tvcg-9507307",
                        "file_name": "v-tvcg-9507307_Hoque_Presentation.mp4",
                        "time_stamp": "2022-10-19T14:24:00Z",
                        "time_start": "2022-10-19T14:24:00Z",
                        "time_end": "2022-10-19T14:34:00Z",
                        "paper_type": "full",
                        "keywords": [
                            "Explainable AI, Causality, Visual Analytics, Human-Computer Interaction."
                        ],
                        "has_image": "1",
                        "has_video": "507",
                        "paper_award": "",
                        "image_caption": "An image containing the title of the paper, author list, and a screenshot of the interface proposed in the paper.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "https://youtu.be/Zyci7BZkIks",
                        "ff_id": "Zyci7BZkIks"
                    },
                    {
                        "slot_id": "v-tvcg-9507307-qa",
                        "session_id": "full30",
                        "type": "In Person Q+A",
                        "title": "Outcome-Explorer: A Causality Guided Interactive Visual Interface for Interpretable Algorithmic Decision Making (Q+A)",
                        "contributors": [
                            "Md Naimul Hoque"
                        ],
                        "authors": [],
                        "abstract": "The widespread adoption of algorithmic decision-making systems has brought about the necessity to interpret the reasoning behind these decisions. The majority of these systems are complex black box models, and auxiliary models are often used to approximate and then explain their behavior. However, recent research suggests that such explanations are not overly accessible to lay users with no specific expertise in machine learning and this can lead to an incorrect interpretation of the underlying model. In this paper, we show that a predictive and interactive model based on causality is inherently interpretable, does not require any auxiliary model, and allows both expert and non-expert users to understand the model comprehensively. To demonstrate our method we developed Outcome Explorer, a causality guided interactive interface, and evaluated it by conducting think-aloud sessions with three expert users and a user study with 18 non-expert users. All three expert users found our tool to be comprehensive in supporting their explanation needs while the non-expert users were able to understand the inner workings of a model easily.",
                        "uid": "v-tvcg-9507307",
                        "file_name": "",
                        "time_stamp": "2022-10-19T14:34:00Z",
                        "time_start": "2022-10-19T14:34:00Z",
                        "time_end": "2022-10-19T14:36:00Z",
                        "paper_type": "full",
                        "keywords": [
                            "Explainable AI, Causality, Visual Analytics, Human-Computer Interaction."
                        ],
                        "has_image": "1",
                        "has_video": "507",
                        "paper_award": "",
                        "image_caption": "An image containing the title of the paper, author list, and a screenshot of the interface proposed in the paper.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "https://youtu.be/Zyci7BZkIks",
                        "ff_id": "Zyci7BZkIks"
                    },
                    {
                        "slot_id": "v-full-1400-pres",
                        "session_id": "full30",
                        "type": "Virtual Presentation (pre-recorded)",
                        "title": "MedChemLens: An Interactive Visual Tool to Support Direction Selection in Interdisciplinary Experimental Research of Medicinal Chemistry",
                        "contributors": [
                            "Chuhan Shi"
                        ],
                        "authors": [
                            "Chuhan Shi",
                            "Fei Nie",
                            "Yicheng Hu",
                            "Yige Xu",
                            "Lei Chen",
                            "Xiaojuan Ma",
                            "Qiong Luo"
                        ],
                        "abstract": "Interdisciplinary experimental science (e.g., medicinal chemistry) refers to the disciplines that integrate knowledge from different scientific backgrounds and involve experiments in the research process. Deciding \"in what direction to proceed\" is critical for the success of the research in such disciplines, since the time, money, and resource costs of the subsequent research steps depend largely on this decision. However, such a direction identification task is challenging in that researchers need to integrate information from large-scale, heterogeneous materials from all associated disciplines and summarize the related publications of which the core contributions are often showcased in diverse formats. The task also requires researchers to estimate the feasibility and potential in future experiments in the selected directions. In this work, we selected medicinal chemistry as a case and presented an interactive visual tool, MedChemLens, to assist medicinal chemists in choosing their intended directions of research. This task is also known as drug target (i.e., disease-linked proteins) selection. Given a candidate target name, MedChemLens automatically extracts the molecular features of drug compounds from chemical papers and clinical trial records, organizes them based on the drug structures, and interactively visualizes factors concerning subsequent experiments. We evaluated MedChemLens through a within-subjects study (N=16). Compared with the control condition (i.e., unrestricted online search without using our tool), participants who only used MedChemLens reported faster search, better-informed selections, higher confidence in their selections, and lower cognitive load.",
                        "uid": "v-full-1400",
                        "file_name": "v-full-1400_Shi_Presentation.mp4",
                        "time_stamp": "2022-10-19T14:36:00Z",
                        "time_start": "2022-10-19T14:36:00Z",
                        "time_end": "2022-10-19T14:44:36Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "516",
                        "paper_award": "",
                        "image_caption": "MedChemLens: The Drug Target Search view allows users to search drug targets. The Signaling Pathway view presents the signaling pathways of the targets under search. The Overview shows the overall distributions of the existing drug compound research. The Publication Trend view displays the number of publications over time related to the targets. The Detail View consists of the Chemistry panel, which summarizes the drug compounds proposed in chemical publications; the Pharmacology panel, which displays the molecular feature values of the compounds tested in pharmacological assays; and the Clinical Pharmacy panel, which visualizes the clinical trial progress of the compounds.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "https://youtu.be/ZiZ9i8QZLR0",
                        "ff_id": "ZiZ9i8QZLR0"
                    },
                    {
                        "slot_id": "v-full-1400-qa",
                        "session_id": "full30",
                        "type": "Virtual Q+A",
                        "title": "MedChemLens: An Interactive Visual Tool to Support Direction Selection in Interdisciplinary Experimental Research of Medicinal Chemistry (Q+A)",
                        "contributors": [
                            "Chuhan Shi"
                        ],
                        "authors": [],
                        "abstract": "Interdisciplinary experimental science (e.g., medicinal chemistry) refers to the disciplines that integrate knowledge from different scientific backgrounds and involve experiments in the research process. Deciding \"in what direction to proceed\" is critical for the success of the research in such disciplines, since the time, money, and resource costs of the subsequent research steps depend largely on this decision. However, such a direction identification task is challenging in that researchers need to integrate information from large-scale, heterogeneous materials from all associated disciplines and summarize the related publications of which the core contributions are often showcased in diverse formats. The task also requires researchers to estimate the feasibility and potential in future experiments in the selected directions. In this work, we selected medicinal chemistry as a case and presented an interactive visual tool, MedChemLens, to assist medicinal chemists in choosing their intended directions of research. This task is also known as drug target (i.e., disease-linked proteins) selection. Given a candidate target name, MedChemLens automatically extracts the molecular features of drug compounds from chemical papers and clinical trial records, organizes them based on the drug structures, and interactively visualizes factors concerning subsequent experiments. We evaluated MedChemLens through a within-subjects study (N=16). Compared with the control condition (i.e., unrestricted online search without using our tool), participants who only used MedChemLens reported faster search, better-informed selections, higher confidence in their selections, and lower cognitive load.",
                        "uid": "v-full-1400",
                        "file_name": "",
                        "time_stamp": "2022-10-19T14:44:36Z",
                        "time_start": "2022-10-19T14:44:36Z",
                        "time_end": "2022-10-19T14:48:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "516",
                        "paper_award": "",
                        "image_caption": "MedChemLens: The Drug Target Search view allows users to search drug targets. The Signaling Pathway view presents the signaling pathways of the targets under search. The Overview shows the overall distributions of the existing drug compound research. The Publication Trend view displays the number of publications over time related to the targets. The Detail View consists of the Chemistry panel, which summarizes the drug compounds proposed in chemical publications; the Pharmacology panel, which displays the molecular feature values of the compounds tested in pharmacological assays; and the Clinical Pharmacy panel, which visualizes the clinical trial progress of the compounds.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "https://youtu.be/ZiZ9i8QZLR0",
                        "ff_id": "ZiZ9i8QZLR0"
                    },
                    {
                        "slot_id": "v-tvcg-9761750-pres",
                        "session_id": "full30",
                        "type": "Virtual Presentation (pre-recorded)",
                        "title": "GestureLens: Visual Analysis of Gestures in Presentation Videos",
                        "contributors": [
                            "Haipeng Zeng"
                        ],
                        "authors": [
                            "Haipeng Zeng",
                            "Xingbo Wang",
                            "Yong Wang",
                            "Aoyu Wu",
                            "Ting Chuen Pong",
                            "Huamin Qu"
                        ],
                        "abstract": "Appropriate gestures can enhance message delivery and audience engagement in both daily communication and public presentations. In this paper, we contribute a visual analytic approach that assists professional public speaking coaches in improving their practice of gesture training through analyzing presentation videos. Manually checking and exploring gesture usage in the presentation videos is often tedious and time-consuming. There lacks an efficient method to help users conduct gesture exploration, which is challenging due to the intrinsically temporal evolution of gestures and their complex correlation to speech content. In this paper, we propose GestureLens, a visual analytics system to facilitate gesture-based and content-based exploration of gesture usage in presentation videos. Specifically, the exploration view enables users to obtain a quick overview of the spatial and temporal distributions of gestures. The dynamic hand movements are firstly aggregated through a heatmap in the gesture space for uncovering spatial patterns, and then decomposed into two mutually perpendicular timelines for revealing temporal patterns. The relation view allows users to explicitly explore the correlation between speech content and gestures by enabling linked analysis and intuitive glyph designs. The video view and dynamic view show the context and overall dynamic movement of the selected gestures, respectively. Two usage scenarios and expert interviews with professional presentation coaches demonstrate the effectiveness and usefulness of GestureLens in facilitating gesture exploration and analysis of presentation videos.",
                        "uid": "v-tvcg-9761750",
                        "file_name": "v-tvcg-9761750_Zeng_Presentation.mp4",
                        "time_stamp": "2022-10-19T14:48:00Z",
                        "time_start": "2022-10-19T14:48:00Z",
                        "time_end": "2022-10-19T14:58:21Z",
                        "paper_type": "full",
                        "keywords": [
                            "Gesture, hand movements, presentation video analysis, visual analysis."
                        ],
                        "has_image": "1",
                        "has_video": "621",
                        "paper_award": "",
                        "image_caption": "Appropriate gestures can enhance message delivery and audience engagement in both daily communication and public presentations. In this paper, we contribute a visual analytic approach that assists professional public speaking coaches in improving their practice of gesture training through analyzing presentation videos. We propose GestureLens, a visual analytics system to facilitate gesture-based and content-based exploration of gesture usage in presentation videos. Two usage scenarios and expert interviews with professional presentation coaches demonstrate the effectiveness and usefulness of GestureLens in facilitating gesture exploration and analysis of presentation videos.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "https://youtu.be/P33NYSWrbNg",
                        "ff_id": "P33NYSWrbNg"
                    },
                    {
                        "slot_id": "v-tvcg-9761750-qa",
                        "session_id": "full30",
                        "type": "Virtual Q+A",
                        "title": "GestureLens: Visual Analysis of Gestures in Presentation Videos (Q+A)",
                        "contributors": [
                            "Haipeng Zeng"
                        ],
                        "authors": [],
                        "abstract": "Appropriate gestures can enhance message delivery and audience engagement in both daily communication and public presentations. In this paper, we contribute a visual analytic approach that assists professional public speaking coaches in improving their practice of gesture training through analyzing presentation videos. Manually checking and exploring gesture usage in the presentation videos is often tedious and time-consuming. There lacks an efficient method to help users conduct gesture exploration, which is challenging due to the intrinsically temporal evolution of gestures and their complex correlation to speech content. In this paper, we propose GestureLens, a visual analytics system to facilitate gesture-based and content-based exploration of gesture usage in presentation videos. Specifically, the exploration view enables users to obtain a quick overview of the spatial and temporal distributions of gestures. The dynamic hand movements are firstly aggregated through a heatmap in the gesture space for uncovering spatial patterns, and then decomposed into two mutually perpendicular timelines for revealing temporal patterns. The relation view allows users to explicitly explore the correlation between speech content and gestures by enabling linked analysis and intuitive glyph designs. The video view and dynamic view show the context and overall dynamic movement of the selected gestures, respectively. Two usage scenarios and expert interviews with professional presentation coaches demonstrate the effectiveness and usefulness of GestureLens in facilitating gesture exploration and analysis of presentation videos.",
                        "uid": "v-tvcg-9761750",
                        "file_name": "",
                        "time_stamp": "2022-10-19T14:58:21Z",
                        "time_start": "2022-10-19T14:58:21Z",
                        "time_end": "2022-10-19T15:00:00Z",
                        "paper_type": "full",
                        "keywords": [
                            "Gesture, hand movements, presentation video analysis, visual analysis."
                        ],
                        "has_image": "1",
                        "has_video": "621",
                        "paper_award": "",
                        "image_caption": "Appropriate gestures can enhance message delivery and audience engagement in both daily communication and public presentations. In this paper, we contribute a visual analytic approach that assists professional public speaking coaches in improving their practice of gesture training through analyzing presentation videos. We propose GestureLens, a visual analytics system to facilitate gesture-based and content-based exploration of gesture usage in presentation videos. Two usage scenarios and expert interviews with professional presentation coaches demonstrate the effectiveness and usefulness of GestureLens in facilitating gesture exploration and analysis of presentation videos.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "https://youtu.be/P33NYSWrbNg",
                        "ff_id": "P33NYSWrbNg"
                    },
                    {
                        "slot_id": "v-full-1667-pres",
                        "session_id": "full30",
                        "type": "In Person Presentation",
                        "title": "Visual Concept Programming: A Visual Analytics Approach to Injecting Human Intelligence at Scale",
                        "contributors": [
                            "Md Naimul Hoque"
                        ],
                        "authors": [
                            "Md Naimul Hoque",
                            "Wenbin He",
                            "Shekar Arvind Kumar",
                            "Liang Gou",
                            "Liu Ren"
                        ],
                        "abstract": "Data-centric AI has emerged as a new research area to systematically engineer the data to land AI models for real-world applications. As a core method for data-centric AI, data programming helps experts inject domain knowledge into data and label data at scale using carefully designed labeling functions (e.g., heuristic rules, logistics). Though data programming has shown great success in the NLP domain, it is challenging to program image data because of a) the challenge to describe images using visual vocabulary without human annotations and b) lacking efficient tools for data programming of images. We present Visual Concept Programming, a first-of-its-kind visual analytics approach of using visual concepts to program image data at scale while requiring a few human efforts. Our approach is built upon three unique components. It first uses a self-supervised learning approach to learn visual representation at the pixel level and extract a dictionary of visual concepts from images without using any human annotations. The visual concepts serve as building blocks of labeling functions for experts to inject their domain knowledge. We then design interactive visualizations to explore and understand visual concepts and compose labeling functions with concepts without writing code. Finally, with the composed labeling functions, users can label the image data at scale and use the labeled data to refine the pixel-wise visual representation and concept quality. We evaluate the learned pixel-wise visual representation for the downstream task of semantic segmentation to show the effectiveness and usefulness of our approach. In addition, we demonstrate how our approach tackles real-world problems of image retrieval for autonomous driving.",
                        "uid": "v-full-1667",
                        "file_name": "v-full-1667_Hoque_Presentation.mp4",
                        "time_stamp": "2022-10-19T15:00:00Z",
                        "time_start": "2022-10-19T15:00:00Z",
                        "time_end": "2022-10-19T15:10:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "586",
                        "paper_award": "",
                        "image_caption": "Visual Concept Programming, the first visual analytics approach of using visual concepts to program image data at scale for improving data quality and model performance. This approach is echoing the current ML research trend of Data-Centric AI by iterating data (e.g., quality or higher-level supervision), not models.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "https://youtu.be/qRkDIc3hd88",
                        "ff_id": "qRkDIc3hd88"
                    },
                    {
                        "slot_id": "v-full-1667-qa",
                        "session_id": "full30",
                        "type": "In Person Q+A",
                        "title": "Visual Concept Programming: A Visual Analytics Approach to Injecting Human Intelligence at Scale (Q+A)",
                        "contributors": [
                            "Md Naimul Hoque"
                        ],
                        "authors": [],
                        "abstract": "Data-centric AI has emerged as a new research area to systematically engineer the data to land AI models for real-world applications. As a core method for data-centric AI, data programming helps experts inject domain knowledge into data and label data at scale using carefully designed labeling functions (e.g., heuristic rules, logistics). Though data programming has shown great success in the NLP domain, it is challenging to program image data because of a) the challenge to describe images using visual vocabulary without human annotations and b) lacking efficient tools for data programming of images. We present Visual Concept Programming, a first-of-its-kind visual analytics approach of using visual concepts to program image data at scale while requiring a few human efforts. Our approach is built upon three unique components. It first uses a self-supervised learning approach to learn visual representation at the pixel level and extract a dictionary of visual concepts from images without using any human annotations. The visual concepts serve as building blocks of labeling functions for experts to inject their domain knowledge. We then design interactive visualizations to explore and understand visual concepts and compose labeling functions with concepts without writing code. Finally, with the composed labeling functions, users can label the image data at scale and use the labeled data to refine the pixel-wise visual representation and concept quality. We evaluate the learned pixel-wise visual representation for the downstream task of semantic segmentation to show the effectiveness and usefulness of our approach. In addition, we demonstrate how our approach tackles real-world problems of image retrieval for autonomous driving.",
                        "uid": "v-full-1667",
                        "file_name": "",
                        "time_stamp": "2022-10-19T15:10:00Z",
                        "time_start": "2022-10-19T15:10:00Z",
                        "time_end": "2022-10-19T15:12:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "586",
                        "paper_award": "",
                        "image_caption": "Visual Concept Programming, the first visual analytics approach of using visual concepts to program image data at scale for improving data quality and model performance. This approach is echoing the current ML research trend of Data-Centric AI by iterating data (e.g., quality or higher-level supervision), not models.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "https://youtu.be/qRkDIc3hd88",
                        "ff_id": "qRkDIc3hd88"
                    }
                ]
            },
            {
                "title": "Provenance and Guidance",
                "session_id": "full31",
                "event_prefix": "v-full",
                "track": "ok1",
                "livestream_id": "ok1-thu",
                "session_image": "full31.png",
                "chair": [
                    "Alvitta Ottley"
                ],
                "organizers": [],
                "time_start": "2022-10-20T20:45:00Z",
                "time_end": "2022-10-20T22:00:00Z",
                "discord_category": "",
                "discord_channel": "oklahoma-st-1",
                "discord_channel_id": "1024600861340606484",
                "discord_link": "https://discord.com/channels/978320435554947082/1024600861340606484",
                "slido_link": "https://app.sli.do/event/i28UETZaiRWr1XwX6eKjcf?section=740d9835-fe47-4cb4-b260-353262a7f8dd",
                "youtube_url": "https://youtu.be/x76ysAHpetE",
                "youtube_id": "x76ysAHpetE",
                "zoom_meeting": "",
                "zoom_password": "",
                "zoom_link": "",
                "ff_link": "https://youtu.be/TU1aIz6xKGw",
                "ff_playlist": "",
                "time_slots": [
                    {
                        "slot_id": "full31-opening",
                        "session_id": "full31",
                        "type": "In Person Other",
                        "title": "Session Opening",
                        "contributors": [
                            "Alvitta Ottley"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "",
                        "file_name": "",
                        "time_stamp": "2022-10-20T20:45:00Z",
                        "time_start": "2022-10-20T20:45:00Z",
                        "time_end": "2022-10-20T20:45:00Z",
                        "paper_type": "",
                        "keywords": [],
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "v-full-1089-pres",
                        "session_id": "full31",
                        "type": "In Person Presentation",
                        "title": "The Influence of Visual Provenance Representations on Strategies in a Collaborative Hand-off Data Analysis Scenario",
                        "contributors": [
                            "Jeremy E Block"
                        ],
                        "authors": [
                            "Jeremy E Block",
                            "Shaghayegh Esmaeili",
                            "Eric Ragan",
                            "John Goodall",
                            "Gregory David Richardson"
                        ],
                        "abstract": "Conducting data analysis tasks rarely occur in isolation. Especially in intelligence analysis scenarios where different experts contribute knowledge to a shared understanding, members must communicate how insights develop to establish common ground among collaborators. The use of provenance to communicate analytic sensemaking carries promise by describing the interactions and summarizing the steps taken to reach insights. Yet, no universal guidelines exist for communicating provenance in different settings. Our work focuses on the presentation of provenance information and the resulting conclusions reached and strategies used by new analysts. In an open-ended, 30-minute, textual exploration scenario, we qualitatively compare how adding different types of provenance information (speci\ufb01cally data coverage and interaction history) affects analysts\u2019 con\ufb01dence in conclusions developed, propensity to repeat work, \ufb01ltering of data, identi\ufb01cation of relevant information, and typical investigation strategies. We see that data coverage (i.e. what was interacted with) provides provenance information without limiting individual investigation freedom. On the other hand, while interaction history (i.e. when something was interacted with) does not signi\ufb01cantly encourage more mimicry, it does take more time to comfortably understand, as represented by less con\ufb01dent conclusions and less relevant information gathering behaviors. Our results contribute empirical data towards understanding how provenance summarizations can in\ufb02uence analysis behaviors.",
                        "uid": "v-full-1089",
                        "file_name": "v-full-1089_Block_Presentation.mp4",
                        "time_stamp": "2022-10-20T20:45:00Z",
                        "time_start": "2022-10-20T20:45:00Z",
                        "time_end": "2022-10-20T20:55:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "539",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "https://youtu.be/x-A2jxT-yJY",
                        "ff_id": "x-A2jxT-yJY"
                    },
                    {
                        "slot_id": "v-full-1089-qa",
                        "session_id": "full31",
                        "type": "In Person Q+A",
                        "title": "The Influence of Visual Provenance Representations on Strategies in a Collaborative Hand-off Data Analysis Scenario (Q+A)",
                        "contributors": [
                            "Jeremy E Block"
                        ],
                        "authors": [],
                        "abstract": "Conducting data analysis tasks rarely occur in isolation. Especially in intelligence analysis scenarios where different experts contribute knowledge to a shared understanding, members must communicate how insights develop to establish common ground among collaborators. The use of provenance to communicate analytic sensemaking carries promise by describing the interactions and summarizing the steps taken to reach insights. Yet, no universal guidelines exist for communicating provenance in different settings. Our work focuses on the presentation of provenance information and the resulting conclusions reached and strategies used by new analysts. In an open-ended, 30-minute, textual exploration scenario, we qualitatively compare how adding different types of provenance information (speci\ufb01cally data coverage and interaction history) affects analysts\u2019 con\ufb01dence in conclusions developed, propensity to repeat work, \ufb01ltering of data, identi\ufb01cation of relevant information, and typical investigation strategies. We see that data coverage (i.e. what was interacted with) provides provenance information without limiting individual investigation freedom. On the other hand, while interaction history (i.e. when something was interacted with) does not signi\ufb01cantly encourage more mimicry, it does take more time to comfortably understand, as represented by less con\ufb01dent conclusions and less relevant information gathering behaviors. Our results contribute empirical data towards understanding how provenance summarizations can in\ufb02uence analysis behaviors.",
                        "uid": "v-full-1089",
                        "file_name": "",
                        "time_stamp": "2022-10-20T20:55:00Z",
                        "time_start": "2022-10-20T20:55:00Z",
                        "time_end": "2022-10-20T20:57:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "539",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "https://youtu.be/x-A2jxT-yJY",
                        "ff_id": "x-A2jxT-yJY"
                    },
                    {
                        "slot_id": "v-tvcg-9768153-pres",
                        "session_id": "full31",
                        "type": "Virtual Presentation (pre-recorded)",
                        "title": "Understanding How In-Visualization Provenance Can Support Trade-off Analysis",
                        "contributors": [
                            "Mehdi Chakhchoukh"
                        ],
                        "authors": [
                            "Mehdi Chakhchoukh",
                            "Nadia Boukhelifa",
                            "Anastasia Bezerianos"
                        ],
                        "abstract": "In domains such as agronomy or manufacturing, experts need to consider trade-offs when making decisions that involve several, often competing, objectives. Such analysis is complex and may be conducted over long periods of time, making it hard to revisit.In this paper, we consider the use of analytic provenance mechanisms to aid experts recall and keep track of trade-off analysis. Weimplemented VisProm, a web-based trade-off analysis system, that incorporates in-visualization provenance views, designed to helpexperts keep track of trade-offs and their objectives. We used VisProm as a technology probe to understand user needs and explore thepotential role of provenance in this context. Through observation sessions with three groups of experts analyzing their own data, we makethe following contributions. We first, identify eight high-level tasks that experts engaged in during trade-off analysis, such as locating andcharacterizing interest zones in the trade-off space, and show how these tasks can be supported by provenance visualization. Second,we refine findings from previous work on provenance purposes such as recall and reproduce, by identifying specific objects of thesepurposes related to trade-off analysis, such as interest zones, and exploration structure (e.g., exploration of alternatives and branches).Third, we discuss insights on how the identified provenance objects and our designs support these trade-off analysis tasks, both whenrevisiting past analysis and while actively exploring. And finally, we identify new opportunities for provenance-driven trade-off analysis, forexample related to monitoring the coverage of the trade-off space, and tracking alternative trade-off scenario.",
                        "uid": "v-tvcg-9768153",
                        "file_name": "v-tvcg-9768153_Chakhchoukh_Presentation.mp4",
                        "time_stamp": "2022-10-20T20:57:00Z",
                        "time_start": "2022-10-20T20:57:00Z",
                        "time_end": "2022-10-20T21:06:47Z",
                        "paper_type": "full",
                        "keywords": [
                            "Provenance, visualization, trade-offs, multi-criteria, decision making, qualitative study"
                        ],
                        "has_image": "1",
                        "has_video": "587",
                        "paper_award": "",
                        "image_caption": "The VisProm technology probe includes several in-visualization provenance views to aid trade-off analysis, such as views of what objectives are maximized and minimized, what trade-offs have been considered, etc. We used it in an observational study with domain experts analyzing their own data, to gain insights into: when and how provenance visualization is used in trade-off analysis, differences in provenance use during a-posteri and active analysis, as well as to identify opportunities for future trade-off provenance visualizations.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "https://youtu.be/-Y5560R-8ic",
                        "ff_id": "-Y5560R-8ic"
                    },
                    {
                        "slot_id": "v-tvcg-9768153-qa",
                        "session_id": "full31",
                        "type": "Virtual Q+A",
                        "title": "Understanding How In-Visualization Provenance Can Support Trade-off Analysis (Q+A)",
                        "contributors": [
                            "Mehdi Chakhchoukh"
                        ],
                        "authors": [],
                        "abstract": "In domains such as agronomy or manufacturing, experts need to consider trade-offs when making decisions that involve several, often competing, objectives. Such analysis is complex and may be conducted over long periods of time, making it hard to revisit.In this paper, we consider the use of analytic provenance mechanisms to aid experts recall and keep track of trade-off analysis. Weimplemented VisProm, a web-based trade-off analysis system, that incorporates in-visualization provenance views, designed to helpexperts keep track of trade-offs and their objectives. We used VisProm as a technology probe to understand user needs and explore thepotential role of provenance in this context. Through observation sessions with three groups of experts analyzing their own data, we makethe following contributions. We first, identify eight high-level tasks that experts engaged in during trade-off analysis, such as locating andcharacterizing interest zones in the trade-off space, and show how these tasks can be supported by provenance visualization. Second,we refine findings from previous work on provenance purposes such as recall and reproduce, by identifying specific objects of thesepurposes related to trade-off analysis, such as interest zones, and exploration structure (e.g., exploration of alternatives and branches).Third, we discuss insights on how the identified provenance objects and our designs support these trade-off analysis tasks, both whenrevisiting past analysis and while actively exploring. And finally, we identify new opportunities for provenance-driven trade-off analysis, forexample related to monitoring the coverage of the trade-off space, and tracking alternative trade-off scenario.",
                        "uid": "v-tvcg-9768153",
                        "file_name": "",
                        "time_stamp": "2022-10-20T21:06:47Z",
                        "time_start": "2022-10-20T21:06:47Z",
                        "time_end": "2022-10-20T21:09:00Z",
                        "paper_type": "full",
                        "keywords": [
                            "Provenance, visualization, trade-offs, multi-criteria, decision making, qualitative study"
                        ],
                        "has_image": "1",
                        "has_video": "587",
                        "paper_award": "",
                        "image_caption": "The VisProm technology probe includes several in-visualization provenance views to aid trade-off analysis, such as views of what objectives are maximized and minimized, what trade-offs have been considered, etc. We used it in an observational study with domain experts analyzing their own data, to gain insights into: when and how provenance visualization is used in trade-off analysis, differences in provenance use during a-posteri and active analysis, as well as to identify opportunities for future trade-off provenance visualizations.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "https://youtu.be/-Y5560R-8ic",
                        "ff_id": "-Y5560R-8ic"
                    },
                    {
                        "slot_id": "v-tvcg-9652041-pres",
                        "session_id": "full31",
                        "type": "Virtual Presentation (pre-recorded)",
                        "title": "Provectories: Embedding-based Analysis of Interaction Provenance Data",
                        "contributors": [
                            "Conny Walchshofer",
                            "Andreas Hinterreiter"
                        ],
                        "authors": [
                            "Conny Walchshofer",
                            "Andreas Hinterreiter",
                            "Kai Xu",
                            "Holger Stitz",
                            "Marc Streit"
                        ],
                        "abstract": "Understanding user behavior patterns and visual analysis strategies is a long-standing challenge. Existing approaches rely largely on time-consuming manual processes such as interviews and the analysis of observational data. While it is technically possible to capture a history of user interactions and application states, it remains difficult to extract and describe analysis strategies based on interaction provenance. In this paper, we propose a novel visual approach to the meta-analysis of interaction provenance. We capture single and multiple user sessions as graphs of high-dimensional application states. Our meta-analysis is based on two different types of two-dimensional embeddings of these high-dimensional states: layouts based on (i) topology and (ii) attribute similarity. We applied these visualization approaches to synthetic and real user provenance data captured in two user studies. From our visualizations, we were able to extract patterns for data types and analytical reasoning strategies.",
                        "uid": "v-tvcg-9652041",
                        "file_name": "v-tvcg-9652041_Walchshofer_Presentation.mp4",
                        "time_stamp": "2022-10-20T21:09:00Z",
                        "time_start": "2022-10-20T21:09:00Z",
                        "time_end": "2022-10-20T21:18:15Z",
                        "paper_type": "full",
                        "keywords": [
                            "Visualization techniques, Information visualization, Visual analytics, Interaction Provenance, Sensemaking"
                        ],
                        "has_image": "1",
                        "has_video": "555",
                        "paper_award": "",
                        "image_caption": "Cluster-based analysis using t-SNE on the example of theoutlier cluster dataset, performing a multiple-user investigation. Distinct clusters (Cluster A--H) can be observed for outlier selections and superimposed trajectories, which indicates that data points were selected performed in the same a similar sequence by multiple users. The ground truth is indicated in orange",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "https://youtu.be/541q-olo6UA",
                        "ff_id": "541q-olo6UA"
                    },
                    {
                        "slot_id": "v-tvcg-9652041-qa",
                        "session_id": "full31",
                        "type": "Virtual Q+A",
                        "title": "Provectories: Embedding-based Analysis of Interaction Provenance Data (Q+A)",
                        "contributors": [
                            "Conny Walchshofer",
                            "Andreas Hinterreiter"
                        ],
                        "authors": [],
                        "abstract": "Understanding user behavior patterns and visual analysis strategies is a long-standing challenge. Existing approaches rely largely on time-consuming manual processes such as interviews and the analysis of observational data. While it is technically possible to capture a history of user interactions and application states, it remains difficult to extract and describe analysis strategies based on interaction provenance. In this paper, we propose a novel visual approach to the meta-analysis of interaction provenance. We capture single and multiple user sessions as graphs of high-dimensional application states. Our meta-analysis is based on two different types of two-dimensional embeddings of these high-dimensional states: layouts based on (i) topology and (ii) attribute similarity. We applied these visualization approaches to synthetic and real user provenance data captured in two user studies. From our visualizations, we were able to extract patterns for data types and analytical reasoning strategies.",
                        "uid": "v-tvcg-9652041",
                        "file_name": "",
                        "time_stamp": "2022-10-20T21:18:15Z",
                        "time_start": "2022-10-20T21:18:15Z",
                        "time_end": "2022-10-20T21:21:00Z",
                        "paper_type": "full",
                        "keywords": [
                            "Visualization techniques, Information visualization, Visual analytics, Interaction Provenance, Sensemaking"
                        ],
                        "has_image": "1",
                        "has_video": "555",
                        "paper_award": "",
                        "image_caption": "Cluster-based analysis using t-SNE on the example of theoutlier cluster dataset, performing a multiple-user investigation. Distinct clusters (Cluster A--H) can be observed for outlier selections and superimposed trajectories, which indicates that data points were selected performed in the same a similar sequence by multiple users. The ground truth is indicated in orange",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "https://youtu.be/541q-olo6UA",
                        "ff_id": "541q-olo6UA"
                    },
                    {
                        "slot_id": "v-full-1003-pres",
                        "session_id": "full31",
                        "type": "Virtual Presentation (pre-recorded)",
                        "title": "Lotse: A Practical Framework for Guidance in Visual Analytics",
                        "contributors": [
                            "Fabian Sperrle"
                        ],
                        "authors": [
                            "Fabian Sperrle",
                            "Davide Ceneda",
                            "Mennatallah El-Assady"
                        ],
                        "abstract": "Co-adaptive guidance aims to enable efficient human-machine collaboration in visual analytics, as proposed by multiple theoretical frameworks. This paper bridges the gap between such conceptual frameworks and practical implementation by introducing an accessible model of guidance and an accompanying guidance library, mapping theory into practice. We contribute a model of system-provided guidance based on design templates and derived strategies. We instantiate the model in a library called Lotse that allows specifying guidance strategies in definition files and generates running code from them. Lotse is the first guidance library using such an approach. It supports the creation of reusable guidance strategies to retrofit existing applications with guidance and fosters the creation of general guidance strategy patterns. We demonstrate its effectiveness through first-use case studies with VA researchers of varying guidance design expertise and find that they are able to effectively and quickly implement guidance with Lotse. Further, we analyze our framework's cognitive dimensions to evaluate its expressiveness and outline a summary of open research questions for aligning guidance practice with its intricate theory.",
                        "uid": "v-full-1003",
                        "file_name": "v-full-1003_Sperrle_Presentation.mp4",
                        "time_stamp": "2022-10-20T21:21:00Z",
                        "time_start": "2022-10-20T21:21:00Z",
                        "time_end": "2022-10-20T21:29:31Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "511",
                        "paper_award": "",
                        "image_caption": "We provide a library called Lotse that allows specifying guidance strategies in yaml definition files and generates running code from them. Lotse is the first guidance library to use such an approach. It supports the creation of reusable guidance strategies to retrofit existing applications with guidance and fosters the creation of general guidance strategy patterns. We demonstrated its effectiveness through first-use case studies with VA researchers of varying guidance design expertise and find that they are able to effectively and quickly implement guidance with Lotse.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "https://youtu.be/AiCCyBacEcs",
                        "ff_id": "AiCCyBacEcs"
                    },
                    {
                        "slot_id": "v-full-1003-qa",
                        "session_id": "full31",
                        "type": "Virtual Q+A",
                        "title": "Lotse: A Practical Framework for Guidance in Visual Analytics (Q+A)",
                        "contributors": [
                            "Fabian Sperrle"
                        ],
                        "authors": [],
                        "abstract": "Co-adaptive guidance aims to enable efficient human-machine collaboration in visual analytics, as proposed by multiple theoretical frameworks. This paper bridges the gap between such conceptual frameworks and practical implementation by introducing an accessible model of guidance and an accompanying guidance library, mapping theory into practice. We contribute a model of system-provided guidance based on design templates and derived strategies. We instantiate the model in a library called Lotse that allows specifying guidance strategies in definition files and generates running code from them. Lotse is the first guidance library using such an approach. It supports the creation of reusable guidance strategies to retrofit existing applications with guidance and fosters the creation of general guidance strategy patterns. We demonstrate its effectiveness through first-use case studies with VA researchers of varying guidance design expertise and find that they are able to effectively and quickly implement guidance with Lotse. Further, we analyze our framework's cognitive dimensions to evaluate its expressiveness and outline a summary of open research questions for aligning guidance practice with its intricate theory.",
                        "uid": "v-full-1003",
                        "file_name": "",
                        "time_stamp": "2022-10-20T21:29:31Z",
                        "time_start": "2022-10-20T21:29:31Z",
                        "time_end": "2022-10-20T21:33:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "511",
                        "paper_award": "",
                        "image_caption": "We provide a library called Lotse that allows specifying guidance strategies in yaml definition files and generates running code from them. Lotse is the first guidance library to use such an approach. It supports the creation of reusable guidance strategies to retrofit existing applications with guidance and fosters the creation of general guidance strategy patterns. We demonstrated its effectiveness through first-use case studies with VA researchers of varying guidance design expertise and find that they are able to effectively and quickly implement guidance with Lotse.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "https://youtu.be/AiCCyBacEcs",
                        "ff_id": "AiCCyBacEcs"
                    },
                    {
                        "slot_id": "v-full-1142-pres",
                        "session_id": "full31",
                        "type": "In Person Presentation",
                        "title": "Medley: Intent-based Recommendations to Support Dashboard Composition",
                        "contributors": [
                            "Arjun Srinivasan"
                        ],
                        "authors": [
                            "Aditeya Pandey",
                            "Arjun Srinivasan",
                            "Vidya Setlur"
                        ],
                        "abstract": "Despite the ever-growing popularity of dashboards across a wide range of domains, their authoring still remains a tedious and complex process. Current tools offer considerable support for creating individual visualizations but provide limited support for discovering groups of visualizations that can be collectively useful for composing analytic dashboards. To address this problem, we present MEDLEY, a mixed-initiative interface that assists in dashboard composition by recommending dashboard collections (i.e., a logically grouped set of views and filtering widgets) that map to specific analytical intents. Users can specify dashboard intents (namely, measure analysis, change analysis, category analysis, or distribution analysis) explicitly through an input panel in the interface or implicitly by selecting data attributes and views of interest. The system recommends collections based on these analytic intents, and views and widgets can be selected to compose a variety of dashboards. MEDLEY also provides a lightweight direct manipulation interface to configure interactions between views in a dashboard. Based on a study with 13 participants performing both targeted and open-ended tasks, we discuss how MEDLEY's recommendations guide dashboard composition and facilitate different user workflows. Observations from the study identify potential directions for future work, including combining manual view specification with dashboard recommendations and designing natural language interfaces for dashboard authoring.",
                        "uid": "v-full-1142",
                        "file_name": "v-full-1142_Pandey_Presentation.mp4",
                        "time_stamp": "2022-10-20T21:33:00Z",
                        "time_start": "2022-10-20T21:33:00Z",
                        "time_end": "2022-10-20T21:43:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "704",
                        "paper_award": "",
                        "image_caption": "MEDLEY\u2019s user interface. Users can select data attribute and intents from the input panel in the left. Collection recommendations are shown in the center of the screen. Views and widgets can be added from the recommendations to the dashboard canvas on the right.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "https://youtu.be/VeGU7bncXes",
                        "ff_id": "VeGU7bncXes"
                    },
                    {
                        "slot_id": "v-full-1142-qa",
                        "session_id": "full31",
                        "type": "In Person Q+A",
                        "title": "Medley: Intent-based Recommendations to Support Dashboard Composition (Q+A)",
                        "contributors": [
                            "Arjun Srinivasan"
                        ],
                        "authors": [],
                        "abstract": "Despite the ever-growing popularity of dashboards across a wide range of domains, their authoring still remains a tedious and complex process. Current tools offer considerable support for creating individual visualizations but provide limited support for discovering groups of visualizations that can be collectively useful for composing analytic dashboards. To address this problem, we present MEDLEY, a mixed-initiative interface that assists in dashboard composition by recommending dashboard collections (i.e., a logically grouped set of views and filtering widgets) that map to specific analytical intents. Users can specify dashboard intents (namely, measure analysis, change analysis, category analysis, or distribution analysis) explicitly through an input panel in the interface or implicitly by selecting data attributes and views of interest. The system recommends collections based on these analytic intents, and views and widgets can be selected to compose a variety of dashboards. MEDLEY also provides a lightweight direct manipulation interface to configure interactions between views in a dashboard. Based on a study with 13 participants performing both targeted and open-ended tasks, we discuss how MEDLEY's recommendations guide dashboard composition and facilitate different user workflows. Observations from the study identify potential directions for future work, including combining manual view specification with dashboard recommendations and designing natural language interfaces for dashboard authoring.",
                        "uid": "v-full-1142",
                        "file_name": "",
                        "time_stamp": "2022-10-20T21:43:00Z",
                        "time_start": "2022-10-20T21:43:00Z",
                        "time_end": "2022-10-20T21:45:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "704",
                        "paper_award": "",
                        "image_caption": "MEDLEY\u2019s user interface. Users can select data attribute and intents from the input panel in the left. Collection recommendations are shown in the center of the screen. Views and widgets can be added from the recommendations to the dashboard canvas on the right.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "https://youtu.be/VeGU7bncXes",
                        "ff_id": "VeGU7bncXes"
                    },
                    {
                        "slot_id": "v-tvcg-9524484-pres",
                        "session_id": "full31",
                        "type": "Virtual Presentation (pre-recorded)",
                        "title": "GEViTRec: Data Reconnaissance Through Recommendation Using a Domain-Specific Visualization Prevalence Design Space",
                        "contributors": [
                            "Tamara Munzner",
                            "Anamaria Crisan"
                        ],
                        "authors": [
                            "Anamaria Crisan",
                            "Shannah Fisher",
                            "Jennifer L. Gardy",
                            "Tamara Munzner"
                        ],
                        "abstract": "Genomic Epidemiology (genEpi) is a branch of public health that uses many different data types including tabular, network, genomic, and geographic, to identify and contain outbreaks of deadly diseases. Due to the volume and variety of data, it is challenging for genEpi domain experts to conduct data reconnaissance; that is, have an overview of the data they have and make assessments toward its quality, completeness, and suitability. We present an algorithm for data reconnaissance through automatic visualization recommendation, GEViTRec. Our approach handles a broad variety of dataset types and automatically generates visually coherent combinations of charts, in contrast to existing systems that primarily focus on singleton visual encodings of tabular datasets. We automatically detect linkages across multiple input datasets by analyzing non-numeric attribute fields, creating a data source graph within which we analyze and rank paths. For each high-ranking path, we specify chart combinations with positional and color alignments between shared fields, using a gradual binding approach to transform initial partial specifications of singleton charts to complete specifications that are aligned and oriented consistently. A novel aspect of our approach is its combination of domain-agnostic elements with domain-specific information that is captured through a domain-specific visualization prevalence design space. Our implementation is applied to both synthetic data and real Ebola outbreak data. We compare GEViTRec\u2019s output to what previous visualization recommendation systems would generate, and to manually crafted visualizations used by practitioners. We conducted formative evaluations with ten genEpi experts to assess the relevance and interpretability of our results.",
                        "uid": "v-tvcg-9524484",
                        "file_name": "v-tvcg-9524484_Crisan_Presentation.mp4",
                        "time_stamp": "2022-10-20T21:45:00Z",
                        "time_start": "2022-10-20T21:45:00Z",
                        "time_end": "2022-10-20T21:54:16Z",
                        "paper_type": "full",
                        "keywords": [
                            "Heterogeneous Data, Multiple Coordinated Views, Data Reconnaissance, Bioinformatics."
                        ],
                        "has_image": "1",
                        "has_video": "556",
                        "paper_award": "",
                        "image_caption": "We present GEViTRec - an approach for automatically generating visually coherent chart combinations from multiple diverse data sources.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "https://youtu.be/ko4ZBpBfFVk",
                        "ff_id": "ko4ZBpBfFVk"
                    },
                    {
                        "slot_id": "v-tvcg-9524484-qa",
                        "session_id": "full31",
                        "type": "Virtual Q+A",
                        "title": "GEViTRec: Data Reconnaissance Through Recommendation Using a Domain-Specific Visualization Prevalence Design Space (Q+A)",
                        "contributors": [
                            "Tamara Munzner",
                            "Anamaria Crisan"
                        ],
                        "authors": [],
                        "abstract": "Genomic Epidemiology (genEpi) is a branch of public health that uses many different data types including tabular, network, genomic, and geographic, to identify and contain outbreaks of deadly diseases. Due to the volume and variety of data, it is challenging for genEpi domain experts to conduct data reconnaissance; that is, have an overview of the data they have and make assessments toward its quality, completeness, and suitability. We present an algorithm for data reconnaissance through automatic visualization recommendation, GEViTRec. Our approach handles a broad variety of dataset types and automatically generates visually coherent combinations of charts, in contrast to existing systems that primarily focus on singleton visual encodings of tabular datasets. We automatically detect linkages across multiple input datasets by analyzing non-numeric attribute fields, creating a data source graph within which we analyze and rank paths. For each high-ranking path, we specify chart combinations with positional and color alignments between shared fields, using a gradual binding approach to transform initial partial specifications of singleton charts to complete specifications that are aligned and oriented consistently. A novel aspect of our approach is its combination of domain-agnostic elements with domain-specific information that is captured through a domain-specific visualization prevalence design space. Our implementation is applied to both synthetic data and real Ebola outbreak data. We compare GEViTRec\u2019s output to what previous visualization recommendation systems would generate, and to manually crafted visualizations used by practitioners. We conducted formative evaluations with ten genEpi experts to assess the relevance and interpretability of our results.",
                        "uid": "v-tvcg-9524484",
                        "file_name": "",
                        "time_stamp": "2022-10-20T21:54:16Z",
                        "time_start": "2022-10-20T21:54:16Z",
                        "time_end": "2022-10-20T21:57:00Z",
                        "paper_type": "full",
                        "keywords": [
                            "Heterogeneous Data, Multiple Coordinated Views, Data Reconnaissance, Bioinformatics."
                        ],
                        "has_image": "1",
                        "has_video": "556",
                        "paper_award": "",
                        "image_caption": "We present GEViTRec - an approach for automatically generating visually coherent chart combinations from multiple diverse data sources.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "https://youtu.be/ko4ZBpBfFVk",
                        "ff_id": "ko4ZBpBfFVk"
                    }
                ]
            },
            {
                "title": "(Volume) Rendering",
                "session_id": "full32",
                "event_prefix": "v-full",
                "track": "ok6",
                "livestream_id": "ok6-wed",
                "session_image": "full32.png",
                "chair": [
                    "Christoph Garth"
                ],
                "organizers": [],
                "time_start": "2022-10-19T19:00:00Z",
                "time_end": "2022-10-19T20:15:00Z",
                "discord_category": "",
                "discord_channel": "oklahoma-st-6",
                "discord_channel_id": "1024600906689421372",
                "discord_link": "https://discord.com/channels/978320435554947082/1024600906689421372",
                "slido_link": "https://app.sli.do/event/i28UETZaiRWr1XwX6eKjcf?section=29ba1f79-04cd-4568-a83c-ab81aa3c28b8",
                "youtube_url": "https://youtu.be/L523gBLIM5c",
                "youtube_id": "L523gBLIM5c",
                "zoom_meeting": "",
                "zoom_password": "",
                "zoom_link": "",
                "ff_link": "https://youtu.be/Er8JJbcfyA4",
                "ff_playlist": "",
                "time_slots": [
                    {
                        "slot_id": "full32-opening",
                        "session_id": "full32",
                        "type": "In Person Other",
                        "title": "Session Opening",
                        "contributors": [
                            "Christoph Garth"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "",
                        "file_name": "",
                        "time_stamp": "2022-10-19T19:00:00Z",
                        "time_start": "2022-10-19T19:00:00Z",
                        "time_end": "2022-10-19T19:00:00Z",
                        "paper_type": "",
                        "keywords": [],
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "v-full-1010-pres",
                        "session_id": "full32",
                        "type": "In Person Presentation",
                        "title": "FoVolNet: Fast Volume Rendering using Foveated Deep Neural Networks",
                        "contributors": [
                            "David Bauer"
                        ],
                        "authors": [
                            "David Bauer",
                            "Qi Wu",
                            "Kwan-Liu Ma"
                        ],
                        "abstract": "Volume data is found in many important scientific and engineering applications. Rendering this data for visualization at high quality and interactive rates for demanding applications such as virtual reality is still not easily achievable even using professional-grade hardware. We introduce FoVolNet --- a method to significantly increase the performance of volume data visualization. We develop a cost-effective foveated rendering pipeline that sparsely samples a volume around a focal point and reconstructs the full-frame using a deep neural network. Foveated rendering is a technique that prioritizes rendering computations around the user's focal point. This approach leverages properties of the human visual system, thereby saving computational resources when rendering data in the periphery of the user's field of vision. Our reconstruction network combines direct and kernel prediction methods to produce fast, stable, and perceptually convincing output. With a slim design and the use of quantization, our method outperforms state-of-the-art neural reconstruction techniques in both end-to-end frame times and visual quality. We conduct extensive evaluations of the system's rendering performance, inference speed, and perceptual properties, and we provide comparisons to competing neural image reconstruction techniques. Our test results show that FoVolNet consistently achieves significant time saving over conventional rendering while preserving perceptual quality.",
                        "uid": "v-full-1010",
                        "file_name": "v-full-1010_Bauer_Presentation.mp4",
                        "time_stamp": "2022-10-19T19:00:00Z",
                        "time_start": "2022-10-19T19:00:00Z",
                        "time_end": "2022-10-19T19:10:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "615",
                        "paper_award": "",
                        "image_caption": "Our neural rendering pipeline improves rendering performance while preserving image quality.\nA foveated ray marcher sparsely samples a volume (top left). The full image is then reconstructed using a multi-stage hybrid neural network (lower row). The resulting image quality is very similar to the ground truth image (top right) while providing between two to three times speed-up.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "https://youtu.be/axiljcwYYMI",
                        "ff_id": "axiljcwYYMI"
                    },
                    {
                        "slot_id": "v-full-1010-qa",
                        "session_id": "full32",
                        "type": "In Person Q+A",
                        "title": "FoVolNet: Fast Volume Rendering using Foveated Deep Neural Networks (Q+A)",
                        "contributors": [
                            "David Bauer"
                        ],
                        "authors": [],
                        "abstract": "Volume data is found in many important scientific and engineering applications. Rendering this data for visualization at high quality and interactive rates for demanding applications such as virtual reality is still not easily achievable even using professional-grade hardware. We introduce FoVolNet --- a method to significantly increase the performance of volume data visualization. We develop a cost-effective foveated rendering pipeline that sparsely samples a volume around a focal point and reconstructs the full-frame using a deep neural network. Foveated rendering is a technique that prioritizes rendering computations around the user's focal point. This approach leverages properties of the human visual system, thereby saving computational resources when rendering data in the periphery of the user's field of vision. Our reconstruction network combines direct and kernel prediction methods to produce fast, stable, and perceptually convincing output. With a slim design and the use of quantization, our method outperforms state-of-the-art neural reconstruction techniques in both end-to-end frame times and visual quality. We conduct extensive evaluations of the system's rendering performance, inference speed, and perceptual properties, and we provide comparisons to competing neural image reconstruction techniques. Our test results show that FoVolNet consistently achieves significant time saving over conventional rendering while preserving perceptual quality.",
                        "uid": "v-full-1010",
                        "file_name": "",
                        "time_stamp": "2022-10-19T19:10:00Z",
                        "time_start": "2022-10-19T19:10:00Z",
                        "time_end": "2022-10-19T19:12:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "615",
                        "paper_award": "",
                        "image_caption": "Our neural rendering pipeline improves rendering performance while preserving image quality.\nA foveated ray marcher sparsely samples a volume (top left). The full image is then reconstructed using a multi-stage hybrid neural network (lower row). The resulting image quality is very similar to the ground truth image (top right) while providing between two to three times speed-up.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "https://youtu.be/axiljcwYYMI",
                        "ff_id": "axiljcwYYMI"
                    },
                    {
                        "slot_id": "v-full-1418-pres",
                        "session_id": "full32",
                        "type": "Virtual Presentation (pre-recorded)",
                        "title": "GRay: Ray Casting for Visualization and Interactive Data Exploration of Gaussian Mixture Models",
                        "contributors": [
                            "Kai Lawonn"
                        ],
                        "authors": [
                            "Kai Lawonn",
                            "Monique Meuschke",
                            "Pepe Eulzer",
                            "Matthias Mitterreiter",
                            "Joachim Giesen",
                            "Tobias G\u00fcnther"
                        ],
                        "abstract": "The Gaussian mixture model (GMM) describes the distribution of random variables from several different populations. GMMs have widespread applications in probability theory, statistics, machine learning for unsupervised cluster analysis and topic modeling, as well as in deep learning pipelines. So far, few efforts have been made to explore the underlying point distribution in combination with the GMMs, in particular when the data becomes high-dimensional and when the GMMs are composed of many Gaussians. We present an analysis tool comprising various GPU-based visualization techniques to explore such complex GMMs. To facilitate the exploration of high-dimensional data, we provide a novel navigation system to analyze the underlying data. Instead of projecting the data to 2D, we utilize interactive 3D views to better support users in understanding the spatial arrangements of the Gaussian distributions. The interactive system is composed of two parts: (1) raycasting-based views that visualize cluster memberships, spatial arrangements, and support the discovery of new modes. (2) overview visualizations that enable the comparison of Gaussians with each other, as well as small multiples of different choices of basis vectors. Users are supported in their exploration with customization tools and smooth camera navigations. Our tool was developed and assessed by five domain experts, and its usefulness was evaluated with 23 participants. To demonstrate the effectiveness, we identify interesting features in several data sets.",
                        "uid": "v-full-1418",
                        "file_name": "v-full-1418_Lawonn_Presentation.mp4",
                        "time_stamp": "2022-10-19T19:12:00Z",
                        "time_start": "2022-10-19T19:12:00Z",
                        "time_end": "2022-10-19T19:21:53Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "593",
                        "paper_award": "",
                        "image_caption": "We present an exploration framework for Gaussian Mixture Models that combines different visualization techniques. The top row shows raycasting-based visualizations that reveal cluster memberships (MIP view), spatial arrangement of Gaussians (hull view), and new modes (DVR view). To manage complexity, the bottom row contains overview visualizations that allow for the comparison of shapes (circle plot, line plot, PCA plot, and small multiples) and different choices of basis vectors (small multiples).",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "https://youtu.be/Vh9iA5A-HNo",
                        "ff_id": "Vh9iA5A-HNo"
                    },
                    {
                        "slot_id": "v-full-1418-qa",
                        "session_id": "full32",
                        "type": "Virtual Q+A",
                        "title": "GRay: Ray Casting for Visualization and Interactive Data Exploration of Gaussian Mixture Models (Q+A)",
                        "contributors": [
                            "Kai Lawonn"
                        ],
                        "authors": [],
                        "abstract": "The Gaussian mixture model (GMM) describes the distribution of random variables from several different populations. GMMs have widespread applications in probability theory, statistics, machine learning for unsupervised cluster analysis and topic modeling, as well as in deep learning pipelines. So far, few efforts have been made to explore the underlying point distribution in combination with the GMMs, in particular when the data becomes high-dimensional and when the GMMs are composed of many Gaussians. We present an analysis tool comprising various GPU-based visualization techniques to explore such complex GMMs. To facilitate the exploration of high-dimensional data, we provide a novel navigation system to analyze the underlying data. Instead of projecting the data to 2D, we utilize interactive 3D views to better support users in understanding the spatial arrangements of the Gaussian distributions. The interactive system is composed of two parts: (1) raycasting-based views that visualize cluster memberships, spatial arrangements, and support the discovery of new modes. (2) overview visualizations that enable the comparison of Gaussians with each other, as well as small multiples of different choices of basis vectors. Users are supported in their exploration with customization tools and smooth camera navigations. Our tool was developed and assessed by five domain experts, and its usefulness was evaluated with 23 participants. To demonstrate the effectiveness, we identify interesting features in several data sets.",
                        "uid": "v-full-1418",
                        "file_name": "",
                        "time_stamp": "2022-10-19T19:21:53Z",
                        "time_start": "2022-10-19T19:21:53Z",
                        "time_end": "2022-10-19T19:24:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "593",
                        "paper_award": "",
                        "image_caption": "We present an exploration framework for Gaussian Mixture Models that combines different visualization techniques. The top row shows raycasting-based visualizations that reveal cluster memberships (MIP view), spatial arrangement of Gaussians (hull view), and new modes (DVR view). To manage complexity, the bottom row contains overview visualizations that allow for the comparison of shapes (circle plot, line plot, PCA plot, and small multiples) and different choices of basis vectors (small multiples).",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "https://youtu.be/Vh9iA5A-HNo",
                        "ff_id": "Vh9iA5A-HNo"
                    },
                    {
                        "slot_id": "v-full-1339-pres",
                        "session_id": "full32",
                        "type": "In Person Presentation",
                        "title": "Quick Clusters: A GPU-Parallel Partitioning for Efficient Path Tracing of Unstructured Volumetric Grids",
                        "contributors": [
                            "Nate Morrical"
                        ],
                        "authors": [
                            "Nate Morrical",
                            "Alper Sahistan",
                            "Ugur Gudukbay",
                            "Ingo Wald",
                            "Valerio Pascucci"
                        ],
                        "abstract": "We propose a simple yet effective method for clustering finite elements to improve preprocessing times and rendering performance of unstructured volumetric grids without requiring auxiliary connectivity data. Rather than building bounding volume hierarchies (BVHs) over individual elements, we sort elements along with a Hilbert curve and aggregate neighboring elements together, improving BVH memory consumption by over an order of magnitude. Then to further reduce memory consumption, we cluster the mesh on the fly into sub-meshes with smaller indices using a series of efficient parallel mesh re-indexing operations. \n These clusters are then passed to a highly optimized ray tracing API for point containment queries and ray-cluster intersection testing. Each cluster is assigned a maximum extinction value for adaptive sampling, which we rasterize into non-overlapping view-aligned bins allocated along the ray. These maximum extinction bins are then used to guide the placement of samples along the ray during visualization, reducing the number of samples required by multiple orders of magnitude (depending on the dataset), thereby improving overall visualization interactivity. Using our approach, we improve rendering performance over a competitive baseline on the NASA Mars Lander dataset from 6X (1 frame per second (fps) and 1.0M rays per second (rps) up to now 6fps and 12.4~M rps, now including volumetric shadows) while simultaneously reducing memory consumption by 3X (33GB down to 11GB) and avoiding any offline preprocessing steps, enabling high-quality interactive visualization on consumer graphics cards. Then by utilizing the full 48GB of an RTX 8000, we improve the performance of Lander by 17X (1fps up to 17fps, 1.0M rps up to 35.6M rps).",
                        "uid": "v-full-1339",
                        "file_name": "v-full-1339_Morrical_Presentation.mp4",
                        "time_stamp": "2022-10-19T19:24:00Z",
                        "time_start": "2022-10-19T19:24:00Z",
                        "time_end": "2022-10-19T19:34:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "0",
                        "has_video": "664",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "https://youtu.be/shi-_p-9QTE",
                        "ff_id": "shi-_p-9QTE"
                    },
                    {
                        "slot_id": "v-full-1339-qa",
                        "session_id": "full32",
                        "type": "In Person Q+A",
                        "title": "Quick Clusters: A GPU-Parallel Partitioning for Efficient Path Tracing of Unstructured Volumetric Grids (Q+A)",
                        "contributors": [
                            "Nate Morrical"
                        ],
                        "authors": [],
                        "abstract": "We propose a simple yet effective method for clustering finite elements to improve preprocessing times and rendering performance of unstructured volumetric grids without requiring auxiliary connectivity data. Rather than building bounding volume hierarchies (BVHs) over individual elements, we sort elements along with a Hilbert curve and aggregate neighboring elements together, improving BVH memory consumption by over an order of magnitude. Then to further reduce memory consumption, we cluster the mesh on the fly into sub-meshes with smaller indices using a series of efficient parallel mesh re-indexing operations. \n These clusters are then passed to a highly optimized ray tracing API for point containment queries and ray-cluster intersection testing. Each cluster is assigned a maximum extinction value for adaptive sampling, which we rasterize into non-overlapping view-aligned bins allocated along the ray. These maximum extinction bins are then used to guide the placement of samples along the ray during visualization, reducing the number of samples required by multiple orders of magnitude (depending on the dataset), thereby improving overall visualization interactivity. Using our approach, we improve rendering performance over a competitive baseline on the NASA Mars Lander dataset from 6X (1 frame per second (fps) and 1.0M rays per second (rps) up to now 6fps and 12.4~M rps, now including volumetric shadows) while simultaneously reducing memory consumption by 3X (33GB down to 11GB) and avoiding any offline preprocessing steps, enabling high-quality interactive visualization on consumer graphics cards. Then by utilizing the full 48GB of an RTX 8000, we improve the performance of Lander by 17X (1fps up to 17fps, 1.0M rps up to 35.6M rps).",
                        "uid": "v-full-1339",
                        "file_name": "",
                        "time_stamp": "2022-10-19T19:34:00Z",
                        "time_start": "2022-10-19T19:34:00Z",
                        "time_end": "2022-10-19T19:36:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "0",
                        "has_video": "664",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "https://youtu.be/shi-_p-9QTE",
                        "ff_id": "shi-_p-9QTE"
                    },
                    {
                        "slot_id": "v-tvcg-9806341-pres",
                        "session_id": "full32",
                        "type": "In Person Presentation",
                        "title": "Finding Nano-\u00d6tzi: Cryo-Electron Tomography Visualization Guided by Learned Segmentation",
                        "contributors": [
                            "Ngan Nguyen",
                            "Ciril Bohak"
                        ],
                        "authors": [
                            "Ngan Nguyen",
                            "Ciril Bohak",
                            "Dominik Engel",
                            "Peter Mindek",
                            "Ond\u0159ej Strnad",
                            "Peter Wonka",
                            "Sai Li",
                            "Timo Ropinski",
                            "Ivan Viola"
                        ],
                        "abstract": "Cryo-electron tomography (cryo-ET) is a new 3D imaging technique with unprecedented potential for resolving submicron structural details. Existing volume visualization methods, however, are not able to reveal details of interest due to low signal-to-noise ratio. In order to design more powerful transfer functions, we propose leveraging soft segmentation as an explicit component of visualization for noisy volumes. Our technical realization is based on semi-supervised learning, where we combine the advantages of two segmentation algorithms. First, the weak segmentation algorithm provides good results for propagating sparse user-provided labels to other voxels in the same volume and is used to generate dense pseudo-labels. Second, the powerful deep-learning-based segmentation algorithm learns from these pseudo-labels to generalize the segmentation to other unseen volumes, a task that the weak segmentation algorithm fails at completely. The proposed volume visualization uses deep-learning-based segmentation as a component for segmentation-aware transfer function design. Appropriate ramp parameters can be suggested automatically through frequency distribution analysis. Furthermore, our visualization uses gradient-free ambient occlusion shading to further suppress the visual presence of noise, and to give structural detail the desired prominence. The cryo-ET data studied in our technical experiments are based on the highest-quality tilted series of intact SARS-CoV-2 virions. Our technique shows the high impact in target sciences for visual data analysis of very noisy volumes that cannot be visualized with existing techniques.",
                        "uid": "v-tvcg-9806341",
                        "file_name": "v-tvcg-9806341_Nguyen_Presentation.mp4",
                        "time_stamp": "2022-10-19T19:36:00Z",
                        "time_start": "2022-10-19T19:36:00Z",
                        "time_end": "2022-10-19T19:46:00Z",
                        "paper_type": "full",
                        "keywords": [
                            "Volume Rendering; Computer Graphics Techniques; Machine Learning Techniques; Scalar Field Data; Life Sciences"
                        ],
                        "has_image": "1",
                        "has_video": "491",
                        "paper_award": "",
                        "image_caption": "The image shows a volume containing several intact SARS-CoV-2 virions acquired using cryo-electron tomography 3D imaging. From left to right: slice of the original data; direct volume rendering of the original data; foreground-background segmentation; color-coded four-class segmented data (background, spikes, membrane, lumen).",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "https://youtu.be/5OuOQMSrEWQ",
                        "ff_id": "5OuOQMSrEWQ"
                    },
                    {
                        "slot_id": "v-tvcg-9806341-qa",
                        "session_id": "full32",
                        "type": "In Person Q+A",
                        "title": "Finding Nano-\u00d6tzi: Cryo-Electron Tomography Visualization Guided by Learned Segmentation (Q+A)",
                        "contributors": [
                            "Ngan Nguyen",
                            "Ciril Bohak"
                        ],
                        "authors": [],
                        "abstract": "Cryo-electron tomography (cryo-ET) is a new 3D imaging technique with unprecedented potential for resolving submicron structural details. Existing volume visualization methods, however, are not able to reveal details of interest due to low signal-to-noise ratio. In order to design more powerful transfer functions, we propose leveraging soft segmentation as an explicit component of visualization for noisy volumes. Our technical realization is based on semi-supervised learning, where we combine the advantages of two segmentation algorithms. First, the weak segmentation algorithm provides good results for propagating sparse user-provided labels to other voxels in the same volume and is used to generate dense pseudo-labels. Second, the powerful deep-learning-based segmentation algorithm learns from these pseudo-labels to generalize the segmentation to other unseen volumes, a task that the weak segmentation algorithm fails at completely. The proposed volume visualization uses deep-learning-based segmentation as a component for segmentation-aware transfer function design. Appropriate ramp parameters can be suggested automatically through frequency distribution analysis. Furthermore, our visualization uses gradient-free ambient occlusion shading to further suppress the visual presence of noise, and to give structural detail the desired prominence. The cryo-ET data studied in our technical experiments are based on the highest-quality tilted series of intact SARS-CoV-2 virions. Our technique shows the high impact in target sciences for visual data analysis of very noisy volumes that cannot be visualized with existing techniques.",
                        "uid": "v-tvcg-9806341",
                        "file_name": "",
                        "time_stamp": "2022-10-19T19:46:00Z",
                        "time_start": "2022-10-19T19:46:00Z",
                        "time_end": "2022-10-19T19:48:00Z",
                        "paper_type": "full",
                        "keywords": [
                            "Volume Rendering; Computer Graphics Techniques; Machine Learning Techniques; Scalar Field Data; Life Sciences"
                        ],
                        "has_image": "1",
                        "has_video": "491",
                        "paper_award": "",
                        "image_caption": "The image shows a volume containing several intact SARS-CoV-2 virions acquired using cryo-electron tomography 3D imaging. From left to right: slice of the original data; direct volume rendering of the original data; foreground-background segmentation; color-coded four-class segmented data (background, spikes, membrane, lumen).",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "https://youtu.be/5OuOQMSrEWQ",
                        "ff_id": "5OuOQMSrEWQ"
                    },
                    {
                        "slot_id": "v-full-1496-pres",
                        "session_id": "full32",
                        "type": "Virtual Presentation (pre-recorded)",
                        "title": "Level Set Restricted Voronoi Tessellation for Large scale Spatial Statistical Analysis",
                        "contributors": [
                            "Tyson Neuroth"
                        ],
                        "authors": [
                            "Tyson Neuroth",
                            "Martin Rieth",
                            "Myoungkyu Lee",
                            "Konduri Aditya",
                            "Jacqueline Chen",
                            "Kwan-Liu Ma"
                        ],
                        "abstract": "Spatial statistical analysis of multivariate volumetric data can be challenging due to scale, complexity, and occlusion. Advances in topological segmentation, feature extraction, and statistical summarization have helped overcome the challenges. This work introduces a new spatial statistical decomposition method based on level sets, connected components, and a novel variation of the restricted centroidal Voronoi tessellation that is better suited for spatial statistical decomposition and parallel efficiency. The resulting data structures organize features into a coherent nested hierarchy to support flexible and efficient out-of-core region-of-interest extraction. Next, we provide an efficient parallel implementation. Finally, an interactive visualization system based on this approach is designed and then applied to turbulent combustion data. The combined approach enables an interactive spatial statistical analysis workflow for large-scale data with a top-down approach through multiple-levels-of-detail that links phase space statistics with spatial features.",
                        "uid": "v-full-1496",
                        "file_name": "v-full-1496_Neuroth_Presentation.mp4",
                        "time_stamp": "2022-10-19T19:48:00Z",
                        "time_start": "2022-10-19T19:48:00Z",
                        "time_end": "2022-10-19T19:58:20Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "620",
                        "paper_award": "",
                        "image_caption": "We decompose volume data hierachically based on isobands, connected components, and then restricted centroidal Voronoi tessellation of the connected components. These segments are then summarized with statistics and the data and the summaries are sorted so that each feature is contiguous on disk at each level. This provides an efficient method for out-of-core extraction to support efficient interactive visualization of the large multivariate data.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "https://youtu.be/QQN-LcMdhkY",
                        "ff_id": "QQN-LcMdhkY"
                    },
                    {
                        "slot_id": "v-full-1496-qa",
                        "session_id": "full32",
                        "type": "Virtual Q+A",
                        "title": "Level Set Restricted Voronoi Tessellation for Large scale Spatial Statistical Analysis (Q+A)",
                        "contributors": [
                            "Tyson Neuroth"
                        ],
                        "authors": [],
                        "abstract": "Spatial statistical analysis of multivariate volumetric data can be challenging due to scale, complexity, and occlusion. Advances in topological segmentation, feature extraction, and statistical summarization have helped overcome the challenges. This work introduces a new spatial statistical decomposition method based on level sets, connected components, and a novel variation of the restricted centroidal Voronoi tessellation that is better suited for spatial statistical decomposition and parallel efficiency. The resulting data structures organize features into a coherent nested hierarchy to support flexible and efficient out-of-core region-of-interest extraction. Next, we provide an efficient parallel implementation. Finally, an interactive visualization system based on this approach is designed and then applied to turbulent combustion data. The combined approach enables an interactive spatial statistical analysis workflow for large-scale data with a top-down approach through multiple-levels-of-detail that links phase space statistics with spatial features.",
                        "uid": "v-full-1496",
                        "file_name": "",
                        "time_stamp": "2022-10-19T19:58:20Z",
                        "time_start": "2022-10-19T19:58:20Z",
                        "time_end": "2022-10-19T20:00:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "620",
                        "paper_award": "",
                        "image_caption": "We decompose volume data hierachically based on isobands, connected components, and then restricted centroidal Voronoi tessellation of the connected components. These segments are then summarized with statistics and the data and the summaries are sorted so that each feature is contiguous on disk at each level. This provides an efficient method for out-of-core extraction to support efficient interactive visualization of the large multivariate data.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "https://youtu.be/QQN-LcMdhkY",
                        "ff_id": "QQN-LcMdhkY"
                    },
                    {
                        "slot_id": "v-tvcg-9770381-pres",
                        "session_id": "full32",
                        "type": "In Person Presentation",
                        "title": "Watertight Incremental Heightfield Tessellation",
                        "contributors": [
                            "Daniel Cornel"
                        ],
                        "authors": [
                            "Daniel Cornel; Silvana Zechmeister; Eduard Gr\u00f6ller; J\u00fcrgen Waser"
                        ],
                        "abstract": "In this paper, we propose a method for the interactive visualization of medium-scale dynamic heightfields without visual artifacts. Our data fall into a category too large to be rendered directly at full resolution, but small enough to fit into GPU memory without pre-filtering and data streaming. We present the real-world use case of unfiltered flood simulation data of such medium scale that need to be visualized in real time for scientific purposes. Our solution facilitates compute shaders to maintain a guaranteed watertight triangulation in GPU memory that approximates the interpolated heightfields with view-dependent, continuous levels of detail. In each frame, the triangulation is updated incrementally by iteratively refining the cached result of the previous frame to minimize the computational effort. In particular, we minimize the number of heightfield sampling operations to make adaptive and higher-order interpolations viable options. We impose no restriction on the number of subdivisions and the achievable level of detail to allow for extreme zoom ranges required in geospatial visualization. Our method provides a stable runtime performance and can be executed with a limited time budget. We present a comparison of our method to three state-of-the-art methods, in which our method is competitive to previous non-watertight methods in terms of runtime, while outperforming them in terms of accuracy.",
                        "uid": "v-tvcg-9770381",
                        "file_name": "v-tvcg-9770381_Cornel_Presentation.mp4",
                        "time_stamp": "2022-10-19T20:00:00Z",
                        "time_start": "2022-10-19T20:00:00Z",
                        "time_end": "2022-10-19T20:10:00Z",
                        "paper_type": "full",
                        "keywords": [
                            "Visualization techniques and methodologies, heightfield rendering, terrain rendering, level of detail, tessellation"
                        ],
                        "has_image": "1",
                        "has_video": "796",
                        "paper_award": "",
                        "image_caption": "Illustration of watertight tessellation of terrain and flood simulation heightfields with view-dependent level of detail. The generated triangulation is visible on the left with alternating colors to indicate odd and even numbers of subdivisions of the triangles. The shaded result rendered in real time is visible on the right.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "https://youtu.be/rmdX-em3JEA",
                        "ff_id": "rmdX-em3JEA"
                    },
                    {
                        "slot_id": "v-tvcg-9770381-qa",
                        "session_id": "full32",
                        "type": "In Person Q+A",
                        "title": "Watertight Incremental Heightfield Tessellation (Q+A)",
                        "contributors": [
                            "Daniel Cornel"
                        ],
                        "authors": [],
                        "abstract": "In this paper, we propose a method for the interactive visualization of medium-scale dynamic heightfields without visual artifacts. Our data fall into a category too large to be rendered directly at full resolution, but small enough to fit into GPU memory without pre-filtering and data streaming. We present the real-world use case of unfiltered flood simulation data of such medium scale that need to be visualized in real time for scientific purposes. Our solution facilitates compute shaders to maintain a guaranteed watertight triangulation in GPU memory that approximates the interpolated heightfields with view-dependent, continuous levels of detail. In each frame, the triangulation is updated incrementally by iteratively refining the cached result of the previous frame to minimize the computational effort. In particular, we minimize the number of heightfield sampling operations to make adaptive and higher-order interpolations viable options. We impose no restriction on the number of subdivisions and the achievable level of detail to allow for extreme zoom ranges required in geospatial visualization. Our method provides a stable runtime performance and can be executed with a limited time budget. We present a comparison of our method to three state-of-the-art methods, in which our method is competitive to previous non-watertight methods in terms of runtime, while outperforming them in terms of accuracy.",
                        "uid": "v-tvcg-9770381",
                        "file_name": "",
                        "time_stamp": "2022-10-19T20:10:00Z",
                        "time_start": "2022-10-19T20:10:00Z",
                        "time_end": "2022-10-19T20:12:00Z",
                        "paper_type": "full",
                        "keywords": [
                            "Visualization techniques and methodologies, heightfield rendering, terrain rendering, level of detail, tessellation"
                        ],
                        "has_image": "1",
                        "has_video": "796",
                        "paper_award": "",
                        "image_caption": "Illustration of watertight tessellation of terrain and flood simulation heightfields with view-dependent level of detail. The generated triangulation is visible on the left with alternating colors to indicate odd and even numbers of subdivisions of the triangles. The shaded result rendered in real time is visible on the right.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "https://youtu.be/rmdX-em3JEA",
                        "ff_id": "rmdX-em3JEA"
                    }
                ]
            }
        ]
    },
    "v-short": {
        "event": "VIS Short Papers",
        "long_name": "VIS Short Papers",
        "event_type": "Paper Presentations",
        "event_prefix": "v-short",
        "event_description": "",
        "event_url": "",
        "organizers": [],
        "sessions": [
            {
                "title": "Visualization Systems and Graph Visualization",
                "session_id": "short1",
                "event_prefix": "v-short",
                "track": "ok1",
                "livestream_id": "ok1-wed",
                "session_image": "short1.png",
                "chair": [
                    "Katherine E. Isaacs"
                ],
                "organizers": [],
                "time_start": "2022-10-19T19:00:00Z",
                "time_end": "2022-10-19T20:15:00Z",
                "discord_category": "",
                "discord_channel": "oklahoma-st-1",
                "discord_channel_id": "1024600861340606484",
                "discord_link": "https://discord.com/channels/978320435554947082/1024600861340606484",
                "slido_link": "https://app.sli.do/event/i28UETZaiRWr1XwX6eKjcf?section=740d9835-fe47-4cb4-b260-353262a7f8dd",
                "youtube_url": "https://youtu.be/EK30lilKKdM",
                "youtube_id": "EK30lilKKdM",
                "zoom_meeting": "",
                "zoom_password": "",
                "zoom_link": "",
                "ff_link": "https://youtu.be/nf5wm5diyI8",
                "ff_playlist": "",
                "time_slots": [
                    {
                        "slot_id": "short1-opening",
                        "session_id": "short1",
                        "type": "In Person Other",
                        "title": "Session Opening",
                        "contributors": [
                            "Katherine E. Isaacs"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "",
                        "file_name": "",
                        "time_stamp": "2022-10-19T19:00:00Z",
                        "time_start": "2022-10-19T19:00:00Z",
                        "time_end": "2022-10-19T19:00:00Z",
                        "paper_type": "",
                        "keywords": [],
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "v-short-1012-pres",
                        "session_id": "short1",
                        "type": "In Person Presentation",
                        "title": "Facilitating Conversational Interaction in Natural Language Interfaces for Visualization",
                        "contributors": [
                            "Arpit Narechania"
                        ],
                        "authors": [
                            "Rishab Mitra",
                            "Arpit Narechania",
                            "Alex Endert",
                            "John Stasko"
                        ],
                        "abstract": "Natural language (NL) toolkits enable visualization developers, who may not have a background in natural language processing (NLP), to create natural language interfaces (NLIs) for end-users to flexibly specify and interact with visualizations. However, these toolkits currently only support one-off utterances, with minimal capability to facilitate a multi-turn dialog between the user and the system. Developing NLIs with such conversational interaction capabilities remains a challenging task, requiring implementations of low-level NLP techniques to process a new query as an intent to follow-up on an older query. We extend an existing Python-based toolkit, NL4DV, that processes an NL query about a tabular dataset and returns an analytic specification containing data attributes, analytic tasks, and relevant visualizations, modeled as a JSON object. Specifically, NL4DV now enables developers to facilitate multiple simultaneous conversations about a dataset and resolve associated ambiguities, augmenting new conversational information into the output JSON object. We demonstrate these capabilities through three examples: (1) an NLI to learn aspects of the Vega-Lite grammar, (2) a mind mapping application to create free-flowing conversations, and (3) a chatbot to answer questions and resolve ambiguities.",
                        "uid": "v-short-1012",
                        "file_name": "v-short-1012_Mitra_Presentation.mp4",
                        "time_stamp": "2022-10-19T19:00:00Z",
                        "time_start": "2022-10-19T19:00:00Z",
                        "time_end": "2022-10-19T19:07:00Z",
                        "paper_type": "short",
                        "keywords": [
                            "Natural Language Interfaces; Visualization Toolkits; Conversational Interaction"
                        ],
                        "has_image": "1",
                        "has_video": "372",
                        "paper_award": "",
                        "image_caption": "Natural language (NL) toolkits enable visualization developers, who may not have a background in natural language processing (NLP), to create natural language interfaces (NLIs) for end-users to flexibly specify and interact with visualizations. NL4DV is one such Python-based toolkit that processes an NL query about a tabular dataset and returns an analytic specification containing data attributes, analytic tasks, and relevant visualizations, modeled as a JSON object. We extend NL4DV to enable developers to integrate powerful conversational interaction capabilities, e.g., facilitate a multi-turn dialog between the user and the system instead of one-off utterances. For example, given a dataset on houses, a user asks, \u201cShow mean prices for different home types over the years.\u201d In response, NL4DV recommends a multi-series line-chart visualization. Desiring a bar chart instead, the user asks a follow-up query, \u201cAs a bar chart\u201d and NL4DV outputs a bar chart visualization. Finally, desiring only certain home types, the user asks, \u201cJust show condos and duplexes.\u201d NL4DV outputs a filtered visualization.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "v-short-1012-qa",
                        "session_id": "short1",
                        "type": "In Person Q+A",
                        "title": "Facilitating Conversational Interaction in Natural Language Interfaces for Visualization (Q+A)",
                        "contributors": [
                            "Arpit Narechania"
                        ],
                        "authors": [],
                        "abstract": "Natural language (NL) toolkits enable visualization developers, who may not have a background in natural language processing (NLP), to create natural language interfaces (NLIs) for end-users to flexibly specify and interact with visualizations. However, these toolkits currently only support one-off utterances, with minimal capability to facilitate a multi-turn dialog between the user and the system. Developing NLIs with such conversational interaction capabilities remains a challenging task, requiring implementations of low-level NLP techniques to process a new query as an intent to follow-up on an older query. We extend an existing Python-based toolkit, NL4DV, that processes an NL query about a tabular dataset and returns an analytic specification containing data attributes, analytic tasks, and relevant visualizations, modeled as a JSON object. Specifically, NL4DV now enables developers to facilitate multiple simultaneous conversations about a dataset and resolve associated ambiguities, augmenting new conversational information into the output JSON object. We demonstrate these capabilities through three examples: (1) an NLI to learn aspects of the Vega-Lite grammar, (2) a mind mapping application to create free-flowing conversations, and (3) a chatbot to answer questions and resolve ambiguities.",
                        "uid": "v-short-1012",
                        "file_name": "",
                        "time_stamp": "2022-10-19T19:07:00Z",
                        "time_start": "2022-10-19T19:07:00Z",
                        "time_end": "2022-10-19T19:09:00Z",
                        "paper_type": "short",
                        "keywords": [
                            "Natural Language Interfaces; Visualization Toolkits; Conversational Interaction"
                        ],
                        "has_image": "1",
                        "has_video": "372",
                        "paper_award": "",
                        "image_caption": "Natural language (NL) toolkits enable visualization developers, who may not have a background in natural language processing (NLP), to create natural language interfaces (NLIs) for end-users to flexibly specify and interact with visualizations. NL4DV is one such Python-based toolkit that processes an NL query about a tabular dataset and returns an analytic specification containing data attributes, analytic tasks, and relevant visualizations, modeled as a JSON object. We extend NL4DV to enable developers to integrate powerful conversational interaction capabilities, e.g., facilitate a multi-turn dialog between the user and the system instead of one-off utterances. For example, given a dataset on houses, a user asks, \u201cShow mean prices for different home types over the years.\u201d In response, NL4DV recommends a multi-series line-chart visualization. Desiring a bar chart instead, the user asks a follow-up query, \u201cAs a bar chart\u201d and NL4DV outputs a bar chart visualization. Finally, desiring only certain home types, the user asks, \u201cJust show condos and duplexes.\u201d NL4DV outputs a filtered visualization.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "v-short-1044-pres",
                        "session_id": "short1",
                        "type": "In Person Presentation",
                        "title": "VegaFusion: Automatic Server-Side Scaling for Interactive Vega Visualizations",
                        "contributors": [
                            "Nicolas Kruchten"
                        ],
                        "authors": [
                            "Nicolas Kruchten",
                            "Jon Mease",
                            "Dominik Moritz"
                        ],
                        "abstract": "The Vega grammar has been broadly adopted by a growing ecosystem of browser-based visualization tools. However, the reference Vega renderer does not scale well to large datasets (e.g., millions of rows or hundreds of megabytes) because it requires the entire dataset to be loaded into browser memory. We introduce VegaFusion, which brings automatic server-side scaling to the Vega ecosystem. VegaFusion accepts generic Vega specifications and partitions the required computation between the client and an out-of-browser, natively-compiled server-side process. Large datasets can be processed server-side to avoid loading them into the browser and to take advantage of multi-threading, more powerful server hardware and caching. We demonstrate how VegaFusion can be integrated into the existing Vega ecosystem, and show that VegaFusion greatly outperforms the reference implementation. We demonstrate these benefits with VegaFusion running on the same machine as the client as well as on a remote machine.",
                        "uid": "v-short-1044",
                        "file_name": "v-short-1044_Kruchten_Presentation.mp4",
                        "time_stamp": "2022-10-19T19:09:00Z",
                        "time_start": "2022-10-19T19:09:00Z",
                        "time_end": "2022-10-19T19:16:00Z",
                        "paper_type": "short",
                        "keywords": [
                            "Human-centered computing\u2014Visualization\u2014Visualization systems and tools\u2014Visualization toolkits; Human-centered computing\u2014Visualization\u2014Visualization application domains\u2014Information visualization;"
                        ],
                        "has_image": "1",
                        "has_video": "413",
                        "paper_award": "",
                        "image_caption": "A generic Vega specification is automatically partitioned by the VegaFusion Planner into a runtime specification for the VegaFusion Middleware (describing operations on large datasets) and a client specification for Vega (describing the visualization of the output of these operations as well as client-side interactions). The Middleware dynamically responds to interaction signals from Vega by querying an out-of-browser, natively-compiled VegaFusion Runtime instance and relaying the results back to Vega.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "v-short-1044-qa",
                        "session_id": "short1",
                        "type": "In Person Q+A",
                        "title": "VegaFusion: Automatic Server-Side Scaling for Interactive Vega Visualizations (Q+A)",
                        "contributors": [
                            "Nicolas Kruchten"
                        ],
                        "authors": [],
                        "abstract": "The Vega grammar has been broadly adopted by a growing ecosystem of browser-based visualization tools. However, the reference Vega renderer does not scale well to large datasets (e.g., millions of rows or hundreds of megabytes) because it requires the entire dataset to be loaded into browser memory. We introduce VegaFusion, which brings automatic server-side scaling to the Vega ecosystem. VegaFusion accepts generic Vega specifications and partitions the required computation between the client and an out-of-browser, natively-compiled server-side process. Large datasets can be processed server-side to avoid loading them into the browser and to take advantage of multi-threading, more powerful server hardware and caching. We demonstrate how VegaFusion can be integrated into the existing Vega ecosystem, and show that VegaFusion greatly outperforms the reference implementation. We demonstrate these benefits with VegaFusion running on the same machine as the client as well as on a remote machine.",
                        "uid": "v-short-1044",
                        "file_name": "",
                        "time_stamp": "2022-10-19T19:16:00Z",
                        "time_start": "2022-10-19T19:16:00Z",
                        "time_end": "2022-10-19T19:18:00Z",
                        "paper_type": "short",
                        "keywords": [
                            "Human-centered computing\u2014Visualization\u2014Visualization systems and tools\u2014Visualization toolkits; Human-centered computing\u2014Visualization\u2014Visualization application domains\u2014Information visualization;"
                        ],
                        "has_image": "1",
                        "has_video": "413",
                        "paper_award": "",
                        "image_caption": "A generic Vega specification is automatically partitioned by the VegaFusion Planner into a runtime specification for the VegaFusion Middleware (describing operations on large datasets) and a client specification for Vega (describing the visualization of the output of these operations as well as client-side interactions). The Middleware dynamically responds to interaction signals from Vega by querying an out-of-browser, natively-compiled VegaFusion Runtime instance and relaying the results back to Vega.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "v-short-1098-pres",
                        "session_id": "short1",
                        "type": "In Person Presentation",
                        "title": "Streamlining Visualization Authoring in D3 Through User-Driven Templates",
                        "contributors": [
                            "Hannah K. Bako"
                        ],
                        "authors": [
                            "Hannah K. Bako",
                            "Alisha Varma",
                            "Anuoluwapo Faboro",
                            "Mahreen Haider",
                            "Favour Nerrise",
                            "Bissaka Kenah",
                            "Leilani Battle"
                        ],
                        "abstract": "D3 is arguably the most popular tool for implementing web-based visualizations. Yet D3 has a steep learning curve that may hinder its adoption and continued use. To simplify the process of programming D3 visualizations, we must first understand the space of implementation practices that D3 users engage in. We present a qualitative analysis of 2500 D3 visualizations and their corresponding implementations. We find that 5 visualization types (Bar Charts, Geomaps, Line Charts, Scatterplots, and Force Directed Graphs) account for 80% of D3 visualizations found in our corpus. While implementation styles vary slightly across designs, the underlying code structure for all visualization types remains the same; presenting an opportunity for code reuse. Using our corpus of D3 examples, we synthesize reusable code templates for eight popular D3 visualization types and share them in our open source repository. Based on our results, we discuss design considerations for leveraging users\u2019 implementation patterns to reduce visualization design effort through design templates and auto-generated code recommendations.",
                        "uid": "v-short-1098",
                        "file_name": "v-short-1098_Bako_Presentation.mp4",
                        "time_stamp": "2022-10-19T19:18:00Z",
                        "time_start": "2022-10-19T19:18:00Z",
                        "time_end": "2022-10-19T19:25:00Z",
                        "paper_type": "short",
                        "keywords": [
                            "Human-centered computing\u2014Visualization\u2014Visualization systems and tools\u2014Visualization toolkits;"
                        ],
                        "has_image": "1",
                        "has_video": "478",
                        "paper_award": "",
                        "image_caption": "The image begins with text which reads \"Streamlining visualization in D3 through user-driven templates\" on a red background across the top of the image. Below is a collage made up of examples of bespoke visualizations from our analysis. (A) renders the number of IMDB votes and corresponding ratings of movies in a movies dataset. (B) is a narrative chart of scenes from Star Wars: Episode IV. (C) visualizes a braille clock, (D) is a D3\nrendering of Sierpinski Charlet, and (E) is a rendering of bounding box collisions using D3\u2019s force simulation.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "v-short-1098-qa",
                        "session_id": "short1",
                        "type": "In Person Q+A",
                        "title": "Streamlining Visualization Authoring in D3 Through User-Driven Templates (Q+A)",
                        "contributors": [
                            "Hannah K. Bako"
                        ],
                        "authors": [],
                        "abstract": "D3 is arguably the most popular tool for implementing web-based visualizations. Yet D3 has a steep learning curve that may hinder its adoption and continued use. To simplify the process of programming D3 visualizations, we must first understand the space of implementation practices that D3 users engage in. We present a qualitative analysis of 2500 D3 visualizations and their corresponding implementations. We find that 5 visualization types (Bar Charts, Geomaps, Line Charts, Scatterplots, and Force Directed Graphs) account for 80% of D3 visualizations found in our corpus. While implementation styles vary slightly across designs, the underlying code structure for all visualization types remains the same; presenting an opportunity for code reuse. Using our corpus of D3 examples, we synthesize reusable code templates for eight popular D3 visualization types and share them in our open source repository. Based on our results, we discuss design considerations for leveraging users\u2019 implementation patterns to reduce visualization design effort through design templates and auto-generated code recommendations.",
                        "uid": "v-short-1098",
                        "file_name": "",
                        "time_stamp": "2022-10-19T19:25:00Z",
                        "time_start": "2022-10-19T19:25:00Z",
                        "time_end": "2022-10-19T19:27:00Z",
                        "paper_type": "short",
                        "keywords": [
                            "Human-centered computing\u2014Visualization\u2014Visualization systems and tools\u2014Visualization toolkits;"
                        ],
                        "has_image": "1",
                        "has_video": "478",
                        "paper_award": "",
                        "image_caption": "The image begins with text which reads \"Streamlining visualization in D3 through user-driven templates\" on a red background across the top of the image. Below is a collage made up of examples of bespoke visualizations from our analysis. (A) renders the number of IMDB votes and corresponding ratings of movies in a movies dataset. (B) is a narrative chart of scenes from Star Wars: Episode IV. (C) visualizes a braille clock, (D) is a D3\nrendering of Sierpinski Charlet, and (E) is a rendering of bounding box collisions using D3\u2019s force simulation.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "v-short-1021-pres",
                        "session_id": "short1",
                        "type": "In Person Presentation",
                        "title": "Plotly-Resampler: Effective Visual Analytics for Large Time Series",
                        "contributors": [
                            "Jonas Van Der Donckt"
                        ],
                        "authors": [
                            "Jonas Van Der Donckt",
                            "Jeroen Van Der Donckt",
                            "Emiel Deprost",
                            "Sofie Van Hoecke"
                        ],
                        "abstract": "Visual analytics is arguably the most important step in getting acquainted with your data. This is especially the case for time series, as this data type is hard to describe and cannot be fully understood when using for example summary statistics. To realize effective time series visualization, four requirements have to be met; a tool should be (1) interactive, (2) scalable to millions of data points, (3) integrable in conventional data science environments, and (4) highly configurable. We observe that open source Python visualization toolkits empower data scientists in most visual analytics tasks, but lack the combination of scalability and interactivity to realize effective time series visualization. As a means to facilitate these requirements, we created Plotly-Resampler, an open source Python library. Plotly-Resampler is an add-on for Plotly's Python bindings, enhancing line chart scalability on top of an interactive toolkit by aggregating the underlying data depending on the current graph view. Plotly-Resampler is built to be snappy, as the reactivity of a tool qualitatively affects how analysts visually explore and analyze data. A benchmark task highlights how our toolkit scales better than alternatives in terms of number of samples and time series. Additionally, Plotly-Resampler's flexible data aggregation functionality paves the path towards researching novel aggregation techniques. Plotly-Resampler's integrability, together with its configurability, convenience, and high scalability, allows to effectively analyze high-frequency data in your day-to-day Python environment.",
                        "uid": "v-short-1021",
                        "file_name": "v-short-1021_Vanderdonckt_Presentation.mp4",
                        "time_stamp": "2022-10-19T19:27:00Z",
                        "time_start": "2022-10-19T19:27:00Z",
                        "time_end": "2022-10-19T19:34:00Z",
                        "paper_type": "short",
                        "keywords": [
                            "Time series, Visual analytics, Python, Dash, Plotly, Open source"
                        ],
                        "has_image": "1",
                        "has_video": "603",
                        "paper_award": "",
                        "image_caption": "v-short 1021 highlights Plotly-Resampler; an open-source Python toolkit which aims to improve effective visual analytics for large time series. Plotly-Resampler adds scalability to an interactive visualization toolkit (Plotly), by separating the visualization into two components; a front- and a back-end. The front-end shows an aggregation of the raw time series data which is stored in the back-end. Optimized callbacks and aggregation methods enable Plotly-Resampler to interactively visualize large time series. Check out the code at GitHub: github.com/predict-idlab/plotly-resampler",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "v-short-1021-qa",
                        "session_id": "short1",
                        "type": "In Person Q+A",
                        "title": "Plotly-Resampler: Effective Visual Analytics for Large Time Series (Q+A)",
                        "contributors": [
                            "Jonas Van Der Donckt"
                        ],
                        "authors": [],
                        "abstract": "Visual analytics is arguably the most important step in getting acquainted with your data. This is especially the case for time series, as this data type is hard to describe and cannot be fully understood when using for example summary statistics. To realize effective time series visualization, four requirements have to be met; a tool should be (1) interactive, (2) scalable to millions of data points, (3) integrable in conventional data science environments, and (4) highly configurable. We observe that open source Python visualization toolkits empower data scientists in most visual analytics tasks, but lack the combination of scalability and interactivity to realize effective time series visualization. As a means to facilitate these requirements, we created Plotly-Resampler, an open source Python library. Plotly-Resampler is an add-on for Plotly's Python bindings, enhancing line chart scalability on top of an interactive toolkit by aggregating the underlying data depending on the current graph view. Plotly-Resampler is built to be snappy, as the reactivity of a tool qualitatively affects how analysts visually explore and analyze data. A benchmark task highlights how our toolkit scales better than alternatives in terms of number of samples and time series. Additionally, Plotly-Resampler's flexible data aggregation functionality paves the path towards researching novel aggregation techniques. Plotly-Resampler's integrability, together with its configurability, convenience, and high scalability, allows to effectively analyze high-frequency data in your day-to-day Python environment.",
                        "uid": "v-short-1021",
                        "file_name": "",
                        "time_stamp": "2022-10-19T19:34:00Z",
                        "time_start": "2022-10-19T19:34:00Z",
                        "time_end": "2022-10-19T19:36:00Z",
                        "paper_type": "short",
                        "keywords": [
                            "Time series, Visual analytics, Python, Dash, Plotly, Open source"
                        ],
                        "has_image": "1",
                        "has_video": "603",
                        "paper_award": "",
                        "image_caption": "v-short 1021 highlights Plotly-Resampler; an open-source Python toolkit which aims to improve effective visual analytics for large time series. Plotly-Resampler adds scalability to an interactive visualization toolkit (Plotly), by separating the visualization into two components; a front- and a back-end. The front-end shows an aggregation of the raw time series data which is stored in the back-end. Optimized callbacks and aggregation methods enable Plotly-Resampler to interactively visualize large time series. Check out the code at GitHub: github.com/predict-idlab/plotly-resampler",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "v-short-1011-pres",
                        "session_id": "short1",
                        "type": "In Person Presentation",
                        "title": "Explaining Website Reliability by Visualizing Hyperlink Connectivity",
                        "contributors": [
                            "Seongmin Lee"
                        ],
                        "authors": [
                            "Seongmin Lee",
                            "Sadia Afroz",
                            "Haekyu Park",
                            "Zijie J. Wang",
                            "Omar Shaikh",
                            "Vibhor Sehgal",
                            "Ankit Peshin",
                            "Duen Horng Chau"
                        ],
                        "abstract": "As the information on the Internet continues growing exponentially, understanding and assessing the reliability of a website is becoming increasingly important. Misinformation has far-ranging repercussions, from sowing mistrust in media to undermining democratic elections. While some research investigates how to alert people to misinformation on the web, much less research has been conducted on explaining how websites engage in spreading false information. To fill the research gap, we present MisVis, a web-based interactive visualization tool that helps users assess a website\u2019s reliability by understanding how it engages in spreading false information on the World Wide Web. MisVis visualizes the hyperlink connectivity of the website and summarizes key characteristics of the Twitter accounts that mention the site. A large-scale user study with 139 participants demonstrates that MisVis facilitates users to assess and understand false information on the web and node-link diagrams can be used to communicate with non-experts. MisVis is available at the public demo link: https://poloclub.github.io/MisVis.",
                        "uid": "v-short-1011",
                        "file_name": "v-short-1011_Lee_Presentation.mp4",
                        "time_stamp": "2022-10-19T19:36:00Z",
                        "time_start": "2022-10-19T19:36:00Z",
                        "time_end": "2022-10-19T19:43:00Z",
                        "paper_type": "short",
                        "keywords": [
                            "Human-centered computing\u2014Visualization\u2014Visualization systems and tools\u2014Visualization toolkits"
                        ],
                        "has_image": "1",
                        "has_video": "419",
                        "paper_award": "",
                        "image_caption": "MisVis helps users assess a website\u2019s reliability and understand how the site may be involved in spreading false information by visualizing its hyperlink connectivity and summarizing how it is shared on Twitter. A large-scale user study demonstrates the effectiveness of MisVis.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "v-short-1011-qa",
                        "session_id": "short1",
                        "type": "In Person Q+A",
                        "title": "Explaining Website Reliability by Visualizing Hyperlink Connectivity (Q+A)",
                        "contributors": [
                            "Seongmin Lee"
                        ],
                        "authors": [],
                        "abstract": "As the information on the Internet continues growing exponentially, understanding and assessing the reliability of a website is becoming increasingly important. Misinformation has far-ranging repercussions, from sowing mistrust in media to undermining democratic elections. While some research investigates how to alert people to misinformation on the web, much less research has been conducted on explaining how websites engage in spreading false information. To fill the research gap, we present MisVis, a web-based interactive visualization tool that helps users assess a website\u2019s reliability by understanding how it engages in spreading false information on the World Wide Web. MisVis visualizes the hyperlink connectivity of the website and summarizes key characteristics of the Twitter accounts that mention the site. A large-scale user study with 139 participants demonstrates that MisVis facilitates users to assess and understand false information on the web and node-link diagrams can be used to communicate with non-experts. MisVis is available at the public demo link: https://poloclub.github.io/MisVis.",
                        "uid": "v-short-1011",
                        "file_name": "",
                        "time_stamp": "2022-10-19T19:43:00Z",
                        "time_start": "2022-10-19T19:43:00Z",
                        "time_end": "2022-10-19T19:45:00Z",
                        "paper_type": "short",
                        "keywords": [
                            "Human-centered computing\u2014Visualization\u2014Visualization systems and tools\u2014Visualization toolkits"
                        ],
                        "has_image": "1",
                        "has_video": "419",
                        "paper_award": "",
                        "image_caption": "MisVis helps users assess a website\u2019s reliability and understand how the site may be involved in spreading false information by visualizing its hyperlink connectivity and summarizing how it is shared on Twitter. A large-scale user study demonstrates the effectiveness of MisVis.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "v-short-1137-pres",
                        "session_id": "short1",
                        "type": "In Person Presentation",
                        "title": "Paths through Spatial Networks",
                        "contributors": [
                            "Alex Godwin"
                        ],
                        "authors": [
                            "Alex Godwin"
                        ],
                        "abstract": "Spatial networks present unique challenges to understanding topological structure. Each node occupies a location in physical space (e.g., longitude and latitude) and each link may either indicate a fixed and well-described path (e.g., along streets) or a logical connection only. Placing these elements in a map maintains these physical relationships while making it more difficult to identify topological features of interest such as clusters, cliques, and paths. While some systems provide coordinated representations of a spatial network, it is almost universally assumed that the network itself is the sole mechanism for movement across space. In this paper, I present a novel approach for exploring spatial networks by orienting them along a point or path in physical space that provides the guide for parameters in a force-directed layout. By specifying a path across topography, networks can be spatially filtered independently of the topology of the network. Initial case studies indicate promising results for exploring spatial networks in transportation and energy distribution.",
                        "uid": "v-short-1137",
                        "file_name": "v-short-1137_Godwin_Presentation.mp4",
                        "time_stamp": "2022-10-19T19:45:00Z",
                        "time_start": "2022-10-19T19:45:00Z",
                        "time_end": "2022-10-19T19:52:00Z",
                        "paper_type": "short",
                        "keywords": [
                            "Human-centered computing\u2014Visualization\u2014Visualization techniques\u2014Graph Drawing; Human-centered computing\u2014Visualization\u2014Visualization application domains\u2014Geographic visualization"
                        ],
                        "has_image": "1",
                        "has_video": "391",
                        "paper_award": "",
                        "image_caption": "The power network around Washington, D.C. Using a path to query the network and create parameters for a new force-direct layout, the resulting image reveals two major energy components around the DC area: one to the Northeast and one to the Southwest.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "v-short-1137-qa",
                        "session_id": "short1",
                        "type": "In Person Q+A",
                        "title": "Paths through Spatial Networks (Q+A)",
                        "contributors": [
                            "Alex Godwin"
                        ],
                        "authors": [],
                        "abstract": "Spatial networks present unique challenges to understanding topological structure. Each node occupies a location in physical space (e.g., longitude and latitude) and each link may either indicate a fixed and well-described path (e.g., along streets) or a logical connection only. Placing these elements in a map maintains these physical relationships while making it more difficult to identify topological features of interest such as clusters, cliques, and paths. While some systems provide coordinated representations of a spatial network, it is almost universally assumed that the network itself is the sole mechanism for movement across space. In this paper, I present a novel approach for exploring spatial networks by orienting them along a point or path in physical space that provides the guide for parameters in a force-directed layout. By specifying a path across topography, networks can be spatially filtered independently of the topology of the network. Initial case studies indicate promising results for exploring spatial networks in transportation and energy distribution.",
                        "uid": "v-short-1137",
                        "file_name": "",
                        "time_stamp": "2022-10-19T19:52:00Z",
                        "time_start": "2022-10-19T19:52:00Z",
                        "time_end": "2022-10-19T19:54:00Z",
                        "paper_type": "short",
                        "keywords": [
                            "Human-centered computing\u2014Visualization\u2014Visualization techniques\u2014Graph Drawing; Human-centered computing\u2014Visualization\u2014Visualization application domains\u2014Geographic visualization"
                        ],
                        "has_image": "1",
                        "has_video": "391",
                        "paper_award": "",
                        "image_caption": "The power network around Washington, D.C. Using a path to query the network and create parameters for a new force-direct layout, the resulting image reveals two major energy components around the DC area: one to the Northeast and one to the Southwest.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "v-short-1070-pres",
                        "session_id": "short1",
                        "type": "Virtual Presentation (pre-recorded)",
                        "title": "LineCap: Line Charts for Data Visualization Captioning Models",
                        "contributors": [
                            "Anita Mahinpei"
                        ],
                        "authors": [
                            "Anita Mahinpei",
                            "Zona Kostic",
                            "Chris Tanner"
                        ],
                        "abstract": "Data visualization captions help readers understand the purpose of a visualization and are crucial for individuals with visual impairments. The prevalence of poor figure captions and the successful application of deep learning approaches to image captioning motivate the use of similar techniques for automated figure captioning. However, research in this field has been stunted by the lack of suitable datasets. We introduce LineCap, a novel figure captioning dataset of 3,528 figures, and we provide insights from curating this dataset and using end-to-end deep learning models for automated figure captioning.",
                        "uid": "v-short-1070",
                        "file_name": "v-short-1070_Mahinpei_Presentation.mp4",
                        "time_stamp": "2022-10-19T19:54:00Z",
                        "time_start": "2022-10-19T19:54:00Z",
                        "time_end": "2022-10-19T20:01:55Z",
                        "paper_type": "short",
                        "keywords": [
                            "figure captioning, line charts, deep learning dataset"
                        ],
                        "has_image": "1",
                        "has_video": "475",
                        "paper_award": "",
                        "image_caption": "Diagram showing a figure captioning system. The system is made of two boxes titled Line Count Prediction Model and Caption Generation Model. Arrows indicate that both boxes are taking a single-lined chart as input. The output of the Line Count Prediction Model is a number. This output is passed to the Caption Generation Model as input. The output of the Caption Generation Model is \"Ylabel decreases at a decreasing rate\".",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "v-short-1070-qa",
                        "session_id": "short1",
                        "type": "Virtual Q+A",
                        "title": "LineCap: Line Charts for Data Visualization Captioning Models (Q+A)",
                        "contributors": [
                            "Anita Mahinpei"
                        ],
                        "authors": [],
                        "abstract": "Data visualization captions help readers understand the purpose of a visualization and are crucial for individuals with visual impairments. The prevalence of poor figure captions and the successful application of deep learning approaches to image captioning motivate the use of similar techniques for automated figure captioning. However, research in this field has been stunted by the lack of suitable datasets. We introduce LineCap, a novel figure captioning dataset of 3,528 figures, and we provide insights from curating this dataset and using end-to-end deep learning models for automated figure captioning.",
                        "uid": "v-short-1070",
                        "file_name": "",
                        "time_stamp": "2022-10-19T20:01:55Z",
                        "time_start": "2022-10-19T20:01:55Z",
                        "time_end": "2022-10-19T20:03:00Z",
                        "paper_type": "short",
                        "keywords": [
                            "figure captioning, line charts, deep learning dataset"
                        ],
                        "has_image": "1",
                        "has_video": "475",
                        "paper_award": "",
                        "image_caption": "Diagram showing a figure captioning system. The system is made of two boxes titled Line Count Prediction Model and Caption Generation Model. Arrows indicate that both boxes are taking a single-lined chart as input. The output of the Line Count Prediction Model is a number. This output is passed to the Caption Generation Model as input. The output of the Caption Generation Model is \"Ylabel decreases at a decreasing rate\".",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "v-short-1048-pres",
                        "session_id": "short1",
                        "type": "In Person Presentation",
                        "title": "Intentable: A Mixed-Initiative System for Intent-Based Chart Captioning",
                        "contributors": [
                            "Jiwon Choi"
                        ],
                        "authors": [
                            "Jiwon Choi",
                            "Jaemin Jo"
                        ],
                        "abstract": "We present Intentable, a mixed-initiative caption authoring system that allows the author to steer an automatic caption generation process to reflect their intent, e.g., the finding that the author gained from visualization and thus wants to write a caption for. We first derive a grammar for specifying the intent, i.e., a caption recipe, and build a neural network that generates caption sentences given a recipe. Our quantitative evaluation revealed that our intent-based generation system not only allows the author to engage in the generation process but also produces more fluent captions than the previous end-to-end approaches without user intervention. Finally, we demonstrate the versatility of our system, such as context adaptation, unit conversion, and sentence reordering.",
                        "uid": "v-short-1048",
                        "file_name": "v-short-1048_Choi_Presentation.mp4",
                        "time_stamp": "2022-10-19T20:03:00Z",
                        "time_start": "2022-10-19T20:03:00Z",
                        "time_end": "2022-10-19T20:10:00Z",
                        "paper_type": "short",
                        "keywords": [
                            "Human-centered computing\u2014Visualization\u2014Visualization systems and tools; Human-centered computing\u2014Human-computer interaction (HCI)\u2014Interactive systems and tools"
                        ],
                        "has_image": "1",
                        "has_video": "457",
                        "paper_award": "",
                        "image_caption": "Overview of mixed-initative interaction with author and Intentable system.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "v-short-1048-qa",
                        "session_id": "short1",
                        "type": "In Person Q+A",
                        "title": "Intentable: A Mixed-Initiative System for Intent-Based Chart Captioning (Q+A)",
                        "contributors": [
                            "Jiwon Choi"
                        ],
                        "authors": [],
                        "abstract": "We present Intentable, a mixed-initiative caption authoring system that allows the author to steer an automatic caption generation process to reflect their intent, e.g., the finding that the author gained from visualization and thus wants to write a caption for. We first derive a grammar for specifying the intent, i.e., a caption recipe, and build a neural network that generates caption sentences given a recipe. Our quantitative evaluation revealed that our intent-based generation system not only allows the author to engage in the generation process but also produces more fluent captions than the previous end-to-end approaches without user intervention. Finally, we demonstrate the versatility of our system, such as context adaptation, unit conversion, and sentence reordering.",
                        "uid": "v-short-1048",
                        "file_name": "",
                        "time_stamp": "2022-10-19T20:10:00Z",
                        "time_start": "2022-10-19T20:10:00Z",
                        "time_end": "2022-10-19T20:12:00Z",
                        "paper_type": "short",
                        "keywords": [
                            "Human-centered computing\u2014Visualization\u2014Visualization systems and tools; Human-centered computing\u2014Human-computer interaction (HCI)\u2014Interactive systems and tools"
                        ],
                        "has_image": "1",
                        "has_video": "457",
                        "paper_award": "",
                        "image_caption": "Overview of mixed-initative interaction with author and Intentable system.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "",
                        "ff_id": ""
                    }
                ]
            },
            {
                "title": "Visual Analytics, Decision Support, and Machine Learning",
                "session_id": "short2",
                "event_prefix": "v-short",
                "track": "ok4",
                "livestream_id": "ok4-wed",
                "session_image": "short2.png",
                "chair": [
                    "Matthew Berger"
                ],
                "organizers": [],
                "time_start": "2022-10-19T20:45:00Z",
                "time_end": "2022-10-19T22:00:00Z",
                "discord_category": "",
                "discord_channel": "oklahoma-st-4",
                "discord_channel_id": "1024600674924765264",
                "discord_link": "https://discord.com/channels/978320435554947082/1024600674924765264",
                "slido_link": "https://app.sli.do/event/i28UETZaiRWr1XwX6eKjcf?section=51658b34-0f5b-40c9-a335-1fd1468fad8d",
                "youtube_url": "https://youtu.be/BKQz_UXnWMA",
                "youtube_id": "BKQz_UXnWMA",
                "zoom_meeting": "",
                "zoom_password": "",
                "zoom_link": "",
                "ff_link": "https://youtu.be/qeScy1HcHHk",
                "ff_playlist": "",
                "time_slots": [
                    {
                        "slot_id": "short2-opening",
                        "session_id": "short2",
                        "type": "In Person Other",
                        "title": "Session Opening",
                        "contributors": [
                            "Matthew Berger"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "",
                        "file_name": "",
                        "time_stamp": "2022-10-19T20:45:00Z",
                        "time_start": "2022-10-19T20:45:00Z",
                        "time_end": "2022-10-19T20:45:00Z",
                        "paper_type": "",
                        "keywords": [],
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "v-short-1004-pres",
                        "session_id": "short2",
                        "type": "Virtual Presentation (pre-recorded)",
                        "title": "Visual Auditor: Interactive Visualization for Detection and Summarization of Model Biases",
                        "contributors": [
                            "David Munechika"
                        ],
                        "authors": [
                            "David Munechika",
                            "Zijie J. Wang",
                            "Jack Reidy",
                            "Josh Rubin",
                            "Krishna Gade",
                            "Krishnaram Kenthapadi",
                            "Duen Horng Chau"
                        ],
                        "abstract": "As machine learning (ML) systems become increasingly widespread, it is necessary to audit these systems for biases prior to their deployment. Recent research has developed algorithms for effectively identifying intersectional bias in the form of interpretable, underperforming subsets (or slices) of the data. However, these solutions and their insights are limited without a tool for visually understanding and interacting with the results of these algorithms. We propose Visual Auditor, an interactive visualization tool for auditing and summarizing model biases. Visual Auditor assists model validation by providing an interpretable overview of intersectional bias (bias that is present when examining populations defined by multiple features), details about relationships between problematic data slices, and a comparison between underperforming and overperforming data slices in a model. Our open-source tool runs directly in both computational notebooks and web browsers, making model auditing accessible and easily integrated into current ML development workflows. An observational user study in collaboration with domain experts at Fiddler AI highlights that our tool can help ML practitioners identify and understand model biases.",
                        "uid": "v-short-1004",
                        "file_name": "v-short-1004_Munechika_Presentation.mp4",
                        "time_stamp": "2022-10-19T20:45:00Z",
                        "time_start": "2022-10-19T20:45:00Z",
                        "time_end": "2022-10-19T20:52:15Z",
                        "paper_type": "short",
                        "keywords": [
                            "Machine Learning, Statistics, Modelling, and Simulation Applications"
                        ],
                        "has_image": "1",
                        "has_video": "435",
                        "paper_award": "",
                        "image_caption": "Visual Auditor provides an overview of underperforming data slices to show where intersectional biases exist. Here currently displays the Force Layout which shows underperforming data slices as nodes on a grid. The location of each node is determined by the features that define the data slice. Users can view clusters of similar data slices to better understand where intersectional bias might exist in their model. The sidebar contains options for filtering the data and modifying the visualization. Visual Auditor is an open-source tool that easily integrates within existing data science workflows and can be accessed directly within computational notebooks.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "https://youtu.be/EwaCATO2QQk",
                        "ff_id": "EwaCATO2QQk"
                    },
                    {
                        "slot_id": "v-short-1004-qa",
                        "session_id": "short2",
                        "type": "Virtual Q+A",
                        "title": "Visual Auditor: Interactive Visualization for Detection and Summarization of Model Biases (Q+A)",
                        "contributors": [
                            "David Munechika"
                        ],
                        "authors": [],
                        "abstract": "As machine learning (ML) systems become increasingly widespread, it is necessary to audit these systems for biases prior to their deployment. Recent research has developed algorithms for effectively identifying intersectional bias in the form of interpretable, underperforming subsets (or slices) of the data. However, these solutions and their insights are limited without a tool for visually understanding and interacting with the results of these algorithms. We propose Visual Auditor, an interactive visualization tool for auditing and summarizing model biases. Visual Auditor assists model validation by providing an interpretable overview of intersectional bias (bias that is present when examining populations defined by multiple features), details about relationships between problematic data slices, and a comparison between underperforming and overperforming data slices in a model. Our open-source tool runs directly in both computational notebooks and web browsers, making model auditing accessible and easily integrated into current ML development workflows. An observational user study in collaboration with domain experts at Fiddler AI highlights that our tool can help ML practitioners identify and understand model biases.",
                        "uid": "v-short-1004",
                        "file_name": "",
                        "time_stamp": "2022-10-19T20:52:15Z",
                        "time_start": "2022-10-19T20:52:15Z",
                        "time_end": "2022-10-19T20:54:00Z",
                        "paper_type": "short",
                        "keywords": [
                            "Machine Learning, Statistics, Modelling, and Simulation Applications"
                        ],
                        "has_image": "1",
                        "has_video": "435",
                        "paper_award": "",
                        "image_caption": "Visual Auditor provides an overview of underperforming data slices to show where intersectional biases exist. Here currently displays the Force Layout which shows underperforming data slices as nodes on a grid. The location of each node is determined by the features that define the data slice. Users can view clusters of similar data slices to better understand where intersectional bias might exist in their model. The sidebar contains options for filtering the data and modifying the visualization. Visual Auditor is an open-source tool that easily integrates within existing data science workflows and can be accessed directly within computational notebooks.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "https://youtu.be/EwaCATO2QQk",
                        "ff_id": "EwaCATO2QQk"
                    },
                    {
                        "slot_id": "v-short-1076-pres",
                        "session_id": "short2",
                        "type": "In Person Presentation",
                        "title": "RMExplorer: A Visual Analytics Approach to Explore the Performance and the Fairness of Disease Risk Models on Population Subgroups",
                        "contributors": [
                            "Bum Chul Kwon"
                        ],
                        "authors": [
                            "Bum Chul Kwon",
                            "Uri Kartoun",
                            "Shaan Khurshid",
                            "Mikhail Yurochkin",
                            "Subha Maity",
                            "Deanna G Brockman",
                            "Amit V Khera",
                            "Patrick T Ellinor",
                            "Steven A Lubitz",
                            "Kenney Ng"
                        ],
                        "abstract": "Disease risk models can identify high-risk patients and help clinicians provide more personalized care. However, risk models developed on one dataset may not generalize across diverse subpopulations of patients in different datasets and may have unexpected performance. It is challenging for clinical researchers to inspect risk models across different subgroups without any tools. Therefore, we developed an interactive visualization system called RMExplorer (Risk Model Explorer) to enable interactive risk model assessment. Specifically, the system allows users to define subgroups of patients by selecting clinical, demographic, or other characteristics, to explore the performance and fairness of risk models on the subgroups, and to understand the feature contributions to risk scores. To demonstrate the usefulness of the tool, we conduct a case study, where we use RMExplorer to explore three atrial fibrillation risk models by applying them to the UK Biobank dataset of 445,329 individuals. RMExplorer can help researchers to evaluate the performance and biases of risk models on subpopulations of interest in their data.",
                        "uid": "v-short-1076",
                        "file_name": "v-short-1076_Kwon_Presentation.mp4",
                        "time_stamp": "2022-10-19T20:54:00Z",
                        "time_start": "2022-10-19T20:54:00Z",
                        "time_end": "2022-10-19T21:01:00Z",
                        "paper_type": "short",
                        "keywords": [
                            "visual analytics, health informatics, fairness, subgroup analysis, explainability, interpretability, electronic health records"
                        ],
                        "has_image": "1",
                        "has_video": "465",
                        "paper_award": "",
                        "image_caption": "RMExplorer (i.e., Risk Model Explorer) helps users to investigate the performance and the fairness of disease risk models through interactive visualizations.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "https://youtu.be/sSqX0AW9xNc",
                        "ff_id": "sSqX0AW9xNc"
                    },
                    {
                        "slot_id": "v-short-1076-qa",
                        "session_id": "short2",
                        "type": "In Person Q+A",
                        "title": "RMExplorer: A Visual Analytics Approach to Explore the Performance and the Fairness of Disease Risk Models on Population Subgroups (Q+A)",
                        "contributors": [
                            "Bum Chul Kwon"
                        ],
                        "authors": [],
                        "abstract": "Disease risk models can identify high-risk patients and help clinicians provide more personalized care. However, risk models developed on one dataset may not generalize across diverse subpopulations of patients in different datasets and may have unexpected performance. It is challenging for clinical researchers to inspect risk models across different subgroups without any tools. Therefore, we developed an interactive visualization system called RMExplorer (Risk Model Explorer) to enable interactive risk model assessment. Specifically, the system allows users to define subgroups of patients by selecting clinical, demographic, or other characteristics, to explore the performance and fairness of risk models on the subgroups, and to understand the feature contributions to risk scores. To demonstrate the usefulness of the tool, we conduct a case study, where we use RMExplorer to explore three atrial fibrillation risk models by applying them to the UK Biobank dataset of 445,329 individuals. RMExplorer can help researchers to evaluate the performance and biases of risk models on subpopulations of interest in their data.",
                        "uid": "v-short-1076",
                        "file_name": "",
                        "time_stamp": "2022-10-19T21:01:00Z",
                        "time_start": "2022-10-19T21:01:00Z",
                        "time_end": "2022-10-19T21:03:00Z",
                        "paper_type": "short",
                        "keywords": [
                            "visual analytics, health informatics, fairness, subgroup analysis, explainability, interpretability, electronic health records"
                        ],
                        "has_image": "1",
                        "has_video": "465",
                        "paper_award": "",
                        "image_caption": "RMExplorer (i.e., Risk Model Explorer) helps users to investigate the performance and the fairness of disease risk models through interactive visualizations.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "https://youtu.be/sSqX0AW9xNc",
                        "ff_id": "sSqX0AW9xNc"
                    },
                    {
                        "slot_id": "v-short-1105-pres",
                        "session_id": "short2",
                        "type": "Virtual Presentation (pre-recorded)",
                        "title": "Visualizing Rule-based Classifiers for Clinical Risk Prognosis",
                        "contributors": [
                            "Dario Antweiler"
                        ],
                        "authors": [
                            "Dario Antweiler",
                            "Georg Fuchs"
                        ],
                        "abstract": "Deteriorating conditions in hospital patients are a major factor in clinical patient mortality. Currently, timely detection is based on clinical experience, expertise, and attention. However, healthcare trends towards larger patient cohorts, more data, and the desire for better and more personalized care are pushing the existing, simple scoring systems to their limits. Data-driven approaches can extract decision rules from available medical coding data, which offer good interpretability and thus are key for successful adoption in practice. Before deployment, models need to be scrutinized by domain experts to identify errors and check them against existing medical knowledge. We propose a visual analytics system to support healthcare professionals in inspecting and enhancing rule-based classifier through identification of similarities and contradictions, as well as modification of rules. This work was developed iteratively in close collaboration with medical professionals. We discuss how our tool supports the inspection and assessment of rule-based classifiers in the clinical coding domain and propose possible extensions.",
                        "uid": "v-short-1105",
                        "file_name": "v-short-1105_Antweiler_Presentation.mp4",
                        "time_stamp": "2022-10-19T21:03:00Z",
                        "time_start": "2022-10-19T21:03:00Z",
                        "time_end": "2022-10-19T21:07:35Z",
                        "paper_type": "short",
                        "keywords": [
                            "Information systems applications, Decision support systems, Data analytics, Human computer interaction (HCI), HCI design and evaluation methods, User studies, Applied computing, Life and medical sciences, Health care information systems"
                        ],
                        "has_image": "1",
                        "has_video": "275",
                        "paper_award": "",
                        "image_caption": "Overview of the our proposed Visual Analytics system with the goal of analyzing rule-based classifiers for clinical risk prognosis as a first prototype. It consists of the main rule list view containing rule attributes as well as quality metrics, a hierarchical tree view containing ICD and OPS codes and a feature interaction view showcasing how code combination are distributed across a user-selected subset of rules. The work was developed in close collaboration with hospital doctors and the dataset used contains patient records from hospitals in Germany.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "https://youtu.be/veh7gsTiLkI",
                        "ff_id": "veh7gsTiLkI"
                    },
                    {
                        "slot_id": "v-short-1105-qa",
                        "session_id": "short2",
                        "type": "Virtual Q+A",
                        "title": "Visualizing Rule-based Classifiers for Clinical Risk Prognosis (Q+A)",
                        "contributors": [
                            "Dario Antweiler"
                        ],
                        "authors": [],
                        "abstract": "Deteriorating conditions in hospital patients are a major factor in clinical patient mortality. Currently, timely detection is based on clinical experience, expertise, and attention. However, healthcare trends towards larger patient cohorts, more data, and the desire for better and more personalized care are pushing the existing, simple scoring systems to their limits. Data-driven approaches can extract decision rules from available medical coding data, which offer good interpretability and thus are key for successful adoption in practice. Before deployment, models need to be scrutinized by domain experts to identify errors and check them against existing medical knowledge. We propose a visual analytics system to support healthcare professionals in inspecting and enhancing rule-based classifier through identification of similarities and contradictions, as well as modification of rules. This work was developed iteratively in close collaboration with medical professionals. We discuss how our tool supports the inspection and assessment of rule-based classifiers in the clinical coding domain and propose possible extensions.",
                        "uid": "v-short-1105",
                        "file_name": "",
                        "time_stamp": "2022-10-19T21:07:35Z",
                        "time_start": "2022-10-19T21:07:35Z",
                        "time_end": "2022-10-19T21:12:00Z",
                        "paper_type": "short",
                        "keywords": [
                            "Information systems applications, Decision support systems, Data analytics, Human computer interaction (HCI), HCI design and evaluation methods, User studies, Applied computing, Life and medical sciences, Health care information systems"
                        ],
                        "has_image": "1",
                        "has_video": "275",
                        "paper_award": "",
                        "image_caption": "Overview of the our proposed Visual Analytics system with the goal of analyzing rule-based classifiers for clinical risk prognosis as a first prototype. It consists of the main rule list view containing rule attributes as well as quality metrics, a hierarchical tree view containing ICD and OPS codes and a feature interaction view showcasing how code combination are distributed across a user-selected subset of rules. The work was developed in close collaboration with hospital doctors and the dataset used contains patient records from hospitals in Germany.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "https://youtu.be/veh7gsTiLkI",
                        "ff_id": "veh7gsTiLkI"
                    },
                    {
                        "slot_id": "v-short-1006-pres",
                        "session_id": "short2",
                        "type": "In Person Presentation",
                        "title": "TimberTrek: Exploring and Curating Sparse Decision Trees with Interactive Visualization",
                        "contributors": [
                            "Zijie J. Wang"
                        ],
                        "authors": [
                            "Zijie J. Wang",
                            "Chudi Zhong",
                            "Rui Xin",
                            "Takuya Takagi",
                            "Zhi Chen",
                            "Duen Horng Chau",
                            "Cynthia Rudin",
                            "Margo Seltzer"
                        ],
                        "abstract": "Given thousands of equally accurate machine learning (ML) models, how can users choose among them? A recent ML technique enables domain experts and data scientists to generate a complete Rashomon set for sparse decision trees\u2014a huge set of almost-optimal interpretable ML models. To help ML practitioners identify models with desirable properties from this Rashomon set, we develop TimberTrek, the first interactive visualization system that summarizes thousands of sparse decision trees at scale. Two usage scenarios highlight how TimberTrek can empower users to easily explore, compare, and curate models that align with their domain knowledge and values. Our open-source tool runs directly in users' computational notebooks and web browsers, lowering the barrier to creating more responsible ML models. TimberTrek is available at the following public demo link: https://poloclub.github.io/timbertrek.",
                        "uid": "v-short-1006",
                        "file_name": "v-short-1006_Wang_Presentation.mp4",
                        "time_stamp": "2022-10-19T21:12:00Z",
                        "time_start": "2022-10-19T21:12:00Z",
                        "time_end": "2022-10-19T21:19:00Z",
                        "paper_type": "short",
                        "keywords": [
                            "Machine Learning, Interpretability, Rashomon Set, Decision Trees"
                        ],
                        "has_image": "1",
                        "has_video": "480",
                        "paper_award": "",
                        "image_caption": "Given thousands of equally accurate machine learning (ML) models, how can users choose among them? A recent ML technique enables domain experts and data scientists to generate a complete Rashomon set for sparse decision trees\u2014a huge set of almost-optimal interpretable ML models. To help ML practitioners identify models with desirable properties from this Rashomon set, we develop TimberTrek, the first interactive visualization system that summarizes thousands of sparse decision trees at scale. Two usage scenarios highlight how TimberTrek can empower users to easily explore, com- pare, and curate models that align with their domain knowledge and values. Our open-source tool runs directly in users\u2019 computational notebooks and web browsers, lowering the barrier to creating more responsible ML models. TimberTrek is available at the following public demo link: https://poloclub.github.io/timbertrek.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "https://youtu.be/djtbBu7cFHg",
                        "ff_id": "djtbBu7cFHg"
                    },
                    {
                        "slot_id": "v-short-1006-qa",
                        "session_id": "short2",
                        "type": "In Person Q+A",
                        "title": "TimberTrek: Exploring and Curating Sparse Decision Trees with Interactive Visualization (Q+A)",
                        "contributors": [
                            "Zijie J. Wang"
                        ],
                        "authors": [],
                        "abstract": "Given thousands of equally accurate machine learning (ML) models, how can users choose among them? A recent ML technique enables domain experts and data scientists to generate a complete Rashomon set for sparse decision trees\u2014a huge set of almost-optimal interpretable ML models. To help ML practitioners identify models with desirable properties from this Rashomon set, we develop TimberTrek, the first interactive visualization system that summarizes thousands of sparse decision trees at scale. Two usage scenarios highlight how TimberTrek can empower users to easily explore, compare, and curate models that align with their domain knowledge and values. Our open-source tool runs directly in users' computational notebooks and web browsers, lowering the barrier to creating more responsible ML models. TimberTrek is available at the following public demo link: https://poloclub.github.io/timbertrek.",
                        "uid": "v-short-1006",
                        "file_name": "",
                        "time_stamp": "2022-10-19T21:19:00Z",
                        "time_start": "2022-10-19T21:19:00Z",
                        "time_end": "2022-10-19T21:21:00Z",
                        "paper_type": "short",
                        "keywords": [
                            "Machine Learning, Interpretability, Rashomon Set, Decision Trees"
                        ],
                        "has_image": "1",
                        "has_video": "480",
                        "paper_award": "",
                        "image_caption": "Given thousands of equally accurate machine learning (ML) models, how can users choose among them? A recent ML technique enables domain experts and data scientists to generate a complete Rashomon set for sparse decision trees\u2014a huge set of almost-optimal interpretable ML models. To help ML practitioners identify models with desirable properties from this Rashomon set, we develop TimberTrek, the first interactive visualization system that summarizes thousands of sparse decision trees at scale. Two usage scenarios highlight how TimberTrek can empower users to easily explore, com- pare, and curate models that align with their domain knowledge and values. Our open-source tool runs directly in users\u2019 computational notebooks and web browsers, lowering the barrier to creating more responsible ML models. TimberTrek is available at the following public demo link: https://poloclub.github.io/timbertrek.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "https://youtu.be/djtbBu7cFHg",
                        "ff_id": "djtbBu7cFHg"
                    },
                    {
                        "slot_id": "v-short-1083-pres",
                        "session_id": "short2",
                        "type": "In Person Presentation",
                        "title": "FairFuse: Interactive Visual Support for Fair Consensus Ranking",
                        "contributors": [
                            "Hilson Shrestha"
                        ],
                        "authors": [
                            "Hilson Shrestha",
                            "Kathleen Cachel",
                            "Mallak Alkhathlan",
                            "Elke A Rundensteiner",
                            "Lane Harrison"
                        ],
                        "abstract": "Fair consensus building combines the preferences of multiple rankers into a single consensus ranking, while ensuring any ranked candidate group defined by a protected attribute (such as race or gender) is not disadvantaged compared to other groups. Manually generating a fair consensus ranking is time-consuming and impractical\u2014 even for a fairly small number of candidates. While algorithmic approaches for auditing and generating fair consensus rankings have been developed recently, these have not been operationalized in interactive systems. To bridge this gap, we introduce FairFuse, a visualization-enabled tool for generating, analyzing, and auditing fair consensus rankings. In developing FairFuse, we construct a data model which includes several base rankings entered by rankers, augmented with measures of group fairness, algorithms for generating consensus rankings with varying degrees of fairness, and other fairness and rank-related capabilities. We design novel visualizations that encode these measures in a parallel-coordinates style rank visualization, with interactive capabilities for generating and exploring fair consensus rankings. We provide case studies in which FairFuse supports a decision-maker in ranking scenarios in which fairness is important. Finally, we discuss emerging challenges for future efforts supporting fairness-oriented rank analysis; including handling intersectionality, defined by multiple protected attributes, and the need for user studies targeting peoples\u2019 perceptions and use of fairness oriented visualization systems. Code and demo videos available at https://osf.io/hd639/.",
                        "uid": "v-short-1083",
                        "file_name": "v-short-1083_Shrestha_Presentation.mp4",
                        "time_stamp": "2022-10-19T21:21:00Z",
                        "time_start": "2022-10-19T21:21:00Z",
                        "time_end": "2022-10-19T21:28:00Z",
                        "paper_type": "short",
                        "keywords": [
                            "Fairness, consensus, rank aggregation, visualization"
                        ],
                        "has_image": "1",
                        "has_video": "446",
                        "paper_award": "",
                        "image_caption": "FairFuse provides multiple views supporting fairness-oriented ranking workflows: A) Consensus Generation View, B) Rank Similarity View, C) Attribute / Protected Attribute Legends, D) Group Fairness View, E) Ranking Exploration View. (Right) Illustrating a fairness-oriented ranking workflow enabled by FairFuse.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "https://youtu.be/m955ronmmS0",
                        "ff_id": "m955ronmmS0"
                    },
                    {
                        "slot_id": "v-short-1083-qa",
                        "session_id": "short2",
                        "type": "In Person Q+A",
                        "title": "FairFuse: Interactive Visual Support for Fair Consensus Ranking (Q+A)",
                        "contributors": [
                            "Hilson Shrestha"
                        ],
                        "authors": [],
                        "abstract": "Fair consensus building combines the preferences of multiple rankers into a single consensus ranking, while ensuring any ranked candidate group defined by a protected attribute (such as race or gender) is not disadvantaged compared to other groups. Manually generating a fair consensus ranking is time-consuming and impractical\u2014 even for a fairly small number of candidates. While algorithmic approaches for auditing and generating fair consensus rankings have been developed recently, these have not been operationalized in interactive systems. To bridge this gap, we introduce FairFuse, a visualization-enabled tool for generating, analyzing, and auditing fair consensus rankings. In developing FairFuse, we construct a data model which includes several base rankings entered by rankers, augmented with measures of group fairness, algorithms for generating consensus rankings with varying degrees of fairness, and other fairness and rank-related capabilities. We design novel visualizations that encode these measures in a parallel-coordinates style rank visualization, with interactive capabilities for generating and exploring fair consensus rankings. We provide case studies in which FairFuse supports a decision-maker in ranking scenarios in which fairness is important. Finally, we discuss emerging challenges for future efforts supporting fairness-oriented rank analysis; including handling intersectionality, defined by multiple protected attributes, and the need for user studies targeting peoples\u2019 perceptions and use of fairness oriented visualization systems. Code and demo videos available at https://osf.io/hd639/.",
                        "uid": "v-short-1083",
                        "file_name": "",
                        "time_stamp": "2022-10-19T21:28:00Z",
                        "time_start": "2022-10-19T21:28:00Z",
                        "time_end": "2022-10-19T21:30:00Z",
                        "paper_type": "short",
                        "keywords": [
                            "Fairness, consensus, rank aggregation, visualization"
                        ],
                        "has_image": "1",
                        "has_video": "446",
                        "paper_award": "",
                        "image_caption": "FairFuse provides multiple views supporting fairness-oriented ranking workflows: A) Consensus Generation View, B) Rank Similarity View, C) Attribute / Protected Attribute Legends, D) Group Fairness View, E) Ranking Exploration View. (Right) Illustrating a fairness-oriented ranking workflow enabled by FairFuse.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "https://youtu.be/m955ronmmS0",
                        "ff_id": "m955ronmmS0"
                    },
                    {
                        "slot_id": "v-short-1041-pres",
                        "session_id": "short2",
                        "type": "In Person Presentation",
                        "title": "Guided Data Discovery in Interactive Visualizations via Active Search",
                        "contributors": [
                            "Shayan Monadjemi"
                        ],
                        "authors": [
                            "Shayan Monadjemi",
                            "Sunwoo Ha",
                            "Quan Nguyen",
                            "Henry Chai",
                            "Roman Garnett",
                            "Alvitta Ottley"
                        ],
                        "abstract": "Recent advances in visual analytics have enabled us to learn from user interactions and uncover analytic goals. These innovations set the foundation for actively guiding users during data exploration. Providing such guidance will become more critical as datasets grow in size and complexity, precluding exhaustive investigation. Meanwhile, the machine learning community also struggles with datasets growing in size and complexity, precluding exhaustive labeling. Active learning is a broad family of algorithms developed for actively guiding models during training. We will consider the intersection of these analogous research thrusts. First, we discuss the nuances of matching the choice of an active learning algorithm to the task at hand. This is critical for performance, a fact we demonstrate in a simulation study. We then present results of a user study for the particular task of data discovery guided by an active learning algorithm specifically designed for this task.",
                        "uid": "v-short-1041",
                        "file_name": "v-short-1041_Monadjemi_Presentation.mp4",
                        "time_stamp": "2022-10-19T21:30:00Z",
                        "time_start": "2022-10-19T21:30:00Z",
                        "time_end": "2022-10-19T21:37:00Z",
                        "paper_type": "short",
                        "keywords": [
                            "visual analytics, empirical studies in visualization, active learning settings"
                        ],
                        "has_image": "1",
                        "has_video": "503",
                        "paper_award": "",
                        "image_caption": "Recent advances in visual analytics have enabled us to learn from user interactions and uncover analytic goals. These innovations set the foundation for actively guiding users during data exploration which become more critical as datasets grow in size and complexity. We will consider how the active search algorithm can learn from user interactions and guide them during data exploration and discovery.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "https://youtu.be/rytnlFy2GuA",
                        "ff_id": "rytnlFy2GuA"
                    },
                    {
                        "slot_id": "v-short-1041-qa",
                        "session_id": "short2",
                        "type": "In Person Q+A",
                        "title": "Guided Data Discovery in Interactive Visualizations via Active Search (Q+A)",
                        "contributors": [
                            "Shayan Monadjemi"
                        ],
                        "authors": [],
                        "abstract": "Recent advances in visual analytics have enabled us to learn from user interactions and uncover analytic goals. These innovations set the foundation for actively guiding users during data exploration. Providing such guidance will become more critical as datasets grow in size and complexity, precluding exhaustive investigation. Meanwhile, the machine learning community also struggles with datasets growing in size and complexity, precluding exhaustive labeling. Active learning is a broad family of algorithms developed for actively guiding models during training. We will consider the intersection of these analogous research thrusts. First, we discuss the nuances of matching the choice of an active learning algorithm to the task at hand. This is critical for performance, a fact we demonstrate in a simulation study. We then present results of a user study for the particular task of data discovery guided by an active learning algorithm specifically designed for this task.",
                        "uid": "v-short-1041",
                        "file_name": "",
                        "time_stamp": "2022-10-19T21:37:00Z",
                        "time_start": "2022-10-19T21:37:00Z",
                        "time_end": "2022-10-19T21:39:00Z",
                        "paper_type": "short",
                        "keywords": [
                            "visual analytics, empirical studies in visualization, active learning settings"
                        ],
                        "has_image": "1",
                        "has_video": "503",
                        "paper_award": "",
                        "image_caption": "Recent advances in visual analytics have enabled us to learn from user interactions and uncover analytic goals. These innovations set the foundation for actively guiding users during data exploration which become more critical as datasets grow in size and complexity. We will consider how the active search algorithm can learn from user interactions and guide them during data exploration and discovery.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "https://youtu.be/rytnlFy2GuA",
                        "ff_id": "rytnlFy2GuA"
                    },
                    {
                        "slot_id": "v-short-1028-pres",
                        "session_id": "short2",
                        "type": "In Person Presentation",
                        "title": "Parametric Dimension Reduction by Preserving Local Structure",
                        "contributors": [
                            "Yu-Shuen Wang",
                            "Yun Hsuan Lien"
                        ],
                        "authors": [
                            "Chien-Hsun Lai",
                            "Ming-Feng Kuo",
                            "Yun-Hsuan Lien",
                            "Kuan-An Su",
                            "Yu-Shuen Wang"
                        ],
                        "abstract": "We extend a well-known dimension reduction method, t-distributed stochastic neighbor embedding (t-SNE), from non-parametric to parametric by training neural networks. The main advantage of a parametric technique is the generalization of handling new data, which is beneficial for streaming data visualization. While previous parametric methods either require a network pre-training by the restricted Boltzmann machine or intermediate results obtained from the traditional non-parametric t-SNE, we found that recent network training skills can enable a direct optimization for the t-SNE objective function. Accordingly, our method achieves high embedding quality while enjoying generalization. Due to mini-batch network training, our parametric dimension reduction method is highly efficient. For evaluation, we compared our method to several baselines on a variety of datasets. Experiment results demonstrate the feasibility of our method. The source code is available at https://github.com/a07458666/parametric_dr.",
                        "uid": "v-short-1028",
                        "file_name": "v-short-1028_Lai_Presentation.mp4",
                        "time_stamp": "2022-10-19T21:39:00Z",
                        "time_start": "2022-10-19T21:39:00Z",
                        "time_end": "2022-10-19T21:46:00Z",
                        "paper_type": "short",
                        "keywords": [
                            "Computing methodologies\u2014Dimensionality reduction and manifold learning\u2014; Human-centered computing\u2014Visualization toolkit"
                        ],
                        "has_image": "1",
                        "has_video": "416",
                        "paper_award": "",
                        "image_caption": "We extend a well-known dimension reduction method, t-distributed stochastic neighbor embedding (t-SNE), from non-parametric to parametric by training neural networks. Our method achieves high embedding quality while enjoying generalization. In addition, our method is highly efficient, thanks to the mini-batch network training.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "https://youtu.be/l3OUYLhjNbE",
                        "ff_id": "l3OUYLhjNbE"
                    },
                    {
                        "slot_id": "v-short-1028-qa",
                        "session_id": "short2",
                        "type": "In Person Q+A",
                        "title": "Parametric Dimension Reduction by Preserving Local Structure (Q+A)",
                        "contributors": [
                            "Yu-Shuen Wang",
                            "Yun Hsuan Lien"
                        ],
                        "authors": [],
                        "abstract": "We extend a well-known dimension reduction method, t-distributed stochastic neighbor embedding (t-SNE), from non-parametric to parametric by training neural networks. The main advantage of a parametric technique is the generalization of handling new data, which is beneficial for streaming data visualization. While previous parametric methods either require a network pre-training by the restricted Boltzmann machine or intermediate results obtained from the traditional non-parametric t-SNE, we found that recent network training skills can enable a direct optimization for the t-SNE objective function. Accordingly, our method achieves high embedding quality while enjoying generalization. Due to mini-batch network training, our parametric dimension reduction method is highly efficient. For evaluation, we compared our method to several baselines on a variety of datasets. Experiment results demonstrate the feasibility of our method. The source code is available at https://github.com/a07458666/parametric_dr.",
                        "uid": "v-short-1028",
                        "file_name": "",
                        "time_stamp": "2022-10-19T21:46:00Z",
                        "time_start": "2022-10-19T21:46:00Z",
                        "time_end": "2022-10-19T21:48:00Z",
                        "paper_type": "short",
                        "keywords": [
                            "Computing methodologies\u2014Dimensionality reduction and manifold learning\u2014; Human-centered computing\u2014Visualization toolkit"
                        ],
                        "has_image": "1",
                        "has_video": "416",
                        "paper_award": "",
                        "image_caption": "We extend a well-known dimension reduction method, t-distributed stochastic neighbor embedding (t-SNE), from non-parametric to parametric by training neural networks. Our method achieves high embedding quality while enjoying generalization. In addition, our method is highly efficient, thanks to the mini-batch network training.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "https://youtu.be/l3OUYLhjNbE",
                        "ff_id": "l3OUYLhjNbE"
                    },
                    {
                        "slot_id": "v-short-1047-pres",
                        "session_id": "short2",
                        "type": "In Person Presentation",
                        "title": "Uniform Manifold Approximation with Two-phase Optimization",
                        "contributors": [
                            "Hyeon Jeon"
                        ],
                        "authors": [
                            "Hyeon Jeon",
                            "Hyung-Kwon Ko",
                            "Soohyun Lee",
                            "Jaemin Jo",
                            "Jinwook Seo"
                        ],
                        "abstract": "We introduce Uniform Manifold Approximation with Two-phase Optimization (UMATO), a dimensionality reduction (DR) technique that improves UMAP to capture the global structure of high-dimensional data more accurately. In UMATO, optimization is divided into two phases so that the resulting embeddings can depict the global structure reliably while preserving the local structure with sufficient accuracy. In the first phase, hub points are identified and projected to construct a skeletal layout for the global structure. In the second phase, the remaining points are added to the embedding preserving the regional characteristics of local areas. Through quantitative experiments, we found that UMATO (1) outperformed widely used DR techniques in preserving the global structure while (2) producing competitive accuracy in representing the local structure. We also verified that UMATO is preferable in terms of robustness over diverse initialization methods, numbers of epochs, and subsampling techniques.",
                        "uid": "v-short-1047",
                        "file_name": "v-short-1047_Jeon_Presentation.mp4",
                        "time_stamp": "2022-10-19T21:48:00Z",
                        "time_start": "2022-10-19T21:48:00Z",
                        "time_end": "2022-10-19T21:55:00Z",
                        "paper_type": "short",
                        "keywords": [
                            "Human-centered computing\u2014Visualization\u2014Visualization techniques; Computing methodologies\u2014Machine learning\u2014Machine learning algorithms"
                        ],
                        "has_image": "1",
                        "has_video": "394",
                        "paper_award": "",
                        "image_caption": "2D embeddings of UMATO and six competitors. Overall, UMATO surpassed other techniques in preserving global structure while showing comparable performance in capturing local structure.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "https://youtu.be/b3KoBux2cSU",
                        "ff_id": "b3KoBux2cSU"
                    },
                    {
                        "slot_id": "v-short-1047-qa",
                        "session_id": "short2",
                        "type": "In Person Q+A",
                        "title": "Uniform Manifold Approximation with Two-phase Optimization (Q+A)",
                        "contributors": [
                            "Hyeon Jeon"
                        ],
                        "authors": [],
                        "abstract": "We introduce Uniform Manifold Approximation with Two-phase Optimization (UMATO), a dimensionality reduction (DR) technique that improves UMAP to capture the global structure of high-dimensional data more accurately. In UMATO, optimization is divided into two phases so that the resulting embeddings can depict the global structure reliably while preserving the local structure with sufficient accuracy. In the first phase, hub points are identified and projected to construct a skeletal layout for the global structure. In the second phase, the remaining points are added to the embedding preserving the regional characteristics of local areas. Through quantitative experiments, we found that UMATO (1) outperformed widely used DR techniques in preserving the global structure while (2) producing competitive accuracy in representing the local structure. We also verified that UMATO is preferable in terms of robustness over diverse initialization methods, numbers of epochs, and subsampling techniques.",
                        "uid": "v-short-1047",
                        "file_name": "",
                        "time_stamp": "2022-10-19T21:55:00Z",
                        "time_start": "2022-10-19T21:55:00Z",
                        "time_end": "2022-10-19T21:57:00Z",
                        "paper_type": "short",
                        "keywords": [
                            "Human-centered computing\u2014Visualization\u2014Visualization techniques; Computing methodologies\u2014Machine learning\u2014Machine learning algorithms"
                        ],
                        "has_image": "1",
                        "has_video": "394",
                        "paper_award": "",
                        "image_caption": "2D embeddings of UMATO and six competitors. Overall, UMATO surpassed other techniques in preserving global structure while showing comparable performance in capturing local structure.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "https://youtu.be/b3KoBux2cSU",
                        "ff_id": "b3KoBux2cSU"
                    }
                ]
            },
            {
                "title": "Scientific Visualization, Ensembles, and Accessibility",
                "session_id": "short3",
                "event_prefix": "v-short",
                "track": "ok1",
                "livestream_id": "ok1-thu",
                "session_image": "short3.png",
                "chair": [
                    "Hanqi Guo"
                ],
                "organizers": [],
                "time_start": "2022-10-20T19:00:00Z",
                "time_end": "2022-10-20T20:15:00Z",
                "discord_category": "",
                "discord_channel": "oklahoma-st-1",
                "discord_channel_id": "1024600861340606484",
                "discord_link": "https://discord.com/channels/978320435554947082/1024600861340606484",
                "slido_link": "https://app.sli.do/event/i28UETZaiRWr1XwX6eKjcf?section=740d9835-fe47-4cb4-b260-353262a7f8dd",
                "youtube_url": "https://youtu.be/x76ysAHpetE",
                "youtube_id": "x76ysAHpetE",
                "zoom_meeting": "",
                "zoom_password": "",
                "zoom_link": "",
                "ff_link": "https://youtu.be/eytoca-Jn68",
                "ff_playlist": "",
                "time_slots": [
                    {
                        "slot_id": "short3-opening",
                        "session_id": "short3",
                        "type": "In Person Other",
                        "title": "Session Opening",
                        "contributors": [
                            "Hanqi Guo"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "",
                        "file_name": "",
                        "time_stamp": "2022-10-20T19:00:00Z",
                        "time_start": "2022-10-20T19:00:00Z",
                        "time_end": "2022-10-20T19:00:00Z",
                        "paper_type": "",
                        "keywords": [],
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "v-short-1072-pres",
                        "session_id": "short3",
                        "type": "In Person Presentation",
                        "title": "Color Coding of Large Value Ranges Applied to Meteorological Data",
                        "contributors": [
                            "Daniel Braun"
                        ],
                        "authors": [
                            "Daniel Braun",
                            "Kerstin Ebell",
                            "Vera Schemann",
                            "Laura Pelchmann",
                            "Susanne Crewell",
                            "Rita Borgo",
                            "Tatiana von Landesberger"
                        ],
                        "abstract": "This paper presents a novel color scheme designed to address the challenge of visualizing data series with large value ranges, where scale transformation provides limited support. We focus on meteorological data, where the presence of large value ranges is common. We apply our approach to meteorological scatterplots, as one of the most common plots used in this domain area. Our approach leverages the numerical representation of mantissa and exponent of the values to guide the design of novel ``nested'' color schemes, able to emphasize differences between magnitudes. Our user study evaluates the new designs, the state of the art color scales and representative color schemes used in the analysis of meteorological data: ColorCrafter, Viridis, and Rainbow. We assess accuracy, time and confidence in the context of discrimination (comparison) and interpretation (reading) tasks. Our proposed color scheme significantly outperforms the others in interpretation tasks, while showing comparable performances in discrimination tasks.",
                        "uid": "v-short-1072",
                        "file_name": "v-short-1072_Braun_Presentation.mp4",
                        "time_stamp": "2022-10-20T19:00:00Z",
                        "time_start": "2022-10-20T19:00:00Z",
                        "time_end": "2022-10-20T19:07:00Z",
                        "paper_type": "short",
                        "keywords": [
                            "Color Coding\u2014Perception\u2014Large Value Ranges\u2014User study"
                        ],
                        "has_image": "1",
                        "has_video": "364",
                        "paper_award": "",
                        "image_caption": "Order of Magnitude Colors: A new color coding approach to encode data with large value ranges applied to meteorological cloud data.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "https://youtu.be/p__ypSjHECA",
                        "ff_id": "p__ypSjHECA"
                    },
                    {
                        "slot_id": "v-short-1072-qa",
                        "session_id": "short3",
                        "type": "In Person Q+A",
                        "title": "Color Coding of Large Value Ranges Applied to Meteorological Data (Q+A)",
                        "contributors": [
                            "Daniel Braun"
                        ],
                        "authors": [],
                        "abstract": "This paper presents a novel color scheme designed to address the challenge of visualizing data series with large value ranges, where scale transformation provides limited support. We focus on meteorological data, where the presence of large value ranges is common. We apply our approach to meteorological scatterplots, as one of the most common plots used in this domain area. Our approach leverages the numerical representation of mantissa and exponent of the values to guide the design of novel ``nested'' color schemes, able to emphasize differences between magnitudes. Our user study evaluates the new designs, the state of the art color scales and representative color schemes used in the analysis of meteorological data: ColorCrafter, Viridis, and Rainbow. We assess accuracy, time and confidence in the context of discrimination (comparison) and interpretation (reading) tasks. Our proposed color scheme significantly outperforms the others in interpretation tasks, while showing comparable performances in discrimination tasks.",
                        "uid": "v-short-1072",
                        "file_name": "",
                        "time_stamp": "2022-10-20T19:07:00Z",
                        "time_start": "2022-10-20T19:07:00Z",
                        "time_end": "2022-10-20T19:09:00Z",
                        "paper_type": "short",
                        "keywords": [
                            "Color Coding\u2014Perception\u2014Large Value Ranges\u2014User study"
                        ],
                        "has_image": "1",
                        "has_video": "364",
                        "paper_award": "",
                        "image_caption": "Order of Magnitude Colors: A new color coding approach to encode data with large value ranges applied to meteorological cloud data.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "https://youtu.be/p__ypSjHECA",
                        "ff_id": "p__ypSjHECA"
                    },
                    {
                        "slot_id": "v-short-1085-pres",
                        "session_id": "short3",
                        "type": "Virtual Presentation (pre-recorded)",
                        "title": "Volume Puzzle: visual analysis of segmented volume data with multivariate attributes",
                        "contributors": [
                            "Marco Agus"
                        ],
                        "authors": [
                            "Marco Agus",
                            "Amal Aboulhassan",
                            "Khaled Ahmed Lutf Al-Thelaya",
                            "Giovanni Pintore",
                            "Enrico Gobbetti",
                            "Corrado Cali'",
                            "Jens Schneider"
                        ],
                        "abstract": "A variety of application domains, including material science, neuroscience, and connectomics, commonly use segmented volume data for explorative visual analysis. In many cases, segmented objects are characterized by multivariate attributes expressing specific geometric or physical features. Objects with similar characteristics, determined by selected attribute configurations, can create peculiar spatial patterns, whose detection and study is of fundamental importance. This task is notoriously difficult, especially when the number of attributes per segment is large. In this work, we propose an interactive framework that combines a state-of-the-art direct volume renderer for categorical volumes with techniques for the analysis of the attribute space and for the automatic creation of 2D transfer function. We show, in particular, how dimensionality reduction, kernel-density estimation, and topological techniques such as Morse analysis combined with scatter and density plots allow the efficient design of two-dimensional color maps that highlight spatial patterns. The capabilities of our framework are demonstrated on synthetic and real-world data from several domains.",
                        "uid": "v-short-1085",
                        "file_name": "v-short-1085_Agus_Presentation.mp4",
                        "time_stamp": "2022-10-20T19:09:00Z",
                        "time_start": "2022-10-20T19:09:00Z",
                        "time_end": "2022-10-20T19:15:19Z",
                        "paper_type": "short",
                        "keywords": [
                            "Segmented Volumes, Multivariate data, Color mapping, Dimensionality reduction"
                        ],
                        "has_image": "1",
                        "has_video": "379",
                        "paper_award": "",
                        "image_caption": "Inspired by word search puzzles,\nwe present Volume Puzzle, a framework that allows practitioners\nto interactively and/or automatically\nreveal spatial patterns from segmented volumes with associated multivariate attributes.\nFor speeding up spatial analysis,\nwe propose an algorithm that computes attribute projection\nthrough dimensionality reduction, kernel density estimation, and\ntopological analysis based on the Morse-Smale complex.\nThe framework can be used for explorative analysis in various domains,\nlike material science or neuroscience.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "https://youtu.be/O0yYXVSBuWU",
                        "ff_id": "O0yYXVSBuWU"
                    },
                    {
                        "slot_id": "v-short-1085-qa",
                        "session_id": "short3",
                        "type": "Virtual Q+A",
                        "title": "Volume Puzzle: visual analysis of segmented volume data with multivariate attributes (Q+A)",
                        "contributors": [
                            "Marco Agus"
                        ],
                        "authors": [],
                        "abstract": "A variety of application domains, including material science, neuroscience, and connectomics, commonly use segmented volume data for explorative visual analysis. In many cases, segmented objects are characterized by multivariate attributes expressing specific geometric or physical features. Objects with similar characteristics, determined by selected attribute configurations, can create peculiar spatial patterns, whose detection and study is of fundamental importance. This task is notoriously difficult, especially when the number of attributes per segment is large. In this work, we propose an interactive framework that combines a state-of-the-art direct volume renderer for categorical volumes with techniques for the analysis of the attribute space and for the automatic creation of 2D transfer function. We show, in particular, how dimensionality reduction, kernel-density estimation, and topological techniques such as Morse analysis combined with scatter and density plots allow the efficient design of two-dimensional color maps that highlight spatial patterns. The capabilities of our framework are demonstrated on synthetic and real-world data from several domains.",
                        "uid": "v-short-1085",
                        "file_name": "",
                        "time_stamp": "2022-10-20T19:15:19Z",
                        "time_start": "2022-10-20T19:15:19Z",
                        "time_end": "2022-10-20T19:18:00Z",
                        "paper_type": "short",
                        "keywords": [
                            "Segmented Volumes, Multivariate data, Color mapping, Dimensionality reduction"
                        ],
                        "has_image": "1",
                        "has_video": "379",
                        "paper_award": "",
                        "image_caption": "Inspired by word search puzzles,\nwe present Volume Puzzle, a framework that allows practitioners\nto interactively and/or automatically\nreveal spatial patterns from segmented volumes with associated multivariate attributes.\nFor speeding up spatial analysis,\nwe propose an algorithm that computes attribute projection\nthrough dimensionality reduction, kernel density estimation, and\ntopological analysis based on the Morse-Smale complex.\nThe framework can be used for explorative analysis in various domains,\nlike material science or neuroscience.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "https://youtu.be/O0yYXVSBuWU",
                        "ff_id": "O0yYXVSBuWU"
                    },
                    {
                        "slot_id": "v-short-1075-pres",
                        "session_id": "short3",
                        "type": "In Person Presentation",
                        "title": "Droplet-Local Line Integration for Multiphase Flow",
                        "contributors": [
                            "Alexander Straub"
                        ],
                        "authors": [
                            "Alexander Straub",
                            "Sebastian Boblest",
                            "Grzegorz Karch",
                            "Filip Sadlo",
                            "Thomas Ertl"
                        ],
                        "abstract": "Line integration of stream-, streak-, and pathlines is widely used and popular for visualizing single-phase flow. In multiphase flow, i.e., where the fluid consists, e.g., of a liquid and a gaseous phase, these techniques could also provide valuable insights into the internal flow of droplets and ligaments, and thus into their dynamics. However, since such structures tend to act as entities, high translational and rotational velocities often obfuscate their detail. As a remedy, we present a method for deriving a droplet-local velocity field, using a decomposition of the original velocity field removing translational and rotational velocity parts, and adapt path- and streaklines. Generally, the resulting integral lines are thus shorter and less tangled, which simplifies their analysis. We demonstrate and discuss the utility of our approach on droplets in two-phase flow data, and visualize the removed velocity parts employing glyphs for context.",
                        "uid": "v-short-1075",
                        "file_name": "v-short-1075_Straub_Presentation.mp4",
                        "time_stamp": "2022-10-20T19:18:00Z",
                        "time_start": "2022-10-20T19:18:00Z",
                        "time_end": "2022-10-20T19:25:00Z",
                        "paper_type": "short",
                        "keywords": [
                            "Human-centered computing\u2014Visualization\u2014Visualization application domains\u2014Scientific visualization;"
                        ],
                        "has_image": "1",
                        "has_video": "431",
                        "paper_award": "",
                        "image_caption": "Comparison between pathlines in the original vector field and droplet-local pathlines.\n(Left:) The original vector field exhibits strong rotation, which obfuscates local details when visualizing pathlines, and hinders the analysis of the internal flow of the droplet.\n(Right:) The droplet-local pathlines show internal flow revealing two rotating regions to either side. Additional arrow glyphs and the rotation axis provide information about the removed translational and rotational velocity components.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "https://youtu.be/jDfjkPOLSpw",
                        "ff_id": "jDfjkPOLSpw"
                    },
                    {
                        "slot_id": "v-short-1075-qa",
                        "session_id": "short3",
                        "type": "In Person Q+A",
                        "title": "Droplet-Local Line Integration for Multiphase Flow (Q+A)",
                        "contributors": [
                            "Alexander Straub"
                        ],
                        "authors": [],
                        "abstract": "Line integration of stream-, streak-, and pathlines is widely used and popular for visualizing single-phase flow. In multiphase flow, i.e., where the fluid consists, e.g., of a liquid and a gaseous phase, these techniques could also provide valuable insights into the internal flow of droplets and ligaments, and thus into their dynamics. However, since such structures tend to act as entities, high translational and rotational velocities often obfuscate their detail. As a remedy, we present a method for deriving a droplet-local velocity field, using a decomposition of the original velocity field removing translational and rotational velocity parts, and adapt path- and streaklines. Generally, the resulting integral lines are thus shorter and less tangled, which simplifies their analysis. We demonstrate and discuss the utility of our approach on droplets in two-phase flow data, and visualize the removed velocity parts employing glyphs for context.",
                        "uid": "v-short-1075",
                        "file_name": "",
                        "time_stamp": "2022-10-20T19:25:00Z",
                        "time_start": "2022-10-20T19:25:00Z",
                        "time_end": "2022-10-20T19:27:00Z",
                        "paper_type": "short",
                        "keywords": [
                            "Human-centered computing\u2014Visualization\u2014Visualization application domains\u2014Scientific visualization;"
                        ],
                        "has_image": "1",
                        "has_video": "431",
                        "paper_award": "",
                        "image_caption": "Comparison between pathlines in the original vector field and droplet-local pathlines.\n(Left:) The original vector field exhibits strong rotation, which obfuscates local details when visualizing pathlines, and hinders the analysis of the internal flow of the droplet.\n(Right:) The droplet-local pathlines show internal flow revealing two rotating regions to either side. Additional arrow glyphs and the rotation axis provide information about the removed translational and rotational velocity components.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "https://youtu.be/jDfjkPOLSpw",
                        "ff_id": "jDfjkPOLSpw"
                    },
                    {
                        "slot_id": "v-short-1037-pres",
                        "session_id": "short3",
                        "type": "In Person Presentation",
                        "title": "Efficient Interpolation-based Pathline Tracing with B-spline Curves in Particle Dataset",
                        "contributors": [
                            "Haoyu Li"
                        ],
                        "authors": [
                            "Haoyu Li",
                            "Tianyu Xiong",
                            "Han-Wei Shen"
                        ],
                        "abstract": "Particle tracing through numerical integration is a well-known approach to generating pathlines for visualization. However, for particle simulations, the computation of pathlines is expensive, since the interpolation method is complicated due to the lack of connectivity information. Previous studies utilize the $k$-d tree to reduce the time for neighborhood search. However, the efficiency is still limited by the number of tracing time steps. Therefore, we propose a novel interpolation-based particle tracing method that first represents particle data as B-spline curves and interpolates B-spline control points to reduce the number of interpolation time steps. We demonstrate our approach achieves good tracing accuracy with much less computation time.",
                        "uid": "v-short-1037",
                        "file_name": "v-short-1037_Li_Presentation.mp4",
                        "time_stamp": "2022-10-20T19:27:00Z",
                        "time_start": "2022-10-20T19:27:00Z",
                        "time_end": "2022-10-20T19:34:00Z",
                        "paper_type": "short",
                        "keywords": [
                            "Particle Tracing, Pathlines, flow visualization, B-spline"
                        ],
                        "has_image": "1",
                        "has_video": "402",
                        "paper_award": "",
                        "image_caption": "The workflow for our B-spline curve control point interpolation method for pathline tracing is shown on the left. The particle dataset is represented as B-spline curves and their control points are interpolated to trace new pathlines. The left figures show a comparison of tracing time, accuracy, and quality between our method and the baseline method by Chandler et al. Our approach reduces the computation time while preserving the tracing quality.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "https://youtu.be/wmRnTd07YGw",
                        "ff_id": "wmRnTd07YGw"
                    },
                    {
                        "slot_id": "v-short-1037-qa",
                        "session_id": "short3",
                        "type": "In Person Q+A",
                        "title": "Efficient Interpolation-based Pathline Tracing with B-spline Curves in Particle Dataset (Q+A)",
                        "contributors": [
                            "Haoyu Li"
                        ],
                        "authors": [],
                        "abstract": "Particle tracing through numerical integration is a well-known approach to generating pathlines for visualization. However, for particle simulations, the computation of pathlines is expensive, since the interpolation method is complicated due to the lack of connectivity information. Previous studies utilize the $k$-d tree to reduce the time for neighborhood search. However, the efficiency is still limited by the number of tracing time steps. Therefore, we propose a novel interpolation-based particle tracing method that first represents particle data as B-spline curves and interpolates B-spline control points to reduce the number of interpolation time steps. We demonstrate our approach achieves good tracing accuracy with much less computation time.",
                        "uid": "v-short-1037",
                        "file_name": "",
                        "time_stamp": "2022-10-20T19:34:00Z",
                        "time_start": "2022-10-20T19:34:00Z",
                        "time_end": "2022-10-20T19:36:00Z",
                        "paper_type": "short",
                        "keywords": [
                            "Particle Tracing, Pathlines, flow visualization, B-spline"
                        ],
                        "has_image": "1",
                        "has_video": "402",
                        "paper_award": "",
                        "image_caption": "The workflow for our B-spline curve control point interpolation method for pathline tracing is shown on the left. The particle dataset is represented as B-spline curves and their control points are interpolated to trace new pathlines. The left figures show a comparison of tracing time, accuracy, and quality between our method and the baseline method by Chandler et al. Our approach reduces the computation time while preserving the tracing quality.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "https://youtu.be/wmRnTd07YGw",
                        "ff_id": "wmRnTd07YGw"
                    },
                    {
                        "slot_id": "v-short-1081-pres",
                        "session_id": "short3",
                        "type": "In Person Presentation",
                        "title": "Visualizing Confidence Intervals for Critical Point Probabilities in 2D Scalar Field Ensembles",
                        "contributors": [
                            "Dominik Vietinghoff"
                        ],
                        "authors": [
                            "Dominik Vietinghoff",
                            "Michael B\u00f6ttinger",
                            "Gerik Scheuermann",
                            "Christian Heine"
                        ],
                        "abstract": "An important task in visualization is the extraction and highlighting of dominant features in data to support users in their analysis process. Topological methods are a well-known means of identifying such features in deterministic fields. However, many real-world phenomena studied today are the result of a chaotic system that cannot be fully described by a single simulation. Instead, the variability of such systems is usually captured with ensemble simulations that produce a variety of possible outcomes of the simulated process. The topological analysis of such ensemble data sets and uncertain data, in general, is less well studied. In this work, we present an approach for the computation and visual representation of confidence intervals for the occurrence probabilities of critical points in ensemble data sets. We demonstrate the added value of our approach over existing methods for critical point prediction in uncertain data on a synthetic data set and show its applicability to a data set from climate research.",
                        "uid": "v-short-1081",
                        "file_name": "v-short-1081_Vietinghoff_Presentation.mp4",
                        "time_stamp": "2022-10-20T19:36:00Z",
                        "time_start": "2022-10-20T19:36:00Z",
                        "time_end": "2022-10-20T19:43:00Z",
                        "paper_type": "short",
                        "keywords": [
                            "Uncertainty visualization, scalar topology, critical points, ensemble data, climate data, inferential statistics, glyphs."
                        ],
                        "has_image": "1",
                        "has_video": "445",
                        "paper_award": "",
                        "image_caption": "Confidence intervals for critical point occurrence probabilities for two ensembles of synthetic fields with a different number of members. The red, blue, and green thirds of the glyphs show the confidence intervals for maxima, minima, and saddles at the respective grid points. The area of the darker, inner arcs is proportional to the lower bounds of the found confidence intervals, the area of the lighter, outer arcs to their upper bounds. The black line marks the point estimate for the probability of a critical point of each type computed from the input ensemble.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "https://youtu.be/sn_7t5PFWhA",
                        "ff_id": "sn_7t5PFWhA"
                    },
                    {
                        "slot_id": "v-short-1081-qa",
                        "session_id": "short3",
                        "type": "In Person Q+A",
                        "title": "Visualizing Confidence Intervals for Critical Point Probabilities in 2D Scalar Field Ensembles (Q+A)",
                        "contributors": [
                            "Dominik Vietinghoff"
                        ],
                        "authors": [],
                        "abstract": "An important task in visualization is the extraction and highlighting of dominant features in data to support users in their analysis process. Topological methods are a well-known means of identifying such features in deterministic fields. However, many real-world phenomena studied today are the result of a chaotic system that cannot be fully described by a single simulation. Instead, the variability of such systems is usually captured with ensemble simulations that produce a variety of possible outcomes of the simulated process. The topological analysis of such ensemble data sets and uncertain data, in general, is less well studied. In this work, we present an approach for the computation and visual representation of confidence intervals for the occurrence probabilities of critical points in ensemble data sets. We demonstrate the added value of our approach over existing methods for critical point prediction in uncertain data on a synthetic data set and show its applicability to a data set from climate research.",
                        "uid": "v-short-1081",
                        "file_name": "",
                        "time_stamp": "2022-10-20T19:43:00Z",
                        "time_start": "2022-10-20T19:43:00Z",
                        "time_end": "2022-10-20T19:45:00Z",
                        "paper_type": "short",
                        "keywords": [
                            "Uncertainty visualization, scalar topology, critical points, ensemble data, climate data, inferential statistics, glyphs."
                        ],
                        "has_image": "1",
                        "has_video": "445",
                        "paper_award": "",
                        "image_caption": "Confidence intervals for critical point occurrence probabilities for two ensembles of synthetic fields with a different number of members. The red, blue, and green thirds of the glyphs show the confidence intervals for maxima, minima, and saddles at the respective grid points. The area of the darker, inner arcs is proportional to the lower bounds of the found confidence intervals, the area of the lighter, outer arcs to their upper bounds. The black line marks the point estimate for the probability of a critical point of each type computed from the input ensemble.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "https://youtu.be/sn_7t5PFWhA",
                        "ff_id": "sn_7t5PFWhA"
                    },
                    {
                        "slot_id": "v-short-1103-pres",
                        "session_id": "short3",
                        "type": "In Person Presentation",
                        "title": "ASEVis: Visual Exploration of Active System Ensembles to Define Characteristic Measures",
                        "contributors": [
                            "Marina Evers"
                        ],
                        "authors": [
                            "Marina Evers",
                            "Raphael Wittkowski",
                            "Lars Linsen"
                        ],
                        "abstract": "Simulation ensembles are a common tool in physics for understanding how a model outcome depends on input parameters. We analyze an active particle system, where each particle can use energy from its surroundings to propel itself. A multi-dimensional feature vector containing all particles' motion information can describe the whole system at each time step. The system's behavior strongly depends on input parameters like the propulsion mechanism of the particles. To understand how the time-varying behavior depends on the input parameters, it is necessary to introduce new measures to quantify the difference of the dynamics of the ensemble members. We propose a tool that supports the interactive visual analysis of time-varying feature-vector ensembles. A core component of our tool allows for the interactive definition and refinement of new measures that can then be used to understand the system's behavior and compare the ensemble members. Different visualizations support the user in finding a characteristic measure for the system. By visualizing the user-defined measure, the user can then investigate the parameter dependencies and gain insights into the relationship between input parameters and simulation output.",
                        "uid": "v-short-1103",
                        "file_name": "v-short-1103_Evers_Presentation.mp4",
                        "time_stamp": "2022-10-20T19:45:00Z",
                        "time_start": "2022-10-20T19:45:00Z",
                        "time_end": "2022-10-20T19:52:00Z",
                        "paper_type": "short",
                        "keywords": [
                            "Physical & Environmental Sciences, Engineering, Mathematics ; Comparison and Similarity ; Coordinated and Multiple Views ; Application Motivated Visualization ; Task Abstractions & Application Domains ; Temporal Data"
                        ],
                        "has_image": "1",
                        "has_video": "402",
                        "paper_award": "",
                        "image_caption": "The interactive analysis tool ASEVis: A programming interface (a) allows for the definition of time-dependent measures as well as aggregations. Aggregated measures are shown in a heatmap (b) while the aggregation over time is visualized in the timeplot (d). Detail visualizations for single ensemble members include animations (c), a line plot, and a scatter plot matrix.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "https://youtu.be/z9VATm0wWns",
                        "ff_id": "z9VATm0wWns"
                    },
                    {
                        "slot_id": "v-short-1103-qa",
                        "session_id": "short3",
                        "type": "In Person Q+A",
                        "title": "ASEVis: Visual Exploration of Active System Ensembles to Define Characteristic Measures (Q+A)",
                        "contributors": [
                            "Marina Evers"
                        ],
                        "authors": [],
                        "abstract": "Simulation ensembles are a common tool in physics for understanding how a model outcome depends on input parameters. We analyze an active particle system, where each particle can use energy from its surroundings to propel itself. A multi-dimensional feature vector containing all particles' motion information can describe the whole system at each time step. The system's behavior strongly depends on input parameters like the propulsion mechanism of the particles. To understand how the time-varying behavior depends on the input parameters, it is necessary to introduce new measures to quantify the difference of the dynamics of the ensemble members. We propose a tool that supports the interactive visual analysis of time-varying feature-vector ensembles. A core component of our tool allows for the interactive definition and refinement of new measures that can then be used to understand the system's behavior and compare the ensemble members. Different visualizations support the user in finding a characteristic measure for the system. By visualizing the user-defined measure, the user can then investigate the parameter dependencies and gain insights into the relationship between input parameters and simulation output.",
                        "uid": "v-short-1103",
                        "file_name": "",
                        "time_stamp": "2022-10-20T19:52:00Z",
                        "time_start": "2022-10-20T19:52:00Z",
                        "time_end": "2022-10-20T19:54:00Z",
                        "paper_type": "short",
                        "keywords": [
                            "Physical & Environmental Sciences, Engineering, Mathematics ; Comparison and Similarity ; Coordinated and Multiple Views ; Application Motivated Visualization ; Task Abstractions & Application Domains ; Temporal Data"
                        ],
                        "has_image": "1",
                        "has_video": "402",
                        "paper_award": "",
                        "image_caption": "The interactive analysis tool ASEVis: A programming interface (a) allows for the definition of time-dependent measures as well as aggregations. Aggregated measures are shown in a heatmap (b) while the aggregation over time is visualized in the timeplot (d). Detail visualizations for single ensemble members include animations (c), a line plot, and a scatter plot matrix.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "https://youtu.be/z9VATm0wWns",
                        "ff_id": "z9VATm0wWns"
                    },
                    {
                        "slot_id": "v-short-1110-pres",
                        "session_id": "short3",
                        "type": "Virtual Presentation (pre-recorded)",
                        "title": "Accelerated Probabilistic Marching Cubes by Deep Learning for Time-Varying Scalar Ensembles",
                        "contributors": [
                            "Mengjiao Han"
                        ],
                        "authors": [
                            "Mengjiao Han",
                            "Tushar M. Athawale",
                            "David Pugmire",
                            "Chris R. Johnson"
                        ],
                        "abstract": "Visualizing the uncertainty of ensemble simulations is challenging due to the large size and multivariate and temporal features of ensemble data sets. One popular approach to studying the uncertainty of ensembles is analyzing the positional uncertainty of the level sets. Probabilistic marching cubes is a technique that performs Monte Carlo sampling of multivariate Gaussian noise distributions for positional uncertainty visualization of level sets. However, the technique suffers from high computational time, making interactive visualization and analysis impossible to achieve. This paper introduces a deep-learning-based approach to learning the level-set uncertainty for two-dimensional ensemble data with a multivariate Gaussian noise assumption. We train the model using the first few time steps from time-varying ensemble data in our workflow. We demonstrate that our trained model accurately infers uncertainty in level sets for new time steps and is up to 170X faster than that of the original probabilistic model with serial computation and 10X faster than that of the original parallel computation.",
                        "uid": "v-short-1110",
                        "file_name": "v-short-1110_Han_Presentation.mp4",
                        "time_stamp": "2022-10-20T19:54:00Z",
                        "time_start": "2022-10-20T19:54:00Z",
                        "time_end": "2022-10-20T20:01:00Z",
                        "paper_type": "short",
                        "keywords": [
                            "Human-centered computing\u2014Visualization\u2014Visualization application domains\u2014Scientific visualization; Computing methodologies\u2014Machine learning\u2014Machine learning approaches\u2014Neural networks"
                        ],
                        "has_image": "1",
                        "has_video": "418",
                        "paper_award": "",
                        "image_caption": "Visualizations of the level-crossing probability for isovalue 0.1 in the Red Sea data set using our proposed neural\nnetwork. Image (left) shows the level-crossing probabilities calculated using the original probabilistic marching cubes algorithm.\nImage (right) shows the result computed by our trained model. The zoomed-in views are displayed in the top right. Our method can provide a visually identical result and is 10X faster than the original probabilistic marching cubes algorithm with parallel computing.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "https://youtu.be/fEigQhTDtHQ",
                        "ff_id": "fEigQhTDtHQ"
                    },
                    {
                        "slot_id": "v-short-1110-qa",
                        "session_id": "short3",
                        "type": "Virtual Q+A",
                        "title": "Accelerated Probabilistic Marching Cubes by Deep Learning for Time-Varying Scalar Ensembles (Q+A)",
                        "contributors": [
                            "Mengjiao Han"
                        ],
                        "authors": [],
                        "abstract": "Visualizing the uncertainty of ensemble simulations is challenging due to the large size and multivariate and temporal features of ensemble data sets. One popular approach to studying the uncertainty of ensembles is analyzing the positional uncertainty of the level sets. Probabilistic marching cubes is a technique that performs Monte Carlo sampling of multivariate Gaussian noise distributions for positional uncertainty visualization of level sets. However, the technique suffers from high computational time, making interactive visualization and analysis impossible to achieve. This paper introduces a deep-learning-based approach to learning the level-set uncertainty for two-dimensional ensemble data with a multivariate Gaussian noise assumption. We train the model using the first few time steps from time-varying ensemble data in our workflow. We demonstrate that our trained model accurately infers uncertainty in level sets for new time steps and is up to 170X faster than that of the original probabilistic model with serial computation and 10X faster than that of the original parallel computation.",
                        "uid": "v-short-1110",
                        "file_name": "",
                        "time_stamp": "2022-10-20T20:01:00Z",
                        "time_start": "2022-10-20T20:01:00Z",
                        "time_end": "2022-10-20T20:03:00Z",
                        "paper_type": "short",
                        "keywords": [
                            "Human-centered computing\u2014Visualization\u2014Visualization application domains\u2014Scientific visualization; Computing methodologies\u2014Machine learning\u2014Machine learning approaches\u2014Neural networks"
                        ],
                        "has_image": "1",
                        "has_video": "418",
                        "paper_award": "",
                        "image_caption": "Visualizations of the level-crossing probability for isovalue 0.1 in the Red Sea data set using our proposed neural\nnetwork. Image (left) shows the level-crossing probabilities calculated using the original probabilistic marching cubes algorithm.\nImage (right) shows the result computed by our trained model. The zoomed-in views are displayed in the top right. Our method can provide a visually identical result and is 10X faster than the original probabilistic marching cubes algorithm with parallel computing.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "https://youtu.be/fEigQhTDtHQ",
                        "ff_id": "fEigQhTDtHQ"
                    },
                    {
                        "slot_id": "v-short-1055-pres",
                        "session_id": "short3",
                        "type": "In Person Presentation",
                        "title": "Beyond Visuals : Examining the Experiences of Geoscience Professionals With Vision Disabilities in Accessing Data Visualizations",
                        "contributors": [
                            "Dr Nihanth W Cherukuru"
                        ],
                        "authors": [
                            "Nihanth W Cherukuru",
                            "David Bailey",
                            "Tiffany Fourment",
                            "Becca Hatheway",
                            "Marika Holland",
                            "Matt Rehme"
                        ],
                        "abstract": "Data visualizations are ubiquitous in all disciplines and have become the primary means of analysing data and communicating insights. However, the predominant reliance on visual encoding of data continues to create accessibility barriers for people who are blind/vision impaired resulting in their under representation in Science, Technology, Engineering and Mathematics (STEM) disciplines. This research study seeks to understand the experiences of professionals who are blind/vision impaired in one such STEM discipline (geosciences) in accessing data visualizations. In-depth, semi-structured interviews with seven professionals were conducted to examine the accessibility barriers and areas for improvement to inform accessibility research pertaining to data visualizations through a socio-technical lens. A reflexive thematic analysis revealed the negative impact of visualizations in influencing their career path, lack of data exploration tools for research, barriers in accessing works of peers and mismatched pace of visualization and accessibility research. The article also includes recommendations from the participants to address some of these accessibility barriers.",
                        "uid": "v-short-1055",
                        "file_name": "v-short-1055_Cherukuru_Presentation.mp4",
                        "time_stamp": "2022-10-20T20:03:00Z",
                        "time_start": "2022-10-20T20:03:00Z",
                        "time_end": "2022-10-20T20:10:00Z",
                        "paper_type": "short",
                        "keywords": [
                            "Human-centered computing\u2014Visualization\u2014Visualization Design and evaluation methods; Human-centered computing\u2014Accessibility\u2014Accessibility technologies"
                        ],
                        "has_image": "1",
                        "has_video": "699",
                        "paper_award": "",
                        "image_caption": "A photograph showing a woman's hand over a tactile data visualization 3D model. The tactile representation is a white 3D printed surface of the earth with cardboard cutouts representing sea-ice data.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "https://youtu.be/YCLDZZpGxek",
                        "ff_id": "YCLDZZpGxek"
                    },
                    {
                        "slot_id": "v-short-1055-qa",
                        "session_id": "short3",
                        "type": "In Person Q+A",
                        "title": "Beyond Visuals : Examining the Experiences of Geoscience Professionals With Vision Disabilities in Accessing Data Visualizations (Q+A)",
                        "contributors": [
                            "Dr Nihanth W Cherukuru"
                        ],
                        "authors": [],
                        "abstract": "Data visualizations are ubiquitous in all disciplines and have become the primary means of analysing data and communicating insights. However, the predominant reliance on visual encoding of data continues to create accessibility barriers for people who are blind/vision impaired resulting in their under representation in Science, Technology, Engineering and Mathematics (STEM) disciplines. This research study seeks to understand the experiences of professionals who are blind/vision impaired in one such STEM discipline (geosciences) in accessing data visualizations. In-depth, semi-structured interviews with seven professionals were conducted to examine the accessibility barriers and areas for improvement to inform accessibility research pertaining to data visualizations through a socio-technical lens. A reflexive thematic analysis revealed the negative impact of visualizations in influencing their career path, lack of data exploration tools for research, barriers in accessing works of peers and mismatched pace of visualization and accessibility research. The article also includes recommendations from the participants to address some of these accessibility barriers.",
                        "uid": "v-short-1055",
                        "file_name": "",
                        "time_stamp": "2022-10-20T20:10:00Z",
                        "time_start": "2022-10-20T20:10:00Z",
                        "time_end": "2022-10-20T20:12:00Z",
                        "paper_type": "short",
                        "keywords": [
                            "Human-centered computing\u2014Visualization\u2014Visualization Design and evaluation methods; Human-centered computing\u2014Accessibility\u2014Accessibility technologies"
                        ],
                        "has_image": "1",
                        "has_video": "699",
                        "paper_award": "",
                        "image_caption": "A photograph showing a woman's hand over a tactile data visualization 3D model. The tactile representation is a white 3D printed surface of the earth with cardboard cutouts representing sea-ice data.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "https://youtu.be/YCLDZZpGxek",
                        "ff_id": "YCLDZZpGxek"
                    }
                ]
            },
            {
                "title": "Personal Visualization, Theory, Evaluation, and eXtended Reality",
                "session_id": "short4",
                "event_prefix": "v-short",
                "track": "ok1",
                "livestream_id": "ok1-thu",
                "session_image": "short4.png",
                "chair": [
                    "Cindy Xiong"
                ],
                "organizers": [],
                "time_start": "2022-10-20T14:00:00Z",
                "time_end": "2022-10-20T15:15:00Z",
                "discord_category": "",
                "discord_channel": "oklahoma-st-1",
                "discord_channel_id": "1024600861340606484",
                "discord_link": "https://discord.com/channels/978320435554947082/1024600861340606484",
                "slido_link": "https://app.sli.do/event/i28UETZaiRWr1XwX6eKjcf?section=740d9835-fe47-4cb4-b260-353262a7f8dd",
                "youtube_url": "https://youtu.be/x76ysAHpetE",
                "youtube_id": "x76ysAHpetE",
                "zoom_meeting": "",
                "zoom_password": "",
                "zoom_link": "",
                "ff_link": "https://youtu.be/GDewdSbR2EM",
                "ff_playlist": "",
                "time_slots": [
                    {
                        "slot_id": "short4-opening",
                        "session_id": "short4",
                        "type": "In Person Other",
                        "title": "Session Opening",
                        "contributors": [
                            "Cindy Xiong"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "",
                        "file_name": "",
                        "time_stamp": "2022-10-20T14:00:00Z",
                        "time_start": "2022-10-20T14:00:00Z",
                        "time_end": "2022-10-20T14:00:00Z",
                        "paper_type": "",
                        "keywords": [],
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "v-short-1064-pres",
                        "session_id": "short4",
                        "type": "Virtual Presentation (pre-recorded)",
                        "title": "Let's Get Personal: Exploring the Design of Personalized Visualizations",
                        "contributors": [
                            "Beleicia Bullock"
                        ],
                        "authors": [
                            "Beleicia Bullock",
                            "Shunan Guo",
                            "Eunyee Koh",
                            "Ryan Rossi",
                            "Fan Du",
                            "Jane Hoffswell"
                        ],
                        "abstract": "Media outlets often publish visualizations that can be personalized based on users\u2019 demographics, such as location, race, and age. However, the design of such personalized visualizations remains underexplored. In this work, we contribute a design space analysis of 47 public-facing articles with personalized visualizations to understand how designers structure content, encourage exploration, and present insights. We find that articles often lack explicit exploration suggestions or instructions, data notices, and personalized visual insights. We then outline three trajectories for future research: (1) explore how users choose to personalize visualizations, (2) examine how exploration suggestions and examples impact user interaction, and (3) investigate how personalization influences user insights.",
                        "uid": "v-short-1064",
                        "file_name": "v-short-1064_Bullock_Presentation.mp4",
                        "time_stamp": "2022-10-20T14:00:00Z",
                        "time_start": "2022-10-20T14:00:00Z",
                        "time_end": "2022-10-20T14:07:42Z",
                        "paper_type": "short",
                        "keywords": [
                            "Human-centered computing\u2014Visualization\u2014Visualization application domains"
                        ],
                        "has_image": "1",
                        "has_video": "462",
                        "paper_award": "",
                        "image_caption": "This heat map provides an overview of the articles in our personalized visualization corpus based on the (a) personalized attributes contained in the article, (b) granularity, and (c) resulting codes for different publications. We also break down the articles by publication. Attributes that only appeared in 1-3 articles are grouped together under \u201cother.\u201d",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "https://youtu.be/SJNY8toifWA",
                        "ff_id": "SJNY8toifWA"
                    },
                    {
                        "slot_id": "v-short-1064-qa",
                        "session_id": "short4",
                        "type": "Virtual Q+A",
                        "title": "Let's Get Personal: Exploring the Design of Personalized Visualizations (Q+A)",
                        "contributors": [
                            "Beleicia Bullock"
                        ],
                        "authors": [],
                        "abstract": "Media outlets often publish visualizations that can be personalized based on users\u2019 demographics, such as location, race, and age. However, the design of such personalized visualizations remains underexplored. In this work, we contribute a design space analysis of 47 public-facing articles with personalized visualizations to understand how designers structure content, encourage exploration, and present insights. We find that articles often lack explicit exploration suggestions or instructions, data notices, and personalized visual insights. We then outline three trajectories for future research: (1) explore how users choose to personalize visualizations, (2) examine how exploration suggestions and examples impact user interaction, and (3) investigate how personalization influences user insights.",
                        "uid": "v-short-1064",
                        "file_name": "",
                        "time_stamp": "2022-10-20T14:07:42Z",
                        "time_start": "2022-10-20T14:07:42Z",
                        "time_end": "2022-10-20T14:09:00Z",
                        "paper_type": "short",
                        "keywords": [
                            "Human-centered computing\u2014Visualization\u2014Visualization application domains"
                        ],
                        "has_image": "1",
                        "has_video": "462",
                        "paper_award": "",
                        "image_caption": "This heat map provides an overview of the articles in our personalized visualization corpus based on the (a) personalized attributes contained in the article, (b) granularity, and (c) resulting codes for different publications. We also break down the articles by publication. Attributes that only appeared in 1-3 articles are grouped together under \u201cother.\u201d",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "https://youtu.be/SJNY8toifWA",
                        "ff_id": "SJNY8toifWA"
                    },
                    {
                        "slot_id": "v-short-1016-pres",
                        "session_id": "short4",
                        "type": "Virtual Presentation (pre-recorded)",
                        "title": "Who benefits from Visualization Adaptations? Towards a better Understanding of the Influence of Visualization Literacy",
                        "contributors": [
                            "Marc Satkowski"
                        ],
                        "authors": [
                            "Marc Satkowski",
                            "Franziska Kessler",
                            "Susanne Narciss",
                            "Raimund Dachselt"
                        ],
                        "abstract": "The ability to read, understand, and comprehend visual information representations is subsumed under the term visualization literacy (VL). One possibility to improve the use of information visualizations is to introduce adaptations. However, it is yet unclear whether people with different VL benefit from adaptations to the same degree. We conducted an online experiment (n = 42) to investigate whether the effect of an adaptation (here: De-Emphasis) of visualizations (bar charts, scatter plots) on performance (accuracy, time) and user experiences depends on users\u2019 VL level. Using linear mixed models for the analyses, we found a positive impact of the De-Emphasis adaptation across all conditions, as well as an interaction effect of adaptation and VL on the task completion time for bar charts. This work contributes to a better understanding of the intertwined relationship of VL and visual adaptations and motivates future research.",
                        "uid": "v-short-1016",
                        "file_name": "v-short-1016_Satkowski_Presentation.mp4",
                        "time_stamp": "2022-10-20T14:09:00Z",
                        "time_start": "2022-10-20T14:09:00Z",
                        "time_end": "2022-10-20T14:16:05Z",
                        "paper_type": "short",
                        "keywords": [
                            "User Study; Visualization Adaptation; Visualization Literacy; Visualization Competence; Information Visualization; Online Survey; User Experience"
                        ],
                        "has_image": "1",
                        "has_video": "425",
                        "paper_award": "",
                        "image_caption": "An altered title slides of the talk.\nAt the top of the image 4 visualizations are shown.\nLeft are two grouped bar charts, right two scatterplots, whereof one is presented normally, and one makes use of the De-Emphasis technique.\nAll 4 visualizations demonstrate the 4 conditions used in the study of this paper.\nIn the middle, the title of the paper, the authors are shown, and their affiliations are shown.\nFurther, a QR code is visible, which encodes the project page: imld.de/VL-Adaptation-Study.\nOn the bottom of the slides, several logos are shown in accordance with the affiliations.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "https://youtu.be/Tfx6vU1tyO0",
                        "ff_id": "Tfx6vU1tyO0"
                    },
                    {
                        "slot_id": "v-short-1016-qa",
                        "session_id": "short4",
                        "type": "Virtual Q+A",
                        "title": "Who benefits from Visualization Adaptations? Towards a better Understanding of the Influence of Visualization Literacy (Q+A)",
                        "contributors": [
                            "Marc Satkowski"
                        ],
                        "authors": [],
                        "abstract": "The ability to read, understand, and comprehend visual information representations is subsumed under the term visualization literacy (VL). One possibility to improve the use of information visualizations is to introduce adaptations. However, it is yet unclear whether people with different VL benefit from adaptations to the same degree. We conducted an online experiment (n = 42) to investigate whether the effect of an adaptation (here: De-Emphasis) of visualizations (bar charts, scatter plots) on performance (accuracy, time) and user experiences depends on users\u2019 VL level. Using linear mixed models for the analyses, we found a positive impact of the De-Emphasis adaptation across all conditions, as well as an interaction effect of adaptation and VL on the task completion time for bar charts. This work contributes to a better understanding of the intertwined relationship of VL and visual adaptations and motivates future research.",
                        "uid": "v-short-1016",
                        "file_name": "",
                        "time_stamp": "2022-10-20T14:16:05Z",
                        "time_start": "2022-10-20T14:16:05Z",
                        "time_end": "2022-10-20T14:18:00Z",
                        "paper_type": "short",
                        "keywords": [
                            "User Study; Visualization Adaptation; Visualization Literacy; Visualization Competence; Information Visualization; Online Survey; User Experience"
                        ],
                        "has_image": "1",
                        "has_video": "425",
                        "paper_award": "",
                        "image_caption": "An altered title slides of the talk.\nAt the top of the image 4 visualizations are shown.\nLeft are two grouped bar charts, right two scatterplots, whereof one is presented normally, and one makes use of the De-Emphasis technique.\nAll 4 visualizations demonstrate the 4 conditions used in the study of this paper.\nIn the middle, the title of the paper, the authors are shown, and their affiliations are shown.\nFurther, a QR code is visible, which encodes the project page: imld.de/VL-Adaptation-Study.\nOn the bottom of the slides, several logos are shown in accordance with the affiliations.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "https://youtu.be/Tfx6vU1tyO0",
                        "ff_id": "Tfx6vU1tyO0"
                    },
                    {
                        "slot_id": "v-short-1141-pres",
                        "session_id": "short4",
                        "type": "In Person Presentation",
                        "title": "VisQuiz: Exploring Feedback Mechanisms to Improve Graphical Perception",
                        "contributors": [
                            "No\u00eblle Rakotondravony",
                            "Yiren Ding"
                        ],
                        "authors": [
                            "Ryan Birchfield",
                            "Maddison Caten",
                            "Errica Cheng",
                            "Madyson Kelly",
                            "Truman Larson",
                            "Hoang Phan Pham",
                            "Yiren Ding",
                            "No\u00eblle Rakotondravony",
                            "Lane Harrison"
                        ],
                        "abstract": "Graphical perception studies are a key element of visualization research, forming the basis of design recommendations and contributing to our understanding of how people make sense of visualizations. However, graphical perception studies typically include only brief training sessions, and the impact of longer and more in-depth feedback remains unclear. In this paper, we explore the design and evaluation of feedback for graphical perception tasks, called VisQuiz. Using a quiz-like metaphor, we design feedback for a typical visualization comparison experiment, showing participants their answer alongside the correct answer in an animated sequence in each trial. We extend this quiz metaphor to include summary feedback after each stage of the experiment, providing additional moments for participants to reflect on their performance. To evaluate VisQuiz, we conduct a between-subjects experiment, including three stages of 40 trials each with a control condition that included only summary feedback. Results from n = 80 participants show that once participants started receiving trial feedback (Stage 2) they performed significantly better with bubble charts than those in the control condition. This effect carried over when feedback was removed (Stage 3). Results also suggest an overall trend of improved performance due to feedback. We discuss these findings in the context of other visualization literacy efforts, and possible future work at the intersection of visualization, feedback, and learning. Experiment data and analysis scripts are available at the following repository https://osf.io/jys5d/",
                        "uid": "v-short-1141",
                        "file_name": "v-short-1141_Birchfield_Presentation.mp4",
                        "time_stamp": "2022-10-20T14:18:00Z",
                        "time_start": "2022-10-20T14:18:00Z",
                        "time_end": "2022-10-20T14:25:00Z",
                        "paper_type": "short",
                        "keywords": [
                            "Visualization, Graphical Perception, Feedback"
                        ],
                        "has_image": "1",
                        "has_video": "415",
                        "paper_award": "",
                        "image_caption": "We explore the design and evaluation of feedback for graphical perception tasks. Using a quiz-like metaphor, we design animation-powered feedback for a typical visualization comparison experiment, showing participants their answer alongside the correct answer in an animated sequence in each trial, as well as summary feedback at the end of trial sections. We conduct a between-subjects experiment, including three stages with a control condition that included only summary feedback. Results show that once participants started receiving trial feedback they performed significantly better with bubble charts than those in the control condition. This effect carried over when feedback was removed.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "https://youtu.be/P7j4c53f3eA",
                        "ff_id": "P7j4c53f3eA"
                    },
                    {
                        "slot_id": "v-short-1141-qa",
                        "session_id": "short4",
                        "type": "In Person Q+A",
                        "title": "VisQuiz: Exploring Feedback Mechanisms to Improve Graphical Perception (Q+A)",
                        "contributors": [
                            "No\u00eblle Rakotondravony",
                            "Yiren Ding"
                        ],
                        "authors": [],
                        "abstract": "Graphical perception studies are a key element of visualization research, forming the basis of design recommendations and contributing to our understanding of how people make sense of visualizations. However, graphical perception studies typically include only brief training sessions, and the impact of longer and more in-depth feedback remains unclear. In this paper, we explore the design and evaluation of feedback for graphical perception tasks, called VisQuiz. Using a quiz-like metaphor, we design feedback for a typical visualization comparison experiment, showing participants their answer alongside the correct answer in an animated sequence in each trial. We extend this quiz metaphor to include summary feedback after each stage of the experiment, providing additional moments for participants to reflect on their performance. To evaluate VisQuiz, we conduct a between-subjects experiment, including three stages of 40 trials each with a control condition that included only summary feedback. Results from n = 80 participants show that once participants started receiving trial feedback (Stage 2) they performed significantly better with bubble charts than those in the control condition. This effect carried over when feedback was removed (Stage 3). Results also suggest an overall trend of improved performance due to feedback. We discuss these findings in the context of other visualization literacy efforts, and possible future work at the intersection of visualization, feedback, and learning. Experiment data and analysis scripts are available at the following repository https://osf.io/jys5d/",
                        "uid": "v-short-1141",
                        "file_name": "",
                        "time_stamp": "2022-10-20T14:25:00Z",
                        "time_start": "2022-10-20T14:25:00Z",
                        "time_end": "2022-10-20T14:27:00Z",
                        "paper_type": "short",
                        "keywords": [
                            "Visualization, Graphical Perception, Feedback"
                        ],
                        "has_image": "1",
                        "has_video": "415",
                        "paper_award": "",
                        "image_caption": "We explore the design and evaluation of feedback for graphical perception tasks. Using a quiz-like metaphor, we design animation-powered feedback for a typical visualization comparison experiment, showing participants their answer alongside the correct answer in an animated sequence in each trial, as well as summary feedback at the end of trial sections. We conduct a between-subjects experiment, including three stages with a control condition that included only summary feedback. Results show that once participants started receiving trial feedback they performed significantly better with bubble charts than those in the control condition. This effect carried over when feedback was removed.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "https://youtu.be/P7j4c53f3eA",
                        "ff_id": "P7j4c53f3eA"
                    },
                    {
                        "slot_id": "v-short-1100-pres",
                        "session_id": "short4",
                        "type": "In Person Presentation",
                        "title": "OSCAR: A Semantic-based Data Binning Approach",
                        "contributors": [
                            "Vidya Setlur"
                        ],
                        "authors": [
                            "Vidya Setlur",
                            "Michael Correll",
                            "Sarah Battersby"
                        ],
                        "abstract": "Binning is applied to categorize data values or to see distributions of data. Existing binning algorithms often rely on statistical properties of data. However, there are semantic considerations for selecting appropriate binning schemes. Surveys, for instance, gather respondent data for demographic-related questions such as age, salary, number of employees, etc., that are bucketed into defined semantic categories. In this paper, we leverage common semantic categories from survey data and Tableau Public visualizations to identify a set of semantic binning categories. We employ these semantic binning categories in OSCAR: a method for automatically selecting bins based on the inferred semantic type of the field. We conducted a crowdsourced study with 120 participants to better understand user preferences for bins generated by OSCAR vs. binning provided in Tableau. We find that maps and histograms using binned values generated by OSCAR are preferred by users as compared to binning schemes based purely on the statistical properties of the data.",
                        "uid": "v-short-1100",
                        "file_name": "v-short-1100_Setlur_Presentation.mp4",
                        "time_stamp": "2022-10-20T14:27:00Z",
                        "time_start": "2022-10-20T14:27:00Z",
                        "time_end": "2022-10-20T14:34:00Z",
                        "paper_type": "short",
                        "keywords": [
                            "Data-driven semantics, binning, constraints, geospatial"
                        ],
                        "has_image": "1",
                        "has_video": "382",
                        "paper_award": "",
                        "image_caption": "Visualizations showing comparisons of bins for data on per-country life expectancy (left) and per-U.S. county obesity rates (right). The top-row bins are computed based on statistical properties, while the bottom-row bins are computed by Oscar. Semantic bins have benefits for legibility, reducing the number of bins (i.e., the visual complexity of the map or histogram), and taking advantage of non-uniformity to either highlight areas of interest or compress long tails of the distribution into single bins.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "https://youtu.be/FgQzahCagPs",
                        "ff_id": "FgQzahCagPs"
                    },
                    {
                        "slot_id": "v-short-1100-qa",
                        "session_id": "short4",
                        "type": "In Person Q+A",
                        "title": "OSCAR: A Semantic-based Data Binning Approach (Q+A)",
                        "contributors": [
                            "Vidya Setlur"
                        ],
                        "authors": [],
                        "abstract": "Binning is applied to categorize data values or to see distributions of data. Existing binning algorithms often rely on statistical properties of data. However, there are semantic considerations for selecting appropriate binning schemes. Surveys, for instance, gather respondent data for demographic-related questions such as age, salary, number of employees, etc., that are bucketed into defined semantic categories. In this paper, we leverage common semantic categories from survey data and Tableau Public visualizations to identify a set of semantic binning categories. We employ these semantic binning categories in OSCAR: a method for automatically selecting bins based on the inferred semantic type of the field. We conducted a crowdsourced study with 120 participants to better understand user preferences for bins generated by OSCAR vs. binning provided in Tableau. We find that maps and histograms using binned values generated by OSCAR are preferred by users as compared to binning schemes based purely on the statistical properties of the data.",
                        "uid": "v-short-1100",
                        "file_name": "",
                        "time_stamp": "2022-10-20T14:34:00Z",
                        "time_start": "2022-10-20T14:34:00Z",
                        "time_end": "2022-10-20T14:36:00Z",
                        "paper_type": "short",
                        "keywords": [
                            "Data-driven semantics, binning, constraints, geospatial"
                        ],
                        "has_image": "1",
                        "has_video": "382",
                        "paper_award": "",
                        "image_caption": "Visualizations showing comparisons of bins for data on per-country life expectancy (left) and per-U.S. county obesity rates (right). The top-row bins are computed based on statistical properties, while the bottom-row bins are computed by Oscar. Semantic bins have benefits for legibility, reducing the number of bins (i.e., the visual complexity of the map or histogram), and taking advantage of non-uniformity to either highlight areas of interest or compress long tails of the distribution into single bins.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "https://youtu.be/FgQzahCagPs",
                        "ff_id": "FgQzahCagPs"
                    },
                    {
                        "slot_id": "v-short-1143-pres",
                        "session_id": "short4",
                        "type": "Virtual Presentation (pre-recorded)",
                        "title": "Towards Systematic Design Considerations of Organizing Multiple Views",
                        "contributors": [
                            "Abdul Rahman Shaikh"
                        ],
                        "authors": [
                            "Abdul Rahman Shaikh",
                            "David Koop",
                            "Hamed Alhoori",
                            "Maoyuan Sun"
                        ],
                        "abstract": "Multiple-view visualization (MV) has been used for visual analytics in various fields (e.g., bioinformatics, cybersecurity, and intelligence analysis). Because each view encodes data from a particular perspective, analysts often use a set of views laid out in 2D space to link and synthesize information. The difficulty of this process is impacted by the spatial organization of these views. For instance, connecting information from views far from each other can be more challenging than neighboring ones. However, most visual analysis tools currently either fix the positions of the views or completely delegate this organization of views to users (who must manually drag and move views). This either limits user involvement in managing the layout of MV or is overly flexible without much guidance. Then, a key design challenge in MV layout is determining the factors in a spatial organization that impact understanding. To address this, we review a set of MV-based systems and identify considerations for MV layout rooted in two key concerns: perception, which considers how users perceive view relationships, and content, which considers the relationships in the data. We show how these allow us to study and analyze the design of MV layout systematically.",
                        "uid": "v-short-1143",
                        "file_name": "v-short-1143_Shaikh_Presentation.mp4",
                        "time_stamp": "2022-10-20T14:36:00Z",
                        "time_start": "2022-10-20T14:36:00Z",
                        "time_end": "2022-10-20T14:42:54Z",
                        "paper_type": "short",
                        "keywords": [
                            "Multiple views, visual analytics, spatial layout"
                        ],
                        "has_image": "1",
                        "has_video": "414",
                        "paper_award": "",
                        "image_caption": "Examples of MV layout designs based on factors of user perception (a-g) and view content (h-j), which highlights broadening the design considerations from purely perception-driven to intelligently content-driven.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "https://youtu.be/PMn4QIJYMSQ",
                        "ff_id": "PMn4QIJYMSQ"
                    },
                    {
                        "slot_id": "v-short-1143-qa",
                        "session_id": "short4",
                        "type": "Virtual Q+A",
                        "title": "Towards Systematic Design Considerations of Organizing Multiple Views (Q+A)",
                        "contributors": [
                            "Abdul Rahman Shaikh"
                        ],
                        "authors": [],
                        "abstract": "Multiple-view visualization (MV) has been used for visual analytics in various fields (e.g., bioinformatics, cybersecurity, and intelligence analysis). Because each view encodes data from a particular perspective, analysts often use a set of views laid out in 2D space to link and synthesize information. The difficulty of this process is impacted by the spatial organization of these views. For instance, connecting information from views far from each other can be more challenging than neighboring ones. However, most visual analysis tools currently either fix the positions of the views or completely delegate this organization of views to users (who must manually drag and move views). This either limits user involvement in managing the layout of MV or is overly flexible without much guidance. Then, a key design challenge in MV layout is determining the factors in a spatial organization that impact understanding. To address this, we review a set of MV-based systems and identify considerations for MV layout rooted in two key concerns: perception, which considers how users perceive view relationships, and content, which considers the relationships in the data. We show how these allow us to study and analyze the design of MV layout systematically.",
                        "uid": "v-short-1143",
                        "file_name": "",
                        "time_stamp": "2022-10-20T14:42:54Z",
                        "time_start": "2022-10-20T14:42:54Z",
                        "time_end": "2022-10-20T14:45:00Z",
                        "paper_type": "short",
                        "keywords": [
                            "Multiple views, visual analytics, spatial layout"
                        ],
                        "has_image": "1",
                        "has_video": "414",
                        "paper_award": "",
                        "image_caption": "Examples of MV layout designs based on factors of user perception (a-g) and view content (h-j), which highlights broadening the design considerations from purely perception-driven to intelligently content-driven.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "https://youtu.be/PMn4QIJYMSQ",
                        "ff_id": "PMn4QIJYMSQ"
                    },
                    {
                        "slot_id": "v-short-1133-pres",
                        "session_id": "short4",
                        "type": "Virtual Presentation (pre-recorded)",
                        "title": "Toward Systematic Considerations of Missingness in Visual Analytics",
                        "contributors": [
                            "Maoyuan Sun"
                        ],
                        "authors": [
                            "Maoyuan Sun",
                            "Yue Ma",
                            "Yuanxin Wang",
                            "Tianyi Li",
                            "Jian Zhao",
                            "Yujun Liu",
                            "Ping-Shou Zhong"
                        ],
                        "abstract": "Data-driven decision making has been a common task in today\u2019s big data era, from simple choices such as finding a fast way to drive home, to complex decisions on medical treatment. It is often supported by visual analytics. For various reasons (e.g., system failure, interrupted network, intentional information hiding, or bias), visual analytics for sensemaking of data involves missingness (e.g., data loss and incomplete analysis), which impacts human decisions. For example, missing data can cost a business millions of dollars, and failing to recognize key evidence can put an innocent person in jail. Being aware of missingness is critical to avoid such catastrophes. To fulfill this, as an initial step, we consider missingness in visual analytics from two aspects: data-centric and human-centric. The former emphasizes missingness in three data-related categories: data composition, data relationship, and data usage. The latter focuses on the human-perceived missingness at three levels: observed-level, inferred-level, and ignored-level. Based on them, we discuss possible roles of visualizations for handling missingness, and conclude our discussion with future research opportunities.",
                        "uid": "v-short-1133",
                        "file_name": "v-short-1133_Sun_Presentation.mp4",
                        "time_stamp": "2022-10-20T14:45:00Z",
                        "time_start": "2022-10-20T14:45:00Z",
                        "time_end": "2022-10-20T14:52:01Z",
                        "paper_type": "short",
                        "keywords": [
                            "Missingness, missing data visualization, sensemaking, visual analytics"
                        ],
                        "has_image": "1",
                        "has_video": "421",
                        "paper_award": "",
                        "image_caption": "Examples corresponding to the data-centric view of missingness.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "https://youtu.be/Owu3hLicrGE",
                        "ff_id": "Owu3hLicrGE"
                    },
                    {
                        "slot_id": "v-short-1133-qa",
                        "session_id": "short4",
                        "type": "Virtual Q+A",
                        "title": "Toward Systematic Considerations of Missingness in Visual Analytics (Q+A)",
                        "contributors": [
                            "Maoyuan Sun"
                        ],
                        "authors": [],
                        "abstract": "Data-driven decision making has been a common task in today\u2019s big data era, from simple choices such as finding a fast way to drive home, to complex decisions on medical treatment. It is often supported by visual analytics. For various reasons (e.g., system failure, interrupted network, intentional information hiding, or bias), visual analytics for sensemaking of data involves missingness (e.g., data loss and incomplete analysis), which impacts human decisions. For example, missing data can cost a business millions of dollars, and failing to recognize key evidence can put an innocent person in jail. Being aware of missingness is critical to avoid such catastrophes. To fulfill this, as an initial step, we consider missingness in visual analytics from two aspects: data-centric and human-centric. The former emphasizes missingness in three data-related categories: data composition, data relationship, and data usage. The latter focuses on the human-perceived missingness at three levels: observed-level, inferred-level, and ignored-level. Based on them, we discuss possible roles of visualizations for handling missingness, and conclude our discussion with future research opportunities.",
                        "uid": "v-short-1133",
                        "file_name": "",
                        "time_stamp": "2022-10-20T14:52:01Z",
                        "time_start": "2022-10-20T14:52:01Z",
                        "time_end": "2022-10-20T14:54:00Z",
                        "paper_type": "short",
                        "keywords": [
                            "Missingness, missing data visualization, sensemaking, visual analytics"
                        ],
                        "has_image": "1",
                        "has_video": "421",
                        "paper_award": "",
                        "image_caption": "Examples corresponding to the data-centric view of missingness.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "https://youtu.be/Owu3hLicrGE",
                        "ff_id": "Owu3hLicrGE"
                    },
                    {
                        "slot_id": "v-short-1061-pres",
                        "session_id": "short4",
                        "type": "Virtual Presentation (pre-recorded)",
                        "title": "The role of extended reality for planning coronary artery bypass graft surgery",
                        "contributors": [
                            "Prof. Amanda Randles",
                            "David Urick"
                        ],
                        "authors": [
                            "Madhurima Vardhan",
                            "Harvey Shi",
                            "David Urick",
                            "Manesh Patel",
                            "Jane A. Leopold",
                            "Amanda Randles"
                        ],
                        "abstract": "Immersive visual displays are becoming more common in the diagnostic imaging and pre-procedural planning of complex cardiology revascularization surgeries. One such procedure is coronary artery bypass grafting (CABG) surgery, which is a gold standard treatment for patients with advanced coronary heart disease. Treatment planning of the CABG surgery can be aided by extended reality (XR) displays as they are known for offering advantageous visualization of spatially heterogeneous and complex tasks. Despite the benefits of XR, it remains unknown whether clinicians will benefit from higher visual immersion offered by XR. In order to assess the impact of increased immersion as well as the latent factor of geometrical complexity, a quantitative user evaluation (n=14) was performed with clinicians of advanced cardiology training simulating CABG placement on sixteen 3D arterial tree models derived from 6 patients two levels of anatomic complexity. These arterial models were rendered on 3D/XR and 2D display modes with the same tactile interaction input device. The findings of this study reveal that compared to a monoscopic 2D display, the greater visual immersion of 3D/XR does not significantly alter clinician accuracy in the task of bypass graft placement. Latent factors such as arterial complexity and clinical experience both influence the accuracy of graft placement. In addition, an anatomically less complex model and greater clinical experience improve accuracy on both 3D/XR and 2D display modes. The findings of this study can help inform design guidelines of efficient and robust XR tools for pre-procedural planning of complex coronary revascularization surgeries such as CABG.",
                        "uid": "v-short-1061",
                        "file_name": "v-short-1061_Vardhan_Presentation.mp4",
                        "time_stamp": "2022-10-20T14:54:00Z",
                        "time_start": "2022-10-20T14:54:00Z",
                        "time_end": "2022-10-20T15:01:31Z",
                        "paper_type": "short",
                        "keywords": [
                            "extended reality, coronary artery bypass graft surgery, anatomic complexity, treatment planning, stereoscopic and monoscopic displays"
                        ],
                        "has_image": "1",
                        "has_video": "451",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "https://youtu.be/qA44CGdneaE",
                        "ff_id": "qA44CGdneaE"
                    },
                    {
                        "slot_id": "v-short-1061-qa",
                        "session_id": "short4",
                        "type": "Virtual Q+A",
                        "title": "The role of extended reality for planning coronary artery bypass graft surgery (Q+A)",
                        "contributors": [
                            "Prof. Amanda Randles",
                            "David Urick"
                        ],
                        "authors": [],
                        "abstract": "Immersive visual displays are becoming more common in the diagnostic imaging and pre-procedural planning of complex cardiology revascularization surgeries. One such procedure is coronary artery bypass grafting (CABG) surgery, which is a gold standard treatment for patients with advanced coronary heart disease. Treatment planning of the CABG surgery can be aided by extended reality (XR) displays as they are known for offering advantageous visualization of spatially heterogeneous and complex tasks. Despite the benefits of XR, it remains unknown whether clinicians will benefit from higher visual immersion offered by XR. In order to assess the impact of increased immersion as well as the latent factor of geometrical complexity, a quantitative user evaluation (n=14) was performed with clinicians of advanced cardiology training simulating CABG placement on sixteen 3D arterial tree models derived from 6 patients two levels of anatomic complexity. These arterial models were rendered on 3D/XR and 2D display modes with the same tactile interaction input device. The findings of this study reveal that compared to a monoscopic 2D display, the greater visual immersion of 3D/XR does not significantly alter clinician accuracy in the task of bypass graft placement. Latent factors such as arterial complexity and clinical experience both influence the accuracy of graft placement. In addition, an anatomically less complex model and greater clinical experience improve accuracy on both 3D/XR and 2D display modes. The findings of this study can help inform design guidelines of efficient and robust XR tools for pre-procedural planning of complex coronary revascularization surgeries such as CABG.",
                        "uid": "v-short-1061",
                        "file_name": "",
                        "time_stamp": "2022-10-20T15:01:31Z",
                        "time_start": "2022-10-20T15:01:31Z",
                        "time_end": "2022-10-20T15:03:00Z",
                        "paper_type": "short",
                        "keywords": [
                            "extended reality, coronary artery bypass graft surgery, anatomic complexity, treatment planning, stereoscopic and monoscopic displays"
                        ],
                        "has_image": "1",
                        "has_video": "451",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "https://youtu.be/qA44CGdneaE",
                        "ff_id": "qA44CGdneaE"
                    },
                    {
                        "slot_id": "v-short-1092-pres",
                        "session_id": "short4",
                        "type": "In Person Presentation",
                        "title": "ARShopping: In-Store Shopping Decision Support Through Augmented Reality and Immersive Visualization",
                        "contributors": [
                            "Shunan Guo"
                        ],
                        "authors": [
                            "Bingjie Xu",
                            "Shunan Guo",
                            "Eunyee Koh",
                            "Jane Hoffswell",
                            "Ryan Rossi",
                            "Fan Du"
                        ],
                        "abstract": "Online shopping gives customers boundless options to choose from, backed by extensive product details and customer reviews, all from the comfort of home; yet, no amount of detailed, online information can outweigh the instant gratification and hands-on understanding of a product that is provided by physical stores. However, making purchasing decisions in physical stores can be challenging due to a large number of similar alternatives and limited accessibility of the relevant product information (e.g., features, ratings, and reviews). In this work, we present ARShopping: a web-based prototype to visually communicate detailed product information from an online setting on portable smart devices (e.g., phones, tablets, glasses), within the physical space at the point of purchase. This prototype uses augmented reality (AR) to identify products and display detailed information to help consumers make purchasing decisions that fulfill their needs while decreasing the decision-making time. In particular, we use a data fusion algorithm to improve the precision of the product detection; we then integrate AR visualizations into the scene to facilitate comparisons across multiple products and features. We designed our prototype based on interviews with 14 participants to better understand the utility and ease of use of the prototype.",
                        "uid": "v-short-1092",
                        "file_name": "v-short-1092_Xu_Presentation.mp4",
                        "time_stamp": "2022-10-20T15:03:00Z",
                        "time_start": "2022-10-20T15:03:00Z",
                        "time_end": "2022-10-20T15:10:00Z",
                        "paper_type": "short",
                        "keywords": [
                            "Human-centered computing\u2014Visualization\u2014Visualization systems and tools; Human-centered computing\u2014Human computer interaction (HCI)\u2014Interaction paradigms\u2014Mixed / augmented reality"
                        ],
                        "has_image": "1",
                        "has_video": "451",
                        "paper_award": "",
                        "image_caption": "ARShopping detects markers attached to the label of the product and also the product object to identify the type of products in the scene. Through the app, user can (a) filter products based on certain features, (b) compare product features intuitively through visualization glyphs, (c) bookmark products to collection for (d) further investigation on more product features and customer reviews.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "https://youtu.be/3lvcgcq1Zr8",
                        "ff_id": "3lvcgcq1Zr8"
                    },
                    {
                        "slot_id": "v-short-1092-qa",
                        "session_id": "short4",
                        "type": "In Person Q+A",
                        "title": "ARShopping: In-Store Shopping Decision Support Through Augmented Reality and Immersive Visualization (Q+A)",
                        "contributors": [
                            "Shunan Guo"
                        ],
                        "authors": [],
                        "abstract": "Online shopping gives customers boundless options to choose from, backed by extensive product details and customer reviews, all from the comfort of home; yet, no amount of detailed, online information can outweigh the instant gratification and hands-on understanding of a product that is provided by physical stores. However, making purchasing decisions in physical stores can be challenging due to a large number of similar alternatives and limited accessibility of the relevant product information (e.g., features, ratings, and reviews). In this work, we present ARShopping: a web-based prototype to visually communicate detailed product information from an online setting on portable smart devices (e.g., phones, tablets, glasses), within the physical space at the point of purchase. This prototype uses augmented reality (AR) to identify products and display detailed information to help consumers make purchasing decisions that fulfill their needs while decreasing the decision-making time. In particular, we use a data fusion algorithm to improve the precision of the product detection; we then integrate AR visualizations into the scene to facilitate comparisons across multiple products and features. We designed our prototype based on interviews with 14 participants to better understand the utility and ease of use of the prototype.",
                        "uid": "v-short-1092",
                        "file_name": "",
                        "time_stamp": "2022-10-20T15:10:00Z",
                        "time_start": "2022-10-20T15:10:00Z",
                        "time_end": "2022-10-20T15:12:00Z",
                        "paper_type": "short",
                        "keywords": [
                            "Human-centered computing\u2014Visualization\u2014Visualization systems and tools; Human-centered computing\u2014Human computer interaction (HCI)\u2014Interaction paradigms\u2014Mixed / augmented reality"
                        ],
                        "has_image": "1",
                        "has_video": "451",
                        "paper_award": "",
                        "image_caption": "ARShopping detects markers attached to the label of the product and also the product object to identify the type of products in the scene. Through the app, user can (a) filter products based on certain features, (b) compare product features intuitively through visualization glyphs, (c) bookmark products to collection for (d) further investigation on more product features and customer reviews.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "https://youtu.be/3lvcgcq1Zr8",
                        "ff_id": "3lvcgcq1Zr8"
                    }
                ]
            }
        ]
    },
    "v-tvcg": {
        "event": "TVCG Invited Partnership Presentations",
        "long_name": "TVCG Invited Partnership Presentations",
        "event_type": "Invited Partnership Presentations",
        "event_prefix": "v-tvcg",
        "event_description": "",
        "event_url": "",
        "organizers": [],
        "sessions": []
    },
    "v-cga": {
        "event": "CG&A Invited Partnership Presentations",
        "long_name": "CG&A Invited Partnership Presentations",
        "event_type": "Invited Partnership Presentations",
        "event_prefix": "v-cga",
        "event_description": "",
        "event_url": "",
        "organizers": [],
        "sessions": [
            {
                "title": "Visualization Teaching and Literacy (cont.) and Machine Learning for Vis.",
                "session_id": "cga1",
                "event_prefix": "v-cga",
                "track": "ok6",
                "livestream_id": "ok6-thu",
                "session_image": "cga1.png",
                "chair": [
                    "Jieqiong Zhao"
                ],
                "organizers": [],
                "time_start": "2022-10-20T20:45:00Z",
                "time_end": "2022-10-20T22:00:00Z",
                "discord_category": "",
                "discord_channel": "oklahoma-st-6",
                "discord_channel_id": "1024600906689421372",
                "discord_link": "https://discord.com/channels/978320435554947082/1024600906689421372",
                "slido_link": "https://app.sli.do/event/i28UETZaiRWr1XwX6eKjcf?section=29ba1f79-04cd-4568-a83c-ab81aa3c28b8",
                "youtube_url": "https://youtu.be/KIIASF6gEIE",
                "youtube_id": "KIIASF6gEIE",
                "zoom_meeting": "",
                "zoom_password": "",
                "zoom_link": "",
                "ff_link": "https://youtu.be/dDfW4fJsiV4",
                "ff_playlist": "",
                "time_slots": [
                    {
                        "slot_id": "cga1-opening",
                        "session_id": "cga1",
                        "type": "In Person Other",
                        "title": "Session Opening",
                        "contributors": [
                            "Jieqiong Zhao"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "",
                        "file_name": "",
                        "time_stamp": "2022-10-20T20:45:00Z",
                        "time_start": "2022-10-20T20:45:00Z",
                        "time_end": "2022-10-20T20:45:00Z",
                        "paper_type": "",
                        "keywords": [],
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "v-cga-9547792-pres",
                        "session_id": "cga1",
                        "type": "Virtual Presentation (pre-recorded)",
                        "title": "What Students Learn With Personal Data Physicalization",
                        "contributors": [
                            "Charles Perin"
                        ],
                        "authors": [
                            "Charles Perin"
                        ],
                        "abstract": "",
                        "uid": "v-cga-9547792",
                        "file_name": "v-cga-9547792_Perin_Presentation.mp4",
                        "time_stamp": "2022-10-20T20:45:00Z",
                        "time_start": "2022-10-20T20:45:00Z",
                        "time_end": "2022-10-20T20:54:27Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "567",
                        "paper_award": "",
                        "image_caption": "Some of the Personal Data Physicalizations created by students for the assignment of the same name.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "https://youtu.be/xZ1itYz-QhQ",
                        "ff_id": "xZ1itYz-QhQ"
                    },
                    {
                        "slot_id": "v-cga-9547792-qa",
                        "session_id": "cga1",
                        "type": "Virtual Q+A",
                        "title": "What Students Learn With Personal Data Physicalization (Q+A)",
                        "contributors": [
                            "Charles Perin"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "v-cga-9547792",
                        "file_name": "",
                        "time_stamp": "2022-10-20T20:54:27Z",
                        "time_start": "2022-10-20T20:54:27Z",
                        "time_end": "2022-10-20T20:57:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "567",
                        "paper_award": "",
                        "image_caption": "Some of the Personal Data Physicalizations created by students for the assignment of the same name.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "https://youtu.be/xZ1itYz-QhQ",
                        "ff_id": "xZ1itYz-QhQ"
                    },
                    {
                        "slot_id": "v-cga-9476996-pres",
                        "session_id": "cga1",
                        "type": "In Person Presentation",
                        "title": "DeepGD: A Deep Learning Framework for Graph Drawing Using GNN",
                        "contributors": [
                            "Xiaoqi Wang"
                        ],
                        "authors": [
                            "Xiaoqi Wang",
                            "Kevin Yen",
                            "Yifan Hu",
                            "Han-Wei Shen"
                        ],
                        "abstract": "",
                        "uid": "v-cga-9476996",
                        "file_name": "v-cga-9476996_Wang_Presentation.mp4",
                        "time_stamp": "2022-10-20T20:57:00Z",
                        "time_start": "2022-10-20T20:57:00Z",
                        "time_end": "2022-10-20T21:07:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "542",
                        "paper_award": "",
                        "image_caption": "We propose a novel Graph Neural Network-based deep learning framework for graph drawing, which can optimize the generated graph layouts toward any differentiable aesthetic metric. Given the fact that a visually pleasing graph layout usually complies with multiple aesthetic aspects, our method attempts to optimize multiple aesthetics simultaneously.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "https://youtu.be/ozGsWy26aRs",
                        "ff_id": "ozGsWy26aRs"
                    },
                    {
                        "slot_id": "v-cga-9476996-qa",
                        "session_id": "cga1",
                        "type": "In Person Q+A",
                        "title": "DeepGD: A Deep Learning Framework for Graph Drawing Using GNN (Q+A)",
                        "contributors": [
                            "Xiaoqi Wang"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "v-cga-9476996",
                        "file_name": "",
                        "time_stamp": "2022-10-20T21:07:00Z",
                        "time_start": "2022-10-20T21:07:00Z",
                        "time_end": "2022-10-20T21:09:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "542",
                        "paper_award": "",
                        "image_caption": "We propose a novel Graph Neural Network-based deep learning framework for graph drawing, which can optimize the generated graph layouts toward any differentiable aesthetic metric. Given the fact that a visually pleasing graph layout usually complies with multiple aesthetic aspects, our method attempts to optimize multiple aesthetics simultaneously.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "https://youtu.be/ozGsWy26aRs",
                        "ff_id": "ozGsWy26aRs"
                    },
                    {
                        "slot_id": "v-cga-9490338-pres",
                        "session_id": "cga1",
                        "type": "In Person Presentation",
                        "title": "Interactive Visualization of Hyperspectral Images based on Neural Networks",
                        "contributors": [
                            "Hongfeng Yu",
                            "Xinyan Xie"
                        ],
                        "authors": [
                            "Feiyu Zhu",
                            "Yu Pan",
                            "Tian Gao",
                            "Harkamal Walia",
                            "Hongfeng Yu"
                        ],
                        "abstract": "",
                        "uid": "v-cga-9490338",
                        "file_name": "v-cga-9490338_Zhu_Presentation.mp4",
                        "time_stamp": "2022-10-20T21:09:00Z",
                        "time_start": "2022-10-20T21:09:00Z",
                        "time_end": "2022-10-20T21:19:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "509",
                        "paper_award": "",
                        "image_caption": "User interface of the visualization tool and its classification results for hyperspectal images based on neural networks.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "https://youtu.be/M3Lem9neOvw",
                        "ff_id": "M3Lem9neOvw"
                    },
                    {
                        "slot_id": "v-cga-9490338-qa",
                        "session_id": "cga1",
                        "type": "In Person Q+A",
                        "title": "Interactive Visualization of Hyperspectral Images based on Neural Networks (Q+A)",
                        "contributors": [
                            "Hongfeng Yu",
                            "Xinyan Xie"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "v-cga-9490338",
                        "file_name": "",
                        "time_stamp": "2022-10-20T21:19:00Z",
                        "time_start": "2022-10-20T21:19:00Z",
                        "time_end": "2022-10-20T21:21:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "509",
                        "paper_award": "",
                        "image_caption": "User interface of the visualization tool and its classification results for hyperspectal images based on neural networks.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "https://youtu.be/M3Lem9neOvw",
                        "ff_id": "M3Lem9neOvw"
                    },
                    {
                        "slot_id": "v-cga-9488227-pres",
                        "session_id": "cga1",
                        "type": "Virtual Presentation (pre-recorded)",
                        "title": "STSRNet: Deep Joint Space\u2013Time Super-Resolution for Vector Field Visualization",
                        "contributors": [
                            "Guihua Shan"
                        ],
                        "authors": [
                            "Yifei An",
                            "Han-Wei Shen",
                            "Guihua Shan",
                            "Guan Li",
                            "Jun Liu"
                        ],
                        "abstract": "",
                        "uid": "v-cga-9488227",
                        "file_name": "v-cga-9488227_Shan_Presentation.mp4",
                        "time_stamp": "2022-10-20T21:21:00Z",
                        "time_start": "2022-10-20T21:21:00Z",
                        "time_end": "2022-10-20T21:30:55Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "595",
                        "paper_award": "",
                        "image_caption": "In this paper, we proposed a joint space-time super-resolution deep learning-based model to reconstruct high temporal and spatial resolution vector field sequences. The model consists of a temporal super-resolution model and a spatial super-resolution, using a physically based loss function combined with temporal coherence for reconstructing vectors. We proved the effectiveness of our model on different datasets through quantitative and qualitative evaluations.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "https://youtu.be/zuS2iWpOrIw",
                        "ff_id": "zuS2iWpOrIw"
                    },
                    {
                        "slot_id": "v-cga-9488227-qa",
                        "session_id": "cga1",
                        "type": "Virtual Q+A",
                        "title": "STSRNet: Deep Joint Space\u2013Time Super-Resolution for Vector Field Visualization (Q+A)",
                        "contributors": [
                            "Guihua Shan"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "v-cga-9488227",
                        "file_name": "",
                        "time_stamp": "2022-10-20T21:30:55Z",
                        "time_start": "2022-10-20T21:30:55Z",
                        "time_end": "2022-10-20T21:33:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "595",
                        "paper_award": "",
                        "image_caption": "In this paper, we proposed a joint space-time super-resolution deep learning-based model to reconstruct high temporal and spatial resolution vector field sequences. The model consists of a temporal super-resolution model and a spatial super-resolution, using a physically based loss function combined with temporal coherence for reconstructing vectors. We proved the effectiveness of our model on different datasets through quantitative and qualitative evaluations.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "https://youtu.be/zuS2iWpOrIw",
                        "ff_id": "zuS2iWpOrIw"
                    },
                    {
                        "slot_id": "v-cga-9495208-pres",
                        "session_id": "cga1",
                        "type": "Virtual Presentation (pre-recorded)",
                        "title": "Visual Clustering Factors in Scatterplots",
                        "contributors": [
                            "Weixing Lin"
                        ],
                        "authors": [
                            "Jiazhi Xia",
                            "Weixing Lin",
                            "Guang Jiang",
                            "Yunhai Wang",
                            "Wei Chen",
                            "Tobias Schreck"
                        ],
                        "abstract": "",
                        "uid": "v-cga-9495208",
                        "file_name": "v-cga-9495208_Xia_Presentation.mp4",
                        "time_stamp": "2022-10-20T21:33:00Z",
                        "time_start": "2022-10-20T21:33:00Z",
                        "time_end": "2022-10-20T21:42:38Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "578",
                        "paper_award": "",
                        "image_caption": "What are the influence factors of visual clustering in scatterplots? We conduct a data-driven study for this question.\nThe study shows that shape and area are not influencing factors. The factors texture, distance, position, density, angle, and noise are influencing factors.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "https://youtu.be/DK89RKCTzbY",
                        "ff_id": "DK89RKCTzbY"
                    },
                    {
                        "slot_id": "v-cga-9495208-qa",
                        "session_id": "cga1",
                        "type": "Virtual Q+A",
                        "title": "Visual Clustering Factors in Scatterplots (Q+A)",
                        "contributors": [
                            "Weixing Lin"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "v-cga-9495208",
                        "file_name": "",
                        "time_stamp": "2022-10-20T21:42:38Z",
                        "time_start": "2022-10-20T21:42:38Z",
                        "time_end": "2022-10-20T21:45:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "578",
                        "paper_award": "",
                        "image_caption": "What are the influence factors of visual clustering in scatterplots? We conduct a data-driven study for this question.\nThe study shows that shape and area are not influencing factors. The factors texture, distance, position, density, angle, and noise are influencing factors.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "https://youtu.be/DK89RKCTzbY",
                        "ff_id": "DK89RKCTzbY"
                    },
                    {
                        "slot_id": "v-cga-9238399-pres",
                        "session_id": "cga1",
                        "type": "In Person Presentation",
                        "title": "Cartolabe: A Web-Based Scalable Visualization of Large Document Collections",
                        "contributors": [
                            "Jean-Daniel Fekete"
                        ],
                        "authors": [
                            "Philippe Caillou",
                            "Jonas Renault",
                            "Jean-Daniel Fekete",
                            "Anne-Catherine Letournel",
                            "Mich\u00e8le Sebag"
                        ],
                        "abstract": "",
                        "uid": "v-cga-9238399",
                        "file_name": "v-cga-9238399_Fekete_Presentation.mp4",
                        "time_stamp": "2022-10-20T21:45:00Z",
                        "time_start": "2022-10-20T21:45:00Z",
                        "time_end": "2022-10-20T21:55:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "531",
                        "paper_award": "",
                        "image_caption": "Cartolabe visualization of the HAL scientific repository, containing all the French scientific articles (about 1 million) and authors (about 2 millions). Blue points represent articles and red points authors.\nThe layout is computed using the UMAP projection so that close points relate to similar entites whereas distant points are less similar.\nOne author is selected (Jean-Daniel Fekete) and  highlighted, along with his 10 nearest neighbors.\nThe visualization is available at https://cartolabe.fr and allow exploring large document collections, such as arXiv, HAL, and wikipedia.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "https://youtu.be/oSiZmN-jQL0",
                        "ff_id": "oSiZmN-jQL0"
                    },
                    {
                        "slot_id": "v-cga-9238399-qa",
                        "session_id": "cga1",
                        "type": "In Person Q+A",
                        "title": "Cartolabe: A Web-Based Scalable Visualization of Large Document Collections (Q+A)",
                        "contributors": [
                            "Jean-Daniel Fekete"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "v-cga-9238399",
                        "file_name": "",
                        "time_stamp": "2022-10-20T21:55:00Z",
                        "time_start": "2022-10-20T21:55:00Z",
                        "time_end": "2022-10-20T21:57:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "531",
                        "paper_award": "",
                        "image_caption": "Cartolabe visualization of the HAL scientific repository, containing all the French scientific articles (about 1 million) and authors (about 2 millions). Blue points represent articles and red points authors.\nThe layout is computed using the UMAP projection so that close points relate to similar entites whereas distant points are less similar.\nOne author is selected (Jean-Daniel Fekete) and  highlighted, along with his 10 nearest neighbors.\nThe visualization is available at https://cartolabe.fr and allow exploring large document collections, such as arXiv, HAL, and wikipedia.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "https://youtu.be/oSiZmN-jQL0",
                        "ff_id": "oSiZmN-jQL0"
                    }
                ]
            },
            {
                "title": "Visualization Teaching and Literacy",
                "session_id": "cga2",
                "event_prefix": "v-cga",
                "track": "ok6",
                "livestream_id": "ok6-thu",
                "session_image": "cga2.png",
                "chair": [
                    "Tamara Munzner"
                ],
                "organizers": [],
                "time_start": "2022-10-20T19:00:00Z",
                "time_end": "2022-10-20T20:15:00Z",
                "discord_category": "",
                "discord_channel": "oklahoma-st-6",
                "discord_channel_id": "1024600906689421372",
                "discord_link": "https://discord.com/channels/978320435554947082/1024600906689421372",
                "slido_link": "https://app.sli.do/event/i28UETZaiRWr1XwX6eKjcf?section=29ba1f79-04cd-4568-a83c-ab81aa3c28b8",
                "youtube_url": "https://youtu.be/KIIASF6gEIE",
                "youtube_id": "KIIASF6gEIE",
                "zoom_meeting": "",
                "zoom_password": "",
                "zoom_link": "",
                "ff_link": "https://youtu.be/rq1qEefuSVU",
                "ff_playlist": "",
                "time_slots": [
                    {
                        "slot_id": "cga2-opening",
                        "session_id": "cga2",
                        "type": "In Person Other",
                        "title": "Session Opening",
                        "contributors": [
                            "Tamara Munzner"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "",
                        "file_name": "",
                        "time_stamp": "2022-10-20T19:00:00Z",
                        "time_start": "2022-10-20T19:00:00Z",
                        "time_end": "2022-10-20T19:00:00Z",
                        "paper_type": "",
                        "keywords": [],
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "v-cga-9556564-pres",
                        "session_id": "cga2",
                        "type": "Virtual Presentation (pre-recorded)",
                        "title": "A Taxonomy-Driven Model for Designing Educational Games in  Visualization",
                        "contributors": [
                            "Renata Raidou"
                        ],
                        "authors": [
                            "Lorenzo Amabili",
                            "Kuhu Gupta",
                            "Renata Georgia Raidou"
                        ],
                        "abstract": "",
                        "uid": "v-cga-9556564",
                        "file_name": "v-cga-9556564_Amabili_Presentation.mp4",
                        "time_stamp": "2022-10-20T19:00:00Z",
                        "time_start": "2022-10-20T19:00:00Z",
                        "time_end": "2022-10-20T19:10:05Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "605",
                        "paper_award": "",
                        "image_caption": "Prototype of the cards used for Guess Viz? and From A to viZ. The upper set of cards shows the proposed sliding cards: In the top layer, a visualization is shown. In the bottom layer, the related label and the visualization characteristics are represented. The lower set of cards shows the proposed legend cards: We encoded the type of visualization characteristic with color [i.e., data-related (light green), users-related (dark green), tasks-related (light blue), visual-variables-related (tan), visualization-vocabulary-related (pink)]. In Guess Viz?, players use both layers of the sliding cards. In From A to viZ, players use primarily the legend cards.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "https://youtu.be/nww4-mZErzM",
                        "ff_id": "nww4-mZErzM"
                    },
                    {
                        "slot_id": "v-cga-9556564-qa",
                        "session_id": "cga2",
                        "type": "Virtual Q+A",
                        "title": "A Taxonomy-Driven Model for Designing Educational Games in  Visualization (Q+A)",
                        "contributors": [
                            "Renata Raidou"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "v-cga-9556564",
                        "file_name": "",
                        "time_stamp": "2022-10-20T19:10:05Z",
                        "time_start": "2022-10-20T19:10:05Z",
                        "time_end": "2022-10-20T19:12:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "605",
                        "paper_award": "",
                        "image_caption": "Prototype of the cards used for Guess Viz? and From A to viZ. The upper set of cards shows the proposed sliding cards: In the top layer, a visualization is shown. In the bottom layer, the related label and the visualization characteristics are represented. The lower set of cards shows the proposed legend cards: We encoded the type of visualization characteristic with color [i.e., data-related (light green), users-related (dark green), tasks-related (light blue), visual-variables-related (tan), visualization-vocabulary-related (pink)]. In Guess Viz?, players use both layers of the sliding cards. In From A to viZ, players use primarily the legend cards.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "https://youtu.be/nww4-mZErzM",
                        "ff_id": "nww4-mZErzM"
                    },
                    {
                        "slot_id": "v-cga-9551781-pres",
                        "session_id": "cga2",
                        "type": "In Person Presentation",
                        "title": "Remote Instruction for Data Visualization Design-A Report From the Trenches",
                        "contributors": [
                            "Jan Aerts"
                        ],
                        "authors": [
                            "Jan Aerts",
                            "Jannes Peeters",
                            "Jelmer Bot",
                            "Danai Kafetzaki",
                            "Houda Lamqaddam"
                        ],
                        "abstract": "",
                        "uid": "v-cga-9551781",
                        "file_name": "v-cga-9551781_Aerts_Presentation.mp4",
                        "time_stamp": "2022-10-20T19:12:00Z",
                        "time_start": "2022-10-20T19:12:00Z",
                        "time_end": "2022-10-20T19:22:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "516",
                        "paper_award": "",
                        "image_caption": "Teaching how to explore visual design space to a large group of students is challenging, especially if it needs to happen in an online setting. Using a combination of gather.town and miro, we were able to provide the students an environment and framework in which they could still be creative in creating, discussing and reworking sketches. Gather.town was set up as a communication channel so that students could interact in a relatively natural and self-organising way. Sketches that they created individually at home were uploaded in miro and could be re-organised and annotated to lead to new versions.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "https://youtu.be/QDD_3lFKuIU",
                        "ff_id": "QDD_3lFKuIU"
                    },
                    {
                        "slot_id": "v-cga-9551781-qa",
                        "session_id": "cga2",
                        "type": "In Person Q+A",
                        "title": "Remote Instruction for Data Visualization Design-A Report From the Trenches (Q+A)",
                        "contributors": [
                            "Jan Aerts"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "v-cga-9551781",
                        "file_name": "",
                        "time_stamp": "2022-10-20T19:22:00Z",
                        "time_start": "2022-10-20T19:22:00Z",
                        "time_end": "2022-10-20T19:24:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "516",
                        "paper_award": "",
                        "image_caption": "Teaching how to explore visual design space to a large group of students is challenging, especially if it needs to happen in an online setting. Using a combination of gather.town and miro, we were able to provide the students an environment and framework in which they could still be creative in creating, discussing and reworking sketches. Gather.town was set up as a communication channel so that students could interact in a relatively natural and self-organising way. Sketches that they created individually at home were uploaded in miro and could be re-organised and annotated to lead to new versions.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "https://youtu.be/QDD_3lFKuIU",
                        "ff_id": "QDD_3lFKuIU"
                    },
                    {
                        "slot_id": "v-cga-9556143-pres",
                        "session_id": "cga2",
                        "type": "Virtual Presentation (pre-recorded)",
                        "title": "A Didactic Framework for Analyzing Learning Activities to Design InfoVis Courses",
                        "contributors": [
                            "Mandy Keck or Dietrich Kammer",
                            "Elena Stoll"
                        ],
                        "authors": [
                            "Mandy Keck",
                            "Elena Stoll",
                            "Dietrich Kammer"
                        ],
                        "abstract": "",
                        "uid": "v-cga-9556143",
                        "file_name": "v-cga-9556143_Keck_Presentation.mp4",
                        "time_stamp": "2022-10-20T19:24:00Z",
                        "time_start": "2022-10-20T19:24:00Z",
                        "time_end": "2022-10-20T19:32:33Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "513",
                        "paper_award": "",
                        "image_caption": "A Data Visualization Activity is a hands-on engagement with data visualization with the goal of learning, reflecting, discussing, or designing visualizations. Today, numerous data visualization activities are available to teach data visualization knowledge in a variety of contexts\u200b. Selecting one or more vis activity in comprehensive courses, however, remains a challenge. To support this process, we propose a didactic vis framework in which complex DataVis activities are broken down into multiple learning activities with different learning goals.\u200b A learning activity template, a learning activity matrix, and a didactic structure chart assist in planning, analyzing and adapting InfoVis courses.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "https://youtu.be/5QM8rz-K4RA",
                        "ff_id": "5QM8rz-K4RA"
                    },
                    {
                        "slot_id": "v-cga-9556143-qa",
                        "session_id": "cga2",
                        "type": "Virtual Q+A",
                        "title": "A Didactic Framework for Analyzing Learning Activities to Design InfoVis Courses (Q+A)",
                        "contributors": [
                            "Mandy Keck or Dietrich Kammer",
                            "Elena Stoll"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "v-cga-9556143",
                        "file_name": "",
                        "time_stamp": "2022-10-20T19:32:33Z",
                        "time_start": "2022-10-20T19:32:33Z",
                        "time_end": "2022-10-20T19:36:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "513",
                        "paper_award": "",
                        "image_caption": "A Data Visualization Activity is a hands-on engagement with data visualization with the goal of learning, reflecting, discussing, or designing visualizations. Today, numerous data visualization activities are available to teach data visualization knowledge in a variety of contexts\u200b. Selecting one or more vis activity in comprehensive courses, however, remains a challenge. To support this process, we propose a didactic vis framework in which complex DataVis activities are broken down into multiple learning activities with different learning goals.\u200b A learning activity template, a learning activity matrix, and a didactic structure chart assist in planning, analyzing and adapting InfoVis courses.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "https://youtu.be/5QM8rz-K4RA",
                        "ff_id": "5QM8rz-K4RA"
                    },
                    {
                        "slot_id": "v-cga-9547773-pres",
                        "session_id": "cga2",
                        "type": "In Person Presentation",
                        "title": "Through the Looking Glass: Insights Into Visualization Pedagogy Through Sentiment Analysis of Peer Review Text",
                        "contributors": [
                            "Paul Rosen"
                        ],
                        "authors": [
                            "Zachariah J. Beasley",
                            "Alon Friedman",
                            "Paul Rosen"
                        ],
                        "abstract": "",
                        "uid": "v-cga-9547773",
                        "file_name": "v-cga-9547773_Beasley_Presentation.mp4",
                        "time_stamp": "2022-10-20T19:36:00Z",
                        "time_start": "2022-10-20T19:36:00Z",
                        "time_end": "2022-10-20T19:46:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "510",
                        "paper_award": "",
                        "image_caption": "Illustrative example of a student\u2019s projects and the feedback they gave to their peers, reflecting applied concepts in Data Visualization.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "https://youtu.be/1ABlFHzFo9g",
                        "ff_id": "1ABlFHzFo9g"
                    },
                    {
                        "slot_id": "v-cga-9547773-qa",
                        "session_id": "cga2",
                        "type": "In Person Q+A",
                        "title": "Through the Looking Glass: Insights Into Visualization Pedagogy Through Sentiment Analysis of Peer Review Text (Q+A)",
                        "contributors": [
                            "Paul Rosen"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "v-cga-9547773",
                        "file_name": "",
                        "time_stamp": "2022-10-20T19:46:00Z",
                        "time_start": "2022-10-20T19:46:00Z",
                        "time_end": "2022-10-20T19:48:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "510",
                        "paper_award": "",
                        "image_caption": "Illustrative example of a student\u2019s projects and the feedback they gave to their peers, reflecting applied concepts in Data Visualization.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "https://youtu.be/1ABlFHzFo9g",
                        "ff_id": "1ABlFHzFo9g"
                    },
                    {
                        "slot_id": "v-cga-9547834-pres",
                        "session_id": "cga2",
                        "type": "In Person Presentation",
                        "title": "Visualization Design Sprints for Online and On-Campus Courses",
                        "contributors": [
                            "Johanna Beyer"
                        ],
                        "authors": [
                            "Johanna Beyer",
                            "Yalong Yang",
                            "Hanspeter Pfister"
                        ],
                        "abstract": "",
                        "uid": "v-cga-9547834",
                        "file_name": "v-cga-9547834_Beyer_Presentation.mp4",
                        "time_stamp": "2022-10-20T19:48:00Z",
                        "time_start": "2022-10-20T19:48:00Z",
                        "time_end": "2022-10-20T19:58:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "655",
                        "paper_award": "",
                        "image_caption": "Visualization design sprints are a learner-centered, project-based teaching approach for visualization courses. Students gain hands-on experience by following the five design sprint steps.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "https://youtu.be/YNu1cuClrAI",
                        "ff_id": "YNu1cuClrAI"
                    },
                    {
                        "slot_id": "v-cga-9547834-qa",
                        "session_id": "cga2",
                        "type": "In Person Q+A",
                        "title": "Visualization Design Sprints for Online and On-Campus Courses (Q+A)",
                        "contributors": [
                            "Johanna Beyer"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "v-cga-9547834",
                        "file_name": "",
                        "time_stamp": "2022-10-20T19:58:00Z",
                        "time_start": "2022-10-20T19:58:00Z",
                        "time_end": "2022-10-20T20:00:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "655",
                        "paper_award": "",
                        "image_caption": "Visualization design sprints are a learner-centered, project-based teaching approach for visualization courses. Students gain hands-on experience by following the five design sprint steps.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "https://youtu.be/YNu1cuClrAI",
                        "ff_id": "YNu1cuClrAI"
                    },
                    {
                        "slot_id": "v-cga-9547790-pres",
                        "session_id": "cga2",
                        "type": "In Person Presentation",
                        "title": "Activity Worksheets for Teaching and Learning Data Visualization",
                        "contributors": [
                            "Vetria Byrd"
                        ],
                        "authors": [
                            "Vetria L. Byrd",
                            "Nicole Dwenger"
                        ],
                        "abstract": "",
                        "uid": "v-cga-9547790",
                        "file_name": "v-cga-9547790_Byrd_Presentation.mp4",
                        "time_stamp": "2022-10-20T20:00:00Z",
                        "time_start": "2022-10-20T20:00:00Z",
                        "time_end": "2022-10-20T20:10:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "428",
                        "paper_award": "",
                        "image_caption": "This talk presents Activity Worksheets for Teaching and Learning Data Visualization.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "https://youtu.be/MQVi2wm2JhQ",
                        "ff_id": "MQVi2wm2JhQ"
                    },
                    {
                        "slot_id": "v-cga-9547790-qa",
                        "session_id": "cga2",
                        "type": "In Person Q+A",
                        "title": "Activity Worksheets for Teaching and Learning Data Visualization (Q+A)",
                        "contributors": [
                            "Vetria Byrd"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "v-cga-9547790",
                        "file_name": "",
                        "time_stamp": "2022-10-20T20:10:00Z",
                        "time_start": "2022-10-20T20:10:00Z",
                        "time_end": "2022-10-20T20:12:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "428",
                        "paper_award": "",
                        "image_caption": "This talk presents Activity Worksheets for Teaching and Learning Data Visualization.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "https://youtu.be/MQVi2wm2JhQ",
                        "ff_id": "MQVi2wm2JhQ"
                    }
                ]
            },
            {
                "title": "Visualization in Industry",
                "session_id": "cga3",
                "event_prefix": "v-cga",
                "track": "mistletoe",
                "livestream_id": "mistletoe-wed",
                "session_image": "cga3.png",
                "chair": [
                    "Michael Wybrow"
                ],
                "organizers": [],
                "time_start": "2022-10-19T20:45:00Z",
                "time_end": "2022-10-19T22:00:00Z",
                "discord_category": "",
                "discord_channel": "mistletoe",
                "discord_channel_id": "1024601055700471839",
                "discord_link": "https://discord.com/channels/978320435554947082/1024601055700471839",
                "slido_link": "https://app.sli.do/event/i28UETZaiRWr1XwX6eKjcf?section=05d57c7a-397b-4e16-a41d-7f06e8e64c37",
                "youtube_url": "https://youtu.be/xub7Gw386eY",
                "youtube_id": "xub7Gw386eY",
                "zoom_meeting": "",
                "zoom_password": "",
                "zoom_link": "",
                "ff_link": "https://youtu.be/NkZVvKLyMhU",
                "ff_playlist": "",
                "time_slots": [
                    {
                        "slot_id": "cga3-opening",
                        "session_id": "cga3",
                        "type": "In Person Other",
                        "title": "Session Opening",
                        "contributors": [
                            "Michael Wybrow"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "",
                        "file_name": "",
                        "time_stamp": "2022-10-19T20:45:00Z",
                        "time_start": "2022-10-19T20:45:00Z",
                        "time_end": "2022-10-19T20:45:00Z",
                        "paper_type": "",
                        "keywords": [],
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "v-cga-8948290-pres",
                        "session_id": "cga3",
                        "type": "Virtual Presentation (pre-recorded)",
                        "title": "Surgical Navigation System for Low-Dose-Rate Brachytherapy Based on Mixed Reality",
                        "contributors": [
                            "Zeyang Zhou"
                        ],
                        "authors": [
                            "Zeyang Zhou",
                            "Zhiyong Yang",
                            "Shan Jiang",
                            "Xiaodong Ma",
                            "Fujun Zhang",
                            "Huzheng Yan"
                        ],
                        "abstract": "",
                        "uid": "v-cga-8948290",
                        "file_name": "v-cga-8948290_Zhou_Presentation.mp4",
                        "time_stamp": "2022-10-19T20:45:00Z",
                        "time_start": "2022-10-19T20:45:00Z",
                        "time_end": "2022-10-19T20:53:59Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "539",
                        "paper_award": "",
                        "image_caption": "Surgical workflow. We prepared a mockup to simulate a real patient with a specific dose plan and fusion of all data\nfrom the mockup. After each mockup surgery was performed with our automated needle and seed detection system, we estimated the location error of the needle and seeds",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "https://youtu.be/z-5ZWB01Vjc",
                        "ff_id": "z-5ZWB01Vjc"
                    },
                    {
                        "slot_id": "v-cga-8948290-qa",
                        "session_id": "cga3",
                        "type": "Virtual Q+A",
                        "title": "Surgical Navigation System for Low-Dose-Rate Brachytherapy Based on Mixed Reality (Q+A)",
                        "contributors": [
                            "Zeyang Zhou"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "v-cga-8948290",
                        "file_name": "",
                        "time_stamp": "2022-10-19T20:53:59Z",
                        "time_start": "2022-10-19T20:53:59Z",
                        "time_end": "2022-10-19T20:57:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "539",
                        "paper_award": "",
                        "image_caption": "Surgical workflow. We prepared a mockup to simulate a real patient with a specific dose plan and fusion of all data\nfrom the mockup. After each mockup surgery was performed with our automated needle and seed detection system, we estimated the location error of the needle and seeds",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "https://youtu.be/z-5ZWB01Vjc",
                        "ff_id": "z-5ZWB01Vjc"
                    },
                    {
                        "slot_id": "v-cga-9709159-pres",
                        "session_id": "cga3",
                        "type": "In Person Presentation",
                        "title": "Visualization for Architecture, Engineering, and Construction: Shaping the Future of Our Built World",
                        "contributors": [
                            "Moataz Abdelaal"
                        ],
                        "authors": [
                            "Moataz Abdelaal",
                            "Felix Amtsberg",
                            "Michael Becher",
                            "Rebeca Duque Estrada",
                            "Fabian Kannenberg",
                            "Aimee Sousa Calepso",
                            "Hans Jakob Wagner",
                            "Guido Reina",
                            "Michael Sedlmair",
                            "Achim Menges",
                            "Daniel Weiskopf"
                        ],
                        "abstract": "",
                        "uid": "v-cga-9709159",
                        "file_name": "v-cga-9709159_Abdelaal_Presentation.mp4",
                        "time_stamp": "2022-10-19T20:57:00Z",
                        "time_start": "2022-10-19T20:57:00Z",
                        "time_end": "2022-10-19T21:07:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "771",
                        "paper_award": "",
                        "image_caption": "Interactive data visualization and immersive technology will be the vehicle to support advanced\ndigital and robotic fabrication and construction. Copyright \u00a9 2019 ICD/ITKE University of Stuttgart.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "https://youtu.be/0WE4cS47FRU",
                        "ff_id": "0WE4cS47FRU"
                    },
                    {
                        "slot_id": "v-cga-9709159-qa",
                        "session_id": "cga3",
                        "type": "In Person Q+A",
                        "title": "Visualization for Architecture, Engineering, and Construction: Shaping the Future of Our Built World (Q+A)",
                        "contributors": [
                            "Moataz Abdelaal"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "v-cga-9709159",
                        "file_name": "",
                        "time_stamp": "2022-10-19T21:07:00Z",
                        "time_start": "2022-10-19T21:07:00Z",
                        "time_end": "2022-10-19T21:09:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "771",
                        "paper_award": "",
                        "image_caption": "Interactive data visualization and immersive technology will be the vehicle to support advanced\ndigital and robotic fabrication and construction. Copyright \u00a9 2019 ICD/ITKE University of Stuttgart.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "https://youtu.be/0WE4cS47FRU",
                        "ff_id": "0WE4cS47FRU"
                    },
                    {
                        "slot_id": "v-cga-9726809-pres",
                        "session_id": "cga3",
                        "type": "In Person Presentation",
                        "title": "Visual Parameter Space Analysis for Optimizing the Quality of  Industrial Nonwovens",
                        "contributors": [
                            "Viny Saajan Victor"
                        ],
                        "authors": [
                            "Viny Saajan Victor",
                            "Andre Schmeiser",
                            "Heike Leitte",
                            "Simone Gramsch"
                        ],
                        "abstract": "",
                        "uid": "v-cga-9726809",
                        "file_name": "v-cga-9726809_Victor_Presentation.mp4",
                        "time_stamp": "2022-10-19T21:09:00Z",
                        "time_start": "2022-10-19T21:09:00Z",
                        "time_end": "2022-10-19T21:19:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "547",
                        "paper_award": "",
                        "image_caption": "'VirtualNonwovenExplorer' is a visualization tool designed to support the textile industry in optimizing the quality of industrial nonwovens.\nThe figure shows the different parameter space analysis strategies that the tool offers in order to obtain optimal and robust process parameter settings.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "https://youtu.be/Rg7q6h2ug9c",
                        "ff_id": "Rg7q6h2ug9c"
                    },
                    {
                        "slot_id": "v-cga-9726809-qa",
                        "session_id": "cga3",
                        "type": "In Person Q+A",
                        "title": "Visual Parameter Space Analysis for Optimizing the Quality of  Industrial Nonwovens (Q+A)",
                        "contributors": [
                            "Viny Saajan Victor"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "v-cga-9726809",
                        "file_name": "",
                        "time_stamp": "2022-10-19T21:19:00Z",
                        "time_start": "2022-10-19T21:19:00Z",
                        "time_end": "2022-10-19T21:21:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "547",
                        "paper_award": "",
                        "image_caption": "'VirtualNonwovenExplorer' is a visualization tool designed to support the textile industry in optimizing the quality of industrial nonwovens.\nThe figure shows the different parameter space analysis strategies that the tool offers in order to obtain optimal and robust process parameter settings.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "https://youtu.be/Rg7q6h2ug9c",
                        "ff_id": "Rg7q6h2ug9c"
                    },
                    {
                        "slot_id": "v-cga-9729397-pres",
                        "session_id": "cga3",
                        "type": "In Person Presentation",
                        "title": "Reflections on Visualization Research Projects in the Manufacturing Industry",
                        "contributors": [
                            "Johanna Schmidt"
                        ],
                        "authors": [
                            "Lena Cibulski",
                            "Johanna Schmidt",
                            "Wolfgang Aigner"
                        ],
                        "abstract": "",
                        "uid": "v-cga-9729397",
                        "file_name": "v-cga-9729397_Cibulski_Presentation.mp4",
                        "time_stamp": "2022-10-19T21:21:00Z",
                        "time_start": "2022-10-19T21:21:00Z",
                        "time_end": "2022-10-19T21:31:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "537",
                        "paper_award": "",
                        "image_caption": "As members of research institutions involved in several applied research projects dealing with visualization in manufacturing, we characterized and analyzed our experiences for a detailed qualitative view, to distill important lessons learned, and to identify research gaps.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "https://youtu.be/OFrJ1bEi1KE",
                        "ff_id": "OFrJ1bEi1KE"
                    },
                    {
                        "slot_id": "v-cga-9729397-qa",
                        "session_id": "cga3",
                        "type": "In Person Q+A",
                        "title": "Reflections on Visualization Research Projects in the Manufacturing Industry (Q+A)",
                        "contributors": [
                            "Johanna Schmidt"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "v-cga-9729397",
                        "file_name": "",
                        "time_stamp": "2022-10-19T21:31:00Z",
                        "time_start": "2022-10-19T21:31:00Z",
                        "time_end": "2022-10-19T21:33:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "537",
                        "paper_award": "",
                        "image_caption": "As members of research institutions involved in several applied research projects dealing with visualization in manufacturing, we characterized and analyzed our experiences for a detailed qualitative view, to distill important lessons learned, and to identify research gaps.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "https://youtu.be/OFrJ1bEi1KE",
                        "ff_id": "OFrJ1bEi1KE"
                    },
                    {
                        "slot_id": "v-cga-9732172-pres",
                        "session_id": "cga3",
                        "type": "In Person Presentation",
                        "title": "Situated Visual Analysis and Live Monitoring for Manufacturing",
                        "contributors": [
                            "Michael Becher"
                        ],
                        "authors": [
                            "Michael Becher",
                            "Dominik Herr",
                            "Christoph Muller",
                            "Kuno Kurzhals",
                            "Guido Reina",
                            "Lena Wagner",
                            "Thomas Ertl",
                            "Daniel Weiskopf"
                        ],
                        "abstract": "",
                        "uid": "v-cga-9732172",
                        "file_name": "v-cga-9732172_Becher_Presentation.mp4",
                        "time_stamp": "2022-10-19T21:33:00Z",
                        "time_start": "2022-10-19T21:33:00Z",
                        "time_end": "2022-10-19T21:43:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "493",
                        "paper_award": "",
                        "image_caption": "Modern machines continuously produce a wealth of data in real time about important events, such as faults during the production process. Hence, the tasks of operators on the shop floor are shifting from manual work to monitoring machines and resolving faults reported by those. With our situated visualization approach, operators can use a touch-based interface to perform a visual analysis of event data directly on the shop floor. Visualization in augmented reality allows them to monitor live events during production processes, guides them to the respective machines and provides important contextual information.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "https://youtu.be/-jgkIbJu9bw",
                        "ff_id": "-jgkIbJu9bw"
                    },
                    {
                        "slot_id": "v-cga-9732172-qa",
                        "session_id": "cga3",
                        "type": "In Person Q+A",
                        "title": "Situated Visual Analysis and Live Monitoring for Manufacturing (Q+A)",
                        "contributors": [
                            "Michael Becher"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "v-cga-9732172",
                        "file_name": "",
                        "time_stamp": "2022-10-19T21:43:00Z",
                        "time_start": "2022-10-19T21:43:00Z",
                        "time_end": "2022-10-19T21:45:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "493",
                        "paper_award": "",
                        "image_caption": "Modern machines continuously produce a wealth of data in real time about important events, such as faults during the production process. Hence, the tasks of operators on the shop floor are shifting from manual work to monitoring machines and resolving faults reported by those. With our situated visualization approach, operators can use a touch-based interface to perform a visual analysis of event data directly on the shop floor. Visualization in augmented reality allows them to monitor live events during production processes, guides them to the respective machines and provides important contextual information.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "https://youtu.be/-jgkIbJu9bw",
                        "ff_id": "-jgkIbJu9bw"
                    },
                    {
                        "slot_id": "v-cga-9709109-pres",
                        "session_id": "cga3",
                        "type": "Virtual Presentation (pre-recorded)",
                        "title": "Stress Visualization for Interface Optimization of a Hybrid Component Using Surface Tensor Spines",
                        "contributors": [
                            "Vanessa Kretzschmar"
                        ],
                        "authors": [
                            "Vanessa Kretzschmar",
                            "Allan Rocha",
                            "Fabian Gunther",
                            "Markus Stommel",
                            "Gerik Scheuermann"
                        ],
                        "abstract": "",
                        "uid": "v-cga-9709109",
                        "file_name": "v-cga-9709109_Kretzschmar_Presentation.mp4",
                        "time_stamp": "2022-10-19T21:45:00Z",
                        "time_start": "2022-10-19T21:45:00Z",
                        "time_end": "2022-10-19T21:54:48Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "588",
                        "paper_award": "",
                        "image_caption": "Surface Tensor Spines visualizing a stress tensor field on a component interface layer.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "https://youtu.be/MyApI3lG2OU",
                        "ff_id": "MyApI3lG2OU"
                    },
                    {
                        "slot_id": "v-cga-9709109-qa",
                        "session_id": "cga3",
                        "type": "Virtual Q+A",
                        "title": "Stress Visualization for Interface Optimization of a Hybrid Component Using Surface Tensor Spines (Q+A)",
                        "contributors": [
                            "Vanessa Kretzschmar"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "v-cga-9709109",
                        "file_name": "",
                        "time_stamp": "2022-10-19T21:54:48Z",
                        "time_start": "2022-10-19T21:54:48Z",
                        "time_end": "2022-10-19T21:57:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "588",
                        "paper_award": "",
                        "image_caption": "Surface Tensor Spines visualizing a stress tensor field on a component interface layer.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "https://youtu.be/MyApI3lG2OU",
                        "ff_id": "MyApI3lG2OU"
                    }
                ]
            }
        ]
    },
    "v-siggraph": {
        "event": "SIGGRAPH Invited Partnership Presentations",
        "long_name": "SIGGRAPH Invited Partnership Presentations",
        "event_type": "Invited Partnership Presentations",
        "event_prefix": "v-siggraph",
        "event_description": "",
        "event_url": "",
        "organizers": [],
        "sessions": [
            {
                "title": "SIGGRAPH Invited Talks",
                "session_id": "sig1",
                "event_prefix": "v-siggraph",
                "track": "mistletoe",
                "livestream_id": "mistletoe-wed",
                "session_image": "sig1.png",
                "chair": [
                    "Markus Hadwiger"
                ],
                "organizers": [],
                "time_start": "2022-10-19T15:45:00Z",
                "time_end": "2022-10-19T17:00:00Z",
                "discord_category": "",
                "discord_channel": "mistletoe",
                "discord_channel_id": "1024601055700471839",
                "discord_link": "https://discord.com/channels/978320435554947082/1024601055700471839",
                "slido_link": "https://app.sli.do/event/i28UETZaiRWr1XwX6eKjcf?section=05d57c7a-397b-4e16-a41d-7f06e8e64c37",
                "youtube_url": "https://youtu.be/xub7Gw386eY",
                "youtube_id": "xub7Gw386eY",
                "zoom_meeting": "",
                "zoom_password": "",
                "zoom_link": "",
                "ff_link": "https://youtu.be/XVyYoC11ey4",
                "ff_playlist": "",
                "time_slots": [
                    {
                        "slot_id": "sig1-opening",
                        "session_id": "sig1",
                        "type": "In Person Other",
                        "title": "Session Opening",
                        "contributors": [
                            "Markus Hadwiger"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "",
                        "file_name": "",
                        "time_stamp": "2022-10-19T15:45:00Z",
                        "time_start": "2022-10-19T15:45:00Z",
                        "time_end": "2022-10-19T15:45:00Z",
                        "paper_type": "",
                        "keywords": [],
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "v-siggraph-1-pres",
                        "session_id": "sig1",
                        "type": "In Person Presentation",
                        "title": "Image Features Influence Reaction Time: A Learned Probabilistic Perceptual Model for Saccade Latency",
                        "contributors": [
                            "Budmonde Duinkharjav"
                        ],
                        "authors": [
                            "Budmonde Duinkharjav",
                            "Praneeth Chakravarthula",
                            "Rachel Brown",
                            "Anjul Patney",
                            "Qi Sun"
                        ],
                        "abstract": "We aim to ask and answer an essential question \"how quickly do we react after observing a displayed visual target?\" To this end, we present psychophysical studies that characterize the remarkable disconnect between human saccadic behaviors and spatial visual acuity. Building on the results of our studies, we develop a perceptual model to predict temporal gaze behavior, particularly saccadic latency, as a function of the statistics of a displayed image. Specifically, we implement a neurologically-inspired probabilistic model that mimics the accumulation of confidence that leads to a perceptual decision. We validate our model with a series of objective measurements and user studies using an eye-tracked VR display. The results demonstrate that our model prediction is in statistical alignment with real-world human behavior. Further, we establish that many sub-threshold image modifications commonly introduced in graphics pipelines may significantly alter human reaction timing, even if the differences are visually undetectable. Finally, we show that our model can serve as a metric to predict and alter reaction latency of users in interactive computer graphics applications, thus may improve gaze-contingent rendering, design of virtual experiences, and player performance in e-sports. We illustrate this with two examples: estimating competition fairness in a video game with two different team colors, and tuning display viewing distance to minimize player reaction time.",
                        "uid": "v-siggraph-1",
                        "file_name": "v-siggraph-1_Duinkharjav_Presentation.mp4",
                        "time_stamp": "2022-10-19T15:45:00Z",
                        "time_start": "2022-10-19T15:45:00Z",
                        "time_end": "2022-10-19T15:55:00Z",
                        "paper_type": "full",
                        "keywords": [
                            "Virtual Reality, Augmented Reality, Visual Perception, Human Performance, Esports, Gaze-Contingent Rendering"
                        ],
                        "has_image": "1",
                        "has_video": "930",
                        "paper_award": "",
                        "image_caption": "We propose a model which predicts the reaction latency for users to identify and saccade to a peripheral target.\nBased on our psychophysical data collected for stimuli with varying visual characteristics, we model the likelihood distribution of the time users take to process, react, and saccade to a target.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "https://youtu.be/snQhLtqSHqU",
                        "ff_id": "snQhLtqSHqU"
                    },
                    {
                        "slot_id": "v-siggraph-1-qa",
                        "session_id": "sig1",
                        "type": "In Person Q+A",
                        "title": "Image Features Influence Reaction Time: A Learned Probabilistic Perceptual Model for Saccade Latency (Q+A)",
                        "contributors": [
                            "Budmonde Duinkharjav"
                        ],
                        "authors": [],
                        "abstract": "We aim to ask and answer an essential question \"how quickly do we react after observing a displayed visual target?\" To this end, we present psychophysical studies that characterize the remarkable disconnect between human saccadic behaviors and spatial visual acuity. Building on the results of our studies, we develop a perceptual model to predict temporal gaze behavior, particularly saccadic latency, as a function of the statistics of a displayed image. Specifically, we implement a neurologically-inspired probabilistic model that mimics the accumulation of confidence that leads to a perceptual decision. We validate our model with a series of objective measurements and user studies using an eye-tracked VR display. The results demonstrate that our model prediction is in statistical alignment with real-world human behavior. Further, we establish that many sub-threshold image modifications commonly introduced in graphics pipelines may significantly alter human reaction timing, even if the differences are visually undetectable. Finally, we show that our model can serve as a metric to predict and alter reaction latency of users in interactive computer graphics applications, thus may improve gaze-contingent rendering, design of virtual experiences, and player performance in e-sports. We illustrate this with two examples: estimating competition fairness in a video game with two different team colors, and tuning display viewing distance to minimize player reaction time.",
                        "uid": "v-siggraph-1",
                        "file_name": "",
                        "time_stamp": "2022-10-19T15:55:00Z",
                        "time_start": "2022-10-19T15:55:00Z",
                        "time_end": "2022-10-19T15:57:00Z",
                        "paper_type": "full",
                        "keywords": [
                            "Virtual Reality, Augmented Reality, Visual Perception, Human Performance, Esports, Gaze-Contingent Rendering"
                        ],
                        "has_image": "1",
                        "has_video": "930",
                        "paper_award": "",
                        "image_caption": "We propose a model which predicts the reaction latency for users to identify and saccade to a peripheral target.\nBased on our psychophysical data collected for stimuli with varying visual characteristics, we model the likelihood distribution of the time users take to process, react, and saccade to a target.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "https://youtu.be/snQhLtqSHqU",
                        "ff_id": "snQhLtqSHqU"
                    },
                    {
                        "slot_id": "v-siggraph-2-pres",
                        "session_id": "sig1",
                        "type": "In Person Presentation",
                        "title": "CLIPasso: Semantically Aware Object Sketching",
                        "contributors": [
                            "Arik Shamir"
                        ],
                        "authors": [
                            "Yael Vinker",
                            "Ehsan Pajouheshgar",
                            "Jessica Y. Bo",
                            "Roman Christian Bachmann",
                            "Amit Bermano",
                            "Daniel Cohen-Or",
                            "Amir Zamir",
                            "Ariel Shamir"
                        ],
                        "abstract": "Abstraction is at the heart of sketching due to the simple and minimal nature of line drawings. Abstraction entails identifying the essential visual properties of an object or scene, which requires semantic understanding and prior knowledge of high-level concepts. Abstract depictions are therefore challenging for artists, and even more so for machines. We present CLIPasso, an object sketching method that can achieve different levels of abstraction, guided by geometric and semantic simplifications. While sketch generation methods often rely on explicit sketch datasets for training, we utilize the remarkable ability of CLIP (Contrastive-Language-Image-Pretraining) to distill semantic concepts from sketches and images alike. We define a sketch as a set of B\u00e9zier curves and use a differentiable rasterizer to optimize the parameters of the curves directly with respect to a CLIP-based perceptual loss. The abstraction degree is controlled by varying the number of strokes. The generated sketches demonstrate multiple levels of abstraction while maintaining recognizability, underlying structure, and essential visual components of the subject drawn.",
                        "uid": "v-siggraph-2",
                        "file_name": "v-siggraph-2_Vinker_Presentation.mp4",
                        "time_stamp": "2022-10-19T15:57:00Z",
                        "time_start": "2022-10-19T15:57:00Z",
                        "time_end": "2022-10-19T16:07:00Z",
                        "paper_type": "full",
                        "keywords": [
                            "Sketch Synthesis, Image-based Rendering, Vector Line Art Generation"
                        ],
                        "has_image": "1",
                        "has_video": "775",
                        "paper_award": "",
                        "image_caption": "{\\rtf1\\ansi\\ansicpg1252\\cocoartf2636\n\\cocoatextscaling0\\cocoaplatform0{\\fonttbl\\f0\\fnil\\fcharset0 HelveticaNeue;}\n{\\colortbl;\\red255\\green255\\blue255;}\n{\\*\\expandedcolortbl;;}\n\\paperw11900\\paperh16840\\margl1440\\margr1440\\vieww11520\\viewh8400\\viewkind0\n\\deftab560\n\\pard\\pardeftab560\\partightenfactor0\n\n\\f0\\fs24 \\cf0 Our work converts an image of an object to a sketch, allowing for varying levels of abstraction, while preserving its key visual features. Even with a very minimal representation (the rightmost flamingo and horse are drawn with only a few strokes), one can recognize both the semantics and the structure of the subject depicted.}",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "https://youtu.be/1yyZwyO975c",
                        "ff_id": "1yyZwyO975c"
                    },
                    {
                        "slot_id": "v-siggraph-2-qa",
                        "session_id": "sig1",
                        "type": "In Person Q+A",
                        "title": "CLIPasso: Semantically Aware Object Sketching (Q+A)",
                        "contributors": [
                            "Arik Shamir"
                        ],
                        "authors": [],
                        "abstract": "Abstraction is at the heart of sketching due to the simple and minimal nature of line drawings. Abstraction entails identifying the essential visual properties of an object or scene, which requires semantic understanding and prior knowledge of high-level concepts. Abstract depictions are therefore challenging for artists, and even more so for machines. We present CLIPasso, an object sketching method that can achieve different levels of abstraction, guided by geometric and semantic simplifications. While sketch generation methods often rely on explicit sketch datasets for training, we utilize the remarkable ability of CLIP (Contrastive-Language-Image-Pretraining) to distill semantic concepts from sketches and images alike. We define a sketch as a set of B\u00e9zier curves and use a differentiable rasterizer to optimize the parameters of the curves directly with respect to a CLIP-based perceptual loss. The abstraction degree is controlled by varying the number of strokes. The generated sketches demonstrate multiple levels of abstraction while maintaining recognizability, underlying structure, and essential visual components of the subject drawn.",
                        "uid": "v-siggraph-2",
                        "file_name": "",
                        "time_stamp": "2022-10-19T16:07:00Z",
                        "time_start": "2022-10-19T16:07:00Z",
                        "time_end": "2022-10-19T16:09:00Z",
                        "paper_type": "full",
                        "keywords": [
                            "Sketch Synthesis, Image-based Rendering, Vector Line Art Generation"
                        ],
                        "has_image": "1",
                        "has_video": "775",
                        "paper_award": "",
                        "image_caption": "{\\rtf1\\ansi\\ansicpg1252\\cocoartf2636\n\\cocoatextscaling0\\cocoaplatform0{\\fonttbl\\f0\\fnil\\fcharset0 HelveticaNeue;}\n{\\colortbl;\\red255\\green255\\blue255;}\n{\\*\\expandedcolortbl;;}\n\\paperw11900\\paperh16840\\margl1440\\margr1440\\vieww11520\\viewh8400\\viewkind0\n\\deftab560\n\\pard\\pardeftab560\\partightenfactor0\n\n\\f0\\fs24 \\cf0 Our work converts an image of an object to a sketch, allowing for varying levels of abstraction, while preserving its key visual features. Even with a very minimal representation (the rightmost flamingo and horse are drawn with only a few strokes), one can recognize both the semantics and the structure of the subject depicted.}",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "https://youtu.be/1yyZwyO975c",
                        "ff_id": "1yyZwyO975c"
                    },
                    {
                        "slot_id": "v-siggraph-3-pres",
                        "session_id": "sig1",
                        "type": "Virtual Presentation (pre-recorded)",
                        "title": "Joint Neural Phase Retrieval and Compression for Energy- and Computation-efficient Holography on the Edge",
                        "contributors": [
                            "Yujie Wang"
                        ],
                        "authors": [
                            "Yujie Wang",
                            "Praneeth Chakravarthula",
                            "Qi Sun",
                            "Baoquan Chen"
                        ],
                        "abstract": "Recent deep learning approaches have shown remarkable promise to enable high fidelity holographic displays. However, lightweight wearable display devices cannot afford the computation demand and energy consumption for hologram generation due to the limited onboard compute capability and battery life. On the other hand, if the computation is conducted entirely remotely on a cloud server, transmitting lossless hologram data is not only challenging but also result in prohibitively high latency and storage. In this work, by distributing the computation and optimizing the transmission, we propose the first framework that jointly generates and compresses high-quality phase-only holograms. Specifically, our framework asymmetrically separates the hologram generation process into high-compute remote encoding (on the server), and low-compute decoding (on the edge) stages. Our encoding enables light weight latent space data, thus faster and efficient transmission to the edge device. With our framework, we observed a reduction of 76% computation and consequently 83% in energy cost on edge devices, compared to the existing hologram generation methods. Our framework is robust to transmission and decoding errors, and approach high image fidelity for as low as 2 bits-per-pixel, and further reduced average bit-rates and decoding time for holographic videos.",
                        "uid": "v-siggraph-3",
                        "file_name": "v-siggraph-3_Wang_Presentation.mp4",
                        "time_stamp": "2022-10-19T16:09:00Z",
                        "time_start": "2022-10-19T16:09:00Z",
                        "time_end": "2022-10-19T16:20:12Z",
                        "paper_type": "full",
                        "keywords": [
                            "Computer generated holography, neural hologram generation, hologram compression"
                        ],
                        "has_image": "1",
                        "has_video": "672",
                        "paper_award": "",
                        "image_caption": "We propose the first framework devised for cloud-based holographic displaying scenarios in the future.\n\nUsing an end-to-end framework with coupling hologram generation and compression, the transmission efficiency is highly improved by reducing the number of bits for coding holograms.\n\nBy asymmetrically distributing the computation between cloud servers (~80%) and edge devices (~20%), \nour framework considerably reduces the computational cost for edge devices.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "https://youtu.be/-9PAyJCWGno",
                        "ff_id": "-9PAyJCWGno"
                    },
                    {
                        "slot_id": "v-siggraph-3-qa",
                        "session_id": "sig1",
                        "type": "Virtual Q+A",
                        "title": "Joint Neural Phase Retrieval and Compression for Energy- and Computation-efficient Holography on the Edge (Q+A)",
                        "contributors": [
                            "Yujie Wang"
                        ],
                        "authors": [],
                        "abstract": "Recent deep learning approaches have shown remarkable promise to enable high fidelity holographic displays. However, lightweight wearable display devices cannot afford the computation demand and energy consumption for hologram generation due to the limited onboard compute capability and battery life. On the other hand, if the computation is conducted entirely remotely on a cloud server, transmitting lossless hologram data is not only challenging but also result in prohibitively high latency and storage. In this work, by distributing the computation and optimizing the transmission, we propose the first framework that jointly generates and compresses high-quality phase-only holograms. Specifically, our framework asymmetrically separates the hologram generation process into high-compute remote encoding (on the server), and low-compute decoding (on the edge) stages. Our encoding enables light weight latent space data, thus faster and efficient transmission to the edge device. With our framework, we observed a reduction of 76% computation and consequently 83% in energy cost on edge devices, compared to the existing hologram generation methods. Our framework is robust to transmission and decoding errors, and approach high image fidelity for as low as 2 bits-per-pixel, and further reduced average bit-rates and decoding time for holographic videos.",
                        "uid": "v-siggraph-3",
                        "file_name": "",
                        "time_stamp": "2022-10-19T16:20:12Z",
                        "time_start": "2022-10-19T16:20:12Z",
                        "time_end": "2022-10-19T16:21:00Z",
                        "paper_type": "full",
                        "keywords": [
                            "Computer generated holography, neural hologram generation, hologram compression"
                        ],
                        "has_image": "1",
                        "has_video": "672",
                        "paper_award": "",
                        "image_caption": "We propose the first framework devised for cloud-based holographic displaying scenarios in the future.\n\nUsing an end-to-end framework with coupling hologram generation and compression, the transmission efficiency is highly improved by reducing the number of bits for coding holograms.\n\nBy asymmetrically distributing the computation between cloud servers (~80%) and edge devices (~20%), \nour framework considerably reduces the computational cost for edge devices.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "https://youtu.be/-9PAyJCWGno",
                        "ff_id": "-9PAyJCWGno"
                    },
                    {
                        "slot_id": "v-siggraph-6-pres",
                        "session_id": "sig1",
                        "type": "Virtual Presentation (pre-recorded)",
                        "title": "Spelunking the Deep: Guaranteed Queries on General Neural Implicit Surfaces",
                        "contributors": [
                            "Nicholas Sharp"
                        ],
                        "authors": [
                            "Nicholas Sharp",
                            "Alec Jacobson"
                        ],
                        "abstract": "Neural implicit representations, which encode a surface as the level set of a neural network applied to spatial coordinates, have proven to be remarkably effective for optimizing, compressing, and generating 3D geometry. Although these representations are easy to fit, it is not clear how to best evaluate geometric queries on the shape, such as intersecting against a ray or finding a closest point. The predominant approach is to encourage the network to have a signed distance property. However, this property typically holds only approximately, leading to robustness issues, and holds only at the conclusion of training, inhibiting the use of queries in loss functions. Instead, this work presents a new approach to perform queries directly on general neural implicit functions for a wide range of existing architectures. Our key tool is the application of range analysis to neural networks, using automatic arithmetic rules to bound the output of a network over a region; we conduct a study of range analysis on neural networks, and identify variants of affine arithmetic which are highly effective. We use the resulting bounds to develop geometric queries including ray casting, intersection testing, constructing spatial hierarchies, fast mesh extraction, closest-point evaluation, evaluating bulk properties, and more. Our queries can be efficiently evaluated on GPUs, and offer concrete accuracy guarantees even on randomly-initialized networks, enabling their use in training objectives and beyond. We also show a preliminary application to inverse rendering.",
                        "uid": "v-siggraph-6",
                        "file_name": "v-siggraph-6_Sharp_Presentation.mp4",
                        "time_stamp": "2022-10-19T16:21:00Z",
                        "time_start": "2022-10-19T16:21:00Z",
                        "time_end": "2022-10-19T16:32:58Z",
                        "paper_type": "full",
                        "keywords": [
                            "implicit surfaces, neural networks, range analysis, geometry processing"
                        ],
                        "has_image": "1",
                        "has_video": "718",
                        "paper_award": "",
                        "image_caption": "This method enables geometric queries on neural implicit surfaces, without relying on fitting a signed distance function. Several queries are shown here on a neural implicit occupancy function encoding a mine cart. These operations open up new explorations of neural implicit surfaces.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "https://youtu.be/jWCZoAkt0Tw",
                        "ff_id": "jWCZoAkt0Tw"
                    },
                    {
                        "slot_id": "v-siggraph-6-qa",
                        "session_id": "sig1",
                        "type": "Virtual Q+A",
                        "title": "Spelunking the Deep: Guaranteed Queries on General Neural Implicit Surfaces (Q+A)",
                        "contributors": [
                            "Nicholas Sharp"
                        ],
                        "authors": [],
                        "abstract": "Neural implicit representations, which encode a surface as the level set of a neural network applied to spatial coordinates, have proven to be remarkably effective for optimizing, compressing, and generating 3D geometry. Although these representations are easy to fit, it is not clear how to best evaluate geometric queries on the shape, such as intersecting against a ray or finding a closest point. The predominant approach is to encourage the network to have a signed distance property. However, this property typically holds only approximately, leading to robustness issues, and holds only at the conclusion of training, inhibiting the use of queries in loss functions. Instead, this work presents a new approach to perform queries directly on general neural implicit functions for a wide range of existing architectures. Our key tool is the application of range analysis to neural networks, using automatic arithmetic rules to bound the output of a network over a region; we conduct a study of range analysis on neural networks, and identify variants of affine arithmetic which are highly effective. We use the resulting bounds to develop geometric queries including ray casting, intersection testing, constructing spatial hierarchies, fast mesh extraction, closest-point evaluation, evaluating bulk properties, and more. Our queries can be efficiently evaluated on GPUs, and offer concrete accuracy guarantees even on randomly-initialized networks, enabling their use in training objectives and beyond. We also show a preliminary application to inverse rendering.",
                        "uid": "v-siggraph-6",
                        "file_name": "",
                        "time_stamp": "2022-10-19T16:32:58Z",
                        "time_start": "2022-10-19T16:32:58Z",
                        "time_end": "2022-10-19T16:33:00Z",
                        "paper_type": "full",
                        "keywords": [
                            "implicit surfaces, neural networks, range analysis, geometry processing"
                        ],
                        "has_image": "1",
                        "has_video": "718",
                        "paper_award": "",
                        "image_caption": "This method enables geometric queries on neural implicit surfaces, without relying on fitting a signed distance function. Several queries are shown here on a neural implicit occupancy function encoding a mine cart. These operations open up new explorations of neural implicit surfaces.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "https://youtu.be/jWCZoAkt0Tw",
                        "ff_id": "jWCZoAkt0Tw"
                    },
                    {
                        "slot_id": "v-siggraph-5-pres",
                        "session_id": "sig1",
                        "type": "Virtual Presentation (pre-recorded)",
                        "title": "Sketch2Pose: estimating a 3D character pose from a bitmap sketch",
                        "contributors": [
                            "Kirill Brodt"
                        ],
                        "authors": [
                            "Kirill Brodt",
                            "Mikhail Bessmeltsev"
                        ],
                        "abstract": "Artists frequently capture character poses via raster sketches, then use these drawings as a reference while posing a 3D character in a specialized 3D software --- a time-consuming process, requiring specialized 3D training and mental effort. We tackle this challenge by proposing the first system for automatically inferring a 3D character pose from a single bitmap sketch, producing poses consistent with viewer expectations. Algorithmically interpreting bitmap sketches is challenging, as they contain significantly distorted proportions and foreshortening. We address this by predicting three key elements of a drawing, necessary to disambiguate the drawn poses: 2D bone tangents, self-contacts, and bone foreshortening. These elements are then leveraged in an optimization inferring the 3D character pose consistent with the artist's intent. Our optimization balances cues derived from artistic literature and perception research to compensate for distorted character proportions. We demonstrate a gallery of results on sketches of numerous styles. We validate our method via numerical evaluations, user studies, and comparisons to manually posed characters and previous work. Code and data for our paper are available at http://www-labs.iro.umontreal.ca/bmpix/sketch2pose/.",
                        "uid": "v-siggraph-5",
                        "file_name": "v-siggraph-5_Brodt_Presentation.mp4",
                        "time_stamp": "2022-10-19T16:33:00Z",
                        "time_start": "2022-10-19T16:33:00Z",
                        "time_end": "2022-10-19T16:40:38Z",
                        "paper_type": "full",
                        "keywords": [
                            "Character posing, rigged and skinned characters, sketch-based posing, character sketches"
                        ],
                        "has_image": "1",
                        "has_video": "458",
                        "paper_award": "",
                        "image_caption": "Given a single natural bitmap sketch of a character, our learning-based\napproach allows to automatically, with no additional input, recover the 3D pose\nconsistent with the viewer expectation. This pose can be then automatically\ncopied a custom rigged and skinned 3D character using standard retargeting\ntools",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "https://youtu.be/-ygJHBxQ4C8",
                        "ff_id": "-ygJHBxQ4C8"
                    },
                    {
                        "slot_id": "v-siggraph-5-qa",
                        "session_id": "sig1",
                        "type": "Virtual Q+A",
                        "title": "Sketch2Pose: estimating a 3D character pose from a bitmap sketch (Q+A)",
                        "contributors": [
                            "Kirill Brodt"
                        ],
                        "authors": [],
                        "abstract": "Artists frequently capture character poses via raster sketches, then use these drawings as a reference while posing a 3D character in a specialized 3D software --- a time-consuming process, requiring specialized 3D training and mental effort. We tackle this challenge by proposing the first system for automatically inferring a 3D character pose from a single bitmap sketch, producing poses consistent with viewer expectations. Algorithmically interpreting bitmap sketches is challenging, as they contain significantly distorted proportions and foreshortening. We address this by predicting three key elements of a drawing, necessary to disambiguate the drawn poses: 2D bone tangents, self-contacts, and bone foreshortening. These elements are then leveraged in an optimization inferring the 3D character pose consistent with the artist's intent. Our optimization balances cues derived from artistic literature and perception research to compensate for distorted character proportions. We demonstrate a gallery of results on sketches of numerous styles. We validate our method via numerical evaluations, user studies, and comparisons to manually posed characters and previous work. Code and data for our paper are available at http://www-labs.iro.umontreal.ca/bmpix/sketch2pose/.",
                        "uid": "v-siggraph-5",
                        "file_name": "",
                        "time_stamp": "2022-10-19T16:40:38Z",
                        "time_start": "2022-10-19T16:40:38Z",
                        "time_end": "2022-10-19T16:45:00Z",
                        "paper_type": "full",
                        "keywords": [
                            "Character posing, rigged and skinned characters, sketch-based posing, character sketches"
                        ],
                        "has_image": "1",
                        "has_video": "458",
                        "paper_award": "",
                        "image_caption": "Given a single natural bitmap sketch of a character, our learning-based\napproach allows to automatically, with no additional input, recover the 3D pose\nconsistent with the viewer expectation. This pose can be then automatically\ncopied a custom rigged and skinned 3D character using standard retargeting\ntools",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "https://youtu.be/-ygJHBxQ4C8",
                        "ff_id": "-ygJHBxQ4C8"
                    },
                    {
                        "slot_id": "v-siggraph-4-pres",
                        "session_id": "sig1",
                        "type": "Virtual Presentation (pre-recorded)",
                        "title": "Umbrella Meshes: Volumetric Elastic Mechanisms for Freeform Shape Deployment",
                        "contributors": [
                            "Yingying Ren",
                            "Uday Kusupati, Yingying Ren"
                        ],
                        "authors": [
                            "Yingying Ren",
                            "Uday Kusupati",
                            "Julian Panetta",
                            "Florin Isvoranu",
                            "Davide Pellis",
                            "Tian Chen",
                            "Mark Pauly"
                        ],
                        "abstract": "We present a computational inverse design framework for a new class of volumetric deployable structures that have compact rest states and deploy into bending-active 3D target surfaces. Umbrella meshes consist of elastic beams, rigid plates, and hinge joints that can be directly printed or assembled in a zero-energy fabrication state. During deployment, as the elastic beams of varying heights rotate from vertical to horizontal configurations, the entire structure transforms from a compact block into a target curved surface. Umbrella Meshes encode both intrinsic and extrinsic curvature of the target surface and in principle are free from the area expansion ratio bounds of past auxetic material systems.  We build a reduced physics-based simulation framework to accurately and efficiently model the complex interaction between the elastically deforming components. To determine the mesh topology and optimal shape parameters for approximating a given target surface, we propose an inverse design optimization algorithm initialized with conformal flattening. Our algorithm minimizes the structure's strain energy in its deployed state and optimizes actuation forces so that the final deployed structure is in stable equilibrium close to the desired surface with few or no external constraints. We validate our approach by fabricating a series of physical models at various scales using different manufacturing techniques.",
                        "uid": "v-siggraph-4",
                        "file_name": "v-siggraph-4_Ren_Presentation.mp4",
                        "time_stamp": "2022-10-19T16:45:00Z",
                        "time_start": "2022-10-19T16:45:00Z",
                        "time_end": "2022-10-19T16:53:44Z",
                        "paper_type": "full",
                        "keywords": [
                            "Deployable structure, physics-based simulation, numerical optimization, computational design, fabrication"
                        ],
                        "has_image": "1",
                        "has_video": "524",
                        "paper_award": "",
                        "image_caption": "An umbrella mesh is a volumetric deployable structure with a compact, zero-energy rest state that deploys into a given 3D target surface. We show here the physical model of an umbrella mesh unit cell and the deployment sequence of an umbrella mesh prototype optimized to match the input design surface.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "https://youtu.be/nTJUle-q4yQ",
                        "ff_id": "nTJUle-q4yQ"
                    },
                    {
                        "slot_id": "v-siggraph-4-qa",
                        "session_id": "sig1",
                        "type": "Virtual Q+A",
                        "title": "Umbrella Meshes: Volumetric Elastic Mechanisms for Freeform Shape Deployment (Q+A)",
                        "contributors": [
                            "Yingying Ren",
                            "Uday Kusupati, Yingying Ren"
                        ],
                        "authors": [],
                        "abstract": "We present a computational inverse design framework for a new class of volumetric deployable structures that have compact rest states and deploy into bending-active 3D target surfaces. Umbrella meshes consist of elastic beams, rigid plates, and hinge joints that can be directly printed or assembled in a zero-energy fabrication state. During deployment, as the elastic beams of varying heights rotate from vertical to horizontal configurations, the entire structure transforms from a compact block into a target curved surface. Umbrella Meshes encode both intrinsic and extrinsic curvature of the target surface and in principle are free from the area expansion ratio bounds of past auxetic material systems.  We build a reduced physics-based simulation framework to accurately and efficiently model the complex interaction between the elastically deforming components. To determine the mesh topology and optimal shape parameters for approximating a given target surface, we propose an inverse design optimization algorithm initialized with conformal flattening. Our algorithm minimizes the structure's strain energy in its deployed state and optimizes actuation forces so that the final deployed structure is in stable equilibrium close to the desired surface with few or no external constraints. We validate our approach by fabricating a series of physical models at various scales using different manufacturing techniques.",
                        "uid": "v-siggraph-4",
                        "file_name": "",
                        "time_stamp": "2022-10-19T16:53:44Z",
                        "time_start": "2022-10-19T16:53:44Z",
                        "time_end": "2022-10-19T16:57:00Z",
                        "paper_type": "full",
                        "keywords": [
                            "Deployable structure, physics-based simulation, numerical optimization, computational design, fabrication"
                        ],
                        "has_image": "1",
                        "has_video": "524",
                        "paper_award": "",
                        "image_caption": "An umbrella mesh is a volumetric deployable structure with a compact, zero-energy rest state that deploys into a given 3D target surface. We show here the physical model of an umbrella mesh unit cell and the deployment sequence of an umbrella mesh prototype optimized to match the input design surface.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "https://youtu.be/nTJUle-q4yQ",
                        "ff_id": "nTJUle-q4yQ"
                    }
                ]
            }
        ]
    },
    "v-vr": {
        "event": "VR Invited Partnership Presentations",
        "long_name": "VR Invited Partnership Presentations",
        "event_type": "Invited Partnership Presentations",
        "event_prefix": "v-vr",
        "event_description": "",
        "event_url": "",
        "organizers": [],
        "sessions": [
            {
                "title": "VR Invited Talks",
                "session_id": "vr1",
                "event_prefix": "v-vr",
                "track": "mistletoe",
                "livestream_id": "mistletoe-thu",
                "session_image": "vr1.png",
                "chair": [
                    "David Laidlaw"
                ],
                "organizers": [],
                "time_start": "2022-10-20T19:00:00Z",
                "time_end": "2022-10-20T20:15:00Z",
                "discord_category": "",
                "discord_channel": "mistletoe",
                "discord_channel_id": "1024601055700471839",
                "discord_link": "https://discord.com/channels/978320435554947082/1024601055700471839",
                "slido_link": "https://app.sli.do/event/i28UETZaiRWr1XwX6eKjcf?section=05d57c7a-397b-4e16-a41d-7f06e8e64c37",
                "youtube_url": "https://youtu.be/QUC4mL-YR40",
                "youtube_id": "QUC4mL-YR40",
                "zoom_meeting": "",
                "zoom_password": "",
                "zoom_link": "",
                "ff_link": "https://youtu.be/OtW3MZxRHwY",
                "ff_playlist": "",
                "time_slots": [
                    {
                        "slot_id": "vr1-opening",
                        "session_id": "vr1",
                        "type": "In Person Other",
                        "title": "Session Opening",
                        "contributors": [
                            "David Laidlaw"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "",
                        "file_name": "",
                        "time_stamp": "2022-10-20T19:00:00Z",
                        "time_start": "2022-10-20T19:00:00Z",
                        "time_end": "2022-10-20T19:00:00Z",
                        "paper_type": "",
                        "keywords": [],
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "v-vr-9714117-pres",
                        "session_id": "vr1",
                        "type": "Virtual Presentation (pre-recorded)",
                        "title": "Breaking Plausibility Without Breaking Presence - Evidence For The Multi-Layer Nature Of Plausibility",
                        "contributors": [
                            "Franziska Westermeier"
                        ],
                        "authors": [
                            "Larissa Br\u00fcbach",
                            "Franziska Westermeier",
                            "Carolin Wienrich",
                            "Marc Erich Latoschik"
                        ],
                        "abstract": "A novel theoretical model recently introduced coherence and plausibility as the essential conditions of XR experiences, challenging contemporary presence-oriented concepts. This article reports on two experiments validating this model, which assumes coherence activation on three layers (cognition, perception, and sensation) as the potential sources leading to a condition of plausibility and from there to other XR qualia such as presence or body ownership. The experiments introduce and utilize breaks in plausibility (in analogy to breaks in presence): We induce incoherence on the perceptual and the cognitive layer simultaneously by a simulation of object behaviors that do not conform to the laws of physics, i.e., gravity. We show that this manipulation breaks plausibility and hence confirm that it results in the desired effects in the theorized condition space but that the breaks in plausibility did not affect presence. In addition, we show that a cognitive manipulation by a storyline framing is too weak to successfully counteract the strong bottom-up inconsistencies. Both results are in line with the predictions of the recently introduced three-layer model of coherence and plausibility, which incorporates well-known top-down and bottom-up rivalries and its theorized increased independence between plausibility and presence.",
                        "uid": "v-vr-9714117",
                        "file_name": "v-vr-9714117_Bruebach_Presentation.mp4",
                        "time_stamp": "2022-10-20T19:00:00Z",
                        "time_start": "2022-10-20T19:00:00Z",
                        "time_end": "2022-10-20T19:05:56Z",
                        "paper_type": "full",
                        "keywords": [
                            "A novel theoretical model recently introduced coherence and plausibility as the essential conditions of XR experiences, challenging contemporary presence-oriented concepts. This article reports on two experiments validating this model, which assumes coherence activation on three layers (cognition, perception, and sensation) as the potential sources leading to a condition of plausibility and from there to other XR qualia such as presence or body ownership. The experiments introduce and utilize breaks in plausibility (in analogy to breaks in presence): We induce incoherence on the perceptual and the cognitive layer simultaneously by a simulation of object behaviors that do not conform to the laws of physics, i.e., gravity. We show that this manipulation breaks plausibility and hence confirm that it results in the desired effects in the theorized condition space but that the breaks in plausibility did not affect presence. In addition, we show that a cognitive manipulation by a storyline framing is too weak to successfully counteract the strong bottom-up inconsistencies. Both results are in line with the predictions of the recently introduced three-layer model of coherence and plausibility, which incorporates well-known top-down and bottom-up rivalries and its theorized increased independence between plausibility and presence."
                        ],
                        "has_image": "1",
                        "has_video": "356",
                        "paper_award": "",
                        "image_caption": "Environment of the experiment showing the participant\u2019s interaction with the circuit breakers in order to fix the ship after the crash.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "https://youtu.be/eK2f_sV5JEE",
                        "ff_id": "eK2f_sV5JEE"
                    },
                    {
                        "slot_id": "v-vr-9714117-qa",
                        "session_id": "vr1",
                        "type": "Virtual Q+A",
                        "title": "Breaking Plausibility Without Breaking Presence - Evidence For The Multi-Layer Nature Of Plausibility (Q+A)",
                        "contributors": [
                            "Franziska Westermeier"
                        ],
                        "authors": [],
                        "abstract": "A novel theoretical model recently introduced coherence and plausibility as the essential conditions of XR experiences, challenging contemporary presence-oriented concepts. This article reports on two experiments validating this model, which assumes coherence activation on three layers (cognition, perception, and sensation) as the potential sources leading to a condition of plausibility and from there to other XR qualia such as presence or body ownership. The experiments introduce and utilize breaks in plausibility (in analogy to breaks in presence): We induce incoherence on the perceptual and the cognitive layer simultaneously by a simulation of object behaviors that do not conform to the laws of physics, i.e., gravity. We show that this manipulation breaks plausibility and hence confirm that it results in the desired effects in the theorized condition space but that the breaks in plausibility did not affect presence. In addition, we show that a cognitive manipulation by a storyline framing is too weak to successfully counteract the strong bottom-up inconsistencies. Both results are in line with the predictions of the recently introduced three-layer model of coherence and plausibility, which incorporates well-known top-down and bottom-up rivalries and its theorized increased independence between plausibility and presence.",
                        "uid": "v-vr-9714117",
                        "file_name": "",
                        "time_stamp": "2022-10-20T19:05:56Z",
                        "time_start": "2022-10-20T19:05:56Z",
                        "time_end": "2022-10-20T19:12:00Z",
                        "paper_type": "full",
                        "keywords": [
                            "A novel theoretical model recently introduced coherence and plausibility as the essential conditions of XR experiences, challenging contemporary presence-oriented concepts. This article reports on two experiments validating this model, which assumes coherence activation on three layers (cognition, perception, and sensation) as the potential sources leading to a condition of plausibility and from there to other XR qualia such as presence or body ownership. The experiments introduce and utilize breaks in plausibility (in analogy to breaks in presence): We induce incoherence on the perceptual and the cognitive layer simultaneously by a simulation of object behaviors that do not conform to the laws of physics, i.e., gravity. We show that this manipulation breaks plausibility and hence confirm that it results in the desired effects in the theorized condition space but that the breaks in plausibility did not affect presence. In addition, we show that a cognitive manipulation by a storyline framing is too weak to successfully counteract the strong bottom-up inconsistencies. Both results are in line with the predictions of the recently introduced three-layer model of coherence and plausibility, which incorporates well-known top-down and bottom-up rivalries and its theorized increased independence between plausibility and presence."
                        ],
                        "has_image": "1",
                        "has_video": "356",
                        "paper_award": "",
                        "image_caption": "Environment of the experiment showing the participant\u2019s interaction with the circuit breakers in order to fix the ship after the crash.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "https://youtu.be/eK2f_sV5JEE",
                        "ff_id": "eK2f_sV5JEE"
                    },
                    {
                        "slot_id": "v-vr-9756791-pres",
                        "session_id": "vr1",
                        "type": "Virtual Presentation (pre-recorded)",
                        "title": "Combining Real-World Constraints on User Behavior with Deep Neural Networks for Virtual Reality (VR) Biometrics",
                        "contributors": [
                            "Sean Banerjee"
                        ],
                        "authors": [
                            "Robert Miller",
                            "Natasha Kholgade Banerjee",
                            "Sean Banerjee"
                        ],
                        "abstract": "Deep networks have demonstrated enormous potential for identification and authentication using behavioral biometrics in virtual reality (VR). However, existing VR behavioral biometrics datasets have small sample sizes which can make it challenging for deep networks to automatically learn features that characterize real-world user behavior and that may enable high success, e.g., high-level spatial relationships between headset and hand controller devices and underlying smoothness of trajectories despite noise. We provide an approach to perform behavioral biometrics using deep networks while incorporating spatial and smoothing constraints on input data to represent real-world behavior. We represent the input data to neural networks as a combination of scale- and translation-invariant device-centric position and orientation features, and displacement vectors representing spatial relationships between device pairs. We assess identification and authentication by including spatial relationships and by performing Gaussian smoothing of the position features. We evaluate our approach against baseline methods that use the raw data directly and that perform a global normalization of the data. By using displacement vectors, our work shows higher success over baseline methods in 36 out of 42 cases of analysis done by varying user sets and pairings of VR systems and sessions.",
                        "uid": "v-vr-9756791",
                        "file_name": "v-vr-9756791_Miller_Presentation.mp4",
                        "time_stamp": "2022-10-20T19:12:00Z",
                        "time_start": "2022-10-20T19:12:00Z",
                        "time_end": "2022-10-20T19:18:56Z",
                        "paper_type": "full",
                        "keywords": [
                            "Virtual reality; Biometrics"
                        ],
                        "has_image": "1",
                        "has_video": "416",
                        "paper_award": "",
                        "image_caption": "We provide user identification and authentication using behavioral biometrics in virtual reality by augmenting orientation and normalized position features, expressed within the local coordinate systems of the hand controllers and headset devices, with inter-device displacement vectors. We demonstrate that using inter-device displacement vectors provides maximum success rate more often than baseline methods.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "v-vr-9756791-qa",
                        "session_id": "vr1",
                        "type": "Virtual Q+A",
                        "title": "Combining Real-World Constraints on User Behavior with Deep Neural Networks for Virtual Reality (VR) Biometrics (Q+A)",
                        "contributors": [
                            "Sean Banerjee"
                        ],
                        "authors": [],
                        "abstract": "Deep networks have demonstrated enormous potential for identification and authentication using behavioral biometrics in virtual reality (VR). However, existing VR behavioral biometrics datasets have small sample sizes which can make it challenging for deep networks to automatically learn features that characterize real-world user behavior and that may enable high success, e.g., high-level spatial relationships between headset and hand controller devices and underlying smoothness of trajectories despite noise. We provide an approach to perform behavioral biometrics using deep networks while incorporating spatial and smoothing constraints on input data to represent real-world behavior. We represent the input data to neural networks as a combination of scale- and translation-invariant device-centric position and orientation features, and displacement vectors representing spatial relationships between device pairs. We assess identification and authentication by including spatial relationships and by performing Gaussian smoothing of the position features. We evaluate our approach against baseline methods that use the raw data directly and that perform a global normalization of the data. By using displacement vectors, our work shows higher success over baseline methods in 36 out of 42 cases of analysis done by varying user sets and pairings of VR systems and sessions.",
                        "uid": "v-vr-9756791",
                        "file_name": "",
                        "time_stamp": "2022-10-20T19:18:56Z",
                        "time_start": "2022-10-20T19:18:56Z",
                        "time_end": "2022-10-20T19:24:00Z",
                        "paper_type": "full",
                        "keywords": [
                            "Virtual reality; Biometrics"
                        ],
                        "has_image": "1",
                        "has_video": "416",
                        "paper_award": "",
                        "image_caption": "We provide user identification and authentication using behavioral biometrics in virtual reality by augmenting orientation and normalized position features, expressed within the local coordinate systems of the hand controllers and headset devices, with inter-device displacement vectors. We demonstrate that using inter-device displacement vectors provides maximum success rate more often than baseline methods.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "v-vr-9756796-pres",
                        "session_id": "vr1",
                        "type": "Virtual Presentation (pre-recorded)",
                        "title": "Real-Time Gaze Tracking with Event-Driven Eye Segmentation",
                        "contributors": [
                            "Feng Yu"
                        ],
                        "authors": [
                            "Yu Feng",
                            "Nathan Goulding-Hotta",
                            "Asif Khan",
                            "Hans Reyserhove",
                            "Yuhao Zhu"
                        ],
                        "abstract": "Gaze tracking is increasingly becoming an essential component in Augmented and Virtual Reality. Modern gaze tracking algorithms are heavyweight; they operate at most 5 Hz on mobile processors despite that near-eye cameras comfortably operate at a real-time rate (> 30 Hz). This paper presents a real-time eye tracking algorithm that, on average, operates at 30 Hz on a mobile processor, achieves 0.1\u00b0\u20130.5\u00b0 gaze accuracies, all the while requiring only 30K parameters, one to two orders of magnitude smaller than state-of-the-art eye tracking algorithms. The crux of our algorithm is an Auto ROI mode, which continuously predicts the Regions of Interest (ROIs) of near-eye images and judiciously processes only the ROIs for gaze estimation. To that end, we introduce a novel, lightweight ROI prediction algorithm by emulating an event camera. We discuss how a software emulation of events enables accurate ROI prediction without requiring special hardware. The code of our paper is available at https://github.com/horizon-research/edgaze.",
                        "uid": "v-vr-9756796",
                        "file_name": "v-vr-9756796_Feng_Presentation.mp4",
                        "time_stamp": "2022-10-20T19:24:00Z",
                        "time_start": "2022-10-20T19:24:00Z",
                        "time_end": "2022-10-20T19:32:56Z",
                        "paper_type": "full",
                        "keywords": [
                            "Gaze, eye tracking, event camera, segmentation"
                        ],
                        "has_image": "1",
                        "has_video": "536",
                        "paper_award": "",
                        "image_caption": "Event-Driven ROI-based eye segmentation.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "https://youtu.be/I26KLGp16ZM",
                        "ff_id": "I26KLGp16ZM"
                    },
                    {
                        "slot_id": "v-vr-9756796-qa",
                        "session_id": "vr1",
                        "type": "Virtual Q+A",
                        "title": "Real-Time Gaze Tracking with Event-Driven Eye Segmentation (Q+A)",
                        "contributors": [
                            "Feng Yu"
                        ],
                        "authors": [],
                        "abstract": "Gaze tracking is increasingly becoming an essential component in Augmented and Virtual Reality. Modern gaze tracking algorithms are heavyweight; they operate at most 5 Hz on mobile processors despite that near-eye cameras comfortably operate at a real-time rate (> 30 Hz). This paper presents a real-time eye tracking algorithm that, on average, operates at 30 Hz on a mobile processor, achieves 0.1\u00b0\u20130.5\u00b0 gaze accuracies, all the while requiring only 30K parameters, one to two orders of magnitude smaller than state-of-the-art eye tracking algorithms. The crux of our algorithm is an Auto ROI mode, which continuously predicts the Regions of Interest (ROIs) of near-eye images and judiciously processes only the ROIs for gaze estimation. To that end, we introduce a novel, lightweight ROI prediction algorithm by emulating an event camera. We discuss how a software emulation of events enables accurate ROI prediction without requiring special hardware. The code of our paper is available at https://github.com/horizon-research/edgaze.",
                        "uid": "v-vr-9756796",
                        "file_name": "",
                        "time_stamp": "2022-10-20T19:32:56Z",
                        "time_start": "2022-10-20T19:32:56Z",
                        "time_end": "2022-10-20T19:36:00Z",
                        "paper_type": "full",
                        "keywords": [
                            "Gaze, eye tracking, event camera, segmentation"
                        ],
                        "has_image": "1",
                        "has_video": "536",
                        "paper_award": "",
                        "image_caption": "Event-Driven ROI-based eye segmentation.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "https://youtu.be/I26KLGp16ZM",
                        "ff_id": "I26KLGp16ZM"
                    },
                    {
                        "slot_id": "v-vr-9714118-pres",
                        "session_id": "vr1",
                        "type": "Virtual Presentation (pre-recorded)",
                        "title": "Mood-Driven Colorization of Virtual Indoor Scenes",
                        "contributors": [
                            "Michael Solah"
                        ],
                        "authors": [
                            "Michael S Solah",
                            "Haikun Huang",
                            "Jiachuan Sheng",
                            "Tian Feng",
                            "Marc Pomplun",
                            "Lap-Fai Yu"
                        ],
                        "abstract": "One of the challenging tasks in virtual scene design for Virtual Reality (VR) is causing it to invoke a particular mood in viewers. The subjective nature of moods brings uncertainty to the purpose. We propose a novel approach to automatic adjustment of the colors of textures for objects in a virtual indoor scene, enabling it to match a target mood. A dataset of 25,000 images, including building/home interiors, was used to train a classifier with the features extracted via deep learning. It contributes to an optimization process that colorizes virtual scenes automatically according to the target mood. Our approach was tested on four different indoor scenes, and we conducted a user study demonstrating its efficacy through statistical analysis with the focus on the impact of the scenes experienced with a VR headset.",
                        "uid": "v-vr-9714118",
                        "file_name": "v-vr-9714118_Solah_Presentation.mp4",
                        "time_stamp": "2022-10-20T19:36:00Z",
                        "time_start": "2022-10-20T19:36:00Z",
                        "time_end": "2022-10-20T19:44:12Z",
                        "paper_type": "full",
                        "keywords": [
                            "Virtual reality; Perception; Visualization design and evaluation methods"
                        ],
                        "has_image": "1",
                        "has_video": "492",
                        "paper_award": "",
                        "image_caption": "An image showing how our approach works. We take a virtual indoor environment and run our optimization process to colorize the textures of objects. The goal is for the colors in the environment to match a target mood. Our approach uses an optimization process with a classifier trained on a dataset of indoor images with features obtained through deep learning. This image shows a bedroom environment, with the input on the left side and the result on the right side. The input mood was cheerful. A person on the lower left is viewing the scene with a VR headset.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "https://youtu.be/LLLRbDES770",
                        "ff_id": "LLLRbDES770"
                    },
                    {
                        "slot_id": "v-vr-9714118-qa",
                        "session_id": "vr1",
                        "type": "Virtual Q+A",
                        "title": "Mood-Driven Colorization of Virtual Indoor Scenes (Q+A)",
                        "contributors": [
                            "Michael Solah"
                        ],
                        "authors": [],
                        "abstract": "One of the challenging tasks in virtual scene design for Virtual Reality (VR) is causing it to invoke a particular mood in viewers. The subjective nature of moods brings uncertainty to the purpose. We propose a novel approach to automatic adjustment of the colors of textures for objects in a virtual indoor scene, enabling it to match a target mood. A dataset of 25,000 images, including building/home interiors, was used to train a classifier with the features extracted via deep learning. It contributes to an optimization process that colorizes virtual scenes automatically according to the target mood. Our approach was tested on four different indoor scenes, and we conducted a user study demonstrating its efficacy through statistical analysis with the focus on the impact of the scenes experienced with a VR headset.",
                        "uid": "v-vr-9714118",
                        "file_name": "",
                        "time_stamp": "2022-10-20T19:44:12Z",
                        "time_start": "2022-10-20T19:44:12Z",
                        "time_end": "2022-10-20T19:48:00Z",
                        "paper_type": "full",
                        "keywords": [
                            "Virtual reality; Perception; Visualization design and evaluation methods"
                        ],
                        "has_image": "1",
                        "has_video": "492",
                        "paper_award": "",
                        "image_caption": "An image showing how our approach works. We take a virtual indoor environment and run our optimization process to colorize the textures of objects. The goal is for the colors in the environment to match a target mood. Our approach uses an optimization process with a classifier trained on a dataset of indoor images with features obtained through deep learning. This image shows a bedroom environment, with the input on the left side and the result on the right side. The input mood was cheerful. A person on the lower left is viewing the scene with a VR headset.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "https://youtu.be/LLLRbDES770",
                        "ff_id": "LLLRbDES770"
                    },
                    {
                        "slot_id": "v-vr-9714040-pres",
                        "session_id": "vr1",
                        "type": "Virtual Presentation (pre-recorded)",
                        "title": "Omnidirectional Galvanic Vestibular Stimulation in Virtual Reality",
                        "contributors": [
                            "Colin Groth"
                        ],
                        "authors": [
                            "Colin Groth",
                            "Jan-Philipp Tauscher",
                            "Nikkel Heesen",
                            "Max Hattenbach",
                            "Susana Castillo",
                            "Marcus Magnor"
                        ],
                        "abstract": "In this paper we propose omnidirectional galvanic vestibular stimulation (GVS) to mitigate cybersickness in virtual reality applications. One of the most accepted theories indicates that Cybersickness is caused by the visually induced impression of ego motion while physically remaining at rest. As a result of this sensory mismatch, people associate negative symptoms with VR and sometimes avoid the technology altogether. To reconcile the two contradicting sensory perceptions, we investigate GVS to stimulate the vestibular canals behind our ears with low-current electrical signals that are specifically attuned to the visually displayed camera motion. We describe how to calibrate and generate the appropriate GVS signals in real-time for pre-recorded omnidirectional videos exhibiting ego-motion in all three spatial directions. For validation, we conduct an experiment presenting real-world 360\u00b0 videos shot from a moving first-person perspective in a VR head-mounted display. Our findings indicate that GVS is able to significantly reduce discomfort for cybersickness-susceptible VR users, creating a deeper and more enjoyable immersive experience for many people.",
                        "uid": "v-vr-9714040",
                        "file_name": "v-vr-9714040_Groth_Presentation.mp4",
                        "time_stamp": "2022-10-20T19:48:00Z",
                        "time_start": "2022-10-20T19:48:00Z",
                        "time_end": "2022-10-20T19:58:33Z",
                        "paper_type": "full",
                        "keywords": [
                            "Galvanic Vestibular Stimulation, GVS, Virtual Reality, VR, 360 Videos, Cybersickness, Presence"
                        ],
                        "has_image": "1",
                        "has_video": "633",
                        "paper_award": "",
                        "image_caption": "To reduce cybersickness for moving-camera sequences in VR, we evaluate the effectiveness of galvanic vestibular stimulation. We stimulate the VR user's vestibular sense in all three spatial directions taking the motion of the camera in the 360\u00b0 video as well as the user\u2019s current viewing direction into account. This way, we aim to reconcile visually induced and felt self-motion.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "https://youtu.be/lUH0qri84Qk",
                        "ff_id": "lUH0qri84Qk"
                    },
                    {
                        "slot_id": "v-vr-9714040-qa",
                        "session_id": "vr1",
                        "type": "Virtual Q+A",
                        "title": "Omnidirectional Galvanic Vestibular Stimulation in Virtual Reality (Q+A)",
                        "contributors": [
                            "Colin Groth"
                        ],
                        "authors": [],
                        "abstract": "In this paper we propose omnidirectional galvanic vestibular stimulation (GVS) to mitigate cybersickness in virtual reality applications. One of the most accepted theories indicates that Cybersickness is caused by the visually induced impression of ego motion while physically remaining at rest. As a result of this sensory mismatch, people associate negative symptoms with VR and sometimes avoid the technology altogether. To reconcile the two contradicting sensory perceptions, we investigate GVS to stimulate the vestibular canals behind our ears with low-current electrical signals that are specifically attuned to the visually displayed camera motion. We describe how to calibrate and generate the appropriate GVS signals in real-time for pre-recorded omnidirectional videos exhibiting ego-motion in all three spatial directions. For validation, we conduct an experiment presenting real-world 360\u00b0 videos shot from a moving first-person perspective in a VR head-mounted display. Our findings indicate that GVS is able to significantly reduce discomfort for cybersickness-susceptible VR users, creating a deeper and more enjoyable immersive experience for many people.",
                        "uid": "v-vr-9714040",
                        "file_name": "",
                        "time_stamp": "2022-10-20T19:58:33Z",
                        "time_start": "2022-10-20T19:58:33Z",
                        "time_end": "2022-10-20T20:00:00Z",
                        "paper_type": "full",
                        "keywords": [
                            "Galvanic Vestibular Stimulation, GVS, Virtual Reality, VR, 360 Videos, Cybersickness, Presence"
                        ],
                        "has_image": "1",
                        "has_video": "633",
                        "paper_award": "",
                        "image_caption": "To reduce cybersickness for moving-camera sequences in VR, we evaluate the effectiveness of galvanic vestibular stimulation. We stimulate the VR user's vestibular sense in all three spatial directions taking the motion of the camera in the 360\u00b0 video as well as the user\u2019s current viewing direction into account. This way, we aim to reconcile visually induced and felt self-motion.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "https://youtu.be/lUH0qri84Qk",
                        "ff_id": "lUH0qri84Qk"
                    },
                    {
                        "slot_id": "v-vr-9714044-pres",
                        "session_id": "vr1",
                        "type": "Virtual Presentation (pre-recorded)",
                        "title": "Comparing Direct and Indirect Methods of Audio Quality Evaluation in Virtual Reality Scenes of Varying Complexity",
                        "contributors": [
                            "Thomas Robotham"
                        ],
                        "authors": [
                            "Thomas Robotham",
                            "Olli S. Rummukainen",
                            "Miriam Kurz",
                            "Marie Eckert",
                            "Emanu\u00ebl A. P. Habets"
                        ],
                        "abstract": "Many quality evaluation methods are used to assess uni-modal audio or video content without considering perceptual, cognitive, and interactive aspects present in virtual reality (VR) settings. Consequently, little is known regarding the repercussions of the employed evaluation method, content, and subject behavior on the quality ratings in VR. This mixed between- and within-subjects study uses four subjective audio quality evaluation methods (viz. multiple-stimulus with and without reference for direct scaling, and rank-order elimination and pairwise comparison for indirect scaling) to investigate the contributing factors present in multi-modal 6-DoF VR on quality ratings of real-time audio rendering. For each between-subjects employed method, two sets of conditions in five VR scenes were evaluated within-subjects. The conditions targeted relevant attributes for binaural audio reproduction using scenes with various amounts of user interactivity. Our results show all referenceless methods produce similar results using both condition sets. However, rank-order elimination proved to be the fastest method, required the least amount of repetitive motion, and yielded the highest discrimination between spatial conditions. Scene complexity was found to be a main effect within results, with behavioral and task load index results implying more complex scenes and interactive aspects of 6-DoF VR can impede quality judgments.",
                        "uid": "v-vr-9714044",
                        "file_name": "v-vr-9714044_Robotham_Presentation.mp4",
                        "time_stamp": "2022-10-20T20:00:00Z",
                        "time_start": "2022-10-20T20:00:00Z",
                        "time_end": "2022-10-20T20:06:58Z",
                        "paper_type": "full",
                        "keywords": [
                            "Multi-modal, virtual reality, 6-Degrees-of-freedom, audio quality, direct scaling, indirect scaling, evaluation methods"
                        ],
                        "has_image": "1",
                        "has_video": "418",
                        "paper_award": "",
                        "image_caption": "Perceptual study focusing on evaluation methods in multi-modal interactive virtual environment settings with different levels of complexity.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "https://youtu.be/K3wDi-rffFk",
                        "ff_id": "K3wDi-rffFk"
                    },
                    {
                        "slot_id": "v-vr-9714044-qa",
                        "session_id": "vr1",
                        "type": "Virtual Q+A",
                        "title": "Comparing Direct and Indirect Methods of Audio Quality Evaluation in Virtual Reality Scenes of Varying Complexity (Q+A)",
                        "contributors": [
                            "Thomas Robotham"
                        ],
                        "authors": [],
                        "abstract": "Many quality evaluation methods are used to assess uni-modal audio or video content without considering perceptual, cognitive, and interactive aspects present in virtual reality (VR) settings. Consequently, little is known regarding the repercussions of the employed evaluation method, content, and subject behavior on the quality ratings in VR. This mixed between- and within-subjects study uses four subjective audio quality evaluation methods (viz. multiple-stimulus with and without reference for direct scaling, and rank-order elimination and pairwise comparison for indirect scaling) to investigate the contributing factors present in multi-modal 6-DoF VR on quality ratings of real-time audio rendering. For each between-subjects employed method, two sets of conditions in five VR scenes were evaluated within-subjects. The conditions targeted relevant attributes for binaural audio reproduction using scenes with various amounts of user interactivity. Our results show all referenceless methods produce similar results using both condition sets. However, rank-order elimination proved to be the fastest method, required the least amount of repetitive motion, and yielded the highest discrimination between spatial conditions. Scene complexity was found to be a main effect within results, with behavioral and task load index results implying more complex scenes and interactive aspects of 6-DoF VR can impede quality judgments.",
                        "uid": "v-vr-9714044",
                        "file_name": "",
                        "time_stamp": "2022-10-20T20:06:58Z",
                        "time_start": "2022-10-20T20:06:58Z",
                        "time_end": "2022-10-20T20:12:00Z",
                        "paper_type": "full",
                        "keywords": [
                            "Multi-modal, virtual reality, 6-Degrees-of-freedom, audio quality, direct scaling, indirect scaling, evaluation methods"
                        ],
                        "has_image": "1",
                        "has_video": "418",
                        "paper_award": "",
                        "image_caption": "Perceptual study focusing on evaluation methods in multi-modal interactive virtual environment settings with different levels of complexity.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "https://youtu.be/K3wDi-rffFk",
                        "ff_id": "K3wDi-rffFk"
                    }
                ]
            }
        ]
    },
    "v-panels1": {
        "event": "Grand Challenges in Visual Analytic Systems",
        "long_name": "Grand Challenges in Visual Analytic Systems",
        "event_type": "Panel",
        "event_prefix": "v-panels1",
        "event_description": "",
        "event_url": "",
        "organizers": [
            "Aoyu Wu",
            "Dazhen Deng",
            "Min Chen",
            "Shixia Liu",
            "Daniel Keim",
            "Ross Maciejewski",
            "Silvia Miksch",
            "Hendrik Strobelt"
        ],
        "sessions": [
            {
                "title": "Grand Challenges in Visual Analytic Systems",
                "session_id": "panel1",
                "event_prefix": "v-panels1",
                "track": "pinon",
                "livestream_id": "pinon-wed",
                "session_image": "panel1.png",
                "chair": [
                    "Aoyu Wu",
                    "Dazhen Deng",
                    "Min Chen",
                    "Shixia Liu",
                    "Daniel Keim",
                    "Ross Maciejewski",
                    "Silvia Miksch",
                    "Hendrik Strobelt"
                ],
                "organizers": [],
                "time_start": "2022-10-19T20:45:00Z",
                "time_end": "2022-10-19T22:00:00Z",
                "discord_category": "",
                "discord_channel": "pinon",
                "discord_channel_id": "1024601027644760074",
                "discord_link": "https://discord.com/channels/978320435554947082/1024601027644760074",
                "slido_link": "https://app.sli.do/event/i28UETZaiRWr1XwX6eKjcf?section=bbc002b7-d3ae-48bc-b2d3-0624a7bfed06",
                "youtube_url": "https://youtu.be/bepnKhvWRtU",
                "youtube_id": "bepnKhvWRtU",
                "zoom_meeting": "",
                "zoom_password": "",
                "zoom_link": "",
                "ff_link": "",
                "ff_playlist": "",
                "time_slots": [
                    {
                        "slot_id": "panel1-prog-1",
                        "session_id": "panel1",
                        "type": "Mixed Panel (virtual/in person)",
                        "title": "Panel",
                        "contributors": [
                            "Aoyu Wu",
                            "Dazhen Deng",
                            "Min Chen",
                            "Shixia Liu",
                            "Daniel Keim",
                            "Ross Maciejewski",
                            "Silvia Miksch",
                            "Hendrik Strobelt"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "",
                        "file_name": "",
                        "time_stamp": "2022-10-19T20:45:00Z",
                        "time_start": "2022-10-19T20:45:00Z",
                        "time_end": "2022-10-19T22:00:00Z",
                        "paper_type": "",
                        "keywords": [],
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    }
                ]
            }
        ]
    },
    "v-panels2": {
        "event": "Merits and Limits of User Study Preregistration",
        "long_name": "Merits and Limits of User Study Preregistration",
        "event_type": "Panel",
        "event_prefix": "v-panels2",
        "event_description": "",
        "event_url": "",
        "organizers": [
            "Brian Nosek",
            "Tamarinde Haven",
            "Miriah Meyer",
            "Lonni Besan\u00e7on",
            "Cody Dunne",
            "Mohammad Ghoniem"
        ],
        "sessions": [
            {
                "title": "Merits and Limits of User Study Preregistration",
                "session_id": "panel2",
                "event_prefix": "v-panels2",
                "track": "pinon",
                "livestream_id": "pinon-thu",
                "session_image": "panel2.png",
                "chair": [
                    "Brian Nosek",
                    "Tamarinde Haven",
                    "Miriah Meyer",
                    "Lonni Besan\u00e7on",
                    "Cody Dunne",
                    "Mohammad Ghoniem"
                ],
                "organizers": [],
                "time_start": "2022-10-20T15:45:00Z",
                "time_end": "2022-10-20T17:00:00Z",
                "discord_category": "",
                "discord_channel": "pinon",
                "discord_channel_id": "1024601027644760074",
                "discord_link": "https://discord.com/channels/978320435554947082/1024601027644760074",
                "slido_link": "https://app.sli.do/event/i28UETZaiRWr1XwX6eKjcf?section=bbc002b7-d3ae-48bc-b2d3-0624a7bfed06",
                "youtube_url": "https://youtu.be/Dj-iscrBm6g",
                "youtube_id": "Dj-iscrBm6g",
                "zoom_meeting": "",
                "zoom_password": "",
                "zoom_link": "",
                "ff_link": "",
                "ff_playlist": "",
                "time_slots": [
                    {
                        "slot_id": "panel2-prog-1",
                        "session_id": "panel2",
                        "type": "Mixed Panel (virtual/in person)",
                        "title": "Panel",
                        "contributors": [
                            "Brian Nosek",
                            "Tamarinde Haven",
                            "Miriah Meyer",
                            "Lonni Besan\u00e7on",
                            "Cody Dunne",
                            "Mohammad Ghoniem"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "",
                        "file_name": "",
                        "time_stamp": "2022-10-20T15:45:00Z",
                        "time_start": "2022-10-20T15:45:00Z",
                        "time_end": "2022-10-20T17:00:00Z",
                        "paper_type": "",
                        "keywords": [],
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    }
                ]
            }
        ]
    },
    "v-panels3": {
        "event": "Is This (Panel) Good Enough for IEEE VIS?",
        "long_name": "Is This (Panel) Good Enough for IEEE VIS?",
        "event_type": "Panel",
        "event_prefix": "v-panels3",
        "event_description": "",
        "event_url": "",
        "organizers": [
            "Cody Dunne",
            "Alexander Lex",
            "Torsten M\u00f6ller",
            "Alvitta Ottley",
            "Melanie Tory",
            "Robert Laramee",
            "Petra Isenberg",
            "Tobias Isenberg"
        ],
        "sessions": [
            {
                "title": "Is This (Panel) Good Enough for IEEE VIS?",
                "session_id": "panel3",
                "event_prefix": "v-panels3",
                "track": "pinon",
                "livestream_id": "pinon-fri",
                "session_image": "panel3.png",
                "chair": [
                    "Cody Dunne",
                    "Alexander Lex",
                    "Torsten M\u00f6ller",
                    "Alvitta Ottley",
                    "Melanie Tory",
                    "Robert Laramee",
                    "Petra Isenberg",
                    "Tobias Isenberg"
                ],
                "organizers": [],
                "time_start": "2022-10-21T14:00:00Z",
                "time_end": "2022-10-21T15:15:00Z",
                "discord_category": "",
                "discord_channel": "pinon",
                "discord_channel_id": "1024601027644760074",
                "discord_link": "https://discord.com/channels/978320435554947082/1024601027644760074",
                "slido_link": "https://app.sli.do/event/i28UETZaiRWr1XwX6eKjcf?section=bbc002b7-d3ae-48bc-b2d3-0624a7bfed06",
                "youtube_url": "https://youtu.be/E4DOKtV7hMM",
                "youtube_id": "E4DOKtV7hMM",
                "zoom_meeting": "",
                "zoom_password": "",
                "zoom_link": "",
                "ff_link": "",
                "ff_playlist": "",
                "time_slots": [
                    {
                        "slot_id": "panel3-prog-1",
                        "session_id": "panel3",
                        "type": "Mixed Panel (virtual/in person)",
                        "title": "Panel",
                        "contributors": [
                            "Cody Dunne",
                            "Alexander Lex",
                            "Torsten M\u00f6ller",
                            "Alvitta Ottley",
                            "Melanie Tory",
                            "Robert Laramee",
                            "Petra Isenberg",
                            "Tobias Isenberg"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "",
                        "file_name": "",
                        "time_stamp": "2022-10-21T14:00:00Z",
                        "time_start": "2022-10-21T14:00:00Z",
                        "time_end": "2022-10-21T15:15:00Z",
                        "paper_type": "",
                        "keywords": [],
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    }
                ]
            }
        ]
    },
    "a-vizsec": {
        "event": "VizSec",
        "long_name": "VizSec",
        "event_type": "Associated Event (Symposium)",
        "event_prefix": "a-vizsec",
        "event_description": "",
        "event_url": "",
        "organizers": [
            "Chris Bryan",
            "Steven Gomez",
            "Sanchari Das",
            "Lyndsey R. Franklin",
            "Fabian B\u00f6hm",
            "No\u00eblle Rakotondravony",
            "Alex Ulmer"
        ],
        "sessions": [
            {
                "title": "VizSec: Opening and Keynote",
                "session_id": "a-vizsec-1",
                "event_prefix": "a-vizsec",
                "track": "pinon",
                "livestream_id": "pinon-wed",
                "session_image": "a-vizsec-1.png",
                "chair": [
                    "Chris Bryan"
                ],
                "organizers": [],
                "time_start": "2022-10-19T14:00:00Z",
                "time_end": "2022-10-19T15:15:00Z",
                "discord_category": "",
                "discord_channel": "pinon",
                "discord_channel_id": "1024601027644760074",
                "discord_link": "https://discord.com/channels/978320435554947082/1024601027644760074",
                "slido_link": "https://app.sli.do/event/i28UETZaiRWr1XwX6eKjcf?section=bbc002b7-d3ae-48bc-b2d3-0624a7bfed06",
                "youtube_url": "https://youtu.be/bepnKhvWRtU",
                "youtube_id": "bepnKhvWRtU",
                "zoom_meeting": "",
                "zoom_password": "",
                "zoom_link": "",
                "ff_link": "",
                "ff_playlist": "",
                "time_slots": [
                    {
                        "slot_id": "a-vizsec-prog-1",
                        "session_id": "a-vizsec-1",
                        "type": "In Person Presentation",
                        "title": "Opening Presentation",
                        "contributors": [
                            "Chris Bryan"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "",
                        "file_name": "",
                        "time_stamp": "2022-10-19T14:00:00Z",
                        "time_start": "2022-10-19T14:00:00Z",
                        "time_end": "2022-10-19T14:15:00Z",
                        "paper_type": "",
                        "keywords": [],
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "a-vizsec-prog-2",
                        "session_id": "a-vizsec-1",
                        "type": "In Person Presentation",
                        "title": "Keynote by Dr. Jordan Crouser",
                        "contributors": [
                            "Jordan Crouser"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "",
                        "file_name": "",
                        "time_stamp": "2022-10-19T14:15:00Z",
                        "time_start": "2022-10-19T14:15:00Z",
                        "time_end": "2022-10-19T15:10:00Z",
                        "paper_type": "",
                        "keywords": [],
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "a-vizsec-prog-3",
                        "session_id": "a-vizsec-1",
                        "type": "In Person Q+A",
                        "title": "Keynote by Dr. Jordan Crouser (Q+A)",
                        "contributors": [
                            "Jordan Crouser"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "",
                        "file_name": "",
                        "time_stamp": "2022-10-19T15:10:00Z",
                        "time_start": "2022-10-19T15:10:00Z",
                        "time_end": "2022-10-19T15:15:00Z",
                        "paper_type": "",
                        "keywords": [],
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    }
                ]
            },
            {
                "title": "VizSec: Best Paper Announcement and Papers",
                "session_id": "a-vizsec-2",
                "event_prefix": "a-vizsec",
                "track": "pinon",
                "livestream_id": "pinon-wed",
                "session_image": "a-vizsec-2.png",
                "chair": [
                    "Steven Gomez"
                ],
                "organizers": [],
                "time_start": "2022-10-19T15:45:00Z",
                "time_end": "2022-10-19T17:00:00Z",
                "discord_category": "",
                "discord_channel": "pinon",
                "discord_channel_id": "1024601027644760074",
                "discord_link": "https://discord.com/channels/978320435554947082/1024601027644760074",
                "slido_link": "https://app.sli.do/event/i28UETZaiRWr1XwX6eKjcf?section=bbc002b7-d3ae-48bc-b2d3-0624a7bfed06",
                "youtube_url": "https://youtu.be/bepnKhvWRtU",
                "youtube_id": "bepnKhvWRtU",
                "zoom_meeting": "",
                "zoom_password": "",
                "zoom_link": "",
                "ff_link": "",
                "ff_playlist": "",
                "time_slots": [
                    {
                        "slot_id": "a-vizsec-prog-4",
                        "session_id": "a-vizsec-2",
                        "type": "In Person Presentation",
                        "title": "Session Intro and Best Paper Announcement",
                        "contributors": [
                            "Steven Gomez"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "",
                        "file_name": "",
                        "time_stamp": "2022-10-19T15:45:00Z",
                        "time_start": "2022-10-19T15:45:00Z",
                        "time_end": "2022-10-19T15:50:00Z",
                        "paper_type": "",
                        "keywords": [],
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "a-vizsec-3950-pres",
                        "session_id": "a-vizsec-2",
                        "type": "In Person Presentation",
                        "title": "Analysis of the Design Space for Cybersecurity Visualizations in VizSec #3950",
                        "contributors": [
                            "Adrian Komadina"
                        ],
                        "authors": [
                            "Adrian Komadina",
                            "\u017deljka Mihajlovi\u0107",
                            "Stjepan Gro\u0161"
                        ],
                        "abstract": "In this paper, we present our design study on developing an interactive visual firewall log analysis system in collaboration with an IT service provider.  We describe the human-centered design process, in which we additionally considered hedonic qualities by including the usage of personas, psychological need cards and interaction vocabulary. For the problem characterization we especially focus on the demands of the two main clusters of requirements: high-level overview and low-level analysis, represented by the two defined personas, namely information security officer and network analyst.  This resulted in the prototype of a visual analysis system consisting of two interlinked parts. One part addresses the needs for rather strategical tasks while also fulfilling the need for an appealing appearance and interaction. The other part rather addresses the requirements for operational tasks and aims to provide a high level of flexibility. We describe our design journey, the derived domain tasks and task abstractions as well as our visual design decisions, and present our final prototypes based on a usage scenario.  We also report on our capstone event, where we conducted an observed experiment and collected feedback from the information security officer. Finally, as a reflection, we propose the extension of a widely used design study process with a track for an additional focus on hedonic qualities.",
                        "uid": "a-vizsec-3950",
                        "file_name": "a-vizsec-3950_Komadina_Presentation.mp4",
                        "time_stamp": "2022-10-19T15:50:00Z",
                        "time_start": "2022-10-19T15:50:00Z",
                        "time_end": "2022-10-19T16:02:00Z",
                        "paper_type": "associated",
                        "keywords": [
                            "Information visualization; Human and societal aspects of security and privacy"
                        ],
                        "has_image": "1",
                        "has_video": "829",
                        "paper_award": "",
                        "image_caption": "Percentage of occurrence of each visualization technique in the VizSec papers",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "a-vizsec-3950-qa",
                        "session_id": "a-vizsec-2",
                        "type": "In Person Q+A",
                        "title": "Analysis of the Design Space for Cybersecurity Visualizations in VizSec #3950 (Q+A)",
                        "contributors": [
                            "Adrian Komadina"
                        ],
                        "authors": [],
                        "abstract": "In this paper, we present our design study on developing an interactive visual firewall log analysis system in collaboration with an IT service provider.  We describe the human-centered design process, in which we additionally considered hedonic qualities by including the usage of personas, psychological need cards and interaction vocabulary. For the problem characterization we especially focus on the demands of the two main clusters of requirements: high-level overview and low-level analysis, represented by the two defined personas, namely information security officer and network analyst.  This resulted in the prototype of a visual analysis system consisting of two interlinked parts. One part addresses the needs for rather strategical tasks while also fulfilling the need for an appealing appearance and interaction. The other part rather addresses the requirements for operational tasks and aims to provide a high level of flexibility. We describe our design journey, the derived domain tasks and task abstractions as well as our visual design decisions, and present our final prototypes based on a usage scenario.  We also report on our capstone event, where we conducted an observed experiment and collected feedback from the information security officer. Finally, as a reflection, we propose the extension of a widely used design study process with a track for an additional focus on hedonic qualities.",
                        "uid": "a-vizsec-3950",
                        "file_name": "",
                        "time_stamp": "2022-10-19T16:02:00Z",
                        "time_start": "2022-10-19T16:02:00Z",
                        "time_end": "2022-10-19T16:05:00Z",
                        "paper_type": "associated",
                        "keywords": [
                            "Information visualization; Human and societal aspects of security and privacy"
                        ],
                        "has_image": "1",
                        "has_video": "829",
                        "paper_award": "",
                        "image_caption": "Percentage of occurrence of each visualization technique in the VizSec papers",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "a-vizsec-7912-pres",
                        "session_id": "a-vizsec-2",
                        "type": "In Person Presentation",
                        "title": "Visualization Of Class Activation Maps To Explain AI Classification Of Network Packet Captures #7912",
                        "contributors": [
                            "Igor Cherepanov"
                        ],
                        "authors": [
                            "Igor Cherepanov",
                            "Alex Ulmer",
                            "Jonathan Geraldi Joewono",
                            "J\u00f6rn Kohlhammer"
                        ],
                        "abstract": "The classification of internet traffic has become increasingly important due to the rapid growth of today\u2019s networks and application variety. The number of connections and the addition of new applications in our networks causes a vast amount of log data and complicates the search for common patterns by experts. Finding such patterns among specific classes of applications is necessary to fulfill various requirements in network analytics. Supervised deep learning methods learn features from raw data and achieve high accuracy in classification. However, these methods are very complex and are used as black-box models, which weakens the experts\u2019 trust in these classifications. Moreover, by using them as a black-box, new knowledge cannot be obtained from the model predictions despite their excellent performance. Therefore, the explainability of the classifications is crucial. Besides increasing trust, the explanation can be used for model evaluation to gain new insights from the data and to improve the model. In this paper, we present a visual and interactive tool that combines the classification of network data with an explanation technique to form an interface between experts, algorithms, and data.",
                        "uid": "a-vizsec-7912",
                        "file_name": "a-vizsec-7912_Cherepanov_Presentation.mp4",
                        "time_stamp": "2022-10-19T16:05:00Z",
                        "time_start": "2022-10-19T16:05:00Z",
                        "time_end": "2022-10-19T16:17:00Z",
                        "paper_type": "associated",
                        "keywords": [
                            "Human-centered computing, Visualization, User interface design, Interpretability, Network Classification, Convolutional Neural Networks"
                        ],
                        "has_image": "1",
                        "has_video": "742",
                        "paper_award": "",
                        "image_caption": "Visualization Of Class Activation Maps To Explain AI Classification Of Network Packet Captures",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "a-vizsec-7912-qa",
                        "session_id": "a-vizsec-2",
                        "type": "In Person Q+A",
                        "title": "Visualization Of Class Activation Maps To Explain AI Classification Of Network Packet Captures #7912 (Q+A)",
                        "contributors": [
                            "Igor Cherepanov"
                        ],
                        "authors": [],
                        "abstract": "The classification of internet traffic has become increasingly important due to the rapid growth of today\u2019s networks and application variety. The number of connections and the addition of new applications in our networks causes a vast amount of log data and complicates the search for common patterns by experts. Finding such patterns among specific classes of applications is necessary to fulfill various requirements in network analytics. Supervised deep learning methods learn features from raw data and achieve high accuracy in classification. However, these methods are very complex and are used as black-box models, which weakens the experts\u2019 trust in these classifications. Moreover, by using them as a black-box, new knowledge cannot be obtained from the model predictions despite their excellent performance. Therefore, the explainability of the classifications is crucial. Besides increasing trust, the explanation can be used for model evaluation to gain new insights from the data and to improve the model. In this paper, we present a visual and interactive tool that combines the classification of network data with an explanation technique to form an interface between experts, algorithms, and data.",
                        "uid": "a-vizsec-7912",
                        "file_name": "",
                        "time_stamp": "2022-10-19T16:17:00Z",
                        "time_start": "2022-10-19T16:17:00Z",
                        "time_end": "2022-10-19T16:20:00Z",
                        "paper_type": "associated",
                        "keywords": [
                            "Human-centered computing, Visualization, User interface design, Interpretability, Network Classification, Convolutional Neural Networks"
                        ],
                        "has_image": "1",
                        "has_video": "742",
                        "paper_award": "",
                        "image_caption": "Visualization Of Class Activation Maps To Explain AI Classification Of Network Packet Captures",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "a-vizsec-7865-pres",
                        "session_id": "a-vizsec-2",
                        "type": "In Person Presentation",
                        "title": "Portola: A Hybrid Tree and Network Visualization Technique for Network Segmentation #7865",
                        "contributors": [
                            "Kuhu Gupta"
                        ],
                        "authors": [
                            "Kuhu Gupta",
                            "Aditeya Pandey",
                            "Larry Chan",
                            "Ambika Yadav",
                            "Brian Staats",
                            "Michelle Borkin"
                        ],
                        "abstract": "In this paper, we present research on the analysis of the design space for cybersecurity visualizations in VizSec. At the beginning of this research, we analyzed 17 survey papers in the field of cybersecurity visualization. Based on the analysis of the focus areas in each of these survey papers, we identified five key components of visual- ization design, i.e. Input Data, Security Tasks, Visual Encoding, Interactivity, and Evaluation. To show how research papers align with these components, we analyzed 60 papers published at the IEEE Symposium on Visualization for Cyber Security (VizSec) be- tween 2016 and 2021 in the context of the five identified components. As a result, each research paper was classified into several categories derived from the selected components of the visualization design. Our contributions are: (i) an analysis of the focus areas in survey papers on cybersecurity visualization and (ii) the classification of 60 research papers in the context of the selected components of the visualization design. Finally, we highlighted the main findings of the analysis and drew conclusions.",
                        "uid": "a-vizsec-7865",
                        "file_name": "a-vizsec-7865_Gupta_Presentation.mp4",
                        "time_stamp": "2022-10-19T16:20:00Z",
                        "time_start": "2022-10-19T16:20:00Z",
                        "time_end": "2022-10-19T16:28:00Z",
                        "paper_type": "associated",
                        "keywords": [
                            "Visualization\u2014Cybersecurity\u2014Analysis\u2014Survey\u2014VizSec"
                        ],
                        "has_image": "1",
                        "has_video": "504",
                        "paper_award": "",
                        "image_caption": "Portola is a hybrid tree and network visualization that presents an overview of a segmented computer network and displays connections within the network. Using Portola, analysts can explore a segmented network, identify nodes and connections of interest through exploratory network analysis and drill down on elements of interest to reason about the patterns of relationships in the network. This work also discusses the design principles that broadly apply to design network security visualizations and lessons learned from the user-centered iterative design of Portola.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "a-vizsec-7865-qa",
                        "session_id": "a-vizsec-2",
                        "type": "In Person Q+A",
                        "title": "Portola: A Hybrid Tree and Network Visualization Technique for Network Segmentation #7865 (Q+A)",
                        "contributors": [
                            "Kuhu Gupta"
                        ],
                        "authors": [],
                        "abstract": "In this paper, we present research on the analysis of the design space for cybersecurity visualizations in VizSec. At the beginning of this research, we analyzed 17 survey papers in the field of cybersecurity visualization. Based on the analysis of the focus areas in each of these survey papers, we identified five key components of visual- ization design, i.e. Input Data, Security Tasks, Visual Encoding, Interactivity, and Evaluation. To show how research papers align with these components, we analyzed 60 papers published at the IEEE Symposium on Visualization for Cyber Security (VizSec) be- tween 2016 and 2021 in the context of the five identified components. As a result, each research paper was classified into several categories derived from the selected components of the visualization design. Our contributions are: (i) an analysis of the focus areas in survey papers on cybersecurity visualization and (ii) the classification of 60 research papers in the context of the selected components of the visualization design. Finally, we highlighted the main findings of the analysis and drew conclusions.",
                        "uid": "a-vizsec-7865",
                        "file_name": "",
                        "time_stamp": "2022-10-19T16:28:00Z",
                        "time_start": "2022-10-19T16:28:00Z",
                        "time_end": "2022-10-19T16:30:00Z",
                        "paper_type": "associated",
                        "keywords": [
                            "Visualization\u2014Cybersecurity\u2014Analysis\u2014Survey\u2014VizSec"
                        ],
                        "has_image": "1",
                        "has_video": "504",
                        "paper_award": "",
                        "image_caption": "Portola is a hybrid tree and network visualization that presents an overview of a segmented computer network and displays connections within the network. Using Portola, analysts can explore a segmented network, identify nodes and connections of interest through exploratory network analysis and drill down on elements of interest to reason about the patterns of relationships in the network. This work also discusses the design principles that broadly apply to design network security visualizations and lessons learned from the user-centered iterative design of Portola.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "a-vizsec-3415-pres",
                        "session_id": "a-vizsec-2",
                        "type": "Virtual Presentation (pre-recorded)",
                        "title": "PRIVEE: A Visual Analytic Workflow for Proactive Privacy Risk Inspection of Open Data #3415",
                        "contributors": [
                            "Kaustav Bhattacharjee"
                        ],
                        "authors": [
                            "Kaustav Bhattacharjee",
                            "Akm Islam",
                            "Jaideep Vaidya",
                            "Aritra Dasgupta"
                        ],
                        "abstract": "Open data sets that contain personal information are susceptible to adversarial attacks even when anonymized. By performing low-cost joins on multiple datasets with shared attributes, malicious users of open data portals might get access to information that violates individuals' privacy. However, open data sets are primarily published using a release-and-forget model, whereby data owners and custodians have little to no cognizance of these privacy risks. We address this critical gap by developing a visual analytic solution that enables data defenders to gain awareness about the disclosure risks in local, joinable data neighborhoods. The solution is derived through a design study with data privacy researchers, where we initially play the role of a red team and engage in an ethical data hacking exercise based on privacy attack scenarios. We use this problem and domain characterization to develop a set of visual analytic interventions as a defense mechanism and realize them in PRIVEE, a visual risk inspection workflow that acts as a proactive monitor for data defenders. PRIVEE uses a combination of risk scores and associated interactive visualizations to let data defenders explore vulnerable joins and interpret risks at multiple levels of data granularity. We demonstrate how PRIVEE can help emulate the attack strategies and diagnose disclosure risks through two case studies with data privacy experts.",
                        "uid": "a-vizsec-3415",
                        "file_name": "a-vizsec-3415_Bhattacharjee_Presentation.mp4",
                        "time_stamp": "2022-10-19T16:30:00Z",
                        "time_start": "2022-10-19T16:30:00Z",
                        "time_end": "2022-10-19T16:42:00Z",
                        "paper_type": "associated",
                        "keywords": [
                            "Human-centered computing, Visualization, Visualization application domains, Visual analytics;"
                        ],
                        "has_image": "1",
                        "has_video": "606",
                        "paper_award": "",
                        "image_caption": "PRIVEE is an end-to-end risk inspection workflow for open datasets that informs the defender in the analytical loop about potential disclosure risks in the presence of joinable datasets. Interactive visualization plays a crucial role in bootstrapping the risk inspection process via risk profiling, triaging and explaining risk signatures, and ultimately detecting instances of true disclosure at a record level. Colored borders track datasets across the goals.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "a-vizsec-3415-qa",
                        "session_id": "a-vizsec-2",
                        "type": "Virtual Q+A",
                        "title": "PRIVEE: A Visual Analytic Workflow for Proactive Privacy Risk Inspection of Open Data #3415 (Q+A)",
                        "contributors": [
                            "Kaustav Bhattacharjee"
                        ],
                        "authors": [],
                        "abstract": "Open data sets that contain personal information are susceptible to adversarial attacks even when anonymized. By performing low-cost joins on multiple datasets with shared attributes, malicious users of open data portals might get access to information that violates individuals' privacy. However, open data sets are primarily published using a release-and-forget model, whereby data owners and custodians have little to no cognizance of these privacy risks. We address this critical gap by developing a visual analytic solution that enables data defenders to gain awareness about the disclosure risks in local, joinable data neighborhoods. The solution is derived through a design study with data privacy researchers, where we initially play the role of a red team and engage in an ethical data hacking exercise based on privacy attack scenarios. We use this problem and domain characterization to develop a set of visual analytic interventions as a defense mechanism and realize them in PRIVEE, a visual risk inspection workflow that acts as a proactive monitor for data defenders. PRIVEE uses a combination of risk scores and associated interactive visualizations to let data defenders explore vulnerable joins and interpret risks at multiple levels of data granularity. We demonstrate how PRIVEE can help emulate the attack strategies and diagnose disclosure risks through two case studies with data privacy experts.",
                        "uid": "a-vizsec-3415",
                        "file_name": "",
                        "time_stamp": "2022-10-19T16:42:00Z",
                        "time_start": "2022-10-19T16:42:00Z",
                        "time_end": "2022-10-19T16:45:00Z",
                        "paper_type": "associated",
                        "keywords": [
                            "Human-centered computing, Visualization, Visualization application domains, Visual analytics;"
                        ],
                        "has_image": "1",
                        "has_video": "606",
                        "paper_award": "",
                        "image_caption": "PRIVEE is an end-to-end risk inspection workflow for open datasets that informs the defender in the analytical loop about potential disclosure risks in the presence of joinable datasets. Interactive visualization plays a crucial role in bootstrapping the risk inspection process via risk profiling, triaging and explaining risk signatures, and ultimately detecting instances of true disclosure at a record level. Colored borders track datasets across the goals.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "a-vizsec-9019-pres",
                        "session_id": "a-vizsec-2",
                        "type": "Virtual Presentation (pre-recorded)",
                        "title": "Visual Firewall Log Analysis - At the Border Between Analytical and Appealing #9019",
                        "contributors": [
                            "Marija Schufrin"
                        ],
                        "authors": [
                            "Marija Schufrin",
                            "Hendrik L\u00fccke-Tieke",
                            "J\u00f6rn Kohlhammer"
                        ],
                        "abstract": "Network security is critical for organizations to secure their network resources from intrusion and attacks. A security policy is a rule enforced in the network to allow or block network traffic. To write security policies, network analysts divide their networks into segments or parts with similar security needs. Segmentation makes writing security policies manageable and identifies more robust security policies for the network. Visualizations can help analysts to understand the segmented network and define security policies. We contribute Portola, a hybrid tree and network visualization technique to display a segmented computer network. Portola presents an overview of the segmentation as a hierarchy and displays connections within the network. Using Portola, analysts can explore a segmented network, identify nodes and connections of interest through exploratory network analysis, and drill down on elements of interest to reason about the patterns of relationships in the network. Through this work, we also discuss the goals of network analysts who work with segmented networks and discuss the lessons learned from the user-centered iterative design of Portola.",
                        "uid": "a-vizsec-9019",
                        "file_name": "a-vizsec-9019_Schufrin_Presentation.mp4",
                        "time_stamp": "2022-10-19T16:45:00Z",
                        "time_start": "2022-10-19T16:45:00Z",
                        "time_end": "2022-10-19T16:57:00Z",
                        "paper_type": "associated",
                        "keywords": [
                            "Human-centered computing, Visualization Techniques, Tree and Network Visualization"
                        ],
                        "has_image": "1",
                        "has_video": "830",
                        "paper_award": "",
                        "image_caption": "We conducted a design study for a visual firewall log analysis system. In the course of this, we identified two types of interests:\nHigh-level overview and low-level analysis. Accordingly, we developed two interlinked concepts resulting in two interfaces\nthat can be used together as a combined visual firewall log analysis system. \nAs a reflection we propose a design pipeline extending the widely used design study process by including a track taking into account \nhedonic qualities through the incorporation of psychological needs, personas and interaction vocabulary.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "a-vizsec-9019-qa",
                        "session_id": "a-vizsec-2",
                        "type": "Virtual Q+A",
                        "title": "Visual Firewall Log Analysis - At the Border Between Analytical and Appealing #9019 (Q+A)",
                        "contributors": [
                            "Marija Schufrin"
                        ],
                        "authors": [],
                        "abstract": "Network security is critical for organizations to secure their network resources from intrusion and attacks. A security policy is a rule enforced in the network to allow or block network traffic. To write security policies, network analysts divide their networks into segments or parts with similar security needs. Segmentation makes writing security policies manageable and identifies more robust security policies for the network. Visualizations can help analysts to understand the segmented network and define security policies. We contribute Portola, a hybrid tree and network visualization technique to display a segmented computer network. Portola presents an overview of the segmentation as a hierarchy and displays connections within the network. Using Portola, analysts can explore a segmented network, identify nodes and connections of interest through exploratory network analysis, and drill down on elements of interest to reason about the patterns of relationships in the network. Through this work, we also discuss the goals of network analysts who work with segmented networks and discuss the lessons learned from the user-centered iterative design of Portola.",
                        "uid": "a-vizsec-9019",
                        "file_name": "",
                        "time_stamp": "2022-10-19T16:57:00Z",
                        "time_start": "2022-10-19T16:57:00Z",
                        "time_end": "2022-10-19T17:00:00Z",
                        "paper_type": "associated",
                        "keywords": [
                            "Human-centered computing, Visualization Techniques, Tree and Network Visualization"
                        ],
                        "has_image": "1",
                        "has_video": "830",
                        "paper_award": "",
                        "image_caption": "We conducted a design study for a visual firewall log analysis system. In the course of this, we identified two types of interests:\nHigh-level overview and low-level analysis. Accordingly, we developed two interlinked concepts resulting in two interfaces\nthat can be used together as a combined visual firewall log analysis system. \nAs a reflection we propose a design pipeline extending the widely used design study process by including a track taking into account \nhedonic qualities through the incorporation of psychological needs, personas and interaction vocabulary.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "",
                        "ff_id": ""
                    }
                ]
            },
            {
                "title": "VizSec: Panel and Closing",
                "session_id": "a-vizsec-3",
                "event_prefix": "a-vizsec",
                "track": "pinon",
                "livestream_id": "pinon-wed",
                "session_image": "a-vizsec-3.png",
                "chair": [
                    "Steven Gomez"
                ],
                "organizers": [],
                "time_start": "2022-10-19T19:00:00Z",
                "time_end": "2022-10-19T20:15:00Z",
                "discord_category": "",
                "discord_channel": "pinon",
                "discord_channel_id": "1024601027644760074",
                "discord_link": "https://discord.com/channels/978320435554947082/1024601027644760074",
                "slido_link": "https://app.sli.do/event/i28UETZaiRWr1XwX6eKjcf?section=bbc002b7-d3ae-48bc-b2d3-0624a7bfed06",
                "youtube_url": "https://youtu.be/bepnKhvWRtU",
                "youtube_id": "bepnKhvWRtU",
                "zoom_meeting": "",
                "zoom_password": "",
                "zoom_link": "",
                "ff_link": "",
                "ff_playlist": "",
                "time_slots": [
                    {
                        "slot_id": "a-vizsec-prog-5",
                        "session_id": "a-vizsec-3",
                        "type": "Mixed Panel (virtual/in person)",
                        "title": "Virtual Panel: \u201cTranslating Real Needs for Cyber Operators into Impactful R&D\"",
                        "contributors": [
                            "Sanchari Das",
                            "Kami Vaniea",
                            "Prashanth Rajivan",
                            "Josiah Dykstra",
                            "Philipp Markert",
                            "Glenn Fink",
                            "Vince Mancuso"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "",
                        "file_name": "",
                        "time_stamp": "2022-10-19T19:00:00Z",
                        "time_start": "2022-10-19T19:00:00Z",
                        "time_end": "2022-10-19T20:00:00Z",
                        "paper_type": "",
                        "keywords": [],
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "a-vizsec-prog-6",
                        "session_id": "a-vizsec-3",
                        "type": "In Person Presentation",
                        "title": "Best Poster Announcement",
                        "contributors": [
                            "Steven Gomez"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "",
                        "file_name": "",
                        "time_stamp": "2022-10-19T20:00:00Z",
                        "time_start": "2022-10-19T20:00:00Z",
                        "time_end": "2022-10-19T20:05:00Z",
                        "paper_type": "",
                        "keywords": [],
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "a-vizsec-prog-7",
                        "session_id": "a-vizsec-3",
                        "type": "In Person Presentation",
                        "title": "Closing",
                        "contributors": [
                            "Chris Bryan"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "",
                        "file_name": "",
                        "time_stamp": "2022-10-19T20:05:00Z",
                        "time_start": "2022-10-19T20:05:00Z",
                        "time_end": "2022-10-19T20:15:00Z",
                        "paper_type": "",
                        "keywords": [],
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    }
                ]
            }
        ]
    },
    "a-visap": {
        "event": "VIS Arts Program",
        "long_name": "VIS Arts Program",
        "event_type": "Associated Event (Arts)",
        "event_prefix": "a-visap",
        "event_description": "",
        "event_url": "",
        "organizers": [
            "Charles Perin",
            "Uta Hinrichs",
            "Rebecca Ruige Xu",
            "Peter Froslie",
            "Bon Adriel Aseniero",
            "Tommaso Elli",
            "Till Nagel",
            "Maria Lantin",
            "Yoon Chung Han"
        ],
        "sessions": [
            {
                "title": "VISAP: Papers 1",
                "session_id": "a-visap-2",
                "event_prefix": "a-visap",
                "track": "mistletoe",
                "livestream_id": "mistletoe-wed",
                "session_image": "a-visap-2.png",
                "chair": [
                    "Uta Hinrichs",
                    "Bon Adriel Aseniero"
                ],
                "organizers": [],
                "time_start": "2022-10-19T14:00:00Z",
                "time_end": "2022-10-19T15:15:00Z",
                "discord_category": "",
                "discord_channel": "mistletoe",
                "discord_channel_id": "1024601055700471839",
                "discord_link": "https://discord.com/channels/978320435554947082/1024601055700471839",
                "slido_link": "https://app.sli.do/event/i28UETZaiRWr1XwX6eKjcf?section=05d57c7a-397b-4e16-a41d-7f06e8e64c37",
                "youtube_url": "https://youtu.be/xub7Gw386eY",
                "youtube_id": "xub7Gw386eY",
                "zoom_meeting": "",
                "zoom_password": "",
                "zoom_link": "",
                "ff_link": "",
                "ff_playlist": "",
                "time_slots": [
                    {
                        "slot_id": "a-visap-1021-pres",
                        "session_id": "a-visap-2",
                        "type": "In Person Presentation",
                        "title": "Shifting winds: Gendered structures of academic mentorship",
                        "contributors": [
                            "Jiabao Li"
                        ],
                        "authors": [
                            "Jiabao Li",
                            "Houjiang Liu",
                            "Jilie Zeng",
                            "Di Wu",
                            "Ying Ding",
                            "Alec McGail"
                        ],
                        "abstract": "Every researcher alive today had their mentors, those who helped assimilate them into a life of scholarly work. And in turn they each had their mentors, and so on to the dawn of knowledge. In the same way, each researcher\u2019s mentees take their perspectives and methods to future mentees, and to their mentees, etc. These comprise the roots and branches, respectively, of the academic tree of a single researcher. If we let these ancestors\u2019 and descendants\u2019 genders affect these trees like a \u201cwind,\u201d most curl nearly to the earth. We depict and describe the structure of these trees, and how this wind has changed over the decades. To set these trees growing upright again we visualize giving differential weight to male and female researchers.",
                        "uid": "a-visap-1021",
                        "file_name": "",
                        "time_stamp": "2022-10-19T14:00:00Z",
                        "time_start": "2022-10-19T14:00:00Z",
                        "time_end": "2022-10-19T14:09:00Z",
                        "paper_type": "associated",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "Mentor tree of Francis Galton. Male mentees tilt left. Female mentees tilt right. Unknown genders grow straight. Galton\u2019s tree sits at the base of the widest and tallest tree in this dataset. For a tree to be wide or tall it must be old and enduring.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "a-visap-1030-pres",
                        "session_id": "a-visap-2",
                        "type": "Virtual Presentation (pre-recorded)",
                        "title": "Diversity traces: an interactive lens on multi-racial families in America",
                        "contributors": [
                            "Predro Cruz"
                        ],
                        "authors": [
                            "Pedro Cruz"
                        ],
                        "abstract": "When looking at diversity from a racial perspective, homogenous communities are still the norm, as they remain siloed not only locally, but in their very own households as well. This visualization project comes as a celebration of the fringe couples and families who have a multi-racial identity, effectively embodying the intermingle of races, and dissolving the systemic barriers put on their very own existence. According to the census, there are only vestiges of these multi-racial families until 1960. More recently, there as been a surge of these families in the data, but they are still a rarity, still mere traces of diversity in America. In this visualization you can see every registered multi-racial couple in America, for recent periods in 1-5% samples of the population, and for older periods in 100% samples of the population. Each couple is represented as a colorful animated chromosome, enabling to see the races within each family, their ages, sexes, and children.",
                        "uid": "a-visap-1030",
                        "file_name": "a-visap-1030_Cruz_Presentation.mp4",
                        "time_stamp": "2022-10-19T14:09:00Z",
                        "time_start": "2022-10-19T14:09:00Z",
                        "time_end": "2022-10-19T14:14:00Z",
                        "paper_type": "associated",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "234",
                        "paper_award": "",
                        "image_caption": "Zoom-in on multi-racial households couples showing their children.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "a-visap-1024-pres",
                        "session_id": "a-visap-2",
                        "type": "Virtual Presentation (pre-recorded)",
                        "title": "#MeToo Anti-Network",
                        "contributors": [
                            "Kim Albrecht"
                        ],
                        "authors": [
                            "Kim Albrecht",
                            "Catherine D'Ignazio",
                            "Matthew Battles",
                            "Nicole Martin"
                        ],
                        "abstract": "Cosmologists say that most of the universe is structured by antimatter. We postulate that social media is similarly structured by effects of the unobserved discourse and experience. The backbone of a movement such as #MeToo is not based on the most-liked and most-retweeted, but by the masses of unobserved tweets. Vast numbers of #MeToo tweets that had no retweets and no likes nonetheless constituted acts of quiet testimony or unassuming solidarity. Conventional measures of network science thus fail to capture the true relevance of #MeToo. As Black feminist Patricia Hill Collins says, \"Most activism is brought about by ordinary people like ourselves.\"  From a distance, the graphics appear as abstract diagrams, similar to Bridget Riley\u2019s work. The beauty of each line contains a powerful request for a reordering of power within society. We present an opportunity to engage with each request\u2014from individual people at individual moments within a collective movement that is not over. #MeToo is urgent, #InvisibleNoMore is urgent, #BelieveBlackWomen is urgent, #MMIWG2S is urgent, #SayHerName is urgent. We are still living in a crisis of sexual violence. So we invite you to ditch the networked metrics and listen.",
                        "uid": "a-visap-1024",
                        "file_name": "Kim Albrecht - Situating MeToo.mp4",
                        "time_stamp": "2022-10-19T14:14:00Z",
                        "time_start": "2022-10-19T14:14:00Z",
                        "time_end": "2022-10-19T14:19:00Z",
                        "paper_type": "associated",
                        "keywords": [],
                        "has_image": "0",
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "a-visap-prog-3",
                        "session_id": "a-visap-2",
                        "type": "Mixed Panel (virtual/in person)",
                        "title": "Discussion Panel: Mingling Perspectives",
                        "contributors": [
                            "Jiabao Li",
                            "Predro Cruz",
                            "Kim Albrecht"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "",
                        "file_name": "",
                        "time_stamp": "2022-10-19T14:19:00Z",
                        "time_start": "2022-10-19T14:19:00Z",
                        "time_end": "2022-10-19T14:30:00Z",
                        "paper_type": "",
                        "keywords": [],
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "a-visap-1041-pres",
                        "session_id": "a-visap-2",
                        "type": "Virtual Presentation (pre-recorded)",
                        "title": "Quaran.tiles. Archiving expressive digital places from instagram during the COVID-19 pandemic",
                        "contributors": [
                            "Andrea Benedetti"
                        ],
                        "authors": [
                            "Andrea Benedetti",
                            "Beatrice Gobbo",
                            "Giacomo Flaim"
                        ],
                        "abstract": "During the spring of 2020, COVID-19 limited contact between people and prevented from meeting and aggregating in real places. Many had to stay at home, and others spent time in quarantine facilities. In this context, virtual aggregation has increased at the expense of in-person aggregation. Expressive geo-tagging, namely the practice of creating locations with fictitious names to express an emotional condition, became worthy of attention. Grounded on anecdotal evidence, fictitious digital locations on social media such as \u201cQuarantine\u201d began to proliferate, which, despite not having a name that could be traced back to an existing place, still carried geo-referenced information with them. Starting from this concept, we present the book Quaran.tiles, an archive of 364 expressive digital places collected from Instagram in April 2020 and enriched with information from Google Street View, which aims to give space and dimension to the resulting collection of fictitious and mingling user-generated places.",
                        "uid": "a-visap-1041",
                        "file_name": "a-visap-1041_Benedetti_Presentation.mp4",
                        "time_stamp": "2022-10-19T14:30:00Z",
                        "time_start": "2022-10-19T14:30:00Z",
                        "time_end": "2022-10-19T14:39:00Z",
                        "paper_type": "associated",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "419",
                        "paper_award": "",
                        "image_caption": "Double spread of the book \"Quarantiles\". Two expressive places from Instagram are shown, one below the other. Each place has six images, a name, coordinates and an address.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "a-visap-1043-pres",
                        "session_id": "a-visap-2",
                        "type": "Virtual Presentation (pre-recorded)",
                        "title": "The memory of street - Hong Kong: history, culture, memory and post-humanism",
                        "contributors": [
                            "Wai Lam Noah Luk"
                        ],
                        "authors": [
                            "Wai Lam Noah Luk",
                            "Chunxi Liu"
                        ],
                        "abstract": "In this annotated portfolio, we will introduce a posthumanist idea of using micro-organism to define the mingling spaces. As a result, to explore and extend the current boundary of the cultural concerns of human beings. Alongside the philosophical discussion, we will also present a multidisciplinarily-fabricated installation to visualize the idea through 3D printing technology and biomedical experiments. (The geographical location of this project is based in Hong Kong).",
                        "uid": "a-visap-1043",
                        "file_name": "a-visap-1043_Luk_Presentation.mp4",
                        "time_stamp": "2022-10-19T14:39:00Z",
                        "time_start": "2022-10-19T14:39:00Z",
                        "time_end": "2022-10-19T14:48:00Z",
                        "paper_type": "associated",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "433",
                        "paper_award": "",
                        "image_caption": "A computer-generated image based on the result of bacteria growth in a microbial experiment. The bacteria are the new narrative for all the events that happened on the streets of Hong Kong Island, witnesses of the collective memories of individuals and communities, evolutionary history, and environmental history.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "a-visap-1019-pres",
                        "session_id": "a-visap-2",
                        "type": "In Person Presentation",
                        "title": "Tangled Tracks",
                        "contributors": [
                            "Luiz Ludwig"
                        ],
                        "authors": [
                            "Luiz Ludwig",
                            "Doris Kosminsky"
                        ],
                        "abstract": "The artwork Tangled Tracks is an installation that consists of a set of 16 red and white ceramic tiles with projections that register the real and virtual paths taken by the artist. As a way of exhibiting them, the tiles are available so the public can interact and rearrange them in their own way. In this manner, participants are invited to form their own paths, imagining and fabulating routes.",
                        "uid": "a-visap-1019",
                        "file_name": "a-visap-1019_Ludwig_Presentation.mp4",
                        "time_stamp": "2022-10-19T14:48:00Z",
                        "time_start": "2022-10-19T14:48:00Z",
                        "time_end": "2022-10-19T14:53:00Z",
                        "paper_type": "associated",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "220",
                        "paper_award": "",
                        "image_caption": "Set of red and gray color ceramic tiles. Each tile has traces engraved on its surface that were created from paths traveled and registered by the artist.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "a-visap-1027-pres",
                        "session_id": "a-visap-2",
                        "type": "Virtual Presentation (pre-recorded)",
                        "title": "ESSYS* Sharing #UC: An Emotion-driven Audiovisual Installation",
                        "contributors": [
                            "Sergio M. Rebelo"
                        ],
                        "authors": [
                            "S\u00e9rgio M. Rebelo",
                            "Mariana Sei\u00e7a",
                            "Pedro Martins",
                            "Jo\u00e3o Bicker",
                            "Penousal Machado"
                        ],
                        "abstract": "We present ESSYS* Sharing #UC, an audiovisual installation artwork that reflects upon the emotional context related to the university and the city of Coimbra, based on the data shared about them on Twitter. The installation was presented in an urban art gallery of C\u00edrculo de Artes Pl\u00e1sticas de Coimbra during the summer and autumn of 2021. In the installation space, one may see a collection of typographic posters displaying the tweets and listening to an ever-changing ambient sound. The present audiovisuals are created by an autonomous computational creative approach, which employs a neural classifier to recognise the emotional context of a tweet and uses this resulting data as feedstock for the audiovisual generation. The installation\u2019s space is designed to promote an approach and blend between the online and physical perceptions of the same location. We applied multiple experiments with the proposed approach to evaluate the capability and performance. Also, we conduct interview-based evaluation sessions to understand how the installation elements, especially poster designs, are experienced by people regarding diversity, expressiveness and possible employment in other commercial and social scenarios.",
                        "uid": "a-visap-1027",
                        "file_name": "a-visap-1027_Rebelo_Presentation.mp4",
                        "time_stamp": "2022-10-19T14:53:00Z",
                        "time_start": "2022-10-19T14:53:00Z",
                        "time_end": "2022-10-19T15:02:00Z",
                        "paper_type": "associated",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "480",
                        "paper_award": "",
                        "image_caption": "Photography of the ESSYS* Sharing #UC installation, displaying one of the installation walls crowed of posters generated based the emotion charge of its contents. Photography by Jorge das Neves. \u00a9 CAPC, 2022",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "a-visap-prog-4",
                        "session_id": "a-visap-2",
                        "type": "Mixed Panel (virtual/in person)",
                        "title": "Discussion Panel: Mingling between the Physical & the Virtual",
                        "contributors": [
                            "Andrea Benedetti",
                            "Wai Lam Noah Luk",
                            "Luiz Ludwig",
                            "Sergio M. Rebelo"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "",
                        "file_name": "",
                        "time_stamp": "2022-10-19T15:02:00Z",
                        "time_start": "2022-10-19T15:02:00Z",
                        "time_end": "2022-10-19T15:15:00Z",
                        "paper_type": "",
                        "keywords": [],
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    }
                ]
            },
            {
                "title": "VISAP: Papers 2",
                "session_id": "a-visap-3",
                "event_prefix": "a-visap",
                "track": "mistletoe",
                "livestream_id": "mistletoe-thu",
                "session_image": "a-visap-3.png",
                "chair": [
                    "Rebecca Ruige Xu & Uta Hinrichs"
                ],
                "organizers": [],
                "time_start": "2022-10-20T14:00:00Z",
                "time_end": "2022-10-20T15:15:00Z",
                "discord_category": "",
                "discord_channel": "mistletoe",
                "discord_channel_id": "1024601055700471839",
                "discord_link": "https://discord.com/channels/978320435554947082/1024601055700471839",
                "slido_link": "https://app.sli.do/event/i28UETZaiRWr1XwX6eKjcf?section=05d57c7a-397b-4e16-a41d-7f06e8e64c37",
                "youtube_url": "https://youtu.be/QUC4mL-YR40",
                "youtube_id": "QUC4mL-YR40",
                "zoom_meeting": "",
                "zoom_password": "",
                "zoom_link": "",
                "ff_link": "",
                "ff_playlist": "",
                "time_slots": [
                    {
                        "slot_id": "a-visap-1032-pres",
                        "session_id": "a-visap-3",
                        "type": "Virtual Presentation (pre-recorded)",
                        "title": "Wind from Bamboo: A Chinese Handwriting Interactive Installation based on Human-AI Collaborative Font Design",
                        "contributors": [
                            "Zhen Zeng"
                        ],
                        "authors": [
                            "Zhen Zeng",
                            "Jie Wang",
                            "Nan He"
                        ],
                        "abstract": "In the era of information, the feeling that the pen tip rubs against the paper is getting farther away, and the way of writing Chinese characters with strokes is gradually alienating. In response to this problem, this work tries to help people relive the touch of handwriting through the mingling of real and virtual experiences. The designer collaborated with AI to design a Chinese font that integrates bamboo leaves' shape and Chinese characters' structure. Based on the font, an interactive installation was set up to start a virtual Chinese poetry bamboo forest scene through real handwriting behavior.",
                        "uid": "a-visap-1032",
                        "file_name": "a-visap-1032_Zeng_Presentation.mp4",
                        "time_stamp": "2022-10-20T14:00:00Z",
                        "time_start": "2022-10-20T14:00:00Z",
                        "time_end": "2022-10-20T14:10:00Z",
                        "paper_type": "associated",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "408",
                        "paper_award": "",
                        "image_caption": "Wind from Bamboo: A Chinese Handwriting Mingling Interactive Installation based on Human-AI Collaborative Font Design\n\nThe designer collaborated with AI to design a Chinese font that integrates bamboo leaves' shape and Chinese characters' structure. Based on the font, they set up an interactive installation to start a virtual Chinese poetry bamboo forest scene through real handwriting behavior to help people relive the touch of traditional Chinese handwriting.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "a-visap-1079-pres",
                        "session_id": "a-visap-3",
                        "type": "Virtual Presentation (pre-recorded)",
                        "title": "Under the Green: Visual data storytelling the process of urban CO2 neutralization by forests",
                        "contributors": [
                            "Wang Linqi"
                        ],
                        "authors": [
                            "Linqi Wang",
                            "Fengzhou Liang",
                            "Fang Liu",
                            "Boai Yang",
                            "Junyan Lv"
                        ],
                        "abstract": "We express the conflict between industrialization and ecological civilization through Cyber Aesthetics and interactive web pages. We popularize the originally cryptic knowledge of Forest Ecology to the public through common visual metaphors and interactive effects. With this work spreading online, we hope to attract more people to join the construction of ecological civilization and pay tribute to Forest Ecological Scientists. There are already some scientific research results on forest carbon fixation, and a large amount of scientific data has been generated. However, these achievements and data are highly specialized, detached from daily life, and subsequently receive rare public attention. The physical space humans depend on is strongly interconnected, and forests and cities seem separate but mingling. The production and living of people produce lots of greenhouse gases, which need to be consumed by forest plants through photosynthesis, fixing CO2 in the form of organic carbon in the soil and biomass to ensure the carbon cycle. Industrialization has led to excessive CO2 emissions, causing severe disturbances to the carbon cycle process. At the same time, nature is constantly warning humanity, accompanied by frequent occurrences of extreme weather. Therefore, natural forest conservation and plantation forest management are crucial for future ecological civilization.",
                        "uid": "a-visap-1079",
                        "file_name": "a-visap-1079_[wang]_Presentation.mp4",
                        "time_stamp": "2022-10-20T14:10:00Z",
                        "time_start": "2022-10-20T14:10:00Z",
                        "time_end": "2022-10-20T14:15:00Z",
                        "paper_type": "associated",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "300",
                        "paper_award": "",
                        "image_caption": "Under the Green is a visual data storytelling website that visualizes the process of forest carbon cycles. We popularize the originally difficult knowledge of Forest Ecology to the public through common visual metaphors. Through this work spreading, we hope to call more people to join the construction of Eco-civilization.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "a-visap-1018-pres",
                        "session_id": "a-visap-3",
                        "type": "Virtual Presentation (pre-recorded)",
                        "title": "Supersynthesis: A Communal Synthesis",
                        "contributors": [
                            "Amay Kataria"
                        ],
                        "authors": [
                            "Amay Kataria"
                        ],
                        "abstract": "This pictorial presents the journey of a light and sound installation called Supersynthesis, which collects data from its users through an interactive digital interface and expresses it through the physical installation. It begins by going over historical works and methodologies that align with this project, goes over the design decisions behind the sculptural form and its software architecture, and finally analyzes its function through the lens of a \u201dperformative object\u201d to draw connections with the theme of Mingling Spaces.",
                        "uid": "a-visap-1018",
                        "file_name": "a-visap-1018_Kataria_Presentation.mp4",
                        "time_stamp": "2022-10-20T14:15:00Z",
                        "time_start": "2022-10-20T14:15:00Z",
                        "time_end": "2022-10-20T14:24:00Z",
                        "paper_type": "associated",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "Amay Kataria",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "a-visap-1013-pres",
                        "session_id": "a-visap-3",
                        "type": "In Person Presentation",
                        "title": "SkyGlyphs: Reflections on the Design of a Delightful Visualization",
                        "contributors": [
                            "Bon Adriel Aseniero"
                        ],
                        "authors": [
                            "Bon Adriel Aseniero",
                            "Sheelagh Carpendale",
                            "George Fitzmaurice",
                            "Justin Matejka"
                        ],
                        "abstract": "In creating SkyGlyphs, our goal was to develop a data visualization that could possibly capture people\u2019s attention and spark their curiosity to explore a dataset. This work was inspired by a mingling of research including serendipitous interactions, visualizations for public displays, and personal visualizations. SkyGlyphs is a nonconventional whimsical visualization, depicting datapoints as animated balloons in space. We designed it to encourage non-experts to casually browse the contents of a repository through visual interactions like linking and grouping of datapoints. Our contributions include SkyGlyphs\u2019 representation and our design reflection that reveals a perspective on how to design delightful visualizations.",
                        "uid": "a-visap-1013",
                        "file_name": "",
                        "time_stamp": "2022-10-20T14:24:00Z",
                        "time_start": "2022-10-20T14:24:00Z",
                        "time_end": "2022-10-20T14:33:00Z",
                        "paper_type": "associated",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "SkyGlyphs is a delightful visualization depicting datapoints as balloons floating in the sky. Through the intentional use of whimsical aesthetics, SkyGlyphs' design aims to captivate people\u2019s attention and leverage their curiosity as entry points to data exploration.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "a-visap-prog-5",
                        "session_id": "a-visap-3",
                        "type": "Mixed Panel (virtual/in person)",
                        "title": "Discussion Panel: Mingling Interfaces",
                        "contributors": [
                            "Zhen Zeng",
                            "Wang Linqi",
                            "Amay Kataria"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "",
                        "file_name": "",
                        "time_stamp": "2022-10-20T14:33:00Z",
                        "time_start": "2022-10-20T14:33:00Z",
                        "time_end": "2022-10-20T14:43:00Z",
                        "paper_type": "",
                        "keywords": [],
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "a-visap-1089-pres",
                        "session_id": "a-visap-3",
                        "type": "In Person Presentation",
                        "title": "Molecular Planets",
                        "contributors": [
                            "Christoph Muller"
                        ],
                        "authors": [
                            "Christoph M\u00fcller",
                            "Karsten Schatz",
                            "Florian Frie\u00df",
                            "Michael Krone"
                        ],
                        "abstract": "The molecular world is always in motion \u2013 molecules are never stationary, their atoms are constantly vibrating due to thermal energies and other external forces. This ongoing motion is the reason that our exhibit is in the form of a mobile \u2013 a form of art already used by Alexander Calder, who believed that the mathematical laws of the universe could not be expressed by static art. The idea of mysterious forces holding the universe in balance inspired his mobiles. Likewise, the ever-moving elements of the molecular space are not only invisible, but their shapes are of purely theoretical nature. Visualisation makes the elegance and beauty of the molecular world visible in virtual space by representing molecular models as molecular surfaces portraying the interface between a protein and its environment. Our exhibit not only makes such visualisations transcend into our three-dimensional, tangible space, but also mingles all intermediate mathematical spaces that the idea of molecular surfaces traverse to reach their visual representation into one object. It therefore makes the visualisation process behind the idea of molecular surface maps more tangible by showing the metaphor of a planet being charted.",
                        "uid": "a-visap-1089",
                        "file_name": "a-visap-1089_M\u00fcller_Presentation.mp4",
                        "time_stamp": "2022-10-20T14:43:00Z",
                        "time_start": "2022-10-20T14:43:00Z",
                        "time_end": "2022-10-20T14:48:00Z",
                        "paper_type": "associated",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "234",
                        "paper_award": "",
                        "image_caption": "Molecular Planets are three-dimensional theoretical intermediate steps on the way from the theoretical model of a molecular surface to two-dimensional maps. Our installation makes these not only visible, but tangible, thus illustrating the whole visualisation process as metaphorically charting the surface of a lipase molecule.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "a-visap-1052-pres",
                        "session_id": "a-visap-3",
                        "type": "Virtual Presentation (pre-recorded)",
                        "title": "SoftVoss & OctoAnemone ",
                        "contributors": [
                            "Yin Yu"
                        ],
                        "authors": [
                            "Yin Yu"
                        ],
                        "abstract": "Human skin exchanges real-time energy, such as temperature, humidity, and pressure, from its surrounding space. What if our skin can listen? SoftVoss is a morphing artificial skin that changes its appearance by real-time sound.   SoftVoss brings sound material into a body architectural space that perfectly represents the show\u2019s theme--Mingling Spaces. In general, skin as the outer layer of a body protects our inner body from the environment. SoftVoss responds to the space through a new dimension of senses: aural. As an artificial skin, SoftVoss becomes a medium for visual communication with the soundscape.   SoftVoss used the information of sound morphology\u2013the transformation of sound material\u2013to shape the feather-like wearables. The realization of SoftVoss has three main components: sound materials, control system, and soft structure. Sound material, captured by microphones, is the input data to control the piece. The four channels of sound materials activate the four layers of feathers using the real-time input. The sounds captured from different source directions activities the specific layer of the feather.   SoftVoss visualizes the sound information through a 3D morphogenesis of wearable art. This work received successful recognition at my solo exhibition at the GlassBox Gallery at the University of California, Santa Barbara, in 2021.",
                        "uid": "a-visap-1052",
                        "file_name": "a-visap-1052_Yu_Presentation.mp4",
                        "time_stamp": "2022-10-20T14:48:00Z",
                        "time_start": "2022-10-20T14:48:00Z",
                        "time_end": "2022-10-20T14:53:00Z",
                        "paper_type": "associated",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "333",
                        "paper_award": "",
                        "image_caption": "SoftVoss: A Morphing Artificial Skin",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "a-visap-1038-pres",
                        "session_id": "a-visap-3",
                        "type": "Virtual Presentation (pre-recorded)",
                        "title": "Beyond Human Perception",
                        "contributors": [
                            "Mar\u00c3\u00ada Castellanos Vicente"
                        ],
                        "authors": [
                            "Mar\u00eda Castellanos Vicente"
                        ],
                        "abstract": "The artwork is a video installation that allows the audience to visualize and compare the reactions of humans and plants to a common stimulus; live music. Erasing boundaries into the communication and understanding between both living beings and by highlighting  the immediate reactions of plants to their surrounding changes.  The installation is the result of several sessions where the brain activity of humans was measured, through the EEG registered wave,  and measuring  the electrical oscillations that are happening into the plants, measured with a sensor developed by the artists, able to detect immediate changes in plants.  Through the use of mathematics,  by using the Fast Fourier Transform, humans data and plants data are able to compare each other. This data can also be displayed graphically thanks to an algorithm developed by the artists that allow the audience to see the data through the shape of little spheres that are moving within the geometric shape of torus. Each little sphere represent each data registered. The graphic representation of  human data and plant data can be seen simultaneously in a video allowing the audience to find patterns by comparing the both living beings reactions to the live music.  The video installation is composed by two synchronized videos. One video with the the concert for plants and humans, and the other one with the data visualization of two living beings responses during the performance.  Through this artistic research we wanted to know more about the secrets language of plants. To know more about the plants\u2019 language and behavior will allow us to know more about nature, thus we could beMer understand our environment.  To have an impact in other fields such as climate change, which is a reality happening now; the more we know about our environment and the living organisms that are living with us on Earth, the more we can do to try to improve the situation. Plants could give us a lot of information that we cannot understand yet, but this can help formulate new questions.",
                        "uid": "a-visap-1038",
                        "file_name": "a-visap-1038_Castellanos_Presentation.mp4",
                        "time_stamp": "2022-10-20T14:53:00Z",
                        "time_start": "2022-10-20T14:53:00Z",
                        "time_end": "2022-10-20T14:58:00Z",
                        "paper_type": "associated",
                        "keywords": [],
                        "has_image": "0",
                        "has_video": "248",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "a-visap-1072-pres",
                        "session_id": "a-visap-3",
                        "type": "In Person Presentation",
                        "title": "Affective, Hand-Sculpted Glyph Forms for Engaging and Expressive Scientific Visualization",
                        "contributors": [
                            "Stephanie Zeller"
                        ],
                        "authors": [
                            "Stephanie Zeller",
                            "Francesca Samsel",
                            "Lyn Bartram"
                        ],
                        "abstract": "As scientific data continues to grow in size, complexity, and density, the representation scope of three-dimensional spaces, data sampling methods, and transfer functions have improved in parallel, allowing visualization practitioners to produce richer multidimensional encodings. Glyphs, in particular, have become an essential encoding tool due to their versatile applications in co-located multivariate volumetric datasets. While prior work has been conducted investigating the perceptual attributes of computationally-generated three-dimensional glyph-forms for scientific visualization, their affective and expressive qualities have yet to be examined. Further, our prior work has demonstrated the benefits of artist hand-created glyph forms in contrast to commonly-used synthetic forms in increasing visual diversity, discrimination, and expressive association in complex environmental datasets. In order to begin to address this gap, we establish preliminary groundwork for an affective design space for hand-created glyph forms, produce a novel set of glyph-forms based on this design space, describe a non-verbal method for discovering affective classifications of glyph-forms adopted from current affect theory, and report the results of two studies that explore how these three-dimensional forms produce consistent affective responses across assorted study cohorts.",
                        "uid": "a-visap-1072",
                        "file_name": "",
                        "time_stamp": "2022-10-20T14:58:00Z",
                        "time_start": "2022-10-20T14:58:00Z",
                        "time_end": "2022-10-20T15:06:00Z",
                        "paper_type": "associated",
                        "keywords": [],
                        "has_image": "0",
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "a-visap-prog-6",
                        "session_id": "a-visap-3",
                        "type": "Mixed Panel (virtual/in person)",
                        "title": "Discussion Panel: Mingling Art & Science",
                        "contributors": [
                            "Christoph Muller",
                            "Yin Yu",
                            "Maria Castellanos Vincente",
                            "Stephanie Zeller"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "",
                        "file_name": "",
                        "time_stamp": "2022-10-20T15:06:00Z",
                        "time_start": "2022-10-20T15:06:00Z",
                        "time_end": "2022-10-20T15:15:00Z",
                        "paper_type": "",
                        "keywords": [],
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    }
                ]
            }
        ]
    },
    "v-spotlights1": {
        "event": "Data Analysis Methods for Climate Modeling of Extreme Weather Events",
        "long_name": "Data Analysis Methods for Climate Modeling of Extreme Weather Events",
        "event_type": "Application Spotlight",
        "event_prefix": "v-spotlights1",
        "event_description": "",
        "event_url": "",
        "organizers": [
            "Divya Banesh",
            "Ayan Biswas",
            "James Benedict",
            "Rupsa Bhowmick",
            "Soumya Dutta",
            "Michael Grosskopf"
        ],
        "sessions": [
            {
                "title": "Data Analysis Methods for Climate Modeling of Extreme Weather Events",
                "session_id": "spotlights1",
                "event_prefix": "v-spotlights1",
                "track": "mistletoe",
                "livestream_id": "mistletoe-fri",
                "session_image": "spotlights1.png",
                "chair": [
                    "Divya Banesh",
                    "Ayan Biswas",
                    "James Benedict",
                    "Rupsa Bhowmick",
                    "Soumya Dutta",
                    "Michael Grosskopf"
                ],
                "organizers": [],
                "time_start": "2022-10-21T14:00:00Z",
                "time_end": "2022-10-21T15:15:00Z",
                "discord_category": "",
                "discord_channel": "mistletoe",
                "discord_channel_id": "1024601055700471839",
                "discord_link": "https://discord.com/channels/978320435554947082/1024601055700471839",
                "slido_link": "https://app.sli.do/event/i28UETZaiRWr1XwX6eKjcf?section=05d57c7a-397b-4e16-a41d-7f06e8e64c37",
                "youtube_url": "https://youtu.be/Sj3RIl7xq6E",
                "youtube_id": "Sj3RIl7xq6E",
                "zoom_meeting": "",
                "zoom_password": "",
                "zoom_link": "",
                "ff_link": "",
                "ff_playlist": "",
                "time_slots": [
                    {
                        "slot_id": "v-spotlights1-prog-1",
                        "session_id": "spotlights1",
                        "type": "In Person Presentation",
                        "title": "In-Situ Inference for E3SM Climate Model",
                        "contributors": [
                            "Ayan Biswas"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "",
                        "file_name": "",
                        "time_stamp": "2022-10-21T14:00:00Z",
                        "time_start": "2022-10-21T14:00:00Z",
                        "time_end": "2022-10-21T14:05:00Z",
                        "paper_type": "",
                        "keywords": [],
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "v-spotlights1-prog-2",
                        "session_id": "spotlights1",
                        "type": "Virtual Presentation (live)",
                        "title": "Earth System Simulation using E3SM",
                        "contributors": [
                            "James Benedict"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "",
                        "file_name": "",
                        "time_stamp": "2022-10-21T14:05:00Z",
                        "time_start": "2022-10-21T14:05:00Z",
                        "time_end": "2022-10-21T14:15:00Z",
                        "paper_type": "",
                        "keywords": [],
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "v-spotlights1-prog-3",
                        "session_id": "spotlights1",
                        "type": "Virtual Presentation (live)",
                        "title": "Efficient Predictive Analytics of Extreme Weather Events via In Situ Data Modeling",
                        "contributors": [
                            "Soumya Dutta"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "",
                        "file_name": "",
                        "time_stamp": "2022-10-21T14:15:00Z",
                        "time_start": "2022-10-21T14:15:00Z",
                        "time_end": "2022-10-21T14:30:00Z",
                        "paper_type": "",
                        "keywords": [],
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "v-spotlights1-prog-4",
                        "session_id": "spotlights1",
                        "type": "Virtual Presentation (live)",
                        "title": "Distributed Gaussian Processes for Climate Modeling",
                        "contributors": [
                            "Mike Grosskopf"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "",
                        "file_name": "",
                        "time_stamp": "2022-10-21T14:30:00Z",
                        "time_start": "2022-10-21T14:30:00Z",
                        "time_end": "2022-10-21T14:45:00Z",
                        "paper_type": "",
                        "keywords": [],
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "v-spotlights1-prog-5",
                        "session_id": "spotlights1",
                        "type": "Virtual Presentation (live)",
                        "title": "Cyclone analysis",
                        "contributors": [
                            "Rupsa Bhowmick"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "",
                        "file_name": "",
                        "time_stamp": "2022-10-21T14:45:00Z",
                        "time_start": "2022-10-21T14:45:00Z",
                        "time_end": "2022-10-21T15:00:00Z",
                        "paper_type": "",
                        "keywords": [],
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "v-spotlights1-prog-6",
                        "session_id": "spotlights1",
                        "type": "In Person Presentation",
                        "title": "Neural Networks for Data Regression in Climate Data",
                        "contributors": [
                            "Divya Banesh"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "",
                        "file_name": "",
                        "time_stamp": "2022-10-21T15:00:00Z",
                        "time_start": "2022-10-21T15:00:00Z",
                        "time_end": "2022-10-21T15:15:00Z",
                        "paper_type": "",
                        "keywords": [],
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    }
                ]
            }
        ]
    },
    "v-spotlights2": {
        "event": "Audio-Visual Analytics: Potential Applications of Combined Sonifications and Visualizations",
        "long_name": "Audio-Visual Analytics: Potential Applications of Combined Sonifications and Visualizations",
        "event_type": "Application Spotlight",
        "event_prefix": "v-spotlights2",
        "event_description": "",
        "event_url": "",
        "organizers": [
            "Wolfgang Aigner",
            "Michael Iber",
            "Kajetan Enge",
            "Alexander Rind",
            "Niklas Elmqvist",
            "Robert H\u00f6ldrich",
            "Niklas R\u00f6nnberg",
            "Bruce Walker"
        ],
        "sessions": [
            {
                "title": "Audio-Visual Analytics: Potential Applications of Combined Sonifications and Visualizations",
                "session_id": "spotlights2",
                "event_prefix": "v-spotlights2",
                "track": "mistletoe",
                "livestream_id": "mistletoe-thu",
                "session_image": "spotlights2.png",
                "chair": [
                    "Wolfgang Aigner",
                    "Michael Iber",
                    "Kajetan Enge",
                    "Alexander Rind",
                    "Niklas Elmqvist",
                    "Robert H\u00f6ldrich",
                    "Niklas R\u00f6nnberg",
                    "Bruce Walker"
                ],
                "organizers": [],
                "time_start": "2022-10-20T15:45:00Z",
                "time_end": "2022-10-20T17:00:00Z",
                "discord_category": "",
                "discord_channel": "mistletoe",
                "discord_channel_id": "1024601055700471839",
                "discord_link": "https://discord.com/channels/978320435554947082/1024601055700471839",
                "slido_link": "https://app.sli.do/event/i28UETZaiRWr1XwX6eKjcf?section=05d57c7a-397b-4e16-a41d-7f06e8e64c37",
                "youtube_url": "https://youtu.be/QUC4mL-YR40",
                "youtube_id": "QUC4mL-YR40",
                "zoom_meeting": "",
                "zoom_password": "",
                "zoom_link": "",
                "ff_link": "",
                "ff_playlist": "",
                "time_slots": [
                    {
                        "slot_id": "v-spotlights2-prog-1",
                        "session_id": "spotlights2",
                        "type": "In Person Other",
                        "title": "Opening Presentation",
                        "contributors": [
                            "Kajetan Enge",
                            "Wolfgang Aigner"
                        ],
                        "authors": [
                            "Wolfgang Aigner, Michael Iber, Kajetan Enge, Alexander Rind, Niklas Elmqvist, Robert H\u00f6ldrich, Niklas R\u00f6nnberg, Bruce N. Walker"
                        ],
                        "abstract": "",
                        "uid": "",
                        "file_name": "",
                        "time_stamp": "2022-10-20T15:45:00Z",
                        "time_start": "2022-10-20T15:45:00Z",
                        "time_end": "2022-10-20T15:50:00Z",
                        "paper_type": "",
                        "keywords": [],
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "v-spotlights2-prog-2",
                        "session_id": "spotlights2",
                        "type": "Virtual Presentation (live)",
                        "title": "Highcharts Sonification Studio: Overview and demo ",
                        "contributors": [
                            "\u00d8ystein Moseng",
                            "Bruce N. Walker"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "",
                        "file_name": "",
                        "time_stamp": "2022-10-20T15:50:00Z",
                        "time_start": "2022-10-20T15:50:00Z",
                        "time_end": "2022-10-20T16:07:00Z",
                        "paper_type": "",
                        "keywords": [],
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "v-spotlights2-prog-3",
                        "session_id": "spotlights2",
                        "type": "Virtual Presentation (live)",
                        "title": "Mapping and Interdisciplinary ground: the Data Sonification Archive.",
                        "contributors": [
                            "Sara Lenzi",
                            "Paolo Ciuccarelli"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "",
                        "file_name": "",
                        "time_stamp": "2022-10-20T16:07:00Z",
                        "time_start": "2022-10-20T16:07:00Z",
                        "time_end": "2022-10-20T16:24:00Z",
                        "paper_type": "",
                        "keywords": [],
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "v-spotlights2-prog-4",
                        "session_id": "spotlights2",
                        "type": "Virtual Presentation (live)",
                        "title": "Real time sonification to support motor learning in health promotion and rehabilitation",
                        "contributors": [
                            "Victor-Adriel de-Jesus-Oliveira"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "",
                        "file_name": "",
                        "time_stamp": "2022-10-20T16:24:00Z",
                        "time_start": "2022-10-20T16:24:00Z",
                        "time_end": "2022-10-20T16:41:00Z",
                        "paper_type": "",
                        "keywords": [],
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "v-spotlights2-prog-5",
                        "session_id": "spotlights2",
                        "type": "Mixed Panel (virtual/in person)",
                        "title": "Panel Q&A and Closing",
                        "contributors": [
                            "Kajetan Enge",
                            "Wolfgang Aigner",
                            "Niklas R\u00f6nnberg",
                            "\u00d8ystein Moseng",
                            "Bruce N. Walker",
                            "Victor-Adriel de-Jesus-Oliveira",
                            "Sara Lenzi",
                            "Paolo Ciuccarelli"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "",
                        "file_name": "",
                        "time_stamp": "2022-10-20T16:41:00Z",
                        "time_start": "2022-10-20T16:41:00Z",
                        "time_end": "2022-10-20T17:00:00Z",
                        "paper_type": "",
                        "keywords": [],
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    }
                ]
            }
        ]
    },
    "v-spotlights3": {
        "event": "Application Papers: How should we deal with them?",
        "long_name": "Application Papers: How should we deal with them?",
        "event_type": "Application Spotlight",
        "event_prefix": "v-spotlights3",
        "event_description": "",
        "event_url": "",
        "organizers": [
            "Johanna Schmidt",
            "Daniel Wiegreffe",
            "Christina Gillmann"
        ],
        "sessions": [
            {
                "title": "Application Papers: How should we deal with them?",
                "session_id": "spotlights3",
                "event_prefix": "v-spotlights3",
                "track": "mistletoe",
                "livestream_id": "mistletoe-wed",
                "session_image": "spotlights3.png",
                "chair": [
                    "Johanna Schmidt",
                    "Daniel Wiegreffe",
                    "Christina Gillmann"
                ],
                "organizers": [],
                "time_start": "2022-10-19T19:00:00Z",
                "time_end": "2022-10-19T20:15:00Z",
                "discord_category": "",
                "discord_channel": "mistletoe",
                "discord_channel_id": "1024601055700471839",
                "discord_link": "https://discord.com/channels/978320435554947082/1024601055700471839",
                "slido_link": "https://app.sli.do/event/i28UETZaiRWr1XwX6eKjcf?section=05d57c7a-397b-4e16-a41d-7f06e8e64c37",
                "youtube_url": "https://youtu.be/xub7Gw386eY",
                "youtube_id": "xub7Gw386eY",
                "zoom_meeting": "",
                "zoom_password": "",
                "zoom_link": "",
                "ff_link": "",
                "ff_playlist": "",
                "time_slots": [
                    {
                        "slot_id": "v-spotlights3-prog-1",
                        "session_id": "spotlights3",
                        "type": "In Person Presentation",
                        "title": "Opening",
                        "contributors": [
                            "Johanna Schmidt",
                            "Daniel Wiegreffe",
                            "Christina Gillmann"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "",
                        "file_name": "",
                        "time_stamp": "2022-10-19T19:00:00Z",
                        "time_start": "2022-10-19T19:00:00Z",
                        "time_end": "2022-10-19T19:05:00Z",
                        "paper_type": "",
                        "keywords": [],
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "v-spotlights3-prog-2",
                        "session_id": "spotlights3",
                        "type": "In Person Presentation",
                        "title": "From domain experts to children ",
                        "contributors": [
                            "Anders Ynnermann"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "",
                        "file_name": "",
                        "time_stamp": "2022-10-19T19:05:00Z",
                        "time_start": "2022-10-19T19:05:00Z",
                        "time_end": "2022-10-19T19:15:00Z",
                        "paper_type": "",
                        "keywords": [],
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "v-spotlights3-prog-3",
                        "session_id": "spotlights3",
                        "type": "In Person Presentation",
                        "title": "Applications in Medical Visualizations: A Case Study",
                        "contributors": [
                            "Thomas Wischgoll"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "",
                        "file_name": "",
                        "time_stamp": "2022-10-19T19:15:00Z",
                        "time_start": "2022-10-19T19:15:00Z",
                        "time_end": "2022-10-19T19:25:00Z",
                        "paper_type": "",
                        "keywords": [],
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "v-spotlights3-prog-4",
                        "session_id": "spotlights3",
                        "type": "In Person Presentation",
                        "title": "Visualization and Applications in Flow Simulation with Simcenter STAR-CCM+",
                        "contributors": [
                            "Helmut Doleisch"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "",
                        "file_name": "",
                        "time_stamp": "2022-10-19T19:25:00Z",
                        "time_start": "2022-10-19T19:25:00Z",
                        "time_end": "2022-10-19T19:35:00Z",
                        "paper_type": "",
                        "keywords": [],
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "v-spotlights3-prog-5",
                        "session_id": "spotlights3",
                        "type": "In Person Presentation",
                        "title": "Panel Discussion",
                        "contributors": [
                            "All"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "",
                        "file_name": "",
                        "time_stamp": "2022-10-19T19:35:00Z",
                        "time_start": "2022-10-19T19:35:00Z",
                        "time_end": "2022-10-19T20:05:00Z",
                        "paper_type": "",
                        "keywords": [],
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "v-spotlights3-prog-6",
                        "session_id": "spotlights3",
                        "type": "In Person Presentation",
                        "title": "Closing",
                        "contributors": [
                            "Johanna Schmidt",
                            "Daniel Wiegreffe",
                            "Christina Gillmann"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "",
                        "file_name": "",
                        "time_stamp": "2022-10-19T20:05:00Z",
                        "time_start": "2022-10-19T20:05:00Z",
                        "time_end": "2022-10-19T20:15:00Z",
                        "paper_type": "",
                        "keywords": [],
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    }
                ]
            }
        ]
    },
    "a-sciviscontest": {
        "event": "SciVis Contest",
        "long_name": "SciVis Contest",
        "event_type": "Associated Event (Competition)",
        "event_prefix": "a-sciviscontest",
        "event_description": "",
        "event_url": "",
        "organizers": [
            "Alex Razoumov",
            "Divya Banesh"
        ],
        "sessions": [
            {
                "title": "SciVis Contest",
                "session_id": "a-sciviscontest-1",
                "event_prefix": "a-sciviscontest",
                "track": "pinon",
                "livestream_id": "pinon-thu",
                "session_image": "a-sciviscontest-1.png",
                "chair": [
                    "Divya Banesh",
                    "Alex Razoumov"
                ],
                "organizers": [],
                "time_start": "2022-10-20T14:00:00Z",
                "time_end": "2022-10-20T15:15:00Z",
                "discord_category": "",
                "discord_channel": "pinon",
                "discord_channel_id": "1024601027644760074",
                "discord_link": "https://discord.com/channels/978320435554947082/1024601027644760074",
                "slido_link": "https://app.sli.do/event/i28UETZaiRWr1XwX6eKjcf?section=bbc002b7-d3ae-48bc-b2d3-0624a7bfed06",
                "youtube_url": "https://youtu.be/Dj-iscrBm6g",
                "youtube_id": "Dj-iscrBm6g",
                "zoom_meeting": "",
                "zoom_password": "",
                "zoom_link": "",
                "ff_link": "",
                "ff_playlist": "",
                "time_slots": [
                    {
                        "slot_id": "a-sciviscontest-prog-1",
                        "session_id": "a-sciviscontest-1",
                        "type": "In Person Presentation",
                        "title": "Opening Presentation",
                        "contributors": [
                            "Divya Banesh"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "",
                        "file_name": "",
                        "time_stamp": "2022-10-20T14:00:00Z",
                        "time_start": "2022-10-20T14:00:00Z",
                        "time_end": "2022-10-20T14:15:00Z",
                        "paper_type": "",
                        "keywords": [],
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "a-sciviscontest-prog-2",
                        "session_id": "a-sciviscontest-1",
                        "type": "Virtual Presentation (live)",
                        "title": "Higrad/Firetec Presentation",
                        "contributors": [
                            "Rodman Linn"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "",
                        "file_name": "",
                        "time_stamp": "2022-10-20T14:15:00Z",
                        "time_start": "2022-10-20T14:15:00Z",
                        "time_end": "2022-10-20T14:30:00Z",
                        "paper_type": "",
                        "keywords": [],
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "a-scivis-1009-pres",
                        "session_id": "a-sciviscontest-1",
                        "type": "In Person Presentation",
                        "title": "Contest Speaker: Marina Evers",
                        "contributors": [
                            "Marina Evers"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "a-sciviscontest-posters-1009",
                        "file_name": "",
                        "time_stamp": "2022-10-20T14:30:00Z",
                        "time_start": "2022-10-20T14:30:00Z",
                        "time_end": "2022-10-20T14:40:00Z",
                        "paper_type": "",
                        "keywords": [],
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "a-scivis-1017-pres",
                        "session_id": "a-sciviscontest-1",
                        "type": "Virtual Presentation (pre-recorded)",
                        "title": "Contest Speaker: Milad Bagheri",
                        "contributors": [
                            "Milad Bagheri"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "a-sciviscontest-posters-1017",
                        "file_name": "",
                        "time_stamp": "2022-10-20T14:40:00Z",
                        "time_start": "2022-10-20T14:40:00Z",
                        "time_end": "2022-10-20T14:50:00Z",
                        "paper_type": "",
                        "keywords": [],
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "a-sciviscontest-prog-3",
                        "session_id": "a-sciviscontest-1",
                        "type": "In Person Presentation",
                        "title": "Award Presentation",
                        "contributors": [
                            "Divya Banesh"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "",
                        "file_name": "",
                        "time_stamp": "2022-10-20T14:50:00Z",
                        "time_start": "2022-10-20T14:50:00Z",
                        "time_end": "2022-10-20T15:00:00Z",
                        "paper_type": "",
                        "keywords": [],
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "a-sciviscontest-prog-4",
                        "session_id": "a-sciviscontest-1",
                        "type": "Virtual Presentation (live)",
                        "title": "Next Year's Contest Presentation",
                        "contributors": [
                            "Tim Gerrits"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "",
                        "file_name": "",
                        "time_stamp": "2022-10-20T15:00:00Z",
                        "time_start": "2022-10-20T15:00:00Z",
                        "time_end": "2022-10-20T15:15:00Z",
                        "paper_type": "",
                        "keywords": [],
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    }
                ]
            }
        ]
    },
    "a-ldav": {
        "event": "LDAV: 12th IEEE Symposium on Large Data Analysis and Visualization",
        "long_name": "LDAV: 12th IEEE Symposium on Large Data Analysis and Visualization",
        "event_type": "Associated Event (Symposium)",
        "event_prefix": "a-ldav",
        "event_description": "",
        "event_url": "",
        "organizers": [
            "Chaoli Wang",
            "Peer-Timo Bremer",
            "Kristi Potter"
        ],
        "sessions": [
            {
                "title": "LDAV: Opening + Keynote",
                "session_id": "a-ldav-1",
                "event_prefix": "a-ldav",
                "track": "ok4",
                "livestream_id": "ok4-sun",
                "session_image": "a-ldav-1.png",
                "chair": [
                    "Chaoli Wang",
                    "Paul Navratil"
                ],
                "organizers": [],
                "time_start": "2022-10-16T14:00:00Z",
                "time_end": "2022-10-16T15:15:00Z",
                "discord_category": "",
                "discord_channel": "oklahoma-st-4",
                "discord_channel_id": "1024600674924765264",
                "discord_link": "https://discord.com/channels/978320435554947082/1024600674924765264",
                "slido_link": "https://app.sli.do/event/i28UETZaiRWr1XwX6eKjcf?section=51658b34-0f5b-40c9-a335-1fd1468fad8d",
                "youtube_url": "https://youtu.be/vNruICFZuvw",
                "youtube_id": "vNruICFZuvw",
                "zoom_meeting": "",
                "zoom_password": "",
                "zoom_link": "",
                "ff_link": "",
                "ff_playlist": "",
                "time_slots": [
                    {
                        "slot_id": "a-ldav-prog-1",
                        "session_id": "a-ldav-1",
                        "type": "In Person Other",
                        "title": "Opening Presentation",
                        "contributors": [
                            "Paul Navratil"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "",
                        "file_name": "",
                        "time_stamp": "2022-10-16T14:00:00Z",
                        "time_start": "2022-10-16T14:00:00Z",
                        "time_end": "2022-10-16T14:15:00Z",
                        "paper_type": "",
                        "keywords": [],
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "a-ldav-prog-2",
                        "session_id": "a-ldav-1",
                        "type": "In Person Presentation",
                        "title": "Machine Learning for Large Scale Scientific Data Analysis and Visualization",
                        "contributors": [
                            "Han-Wei Shen"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "",
                        "file_name": "",
                        "time_stamp": "2022-10-16T14:15:00Z",
                        "time_start": "2022-10-16T14:15:00Z",
                        "time_end": "2022-10-16T15:15:00Z",
                        "paper_type": "",
                        "keywords": [],
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    }
                ]
            },
            {
                "title": "LDAV: Parallelization & Progressiveness",
                "session_id": "a-ldav-2",
                "event_prefix": "a-ldav",
                "track": "ok4",
                "livestream_id": "ok4-sun",
                "session_image": "a-ldav-2.png",
                "chair": [
                    "David Pugmire"
                ],
                "organizers": [],
                "time_start": "2022-10-16T15:45:00Z",
                "time_end": "2022-10-16T17:00:00Z",
                "discord_category": "",
                "discord_channel": "oklahoma-st-4",
                "discord_channel_id": "1024600674924765264",
                "discord_link": "https://discord.com/channels/978320435554947082/1024600674924765264",
                "slido_link": "https://app.sli.do/event/i28UETZaiRWr1XwX6eKjcf?section=51658b34-0f5b-40c9-a335-1fd1468fad8d",
                "youtube_url": "https://youtu.be/vNruICFZuvw",
                "youtube_id": "vNruICFZuvw",
                "zoom_meeting": "",
                "zoom_password": "",
                "zoom_link": "",
                "ff_link": "",
                "ff_playlist": "",
                "time_slots": [
                    {
                        "slot_id": "a-ldav-1016-pres",
                        "session_id": "a-ldav-2",
                        "type": "Virtual Presentation (pre-recorded)",
                        "title": "Hybrid Image-/Data-Parallel Rendering Using Island Parallelism",
                        "contributors": [
                            "Stefan Zellmann"
                        ],
                        "authors": [
                            "Stefan Zellmann",
                            "Ingo Wald",
                            "Joao Barbosa",
                            "Serkan Demirci",
                            "Alper Sahistan",
                            "Ugur Gudukbay"
                        ],
                        "abstract": "In parallel ray tracing, techniques fall into one of two camps: image-parallel techniques aim at increasing frame rate by replicating scene data across nodes and splitting the rendering work across different ranks, and data-parallel techniques aim at increasing the size of the model that can be rendered by splitting the model across multiple ranks, but typically cannot scale much in frame rate. We propose and evaluate a hybrid approach that combines the advantages of both by splitting a set of N \u00d7 M ranks into M islands of N ranks each and using data-parallel rendering within each island and image parallelism across islands. We discuss the integration of this concept into four wildly different parallel renderers and evaluate the efficacy of this approach based on multiple different data sets.",
                        "uid": "a-ldav-1016",
                        "file_name": "a-ldav-1016_Zellmann_Presentation.mp4",
                        "time_stamp": "2022-10-16T15:45:00Z",
                        "time_start": "2022-10-16T15:45:00Z",
                        "time_end": "2022-10-16T16:05:00Z",
                        "paper_type": "associated",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "1242",
                        "paper_award": "",
                        "image_caption": "Four very different data-parallel, ray-based renderers. a) Sort-last absorption + emission volume renderer. b) Ray- forwarding production path tracer realized using OptiX and hardware ray tracing. c) Volumetric path tracer optimized for scientific visualization (sci-vis). d) Large unstructured element-marcher with simulation-provided bounding geometry. All four renderers operate on huge data, but have largely different scaling properties; we show how hybrid image-/data-parallel rendering with islands can benefit these renderers to scale far beyond either image or data-parallel rendering applied in isolation.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "a-ldav-1016-qa",
                        "session_id": "a-ldav-2",
                        "type": "Virtual Q+A",
                        "title": "Hybrid Image-/Data-Parallel Rendering Using Island Parallelism (Q+A)",
                        "contributors": [
                            "Stefan Zellmann"
                        ],
                        "authors": [],
                        "abstract": "In parallel ray tracing, techniques fall into one of two camps: image-parallel techniques aim at increasing frame rate by replicating scene data across nodes and splitting the rendering work across different ranks, and data-parallel techniques aim at increasing the size of the model that can be rendered by splitting the model across multiple ranks, but typically cannot scale much in frame rate. We propose and evaluate a hybrid approach that combines the advantages of both by splitting a set of N \u00d7 M ranks into M islands of N ranks each and using data-parallel rendering within each island and image parallelism across islands. We discuss the integration of this concept into four wildly different parallel renderers and evaluate the efficacy of this approach based on multiple different data sets.",
                        "uid": "a-ldav-1016",
                        "file_name": "",
                        "time_stamp": "2022-10-16T16:05:00Z",
                        "time_start": "2022-10-16T16:05:00Z",
                        "time_end": "2022-10-16T16:10:00Z",
                        "paper_type": "associated",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "1242",
                        "paper_award": "",
                        "image_caption": "Four very different data-parallel, ray-based renderers. a) Sort-last absorption + emission volume renderer. b) Ray- forwarding production path tracer realized using OptiX and hardware ray tracing. c) Volumetric path tracer optimized for scientific visualization (sci-vis). d) Large unstructured element-marcher with simulation-provided bounding geometry. All four renderers operate on huge data, but have largely different scaling properties; we show how hybrid image-/data-parallel rendering with islands can benefit these renderers to scale far beyond either image or data-parallel rendering applied in isolation.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "a-ldav-1012-pres",
                        "session_id": "a-ldav-2",
                        "type": "In Person Presentation",
                        "title": "A Prototype for Pipeline-Composable Task-Based Visualization Algorithms",
                        "contributors": [
                            "Marvin Petersen"
                        ],
                        "authors": [
                            "Marvin Petersen",
                            "Kilian Werner",
                            "Andrea Schnorr",
                            "Torsten Wolfgang Kuhlen",
                            "Christoph Garth"
                        ],
                        "abstract": "For next generation platforms, the paradigm of task-based parallelism has the potential to overcome some of the accompanying challenges. This paradigm has already been applied by the visualization community to specific algorithms and problems. However, one advantage of the task-based paradigm\u2014the interleaving of work\u2014should lead to better utilization of resources and ultimately lower execution times if the paradigm is applied to whole pipelines. In order to investigate this potential, we build a prototype framework for composable task-based parallel visualization algorithms. With this we explore the combination of a strictly task-based approach with the addition of a pipeline layer for visualization algorithms. This additional layer eases the composition of larger task-based parallel visualization applications without the need to explicitly define the exact connection in a task graph between algorithms. In this manner, task-based visualization algorithms can be designed towards a common interface, be easily combined, and still benefit from the advantages of the task-based paradigm across algorithm boundaries, such as latency hiding. We explore the design implications of this combination and show initial results of the scalability and the impact of task interleaving on the runtime of exemplary pipelines.",
                        "uid": "a-ldav-1012",
                        "file_name": "",
                        "time_stamp": "2022-10-16T16:10:00Z",
                        "time_start": "2022-10-16T16:10:00Z",
                        "time_end": "2022-10-16T16:30:00Z",
                        "paper_type": "associated",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "Comparison of a task-based volume rendering pipeline with artificial barriers across algorithm boundaries (top row) with a task-based volume rendering pipeline without barriers (bottom row). On the left, the coupling of the module task graphs in the pipeline is exemplified and on the right thread occupation of the different tasks is shown for the pipeline execution on one compute node.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "a-ldav-1012-qa",
                        "session_id": "a-ldav-2",
                        "type": "In Person Q+A",
                        "title": "A Prototype for Pipeline-Composable Task-Based Visualization Algorithms (Q+A)",
                        "contributors": [
                            "Marvin Petersen"
                        ],
                        "authors": [],
                        "abstract": "For next generation platforms, the paradigm of task-based parallelism has the potential to overcome some of the accompanying challenges. This paradigm has already been applied by the visualization community to specific algorithms and problems. However, one advantage of the task-based paradigm\u2014the interleaving of work\u2014should lead to better utilization of resources and ultimately lower execution times if the paradigm is applied to whole pipelines. In order to investigate this potential, we build a prototype framework for composable task-based parallel visualization algorithms. With this we explore the combination of a strictly task-based approach with the addition of a pipeline layer for visualization algorithms. This additional layer eases the composition of larger task-based parallel visualization applications without the need to explicitly define the exact connection in a task graph between algorithms. In this manner, task-based visualization algorithms can be designed towards a common interface, be easily combined, and still benefit from the advantages of the task-based paradigm across algorithm boundaries, such as latency hiding. We explore the design implications of this combination and show initial results of the scalability and the impact of task interleaving on the runtime of exemplary pipelines.",
                        "uid": "a-ldav-1012",
                        "file_name": "",
                        "time_stamp": "2022-10-16T16:30:00Z",
                        "time_start": "2022-10-16T16:30:00Z",
                        "time_end": "2022-10-16T16:35:00Z",
                        "paper_type": "associated",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "Comparison of a task-based volume rendering pipeline with artificial barriers across algorithm boundaries (top row) with a task-based volume rendering pipeline without barriers (bottom row). On the left, the coupling of the module task graphs in the pipeline is exemplified and on the right thread occupation of the different tasks is shown for the pipeline execution on one compute node.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "a-ldav-1008-pres",
                        "session_id": "a-ldav-2",
                        "type": "Virtual Presentation (pre-recorded)",
                        "title": "High-Quality Progressive Alignment of Large 3D Microscopy Data",
                        "contributors": [
                            "Aniketh Venkat"
                        ],
                        "authors": [
                            "Aniketh Venkat"
                        ],
                        "abstract": "Large-scale three-dimensional (3D) microscopy acquisitions frequently create terabytes of image data at high resolution and magnification. Imaging large specimens at high magnifications requires acquiring 3D overlapping image stacks as tiles arranged on a two-dimensional (2D) grid that must subsequently be aligned and fused into a single 3D volume. Due to their sheer size, aligning many overlapping gigabyte-sized 3D tiles in parallel and at full resolution is memory intensive and often I/O bound. Current techniques trade accuracy for scalability, perform alignment on subsampled images, and require additional postprocess algorithms to refine the alignment quality, usually with high computational requirements. One common solution to the memory problem is to subdivide the overlap region into smaller chunks (sub-blocks) and align the sub-block pairs in parallel, choosing the pair with the most reliable alignment to determine the global transformation. Yet aligning all sub-block pairs at full resolution remains computationally expensive. The key to quickly developing a fast, high-quality, low-memory solution is to identify a single or a small set of sub-blocks that give good alignment at full resolution without touching all the overlapping data. In this paper, we present a new iterative approach that leverages coarse resolution alignments to progressively refine and align only the promising candidates at finer resolutions, thereby aligning only a small user-defined number of sub-blocks at full resolution to determine the lowest error transformation between pairwise overlapping tiles. Our progressive approach is 2.6x faster than the state of the art, requires less than 450MB of peak RAM (per parallel thread), and offers a higher quality alignment without the need for additional postprocessing refinement steps to correct for alignment errors.",
                        "uid": "a-ldav-1008",
                        "file_name": "a-ldav-1008_Venkat_Presentation.mp4",
                        "time_stamp": "2022-10-16T16:35:00Z",
                        "time_start": "2022-10-16T16:35:00Z",
                        "time_end": "2022-10-16T16:55:00Z",
                        "paper_type": "associated",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "872",
                        "paper_award": "",
                        "image_caption": "Our new progressive approach aligns a 616GB image in 4.97 minutes, it is 2.6 times faster than the state of the art and it requires 6.5 times smaller memory footprint.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "a-ldav-1008-qa",
                        "session_id": "a-ldav-2",
                        "type": "Virtual Q+A",
                        "title": "High-Quality Progressive Alignment of Large 3D Microscopy Data (Q+A)",
                        "contributors": [
                            "Aniketh Venkat"
                        ],
                        "authors": [],
                        "abstract": "Large-scale three-dimensional (3D) microscopy acquisitions frequently create terabytes of image data at high resolution and magnification. Imaging large specimens at high magnifications requires acquiring 3D overlapping image stacks as tiles arranged on a two-dimensional (2D) grid that must subsequently be aligned and fused into a single 3D volume. Due to their sheer size, aligning many overlapping gigabyte-sized 3D tiles in parallel and at full resolution is memory intensive and often I/O bound. Current techniques trade accuracy for scalability, perform alignment on subsampled images, and require additional postprocess algorithms to refine the alignment quality, usually with high computational requirements. One common solution to the memory problem is to subdivide the overlap region into smaller chunks (sub-blocks) and align the sub-block pairs in parallel, choosing the pair with the most reliable alignment to determine the global transformation. Yet aligning all sub-block pairs at full resolution remains computationally expensive. The key to quickly developing a fast, high-quality, low-memory solution is to identify a single or a small set of sub-blocks that give good alignment at full resolution without touching all the overlapping data. In this paper, we present a new iterative approach that leverages coarse resolution alignments to progressively refine and align only the promising candidates at finer resolutions, thereby aligning only a small user-defined number of sub-blocks at full resolution to determine the lowest error transformation between pairwise overlapping tiles. Our progressive approach is 2.6x faster than the state of the art, requires less than 450MB of peak RAM (per parallel thread), and offers a higher quality alignment without the need for additional postprocessing refinement steps to correct for alignment errors.",
                        "uid": "a-ldav-1008",
                        "file_name": "",
                        "time_stamp": "2022-10-16T16:55:00Z",
                        "time_start": "2022-10-16T16:55:00Z",
                        "time_end": "2022-10-16T17:00:00Z",
                        "paper_type": "associated",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "872",
                        "paper_award": "",
                        "image_caption": "Our new progressive approach aligns a 616GB image in 4.97 minutes, it is 2.6 times faster than the state of the art and it requires 6.5 times smaller memory footprint.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "",
                        "ff_id": ""
                    }
                ]
            },
            {
                "title": "LDAV: Topology & Ensembles",
                "session_id": "a-ldav-3",
                "event_prefix": "a-ldav",
                "track": "ok4",
                "livestream_id": "ok4-sun",
                "session_image": "a-ldav-3.png",
                "chair": [
                    "Markus Hadwiger"
                ],
                "organizers": [],
                "time_start": "2022-10-16T19:00:00Z",
                "time_end": "2022-10-16T20:15:00Z",
                "discord_category": "",
                "discord_channel": "oklahoma-st-4",
                "discord_channel_id": "1024600674924765264",
                "discord_link": "https://discord.com/channels/978320435554947082/1024600674924765264",
                "slido_link": "https://app.sli.do/event/i28UETZaiRWr1XwX6eKjcf?section=51658b34-0f5b-40c9-a335-1fd1468fad8d",
                "youtube_url": "https://youtu.be/vNruICFZuvw",
                "youtube_id": "vNruICFZuvw",
                "zoom_meeting": "",
                "zoom_password": "",
                "zoom_link": "",
                "ff_link": "",
                "ff_playlist": "",
                "time_slots": [
                    {
                        "slot_id": "a-ldav-1013-pres",
                        "session_id": "a-ldav-3",
                        "type": "Virtual Presentation (pre-recorded)",
                        "title": "Distributed Hierarchical Contour Trees",
                        "contributors": [
                            "Hamish Carr"
                        ],
                        "authors": [
                            "Hamish Carr",
                            "Oliver R\u00fcbel",
                            "Gunther H Weber"
                        ],
                        "abstract": "Contour trees are a significant tool for data analysis as they capture both local and global variation. However, their utility has been limited by scalability, in particular for distributed computation and storage. We report a distributed data structure for storing the contour tree of a data set distributed on a cluster, based on a fan-in hierarchy, and an algorithm for computing it based on the boundary tree that represents only the superarcs of a contour tree that involve contours that cross boundaries between blocks. This allows us to limit the communication cost for contour tree computation to the complexity of the block boundaries rather than of the entire data set.",
                        "uid": "a-ldav-1013",
                        "file_name": "",
                        "time_stamp": "2022-10-16T19:00:00Z",
                        "time_start": "2022-10-16T19:00:00Z",
                        "time_end": "2022-10-16T19:20:00Z",
                        "paper_type": "associated",
                        "keywords": [],
                        "has_image": "0",
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "a-ldav-1013-qa",
                        "session_id": "a-ldav-3",
                        "type": "Virtual Q+A",
                        "title": "Distributed Hierarchical Contour Trees (Q+A)",
                        "contributors": [
                            "Hamish Carr"
                        ],
                        "authors": [],
                        "abstract": "Contour trees are a significant tool for data analysis as they capture both local and global variation. However, their utility has been limited by scalability, in particular for distributed computation and storage. We report a distributed data structure for storing the contour tree of a data set distributed on a cluster, based on a fan-in hierarchy, and an algorithm for computing it based on the boundary tree that represents only the superarcs of a contour tree that involve contours that cross boundaries between blocks. This allows us to limit the communication cost for contour tree computation to the complexity of the block boundaries rather than of the entire data set.",
                        "uid": "a-ldav-1013",
                        "file_name": "",
                        "time_stamp": "2022-10-16T19:20:00Z",
                        "time_start": "2022-10-16T19:20:00Z",
                        "time_end": "2022-10-16T19:25:00Z",
                        "paper_type": "associated",
                        "keywords": [],
                        "has_image": "0",
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "a-ldav-1001-pres",
                        "session_id": "a-ldav-3",
                        "type": "In Person Presentation",
                        "title": "Topological Analysis of Ensembles of Hydrodynamic Turbulent Flows, an Experimental Study",
                        "contributors": [
                            "Florent Nauleau"
                        ],
                        "authors": [
                            "Florent Nauleau",
                            "Fabien Vivodtzev",
                            "Thibault Bridel-Bertomeu",
                            "H\u00e9lo\u00efse Beaugendre",
                            "Julien Tierny"
                        ],
                        "abstract": "This application paper presents a comprehensive experimental evaluation of the suitability of Topological Data Analysis (TDA) for the quantitative comparison of turbulent flows. Specifically, our study documents the usage of the persistence diagram of the maxima of flow enstrophy (an established vorticity indicator), for the topological representation of 180 ensemble members, generated by a coarse sampling of the parameter space of five numerical solvers. We document five main hypotheses reported by domain experts, describing their expectations regarding the variability of the flows generated by the distinct solver configurations. We contribute three evaluation protocols to assess the validation of the above hypotheses by two comparison measures: (i) a standard distance used in distance between persistence diagrams (the L2 -Wasserstein metric). Extensive experiments on the input ensemble demonstrate the superiority of the topological distance (ii) to report as close to each other flows which are expected to be similar by domain experts, due to the configuration of their vortices. Overall, the insights reported by our study bring an experimental evidence of the suitability of TDA for representing and comparing turbulent flows, thereby providing to the fluid dynamics community confidence for its usage in future work. Also, our flow data and evaluation protocols provide to the TDA community an application-approved benchmark for the evaluation and design of further topological distances.",
                        "uid": "a-ldav-1001",
                        "file_name": "a-ldav-1001_Nauleau_Presentation.mp4",
                        "time_stamp": "2022-10-16T19:25:00Z",
                        "time_start": "2022-10-16T19:25:00Z",
                        "time_end": "2022-10-16T19:45:00Z",
                        "paper_type": "associated",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "1230",
                        "paper_award": "",
                        "image_caption": "Topological Data Analysis protocols applied on an ensemble dataset of a Kelvin-Helmholtz instability. (a) The 180 members of the ensemble obtained with variations of timesteps, interpolation schemes, orders, resolutions and Riemann solvers (Tab. 1). (b) The top cluster represents the time separation of t0 and t1 for the flows S1 and S2 with the Wasserstein distance and the bottom cluster with the L2 -norm. Red lines show the timestep separation with our clustering method whereas the sphere colors are the ground truth, illustrating the limitation of the L2 -norm. (c) Persistence curve protocol: Differences between integrals of persistence curves (gray area) of the enstrophy computed with a SLAU2 solver, an order 7 TENO scheme and a resolution of 1024 \u00d7 1024 for various configurations (S1 at t0, S2 and S3 at t1). These integral differences exhibit the appearance of vortices (critical points) as the time increases. (d) Outlier distance protocol: Wasserstein distance matrix for 5 configurations S1 (t0 , HLLC), S2 (t1 , Roe), S3 (t1 , HLLC), S4 (t2 , Roe), S5 (t2 , HLLC) computed with an order 7 WENO-Z interpolation scheme at 512 \u00d7 512. The sum of each row the configuration maximizing this distance between solvers and timesteps, here S1 . (e) Unsupervised classification: Wasserstein distance matrix for the previous configurations with an order 7 WENO-Z interpolation scheme at 256 \u00d7 256. The clustering based on the Wasserstein distance and colored according to the Kmeans clustering method successfully segments the time steps.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "a-ldav-1001-qa",
                        "session_id": "a-ldav-3",
                        "type": "In Person Q+A",
                        "title": "Topological Analysis of Ensembles of Hydrodynamic Turbulent Flows, an Experimental Study (Q+A)",
                        "contributors": [
                            "Florent Nauleau"
                        ],
                        "authors": [],
                        "abstract": "This application paper presents a comprehensive experimental evaluation of the suitability of Topological Data Analysis (TDA) for the quantitative comparison of turbulent flows. Specifically, our study documents the usage of the persistence diagram of the maxima of flow enstrophy (an established vorticity indicator), for the topological representation of 180 ensemble members, generated by a coarse sampling of the parameter space of five numerical solvers. We document five main hypotheses reported by domain experts, describing their expectations regarding the variability of the flows generated by the distinct solver configurations. We contribute three evaluation protocols to assess the validation of the above hypotheses by two comparison measures: (i) a standard distance used in distance between persistence diagrams (the L2 -Wasserstein metric). Extensive experiments on the input ensemble demonstrate the superiority of the topological distance (ii) to report as close to each other flows which are expected to be similar by domain experts, due to the configuration of their vortices. Overall, the insights reported by our study bring an experimental evidence of the suitability of TDA for representing and comparing turbulent flows, thereby providing to the fluid dynamics community confidence for its usage in future work. Also, our flow data and evaluation protocols provide to the TDA community an application-approved benchmark for the evaluation and design of further topological distances.",
                        "uid": "a-ldav-1001",
                        "file_name": "",
                        "time_stamp": "2022-10-16T19:45:00Z",
                        "time_start": "2022-10-16T19:45:00Z",
                        "time_end": "2022-10-16T19:50:00Z",
                        "paper_type": "associated",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "1230",
                        "paper_award": "",
                        "image_caption": "Topological Data Analysis protocols applied on an ensemble dataset of a Kelvin-Helmholtz instability. (a) The 180 members of the ensemble obtained with variations of timesteps, interpolation schemes, orders, resolutions and Riemann solvers (Tab. 1). (b) The top cluster represents the time separation of t0 and t1 for the flows S1 and S2 with the Wasserstein distance and the bottom cluster with the L2 -norm. Red lines show the timestep separation with our clustering method whereas the sphere colors are the ground truth, illustrating the limitation of the L2 -norm. (c) Persistence curve protocol: Differences between integrals of persistence curves (gray area) of the enstrophy computed with a SLAU2 solver, an order 7 TENO scheme and a resolution of 1024 \u00d7 1024 for various configurations (S1 at t0, S2 and S3 at t1). These integral differences exhibit the appearance of vortices (critical points) as the time increases. (d) Outlier distance protocol: Wasserstein distance matrix for 5 configurations S1 (t0 , HLLC), S2 (t1 , Roe), S3 (t1 , HLLC), S4 (t2 , Roe), S5 (t2 , HLLC) computed with an order 7 WENO-Z interpolation scheme at 512 \u00d7 512. The sum of each row the configuration maximizing this distance between solvers and timesteps, here S1 . (e) Unsupervised classification: Wasserstein distance matrix for the previous configurations with an order 7 WENO-Z interpolation scheme at 256 \u00d7 256. The clustering based on the Wasserstein distance and colored according to the Kmeans clustering method successfully segments the time steps.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "a-ldav-1006-pres",
                        "session_id": "a-ldav-3",
                        "type": "In Person Presentation",
                        "title": "Angular-based Edge Bundled Parallel Coordinates Plot for the Visual Analysis of Large Ensemble Simulation Data",
                        "contributors": [
                            "Keita Watanabe"
                        ],
                        "authors": [
                            "Keita Watanabe",
                            "Naohisa Sakamoto",
                            "Jorji Nonaka",
                            "Yasumitsu Maejima"
                        ],
                        "abstract": "With the continuous increase in the computational power and resources of modern high-performance computing (HPC) systems, large-scale ensemble simulations have become widely used in various fields of science and engineering, and especially in meteorological and climate science. It is widely known that the simulation outputs are large time-varying, multivariate, and multivalued datasets which pose a particular challenge to the visualization and analysis tasks. In this work, we focused on the widely used Parallel Coordinates Plot (PCP) to analyze the interrelations between different parameters, such as variables, among the members. However, PCP may suffer from visual cluttering and drawing performance with the increase on the data size to be analyzed, that is, the number of polylines. To overcome this problem, we present an extension to the PCP by adding B\\'{e}zier curves connecting the angular distribution plots representing the mean and variance of the inclination of the line segments between parallel axes. The proposed Angular-based Parallel Coordinates Plot (APCP) is capable of presenting a simplified overview of the entire ensemble data set while maintaining the correlation information between the adjacent variables. To verify its effectiveness, we developed a visual analytics prototype system and evaluated by using a meteorological ensemble simulation output from the supercomputer Fugaku.",
                        "uid": "a-ldav-1006",
                        "file_name": "a-ldav-1006_Keita_Presentation.mp4",
                        "time_stamp": "2022-10-16T19:50:00Z",
                        "time_start": "2022-10-16T19:50:00Z",
                        "time_end": "2022-10-16T20:10:00Z",
                        "paper_type": "associated",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "1180",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "a-ldav-1006-qa",
                        "session_id": "a-ldav-3",
                        "type": "In Person Q+A",
                        "title": "Angular-based Edge Bundled Parallel Coordinates Plot for the Visual Analysis of Large Ensemble Simulation Data (Q+A)",
                        "contributors": [
                            "Keita Watanabe"
                        ],
                        "authors": [],
                        "abstract": "With the continuous increase in the computational power and resources of modern high-performance computing (HPC) systems, large-scale ensemble simulations have become widely used in various fields of science and engineering, and especially in meteorological and climate science. It is widely known that the simulation outputs are large time-varying, multivariate, and multivalued datasets which pose a particular challenge to the visualization and analysis tasks. In this work, we focused on the widely used Parallel Coordinates Plot (PCP) to analyze the interrelations between different parameters, such as variables, among the members. However, PCP may suffer from visual cluttering and drawing performance with the increase on the data size to be analyzed, that is, the number of polylines. To overcome this problem, we present an extension to the PCP by adding B\\'{e}zier curves connecting the angular distribution plots representing the mean and variance of the inclination of the line segments between parallel axes. The proposed Angular-based Parallel Coordinates Plot (APCP) is capable of presenting a simplified overview of the entire ensemble data set while maintaining the correlation information between the adjacent variables. To verify its effectiveness, we developed a visual analytics prototype system and evaluated by using a meteorological ensemble simulation output from the supercomputer Fugaku.",
                        "uid": "a-ldav-1006",
                        "file_name": "",
                        "time_stamp": "2022-10-16T20:10:00Z",
                        "time_start": "2022-10-16T20:10:00Z",
                        "time_end": "2022-10-16T20:15:00Z",
                        "paper_type": "associated",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "1180",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "",
                        "ff_id": ""
                    }
                ]
            },
            {
                "title": "LDAV: Early Career Researcher Lightning Talks + Closing",
                "session_id": "a-ldav-4",
                "event_prefix": "a-ldav",
                "track": "ok4",
                "livestream_id": "ok4-sun",
                "session_image": "a-ldav-4.png",
                "chair": [
                    "Kristi Potter"
                ],
                "organizers": [],
                "time_start": "2022-10-16T20:45:00Z",
                "time_end": "2022-10-16T22:00:00Z",
                "discord_category": "",
                "discord_channel": "oklahoma-st-4",
                "discord_channel_id": "1024600674924765264",
                "discord_link": "https://discord.com/channels/978320435554947082/1024600674924765264",
                "slido_link": "https://app.sli.do/event/i28UETZaiRWr1XwX6eKjcf?section=51658b34-0f5b-40c9-a335-1fd1468fad8d",
                "youtube_url": "https://youtu.be/vNruICFZuvw",
                "youtube_id": "vNruICFZuvw",
                "zoom_meeting": "",
                "zoom_password": "",
                "zoom_link": "",
                "ff_link": "",
                "ff_playlist": "",
                "time_slots": [
                    {
                        "slot_id": "a-ldav-prog-3",
                        "session_id": "a-ldav-4",
                        "type": "In Person Presentation",
                        "title": "In-situ analyses and visualization of large climate model data",
                        "contributors": [
                            "Nils-Arne Dreier"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "",
                        "file_name": "",
                        "time_stamp": "2022-10-16T20:45:00Z",
                        "time_start": "2022-10-16T20:45:00Z",
                        "time_end": "2022-10-16T20:55:00Z",
                        "paper_type": "",
                        "keywords": [],
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "a-ldav-prog-4",
                        "session_id": "a-ldav-4",
                        "type": "In Person Presentation",
                        "title": "Instant Neural Representation for Interactive Volume Rendering",
                        "contributors": [
                            "Qi Wu"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "",
                        "file_name": "",
                        "time_stamp": "2022-10-16T20:55:00Z",
                        "time_start": "2022-10-16T20:55:00Z",
                        "time_end": "2022-10-16T21:05:00Z",
                        "paper_type": "",
                        "keywords": [],
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "a-ldav-prog-5",
                        "session_id": "a-ldav-4",
                        "type": "In Person Presentation",
                        "title": "Metric Learning for Topological Comparisons",
                        "contributors": [
                            "Yu Qin"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "",
                        "file_name": "",
                        "time_stamp": "2022-10-16T21:05:00Z",
                        "time_start": "2022-10-16T21:05:00Z",
                        "time_end": "2022-10-16T21:15:00Z",
                        "paper_type": "",
                        "keywords": [],
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "a-ldav-prog-6",
                        "session_id": "a-ldav-4",
                        "type": "In Person Presentation",
                        "title": "In-Transit Methods for Immersive Visualization",
                        "contributors": [
                            "Isaac Nealey"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "",
                        "file_name": "",
                        "time_stamp": "2022-10-16T21:15:00Z",
                        "time_start": "2022-10-16T21:15:00Z",
                        "time_end": "2022-10-16T21:25:00Z",
                        "paper_type": "",
                        "keywords": [],
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "a-ldav-prog-7",
                        "session_id": "a-ldav-4",
                        "type": "In Person Presentation",
                        "title": "Large Image Viewer for Fast Annotation",
                        "contributors": [
                            "Cooper Maira"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "",
                        "file_name": "",
                        "time_stamp": "2022-10-16T21:25:00Z",
                        "time_start": "2022-10-16T21:25:00Z",
                        "time_end": "2022-10-16T21:35:00Z",
                        "paper_type": "",
                        "keywords": [],
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "a-ldav-prog-8",
                        "session_id": "a-ldav-4",
                        "type": "In Person Presentation",
                        "title": "Senior Expert, Data Science and Advanced Visual Analytics",
                        "contributors": [
                            "Dylan Cashman"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "",
                        "file_name": "",
                        "time_stamp": "2022-10-16T21:35:00Z",
                        "time_start": "2022-10-16T21:35:00Z",
                        "time_end": "2022-10-16T21:45:00Z",
                        "paper_type": "",
                        "keywords": [],
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "a-ldav-prog-9",
                        "session_id": "a-ldav-4",
                        "type": "In Person Other",
                        "title": "Closing Presentation + Best Paper Award",
                        "contributors": [
                            "Kristi Potter"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "",
                        "file_name": "",
                        "time_stamp": "2022-10-16T21:45:00Z",
                        "time_start": "2022-10-16T21:45:00Z",
                        "time_end": "2022-10-16T22:00:00Z",
                        "paper_type": "",
                        "keywords": [],
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    }
                ]
            }
        ]
    },
    "w-beliv": {
        "event": "BELIV: 9th Workshop on evaluation and BEyond - methodoLogIcal approaches for Visualization",
        "long_name": "BELIV: 9th Workshop on evaluation and BEyond - methodoLogIcal approaches for Visualization",
        "event_type": "Associated Event (Workshop)",
        "event_prefix": "w-beliv",
        "event_description": "",
        "event_url": "",
        "organizers": [
            "Anastasia Bezerianos",
            "Kyle Hall",
            "Samuel Huron",
            "Matthew Kay",
            "Miriah Meyer",
            "Michael Corell"
        ],
        "sessions": [
            {
                "title": "BELIV: Keynote and Q&A",
                "session_id": "w-beliv-1",
                "event_prefix": "w-beliv",
                "track": "ok4",
                "livestream_id": "ok4-mon",
                "session_image": "w-beliv-1.png",
                "chair": [
                    "Kyle Hall"
                ],
                "organizers": [],
                "time_start": "2022-10-17T14:00:00Z",
                "time_end": "2022-10-17T15:15:00Z",
                "discord_category": "",
                "discord_channel": "oklahoma-st-4",
                "discord_channel_id": "1024600674924765264",
                "discord_link": "https://discord.com/channels/978320435554947082/1024600674924765264",
                "slido_link": "https://app.sli.do/event/i28UETZaiRWr1XwX6eKjcf?section=51658b34-0f5b-40c9-a335-1fd1468fad8d",
                "youtube_url": "https://youtu.be/G-Sc7AcBkrA",
                "youtube_id": "G-Sc7AcBkrA",
                "zoom_meeting": "",
                "zoom_password": "",
                "zoom_link": "",
                "ff_link": "",
                "ff_playlist": "",
                "time_slots": [
                    {
                        "slot_id": "w-beliv-prog-1",
                        "session_id": "w-beliv-1",
                        "type": "In Person Other",
                        "title": "Opening Presentation",
                        "contributors": [
                            "Kyle Hall"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "",
                        "file_name": "",
                        "time_stamp": "2022-10-17T14:00:00Z",
                        "time_start": "2022-10-17T14:00:00Z",
                        "time_end": "2022-10-17T14:15:00Z",
                        "paper_type": "",
                        "keywords": [],
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "w-beliv-prog-2",
                        "session_id": "w-beliv-1",
                        "type": "Virtual Presentation (live)",
                        "title": "Keynote by Casey Fiesler",
                        "contributors": [
                            "Casey Fiesler"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "",
                        "file_name": "",
                        "time_stamp": "2022-10-17T14:15:00Z",
                        "time_start": "2022-10-17T14:15:00Z",
                        "time_end": "2022-10-17T15:00:00Z",
                        "paper_type": "",
                        "keywords": [],
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "w-beliv-prog-3",
                        "session_id": "w-beliv-1",
                        "type": "Virtual Q+A",
                        "title": "Keynote Q+A",
                        "contributors": [
                            "Casey Fiesler"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "",
                        "file_name": "",
                        "time_stamp": "2022-10-17T15:00:00Z",
                        "time_start": "2022-10-17T15:00:00Z",
                        "time_end": "2022-10-17T15:15:00Z",
                        "paper_type": "",
                        "keywords": [],
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    }
                ]
            },
            {
                "title": "BELIV: Paper Session 1",
                "session_id": "w-beliv-2",
                "event_prefix": "w-beliv",
                "track": "ok4",
                "livestream_id": "ok4-mon",
                "session_image": "w-beliv-2.png",
                "chair": [
                    "Michael Correll"
                ],
                "organizers": [],
                "time_start": "2022-10-17T15:45:00Z",
                "time_end": "2022-10-17T17:00:00Z",
                "discord_category": "",
                "discord_channel": "oklahoma-st-4",
                "discord_channel_id": "1024600674924765264",
                "discord_link": "https://discord.com/channels/978320435554947082/1024600674924765264",
                "slido_link": "https://app.sli.do/event/i28UETZaiRWr1XwX6eKjcf?section=51658b34-0f5b-40c9-a335-1fd1468fad8d",
                "youtube_url": "https://youtu.be/G-Sc7AcBkrA",
                "youtube_id": "G-Sc7AcBkrA",
                "zoom_meeting": "",
                "zoom_password": "",
                "zoom_link": "",
                "ff_link": "",
                "ff_playlist": "",
                "time_slots": [
                    {
                        "slot_id": "w-beliv-2070-pres",
                        "session_id": "w-beliv-2",
                        "type": "Virtual Presentation (live)",
                        "title": "A Data-Centric Methodology and Task Taxonomy for Time-Stamped Event Sequences",
                        "contributors": [
                            "Yasara Peiris"
                        ],
                        "authors": [
                            "Yasara Peiris",
                            "Clara-Maria Barth",
                            "Elaine M. Huang",
                            "J\u00fcrgen Bernard"
                        ],
                        "abstract": "Task abstractions and taxonomic structures for tasks are useful for designers of interactive data analysis approaches, serving as design targets and evaluation criteria alike. For individual data types, dataset-specific taxonomic structures capture unique data characteristics, while being generalizable across application domains. The creation of dataset-centric but domain-agnostic taxonomic structures is difficult, especially if best practices for a focused data type are still missing, observing experts is not feasible, and means for reflection and generalization are scarce. We discovered this need for methodological support when working with time-stamped event sequences, a datatype that has not yet been fully systematically studied in visualization research. To address this shortcoming, we present a methodology that enables researchers to abstract tasks and build dataset-centric taxonomic structures in five phases (data collection, coding, task categorization, task synthesis, and action-target-(criterion) crosscut). We validate the methodology by applying it to time-stamped event sequences and present a task typology that uses triples as a novel language of description for tasks: (1) action, (2) data target, and (3) data criterion. We further evaluate the descriptive power of the typology with a real-world case on cybersecurity.",
                        "uid": "w-beliv-2070",
                        "file_name": "w-beliv-2070_Peiris_Presentation.mp4",
                        "time_stamp": "2022-10-17T15:45:00Z",
                        "time_start": "2022-10-17T15:45:00Z",
                        "time_end": "2022-10-17T15:55:00Z",
                        "paper_type": "workshop",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "625",
                        "paper_award": "",
                        "image_caption": "We present a methodology comprising five phases to build dataset-specific and domain-agnostic taxonomic structures for individual data types. A variety of methods can be applied in the three early phases of data collection, coding, and task categorization. Phase four includes a task synthesis, followed by the fine-grained elaboration on action-target-(criterion) crosscuts. We validate the methodology by applying it to time-stamped event sequences and present a task typology that uses triples as a novel language of description for tasks. We further evaluate the descriptive power of the typology with a real-world case on cybersecurity.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "w-beliv-2070-qa",
                        "session_id": "w-beliv-2",
                        "type": "Virtual Q+A",
                        "title": "A Data-Centric Methodology and Task Taxonomy for Time-Stamped Event Sequences (Q+A)",
                        "contributors": [
                            "Yasara Peiris"
                        ],
                        "authors": [],
                        "abstract": "Task abstractions and taxonomic structures for tasks are useful for designers of interactive data analysis approaches, serving as design targets and evaluation criteria alike. For individual data types, dataset-specific taxonomic structures capture unique data characteristics, while being generalizable across application domains. The creation of dataset-centric but domain-agnostic taxonomic structures is difficult, especially if best practices for a focused data type are still missing, observing experts is not feasible, and means for reflection and generalization are scarce. We discovered this need for methodological support when working with time-stamped event sequences, a datatype that has not yet been fully systematically studied in visualization research. To address this shortcoming, we present a methodology that enables researchers to abstract tasks and build dataset-centric taxonomic structures in five phases (data collection, coding, task categorization, task synthesis, and action-target-(criterion) crosscut). We validate the methodology by applying it to time-stamped event sequences and present a task typology that uses triples as a novel language of description for tasks: (1) action, (2) data target, and (3) data criterion. We further evaluate the descriptive power of the typology with a real-world case on cybersecurity.",
                        "uid": "w-beliv-2070",
                        "file_name": "",
                        "time_stamp": "2022-10-17T15:55:00Z",
                        "time_start": "2022-10-17T15:55:00Z",
                        "time_end": "2022-10-17T16:00:00Z",
                        "paper_type": "workshop",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "625",
                        "paper_award": "",
                        "image_caption": "We present a methodology comprising five phases to build dataset-specific and domain-agnostic taxonomic structures for individual data types. A variety of methods can be applied in the three early phases of data collection, coding, and task categorization. Phase four includes a task synthesis, followed by the fine-grained elaboration on action-target-(criterion) crosscuts. We validate the methodology by applying it to time-stamped event sequences and present a task typology that uses triples as a novel language of description for tasks. We further evaluate the descriptive power of the typology with a real-world case on cybersecurity.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "w-beliv-4877-pres",
                        "session_id": "w-beliv-2",
                        "type": "Virtual Presentation (live)",
                        "title": "Creative Visualisation Opportunities Workshops: A Case Study in Population Health",
                        "contributors": [
                            "Mai Elshehaly"
                        ],
                        "authors": [
                            "Mai Elshehaly",
                            "Kuldeep Sohal",
                            "Tom Lawton",
                            "Maria Bryant",
                            "Mark Mon-Williams"
                        ],
                        "abstract": "Population Health Management (PHM) relies on the analysis of data from several sources to account for the complex interaction of factors that contribute to the health and well-being of a population, while considering biases and inequalities across sub-populations. Visualisation is emerging as an essential tool for insight generation from data shared and linked across services including healthcare, education, housing, policing, etc. However, visualisation design is challenged by poor data connectivity and quality, high dimensionality and complexity of real-world routinely collected data, in addition to the heterogeneity of users\u2019 backgrounds and tasks. The Creative Visualisation Opportunities (CVO) framework provides a structured approach for working with diverse communities of visualisation stakeholders and defines a set of participatory activities for the effective elicitation of requirements and visualisation design alternatives. We conducted three workshops, applying variations of the CVO framework, with over one hundred participants from the PHM domain, including clinicians, researchers, government and private sector representatives, and local communities. In this paper, we present the results of preliminary analysis of these activities and report on the perceived impact of visualisation in this domain from a stakeholders\u2019 perspective. We report real-world successes and limitations of applying the framework in different formats (through online and in-person workshops), and reflect on lessons learned for task analysis and visualisation design in the PHM domain.",
                        "uid": "w-beliv-4877",
                        "file_name": "w-beliv-4877_Elshehaly_Presentation.mp4",
                        "time_stamp": "2022-10-17T16:00:00Z",
                        "time_start": "2022-10-17T16:00:00Z",
                        "time_end": "2022-10-17T16:10:00Z",
                        "paper_type": "workshop",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "595",
                        "paper_award": "",
                        "image_caption": "\ufffdWizard of Oz\ufffd prototype used in face-to-face CVO workshop showing: (A) incidence for White British ethnicity (top row) and Asian ethnicity (bottom row) for cohort comparison (T3), (B) number of GP visits before and after diagnosis broken down by locality, and (C) age distribution at time of diagnosis. (D) Selection of a postcode area creates a parallel coordinates plot showing population pathways (T4) across services. (E) Selecting a sub-cohort of interest creates a Radial Sets [3] visualisation displaying overlap of reasons for A&E visits. (F) Education data for patients who suffer from headaches including special education needs and eligibility for free school meals. While online workshops greatly supported the wishful thinking stage of the CVO framework, face-to-face interaction when supplemented with this demo enabled participants to explore task sequences and tell data-driven stories in later stages.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "w-beliv-4877-qa",
                        "session_id": "w-beliv-2",
                        "type": "Virtual Q+A",
                        "title": "Creative Visualisation Opportunities Workshops: A Case Study in Population Health (Q+A)",
                        "contributors": [
                            "Mai Elshehaly"
                        ],
                        "authors": [],
                        "abstract": "Population Health Management (PHM) relies on the analysis of data from several sources to account for the complex interaction of factors that contribute to the health and well-being of a population, while considering biases and inequalities across sub-populations. Visualisation is emerging as an essential tool for insight generation from data shared and linked across services including healthcare, education, housing, policing, etc. However, visualisation design is challenged by poor data connectivity and quality, high dimensionality and complexity of real-world routinely collected data, in addition to the heterogeneity of users\u2019 backgrounds and tasks. The Creative Visualisation Opportunities (CVO) framework provides a structured approach for working with diverse communities of visualisation stakeholders and defines a set of participatory activities for the effective elicitation of requirements and visualisation design alternatives. We conducted three workshops, applying variations of the CVO framework, with over one hundred participants from the PHM domain, including clinicians, researchers, government and private sector representatives, and local communities. In this paper, we present the results of preliminary analysis of these activities and report on the perceived impact of visualisation in this domain from a stakeholders\u2019 perspective. We report real-world successes and limitations of applying the framework in different formats (through online and in-person workshops), and reflect on lessons learned for task analysis and visualisation design in the PHM domain.",
                        "uid": "w-beliv-4877",
                        "file_name": "",
                        "time_stamp": "2022-10-17T16:10:00Z",
                        "time_start": "2022-10-17T16:10:00Z",
                        "time_end": "2022-10-17T16:15:00Z",
                        "paper_type": "workshop",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "595",
                        "paper_award": "",
                        "image_caption": "\ufffdWizard of Oz\ufffd prototype used in face-to-face CVO workshop showing: (A) incidence for White British ethnicity (top row) and Asian ethnicity (bottom row) for cohort comparison (T3), (B) number of GP visits before and after diagnosis broken down by locality, and (C) age distribution at time of diagnosis. (D) Selection of a postcode area creates a parallel coordinates plot showing population pathways (T4) across services. (E) Selecting a sub-cohort of interest creates a Radial Sets [3] visualisation displaying overlap of reasons for A&E visits. (F) Education data for patients who suffer from headaches including special education needs and eligibility for free school meals. While online workshops greatly supported the wishful thinking stage of the CVO framework, face-to-face interaction when supplemented with this demo enabled participants to explore task sequences and tell data-driven stories in later stages.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "w-beliv-5626-pres",
                        "session_id": "w-beliv-2",
                        "type": "In Person Presentation",
                        "title": "How Do We Measure Trust in Visual Data Communication?",
                        "contributors": [
                            "Hamza Elhamdadi"
                        ],
                        "authors": [
                            "Hamza Elhamdadi",
                            "Aimen Gaba",
                            "Yea-Seul Kim",
                            "Cindy Xiong"
                        ],
                        "abstract": "Trust is fundamental to effective visual data communication between the visualization designer and the reader. Although personal experience and preference influence readers' trust in visualizations, visualization designers can leverage design techniques to create visualizations that evoke a ``calibrated trust,\" at which readers arrive after critically evaluating the information presented. To systematically understand what drives readers to engage in ``calibrated trust,\" we must first equip ourselves with reliable and valid methods for measuring trust. Computer science and data visualization researchers have not yet reached a consensus on a trust definition or metric, which are essential to building a comprehensive trust model in human-data interaction. On the other hand, social scientists and behavioral economists have developed and perfected metrics that can measure generalized and interpersonal trust, which the visualization community can reference, modify, and adapt for our needs. In this paper, we gather existing methods for evaluating trust from other disciplines and discuss how we might use them to measure, define, and model trust in data visualization research. Specifically, we discuss quantitative surveys from social sciences, trust games from behavioral economics, measuring trust through measuring belief updating, and measuring trust through perceptual methods. We assess the potential issues with these methods and consider how we can systematically apply them to visualization research.",
                        "uid": "w-beliv-5626",
                        "file_name": "",
                        "time_stamp": "2022-10-17T16:15:00Z",
                        "time_start": "2022-10-17T16:15:00Z",
                        "time_end": "2022-10-17T16:25:00Z",
                        "paper_type": "workshop",
                        "keywords": [],
                        "has_image": "0",
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "w-beliv-5626-qa",
                        "session_id": "w-beliv-2",
                        "type": "In Person Q+A",
                        "title": "How Do We Measure Trust in Visual Data Communication? (Q+A)",
                        "contributors": [
                            "Hamza Elhamdadi"
                        ],
                        "authors": [],
                        "abstract": "Trust is fundamental to effective visual data communication between the visualization designer and the reader. Although personal experience and preference influence readers' trust in visualizations, visualization designers can leverage design techniques to create visualizations that evoke a ``calibrated trust,\" at which readers arrive after critically evaluating the information presented. To systematically understand what drives readers to engage in ``calibrated trust,\" we must first equip ourselves with reliable and valid methods for measuring trust. Computer science and data visualization researchers have not yet reached a consensus on a trust definition or metric, which are essential to building a comprehensive trust model in human-data interaction. On the other hand, social scientists and behavioral economists have developed and perfected metrics that can measure generalized and interpersonal trust, which the visualization community can reference, modify, and adapt for our needs. In this paper, we gather existing methods for evaluating trust from other disciplines and discuss how we might use them to measure, define, and model trust in data visualization research. Specifically, we discuss quantitative surveys from social sciences, trust games from behavioral economics, measuring trust through measuring belief updating, and measuring trust through perceptual methods. We assess the potential issues with these methods and consider how we can systematically apply them to visualization research.",
                        "uid": "w-beliv-5626",
                        "file_name": "",
                        "time_stamp": "2022-10-17T16:25:00Z",
                        "time_start": "2022-10-17T16:25:00Z",
                        "time_end": "2022-10-17T16:30:00Z",
                        "paper_type": "workshop",
                        "keywords": [],
                        "has_image": "0",
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "w-beliv-7504-pres",
                        "session_id": "w-beliv-2",
                        "type": "Virtual Presentation (live)",
                        "title": "How Personality and Visual Channels Affect Insight Generation",
                        "contributors": [
                            "Tom\u00e1s Alves"
                        ],
                        "authors": [
                            "Tom\u00e1s Alves",
                            "Carlota Dias",
                            "Daniel Gon\u00e7alves",
                            "Joana Henriques-Calado",
                            "Sandra Gama"
                        ],
                        "abstract": "Gaining insight is considered one of the relevant purposes of visual data exploration, yet studies that categorize insights are rare. This paper reports on a study to understand if the categorization model used to describe insights and personality factors affect insight-based evaluations' findings. Participants completed a set of tasks with three hierarchical visualizations and then reported what insights they could gather from them. Results show that the insight categorization taxonomies produce different descriptions of insights based on the same corpus of responses. In addition, our findings suggest that the openness to experience trait positively influences the number of reported insights. Both these factors may create obstacles to the design of insight-based evaluations and, consequently, should be controlled in the experimental design. We discuss the study implications, lessons learned, and future work opportunities.",
                        "uid": "w-beliv-7504",
                        "file_name": "w-beliv-7504_Alves_Presentation.mp4",
                        "time_stamp": "2022-10-17T16:30:00Z",
                        "time_start": "2022-10-17T16:30:00Z",
                        "time_end": "2022-10-17T16:40:00Z",
                        "paper_type": "workshop",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "383",
                        "paper_award": "",
                        "image_caption": "Graphical abstract of the paper. We study how the insight categorization model and personality traits create obstacles in insight-based evaluations.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "w-beliv-7504-qa",
                        "session_id": "w-beliv-2",
                        "type": "Virtual Q+A",
                        "title": "How Personality and Visual Channels Affect Insight Generation (Q+A)",
                        "contributors": [
                            "Tom\u00e1s Alves"
                        ],
                        "authors": [],
                        "abstract": "Gaining insight is considered one of the relevant purposes of visual data exploration, yet studies that categorize insights are rare. This paper reports on a study to understand if the categorization model used to describe insights and personality factors affect insight-based evaluations' findings. Participants completed a set of tasks with three hierarchical visualizations and then reported what insights they could gather from them. Results show that the insight categorization taxonomies produce different descriptions of insights based on the same corpus of responses. In addition, our findings suggest that the openness to experience trait positively influences the number of reported insights. Both these factors may create obstacles to the design of insight-based evaluations and, consequently, should be controlled in the experimental design. We discuss the study implications, lessons learned, and future work opportunities.",
                        "uid": "w-beliv-7504",
                        "file_name": "",
                        "time_stamp": "2022-10-17T16:40:00Z",
                        "time_start": "2022-10-17T16:40:00Z",
                        "time_end": "2022-10-17T16:45:00Z",
                        "paper_type": "workshop",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "383",
                        "paper_award": "",
                        "image_caption": "Graphical abstract of the paper. We study how the insight categorization model and personality traits create obstacles in insight-based evaluations.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "w-beliv-4009-pres",
                        "session_id": "w-beliv-2",
                        "type": "In Person Presentation",
                        "title": "Evaluating Situated Visualization with Eye Tracking",
                        "contributors": [
                            "Kuno Kurzhals"
                        ],
                        "authors": [
                            "Kuno Kurzhals",
                            "Michael Becher",
                            "Nelusa Pathmanathan",
                            "Guido Reina"
                        ],
                        "abstract": "Augmented reality (AR) technology provides means for embedding visualization in a real-world context. Such techniques allow situated analyses of live data in their spatial domain. However, as existing techniques have to be adapted for this context and new approaches will be developed, the evaluation thereof poses new challenges for researchers. Apart from established performance measures, eye tracking has proven to be a valuable means to assess visualizations qualitatively and quantitatively. We discuss the challenges and opportunities of eye tracking for the evaluation of situated visualizations. We envision that an extension of gaze-based evaluation methodology into this field will provide new insights on how people perceive and interact with visualizations in augmented reality.",
                        "uid": "w-beliv-4009",
                        "file_name": "",
                        "time_stamp": "2022-10-17T16:45:00Z",
                        "time_start": "2022-10-17T16:45:00Z",
                        "time_end": "2022-10-17T16:55:00Z",
                        "paper_type": "workshop",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "Example of recorded eye tracking data with an augmented reality device. \nParticipants can interact with their environment in the real world while position and gaze are captured. \nThe data is later represented with heat maps in a virtual model of the scene.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "w-beliv-4009-qa",
                        "session_id": "w-beliv-2",
                        "type": "In Person Q+A",
                        "title": "Evaluating Situated Visualization with Eye Tracking (Q+A)",
                        "contributors": [
                            "Kuno Kurzhals"
                        ],
                        "authors": [],
                        "abstract": "Augmented reality (AR) technology provides means for embedding visualization in a real-world context. Such techniques allow situated analyses of live data in their spatial domain. However, as existing techniques have to be adapted for this context and new approaches will be developed, the evaluation thereof poses new challenges for researchers. Apart from established performance measures, eye tracking has proven to be a valuable means to assess visualizations qualitatively and quantitatively. We discuss the challenges and opportunities of eye tracking for the evaluation of situated visualizations. We envision that an extension of gaze-based evaluation methodology into this field will provide new insights on how people perceive and interact with visualizations in augmented reality.",
                        "uid": "w-beliv-4009",
                        "file_name": "",
                        "time_stamp": "2022-10-17T16:55:00Z",
                        "time_start": "2022-10-17T16:55:00Z",
                        "time_end": "2022-10-17T17:00:00Z",
                        "paper_type": "workshop",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "Example of recorded eye tracking data with an augmented reality device. \nParticipants can interact with their environment in the real world while position and gaze are captured. \nThe data is later represented with heat maps in a virtual model of the scene.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "",
                        "ff_id": ""
                    }
                ]
            },
            {
                "title": "BELIV: Paper Session 2",
                "session_id": "w-beliv-3",
                "event_prefix": "w-beliv",
                "track": "ok4",
                "livestream_id": "ok4-mon",
                "session_image": "w-beliv-3.png",
                "chair": [
                    "Miriah Meyer"
                ],
                "organizers": [],
                "time_start": "2022-10-17T19:00:00Z",
                "time_end": "2022-10-17T20:15:00Z",
                "discord_category": "",
                "discord_channel": "oklahoma-st-4",
                "discord_channel_id": "1024600674924765264",
                "discord_link": "https://discord.com/channels/978320435554947082/1024600674924765264",
                "slido_link": "https://app.sli.do/event/i28UETZaiRWr1XwX6eKjcf?section=51658b34-0f5b-40c9-a335-1fd1468fad8d",
                "youtube_url": "https://youtu.be/G-Sc7AcBkrA",
                "youtube_id": "G-Sc7AcBkrA",
                "zoom_meeting": "",
                "zoom_password": "",
                "zoom_link": "",
                "ff_link": "",
                "ff_playlist": "",
                "time_slots": [
                    {
                        "slot_id": "w-beliv-6205-pres",
                        "session_id": "w-beliv-3",
                        "type": "Virtual Presentation (live)",
                        "title": "Toward Inclusiveness and Accessibility in Visualization Research: Speculations on Challenges, Solution Strategies, and Calls for Action (Position Paper)",
                        "contributors": [
                            "Katrin Angerbauer"
                        ],
                        "authors": [
                            "Katrin Angerbauer",
                            "Michael Sedlmair"
                        ],
                        "abstract": "Inclusion and accessibility in visualization research have gained increasing attention over recent years. However, many challenges still remain to be solved on the road toward a more inclusive, shared-experience-driven visualization design and evaluation process. In this position paper, discuss challenges and speculate about potential solutions, based on related work, our own research, as well as personal experiences. The goal of this paper is to start discussions on the role of accessibility and inclusiveness in visualization design and evaluation.",
                        "uid": "w-beliv-6205",
                        "file_name": "",
                        "time_stamp": "2022-10-17T19:00:00Z",
                        "time_start": "2022-10-17T19:00:00Z",
                        "time_end": "2022-10-17T19:10:00Z",
                        "paper_type": "workshop",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "Keywords of the position paper shown as a caleidoskope, as multiple perspectives are needed to gain knowledge about accessibility and inclusion.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "w-beliv-6205-qa",
                        "session_id": "w-beliv-3",
                        "type": "Virtual Q+A",
                        "title": "Toward Inclusiveness and Accessibility in Visualization Research: Speculations on Challenges, Solution Strategies, and Calls for Action (Position Paper) (Q+A)",
                        "contributors": [
                            "Katrin Angerbauer"
                        ],
                        "authors": [],
                        "abstract": "Inclusion and accessibility in visualization research have gained increasing attention over recent years. However, many challenges still remain to be solved on the road toward a more inclusive, shared-experience-driven visualization design and evaluation process. In this position paper, discuss challenges and speculate about potential solutions, based on related work, our own research, as well as personal experiences. The goal of this paper is to start discussions on the role of accessibility and inclusiveness in visualization design and evaluation.",
                        "uid": "w-beliv-6205",
                        "file_name": "",
                        "time_stamp": "2022-10-17T19:10:00Z",
                        "time_start": "2022-10-17T19:10:00Z",
                        "time_end": "2022-10-17T19:15:00Z",
                        "paper_type": "workshop",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "Keywords of the position paper shown as a caleidoskope, as multiple perspectives are needed to gain knowledge about accessibility and inclusion.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "w-beliv-7497-pres",
                        "session_id": "w-beliv-3",
                        "type": "Virtual Presentation (live)",
                        "title": "Research Data Curation in Visualization",
                        "contributors": [
                            "Dimitar Garkov"
                        ],
                        "authors": [
                            "Dimitar Garkov",
                            "Christoph M\u00fcller",
                            "Matthias Braun",
                            "Daniel Weiskopf",
                            "Falk Schreiber"
                        ],
                        "abstract": "Research data curation is the act of carefully preparing research data and artifacts for sharing and long-term preservation. Research data management is centrally implemented and formally defined in a data management plan to enable data curation. In tandem, data curation and management facilitate research repeatability. In contrast to other research fields, data curation and management in visualization are not yet part of the researcher's compendium. In this position paper, we discuss the unique challenges visualization faces and propose how data curation can be practically realized. We share eight lessons learned in managing data in two large research consortia, outline the larger curation workflow, and define the typical roles. We complement our lessons with minimum criteria for selecting a suitable data repository and five challenging scenarios that occur in practice. We conclude with a vision of how the visualization research community can pave the way for new curation standards.",
                        "uid": "w-beliv-7497",
                        "file_name": "w-beliv-7497_Garkov_Presentation.mp4",
                        "time_stamp": "2022-10-17T19:15:00Z",
                        "time_start": "2022-10-17T19:15:00Z",
                        "time_end": "2022-10-17T19:25:00Z",
                        "paper_type": "workshop",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "596",
                        "paper_award": "",
                        "image_caption": "Mapping the interplay between publication, curation, and research repeatability. Each publication is characterized by its own\nresearch pipeline, where data is produced or reused. Produced data is not only any collected or generated data, but also data from the research\nactivity itself. In the curation pipeline, produced data sets are curated, ideally as these emerge, and deposited in data repositories for sharing\nand long-term preservation. The producer and depositor roles can be filled by different persons. To facilitate research repeatability,\ni.e. reproduction and replication, a two-step review ensures quality and formal policies compliance before the data set is finally stored.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "w-beliv-7497-qa",
                        "session_id": "w-beliv-3",
                        "type": "Virtual Q+A",
                        "title": "Research Data Curation in Visualization (Q+A)",
                        "contributors": [
                            "Dimitar Garkov"
                        ],
                        "authors": [],
                        "abstract": "Research data curation is the act of carefully preparing research data and artifacts for sharing and long-term preservation. Research data management is centrally implemented and formally defined in a data management plan to enable data curation. In tandem, data curation and management facilitate research repeatability. In contrast to other research fields, data curation and management in visualization are not yet part of the researcher's compendium. In this position paper, we discuss the unique challenges visualization faces and propose how data curation can be practically realized. We share eight lessons learned in managing data in two large research consortia, outline the larger curation workflow, and define the typical roles. We complement our lessons with minimum criteria for selecting a suitable data repository and five challenging scenarios that occur in practice. We conclude with a vision of how the visualization research community can pave the way for new curation standards.",
                        "uid": "w-beliv-7497",
                        "file_name": "",
                        "time_stamp": "2022-10-17T19:25:00Z",
                        "time_start": "2022-10-17T19:25:00Z",
                        "time_end": "2022-10-17T19:30:00Z",
                        "paper_type": "workshop",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "596",
                        "paper_award": "",
                        "image_caption": "Mapping the interplay between publication, curation, and research repeatability. Each publication is characterized by its own\nresearch pipeline, where data is produced or reused. Produced data is not only any collected or generated data, but also data from the research\nactivity itself. In the curation pipeline, produced data sets are curated, ideally as these emerge, and deposited in data repositories for sharing\nand long-term preservation. The producer and depositor roles can be filled by different persons. To facilitate research repeatability,\ni.e. reproduction and replication, a two-step review ensures quality and formal policies compliance before the data set is finally stored.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "w-beliv-3665-pres",
                        "session_id": "w-beliv-3",
                        "type": "Virtual Presentation (live)",
                        "title": "An Interdisciplinary Perspective on Evaluation and Experimental Design for Visual Text Analytics",
                        "contributors": [
                            "Kostiantyn Kucher"
                        ],
                        "authors": [
                            "Kostiantyn Kucher",
                            "Nicole Sultanum",
                            "Angel Daza",
                            "Vasiliki Simaki",
                            "Maria Skeppstedt",
                            "Barbara Plank",
                            "Jean-Daniel Fekete",
                            "Narges Mahyar"
                        ],
                        "abstract": "Appropriate evaluation and experimental design are fundamental for empirical sciences, particularly in data-driven fields. Due to the successes in computational modeling of languages, for instance, research outcomes are having an increasingly immediate impact on end users. As the gap in adoption by end users decreases, the need increases to ensure that tools and models developed by the research communities and practitioners are reliable, trustworthy, and supportive of the users in their goals. In this position paper, we focus on the issues of evaluating visual text analytic approaches. We take an interdisciplinary perspective from the visualization and natural language processing communities, as we argue that the design and validation of visual text analytics include concerns beyond computational or visual/interactive methods on their own. We identify four key groups of challenges for evaluating visual text analytic approaches (data ambiguity, experimental design, user trust, and big picture concerns) and provide suggestions for research opportunities from an interdisciplinary perspective.",
                        "uid": "w-beliv-3665",
                        "file_name": "w-beliv-3665_Kucher_Presentation.mp4",
                        "time_stamp": "2022-10-17T19:30:00Z",
                        "time_start": "2022-10-17T19:30:00Z",
                        "time_end": "2022-10-17T19:40:00Z",
                        "paper_type": "workshop",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "532",
                        "paper_award": "",
                        "image_caption": "Sketch of a visual text analytics design and evaluation workflow with explicit identification of validation concerns which can guide our future efforts for systematic analysis and preparation of guidelines.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "w-beliv-3665-qa",
                        "session_id": "w-beliv-3",
                        "type": "Virtual Q+A",
                        "title": "An Interdisciplinary Perspective on Evaluation and Experimental Design for Visual Text Analytics (Q+A)",
                        "contributors": [
                            "Kostiantyn Kucher"
                        ],
                        "authors": [],
                        "abstract": "Appropriate evaluation and experimental design are fundamental for empirical sciences, particularly in data-driven fields. Due to the successes in computational modeling of languages, for instance, research outcomes are having an increasingly immediate impact on end users. As the gap in adoption by end users decreases, the need increases to ensure that tools and models developed by the research communities and practitioners are reliable, trustworthy, and supportive of the users in their goals. In this position paper, we focus on the issues of evaluating visual text analytic approaches. We take an interdisciplinary perspective from the visualization and natural language processing communities, as we argue that the design and validation of visual text analytics include concerns beyond computational or visual/interactive methods on their own. We identify four key groups of challenges for evaluating visual text analytic approaches (data ambiguity, experimental design, user trust, and big picture concerns) and provide suggestions for research opportunities from an interdisciplinary perspective.",
                        "uid": "w-beliv-3665",
                        "file_name": "",
                        "time_stamp": "2022-10-17T19:40:00Z",
                        "time_start": "2022-10-17T19:40:00Z",
                        "time_end": "2022-10-17T19:45:00Z",
                        "paper_type": "workshop",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "532",
                        "paper_award": "",
                        "image_caption": "Sketch of a visual text analytics design and evaluation workflow with explicit identification of validation concerns which can guide our future efforts for systematic analysis and preparation of guidelines.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "w-beliv-3875-pres",
                        "session_id": "w-beliv-3",
                        "type": "In Person Presentation",
                        "title": "Position Paper: Are We Making Progress In Visualization Research?",
                        "contributors": [
                            "Michael Correll"
                        ],
                        "authors": [
                            "Michael Correll"
                        ],
                        "abstract": "In this work I use a survey of senior visualization researchers and thinkers to ideate about the notion of progress in visualization research: how are we growing as a field, what are we building towards, and are our existing methods sufficient to get us there? My respondents discussed several potential challenges for visualization research in terms of knowledge formation: a lack of rigor in the methods used, a lack of applicability to actual communities of practice, and a lack of theoretical structures that incorporate everything that happens to people and to data both before and after the few seconds when a viewer looks at a value in a chart. Orienting the field around progress (if such a thing is even desirable, which is another point of contention) I believe will require drastic re-conceptions of what the field is, what it values, and how it is taught.",
                        "uid": "w-beliv-3875",
                        "file_name": "",
                        "time_stamp": "2022-10-17T19:45:00Z",
                        "time_start": "2022-10-17T19:45:00Z",
                        "time_end": "2022-10-17T19:55:00Z",
                        "paper_type": "workshop",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "Three images of the solar system: a geocentric model, a geocentric model with epicycles, and, finally, the Copernican heliocentric model. Adding epicycles adds complexity and predictive power but didn't get us closer to \"the truth\" of our place in the cosmos. This is all a metaphor for visualization research.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "w-beliv-3875-qa",
                        "session_id": "w-beliv-3",
                        "type": "In Person Q+A",
                        "title": "Position Paper: Are We Making Progress In Visualization Research? (Q+A)",
                        "contributors": [
                            "Michael Correll"
                        ],
                        "authors": [],
                        "abstract": "In this work I use a survey of senior visualization researchers and thinkers to ideate about the notion of progress in visualization research: how are we growing as a field, what are we building towards, and are our existing methods sufficient to get us there? My respondents discussed several potential challenges for visualization research in terms of knowledge formation: a lack of rigor in the methods used, a lack of applicability to actual communities of practice, and a lack of theoretical structures that incorporate everything that happens to people and to data both before and after the few seconds when a viewer looks at a value in a chart. Orienting the field around progress (if such a thing is even desirable, which is another point of contention) I believe will require drastic re-conceptions of what the field is, what it values, and how it is taught.",
                        "uid": "w-beliv-3875",
                        "file_name": "",
                        "time_stamp": "2022-10-17T19:55:00Z",
                        "time_start": "2022-10-17T19:55:00Z",
                        "time_end": "2022-10-17T20:00:00Z",
                        "paper_type": "workshop",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "Three images of the solar system: a geocentric model, a geocentric model with epicycles, and, finally, the Copernican heliocentric model. Adding epicycles adds complexity and predictive power but didn't get us closer to \"the truth\" of our place in the cosmos. This is all a metaphor for visualization research.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "w-beliv-7664-pres",
                        "session_id": "w-beliv-3",
                        "type": "In Person Presentation",
                        "title": "Power Overwhelming: Quantifying the Energy Cost of Visualisation",
                        "contributors": [
                            "Christoph M\u00fcller"
                        ],
                        "authors": [
                            "Christoph M\u00fcller",
                            "Moritz Heinemann",
                            "Daniel Weiskopf",
                            "Thomas Ertl"
                        ],
                        "abstract": "GPUs are the power-hungry tool of many visualisation researchers. However, their energy consumption has mostly been investigated outside the visualisation community, albeit our algorithms can generate more complex workloads than compute kernels. Additionally, a raising number of web-based visualisations potentially makes consumers other than the GPU more relevant. We present measurement setups for quantifying the energy cost of visualisation, ranging from software sensors over external power meters and micro controller-based setups to using oscilloscopes. These setups cover energy consumption of GPUs, CPUs and other components of a computing system. Using raycasting of spherical glyphs, volume rendering and D3 visualisations as examples, we show that there are viable options for evaluating most kinds of visualisations. We conclude by stating the challenges to a broader application of these techniques and by making recommendations on how to overcome these.",
                        "uid": "w-beliv-7664",
                        "file_name": "",
                        "time_stamp": "2022-10-17T20:00:00Z",
                        "time_start": "2022-10-17T20:00:00Z",
                        "time_end": "2022-10-17T20:10:00Z",
                        "paper_type": "workshop",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "The measurement setup we used to determine the energy consumption of two scientific visualisation algorithms and of web-based visualisations.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "w-beliv-7664-qa",
                        "session_id": "w-beliv-3",
                        "type": "In Person Q+A",
                        "title": "Power Overwhelming: Quantifying the Energy Cost of Visualisation (Q+A)",
                        "contributors": [
                            "Christoph M\u00fcller"
                        ],
                        "authors": [],
                        "abstract": "GPUs are the power-hungry tool of many visualisation researchers. However, their energy consumption has mostly been investigated outside the visualisation community, albeit our algorithms can generate more complex workloads than compute kernels. Additionally, a raising number of web-based visualisations potentially makes consumers other than the GPU more relevant. We present measurement setups for quantifying the energy cost of visualisation, ranging from software sensors over external power meters and micro controller-based setups to using oscilloscopes. These setups cover energy consumption of GPUs, CPUs and other components of a computing system. Using raycasting of spherical glyphs, volume rendering and D3 visualisations as examples, we show that there are viable options for evaluating most kinds of visualisations. We conclude by stating the challenges to a broader application of these techniques and by making recommendations on how to overcome these.",
                        "uid": "w-beliv-7664",
                        "file_name": "",
                        "time_stamp": "2022-10-17T20:10:00Z",
                        "time_start": "2022-10-17T20:10:00Z",
                        "time_end": "2022-10-17T20:15:00Z",
                        "paper_type": "workshop",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "The measurement setup we used to determine the energy consumption of two scientific visualisation algorithms and of web-based visualisations.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "",
                        "ff_id": ""
                    }
                ]
            },
            {
                "title": "BELIV: Breakout Sessions",
                "session_id": "w-beliv-4",
                "event_prefix": "w-beliv",
                "track": "ok4",
                "livestream_id": "ok4-mon",
                "session_image": "w-beliv-4.png",
                "chair": [
                    "Matt Kay",
                    "Michael Correll"
                ],
                "organizers": [],
                "time_start": "2022-10-17T20:45:00Z",
                "time_end": "2022-10-17T22:00:00Z",
                "discord_category": "",
                "discord_channel": "oklahoma-st-4",
                "discord_channel_id": "1024600674924765264",
                "discord_link": "https://discord.com/channels/978320435554947082/1024600674924765264",
                "slido_link": "https://app.sli.do/event/i28UETZaiRWr1XwX6eKjcf?section=51658b34-0f5b-40c9-a335-1fd1468fad8d",
                "youtube_url": "https://youtu.be/G-Sc7AcBkrA",
                "youtube_id": "G-Sc7AcBkrA",
                "zoom_meeting": "",
                "zoom_password": "",
                "zoom_link": "",
                "ff_link": "",
                "ff_playlist": "",
                "time_slots": [
                    {
                        "slot_id": "w-beliv-prog-4",
                        "session_id": "w-beliv-4",
                        "type": "In Person Other",
                        "title": "Breakout 1: Inclusivity and Accessibility",
                        "contributors": [
                            "Matt Kay"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "",
                        "file_name": "",
                        "time_stamp": "2022-10-17T20:45:00Z",
                        "time_start": "2022-10-17T20:45:00Z",
                        "time_end": "2022-10-17T21:45:00Z",
                        "paper_type": "",
                        "keywords": [],
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "w-beliv-prog-5",
                        "session_id": "w-beliv-4",
                        "type": "In Person Other",
                        "title": "Breakout 2: The Future of VIS",
                        "contributors": [
                            "Matt Kay"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "",
                        "file_name": "",
                        "time_stamp": "2022-10-17T20:45:00Z",
                        "time_start": "2022-10-17T20:45:00Z",
                        "time_end": "2022-10-17T21:45:00Z",
                        "paper_type": "",
                        "keywords": [],
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "w-beliv-prog-6",
                        "session_id": "w-beliv-4",
                        "type": "In Person Other",
                        "title": "Breakout 3: Wildcard",
                        "contributors": [
                            "Matt Kay"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "",
                        "file_name": "",
                        "time_stamp": "2022-10-17T20:45:00Z",
                        "time_start": "2022-10-17T20:45:00Z",
                        "time_end": "2022-10-17T21:45:00Z",
                        "paper_type": "",
                        "keywords": [],
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "w-beliv-prog-7",
                        "session_id": "w-beliv-4",
                        "type": "In Person Other",
                        "title": "Closing",
                        "contributors": [
                            "Michael Correll"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "",
                        "file_name": "",
                        "time_stamp": "2022-10-17T21:45:00Z",
                        "time_start": "2022-10-17T21:45:00Z",
                        "time_end": "2022-10-17T22:00:00Z",
                        "paper_type": "",
                        "keywords": [],
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    }
                ]
            }
        ]
    },
    "a-vds": {
        "event": "VDS: Visualization in Data Science Symposium",
        "long_name": "VDS: Visualization in Data Science Symposium",
        "event_type": "Associated Event (Symposium)",
        "event_prefix": "a-vds",
        "event_description": "",
        "event_url": "",
        "organizers": [
            "Liang Gou",
            "Alvitta Ottley",
            "Anamaria Crisan"
        ],
        "sessions": [
            {
                "title": "VDS: Opening / Keynote 1 + Paper Session 1",
                "session_id": "a-vds-1",
                "event_prefix": "a-vds",
                "track": "ok5",
                "livestream_id": "ok5-sun",
                "session_image": "a-vds-1.png",
                "chair": [
                    "Liang Gou",
                    "Alvitta Ottley",
                    "Anamaria Crisan"
                ],
                "organizers": [],
                "time_start": "2022-10-16T14:00:00Z",
                "time_end": "2022-10-16T15:25:00Z",
                "discord_category": "",
                "discord_channel": "oklahoma-st-5",
                "discord_channel_id": "1024600839295356939",
                "discord_link": "https://discord.com/channels/978320435554947082/1024600839295356939",
                "slido_link": "https://app.sli.do/event/i28UETZaiRWr1XwX6eKjcf?section=d355f89b-fbd2-4abf-9958-741607905551",
                "youtube_url": "https://youtu.be/KgfU4nbKkdY",
                "youtube_id": "KgfU4nbKkdY",
                "zoom_meeting": "",
                "zoom_password": "",
                "zoom_link": "",
                "ff_link": "",
                "ff_playlist": "",
                "time_slots": [
                    {
                        "slot_id": "a-vds-prog-1",
                        "session_id": "a-vds-1",
                        "type": "In Person Presentation",
                        "title": "Opening Presentation",
                        "contributors": [
                            "Liang Gou"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "",
                        "file_name": "",
                        "time_stamp": "2022-10-16T14:00:00Z",
                        "time_start": "2022-10-16T14:00:00Z",
                        "time_end": "2022-10-16T14:05:00Z",
                        "paper_type": "",
                        "keywords": [],
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "a-vds-prog-2",
                        "session_id": "a-vds-1",
                        "type": "In Person Presentation",
                        "title": "Keynote 1 by Jean-Daniel Fekete",
                        "contributors": [
                            "Jean-Daniel Fekete"
                        ],
                        "authors": [
                            "Jean-Daniel Fekete"
                        ],
                        "abstract": "",
                        "uid": "",
                        "file_name": "Scalability with Progressive Data Science",
                        "time_stamp": "2022-10-16T14:05:00Z",
                        "time_start": "2022-10-16T14:05:00Z",
                        "time_end": "2022-10-16T14:35:00Z",
                        "paper_type": "",
                        "keywords": [],
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "a-vds-prog-3",
                        "session_id": "a-vds-1",
                        "type": "In Person Q+A",
                        "title": "Keynote 1 Q+A",
                        "contributors": [
                            "Jean-Daniel Fekete"
                        ],
                        "authors": [
                            "Jean-Daniel Fekete"
                        ],
                        "abstract": "",
                        "uid": "",
                        "file_name": "Scalability with Progressive Data Science",
                        "time_stamp": "2022-10-16T14:35:00Z",
                        "time_start": "2022-10-16T14:35:00Z",
                        "time_end": "2022-10-16T14:40:00Z",
                        "paper_type": "",
                        "keywords": [],
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "a-vds-prog-4",
                        "session_id": "a-vds-1",
                        "type": "In Person Presentation",
                        "title": "Best Paper Award Annoucement",
                        "contributors": [
                            "Alvitta Ottley"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "",
                        "file_name": "",
                        "time_stamp": "2022-10-16T14:40:00Z",
                        "time_start": "2022-10-16T14:40:00Z",
                        "time_end": "2022-10-16T14:41:00Z",
                        "paper_type": "",
                        "keywords": [],
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "a-vds-prog-5",
                        "session_id": "a-vds-1",
                        "type": "Virtual Presentation (pre-recorded)",
                        "title": "Paper 1: [Best Paper Award] PSEUDo: Interactive Pattern Search in Multivariate Time Series with Locality-Sensitive Hashing and Relevance Feedback",
                        "contributors": [
                            "Yuncong Yu"
                        ],
                        "authors": [
                            "Yuncong Yu",
                            "Dylan Kruyff",
                            "Jiao Jiao",
                            "Tim Becker",
                            "Michael Behrisch"
                        ],
                        "abstract": "",
                        "uid": "a-vds-1033",
                        "file_name": "a-vds-1033_Yu_Presentation.mp4",
                        "time_stamp": "2022-10-16T14:41:00Z",
                        "time_start": "2022-10-16T14:41:00Z",
                        "time_end": "2022-10-16T14:50:00Z",
                        "paper_type": "associated",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "498",
                        "paper_award": "",
                        "image_caption": "PSEUDo is an efficient, adaptive and interpretable tool for visual pattern retrieval in multivariate time series.\nIt makes locality-sensitive hashing trainable while maintaining its efficiency and promoting interpretability.\nIt introduced a relevance feedback mechanism to capture subjective similarity through feature selection.  \nPSEUDo is particularly efficient for very high-dimensional time series and in cases where initial labels are meager, and the promptness of the outcome counts.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "a-vds-prog-6",
                        "session_id": "a-vds-1",
                        "type": "Virtual Q+A",
                        "title": "Paper 1 Q+A",
                        "contributors": [
                            "Yuncong Yu"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "a-vds-1033",
                        "file_name": "",
                        "time_stamp": "2022-10-16T14:50:00Z",
                        "time_start": "2022-10-16T14:50:00Z",
                        "time_end": "2022-10-16T14:52:00Z",
                        "paper_type": "associated",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "498",
                        "paper_award": "",
                        "image_caption": "PSEUDo is an efficient, adaptive and interpretable tool for visual pattern retrieval in multivariate time series.\nIt makes locality-sensitive hashing trainable while maintaining its efficiency and promoting interpretability.\nIt introduced a relevance feedback mechanism to capture subjective similarity through feature selection.  \nPSEUDo is particularly efficient for very high-dimensional time series and in cases where initial labels are meager, and the promptness of the outcome counts.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "a-vds-1019-pres",
                        "session_id": "a-vds-1",
                        "type": "Virtual Presentation (pre-recorded)",
                        "title": "Paper 2: Motif-Based Visual Analysis of Dynamic Network",
                        "contributors": [
                            "Eren Cakmak"
                        ],
                        "authors": [
                            "Eren Cakmak",
                            "Johannes Fuchs",
                            "Dominik J\u00e4ckle",
                            "Tobias Schreck",
                            "Ulrik Brandes",
                            "Daniel Keim"
                        ],
                        "abstract": "",
                        "uid": "a-vds-1019",
                        "file_name": "a-vds-1019_Cakmak_Presentation.mp4",
                        "time_stamp": "2022-10-16T14:52:00Z",
                        "time_start": "2022-10-16T14:52:00Z",
                        "time_end": "2022-10-16T15:01:00Z",
                        "paper_type": "associated",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "540",
                        "paper_award": "",
                        "image_caption": "We propose two complementary scalable pixel visualizations to provide an overview of changing motif structures in large-scale dynamic networks. The network-level census pixel visualization reveals structural changes, trends, states, and outliers. The pipeline displays the steps for generating the network-level census visualization. (1) the network motif significance profile (census) is calculated for each time step, (2) the vectors are presented as a pixel-based visualization, and (3) reordering strategies are used to reveal similar network superfamilies. The reordering strategies are crucial for grouping similar network topologies to emphasize structural changes, trends, states, and outliers.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "a-vds-1019-qa",
                        "session_id": "a-vds-1",
                        "type": "Virtual Q+A",
                        "title": "Paper 2 Q+A",
                        "contributors": [
                            "Eren Cakmak"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "a-vds-1019",
                        "file_name": "",
                        "time_stamp": "2022-10-16T15:01:00Z",
                        "time_start": "2022-10-16T15:01:00Z",
                        "time_end": "2022-10-16T15:03:00Z",
                        "paper_type": "associated",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "540",
                        "paper_award": "",
                        "image_caption": "We propose two complementary scalable pixel visualizations to provide an overview of changing motif structures in large-scale dynamic networks. The network-level census pixel visualization reveals structural changes, trends, states, and outliers. The pipeline displays the steps for generating the network-level census visualization. (1) the network motif significance profile (census) is calculated for each time step, (2) the vectors are presented as a pixel-based visualization, and (3) reordering strategies are used to reveal similar network superfamilies. The reordering strategies are crucial for grouping similar network topologies to emphasize structural changes, trends, states, and outliers.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "a-vds-1025-pres",
                        "session_id": "a-vds-1",
                        "type": "Virtual Presentation (pre-recorded)",
                        "title": "Paper 3: How Do Data Scientists Communicate Intermediate Results?",
                        "contributors": [
                            "Rock Yuren Pan",
                            "Leilani Battle"
                        ],
                        "authors": [
                            "Yuren Pang",
                            "Ruotong Wang",
                            "Leilani Battle"
                        ],
                        "abstract": "",
                        "uid": "a-vds-1025",
                        "file_name": "a-vds-1025_Pang_Presentation.mp4",
                        "time_stamp": "2022-10-16T15:03:00Z",
                        "time_start": "2022-10-16T15:03:00Z",
                        "time_end": "2022-10-16T15:12:00Z",
                        "paper_type": "associated",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "603",
                        "paper_award": "",
                        "image_caption": "A scene where two data science workers collaborate to make visualizations. The image is generated by DALL-E.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "a-vds-1025-qa",
                        "session_id": "a-vds-1",
                        "type": "Virtual Q+A",
                        "title": "Paper 3 Q+A",
                        "contributors": [
                            "Rock Yuren Pan",
                            "Leilani Battle"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "a-vds-1025",
                        "file_name": "",
                        "time_stamp": "2022-10-16T15:12:00Z",
                        "time_start": "2022-10-16T15:12:00Z",
                        "time_end": "2022-10-16T15:14:00Z",
                        "paper_type": "associated",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "603",
                        "paper_award": "",
                        "image_caption": "A scene where two data science workers collaborate to make visualizations. The image is generated by DALL-E.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "a-vds-1030-pres",
                        "session_id": "a-vds-1",
                        "type": "Virtual Presentation (pre-recorded)",
                        "title": "Paper 4 BiaScope: Visual Unfairness Diagnosis for Graph Embeddings",
                        "contributors": [
                            "Agapi Rissaki"
                        ],
                        "authors": [
                            "Agapi Rissaki",
                            "Bruno Scarone",
                            "David Liu",
                            "Aditeya pandey",
                            "Brennan Klein",
                            "Tina Eliassi-Rad",
                            "Michelle Borkin"
                        ],
                        "abstract": "",
                        "uid": "a-vds-1030",
                        "file_name": "a-vds-1030_Rissaki_Presentation.mp4",
                        "time_stamp": "2022-10-16T15:14:00Z",
                        "time_start": "2022-10-16T15:14:00Z",
                        "time_end": "2022-10-16T15:23:00Z",
                        "paper_type": "associated",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "530",
                        "paper_award": "",
                        "image_caption": "BiaScope is an interactive tool for visual unfairness diagnosis for graph embeddings. The tool consists of three views:\n\n- (A) The Statistical Summary View summarizes key properties of the selected network.\n- (B) The Unfairness Comparison View is a side-by-side view that facilitates the fairness comparison of two embeddings of the same network. \n- (C) The Diagnose View shows the embedding subspace affecting the unfairness score of a selected node, together with its ego network.\n \nInteractive linking allows the user to investigate how bias appears in an embedding and how it relates to the relevant graph structure.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "a-vds-1030-qa",
                        "session_id": "a-vds-1",
                        "type": "Virtual Q+A",
                        "title": "Paper 4 Q+A",
                        "contributors": [
                            "Agapi Rissaki"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "a-vds-1030",
                        "file_name": "",
                        "time_stamp": "2022-10-16T15:23:00Z",
                        "time_start": "2022-10-16T15:23:00Z",
                        "time_end": "2022-10-16T15:25:00Z",
                        "paper_type": "associated",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "530",
                        "paper_award": "",
                        "image_caption": "BiaScope is an interactive tool for visual unfairness diagnosis for graph embeddings. The tool consists of three views:\n\n- (A) The Statistical Summary View summarizes key properties of the selected network.\n- (B) The Unfairness Comparison View is a side-by-side view that facilitates the fairness comparison of two embeddings of the same network. \n- (C) The Diagnose View shows the embedding subspace affecting the unfairness score of a selected node, together with its ego network.\n \nInteractive linking allows the user to investigate how bias appears in an embedding and how it relates to the relevant graph structure.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "",
                        "ff_id": ""
                    }
                ]
            },
            {
                "title": "VDS: Paper Session 2 + Keynote 2",
                "session_id": "a-vds-2",
                "event_prefix": "a-vds",
                "track": "ok5",
                "livestream_id": "ok5-sun",
                "session_image": "a-vds-2.png",
                "chair": [
                    "Liang Gou",
                    "Alvitta Ottley",
                    "Anamaria Crisan"
                ],
                "organizers": [],
                "time_start": "2022-10-16T15:55:00Z",
                "time_end": "2022-10-16T17:00:00Z",
                "discord_category": "",
                "discord_channel": "oklahoma-st-5",
                "discord_channel_id": "1024600839295356939",
                "discord_link": "https://discord.com/channels/978320435554947082/1024600839295356939",
                "slido_link": "https://app.sli.do/event/i28UETZaiRWr1XwX6eKjcf?section=d355f89b-fbd2-4abf-9958-741607905551",
                "youtube_url": "https://youtu.be/KgfU4nbKkdY",
                "youtube_id": "KgfU4nbKkdY",
                "zoom_meeting": "",
                "zoom_password": "",
                "zoom_link": "",
                "ff_link": "",
                "ff_playlist": "",
                "time_slots": [
                    {
                        "slot_id": "a-vds-1038-pres",
                        "session_id": "a-vds-2",
                        "type": "In Person Presentation",
                        "title": "Paper 5: [Short Paper] Comparison of Computational Notebook Systems for Interactive Visual Analytics",
                        "contributors": [
                            "Han Liu"
                        ],
                        "authors": [
                            "Han Liu",
                            "Chris North"
                        ],
                        "abstract": "",
                        "uid": "a-vds-1038",
                        "file_name": "a-vds-1038_Han_Presentation.mp4",
                        "time_stamp": "2022-10-16T15:55:00Z",
                        "time_start": "2022-10-16T15:55:00Z",
                        "time_end": "2022-10-16T16:02:00Z",
                        "paper_type": "associated",
                        "keywords": [],
                        "has_image": "0",
                        "has_video": "372",
                        "paper_award": "",
                        "image_caption": "An overview of the example Andromeda algorithm in Jupyter Notebook with three main parts: 1) sliders to adjust attribute weights, 2) dimension-reduction projection algorithm results, and 3) the resulting weights generated by the inverse projection algorithm.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "a-vds-1038-qa",
                        "session_id": "a-vds-2",
                        "type": "In Person Q+A",
                        "title": "Paper 5 Q+A",
                        "contributors": [
                            "Han Liu"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "a-vds-1038",
                        "file_name": "",
                        "time_stamp": "2022-10-16T16:02:00Z",
                        "time_start": "2022-10-16T16:02:00Z",
                        "time_end": "2022-10-16T16:03:00Z",
                        "paper_type": "associated",
                        "keywords": [],
                        "has_image": "0",
                        "has_video": "372",
                        "paper_award": "",
                        "image_caption": "An overview of the example Andromeda algorithm in Jupyter Notebook with three main parts: 1) sliders to adjust attribute weights, 2) dimension-reduction projection algorithm results, and 3) the resulting weights generated by the inverse projection algorithm.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "a-vds-1021-pres",
                        "session_id": "a-vds-2",
                        "type": "In Person Presentation",
                        "title": "Paper 6: Interactive Visualization for Data Science Scripts ",
                        "contributors": [
                            "Rebecca Faust"
                        ],
                        "authors": [
                            "Rebecca Faust",
                            "Carlos Scheidegger",
                            "Katherine Isaacs",
                            "William Bernstein",
                            "Michael Sharp",
                            "Chris North"
                        ],
                        "abstract": "",
                        "uid": "a-vds-1021",
                        "file_name": "a-vds-1021_Faust_Presentation.mp4",
                        "time_stamp": "2022-10-16T16:03:00Z",
                        "time_start": "2022-10-16T16:03:00Z",
                        "time_end": "2022-10-16T16:12:00Z",
                        "paper_type": "associated",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "553",
                        "paper_award": "",
                        "image_caption": "An example of the interactive visualizations provided by our debugging method, Anteater. Anteater traces data science scripts and computational notebooks as they execute and automatically generates interactive visualizations to enable exploration of execution data. It provides a plot of the execution hierarchy as well as plots of variable values with linking interactions. Anteater quickly illustrates the progression of execution values and enables insight into the behavior of the analysis.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "a-vds-1021-qa",
                        "session_id": "a-vds-2",
                        "type": "In Person Q+A",
                        "title": "Paper 6 Q+A",
                        "contributors": [
                            "Rebecca Faust"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "a-vds-1021",
                        "file_name": "",
                        "time_stamp": "2022-10-16T16:12:00Z",
                        "time_start": "2022-10-16T16:12:00Z",
                        "time_end": "2022-10-16T16:14:00Z",
                        "paper_type": "associated",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "553",
                        "paper_award": "",
                        "image_caption": "An example of the interactive visualizations provided by our debugging method, Anteater. Anteater traces data science scripts and computational notebooks as they execute and automatically generates interactive visualizations to enable exploration of execution data. It provides a plot of the execution hierarchy as well as plots of variable values with linking interactions. Anteater quickly illustrates the progression of execution values and enables insight into the behavior of the analysis.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "a-vds-1017-pres",
                        "session_id": "a-vds-2",
                        "type": "In Person Presentation",
                        "title": "Paper 7: Communication Analysis through Visual Analytics: Current Practices, Challenges, and New Frontie",
                        "contributors": [
                            "Maximilian T. Fischer"
                        ],
                        "authors": [
                            "Maximilian Fischer",
                            "Frederik Dennig",
                            "Daniel Seebacher",
                            "Daniel Keim",
                            "Mennatallah El-Assady"
                        ],
                        "abstract": "",
                        "uid": "a-vds-1017",
                        "file_name": "a-vds-1017_Fischer_Presentation.mp4",
                        "time_stamp": "2022-10-16T16:14:00Z",
                        "time_start": "2022-10-16T16:14:00Z",
                        "time_end": "2022-10-16T16:23:00Z",
                        "paper_type": "associated",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "540",
                        "paper_award": "",
                        "image_caption": "Survey of communication analysis system summarizing the approaches properties in the four main dimensions (1) Input: Data and Information, (2) Processing and Models, (3) Visual Interface, and (4) Knowledge Generation. The survey is used to motivate a conceptual framework of communication analysis through visual analytics, which is further based on communication research and technical considerations.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "a-vds-1017-qa",
                        "session_id": "a-vds-2",
                        "type": "In Person Q+A",
                        "title": "Paper 7 Q+A",
                        "contributors": [
                            "Maximilian T. Fischer"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "a-vds-1017",
                        "file_name": "",
                        "time_stamp": "2022-10-16T16:23:00Z",
                        "time_start": "2022-10-16T16:23:00Z",
                        "time_end": "2022-10-16T16:25:00Z",
                        "paper_type": "associated",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "540",
                        "paper_award": "",
                        "image_caption": "Survey of communication analysis system summarizing the approaches properties in the four main dimensions (1) Input: Data and Information, (2) Processing and Models, (3) Visual Interface, and (4) Knowledge Generation. The survey is used to motivate a conceptual framework of communication analysis through visual analytics, which is further based on communication research and technical considerations.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "a-vds-prog-7",
                        "session_id": "a-vds-2",
                        "type": "In Person Presentation",
                        "title": "Keynote 2 by Remco Chang",
                        "contributors": [
                            "Remco Chang"
                        ],
                        "authors": [
                            "Remco Chang"
                        ],
                        "abstract": "",
                        "uid": "",
                        "file_name": "Towards a Unifying Theory of Data, Task, and Visualization with a Grammar of Hypothesis",
                        "time_stamp": "2022-10-16T16:25:00Z",
                        "time_start": "2022-10-16T16:25:00Z",
                        "time_end": "2022-10-16T16:55:00Z",
                        "paper_type": "",
                        "keywords": [],
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "a-vds-prog-8",
                        "session_id": "a-vds-2",
                        "type": "In Person Q+A",
                        "title": "Keynote 2 Q+A",
                        "contributors": [
                            "Remco Chang"
                        ],
                        "authors": [
                            "Remco Chang"
                        ],
                        "abstract": "",
                        "uid": "",
                        "file_name": "Towards a Unifying Theory of Data, Task, and Visualization with a Grammar of Hypothesis",
                        "time_stamp": "2022-10-16T16:55:00Z",
                        "time_start": "2022-10-16T16:55:00Z",
                        "time_end": "2022-10-16T16:59:00Z",
                        "paper_type": "",
                        "keywords": [],
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "a-vds-prog-9",
                        "session_id": "a-vds-2",
                        "type": "In Person Presentation",
                        "title": "Closing",
                        "contributors": [
                            "Alvitta Ottley"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "",
                        "file_name": "",
                        "time_stamp": "2022-10-16T16:59:00Z",
                        "time_start": "2022-10-16T16:59:00Z",
                        "time_end": "2022-10-16T17:00:00Z",
                        "paper_type": "",
                        "keywords": [],
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    }
                ]
            }
        ]
    },
    "a-biomedvischallenge": {
        "event": "Bio+MedVis Challenges",
        "long_name": "Bio+MedVis Challenges",
        "event_type": "Associated Event (Competition)",
        "event_prefix": "a-biomedvischallenge",
        "event_description": "",
        "event_url": "",
        "organizers": [
            "Thomas H\u00f6llt",
            "Zeynep Gumus",
            "Daniel J\u00f6nsson",
            "Renata Raidou"
        ],
        "sessions": [
            {
                "title": "Bio+MedVis: Challenges",
                "session_id": "a-biomedvischallenge-1",
                "event_prefix": "a-biomedvischallenge",
                "track": "ok5",
                "livestream_id": "ok5-sun",
                "session_image": "a-biomedvischallenge-1.png",
                "chair": [
                    "Thomas H\u00f6llt",
                    "Zeynep Gumus",
                    "Daniel J\u00f6nsson",
                    "Renata Raidou"
                ],
                "organizers": [],
                "time_start": "2022-10-16T19:00:00Z",
                "time_end": "2022-10-16T20:15:00Z",
                "discord_category": "",
                "discord_channel": "oklahoma-st-5",
                "discord_channel_id": "1024600839295356939",
                "discord_link": "https://discord.com/channels/978320435554947082/1024600839295356939",
                "slido_link": "https://app.sli.do/event/i28UETZaiRWr1XwX6eKjcf?section=d355f89b-fbd2-4abf-9958-741607905551",
                "youtube_url": "https://youtu.be/KgfU4nbKkdY",
                "youtube_id": "KgfU4nbKkdY",
                "zoom_meeting": "",
                "zoom_password": "",
                "zoom_link": "",
                "ff_link": "",
                "ff_playlist": "",
                "time_slots": [
                    {
                        "slot_id": "a-biomedvischallenge-prog-1",
                        "session_id": "a-biomedvischallenge-1",
                        "type": "In Person Presentation",
                        "title": "Opening Presentation",
                        "contributors": [
                            "Daniel J\u00f6nsson"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "",
                        "file_name": "",
                        "time_stamp": "2022-10-16T19:00:00Z",
                        "time_start": "2022-10-16T19:00:00Z",
                        "time_end": "2022-10-16T19:05:00Z",
                        "paper_type": "",
                        "keywords": [],
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "a-biomedvischallenge-prog-2",
                        "session_id": "a-biomedvischallenge-1",
                        "type": "In Person Presentation",
                        "title": "Challenge Introduction: Deciphering Protein Modifications with the Power of Visualization",
                        "contributors": [
                            "Enrico Massignani"
                        ],
                        "authors": [
                            "Enrico Massignani"
                        ],
                        "abstract": "",
                        "uid": "",
                        "file_name": "",
                        "time_stamp": "2022-10-16T19:05:00Z",
                        "time_start": "2022-10-16T19:05:00Z",
                        "time_end": "2022-10-16T19:20:00Z",
                        "paper_type": "",
                        "keywords": [],
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "a-biomedvischallenge-prog-3",
                        "session_id": "a-biomedvischallenge-1",
                        "type": "In Person Q+A",
                        "title": "Challenge Introduction: Deciphering Protein Modifications with the Power of Visualization (Q+A)",
                        "contributors": [
                            "Enrico Massignani"
                        ],
                        "authors": [
                            "Enrico Massignani"
                        ],
                        "abstract": "",
                        "uid": "",
                        "file_name": "",
                        "time_stamp": "2022-10-16T19:20:00Z",
                        "time_start": "2022-10-16T19:20:00Z",
                        "time_end": "2022-10-16T19:25:00Z",
                        "paper_type": "",
                        "keywords": [],
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "a-biomedvischallenge-1004-pres",
                        "session_id": "a-biomedvischallenge-1",
                        "type": "In Person Presentation",
                        "title": "EProM - Exploration of Protein Modifications",
                        "contributors": [
                            "Jannes Peeters"
                        ],
                        "authors": [
                            "Dani\u00ebl M. Bot",
                            "Jannes Peeters",
                            "Jan Aerts"
                        ],
                        "abstract": "",
                        "uid": "a-biovischallenge-posters-1004",
                        "file_name": "a-biovischallenge-1004_Bot_Presentation.mp4",
                        "time_stamp": "2022-10-16T19:25:00Z",
                        "time_start": "2022-10-16T19:25:00Z",
                        "time_end": "2022-10-16T19:30:00Z",
                        "paper_type": "",
                        "keywords": [],
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "a-biomedvischallenge-1005-pres",
                        "session_id": "a-biomedvischallenge-1",
                        "type": "Virtual Presentation (pre-recorded)",
                        "title": "Mouse-Human Hybrids",
                        "contributors": [
                            "Hauke Bartsch"
                        ],
                        "authors": [
                            "Hauke Bartsch",
                            "Laura Garrison",
                            "Stefan Bruckner"
                        ],
                        "abstract": "",
                        "uid": "a-biovischallenge-posters-1005",
                        "file_name": "a-biovischallenge-1005_Bartsch_Presentation.mp4",
                        "time_stamp": "2022-10-16T19:30:00Z",
                        "time_start": "2022-10-16T19:30:00Z",
                        "time_end": "2022-10-16T19:35:00Z",
                        "paper_type": "",
                        "keywords": [],
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "a-biomedvischallenge-1001-pres",
                        "session_id": "a-biomedvischallenge-1",
                        "type": "In Person Presentation",
                        "title": "ANNOTED: Taming Protein Beasts Through Interactive Visualization",
                        "contributors": [
                            "Aaron Watters"
                        ],
                        "authors": [
                            "Aaron Watters",
                            "Vikram Mulligan"
                        ],
                        "abstract": "ANNOTED is an interactive visualization designed to address the requirements of the Bio+MedViz Challenge 2022. Briefly, it is intended to permit visualization of proteins involved in disease, and to allow users to explore sites at which amino acid residues are modified in potentially disease-relevant ways. It is published as a public web application at https://aaronwatters.github.io/bioviz2022/challenge.html with open source code available at https://github.com/AaronWatters/bioviz2022 as a Github repository. ",
                        "uid": "a-biovischallenge-1001",
                        "file_name": "a-biovischallenge-1001_Watters_Presentation.mp4",
                        "time_stamp": "2022-10-16T19:35:00Z",
                        "time_start": "2022-10-16T19:35:00Z",
                        "time_end": "2022-10-16T19:40:00Z",
                        "paper_type": "associated",
                        "keywords": [],
                        "has_image": "0",
                        "has_video": "231",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "a-biomedvischallenge-1002-pres",
                        "session_id": "a-biomedvischallenge-1",
                        "type": "Virtual Presentation (pre-recorded)",
                        "title": "A simple pipeline for creating cover quality images of post-translational modifications to molecular structures in rare disease",
                        "contributors": [
                            "Mo Rahman"
                        ],
                        "authors": [
                            "Mo Rahman",
                            "Onno Faber",
                            "Pascale Marill",
                            "Otavio Good"
                        ],
                        "abstract": "Changes to molecular structure can have a large impact on biological function and are the basis for many diseases. Structural modifications to proteins, while not changes to the amino acid sequence, can alter their function and have been implicated in rare diseases. Visualizing these modifications on a 3D model helps understand how the characteristics of a molecule are affected. We present a simple pipeline to create high-quality rendered images of post-translational protein modifications. Data provided by CompOmics group at VIB and Ghent University in Ghent, Belgium is parsed using standard UNIX commands, then added to AlphaFold predicted structures using a newly developed viewer Dalton, and finally rendered to an image. Well-designed and simple to make visualizations demystify rare diseases and mechanisms of post-translational structural modifications, making it easier to conduct further research in these fields.",
                        "uid": "a-biovischallenge-1002",
                        "file_name": "a-biovischallenge-1002_Rahman_Presentation.mp4",
                        "time_stamp": "2022-10-16T19:40:00Z",
                        "time_start": "2022-10-16T19:40:00Z",
                        "time_end": "2022-10-16T19:45:00Z",
                        "paper_type": "associated",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "301",
                        "paper_award": "",
                        "image_caption": "Three-step pipeline begins with parsing tab-delimited file of modified residues and type for three proteins \nimportant in rare diseases. UNIX Bash is used for its adaptability, as files with modifications loci may be \ndifferent across projects. In step two modification data and AlphaFold predictions are loaded in Dalton a novel \nmolecular viewer. Here we are able to edit the structure and add atoms or residues, within a real-time rendered \nenvironment. The last step loads the 3D model into rendering software of user choice, iIn the final image we see \nTGFB1 implicated in Camurati-Engelmann disease, with its pathogenic modifications.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "a-biomedvischallenge-1003-pres",
                        "session_id": "a-biomedvischallenge-1",
                        "type": "Virtual Presentation (pre-recorded)",
                        "title": "ProtoFold Neighborhood Inspector",
                        "contributors": [
                            "Nicolas F. Chaves-de-Plaza"
                        ],
                        "authors": [
                            "Nicolas F. Chaves-de-Plaza",
                            "Klaus Hildebrandt",
                            "Anna Vilanova"
                        ],
                        "abstract": "Post-translational modifications (PTMs) affecting a protein's residues (amino acids) can disturb its function, leading to illness. Whether or not a PTM is pathogenic depends on its type and the status of neighboring residues. In this paper, we present the ProtoFold Neighborhood Inspector (PFNI), a visualization system for analyzing residues neighborhoods. The main contribution is a visualization idiom, the Residue Constellation (RC), for identifying and comparing three-dimensional neighborhoods based on per-residue features and spatial characteristics. The RC leverages two-dimensional representations of the protein's three-dimensional structure to overcome problems like occlusion, easing the analysis of neighborhoods that often have complicated spatial arrangements. Using the PFNI, we explored proteins\u2019 structural PTM data, which allowed us to identify patterns in the distribution and quantity of per-neighborhood PTMs that might be related to their pathogenic status. In the following, we define the tasks that guided the development of the PFNI and describe the data sources we derived and used. Then, we introduce the PFNI and illustrate its usage through an example of an analysis workflow. We conclude by reflecting on preliminary findings obtained while using the tool on the provided data and future directions concerning the development of the PFNI.",
                        "uid": "a-biovischallenge-1003",
                        "file_name": "a-biovischallenge-1003_Plaza_Presentation.mp4",
                        "time_stamp": "2022-10-16T19:45:00Z",
                        "time_start": "2022-10-16T19:45:00Z",
                        "time_end": "2022-10-16T19:50:00Z",
                        "paper_type": "associated",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "303",
                        "paper_award": "",
                        "image_caption": "User interface of the ProtoFold Neighborhood Inspector. With the Residue Constellation, users can select, inspect and analyze neighborhoods in protein structures. The figure shows an example of the analysis workflow for the transforming growth factor beta-1 proprotein (HUMAN). The user selected all residues containing a pathogenic modification using the bulk selection widget. The neighborhood summarization glyphs in the center of the residue constellation permit comparing the neighborhoods of these residues.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "a-biomedvischallenge-prog-4",
                        "session_id": "a-biomedvischallenge-1",
                        "type": "In Person Presentation",
                        "title": "Re-design challenge introduction",
                        "contributors": [
                            "Enrico Massignani"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "",
                        "file_name": "",
                        "time_stamp": "2022-10-16T19:50:00Z",
                        "time_start": "2022-10-16T19:50:00Z",
                        "time_end": "2022-10-16T19:55:00Z",
                        "paper_type": "",
                        "keywords": [],
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "a-biomedvischallenge-1006-pres",
                        "session_id": "a-biomedvischallenge-1",
                        "type": "In Person Presentation",
                        "title": "Modie Viewer: Protein Beasts and How to View Them",
                        "contributors": [
                            "Huyen N. Nguyen"
                        ],
                        "authors": [
                            "Huyen N. Nguyen",
                            "Caleb Trujillo",
                            "Tommy Dang"
                        ],
                        "abstract": "",
                        "uid": "a-biovischallenge-posters-1006",
                        "file_name": "a-biovischallenge-1006_Nguyen_Presentation.mp4",
                        "time_stamp": "2022-10-16T19:55:00Z",
                        "time_start": "2022-10-16T19:55:00Z",
                        "time_end": "2022-10-16T20:00:00Z",
                        "paper_type": "",
                        "keywords": [],
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "a-biomedvischallenge-1004-pres",
                        "session_id": "a-biomedvischallenge-1",
                        "type": "In Person Presentation",
                        "title": "(Re-design) EProM - Exploration of Protein Modifications ",
                        "contributors": [
                            "Dani\u00ebl M. Bot"
                        ],
                        "authors": [
                            "Dani\u00ebl M. Bot",
                            "Jannes Peeters",
                            "Jan Aerts"
                        ],
                        "abstract": "We present EProM\u2014a visual analysis interface for the exploration of protein modifications\u2014as a contribution to the IEEE VIS 2022 Bio+MedVis Challenge. The interface targets researchers in bio-chemistry, proteomics, and precision medicine as its primary users. Observed modifications can be inspected from the protein\u2019s primary, secondary, and tertiary structure, using a straightforward design and intuitive interactions. Modifications\u2019 measurement uncertainty and relation to residues with identified pathogenic mutations are considered.",
                        "uid": "a-biovischallenge-1004",
                        "file_name": "a-biovischallenge-1004_Bot_Presentation.mp4",
                        "time_stamp": "2022-10-16T20:00:00Z",
                        "time_start": "2022-10-16T20:00:00Z",
                        "time_end": "2022-10-16T20:05:00Z",
                        "paper_type": "associated",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "547",
                        "paper_award": "",
                        "image_caption": "EProM - Exploration of Protein Modifications - is an interactive interface \ndeveloped by Dani\u00ebl M. Bot, Jannes Peeters and Jan Aerts as submission for the \n2022 Bio+MedVis Challenge.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "a-biomedvischallenge-1008-pres",
                        "session_id": "a-biomedvischallenge-1",
                        "type": "In Person Presentation",
                        "title": "Visualizing Protein Residue Chemical Modifications",
                        "contributors": [
                            "Laura Garrison"
                        ],
                        "authors": [
                            "Laura Garrison",
                            "Hauke Bartsch",
                            "Stefan Bruckner"
                        ],
                        "abstract": "",
                        "uid": "a-biovischallenge-posters-1008",
                        "file_name": "a-biovischallenge-1008_Garrison_Presentation.mp4",
                        "time_stamp": "2022-10-16T20:05:00Z",
                        "time_start": "2022-10-16T20:05:00Z",
                        "time_end": "2022-10-16T20:10:00Z",
                        "paper_type": "",
                        "keywords": [],
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "a-biomedvischallenge-1007-pres",
                        "session_id": "a-biomedvischallenge-1",
                        "type": "Virtual Presentation (pre-recorded)",
                        "title": "A Dashboard for the Visualisation of a Protein\u2019s PTMs",
                        "contributors": [
                            "Simon Hackl"
                        ],
                        "authors": [
                            "Simon Hackl",
                            "Theresa Harbig",
                            "Caroline Jachmann",
                            "Mathias Witte Paz",
                            "Kay Nieselt"
                        ],
                        "abstract": "In our submission we address the redesign task of the Bio+MedVis Challenge 2022. The existing visualization shows all post-translational modifications (PTMs) identified on a protein as circles that are connected to their position on the protein\u2019s primary sequence by a line. Overall, the figure communicates well that there can be significantly more PTMs on proteins than anticipated. However, the visualization suffers from overplotting in multiple dimensions, rendering a detailed interpretation impossible. First, on the x-axis adjacent positions may carry PTMs, which causes the circles to overlap. Second, on the y-axis the PTMs for a position are stacked. From the visualizations, it is not clear if some PTMs are not displayed due to space limitations. Moreover, there are more different PTMs than there are colors that can be differentiated and a legend is missing. Most importantly, the visualization is based only on the primary sequence. No (visual) relation between PTMs and the protein structure can be established.",
                        "uid": "a-biovischallenge-1007",
                        "file_name": "a-biovischallenge-1007_Hackl_Presentation.mp4",
                        "time_stamp": "2022-10-16T20:10:00Z",
                        "time_start": "2022-10-16T20:10:00Z",
                        "time_end": "2022-10-16T20:15:00Z",
                        "paper_type": "associated",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "272",
                        "paper_award": "",
                        "image_caption": "The proposed visualization of the PTMs applied to the human ALDOA protein. It consists of two main components: a contact map and a presence-absence heat map. The contact map shows the residues of close proximity (at most 6 Angstrom) where color encodes how many PTMS two residues in contact share. The presence-absence heat map shows the PTM composition of the protein along the primary sequence. Finally, we have included bar charts to summarise the number of PTMs by positions and single-axis traces that visually encode the secondary structure of each residue. All these components are part of an interactive dashboard.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    }
                ]
            },
            {
                "title": "Bio+MedVis: Invited Talks",
                "session_id": "a-biomedvischallenge-2",
                "event_prefix": "a-biomedvischallenge",
                "track": "ok5",
                "livestream_id": "ok5-sun",
                "session_image": "a-biomedvischallenge-2.png",
                "chair": [
                    "Thomas H\u00f6llt",
                    "Zeynep Gumus",
                    "Daniel J\u00f6nsson",
                    "Renata Raidou"
                ],
                "organizers": [],
                "time_start": "2022-10-16T20:45:00Z",
                "time_end": "2022-10-16T22:00:00Z",
                "discord_category": "",
                "discord_channel": "oklahoma-st-5",
                "discord_channel_id": "1024600839295356939",
                "discord_link": "https://discord.com/channels/978320435554947082/1024600839295356939",
                "slido_link": "https://app.sli.do/event/i28UETZaiRWr1XwX6eKjcf?section=d355f89b-fbd2-4abf-9958-741607905551",
                "youtube_url": "https://youtu.be/KgfU4nbKkdY",
                "youtube_id": "KgfU4nbKkdY",
                "zoom_meeting": "",
                "zoom_password": "",
                "zoom_link": "",
                "ff_link": "",
                "ff_playlist": "",
                "time_slots": [
                    {
                        "slot_id": "a-biomedvischallenge-prog-5",
                        "session_id": "a-biomedvischallenge-2",
                        "type": "Virtual Presentation (live)",
                        "title": "Visual Analysis of Large-scale Biological Data to Understand Microbial Threats",
                        "contributors": [
                            "Haichao Miao"
                        ],
                        "authors": [
                            "Haichao Miao"
                        ],
                        "abstract": "",
                        "uid": "",
                        "file_name": "",
                        "time_stamp": "2022-10-16T20:45:00Z",
                        "time_start": "2022-10-16T20:45:00Z",
                        "time_end": "2022-10-16T21:00:00Z",
                        "paper_type": "",
                        "keywords": [],
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "a-biomedvischallenge-prog-6",
                        "session_id": "a-biomedvischallenge-2",
                        "type": "Virtual Q+A",
                        "title": "Q+A Haichao Miao",
                        "contributors": [
                            "Haichao Miao"
                        ],
                        "authors": [
                            "Haichao Miao"
                        ],
                        "abstract": "",
                        "uid": "",
                        "file_name": "",
                        "time_stamp": "2022-10-16T21:00:00Z",
                        "time_start": "2022-10-16T21:00:00Z",
                        "time_end": "2022-10-16T21:05:00Z",
                        "paper_type": "",
                        "keywords": [],
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "a-biomedvischallenge-prog-7",
                        "session_id": "a-biomedvischallenge-2",
                        "type": "In Person Presentation",
                        "title": "A Heatmap for Enzyme Engineering: Analysis of Residue-by-Residue Contributions to the Free Energy Barrier of Enzyme Reactions",
                        "contributors": [
                            "Yihan Shao"
                        ],
                        "authors": [
                            "Yihan Shao"
                        ],
                        "abstract": "",
                        "uid": "",
                        "file_name": "",
                        "time_stamp": "2022-10-16T21:05:00Z",
                        "time_start": "2022-10-16T21:05:00Z",
                        "time_end": "2022-10-16T21:20:00Z",
                        "paper_type": "",
                        "keywords": [],
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "a-biomedvischallenge-prog-8",
                        "session_id": "a-biomedvischallenge-2",
                        "type": "In Person Q+A",
                        "title": "Q+A Yihan Shao",
                        "contributors": [
                            "Yihan Shao"
                        ],
                        "authors": [
                            "Yihan Shao"
                        ],
                        "abstract": "",
                        "uid": "",
                        "file_name": "",
                        "time_stamp": "2022-10-16T21:20:00Z",
                        "time_start": "2022-10-16T21:20:00Z",
                        "time_end": "2022-10-16T21:25:00Z",
                        "paper_type": "",
                        "keywords": [],
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "a-biomedvischallenge-prog-9",
                        "session_id": "a-biomedvischallenge-2",
                        "type": "In Person Presentation",
                        "title": "Capstone: Bio+MedVis: Current and future opportunities",
                        "contributors": [
                            "Barbara Kozlikova"
                        ],
                        "authors": [
                            "Barbara Kozlikova"
                        ],
                        "abstract": "",
                        "uid": "",
                        "file_name": "",
                        "time_stamp": "2022-10-16T21:25:00Z",
                        "time_start": "2022-10-16T21:25:00Z",
                        "time_end": "2022-10-16T21:50:00Z",
                        "paper_type": "",
                        "keywords": [],
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "a-biomedvischallenge-prog-10",
                        "session_id": "a-biomedvischallenge-2",
                        "type": "In Person Q+A",
                        "title": "Q+A Barbara Kozlikova",
                        "contributors": [
                            "Barbara Kozlikova"
                        ],
                        "authors": [
                            "Barbara Kozlikova"
                        ],
                        "abstract": "",
                        "uid": "",
                        "file_name": "",
                        "time_stamp": "2022-10-16T21:50:00Z",
                        "time_start": "2022-10-16T21:50:00Z",
                        "time_end": "2022-10-16T21:55:00Z",
                        "paper_type": "",
                        "keywords": [],
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "a-biomedvischallenge-prog-11",
                        "session_id": "a-biomedvischallenge-2",
                        "type": "In Person Presentation",
                        "title": "Closing",
                        "contributors": [
                            "Daniel J\u00f6nsson"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "",
                        "file_name": "",
                        "time_stamp": "2022-10-16T21:55:00Z",
                        "time_start": "2022-10-16T21:55:00Z",
                        "time_end": "2022-10-16T22:00:00Z",
                        "paper_type": "",
                        "keywords": [],
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    }
                ]
            }
        ]
    },
    "w-biomedicalai": {
        "event": "Workshop on Visualization in BioMedical AI",
        "long_name": "Workshop on Visualization in BioMedical AI",
        "event_type": "Workshop",
        "event_prefix": "w-biomedicalai",
        "event_description": "",
        "event_url": "",
        "organizers": [
            "Qianwen Wang",
            "Vicky Yao",
            "Bum Chul Kwon",
            "Nils Gehlenborg"
        ],
        "sessions": [
            {
                "title": "BioMedical AI: Keynote and Session 1",
                "session_id": "w-biomedicalai-1",
                "event_prefix": "w-biomedicalai",
                "track": "ok5",
                "livestream_id": "ok5-mon",
                "session_image": "w-biomedicalai-1.png",
                "chair": [
                    "Qianwen Wang",
                    "Vicky Yao",
                    "Bum Chul Kwon",
                    "Nils Gehlenborg"
                ],
                "organizers": [],
                "time_start": "2022-10-17T14:00:00Z",
                "time_end": "2022-10-17T15:15:00Z",
                "discord_category": "",
                "discord_channel": "oklahoma-st-5",
                "discord_channel_id": "1024600839295356939",
                "discord_link": "https://discord.com/channels/978320435554947082/1024600839295356939",
                "slido_link": "https://app.sli.do/event/i28UETZaiRWr1XwX6eKjcf?section=d355f89b-fbd2-4abf-9958-741607905551",
                "youtube_url": "https://youtu.be/V3jND9JjgXU",
                "youtube_id": "V3jND9JjgXU",
                "zoom_meeting": "",
                "zoom_password": "",
                "zoom_link": "",
                "ff_link": "",
                "ff_playlist": "",
                "time_slots": [
                    {
                        "slot_id": "w-biomedicalai-prog-1",
                        "session_id": "w-biomedicalai-1",
                        "type": "Virtual Presentation (live)",
                        "title": "Opening",
                        "contributors": [
                            "Nils Gehlenborg",
                            "Vicky Yao"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "",
                        "file_name": "",
                        "time_stamp": "2022-10-17T14:00:00Z",
                        "time_start": "2022-10-17T14:00:00Z",
                        "time_end": "2022-10-17T14:05:00Z",
                        "paper_type": "",
                        "keywords": [],
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "w-biomedicalai-prog-2",
                        "session_id": "w-biomedicalai-1",
                        "type": "Virtual Presentation (live)",
                        "title": "Human-AI collaboration in Medicine",
                        "contributors": [
                            "Carrie Cai"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "",
                        "file_name": "",
                        "time_stamp": "2022-10-17T14:05:00Z",
                        "time_start": "2022-10-17T14:05:00Z",
                        "time_end": "2022-10-17T14:55:00Z",
                        "paper_type": "",
                        "keywords": [],
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "w-biomedicalai-1053-pres",
                        "session_id": "w-biomedicalai-1",
                        "type": "Virtual Presentation (pre-recorded)",
                        "title": "Tightening the Loop in Mixed-Initiative ML Engineering and Domain Annotation using Active Learning and Visual Analytics",
                        "contributors": [
                            "Mert Erkul"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "w-biomedicalai-1053",
                        "file_name": "w-biomedicalai-1053_Erkul_Presentation.mp4",
                        "time_stamp": "2022-10-17T14:55:00Z",
                        "time_start": "2022-10-17T14:55:00Z",
                        "time_end": "2022-10-17T15:02:00Z",
                        "paper_type": "workshop",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "431",
                        "paper_award": "",
                        "image_caption": "This figure illustrates the dashboard workflow. The data annotation loop involves domain experts who annotate the data through frontend and receive feedback after the backend machine learning (ML) engine processes, which data are most important to annotate next. The model verification loop involves ML experts, who configure model parameters based on the already-labelled data. The two loops are tightened by constant feedback from one another: domain experts make more annotated data available and learn which further data is needed, while ML experts learn about how the model is currently performing based on the available data and configure the model accordingly.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "w-biomedicalai-1053-qa",
                        "session_id": "w-biomedicalai-1",
                        "type": "Virtual Q+A",
                        "title": "Tightening the Loop in Mixed-Initiative ML Engineering and Domain Annotation using Active Learning and Visual Analytics (Q+A)",
                        "contributors": [
                            "Mert Erkul"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "w-biomedicalai-1053",
                        "file_name": "",
                        "time_stamp": "2022-10-17T15:02:00Z",
                        "time_start": "2022-10-17T15:02:00Z",
                        "time_end": "2022-10-17T15:05:00Z",
                        "paper_type": "workshop",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "431",
                        "paper_award": "",
                        "image_caption": "This figure illustrates the dashboard workflow. The data annotation loop involves domain experts who annotate the data through frontend and receive feedback after the backend machine learning (ML) engine processes, which data are most important to annotate next. The model verification loop involves ML experts, who configure model parameters based on the already-labelled data. The two loops are tightened by constant feedback from one another: domain experts make more annotated data available and learn which further data is needed, while ML experts learn about how the model is currently performing based on the available data and configure the model accordingly.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "w-biomedicalai-7627-pres",
                        "session_id": "w-biomedicalai-1",
                        "type": "Virtual Presentation (pre-recorded)",
                        "title": "Opening Access to Visual Exploration of Audiovisual Digital Biomarkers: an OpenDBM Analytics Tool",
                        "contributors": [
                            "Carla Floricel"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "w-biomedicalai-7627",
                        "file_name": "w-biomedicalai-7627_Floricel_Presentation.mp4",
                        "time_stamp": "2022-10-17T15:05:00Z",
                        "time_start": "2022-10-17T15:05:00Z",
                        "time_end": "2022-10-17T15:12:00Z",
                        "paper_type": "workshop",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "481",
                        "paper_award": "",
                        "image_caption": "We propose an open source visualization interface for audiovisual digital biomarker analysis. \nOpenDBM is an open source project that produces derived and raw digital biomarker variables from audiovisual sources.\nThese digital biomarker variables are split into four categories: speech, verbal acoustics, head movement, and facial activity.\nOur tool visualizes derived variables from video cohorts using the Cohort Panel and raw variables from one video using the Individual Panel.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "w-biomedicalai-7627-qa",
                        "session_id": "w-biomedicalai-1",
                        "type": "Virtual Q+A",
                        "title": "Opening Access to Visual Exploration of Audiovisual Digital Biomarkers: an OpenDBM Analytics Tool (Q+A)",
                        "contributors": [
                            "Carla Floricel"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "w-biomedicalai-7627",
                        "file_name": "",
                        "time_stamp": "2022-10-17T15:12:00Z",
                        "time_start": "2022-10-17T15:12:00Z",
                        "time_end": "2022-10-17T15:15:00Z",
                        "paper_type": "workshop",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "481",
                        "paper_award": "",
                        "image_caption": "We propose an open source visualization interface for audiovisual digital biomarker analysis. \nOpenDBM is an open source project that produces derived and raw digital biomarker variables from audiovisual sources.\nThese digital biomarker variables are split into four categories: speech, verbal acoustics, head movement, and facial activity.\nOur tool visualizes derived variables from video cohorts using the Cohort Panel and raw variables from one video using the Individual Panel.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "",
                        "ff_id": ""
                    }
                ]
            },
            {
                "title": "BioMedical AI: Session 2 and Panel",
                "session_id": "w-biomedicalai-2",
                "event_prefix": "w-biomedicalai",
                "track": "ok5",
                "livestream_id": "ok5-mon",
                "session_image": "w-biomedicalai-2.png",
                "chair": [
                    "Qianwen Wang",
                    "Vicky Yao",
                    "Bum Chul Kwon",
                    "Nils Gehlenborg"
                ],
                "organizers": [],
                "time_start": "2022-10-17T15:45:00Z",
                "time_end": "2022-10-17T17:00:00Z",
                "discord_category": "",
                "discord_channel": "oklahoma-st-5",
                "discord_channel_id": "1024600839295356939",
                "discord_link": "https://discord.com/channels/978320435554947082/1024600839295356939",
                "slido_link": "https://app.sli.do/event/i28UETZaiRWr1XwX6eKjcf?section=d355f89b-fbd2-4abf-9958-741607905551",
                "youtube_url": "https://youtu.be/V3jND9JjgXU",
                "youtube_id": "V3jND9JjgXU",
                "zoom_meeting": "",
                "zoom_password": "",
                "zoom_link": "",
                "ff_link": "",
                "ff_playlist": "",
                "time_slots": [
                    {
                        "slot_id": "w-biomedicalai-4793-pres",
                        "session_id": "w-biomedicalai-2",
                        "type": "Virtual Presentation (pre-recorded)",
                        "title": "An Interactive Interpretability System for Breast Cancer Screening with Deep Learning",
                        "contributors": [
                            "Yuzhe Lu"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "w-biomedicalai-4793",
                        "file_name": "w-biomedicalai-4793_Lu_Presentation.mp4",
                        "time_stamp": "2022-10-17T15:45:00Z",
                        "time_start": "2022-10-17T15:45:00Z",
                        "time_end": "2022-10-17T15:52:00Z",
                        "paper_type": "workshop",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "482",
                        "paper_award": "",
                        "image_caption": "An overview of our proposed interface.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "w-biomedicalai-4793-qa",
                        "session_id": "w-biomedicalai-2",
                        "type": "Virtual Q+A",
                        "title": "An Interactive Interpretability System for Breast Cancer Screening with Deep Learning (Q&A)",
                        "contributors": [
                            "Yuzhe Lu"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "w-biomedicalai-4793",
                        "file_name": "",
                        "time_stamp": "2022-10-17T15:52:00Z",
                        "time_start": "2022-10-17T15:52:00Z",
                        "time_end": "2022-10-17T15:55:00Z",
                        "paper_type": "workshop",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "482",
                        "paper_award": "",
                        "image_caption": "An overview of our proposed interface.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "w-biomedicalai-9146-pres",
                        "session_id": "w-biomedicalai-2",
                        "type": "Virtual Presentation (pre-recorded)",
                        "title": "Kokiri: Random Forest-Based Cohort Comparison and Characterization",
                        "contributors": [
                            "Klaus Eckelt"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "w-biomedicalai-9146",
                        "file_name": "w-biomedicalai-9146_Eckelt_Presentation.mp4",
                        "time_stamp": "2022-10-17T15:55:00Z",
                        "time_start": "2022-10-17T15:55:00Z",
                        "time_end": "2022-10-17T16:02:00Z",
                        "paper_type": "workshop",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "486",
                        "paper_award": "",
                        "image_caption": "The figure shows Kokiri integrated in Coral, comparing six lung cancer patient cohorts of different race and gender. A ranked list shows the most important attributes to differentiate the cohorts. The overall separability is shown with stacked bar charts. A scatterplot gives an overview of the predicted cohort affiliations of the patients, and a second ranked list displays all items, the cohorts they belong to, and the probabilities to belong to any of the cohorts based on the data.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "w-biomedicalai-9146-qa",
                        "session_id": "w-biomedicalai-2",
                        "type": "Virtual Q+A",
                        "title": "Kokiri: Random Forest-Based Cohort Comparison and Characterization (Q&A)",
                        "contributors": [
                            "Klaus Eckelt"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "w-biomedicalai-9146",
                        "file_name": "",
                        "time_stamp": "2022-10-17T16:02:00Z",
                        "time_start": "2022-10-17T16:02:00Z",
                        "time_end": "2022-10-17T16:05:00Z",
                        "paper_type": "workshop",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "486",
                        "paper_award": "",
                        "image_caption": "The figure shows Kokiri integrated in Coral, comparing six lung cancer patient cohorts of different race and gender. A ranked list shows the most important attributes to differentiate the cohorts. The overall separability is shown with stacked bar charts. A scatterplot gives an overview of the predicted cohort affiliations of the patients, and a second ranked list displays all items, the cohorts they belong to, and the probabilities to belong to any of the cohorts based on the data.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "w-biomedicalai-prog-3",
                        "session_id": "w-biomedicalai-2",
                        "type": "Virtual Presentation (live)",
                        "title": "Panel",
                        "contributors": [
                            "Adam Perer",
                            "David Gotz",
                            "Marc Streit",
                            "Marinka Zitnik"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "",
                        "file_name": "",
                        "time_stamp": "2022-10-17T16:05:00Z",
                        "time_start": "2022-10-17T16:05:00Z",
                        "time_end": "2022-10-17T16:55:00Z",
                        "paper_type": "",
                        "keywords": [],
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "w-biomedicalai-prog-4",
                        "session_id": "w-biomedicalai-2",
                        "type": "Virtual Presentation (live)",
                        "title": "Closing",
                        "contributors": [
                            "Bum c Kwon",
                            "Qianwen Wang"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "",
                        "file_name": "",
                        "time_stamp": "2022-10-17T16:55:00Z",
                        "time_start": "2022-10-17T16:55:00Z",
                        "time_end": "2022-10-17T17:00:00Z",
                        "paper_type": "",
                        "keywords": [],
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    }
                ]
            }
        ]
    },
    "w-visxai": {
        "event": "VISxAI: 5th Workshop on Visualization for AI Explainability",
        "long_name": "VISxAI: 5th Workshop on Visualization for AI Explainability",
        "event_type": "Associated Event (Workshop)",
        "event_prefix": "w-visxai",
        "event_description": "",
        "event_url": "",
        "organizers": [
            "Adam Perer",
            "Angie Boggust",
            "Fred Hohman",
            "Hendrik Strobelt",
            "Mennatallah El-Assady",
            "Zijie Jay Wang"
        ],
        "sessions": [
            {
                "title": "VISxAI: Opening, Keynote + Session 1",
                "session_id": "w-visxai-1",
                "event_prefix": "w-visxai",
                "track": "ok5",
                "livestream_id": "ok5-mon",
                "session_image": "w-visxai-1.png",
                "chair": [
                    "Adam Perer",
                    "Angie Boggust",
                    "Fred Hohman",
                    "Hendrik Strobelt",
                    "Mennatallah El-Assady",
                    "Zijie Jay Wang"
                ],
                "organizers": [],
                "time_start": "2022-10-17T19:00:00Z",
                "time_end": "2022-10-17T20:15:00Z",
                "discord_category": "",
                "discord_channel": "oklahoma-st-5",
                "discord_channel_id": "1024600839295356939",
                "discord_link": "https://discord.com/channels/978320435554947082/1024600839295356939",
                "slido_link": "https://app.sli.do/event/i28UETZaiRWr1XwX6eKjcf?section=d355f89b-fbd2-4abf-9958-741607905551",
                "youtube_url": "https://youtu.be/V3jND9JjgXU",
                "youtube_id": "V3jND9JjgXU",
                "zoom_meeting": "",
                "zoom_password": "",
                "zoom_link": "",
                "ff_link": "",
                "ff_playlist": "",
                "time_slots": [
                    {
                        "slot_id": "w-visxai-prog-1",
                        "session_id": "w-visxai-1",
                        "type": "In Person Other",
                        "title": "Welcome",
                        "contributors": [
                            "Hendrik Strobelt"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "",
                        "file_name": "",
                        "time_stamp": "2022-10-17T19:00:00Z",
                        "time_start": "2022-10-17T19:00:00Z",
                        "time_end": "2022-10-17T19:05:00Z",
                        "paper_type": "",
                        "keywords": [],
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "w-visxai-prog-2",
                        "session_id": "w-visxai-1",
                        "type": "In Person Other",
                        "title": "Keynote",
                        "contributors": [
                            "Ian Johnson"
                        ],
                        "authors": [
                            "Ian Johnson"
                        ],
                        "abstract": "",
                        "uid": "",
                        "file_name": "",
                        "time_stamp": "2022-10-17T19:05:00Z",
                        "time_start": "2022-10-17T19:05:00Z",
                        "time_end": "2022-10-17T19:45:00Z",
                        "paper_type": "",
                        "keywords": [],
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "w-visxai-prog-3",
                        "session_id": "w-visxai-1",
                        "type": "In Person Q+A",
                        "title": "Keynote Q+A",
                        "contributors": [
                            "Ian Johnson"
                        ],
                        "authors": [
                            "Ian Johnson"
                        ],
                        "abstract": "",
                        "uid": "",
                        "file_name": "",
                        "time_stamp": "2022-10-17T19:45:00Z",
                        "time_start": "2022-10-17T19:45:00Z",
                        "time_end": "2022-10-17T19:55:00Z",
                        "paper_type": "",
                        "keywords": [],
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "w-visxai-2556-pres",
                        "session_id": "w-visxai-1",
                        "type": "In Person Presentation",
                        "title": "K-Means Clustering: An Explorable Explainer",
                        "contributors": [
                            "Yi Zhe Ang"
                        ],
                        "authors": [
                            "Yi Zhe Ang"
                        ],
                        "abstract": "",
                        "uid": "w-visxai-2556",
                        "file_name": "",
                        "time_stamp": "2022-10-17T19:55:00Z",
                        "time_start": "2022-10-17T19:55:00Z",
                        "time_end": "2022-10-17T20:02:00Z",
                        "paper_type": "workshop",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "Interactive sandbox of the k-means clustering explorable. Readers can tweak various parameters of the algorithm and observe any changes in the clustering output.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "w-visxai-2556-qa",
                        "session_id": "w-visxai-1",
                        "type": "In Person Q+A",
                        "title": "K-Means Clustering: An Explorable Explainer (Q+A)",
                        "contributors": [
                            "Yi Zhe Ang"
                        ],
                        "authors": [
                            "Yi Zhe Ang"
                        ],
                        "abstract": "",
                        "uid": "w-visxai-2556",
                        "file_name": "",
                        "time_stamp": "2022-10-17T20:02:00Z",
                        "time_start": "2022-10-17T20:02:00Z",
                        "time_end": "2022-10-17T20:05:00Z",
                        "paper_type": "workshop",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "Interactive sandbox of the k-means clustering explorable. Readers can tweak various parameters of the algorithm and observe any changes in the clustering output.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "w-visxai-7873-pres",
                        "session_id": "w-visxai-1",
                        "type": "Virtual Presentation (pre-recorded)",
                        "title": "Action as Information",
                        "contributors": [
                            "Paschalis Bizopoulos"
                        ],
                        "authors": [
                            "Paschalis Bizopoulos"
                        ],
                        "abstract": "",
                        "uid": "w-visxai-7873",
                        "file_name": "w-visxai-7873_Bizopoulos_Presentation.mp4",
                        "time_stamp": "2022-10-17T20:05:00Z",
                        "time_start": "2022-10-17T20:05:00Z",
                        "time_end": "2022-10-17T20:12:00Z",
                        "paper_type": "workshop",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "291",
                        "paper_award": "",
                        "image_caption": "The dashboard can be used to demonstrate the calculation of action of motif-based time-series w.r.t. SANs as descriptions, by changing the model configuration values in real time during training.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "w-visxai-7873-qa",
                        "session_id": "w-visxai-1",
                        "type": "Virtual Q+A",
                        "title": "Action as Information (Q+A)",
                        "contributors": [
                            "Paschalis Bizopoulos"
                        ],
                        "authors": [
                            "Paschalis Bizopoulos"
                        ],
                        "abstract": "",
                        "uid": "w-visxai-7873",
                        "file_name": "",
                        "time_stamp": "2022-10-17T20:12:00Z",
                        "time_start": "2022-10-17T20:12:00Z",
                        "time_end": "2022-10-17T20:15:00Z",
                        "paper_type": "workshop",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "291",
                        "paper_award": "",
                        "image_caption": "The dashboard can be used to demonstrate the calculation of action of motif-based time-series w.r.t. SANs as descriptions, by changing the model configuration values in real time during training.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    }
                ]
            },
            {
                "title": "VISxAI: Session 2, Session 3, Closing",
                "session_id": "w-visxai-2",
                "event_prefix": "w-visxai",
                "track": "ok5",
                "livestream_id": "ok5-mon",
                "session_image": "w-visxai-2.png",
                "chair": [
                    "Adam Perer",
                    "Angie Boggust",
                    "Fred Hohman",
                    "Hendrik Strobelt",
                    "Mennatallah El-Assady",
                    "Zijie Jay Wang"
                ],
                "organizers": [],
                "time_start": "2022-10-17T20:45:00Z",
                "time_end": "2022-10-17T22:00:00Z",
                "discord_category": "",
                "discord_channel": "oklahoma-st-5",
                "discord_channel_id": "1024600839295356939",
                "discord_link": "https://discord.com/channels/978320435554947082/1024600839295356939",
                "slido_link": "https://app.sli.do/event/i28UETZaiRWr1XwX6eKjcf?section=d355f89b-fbd2-4abf-9958-741607905551",
                "youtube_url": "https://youtu.be/V3jND9JjgXU",
                "youtube_id": "V3jND9JjgXU",
                "zoom_meeting": "",
                "zoom_password": "",
                "zoom_link": "",
                "ff_link": "",
                "ff_playlist": "",
                "time_slots": [
                    {
                        "slot_id": "w-visxai-7361-pres",
                        "session_id": "w-visxai-2",
                        "type": "Virtual Presentation (pre-recorded)",
                        "title": "How is Real-World Gender Bias Reflected in Language Models?",
                        "contributors": [
                            "Alexander Theus"
                        ],
                        "authors": [
                            "Alexander Theus",
                            "Javier Rando",
                            "Rita Sevastjanova",
                            "Mennatallah El-Assady"
                        ],
                        "abstract": "",
                        "uid": "w-visxai-7361",
                        "file_name": "w-visxai-7361_Rando_Presentation.mp4",
                        "time_stamp": "2022-10-17T20:45:00Z",
                        "time_start": "2022-10-17T20:45:00Z",
                        "time_end": "2022-10-17T20:52:00Z",
                        "paper_type": "workshop",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "445",
                        "paper_award": "",
                        "image_caption": "How is Real-World Gender Bias Reflected in Language Models?\n\nJavier Rando, Alexander Theus, Rita Sevastjanova, Menna El-Assady\n\nThis work explores what language models have learned about gender as it relates to professions. Does their output align with real-world demographics?",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "w-visxai-7361-qa",
                        "session_id": "w-visxai-2",
                        "type": "Virtual Q+A",
                        "title": "How is Real-World Gender Bias Reflected in Language Models? (Q+A)",
                        "contributors": [
                            "Alexander Theus"
                        ],
                        "authors": [
                            "Javier Rando",
                            "Alexander Theus",
                            "Rita Sevastjanova",
                            "Mennatallah El-Assady"
                        ],
                        "abstract": "",
                        "uid": "w-visxai-7361",
                        "file_name": "",
                        "time_stamp": "2022-10-17T20:52:00Z",
                        "time_start": "2022-10-17T20:52:00Z",
                        "time_end": "2022-10-17T20:55:00Z",
                        "paper_type": "workshop",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "445",
                        "paper_award": "",
                        "image_caption": "How is Real-World Gender Bias Reflected in Language Models?\n\nJavier Rando, Alexander Theus, Rita Sevastjanova, Menna El-Assady\n\nThis work explores what language models have learned about gender as it relates to professions. Does their output align with real-world demographics?",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "w-visxai-6729-pres",
                        "session_id": "w-visxai-2",
                        "type": "Virtual Presentation (pre-recorded)",
                        "title": "Explaining Image Classifiers with Wavelets",
                        "contributors": [
                            "Julius Hege"
                        ],
                        "authors": [
                            "Julius Hege",
                            "Stefan Kolek",
                            "Gitta Kutyniok"
                        ],
                        "abstract": "",
                        "uid": "w-visxai-6729",
                        "file_name": "",
                        "time_stamp": "2022-10-17T20:55:00Z",
                        "time_start": "2022-10-17T20:55:00Z",
                        "time_end": "2022-10-17T21:02:00Z",
                        "paper_type": "workshop",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "w-visxai-6729-qa",
                        "session_id": "w-visxai-2",
                        "type": "Virtual Q+A",
                        "title": "Explaining Image Classifiers with Wavelets (Q+A)",
                        "contributors": [
                            "Julius Hege"
                        ],
                        "authors": [
                            "Julius Hege",
                            "Stefan Kolek",
                            "Gitta Kutyniok"
                        ],
                        "abstract": "",
                        "uid": "w-visxai-6729",
                        "file_name": "",
                        "time_stamp": "2022-10-17T21:02:00Z",
                        "time_start": "2022-10-17T21:02:00Z",
                        "time_end": "2022-10-17T21:05:00Z",
                        "paper_type": "workshop",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "w-visxai-7608-pres",
                        "session_id": "w-visxai-2",
                        "type": "Virtual Presentation (pre-recorded)",
                        "title": "What should we watch tonight?",
                        "contributors": [
                            "Ibrahim Al-Hazwani"
                        ],
                        "authors": [
                            "Ibrahim Al-Hazwani",
                            "Gabriela Morgenshtern",
                            "Yves Rutishauser",
                            "Mennatallah El-Assady",
                            "J\u00fcrgen Bernard"
                        ],
                        "abstract": "",
                        "uid": "w-visxai-7608",
                        "file_name": "w-visxai-7608_Alhazwani_Presentation.mp4",
                        "time_stamp": "2022-10-17T21:05:00Z",
                        "time_start": "2022-10-17T21:05:00Z",
                        "time_end": "2022-10-17T21:12:00Z",
                        "paper_type": "workshop",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "327",
                        "paper_award": "",
                        "image_caption": "Image shows the 3 ground Venn diagram. Inside each region, movies have been encoded as circles. The dimensions of the each circle reflect the model error in predicting whether the user likes this movie.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "w-visxai-7608-qa",
                        "session_id": "w-visxai-2",
                        "type": "Virtual Q+A",
                        "title": "What should we watch tonight? (Q+A)",
                        "contributors": [
                            "Ibrahim Al-Hazwani"
                        ],
                        "authors": [
                            "Ibrahim Al-Hazwani",
                            "Gabriela Morgenshtern",
                            "Yves Rutishauser",
                            "Mennatallah El-Assady",
                            "J\u00fcrgen Bernard"
                        ],
                        "abstract": "",
                        "uid": "w-visxai-7608",
                        "file_name": "",
                        "time_stamp": "2022-10-17T21:12:00Z",
                        "time_start": "2022-10-17T21:12:00Z",
                        "time_end": "2022-10-17T21:15:00Z",
                        "paper_type": "workshop",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "327",
                        "paper_award": "",
                        "image_caption": "Image shows the 3 ground Venn diagram. Inside each region, movies have been encoded as circles. The dimensions of the each circle reflect the model error in predicting whether the user likes this movie.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "w-visxai-3404-pres",
                        "session_id": "w-visxai-2",
                        "type": "In Person Presentation",
                        "title": "Poisoning Attacks and Subpopulation Susceptibility",
                        "contributors": [
                            "Evan Rose"
                        ],
                        "authors": [
                            "Evan Rose",
                            "David Evans",
                            "Fnu Suya"
                        ],
                        "abstract": "",
                        "uid": "w-visxai-3404",
                        "file_name": "",
                        "time_stamp": "2022-10-17T21:15:00Z",
                        "time_start": "2022-10-17T21:15:00Z",
                        "time_end": "2022-10-17T21:22:00Z",
                        "paper_type": "workshop",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "Machine learning is susceptible to poisoning attacks, in which adversaries inject maliciously crafted training data into the training set to induce specific model behavior. What happens when attackers focus their efforts on specific subpopulations of the input space?",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "w-visxai-3404-qa",
                        "session_id": "w-visxai-2",
                        "type": "In Person Q+A",
                        "title": "Poisoning Attacks and Subpopulation Susceptibility (Q+A)",
                        "contributors": [
                            "Evan Rose"
                        ],
                        "authors": [
                            "Evan Rose",
                            "Fnu Suya",
                            "David Evans"
                        ],
                        "abstract": "",
                        "uid": "w-visxai-3404",
                        "file_name": "",
                        "time_stamp": "2022-10-17T21:22:00Z",
                        "time_start": "2022-10-17T21:22:00Z",
                        "time_end": "2022-10-17T21:25:00Z",
                        "paper_type": "workshop",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "Machine learning is susceptible to poisoning attacks, in which adversaries inject maliciously crafted training data into the training set to induce specific model behavior. What happens when attackers focus their efforts on specific subpopulations of the input space?",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "w-visxai-8479-pres",
                        "session_id": "w-visxai-2",
                        "type": "In Person Presentation",
                        "title": "Mapping Wikipedia with BERT and UMAP",
                        "contributors": [
                            "Brandon Duderstadt"
                        ],
                        "authors": [
                            "Brandon Duderstadt",
                            "Andriy Mulyar"
                        ],
                        "abstract": "",
                        "uid": "w-visxai-8479",
                        "file_name": "w-visxai-8479_Duderstadt_Presentation.mp4",
                        "time_stamp": "2022-10-17T21:25:00Z",
                        "time_start": "2022-10-17T21:25:00Z",
                        "time_end": "2022-10-17T21:32:00Z",
                        "paper_type": "workshop",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "375",
                        "paper_award": "",
                        "image_caption": "The first map of the entirety of English Wikipedia. Each dot corresponds to an article, and dots are close to each other if the text of their corresponding articles is similar. Colors represent human annotated article categories, which were not used during map production.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "w-visxai-8479-qa",
                        "session_id": "w-visxai-2",
                        "type": "In Person Q+A",
                        "title": "Mapping Wikipedia with BERT and UMAP (Q+A)",
                        "contributors": [
                            "Brandon Duderstadt"
                        ],
                        "authors": [
                            "Brandon Duderstadt",
                            "Andriy Mulyar"
                        ],
                        "abstract": "",
                        "uid": "w-visxai-8479",
                        "file_name": "",
                        "time_stamp": "2022-10-17T21:32:00Z",
                        "time_start": "2022-10-17T21:32:00Z",
                        "time_end": "2022-10-17T21:35:00Z",
                        "paper_type": "workshop",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "375",
                        "paper_award": "",
                        "image_caption": "The first map of the entirety of English Wikipedia. Each dot corresponds to an article, and dots are close to each other if the text of their corresponding articles is similar. Colors represent human annotated article categories, which were not used during map production.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "w-visxai-1801-pres",
                        "session_id": "w-visxai-2",
                        "type": "Virtual Presentation (pre-recorded)",
                        "title": "An Interactive Introduction to Causal Inference",
                        "contributors": [
                            "Lucius Bynum"
                        ],
                        "authors": [
                            "Lucius Bynum",
                            "Falaah Arif Khan",
                            "Oleksandra Konopatska",
                            "Julia Stoyanovich",
                            "Joshua Loftus"
                        ],
                        "abstract": "",
                        "uid": "w-visxai-1801",
                        "file_name": "w-visxai-1801_Bynum_Presentation.mp4",
                        "time_stamp": "2022-10-17T21:35:00Z",
                        "time_start": "2022-10-17T21:35:00Z",
                        "time_end": "2022-10-17T21:42:00Z",
                        "paper_type": "workshop",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "320",
                        "paper_award": "",
                        "image_caption": "A stylized causal graphical model with illustrated nodes, depicting a relationship between income, childcare, and standardized test scores.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "w-visxai-1801-qa",
                        "session_id": "w-visxai-2",
                        "type": "Virtual Q+A",
                        "title": "An Interactive Introduction to Causal Inference (Q+A)",
                        "contributors": [
                            "Lucius Bynum"
                        ],
                        "authors": [
                            "Lucius E.J. Bynum",
                            "Falaah Arif Khan",
                            "Oleksandra Konopatska",
                            "Julia Stoyanovich",
                            "Joshua Loftus"
                        ],
                        "abstract": "",
                        "uid": "w-visxai-1801",
                        "file_name": "",
                        "time_stamp": "2022-10-17T21:42:00Z",
                        "time_start": "2022-10-17T21:42:00Z",
                        "time_end": "2022-10-17T21:45:00Z",
                        "paper_type": "workshop",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "320",
                        "paper_award": "",
                        "image_caption": "A stylized causal graphical model with illustrated nodes, depicting a relationship between income, childcare, and standardized test scores.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "w-visxai-prog-4",
                        "session_id": "w-visxai-2",
                        "type": "In Person Other",
                        "title": "Closing",
                        "contributors": [
                            "Hendrik Strobelt"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "",
                        "file_name": "",
                        "time_stamp": "2022-10-17T21:45:00Z",
                        "time_start": "2022-10-17T21:45:00Z",
                        "time_end": "2022-10-17T22:00:00Z",
                        "paper_type": "",
                        "keywords": [],
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    }
                ]
            }
        ]
    },
    "w-vis4dh": {
        "event": "VIS4DH: 7th Workshop on Visualization for the Digital Humanities",
        "long_name": "VIS4DH: 7th Workshop on Visualization for the Digital Humanities",
        "event_type": "Associated Event (Workshop)",
        "event_prefix": "w-vis4dh",
        "event_description": "",
        "event_url": "",
        "organizers": [
            "Alfie Abdul-Rahman",
            "Houda Lamqaddam",
            "Chris Weaver"
        ],
        "sessions": [
            {
                "title": "VIS4DH: Paper Session 1",
                "session_id": "w-vis4dh-1",
                "event_prefix": "w-vis4dh",
                "track": "ok1",
                "livestream_id": "ok1-sun",
                "session_image": "w-vis4dh-1.png",
                "chair": [
                    "Alfie Abdul-Rahman",
                    "Houda Lamqaddam",
                    "Chris Weaver"
                ],
                "organizers": [],
                "time_start": "2022-10-16T14:00:00Z",
                "time_end": "2022-10-16T15:15:00Z",
                "discord_category": "",
                "discord_channel": "oklahoma-st-1",
                "discord_channel_id": "1024600861340606484",
                "discord_link": "https://discord.com/channels/978320435554947082/1024600861340606484",
                "slido_link": "https://app.sli.do/event/i28UETZaiRWr1XwX6eKjcf?section=740d9835-fe47-4cb4-b260-353262a7f8dd",
                "youtube_url": "https://youtu.be/2wc4IY5A70Y",
                "youtube_id": "2wc4IY5A70Y",
                "zoom_meeting": "",
                "zoom_password": "",
                "zoom_link": "",
                "ff_link": "",
                "ff_playlist": "",
                "time_slots": [
                    {
                        "slot_id": "w-vis4dh-prog-1",
                        "session_id": "w-vis4dh-1",
                        "type": "In Person Presentation",
                        "title": "Opening Presentation",
                        "contributors": [
                            "Alfie Abdul-Rahman",
                            "Eric Alexander"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "",
                        "file_name": "",
                        "time_stamp": "2022-10-16T14:00:00Z",
                        "time_start": "2022-10-16T14:00:00Z",
                        "time_end": "2022-10-16T14:15:00Z",
                        "paper_type": "",
                        "keywords": [],
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "w-visdh-1009",
                        "session_id": "w-vis4dh-1",
                        "type": "Virtual Presentation (pre-recorded)",
                        "title": "Data Feel: Exploring Visual Effects in Video Games to Support Sensemaking Tasks",
                        "contributors": [
                            "Hongwei Zhou"
                        ],
                        "authors": [
                            "Hongwei Zhou",
                            "Angus Forbes"
                        ],
                        "abstract": "",
                        "uid": "w-vis4dh-1009",
                        "file_name": "w-vis4dh-1009_Zhou_Presentation.mp4",
                        "time_stamp": "2022-10-16T14:15:00Z",
                        "time_start": "2022-10-16T14:15:00Z",
                        "time_end": "2022-10-16T14:27:00Z",
                        "paper_type": "workshop",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "635",
                        "paper_award": "",
                        "image_caption": "In this paper, we explore the potential of visual effects and animations for supporting sensemaking tasks. We contrast the minimalist aesthetics in data visualization applications with those in video games. We survey video games across different genres and identify three non-exclusive uses of visual effects and animations to support sensemaking.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "w-visdh-1011",
                        "session_id": "w-vis4dh-1",
                        "type": "In Person Presentation",
                        "title": "Multimodal analogs to infer humanities visualization requirements",
                        "contributors": [
                            "Richard Brath"
                        ],
                        "authors": [
                            "Richard Brath"
                        ],
                        "abstract": "",
                        "uid": "w-vis4dh-1011",
                        "file_name": "",
                        "time_stamp": "2022-10-16T14:27:00Z",
                        "time_start": "2022-10-16T14:27:00Z",
                        "time_end": "2022-10-16T14:39:00Z",
                        "paper_type": "workshop",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "Gaps and requirements for multi-modal interfaces for humanities can be explored by observing the configuration of real-world environments and visitor tasks. Real world stores, musuems, galleries and stages provide for tasks such as overviewing, clustering, collaboration and comparion with much richer interactions. Some of these technical capabilities exist but are not routinely available in implementations.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "w-vis4dh-prog-2",
                        "session_id": "w-vis4dh-1",
                        "type": "Virtual Q+A",
                        "title": "Paired Paper Discussion / Q&A",
                        "contributors": [
                            "Florian Windhager"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "",
                        "file_name": "",
                        "time_stamp": "2022-10-16T14:39:00Z",
                        "time_start": "2022-10-16T14:39:00Z",
                        "time_end": "2022-10-16T14:45:00Z",
                        "paper_type": "",
                        "keywords": [],
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "w-visdh-1016",
                        "session_id": "w-vis4dh-1",
                        "type": "Virtual Presentation (pre-recorded)",
                        "title": "Boundaries, Extensions, and Challenges of Visualization for Humanities Data: Reflections on Three Cases",
                        "contributors": [
                            "Rongqian Ma"
                        ],
                        "authors": [
                            "Rongqian Ma"
                        ],
                        "abstract": "",
                        "uid": "w-vis4dh-1016",
                        "file_name": "w-vis4dh-1016_Ma_Presentation.mp4",
                        "time_stamp": "2022-10-16T14:45:00Z",
                        "time_start": "2022-10-16T14:45:00Z",
                        "time_end": "2022-10-16T14:57:00Z",
                        "paper_type": "workshop",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "623",
                        "paper_award": "",
                        "image_caption": "This paper, titled \u201cBoundaries, Extensions, and Challenges of Visualization for Humanities Data,\u201d examines the application of visual technologies in three broadly-defined digital humanities projects and explores how the practices and challenges along the way would push forward our connotations of a visual representation in the context of humanities research.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "w-visdh-1015",
                        "session_id": "w-vis4dh-1",
                        "type": "In Person Presentation",
                        "title": "Word Clouds in the Wild",
                        "contributors": [
                            "Rebecca M. M. Hicke"
                        ],
                        "authors": [
                            "Rebecca Hicke",
                            "Maanya Goenka",
                            "Eric Alexander"
                        ],
                        "abstract": "",
                        "uid": "w-vis4dh-1015",
                        "file_name": "",
                        "time_stamp": "2022-10-16T14:57:00Z",
                        "time_start": "2022-10-16T14:57:00Z",
                        "time_end": "2022-10-16T15:09:00Z",
                        "paper_type": "workshop",
                        "keywords": [],
                        "has_image": "0",
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "w-vis4dh-prog-3",
                        "session_id": "w-vis4dh-1",
                        "type": "Virtual Q+A",
                        "title": "Paired Paper Discussion / Q&A",
                        "contributors": [
                            "Florian Windhager"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "",
                        "file_name": "",
                        "time_stamp": "2022-10-16T15:09:00Z",
                        "time_start": "2022-10-16T15:09:00Z",
                        "time_end": "2022-10-16T15:15:00Z",
                        "paper_type": "",
                        "keywords": [],
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    }
                ]
            },
            {
                "title": "VIS4DH: Keynote and Lab Talks 1",
                "session_id": "w-vis4dh-2",
                "event_prefix": "w-vis4dh",
                "track": "ok1",
                "livestream_id": "ok1-sun",
                "session_image": "w-vis4dh-2.png",
                "chair": [
                    "Alfie Abdul-Rahman",
                    "Houda Lamqaddam",
                    "Chris Weaver"
                ],
                "organizers": [],
                "time_start": "2022-10-16T15:45:00Z",
                "time_end": "2022-10-16T17:00:00Z",
                "discord_category": "",
                "discord_channel": "oklahoma-st-1",
                "discord_channel_id": "1024600861340606484",
                "discord_link": "https://discord.com/channels/978320435554947082/1024600861340606484",
                "slido_link": "https://app.sli.do/event/i28UETZaiRWr1XwX6eKjcf?section=740d9835-fe47-4cb4-b260-353262a7f8dd",
                "youtube_url": "https://youtu.be/2wc4IY5A70Y",
                "youtube_id": "2wc4IY5A70Y",
                "zoom_meeting": "",
                "zoom_password": "",
                "zoom_link": "",
                "ff_link": "",
                "ff_playlist": "",
                "time_slots": [
                    {
                        "slot_id": "w-vis4dh-prog-4",
                        "session_id": "w-vis4dh-2",
                        "type": "Virtual Presentation (live)",
                        "title": "A Path Towards Inclusive Visualization",
                        "contributors": [
                            "Ed Summers"
                        ],
                        "authors": [
                            "Ed Summers"
                        ],
                        "abstract": "",
                        "uid": "",
                        "file_name": "",
                        "time_stamp": "2022-10-16T15:45:00Z",
                        "time_start": "2022-10-16T15:45:00Z",
                        "time_end": "2022-10-16T16:30:00Z",
                        "paper_type": "",
                        "keywords": [],
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "w-vis4dh-prog-5",
                        "session_id": "w-vis4dh-2",
                        "type": "Virtual Q+A",
                        "title": "Keynote Q+A",
                        "contributors": [
                            "Chris Weaver",
                            "Alfie Abdul-Rahman"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "",
                        "file_name": "",
                        "time_stamp": "2022-10-16T16:30:00Z",
                        "time_start": "2022-10-16T16:30:00Z",
                        "time_end": "2022-10-16T16:45:00Z",
                        "paper_type": "",
                        "keywords": [],
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "w-vis4dh-prog-6",
                        "session_id": "w-vis4dh-2",
                        "type": "Mixed Panel (virtual/in person)",
                        "title": "Intro to Lab Talks",
                        "contributors": [
                            "Alfie Abdul-Rahman"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "",
                        "file_name": "",
                        "time_stamp": "2022-10-16T16:45:00Z",
                        "time_start": "2022-10-16T16:45:00Z",
                        "time_end": "2022-10-16T16:50:00Z",
                        "paper_type": "",
                        "keywords": [],
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "w-vis4dh-prog-7",
                        "session_id": "w-vis4dh-2",
                        "type": "Virtual Presentation (pre-recorded)",
                        "title": "Centre for Digital Humanities Uppsala \u2013 Uppsala, Sweden",
                        "contributors": [
                            "Anna Foka"
                        ],
                        "authors": [
                            "Anna Foka"
                        ],
                        "abstract": "",
                        "uid": "",
                        "file_name": "Anna Foka - video1368741963.mp4",
                        "time_stamp": "2022-10-16T16:50:00Z",
                        "time_start": "2022-10-16T16:50:00Z",
                        "time_end": "2022-10-16T16:55:00Z",
                        "paper_type": "",
                        "keywords": [],
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "w-vis4dh-prog-8",
                        "session_id": "w-vis4dh-2",
                        "type": "Virtual Presentation (live)",
                        "title": "Visuality and Visualization Lab (LabVis) \u2013 School of Fine Arts, Universidade Federal do Rio de Janeiro, Brazil",
                        "contributors": [
                            "Doris Kosminsky"
                        ],
                        "authors": [
                            "Doris Kosminsky"
                        ],
                        "abstract": "",
                        "uid": "",
                        "file_name": "",
                        "time_stamp": "2022-10-16T16:55:00Z",
                        "time_start": "2022-10-16T16:55:00Z",
                        "time_end": "2022-10-16T17:00:00Z",
                        "paper_type": "",
                        "keywords": [],
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    }
                ]
            },
            {
                "title": "VIS4DH: Keynote and Townhall",
                "session_id": "w-vis4dh-3",
                "event_prefix": "w-vis4dh",
                "track": "ok1",
                "livestream_id": "ok1-sun",
                "session_image": "w-vis4dh-3.png",
                "chair": [
                    "Alfie Abdul-Rahman",
                    "Houda Lamqaddam",
                    "Chris Weaver"
                ],
                "organizers": [],
                "time_start": "2022-10-16T19:00:00Z",
                "time_end": "2022-10-16T20:15:00Z",
                "discord_category": "",
                "discord_channel": "oklahoma-st-1",
                "discord_channel_id": "1024600861340606484",
                "discord_link": "https://discord.com/channels/978320435554947082/1024600861340606484",
                "slido_link": "https://app.sli.do/event/i28UETZaiRWr1XwX6eKjcf?section=740d9835-fe47-4cb4-b260-353262a7f8dd",
                "youtube_url": "https://youtu.be/2wc4IY5A70Y",
                "youtube_id": "2wc4IY5A70Y",
                "zoom_meeting": "",
                "zoom_password": "",
                "zoom_link": "",
                "ff_link": "",
                "ff_playlist": "",
                "time_slots": [
                    {
                        "slot_id": "w-vis4dh-prog-9",
                        "session_id": "w-vis4dh-3",
                        "type": "Virtual Presentation (live)",
                        "title": "Varieties of Uncertainty",
                        "contributors": [
                            "Miriam Posner"
                        ],
                        "authors": [
                            "Miriam Posner"
                        ],
                        "abstract": "",
                        "uid": "",
                        "file_name": "",
                        "time_stamp": "2022-10-16T19:00:00Z",
                        "time_start": "2022-10-16T19:00:00Z",
                        "time_end": "2022-10-16T19:45:00Z",
                        "paper_type": "",
                        "keywords": [],
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "w-vis4dh-prog-10",
                        "session_id": "w-vis4dh-3",
                        "type": "Virtual Q+A",
                        "title": "Keynote Q+A",
                        "contributors": [
                            "Michael Correll"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "",
                        "file_name": "",
                        "time_stamp": "2022-10-16T19:45:00Z",
                        "time_start": "2022-10-16T19:45:00Z",
                        "time_end": "2022-10-16T20:00:00Z",
                        "paper_type": "",
                        "keywords": [],
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "w-vis4dh-prog-11",
                        "session_id": "w-vis4dh-3",
                        "type": "In Person Presentation",
                        "title": "Townhall",
                        "contributors": [
                            "Menna El-Assady"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "",
                        "file_name": "",
                        "time_stamp": "2022-10-16T20:00:00Z",
                        "time_start": "2022-10-16T20:00:00Z",
                        "time_end": "2022-10-16T20:15:00Z",
                        "paper_type": "",
                        "keywords": [],
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    }
                ]
            },
            {
                "title": "VIS4DH: Paper Session 2, Lab Talks 2, Closing",
                "session_id": "w-vis4dh-4",
                "event_prefix": "w-vis4dh",
                "track": "ok1",
                "livestream_id": "ok1-sun",
                "session_image": "w-vis4dh-4.png",
                "chair": [
                    "Alfie Abdul-Rahman",
                    "Houda Lamqaddam",
                    "Chris Weaver"
                ],
                "organizers": [],
                "time_start": "2022-10-16T20:45:00Z",
                "time_end": "2022-10-16T22:00:00Z",
                "discord_category": "",
                "discord_channel": "oklahoma-st-1",
                "discord_channel_id": "1024600861340606484",
                "discord_link": "https://discord.com/channels/978320435554947082/1024600861340606484",
                "slido_link": "https://app.sli.do/event/i28UETZaiRWr1XwX6eKjcf?section=740d9835-fe47-4cb4-b260-353262a7f8dd",
                "youtube_url": "https://youtu.be/2wc4IY5A70Y",
                "youtube_id": "2wc4IY5A70Y",
                "zoom_meeting": "",
                "zoom_password": "",
                "zoom_link": "",
                "ff_link": "",
                "ff_playlist": "",
                "time_slots": [
                    {
                        "slot_id": "w-vis4dh-prog-12",
                        "session_id": "w-vis4dh-4",
                        "type": "In Person Presentation",
                        "title": "From Historical Documents To Social Network Visualization: Potential Pitfalls and Network Modeling",
                        "contributors": [
                            "Alexis Pister"
                        ],
                        "authors": [
                            "Alexis Pister",
                            "Nicole Dufournaud",
                            "Pascal Cristofoli",
                            "Christophe Prieur",
                            "Jean-Daniel Fekete"
                        ],
                        "abstract": "",
                        "uid": "",
                        "file_name": "",
                        "time_stamp": "2022-10-16T20:45:00Z",
                        "time_start": "2022-10-16T20:45:00Z",
                        "time_end": "2022-10-16T20:57:00Z",
                        "paper_type": "",
                        "keywords": [],
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "w-vis4dh-prog-13",
                        "session_id": "w-vis4dh-4",
                        "type": "In Person Presentation",
                        "title": "Characterizing Uncertainty in the Visual Text Analysis Pipeline",
                        "contributors": [
                            "Jean-Daniel Fekete"
                        ],
                        "authors": [
                            "Pantea Haghighatkhah",
                            "Mennatallah El-Assady",
                            "Jean-Daniel Fekete",
                            "Narges Mahyar",
                            "Carita Paradis",
                            "Vasiliki Simaki",
                            "Bettina Speckmann"
                        ],
                        "abstract": "",
                        "uid": "",
                        "file_name": "",
                        "time_stamp": "2022-10-16T20:57:00Z",
                        "time_start": "2022-10-16T20:57:00Z",
                        "time_end": "2022-10-16T21:09:00Z",
                        "paper_type": "",
                        "keywords": [],
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "w-vis4dh-prog-14",
                        "session_id": "w-vis4dh-4",
                        "type": "In Person Q+A",
                        "title": "Paired Paper Discussion / Q&A",
                        "contributors": [
                            "Eric Alexander"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "",
                        "file_name": "",
                        "time_stamp": "2022-10-16T21:09:00Z",
                        "time_start": "2022-10-16T21:09:00Z",
                        "time_end": "2022-10-16T21:15:00Z",
                        "paper_type": "",
                        "keywords": [],
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "w-vis4dh-prog-15",
                        "session_id": "w-vis4dh-4",
                        "type": "In Person Presentation",
                        "title": "Labeling of Cultural Heritage Collections on the Intersection of Visual Analytics and Digital Humanities",
                        "contributors": [
                            "Christofer Meinecke"
                        ],
                        "authors": [
                            "Christofer Meinecke"
                        ],
                        "abstract": "",
                        "uid": "",
                        "file_name": "",
                        "time_stamp": "2022-10-16T21:15:00Z",
                        "time_start": "2022-10-16T21:15:00Z",
                        "time_end": "2022-10-16T21:27:00Z",
                        "paper_type": "",
                        "keywords": [],
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "w-vis4dh-prog-16",
                        "session_id": "w-vis4dh-4",
                        "type": "Virtual Presentation (pre-recorded)",
                        "title": "The Multiple Faces of Cultural Heritage: Towards an Integrated Visualization Platform for Tangible and Intangible Cultural Assets",
                        "contributors": [
                            "Eva Mayr"
                        ],
                        "authors": [
                            "Eva Mayr",
                            "Florian Windhager",
                            "Johannes Liem",
                            "Samuel Beck",
                            "Steffen Koch",
                            "Jakob Kusnick",
                            "Stefan J\u00e4nicke"
                        ],
                        "abstract": "",
                        "uid": "w-vis4dh-1017",
                        "file_name": "w-vis4dh-1017_Mayr_Presentation.mp4",
                        "time_stamp": "2022-10-16T21:27:00Z",
                        "time_start": "2022-10-16T21:27:00Z",
                        "time_end": "2022-10-16T21:39:00Z",
                        "paper_type": "workshop",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "548",
                        "paper_award": "",
                        "image_caption": "Multiple Faces of Cultural Heritage",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "w-vis4dh-prog-17",
                        "session_id": "w-vis4dh-4",
                        "type": "Virtual Q+A",
                        "title": "Paired Paper Discussion / Q&A",
                        "contributors": [
                            "Eric Alexander"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "",
                        "file_name": "",
                        "time_stamp": "2022-10-16T21:39:00Z",
                        "time_start": "2022-10-16T21:39:00Z",
                        "time_end": "2022-10-16T21:45:00Z",
                        "paper_type": "",
                        "keywords": [],
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "w-vis4dh-prog-18",
                        "session_id": "w-vis4dh-4",
                        "type": "Virtual Presentation (live)",
                        "title": "Center for Research Data & Digital Scholarship \u2013 University of Colorado Boulder, Boulder, Colorado USA",
                        "contributors": [
                            "Nickoal Eichmann-Kalwara"
                        ],
                        "authors": [
                            "Nickoal Eichmann-Kalwara"
                        ],
                        "abstract": "",
                        "uid": "",
                        "file_name": "",
                        "time_stamp": "2022-10-16T21:45:00Z",
                        "time_start": "2022-10-16T21:45:00Z",
                        "time_end": "2022-10-16T21:50:00Z",
                        "paper_type": "",
                        "keywords": [],
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "w-vis4dh-prog-19",
                        "session_id": "w-vis4dh-4",
                        "type": "In Person Presentation",
                        "title": "Coptic Scriptorium \u2013 University of Oklahoma and Georgetown University",
                        "contributors": [
                            "Caroline T. Schroeder"
                        ],
                        "authors": [
                            "Caroline T. Schroeder"
                        ],
                        "abstract": "",
                        "uid": "",
                        "file_name": "",
                        "time_stamp": "2022-10-16T21:50:00Z",
                        "time_start": "2022-10-16T21:50:00Z",
                        "time_end": "2022-10-16T21:55:00Z",
                        "paper_type": "",
                        "keywords": [],
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "w-vis4dh-prog-20",
                        "session_id": "w-vis4dh-4",
                        "type": "In Person Presentation",
                        "title": "Closing",
                        "contributors": [
                            "Alfie Abdul-Rahman"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "",
                        "file_name": "",
                        "time_stamp": "2022-10-16T21:55:00Z",
                        "time_start": "2022-10-16T21:55:00Z",
                        "time_end": "2022-10-16T22:00:00Z",
                        "paper_type": "",
                        "keywords": [],
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    }
                ]
            }
        ]
    },
    "a-visinpractice": {
        "event": "VisInPractice",
        "long_name": "VisInPractice",
        "event_type": "Associated Event",
        "event_prefix": "a-visinpractice",
        "event_description": "",
        "event_url": "",
        "organizers": [
            "Anamaria Crisan",
            "Zhicheng (Leo) Liu",
            "Sudhanshu Kumar Semwal",
            "Chris Weaver"
        ],
        "sessions": [
            {
                "title": "VisInPractice: Diversity of Ideas",
                "session_id": "a-visinpractice-1",
                "event_prefix": "a-visinpractice",
                "track": "ok1",
                "livestream_id": "ok1-mon",
                "session_image": "a-visinpractice-1.png",
                "chair": [
                    "Anamaria Crisan",
                    "Zhicheng (Leo) Liu",
                    "Sudhanshu Kumar Semwal",
                    "Chris Weaver"
                ],
                "organizers": [],
                "time_start": "2022-10-17T14:00:00Z",
                "time_end": "2022-10-17T15:15:00Z",
                "discord_category": "",
                "discord_channel": "oklahoma-st-1",
                "discord_channel_id": "1024600861340606484",
                "discord_link": "https://discord.com/channels/978320435554947082/1024600861340606484",
                "slido_link": "https://app.sli.do/event/i28UETZaiRWr1XwX6eKjcf?section=740d9835-fe47-4cb4-b260-353262a7f8dd",
                "youtube_url": "https://youtu.be/r4A4lla0eXI",
                "youtube_id": "r4A4lla0eXI",
                "zoom_meeting": "",
                "zoom_password": "",
                "zoom_link": "",
                "ff_link": "",
                "ff_playlist": "",
                "time_slots": [
                    {
                        "slot_id": "a-visinpractice-prog-1",
                        "session_id": "a-visinpractice-1",
                        "type": "Mixed Panel (virtual/in person)",
                        "title": "Panel: Diversity of Ideas",
                        "contributors": [
                            "Dr. A. Paz de Araujo",
                            "Dr. Hanes Oliveira",
                            "Dr. David William Silva",
                            "Dr. Hector Gerardo Perez Gonzalez",
                            "Dr. Terry Janssen",
                            "Dr. Alireza Taheri"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "",
                        "file_name": "",
                        "time_stamp": "2022-10-17T14:00:00Z",
                        "time_start": "2022-10-17T14:00:00Z",
                        "time_end": "2022-10-17T15:15:00Z",
                        "paper_type": "",
                        "keywords": [],
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    }
                ]
            },
            {
                "title": "VisInPractice: Vis x Accessibility",
                "session_id": "a-visinpractice-2",
                "event_prefix": "a-visinpractice",
                "track": "ok1",
                "livestream_id": "ok1-mon",
                "session_image": "a-visinpractice-2.png",
                "chair": [
                    "Anamaria Crisan",
                    "Zhicheng (Leo) Liu",
                    "Sudhanshu Kumar Semwal",
                    "Chris Weaver"
                ],
                "organizers": [],
                "time_start": "2022-10-17T15:45:00Z",
                "time_end": "2022-10-17T17:00:00Z",
                "discord_category": "",
                "discord_channel": "oklahoma-st-1",
                "discord_channel_id": "1024600861340606484",
                "discord_link": "https://discord.com/channels/978320435554947082/1024600861340606484",
                "slido_link": "https://app.sli.do/event/i28UETZaiRWr1XwX6eKjcf?section=740d9835-fe47-4cb4-b260-353262a7f8dd",
                "youtube_url": "https://youtu.be/r4A4lla0eXI",
                "youtube_id": "r4A4lla0eXI",
                "zoom_meeting": "",
                "zoom_password": "",
                "zoom_link": "",
                "ff_link": "",
                "ff_playlist": "",
                "time_slots": [
                    {
                        "slot_id": "a-visinpractice-prog-2",
                        "session_id": "a-visinpractice-2",
                        "type": "Mixed Panel (virtual/in person)",
                        "title": "Panel: Vis x Accessibility",
                        "contributors": [
                            "Frank Elavsky",
                            "Kai Chang",
                            "Jamie Tanner",
                            "Jennifer Mankoff"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "",
                        "file_name": "",
                        "time_stamp": "2022-10-17T15:45:00Z",
                        "time_start": "2022-10-17T15:45:00Z",
                        "time_end": "2022-10-17T17:00:00Z",
                        "paper_type": "",
                        "keywords": [],
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    }
                ]
            },
            {
                "title": "VisInPractice: Bridging Academia, Industry, and Government",
                "session_id": "a-visinpractice-3",
                "event_prefix": "a-visinpractice",
                "track": "ok1",
                "livestream_id": "ok1-mon",
                "session_image": "a-visinpractice-3.png",
                "chair": [
                    "Anamaria Crisan",
                    "Zhicheng (Leo) Liu",
                    "Sudhanshu Kumar Semwal",
                    "Chris Weaver"
                ],
                "organizers": [],
                "time_start": "2022-10-17T19:00:00Z",
                "time_end": "2022-10-17T20:15:00Z",
                "discord_category": "",
                "discord_channel": "oklahoma-st-1",
                "discord_channel_id": "1024600861340606484",
                "discord_link": "https://discord.com/channels/978320435554947082/1024600861340606484",
                "slido_link": "https://app.sli.do/event/i28UETZaiRWr1XwX6eKjcf?section=740d9835-fe47-4cb4-b260-353262a7f8dd",
                "youtube_url": "https://youtu.be/r4A4lla0eXI",
                "youtube_id": "r4A4lla0eXI",
                "zoom_meeting": "",
                "zoom_password": "",
                "zoom_link": "",
                "ff_link": "",
                "ff_playlist": "",
                "time_slots": [
                    {
                        "slot_id": "a-visinpractice-prog-3",
                        "session_id": "a-visinpractice-3",
                        "type": "In Person Panel",
                        "title": "Panel: Bridging Academia, Industry, and Government",
                        "contributors": [
                            "William Endres",
                            "Dave King",
                            "Kim Klockow-McClain",
                            "Johanna Schmidt"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "",
                        "file_name": "",
                        "time_stamp": "2022-10-17T19:00:00Z",
                        "time_start": "2022-10-17T19:00:00Z",
                        "time_end": "2022-10-17T20:15:00Z",
                        "paper_type": "",
                        "keywords": [],
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    }
                ]
            },
            {
                "title": "VisInPractice: Integrating Research and Products",
                "session_id": "a-visinpractice-4",
                "event_prefix": "a-visinpractice",
                "track": "ok1",
                "livestream_id": "ok1-mon",
                "session_image": "a-visinpractice-4.png",
                "chair": [
                    "Anamaria Crisan",
                    "Zhicheng (Leo) Liu",
                    "Sudhanshu Kumar Semwal",
                    "Chris Weaver"
                ],
                "organizers": [],
                "time_start": "2022-10-17T20:45:00Z",
                "time_end": "2022-10-17T22:00:00Z",
                "discord_category": "",
                "discord_channel": "oklahoma-st-1",
                "discord_channel_id": "1024600861340606484",
                "discord_link": "https://discord.com/channels/978320435554947082/1024600861340606484",
                "slido_link": "https://app.sli.do/event/i28UETZaiRWr1XwX6eKjcf?section=740d9835-fe47-4cb4-b260-353262a7f8dd",
                "youtube_url": "https://youtu.be/r4A4lla0eXI",
                "youtube_id": "r4A4lla0eXI",
                "zoom_meeting": "",
                "zoom_password": "",
                "zoom_link": "",
                "ff_link": "",
                "ff_playlist": "",
                "time_slots": [
                    {
                        "slot_id": "a-visinpractice-prog-4",
                        "session_id": "a-visinpractice-4",
                        "type": "Mixed Panel (virtual/in person)",
                        "title": "Panel: Integrating Research and Products",
                        "contributors": [
                            "Richard Brath",
                            "Liu Ren",
                            "Vidya Setlur",
                            "Dominik Moritz"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "",
                        "file_name": "",
                        "time_stamp": "2022-10-17T20:45:00Z",
                        "time_start": "2022-10-17T20:45:00Z",
                        "time_end": "2022-10-17T22:00:00Z",
                        "paper_type": "",
                        "keywords": [],
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    }
                ]
            }
        ]
    },
    "a-vast": {
        "event": "VAST Challenge",
        "long_name": "VAST Challenge",
        "event_type": "Associated Event (Competition)",
        "event_prefix": "a-vast",
        "event_description": "",
        "event_url": "",
        "organizers": [
            "Kristin Cook",
            "R. Jordan Crouser"
        ],
        "sessions": [
            {
                "title": "VAST Challenge: Opening, Challenge 1, and Start of Challenge 2",
                "session_id": "a-vast-1",
                "event_prefix": "a-vast",
                "track": "ok6",
                "livestream_id": "ok6-sun",
                "session_image": "a-vast-1.png",
                "chair": [
                    "Kristin Cook",
                    "R. Jordan Crouser",
                    "Steve Gomez"
                ],
                "organizers": [],
                "time_start": "2022-10-16T14:00:00Z",
                "time_end": "2022-10-16T15:15:00Z",
                "discord_category": "",
                "discord_channel": "oklahoma-st-6",
                "discord_channel_id": "1024600906689421372",
                "discord_link": "https://discord.com/channels/978320435554947082/1024600906689421372",
                "slido_link": "https://app.sli.do/event/i28UETZaiRWr1XwX6eKjcf?section=29ba1f79-04cd-4568-a83c-ab81aa3c28b8",
                "youtube_url": "https://youtu.be/36kdnGXsSvM",
                "youtube_id": "36kdnGXsSvM",
                "zoom_meeting": "",
                "zoom_password": "",
                "zoom_link": "",
                "ff_link": "",
                "ff_playlist": "",
                "time_slots": [
                    {
                        "slot_id": "a-vast-prog-1",
                        "session_id": "a-vast-1",
                        "type": "In Person Other",
                        "title": "Opening Presentation",
                        "contributors": [
                            "R. Jordan Crouser"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "",
                        "file_name": "",
                        "time_stamp": "2022-10-16T14:00:00Z",
                        "time_start": "2022-10-16T14:00:00Z",
                        "time_end": "2022-10-16T14:15:00Z",
                        "paper_type": "",
                        "keywords": [],
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "a-vast-1032-pres",
                        "session_id": "a-vast-1",
                        "type": "Virtual Presentation (pre-recorded)",
                        "title": "Fudan University #1032",
                        "contributors": [
                            "Yuxiao Li"
                        ],
                        "authors": [
                            "Yuxiao Li",
                            "Xuexi Wang",
                            "Yue Wang",
                            "Siming Chen",
                            "Ting Liu",
                            "Ziyue Lin",
                            "Huiting Wang"
                        ],
                        "abstract": "",
                        "uid": "a-vast-posters-1032",
                        "file_name": "a-vast-1032_Li_Presentation.mp4",
                        "time_stamp": "2022-10-16T14:15:00Z",
                        "time_start": "2022-10-16T14:15:00Z",
                        "time_end": "2022-10-16T14:30:00Z",
                        "paper_type": "",
                        "keywords": [],
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "a-vast-1032-qa",
                        "session_id": "a-vast-1",
                        "type": "Virtual Q+A",
                        "title": "Fudan University #1032 (Q+A)",
                        "contributors": [
                            "Yuxiao Li"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "a-vast-posters-1032",
                        "file_name": "",
                        "time_stamp": "2022-10-16T14:30:00Z",
                        "time_start": "2022-10-16T14:30:00Z",
                        "time_end": "2022-10-16T14:35:00Z",
                        "paper_type": "",
                        "keywords": [],
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "a-vast-1013-pres",
                        "session_id": "a-vast-1",
                        "type": "Virtual Presentation (pre-recorded)",
                        "title": "TU Darmstadt and Fraunhofer #1013",
                        "contributors": [
                            "Jan Burmeister"
                        ],
                        "authors": [
                            "Jan Burmeister",
                            "Jilin Liao",
                            "Jieqing Yang",
                            "Qingtian Wei",
                            "Kexin Wang"
                        ],
                        "abstract": "",
                        "uid": "a-vast-1013",
                        "file_name": "a-vast-1013_Burmeister_Presentation.mp4",
                        "time_stamp": "2022-10-16T14:35:00Z",
                        "time_start": "2022-10-16T14:35:00Z",
                        "time_end": "2022-10-16T14:47:00Z",
                        "paper_type": "associated",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "780",
                        "paper_award": "",
                        "image_caption": "One dashboard of our visual analytics application for the VAST Challenge task 1.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "a-vast-1013-qa",
                        "session_id": "a-vast-1",
                        "type": "Virtual Q+A",
                        "title": "TU Darmstadt and Fraunhofer #1013 (Q+A)",
                        "contributors": [
                            "Jan Burmeister"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "a-vast-1013",
                        "file_name": "",
                        "time_stamp": "2022-10-16T14:47:00Z",
                        "time_start": "2022-10-16T14:47:00Z",
                        "time_end": "2022-10-16T14:50:00Z",
                        "paper_type": "associated",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "780",
                        "paper_award": "",
                        "image_caption": "One dashboard of our visual analytics application for the VAST Challenge task 1.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "a-vast-prog-2",
                        "session_id": "a-vast-1",
                        "type": "In Person Other",
                        "title": "Challenge 2 Introduction",
                        "contributors": [
                            "Steve Gomez"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "",
                        "file_name": "",
                        "time_stamp": "2022-10-16T14:50:00Z",
                        "time_start": "2022-10-16T14:50:00Z",
                        "time_end": "2022-10-16T14:55:00Z",
                        "paper_type": "",
                        "keywords": [],
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "a-vast-1017-pres",
                        "session_id": "a-vast-1",
                        "type": "Virtual Presentation (pre-recorded)",
                        "title": "giCentre, City, University of London #1017",
                        "contributors": [
                            "Jo Wood"
                        ],
                        "authors": [
                            "Jo Wood"
                        ],
                        "abstract": "The VAST 2022 Challenge 1 put us in place of an urban planning analyst for the fictitious city Engagement, Ohio. Given a large, interlinked dataset of its citizens, businesses and activities, we were tasked to assemble a summary of the state of Engagement to serve as basis for future investments. We present a visual analytics application to interactively explore this complex dataset using a combination of dashboards and a 2D/3D map. Our application uses interactive visualizations to iteratively build and analyze subgroups, sync their abstract data with spatial positions, and detect clusters and patterns in the data.",
                        "uid": "a-vast-1017",
                        "file_name": "a-vast-1017_Wood_Presentation.mp4",
                        "time_stamp": "2022-10-16T14:55:00Z",
                        "time_start": "2022-10-16T14:55:00Z",
                        "time_end": "2022-10-16T15:10:00Z",
                        "paper_type": "associated",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "1037",
                        "paper_award": "",
                        "image_caption": "VAST Challenge: Exploring urban dynamics with literate visual analytics. This presentation describes how the VAST Challenge Two task was completed using interactive notebooks to document the visual analytic and design process. The work won the VAST Award for \"Comprehensive Challenge 2 Solution\".",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "a-vast-1017-qa",
                        "session_id": "a-vast-1",
                        "type": "Virtual Q+A",
                        "title": "giCentre, City, University of London #1017 (Q+A)",
                        "contributors": [
                            "Jo Wood"
                        ],
                        "authors": [],
                        "abstract": "The VAST 2022 Challenge 1 put us in place of an urban planning analyst for the fictitious city Engagement, Ohio. Given a large, interlinked dataset of its citizens, businesses and activities, we were tasked to assemble a summary of the state of Engagement to serve as basis for future investments. We present a visual analytics application to interactively explore this complex dataset using a combination of dashboards and a 2D/3D map. Our application uses interactive visualizations to iteratively build and analyze subgroups, sync their abstract data with spatial positions, and detect clusters and patterns in the data.",
                        "uid": "a-vast-1017",
                        "file_name": "",
                        "time_stamp": "2022-10-16T15:10:00Z",
                        "time_start": "2022-10-16T15:10:00Z",
                        "time_end": "2022-10-16T15:15:00Z",
                        "paper_type": "associated",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "1037",
                        "paper_award": "",
                        "image_caption": "VAST Challenge: Exploring urban dynamics with literate visual analytics. This presentation describes how the VAST Challenge Two task was completed using interactive notebooks to document the visual analytic and design process. The work won the VAST Award for \"Comprehensive Challenge 2 Solution\".",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    }
                ]
            },
            {
                "title": "VAST Challenge: Challenges 2 and 3",
                "session_id": "a-vast-2",
                "event_prefix": "a-vast",
                "track": "ok6",
                "livestream_id": "ok6-sun",
                "session_image": "a-vast-2.png",
                "chair": [
                    "Kristin Cook",
                    "R. Jordan Crouser",
                    "Steve Gomez"
                ],
                "organizers": [],
                "time_start": "2022-10-16T15:45:00Z",
                "time_end": "2022-10-16T17:00:00Z",
                "discord_category": "",
                "discord_channel": "oklahoma-st-6",
                "discord_channel_id": "1024600906689421372",
                "discord_link": "https://discord.com/channels/978320435554947082/1024600906689421372",
                "slido_link": "https://app.sli.do/event/i28UETZaiRWr1XwX6eKjcf?section=29ba1f79-04cd-4568-a83c-ab81aa3c28b8",
                "youtube_url": "https://youtu.be/36kdnGXsSvM",
                "youtube_id": "36kdnGXsSvM",
                "zoom_meeting": "",
                "zoom_password": "",
                "zoom_link": "",
                "ff_link": "",
                "ff_playlist": "",
                "time_slots": [
                    {
                        "slot_id": "a-vast-1004-pres",
                        "session_id": "a-vast-2",
                        "type": "Virtual Presentation (pre-recorded)",
                        "title": "SAS Institute #1004",
                        "contributors": [
                            "Falko Schulz"
                        ],
                        "authors": [
                            "Falko Schulz",
                            "Don Chapman",
                            "Steven Harenberg",
                            "Stu Sztukowski",
                            "Cheryl LeSaint"
                        ],
                        "abstract": "",
                        "uid": "a-vast-1004",
                        "file_name": "a-vast-1004_Schulz_Presentation.mp4",
                        "time_stamp": "2022-10-16T15:45:00Z",
                        "time_start": "2022-10-16T15:45:00Z",
                        "time_end": "2022-10-16T15:57:00Z",
                        "paper_type": "associated",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "483",
                        "paper_award": "",
                        "image_caption": "Analyzing demographics and patterns-of-life using SAS Visual Analytics. A presentation from SAS about the submission for VAST 2022.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "a-vast-1004-qa",
                        "session_id": "a-vast-2",
                        "type": "Virtual Q+A",
                        "title": "SAS Institute #1004 (Q+A)",
                        "contributors": [
                            "Falko Schulz"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "a-vast-1004",
                        "file_name": "",
                        "time_stamp": "2022-10-16T15:57:00Z",
                        "time_start": "2022-10-16T15:57:00Z",
                        "time_end": "2022-10-16T16:00:00Z",
                        "paper_type": "associated",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "483",
                        "paper_award": "",
                        "image_caption": "Analyzing demographics and patterns-of-life using SAS Visual Analytics. A presentation from SAS about the submission for VAST 2022.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "vast-1012-pres",
                        "session_id": "a-vast-2",
                        "type": "Virtual Presentation (pre-recorded)",
                        "title": "Purdue University #1012",
                        "contributors": [
                            "William Fei",
                            "Hao Chang",
                            "Yawen Lu"
                        ],
                        "authors": [
                            "\u0443awen lu",
                            "Hao Wang",
                            "Xingyu Jiang",
                            "Tianyi Zhang",
                            "William Fei",
                            "Zhenyu Qian",
                            "Yingjie Chen"
                        ],
                        "abstract": "",
                        "uid": "a-vast-posters-1012",
                        "file_name": "a-vast-1012_Lu_Presentation.mp4",
                        "time_stamp": "2022-10-16T16:00:00Z",
                        "time_start": "2022-10-16T16:00:00Z",
                        "time_end": "2022-10-16T16:12:00Z",
                        "paper_type": "",
                        "keywords": [],
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "vast-1012-qa",
                        "session_id": "a-vast-2",
                        "type": "Virtual Q+A",
                        "title": "Purdue University #1012 (Q+A)",
                        "contributors": [
                            "William Fei",
                            "Hao Chang",
                            "Yawen Lu"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "a-vast-posters-1012",
                        "file_name": "",
                        "time_stamp": "2022-10-16T16:12:00Z",
                        "time_start": "2022-10-16T16:12:00Z",
                        "time_end": "2022-10-16T16:15:00Z",
                        "paper_type": "",
                        "keywords": [],
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "a-vast-prog-3",
                        "session_id": "a-vast-2",
                        "type": "In Person Presentation",
                        "title": "Challenge 3 Introduction",
                        "contributors": [
                            "Steve Gomez"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "",
                        "file_name": "",
                        "time_stamp": "2022-10-16T16:15:00Z",
                        "time_start": "2022-10-16T16:15:00Z",
                        "time_end": "2022-10-16T16:20:00Z",
                        "paper_type": "",
                        "keywords": [],
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "a-vast-1033-pres",
                        "session_id": "a-vast-2",
                        "type": "Virtual Presentation (pre-recorded)",
                        "title": "Fudan University #1033",
                        "contributors": [
                            "Ting Liu"
                        ],
                        "authors": [
                            "Ting Liu",
                            "Huiting Wang",
                            "Ziyue Lin",
                            "Yuxiao Li",
                            "Xuexi Wang",
                            "Yue Wang",
                            "Siming Chen"
                        ],
                        "abstract": "",
                        "uid": "a-vast-posters-1033",
                        "file_name": "a-vast-1033_Liu_Presentation.mp4",
                        "time_stamp": "2022-10-16T16:20:00Z",
                        "time_start": "2022-10-16T16:20:00Z",
                        "time_end": "2022-10-16T16:35:00Z",
                        "paper_type": "",
                        "keywords": [],
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "a-vast-1033-qa",
                        "session_id": "a-vast-2",
                        "type": "Virtual Q+A",
                        "title": "Fudan University #1033 (Q+A)",
                        "contributors": [
                            "Ting Liu"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "a-vast-posters-1033",
                        "file_name": "",
                        "time_stamp": "2022-10-16T16:35:00Z",
                        "time_start": "2022-10-16T16:35:00Z",
                        "time_end": "2022-10-16T16:40:00Z",
                        "paper_type": "",
                        "keywords": [],
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "a-vast-1024-pres",
                        "session_id": "a-vast-2",
                        "type": "In Person Presentation",
                        "title": "VRVis and Virginia Tech #1024",
                        "contributors": [
                            "Kresimir Matkovic"
                        ],
                        "authors": [
                            "Rainer Splechtna",
                            "Thomas Hulka",
                            "Disha Sardana",
                            "Nikitha Donekal Chandrashekar",
                            "Denis Gracanin",
                            "Kresimir Matkovic"
                        ],
                        "abstract": "",
                        "uid": "a-vast-posters-1024",
                        "file_name": "a-vast-1024_Splechtna_Presentation.mp4",
                        "time_stamp": "2022-10-16T16:40:00Z",
                        "time_start": "2022-10-16T16:40:00Z",
                        "time_end": "2022-10-16T16:52:00Z",
                        "paper_type": "",
                        "keywords": [],
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "a-vast-1024-qa",
                        "session_id": "a-vast-2",
                        "type": "In Person Q+A",
                        "title": "VRVis and Virginia Tech #1024 (Q+A)",
                        "contributors": [
                            "Kresimir Matkovic"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "a-vast-posters-1024",
                        "file_name": "",
                        "time_stamp": "2022-10-16T16:52:00Z",
                        "time_start": "2022-10-16T16:52:00Z",
                        "time_end": "2022-10-16T16:55:00Z",
                        "paper_type": "",
                        "keywords": [],
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "a-vast-prog-4",
                        "session_id": "a-vast-2",
                        "type": "In Person Presentation",
                        "title": "Closing",
                        "contributors": [
                            "Steve Gomez",
                            "Jordan Crouser"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "",
                        "file_name": "",
                        "time_stamp": "2022-10-16T16:55:00Z",
                        "time_start": "2022-10-16T16:55:00Z",
                        "time_end": "2022-10-16T17:00:00Z",
                        "paper_type": "",
                        "keywords": [],
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    }
                ]
            }
        ]
    },
    "t-ttk": {
        "event": "Topological Analysis of Ensemble Scalar Data with TTK, A Sequel",
        "long_name": "Topological Analysis of Ensemble Scalar Data with TTK, A Sequel",
        "event_type": "Tutorial",
        "event_prefix": "t-ttk",
        "event_description": "",
        "event_url": "",
        "organizers": [
            "Christoph Garth",
            "Charles Gueunet",
            "Pierre Guillou",
            "Federico Iuricich",
            "Joshua Levine",
            "Jonas Lukasczyk",
            "Mathieu Pont",
            "Julien Tierny",
            "Jules Vidal",
            "Bei Wang",
            "Florian Wetzels"
        ],
        "sessions": [
            {
                "title": "Topological Analysis of Ensemble Scalar Data with TTK, A Sequel 1",
                "session_id": "t-ttk-1",
                "event_prefix": "t-ttk",
                "track": "ok6",
                "livestream_id": "ok6-mon",
                "session_image": "t-ttk-1.png",
                "chair": [
                    "Christoph Garth",
                    "Charles Gueunet",
                    "Pierre Guillou",
                    "Federico Iuricich",
                    "Joshua Levine",
                    "Jonas Lukasczyk",
                    "Mathieu Pont",
                    "Julien Tierny",
                    "Jules Vidal",
                    "Bei Wang",
                    "Florian Wetzels"
                ],
                "organizers": [],
                "time_start": "2022-10-17T14:00:00Z",
                "time_end": "2022-10-17T15:15:00Z",
                "discord_category": "",
                "discord_channel": "oklahoma-st-6",
                "discord_channel_id": "1024600906689421372",
                "discord_link": "https://discord.com/channels/978320435554947082/1024600906689421372",
                "slido_link": "https://app.sli.do/event/i28UETZaiRWr1XwX6eKjcf?section=29ba1f79-04cd-4568-a83c-ab81aa3c28b8",
                "youtube_url": "https://youtu.be/ny0kgoMbOfg",
                "youtube_id": "ny0kgoMbOfg",
                "zoom_meeting": "",
                "zoom_password": "",
                "zoom_link": "",
                "ff_link": "",
                "ff_playlist": "",
                "time_slots": [
                    {
                        "slot_id": "t-ttk-prog-1",
                        "session_id": "t-ttk-1",
                        "type": "In Person Presentation",
                        "title": "Topological Analysis of Ensemble Scalar Data with TTK, A Sequel 1",
                        "contributors": [
                            "Christoph Garth",
                            "Charles Gueunet",
                            "Pierre Guillou",
                            "Federico Iuricich",
                            "Joshua Levine",
                            "Jonas Lukasczyk",
                            "Mathieu Pont",
                            "Julien Tierny",
                            "Jules Vidal",
                            "Bei Wang",
                            "Florian Wetzels"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "",
                        "file_name": "",
                        "time_stamp": "2022-10-17T14:00:00Z",
                        "time_start": "2022-10-17T14:00:00Z",
                        "time_end": "2022-10-17T15:15:00Z",
                        "paper_type": "",
                        "keywords": [],
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    }
                ]
            },
            {
                "title": "Topological Analysis of Ensemble Scalar Data with TTK, A Sequel 2",
                "session_id": "t-ttk-2",
                "event_prefix": "t-ttk",
                "track": "ok6",
                "livestream_id": "ok6-mon",
                "session_image": "t-ttk-2.png",
                "chair": [
                    "Christoph Garth",
                    "Charles Gueunet",
                    "Pierre Guillou",
                    "Federico Iuricich",
                    "Joshua Levine",
                    "Jonas Lukasczyk",
                    "Mathieu Pont",
                    "Julien Tierny",
                    "Jules Vidal",
                    "Bei Wang",
                    "Florian Wetzels"
                ],
                "organizers": [],
                "time_start": "2022-10-17T15:45:00Z",
                "time_end": "2022-10-17T17:00:00Z",
                "discord_category": "",
                "discord_channel": "oklahoma-st-6",
                "discord_channel_id": "1024600906689421372",
                "discord_link": "https://discord.com/channels/978320435554947082/1024600906689421372",
                "slido_link": "https://app.sli.do/event/i28UETZaiRWr1XwX6eKjcf?section=29ba1f79-04cd-4568-a83c-ab81aa3c28b8",
                "youtube_url": "https://youtu.be/ny0kgoMbOfg",
                "youtube_id": "ny0kgoMbOfg",
                "zoom_meeting": "",
                "zoom_password": "",
                "zoom_link": "",
                "ff_link": "",
                "ff_playlist": "",
                "time_slots": [
                    {
                        "slot_id": "t-ttk-prog-2",
                        "session_id": "t-ttk-2",
                        "type": "In Person Presentation",
                        "title": "Topological Analysis of Ensemble Scalar Data with TTK, A Sequel 2",
                        "contributors": [
                            "Christoph Garth",
                            "Charles Gueunet",
                            "Pierre Guillou",
                            "Federico Iuricich",
                            "Joshua Levine",
                            "Jonas Lukasczyk",
                            "Mathieu Pont",
                            "Julien Tierny",
                            "Jules Vidal",
                            "Bei Wang",
                            "Florian Wetzels"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "",
                        "file_name": "",
                        "time_stamp": "2022-10-17T15:45:00Z",
                        "time_start": "2022-10-17T15:45:00Z",
                        "time_end": "2022-10-17T17:00:00Z",
                        "paper_type": "",
                        "keywords": [],
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    }
                ]
            }
        ]
    },
    "w-topoinvis": {
        "event": "TopoInVis: Topological Data Analysis and Visualization",
        "long_name": "TopoInVis: Topological Data Analysis and Visualization",
        "event_type": "Workshop",
        "event_prefix": "w-topoinvis",
        "event_description": "",
        "event_url": "",
        "organizers": [
            "Talha Bin Masood",
            "Vijay Natarajan",
            "Paul Rosen",
            "Julien Tierny"
        ],
        "sessions": [
            {
                "title": "TopoInVis: Opening, Keynote + Session 1",
                "session_id": "w-topoinvis-1",
                "event_prefix": "w-topoinvis",
                "track": "ok6",
                "livestream_id": "ok6-mon",
                "session_image": "w-topoinvis-1.png",
                "chair": [
                    "Julien Tierny",
                    "Brian Summa"
                ],
                "organizers": [],
                "time_start": "2022-10-17T19:00:00Z",
                "time_end": "2022-10-17T20:15:00Z",
                "discord_category": "",
                "discord_channel": "oklahoma-st-6",
                "discord_channel_id": "1024600906689421372",
                "discord_link": "https://discord.com/channels/978320435554947082/1024600906689421372",
                "slido_link": "https://app.sli.do/event/i28UETZaiRWr1XwX6eKjcf?section=29ba1f79-04cd-4568-a83c-ab81aa3c28b8",
                "youtube_url": "https://youtu.be/ny0kgoMbOfg",
                "youtube_id": "ny0kgoMbOfg",
                "zoom_meeting": "",
                "zoom_password": "",
                "zoom_link": "",
                "ff_link": "",
                "ff_playlist": "",
                "time_slots": [
                    {
                        "slot_id": "w-topoinvis-prog-1",
                        "session_id": "w-topoinvis-1",
                        "type": "In Person Other",
                        "title": "Opening Remarks",
                        "contributors": [
                            "Julien Tierny",
                            "Paul Rosen"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "",
                        "file_name": "",
                        "time_stamp": "2022-10-17T19:00:00Z",
                        "time_start": "2022-10-17T19:00:00Z",
                        "time_end": "2022-10-17T19:05:00Z",
                        "paper_type": "",
                        "keywords": [],
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "w-topoinvis-prog-2",
                        "session_id": "w-topoinvis-1",
                        "type": "In Person Presentation",
                        "title": "Keynote by Robert Ghrist",
                        "contributors": [
                            "Robert Ghrist"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "",
                        "file_name": "",
                        "time_stamp": "2022-10-17T19:05:00Z",
                        "time_start": "2022-10-17T19:05:00Z",
                        "time_end": "2022-10-17T20:00:00Z",
                        "paper_type": "",
                        "keywords": [],
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "w-topoinvis-1026-pres",
                        "session_id": "w-topoinvis-1",
                        "type": "Virtual Presentation (pre-recorded)",
                        "title": "Fast Merge Tree Computation via SYCL",
                        "contributors": [
                            "Arnur Nigmetov"
                        ],
                        "authors": [
                            "Arnur Nigmetov",
                            "Dmitriy Morozov"
                        ],
                        "abstract": "A merge tree is a topological descriptor of a real-valued function. Merge trees are used in visualization and topological data analysis, either directly or as a means to another end: computing a 0-dimensional persistence diagram, identifying connected components, performing topological simplification, etc. Scientific computing relies more and more on GPUs to achieve fast, scalable computation. For efficiency, data analysis should take place at the same location as the main computation, which motivates interest in parallel algorithms and portable software for merge trees that can run not only on a CPU, but also on a GPU, or other types of accelerators. The SYCL standard defines a programming model that allows the same code, written in standard C++, to compile targets for multiple parallel backends (CPUs via OpenMP or TBB, NVIDIA GPUs via CUDA, AMD GPUs via ROCm, Intel GPUs via Level Zero, FPGAs). In this paper, we adapt the triplet merge tree algorithm to SYCL and compare our implementation with the \\vtkm implementation, which is the only other implementation of merge trees for GPUs that we know of.",
                        "uid": "w-topoinvis-1026",
                        "file_name": "w-topoinvis-1026_Nigmetov_Presentation.mp4",
                        "time_stamp": "2022-10-17T20:00:00Z",
                        "time_start": "2022-10-17T20:00:00Z",
                        "time_end": "2022-10-17T20:04:00Z",
                        "paper_type": "workshop",
                        "keywords": [
                            "triplet merge tree, computations on GPU, SYCL"
                        ],
                        "has_image": "1",
                        "has_video": "249",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "w-topoinvis-1026-qa",
                        "session_id": "w-topoinvis-1",
                        "type": "Virtual Q+A",
                        "title": "Fast Merge Tree Computation via SYCL (Q+A)",
                        "contributors": [
                            "Arnur Nigmetov"
                        ],
                        "authors": [],
                        "abstract": "A merge tree is a topological descriptor of a real-valued function. Merge trees are used in visualization and topological data analysis, either directly or as a means to another end: computing a 0-dimensional persistence diagram, identifying connected components, performing topological simplification, etc. Scientific computing relies more and more on GPUs to achieve fast, scalable computation. For efficiency, data analysis should take place at the same location as the main computation, which motivates interest in parallel algorithms and portable software for merge trees that can run not only on a CPU, but also on a GPU, or other types of accelerators. The SYCL standard defines a programming model that allows the same code, written in standard C++, to compile targets for multiple parallel backends (CPUs via OpenMP or TBB, NVIDIA GPUs via CUDA, AMD GPUs via ROCm, Intel GPUs via Level Zero, FPGAs). In this paper, we adapt the triplet merge tree algorithm to SYCL and compare our implementation with the \\vtkm implementation, which is the only other implementation of merge trees for GPUs that we know of.",
                        "uid": "w-topoinvis-1026",
                        "file_name": "",
                        "time_stamp": "2022-10-17T20:04:00Z",
                        "time_start": "2022-10-17T20:04:00Z",
                        "time_end": "2022-10-17T20:07:00Z",
                        "paper_type": "workshop",
                        "keywords": [
                            "triplet merge tree, computations on GPU, SYCL"
                        ],
                        "has_image": "1",
                        "has_video": "249",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "w-topoinvis-1010-pres",
                        "session_id": "w-topoinvis-1",
                        "type": "In Person Presentation",
                        "title": "Counterexamples expose gaps in the proof of time complexity for cover trees introduced in 2006",
                        "contributors": [
                            "Vitaliy Kurlin"
                        ],
                        "authors": [
                            "Yury Elkin",
                            "Vitaliy Kurlin"
                        ],
                        "abstract": "This paper is motivated by the k-nearest neighbors search: given an arbitrary metric space, and its finite subsets (a reference set R and a query set Q), design a fast algorithm to find all k-nearest neighbors in R for every point q in Q. In 2006, Beygelzimer, Kakade, and Langford introduced cover trees to justify a near-linear time complexity for the neighbor search in the sizes of Q,R.   Section 5.3 of Curtin's PhD (2015) pointed out that the proof of this result was wrong.  The key step in the original proof attempted to show that the number of iterations can be estimated by multiplying the length of the longest root-to-leaf path in a cover tree by a constant factor. However, this estimate can miss many potential nodes in several branches of a cover tree, that should be considered during the neighbor search. The same argument was unfortunately repeated in several subsequent papers using cover trees from 2006.  This paper explicitly constructs challenging datasets that provide counterexamples to the past proofs of time complexity for the cover tree construction, the k-nearest neighbor search presented at ICML 2006, and the dual-tree search algorithm published in NIPS 2009.  The corrected near-linear time complexities with extra parameters are proved in another forthcoming paper by using a new compressed cover tree simplifying the original tree structure.",
                        "uid": "w-topoinvis-1010",
                        "file_name": "w-topoinvis-1010_Elkin_Presentation.mp4",
                        "time_stamp": "2022-10-17T20:07:00Z",
                        "time_start": "2022-10-17T20:07:00Z",
                        "time_end": "2022-10-17T20:11:00Z",
                        "paper_type": "workshop",
                        "keywords": [
                            "Cover tree, k-nearest neighbors, counterexample, compressed cover tree, dual-tree algorithms, parametrized time complexity"
                        ],
                        "has_image": "1",
                        "has_video": "287",
                        "paper_award": "",
                        "image_caption": "A comparison of different cover trees built on datasets from Example 2.4. \\textbf{Left:} an implicit cover tree introduced in 2006 contains infinite repetitions of given points, see Definition 2.1. \\textbf{Middle:} an explicit cover tree still includes repeated points, see Definition 2.2 \\textbf{Right:} a new compressed cover tree is much smaller and includes each point only once, see [18,Definition 3.5].",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "w-topoinvis-1010-qa",
                        "session_id": "w-topoinvis-1",
                        "type": "In Person Q+A",
                        "title": "Counterexamples expose gaps in the proof of time complexity for cover trees introduced in 2006 (Q+A)",
                        "contributors": [
                            "Vitaliy Kurlin"
                        ],
                        "authors": [],
                        "abstract": "This paper is motivated by the k-nearest neighbors search: given an arbitrary metric space, and its finite subsets (a reference set R and a query set Q), design a fast algorithm to find all k-nearest neighbors in R for every point q in Q. In 2006, Beygelzimer, Kakade, and Langford introduced cover trees to justify a near-linear time complexity for the neighbor search in the sizes of Q,R.   Section 5.3 of Curtin's PhD (2015) pointed out that the proof of this result was wrong.  The key step in the original proof attempted to show that the number of iterations can be estimated by multiplying the length of the longest root-to-leaf path in a cover tree by a constant factor. However, this estimate can miss many potential nodes in several branches of a cover tree, that should be considered during the neighbor search. The same argument was unfortunately repeated in several subsequent papers using cover trees from 2006.  This paper explicitly constructs challenging datasets that provide counterexamples to the past proofs of time complexity for the cover tree construction, the k-nearest neighbor search presented at ICML 2006, and the dual-tree search algorithm published in NIPS 2009.  The corrected near-linear time complexities with extra parameters are proved in another forthcoming paper by using a new compressed cover tree simplifying the original tree structure.",
                        "uid": "w-topoinvis-1010",
                        "file_name": "",
                        "time_stamp": "2022-10-17T20:11:00Z",
                        "time_start": "2022-10-17T20:11:00Z",
                        "time_end": "2022-10-17T20:14:00Z",
                        "paper_type": "workshop",
                        "keywords": [
                            "Cover tree, k-nearest neighbors, counterexample, compressed cover tree, dual-tree algorithms, parametrized time complexity"
                        ],
                        "has_image": "1",
                        "has_video": "287",
                        "paper_award": "",
                        "image_caption": "A comparison of different cover trees built on datasets from Example 2.4. \\textbf{Left:} an implicit cover tree introduced in 2006 contains infinite repetitions of given points, see Definition 2.1. \\textbf{Middle:} an explicit cover tree still includes repeated points, see Definition 2.2 \\textbf{Right:} a new compressed cover tree is much smaller and includes each point only once, see [18,Definition 3.5].",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "",
                        "ff_id": ""
                    }
                ]
            },
            {
                "title": "TopoInVis: Session 2, Early Career Lightning Talks + Best Paper Awards ",
                "session_id": "w-topoinvis-2",
                "event_prefix": "w-topoinvis",
                "track": "ok6",
                "livestream_id": "ok6-mon",
                "session_image": "w-topoinvis-2.png",
                "chair": [
                    "Brian Summa",
                    "Christoph Garth",
                    "Federico Luricich",
                    "Paul Rosen"
                ],
                "organizers": [],
                "time_start": "2022-10-17T20:45:00Z",
                "time_end": "2022-10-17T22:00:00Z",
                "discord_category": "",
                "discord_channel": "oklahoma-st-6",
                "discord_channel_id": "1024600906689421372",
                "discord_link": "https://discord.com/channels/978320435554947082/1024600906689421372",
                "slido_link": "https://app.sli.do/event/i28UETZaiRWr1XwX6eKjcf?section=29ba1f79-04cd-4568-a83c-ab81aa3c28b8",
                "youtube_url": "https://youtu.be/ny0kgoMbOfg",
                "youtube_id": "ny0kgoMbOfg",
                "zoom_meeting": "",
                "zoom_password": "",
                "zoom_link": "",
                "ff_link": "",
                "ff_playlist": "",
                "time_slots": [
                    {
                        "slot_id": "w-topoinvis-1004-pres",
                        "session_id": "w-topoinvis-2",
                        "type": "In Person Presentation",
                        "title": "Autoencoder-Aided Visualization of Collections of Morse Complexes",
                        "contributors": [
                            "Jixian Li"
                        ],
                        "authors": [
                            "Jixian Li",
                            "Dan Van Boxel",
                            "Joshua Levine"
                        ],
                        "abstract": "Though analyzing a single scalar field using Morse complexes is well studied, there are few techniques for visualizing a collection of Morse complexes. We focus on analyses that are enabled by looking at a Morse complex as an embedded domain decomposition. Specifically, we target 2D scalar fields, and we encode the Morse complex through binary images of the boundaries of decomposition. Then we use image-based autoencoders to create a feature space for the Morse complexes. We apply additional dimensionality reduction methods to construct a scatterplot as a visual interface of the feature space. This allows us to investigate individual Morse complexes, as they relate to the collection, through interaction with the scatterplot. We demonstrate our approach using a synthetic data set, microscopy images, and time-varying vorticity magnitude fields of flow.  Through these, we show that our method can produce insights about structures within the collection of Morse complexes.",
                        "uid": "w-topoinvis-1004",
                        "file_name": "w-topoinvis-1004_Li_Presentation.mp4",
                        "time_stamp": "2022-10-17T20:45:00Z",
                        "time_start": "2022-10-17T20:45:00Z",
                        "time_end": "2022-10-17T20:49:00Z",
                        "paper_type": "workshop",
                        "keywords": [
                            "Morse complex, Dimensionality reduction, Autoencoders"
                        ],
                        "has_image": "1",
                        "has_video": "238",
                        "paper_award": "",
                        "image_caption": "From a collection of scalar fields, we first compute the arcs of their Morse complexes. Then we convert the arcs of Morse complexes into image-based encoding to obtain a set of images of arcs. Next, we train an autoencoder to reconstruct those images. The autoencoder learns latent representations of those images of arcs. We use those latent representations as a proxy for the geometry of Morse complexes. Projecting those latent representations into a visual space helps us study the collection of Morse complexes.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "w-topoinvis-1004-qa",
                        "session_id": "w-topoinvis-2",
                        "type": "In Person Q+A",
                        "title": "Autoencoder-Aided Visualization of Collections of Morse Complexes (Q+A)",
                        "contributors": [
                            "Jixian Li"
                        ],
                        "authors": [],
                        "abstract": "Though analyzing a single scalar field using Morse complexes is well studied, there are few techniques for visualizing a collection of Morse complexes. We focus on analyses that are enabled by looking at a Morse complex as an embedded domain decomposition. Specifically, we target 2D scalar fields, and we encode the Morse complex through binary images of the boundaries of decomposition. Then we use image-based autoencoders to create a feature space for the Morse complexes. We apply additional dimensionality reduction methods to construct a scatterplot as a visual interface of the feature space. This allows us to investigate individual Morse complexes, as they relate to the collection, through interaction with the scatterplot. We demonstrate our approach using a synthetic data set, microscopy images, and time-varying vorticity magnitude fields of flow.  Through these, we show that our method can produce insights about structures within the collection of Morse complexes.",
                        "uid": "w-topoinvis-1004",
                        "file_name": "",
                        "time_stamp": "2022-10-17T20:49:00Z",
                        "time_start": "2022-10-17T20:49:00Z",
                        "time_end": "2022-10-17T20:52:00Z",
                        "paper_type": "workshop",
                        "keywords": [
                            "Morse complex, Dimensionality reduction, Autoencoders"
                        ],
                        "has_image": "1",
                        "has_video": "238",
                        "paper_award": "",
                        "image_caption": "From a collection of scalar fields, we first compute the arcs of their Morse complexes. Then we convert the arcs of Morse complexes into image-based encoding to obtain a set of images of arcs. Next, we train an autoencoder to reconstruct those images. The autoencoder learns latent representations of those images of arcs. We use those latent representations as a proxy for the geometry of Morse complexes. Projecting those latent representations into a visual space helps us study the collection of Morse complexes.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "w-topoinvis-1005-pres",
                        "session_id": "w-topoinvis-2",
                        "type": "In Person Presentation",
                        "title": "A Deformation-based Edit Distance for Merge Trees",
                        "contributors": [
                            "Florian Wetzels"
                        ],
                        "authors": [
                            "Florian Wetzels",
                            "Christoph Garth"
                        ],
                        "abstract": "In scientific visualization, scalar fields are often compared through edit distances between their merge trees. Typical tasks include ensemble analysis, feature tracking and symmetry or periodicity detection. Tree edit distances represent how one tree can be transformed into another through a sequence of simple edit operations: relabeling, insertion and deletion of nodes. In this paper, we present a new set of edit operations working directly on the merge tree as an geometrical or topological object: the represented operations are deformation retractions and inverse transformations on merge trees, which stands in contrast to other methods working on branch decomposition trees. We present a quartic time algorithm for the new edit distance, which is branch decomposition-independent and a metric on the set of all merge trees.",
                        "uid": "w-topoinvis-1005",
                        "file_name": "w-topoinvis-1005_Wetzels_Presentation.mp4",
                        "time_stamp": "2022-10-17T20:52:00Z",
                        "time_start": "2022-10-17T20:52:00Z",
                        "time_end": "2022-10-17T20:56:00Z",
                        "paper_type": "workshop",
                        "keywords": [
                            "Scalar data, Topological data analysis, Merge trees, Edit distance"
                        ],
                        "has_image": "1",
                        "has_video": "209",
                        "paper_award": "",
                        "image_caption": "Illustration of the edit operations for merge trees, on which the new edit distance is based.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "w-topoinvis-1005-qa",
                        "session_id": "w-topoinvis-2",
                        "type": "In Person Q+A",
                        "title": "A Deformation-based Edit Distance for Merge Trees (Q+A)",
                        "contributors": [
                            "Florian Wetzels"
                        ],
                        "authors": [],
                        "abstract": "In scientific visualization, scalar fields are often compared through edit distances between their merge trees. Typical tasks include ensemble analysis, feature tracking and symmetry or periodicity detection. Tree edit distances represent how one tree can be transformed into another through a sequence of simple edit operations: relabeling, insertion and deletion of nodes. In this paper, we present a new set of edit operations working directly on the merge tree as an geometrical or topological object: the represented operations are deformation retractions and inverse transformations on merge trees, which stands in contrast to other methods working on branch decomposition trees. We present a quartic time algorithm for the new edit distance, which is branch decomposition-independent and a metric on the set of all merge trees.",
                        "uid": "w-topoinvis-1005",
                        "file_name": "",
                        "time_stamp": "2022-10-17T20:56:00Z",
                        "time_start": "2022-10-17T20:56:00Z",
                        "time_end": "2022-10-17T20:59:00Z",
                        "paper_type": "workshop",
                        "keywords": [
                            "Scalar data, Topological data analysis, Merge trees, Edit distance"
                        ],
                        "has_image": "1",
                        "has_video": "209",
                        "paper_award": "",
                        "image_caption": "Illustration of the edit operations for merge trees, on which the new edit distance is based.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "w-topoinvis-1002-pres",
                        "session_id": "w-topoinvis-2",
                        "type": "In Person Presentation",
                        "title": "Reduced Connectivity for Local Bilinear Jacobi Sets",
                        "contributors": [
                            "Daniel Kl\u00f6tzl"
                        ],
                        "authors": [
                            "Daniel Kl\u00f6tzl",
                            "Tim Krake",
                            "Youjia Zhou",
                            "Jonathan Stober",
                            "Kathrin Schulte",
                            "Ingrid Hotz",
                            "Bei Wang",
                            "Daniel Weiskopf"
                        ],
                        "abstract": "We present a new topological connection method for the local bilinear computation of Jacobi sets that improves the visual representation while preserving the topological structure and geometric configuration. To this end, the topological structure of the local bilinear method is utilized, which is given by the nerve complex of the traditional piecewise linear method. Since the nerve complex consists of higher-dimensional simplices, the local bilinear method (visually represented by the 1-skeleton of the nerve complex) leads to clutter via crossings of line segments. Therefore, we propose a homotopy-equivalent representation that uses different collapses and edge contractions to remove such artifacts. Our new connectivity method is easy to implement, comes with only little overhead, and results in a less cluttered representation.",
                        "uid": "w-topoinvis-1002",
                        "file_name": "w-topoinvis-1002_Kloetzl_Presentation.mp4",
                        "time_stamp": "2022-10-17T20:59:00Z",
                        "time_start": "2022-10-17T20:59:00Z",
                        "time_end": "2022-10-17T21:03:00Z",
                        "paper_type": "workshop",
                        "keywords": [
                            "Human-centered computing - Visualization - Visualization techniques; Mathematics of computing - Discrete mathematics"
                        ],
                        "has_image": "1",
                        "has_video": "236",
                        "paper_award": "",
                        "image_caption": "The Jacobi set is a topological descriptor that captures gradient alignments and is originally computed via a piecewise linear method. The local bilinear method introduces a more precise approximation of Jacobi sets but leads to clutter via crossings of line segments. Our reduced connectivity for local bilinear Jacobi sets is inspired by different collapsing methods and improves the visual representation while preserving the topological and geometrical structure.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "w-topoinvis-1002-qa",
                        "session_id": "w-topoinvis-2",
                        "type": "In Person Q+A",
                        "title": "Reduced Connectivity for Local Bilinear Jacobi Sets (Q+A)",
                        "contributors": [
                            "Daniel Kl\u00f6tzl"
                        ],
                        "authors": [],
                        "abstract": "We present a new topological connection method for the local bilinear computation of Jacobi sets that improves the visual representation while preserving the topological structure and geometric configuration. To this end, the topological structure of the local bilinear method is utilized, which is given by the nerve complex of the traditional piecewise linear method. Since the nerve complex consists of higher-dimensional simplices, the local bilinear method (visually represented by the 1-skeleton of the nerve complex) leads to clutter via crossings of line segments. Therefore, we propose a homotopy-equivalent representation that uses different collapses and edge contractions to remove such artifacts. Our new connectivity method is easy to implement, comes with only little overhead, and results in a less cluttered representation.",
                        "uid": "w-topoinvis-1002",
                        "file_name": "",
                        "time_stamp": "2022-10-17T21:03:00Z",
                        "time_start": "2022-10-17T21:03:00Z",
                        "time_end": "2022-10-17T21:06:00Z",
                        "paper_type": "workshop",
                        "keywords": [
                            "Human-centered computing - Visualization - Visualization techniques; Mathematics of computing - Discrete mathematics"
                        ],
                        "has_image": "1",
                        "has_video": "236",
                        "paper_award": "",
                        "image_caption": "The Jacobi set is a topological descriptor that captures gradient alignments and is originally computed via a piecewise linear method. The local bilinear method introduces a more precise approximation of Jacobi sets but leads to clutter via crossings of line segments. Our reduced connectivity for local bilinear Jacobi sets is inspired by different collapsing methods and improves the visual representation while preserving the topological and geometrical structure.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "w-topoinvis-1008-pres",
                        "session_id": "w-topoinvis-2",
                        "type": "Virtual Presentation (pre-recorded)",
                        "title": "Jacobi Set Driven Search for Flexible Fiber Surface Extraction",
                        "contributors": [
                            "Mohit Sharma"
                        ],
                        "authors": [
                            "Mohit Sharma",
                            "Vijay Natarajan"
                        ],
                        "abstract": "Isosurfaces are an important tool for analysis and visualization of univariate scalar fields. Earlier works have demonstrated the presence of interesting isosurfaces at isovalues close to critical values. This motivated the development of efficient methods for computing individual components of isosurfaces restricted to a region of interest. Generalization of isosurfaces to fiber surfaces and critical points to Jacobi sets has resulted in new approaches for analyzing bivariate scalar fields. Unlike isosurfaces, there exists no output sensitive method for computing fiber surfaces. Existing methods traverse through all the tetrahedra in the domain. In this paper, we propose the use of the Jacobi set to identify fiber surface components of interest and present an output sensitive approach for its computation. The Jacobi edges are used to initiate the search towards seed tetrahedra that contain the fiber surface, thereby reducing the search space. This approach also leads to effective analysis of the bivariate field by supporting the identification of relevant fiber surfaces near Jacobi edges.",
                        "uid": "w-topoinvis-1008",
                        "file_name": "w-topoinvis-1008_Sharma_Presentation.mp4",
                        "time_stamp": "2022-10-17T21:06:00Z",
                        "time_start": "2022-10-17T21:06:00Z",
                        "time_end": "2022-10-17T21:10:00Z",
                        "paper_type": "workshop",
                        "keywords": [
                            "Human-centered computing\u2014Visualization\u2014Visualization techniques; Human-centered computing\u2014Visualization application domains\u2014Scientific visualization;"
                        ],
                        "has_image": "1",
                        "has_video": "198",
                        "paper_award": "",
                        "image_caption": "Fiber surface components extracted for Ethanediol dataset using Jacobi set driven computation.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "w-topoinvis-1008-qa",
                        "session_id": "w-topoinvis-2",
                        "type": "Virtual Q+A",
                        "title": "Jacobi Set Driven Search for Flexible Fiber Surface Extraction (Q+A)",
                        "contributors": [
                            "Mohit Sharma"
                        ],
                        "authors": [],
                        "abstract": "Isosurfaces are an important tool for analysis and visualization of univariate scalar fields. Earlier works have demonstrated the presence of interesting isosurfaces at isovalues close to critical values. This motivated the development of efficient methods for computing individual components of isosurfaces restricted to a region of interest. Generalization of isosurfaces to fiber surfaces and critical points to Jacobi sets has resulted in new approaches for analyzing bivariate scalar fields. Unlike isosurfaces, there exists no output sensitive method for computing fiber surfaces. Existing methods traverse through all the tetrahedra in the domain. In this paper, we propose the use of the Jacobi set to identify fiber surface components of interest and present an output sensitive approach for its computation. The Jacobi edges are used to initiate the search towards seed tetrahedra that contain the fiber surface, thereby reducing the search space. This approach also leads to effective analysis of the bivariate field by supporting the identification of relevant fiber surfaces near Jacobi edges.",
                        "uid": "w-topoinvis-1008",
                        "file_name": "",
                        "time_stamp": "2022-10-17T21:10:00Z",
                        "time_start": "2022-10-17T21:10:00Z",
                        "time_end": "2022-10-17T21:13:00Z",
                        "paper_type": "workshop",
                        "keywords": [
                            "Human-centered computing\u2014Visualization\u2014Visualization techniques; Human-centered computing\u2014Visualization application domains\u2014Scientific visualization;"
                        ],
                        "has_image": "1",
                        "has_video": "198",
                        "paper_award": "",
                        "image_caption": "Fiber surface components extracted for Ethanediol dataset using Jacobi set driven computation.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "w-topoinvis-1006-pres",
                        "session_id": "w-topoinvis-2",
                        "type": "Virtual Presentation (pre-recorded)",
                        "title": "Discussion and Visualization of Distinguished Hyperbolic Trajectories as a Generalization of Critical Points to 2D Time-dependent Flow",
                        "contributors": [
                            "Roxana Bujack"
                        ],
                        "authors": [
                            "Roxana Bujack"
                        ],
                        "abstract": "Classical vector field topology has proven to be a useful visualization technique for steady flow, but its straightforward application to time-dependent flows lacks physical meaning. Necessary requirements for physical meaningfulness include the results to be objective, i.e., independent of the frame of reference of the observer, and Lagrangian, i.e., that the generalized critical points are trajectories. We analyze whether the theoretical concept of distinguished hyperbolic trajectories provides a physically meaningful generalization to classical critical points and if the existing extraction algorithms correctly compute what has been defined mathematically. We show that both theory and algorithms constitute a significant improvement over previous methods. We further present a method to visualize a time-dependent flow field in the reference frames of distinguished trajectories. The result is easy to interpret because it makes these trajectories look like classical critical points for each instance in time, but it is meaningful because it is Lagrangian and objective.",
                        "uid": "w-topoinvis-1006",
                        "file_name": "w-topoinvis-1006_Bujack_Presentation.mp4",
                        "time_stamp": "2022-10-17T21:13:00Z",
                        "time_start": "2022-10-17T21:13:00Z",
                        "time_end": "2022-10-17T21:17:00Z",
                        "paper_type": "workshop",
                        "keywords": [
                            "visualization, vector field, flow, topology, Lagrangian, objective, time-dependent, distinguished hyperbolic trajectory"
                        ],
                        "has_image": "1",
                        "has_video": "239",
                        "paper_award": "",
                        "image_caption": "Top row: different time steps of a saddle under accelerated rotation visualized using LIC. \nMiddle row: the trajectory of the approximate DHT computed with the algorithm by Ju et al. on top of the instantaneous velocity of the field from its perspective computed with our algorithm. \nBottom row: the trajectory of the true DHT on top of the instantaneous velocity of the field from the DHT\u2019s perspective computed with our algorithm.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "w-topoinvis-1006-qa",
                        "session_id": "w-topoinvis-2",
                        "type": "Virtual Q+A",
                        "title": "Discussion and Visualization of Distinguished Hyperbolic Trajectories as a Generalization of Critical Points to 2D Time-dependent Flow (Q+A)",
                        "contributors": [
                            "Roxana Bujack"
                        ],
                        "authors": [],
                        "abstract": "Classical vector field topology has proven to be a useful visualization technique for steady flow, but its straightforward application to time-dependent flows lacks physical meaning. Necessary requirements for physical meaningfulness include the results to be objective, i.e., independent of the frame of reference of the observer, and Lagrangian, i.e., that the generalized critical points are trajectories. We analyze whether the theoretical concept of distinguished hyperbolic trajectories provides a physically meaningful generalization to classical critical points and if the existing extraction algorithms correctly compute what has been defined mathematically. We show that both theory and algorithms constitute a significant improvement over previous methods. We further present a method to visualize a time-dependent flow field in the reference frames of distinguished trajectories. The result is easy to interpret because it makes these trajectories look like classical critical points for each instance in time, but it is meaningful because it is Lagrangian and objective.",
                        "uid": "w-topoinvis-1006",
                        "file_name": "",
                        "time_stamp": "2022-10-17T21:17:00Z",
                        "time_start": "2022-10-17T21:17:00Z",
                        "time_end": "2022-10-17T21:20:00Z",
                        "paper_type": "workshop",
                        "keywords": [
                            "visualization, vector field, flow, topology, Lagrangian, objective, time-dependent, distinguished hyperbolic trajectory"
                        ],
                        "has_image": "1",
                        "has_video": "239",
                        "paper_award": "",
                        "image_caption": "Top row: different time steps of a saddle under accelerated rotation visualized using LIC. \nMiddle row: the trajectory of the approximate DHT computed with the algorithm by Ju et al. on top of the instantaneous velocity of the field from its perspective computed with our algorithm. \nBottom row: the trajectory of the true DHT on top of the instantaneous velocity of the field from the DHT\u2019s perspective computed with our algorithm.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "w-topoinvis-1007-pres",
                        "session_id": "w-topoinvis-2",
                        "type": "Virtual Presentation (pre-recorded)",
                        "title": "ClassMat: a Matrix of Small Multiples to Analyze the Topology of Multiclass Multidimensional Data",
                        "contributors": [
                            "Michael Aupetit"
                        ],
                        "authors": [
                            "Michael Aupetit",
                            "Ahmed Ali",
                            "Abdelkader Baggag",
                            "Halima Bensmail"
                        ],
                        "abstract": "Data scientists often deal with multiclass multidimensional data. There is a need to support the exploration of such data with topological methods. We propose a new visualization metaphor for multiclass data and illustrate it with two complementary analytic approaches. We design ClassMat, a visualization matrix similar in spirit to the scatterplot matrix (SPLOM or pairs plot) but focused on pairs of classes rather than pairs of dimensions. We show how this visualization matrix can be used in two main multidimensional data visualization pipelines: visualization-then-topological-analysis and topological-analysis-then-visualization. In particular, we show it can support the analyst in detecting interferences between topological features of different classes.",
                        "uid": "w-topoinvis-1007",
                        "file_name": "w-topoinvis-1007_Aupetit_Presentation.mp4",
                        "time_stamp": "2022-10-17T21:20:00Z",
                        "time_start": "2022-10-17T21:20:00Z",
                        "time_end": "2022-10-17T21:24:00Z",
                        "paper_type": "workshop",
                        "keywords": [
                            "Visualization; topological data analysis;"
                        ],
                        "has_image": "1",
                        "has_video": "240",
                        "paper_award": "",
                        "image_caption": "ClassMat aims to help the analyst explore the topology of multidimensional labeled data by focusing their attention on within-class or between-class topology, one or two classes at a time. ClassMat organizes data visualizations in a matrix: one class per chart on-diagonal, and pairs of classes off-diagonal such that all pairs are uniquely represented. Chart dimensions are determined by the displayed classes: in this picture, Linear Discriminant projections are used off-diagonal, and Principal Components on-diagonal. ClassMat complements SPLOM/pairplot/GPLOM organizing the matrix per class labels rather than per dimensions of the data. As GPLOM, it extends beyond scatterplots.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "w-topoinvis-1007-qa",
                        "session_id": "w-topoinvis-2",
                        "type": "Virtual Q+A",
                        "title": "ClassMat: a Matrix of Small Multiples to Analyze the Topology of Multiclass Multidimensional Data (Q+A)",
                        "contributors": [
                            "Michael Aupetit"
                        ],
                        "authors": [],
                        "abstract": "Data scientists often deal with multiclass multidimensional data. There is a need to support the exploration of such data with topological methods. We propose a new visualization metaphor for multiclass data and illustrate it with two complementary analytic approaches. We design ClassMat, a visualization matrix similar in spirit to the scatterplot matrix (SPLOM or pairs plot) but focused on pairs of classes rather than pairs of dimensions. We show how this visualization matrix can be used in two main multidimensional data visualization pipelines: visualization-then-topological-analysis and topological-analysis-then-visualization. In particular, we show it can support the analyst in detecting interferences between topological features of different classes.",
                        "uid": "w-topoinvis-1007",
                        "file_name": "",
                        "time_stamp": "2022-10-17T21:24:00Z",
                        "time_start": "2022-10-17T21:24:00Z",
                        "time_end": "2022-10-17T21:27:00Z",
                        "paper_type": "workshop",
                        "keywords": [
                            "Visualization; topological data analysis;"
                        ],
                        "has_image": "1",
                        "has_video": "240",
                        "paper_award": "",
                        "image_caption": "ClassMat aims to help the analyst explore the topology of multidimensional labeled data by focusing their attention on within-class or between-class topology, one or two classes at a time. ClassMat organizes data visualizations in a matrix: one class per chart on-diagonal, and pairs of classes off-diagonal such that all pairs are uniquely represented. Chart dimensions are determined by the displayed classes: in this picture, Linear Discriminant projections are used off-diagonal, and Principal Components on-diagonal. ClassMat complements SPLOM/pairplot/GPLOM organizing the matrix per class labels rather than per dimensions of the data. As GPLOM, it extends beyond scatterplots.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "w-topoinvis-1000-pres",
                        "session_id": "w-topoinvis-2",
                        "type": "In Person Presentation",
                        "title": "Untangling Force-Directed Layouts Using Persistent Homology",
                        "contributors": [
                            "Bhavana Doppalapudi"
                        ],
                        "authors": [
                            "Bhavana Doppalapudi",
                            "Bei Wang",
                            "Paul Rosen"
                        ],
                        "abstract": "Force-directed layouts belong to a popular class of methods used to position nodes in a node-link diagram. However, they typically lack direct consideration of global structures, which can result in visual clutter and the overlap of unrelated structures. In this paper, we use the principles of persistent homology to untangle force-directed layouts thus mitigating these issues. First, we devise a new method to use 0-dimensional persistent homology to efficiently generate an initial graph layout. The approach results in faster convergence and better quality graph layouts. Second, we provide a new definition and an efficient algorithm for 1-dimensional persistent homology features (i.e., tunnels/cycles) on graphs. We provide users the ability to interact with the 1-dimensional features by highlighting them and adding cycle-emphasizing forces to the layout. Finally, we evaluate our approach with 32 synthetic and real-world graphs by computing various metrics, e.g., co-ranking, edge crossing, etc., to demonstrate the efficacy of our proposed method.",
                        "uid": "w-topoinvis-1000",
                        "file_name": "w-topoinvis-1000_Doppalapudi_Presentation.mp4",
                        "time_stamp": "2022-10-17T21:27:00Z",
                        "time_start": "2022-10-17T21:27:00Z",
                        "time_end": "2022-10-17T21:31:00Z",
                        "paper_type": "workshop",
                        "keywords": [
                            "Force-directed layout, persistent homology, graph clustering, graph cycles"
                        ],
                        "has_image": "1",
                        "has_video": "297",
                        "paper_award": "",
                        "image_caption": "A graph layout where nodes appear as a single hairball beside a graph layout where clusters have been formed.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "w-topoinvis-1000-qa",
                        "session_id": "w-topoinvis-2",
                        "type": "In Person Q+A",
                        "title": "Untangling Force-Directed Layouts Using Persistent Homology (Q+A)",
                        "contributors": [
                            "Bhavana Doppalapudi"
                        ],
                        "authors": [],
                        "abstract": "Force-directed layouts belong to a popular class of methods used to position nodes in a node-link diagram. However, they typically lack direct consideration of global structures, which can result in visual clutter and the overlap of unrelated structures. In this paper, we use the principles of persistent homology to untangle force-directed layouts thus mitigating these issues. First, we devise a new method to use 0-dimensional persistent homology to efficiently generate an initial graph layout. The approach results in faster convergence and better quality graph layouts. Second, we provide a new definition and an efficient algorithm for 1-dimensional persistent homology features (i.e., tunnels/cycles) on graphs. We provide users the ability to interact with the 1-dimensional features by highlighting them and adding cycle-emphasizing forces to the layout. Finally, we evaluate our approach with 32 synthetic and real-world graphs by computing various metrics, e.g., co-ranking, edge crossing, etc., to demonstrate the efficacy of our proposed method.",
                        "uid": "w-topoinvis-1000",
                        "file_name": "",
                        "time_stamp": "2022-10-17T21:31:00Z",
                        "time_start": "2022-10-17T21:31:00Z",
                        "time_end": "2022-10-17T21:34:00Z",
                        "paper_type": "workshop",
                        "keywords": [
                            "Force-directed layout, persistent homology, graph clustering, graph cycles"
                        ],
                        "has_image": "1",
                        "has_video": "297",
                        "paper_award": "",
                        "image_caption": "A graph layout where nodes appear as a single hairball beside a graph layout where clusters have been formed.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "w-topoinvis-1016",
                        "session_id": "w-topoinvis-2",
                        "type": "In Person Presentation",
                        "title": "Exploring Cyclone Evolution with Hierarchical Features",
                        "contributors": [
                            "Emma Nilsson"
                        ],
                        "authors": [
                            "Emma Nilsson",
                            "Jonas Lukasczyk",
                            "Wito Engelke",
                            "Talha Bin Masood",
                            "Gunilla Svensson",
                            "Rodrigo Caballero",
                            "Christoph Garth",
                            "Ingrid Hotz"
                        ],
                        "abstract": "The problem of tracking and visualizing cyclones is still an active area of climate research, since the nature of cyclones varies depending on geospatial location and temporal season, resulting in no clear mathematical definition. Thus, many cyclone tracking methods are tailored to specific datasets and therefore do not support general cyclone extraction across the globe. To address this challenge, we present a conceptual application for exploring cyclone evolution by organizing the extracted cyclone tracks into hierarchical groups. Our approach is based on extrema tracking, and the resulting tracks can be defined in a multi-scale structure by grouping the points based on a novel feature descriptor defined on the merge tree, so-called crown features. Consequently, multiple parameter settings can be visualized and explored in a level-of-detail approach, supporting experts to quickly gain insights on cyclonic formation and evolution. We describe a general cyclone exploration pipeline that consists of four modular building blocks: (1) an extrema tracking method, (2) multiple definitions of cyclones as groups of extrema, including crown features, (3) the correlation of cyclones based on the underlying tracking information, and (4) a hierarchical visualization of the resulting feature tracks and their spatial embedding, allowing exploration on a global and local scale. In order to be as flexible as possible, our pipeline allows for exchanging every module with different techniques, such as other tracking methods and cyclone definitions.",
                        "uid": "w-topoinvis-1016",
                        "file_name": "w-topoinvis-1016_Nilsson_Presentation.mp4",
                        "time_stamp": "2022-10-17T21:34:00Z",
                        "time_start": "2022-10-17T21:34:00Z",
                        "time_end": "2022-10-17T21:38:00Z",
                        "paper_type": "workshop",
                        "keywords": [
                            "Human-centered computing\u2014Visualization\u2014Visualization design and evaluation methods; Human-centered computing\u2014Visualization\u2014Visualization application domains\u2014Scientific visualization"
                        ],
                        "has_image": "1",
                        "has_video": "239",
                        "paper_award": "",
                        "image_caption": "The image shows the results of tracking the extratropical cyclone Klaus using our framework for cyclone tracking and exploration with hierarchical features. On the left is a nested tracking graph, where the hierarchical features used are the proposed crown components, showing two parameter settings at once. The tracks are colored by cyclone and linked to the spatial view of the globe, see the right image. We capture the split-merge event for Klaus over the Mediterranean on January 24th, 2009, but the graph shows that detection would have been impossible for a higher hierarchy level, see upper left.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "w-topoinvis-1016",
                        "session_id": "w-topoinvis-2",
                        "type": "In Person Q+A",
                        "title": "Exploring Cyclone Evolution with Hierarchical Features (Q+A)",
                        "contributors": [
                            "Emma Nilsson"
                        ],
                        "authors": [],
                        "abstract": "The problem of tracking and visualizing cyclones is still an active area of climate research, since the nature of cyclones varies depending on geospatial location and temporal season, resulting in no clear mathematical definition. Thus, many cyclone tracking methods are tailored to specific datasets and therefore do not support general cyclone extraction across the globe. To address this challenge, we present a conceptual application for exploring cyclone evolution by organizing the extracted cyclone tracks into hierarchical groups. Our approach is based on extrema tracking, and the resulting tracks can be defined in a multi-scale structure by grouping the points based on a novel feature descriptor defined on the merge tree, so-called crown features. Consequently, multiple parameter settings can be visualized and explored in a level-of-detail approach, supporting experts to quickly gain insights on cyclonic formation and evolution. We describe a general cyclone exploration pipeline that consists of four modular building blocks: (1) an extrema tracking method, (2) multiple definitions of cyclones as groups of extrema, including crown features, (3) the correlation of cyclones based on the underlying tracking information, and (4) a hierarchical visualization of the resulting feature tracks and their spatial embedding, allowing exploration on a global and local scale. In order to be as flexible as possible, our pipeline allows for exchanging every module with different techniques, such as other tracking methods and cyclone definitions.",
                        "uid": "w-topoinvis-1016",
                        "file_name": "",
                        "time_stamp": "2022-10-17T21:38:00Z",
                        "time_start": "2022-10-17T21:38:00Z",
                        "time_end": "2022-10-17T21:41:00Z",
                        "paper_type": "workshop",
                        "keywords": [
                            "Human-centered computing\u2014Visualization\u2014Visualization design and evaluation methods; Human-centered computing\u2014Visualization\u2014Visualization application domains\u2014Scientific visualization"
                        ],
                        "has_image": "1",
                        "has_video": "239",
                        "paper_award": "",
                        "image_caption": "The image shows the results of tracking the extratropical cyclone Klaus using our framework for cyclone tracking and exploration with hierarchical features. On the left is a nested tracking graph, where the hierarchical features used are the proposed crown components, showing two parameter settings at once. The tracks are colored by cyclone and linked to the spatial view of the globe, see the right image. We capture the split-merge event for Klaus over the Mediterranean on January 24th, 2009, but the graph shows that detection would have been impossible for a higher hierarchy level, see upper left.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "w-topoinvis-1017",
                        "session_id": "w-topoinvis-2",
                        "type": "In Person Presentation",
                        "title": "Towards Benchmark Data Generation for Feature Tracking in Scalar Fields",
                        "contributors": [
                            "Emma Nilsson"
                        ],
                        "authors": [
                            "Emma Nilsson",
                            "Jonas Lukasczyk",
                            "Talha Bin Masood",
                            "Christoph Garth",
                            "Ingrid Hotz"
                        ],
                        "abstract": "We describe a benchmark data generator for tracking methods for two- and three-dimensional time-dependent scalar fields. More and more topology-based tracking methods are presented in the visualization community, but the validation and evaluation of the tracking results are currently limited to qualitative visual approaches. We present a pipeline for creating different ground truth features that support evaluating tracking methods based on quantitative measures. In short, our approach randomly simulates a temporal point cloud with birth, death, split, merge, and continuation events, where the points are then used to derive a scalar field whose topological features correspond to the points. These scalar fields can be used as the input for different tracking methods, where the computed tracks can be compared against the ground truth feature evolution. This approach facilitates directly comparing the results of different tracking methods, independent of the initial feature characterization.",
                        "uid": "w-topoinvis-1017",
                        "file_name": "w-topoinvis-1017_Nilsson_Presentation.mp4",
                        "time_stamp": "2022-10-17T21:41:00Z",
                        "time_start": "2022-10-17T21:41:00Z",
                        "time_end": "2022-10-17T21:45:00Z",
                        "paper_type": "workshop",
                        "keywords": [
                            "Human-centered computing\u2014Visualization\u2014Visualization design and evaluation methods; Human-centered computing\u2014Visualization\u2014Visualization application domains\u2014Scientific visualization"
                        ],
                        "has_image": "1",
                        "has_video": "235",
                        "paper_award": "",
                        "image_caption": "An example of a time-dependent 2D benchmark dataset for evaluating feature tracking in scalar fields. Four points are propagated through a vector field with a sink in the center during fifteen timesteps. In the upper row, the spatial embeddings of the points are shown with the scalar field and iso-contours. The scalar values in the field are calculated by taking the max of the Gaussian functions each point carries. Below the embeddings is the tracking graph of the four features, which merge in multiple steps into one feature.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "w-topoinvis-1017",
                        "session_id": "w-topoinvis-2",
                        "type": "In Person Q+A",
                        "title": "Towards Benchmark Data Generation for Feature Tracking in Scalar Fields (Q+A)",
                        "contributors": [
                            "Emma Nilsson"
                        ],
                        "authors": [],
                        "abstract": "We describe a benchmark data generator for tracking methods for two- and three-dimensional time-dependent scalar fields. More and more topology-based tracking methods are presented in the visualization community, but the validation and evaluation of the tracking results are currently limited to qualitative visual approaches. We present a pipeline for creating different ground truth features that support evaluating tracking methods based on quantitative measures. In short, our approach randomly simulates a temporal point cloud with birth, death, split, merge, and continuation events, where the points are then used to derive a scalar field whose topological features correspond to the points. These scalar fields can be used as the input for different tracking methods, where the computed tracks can be compared against the ground truth feature evolution. This approach facilitates directly comparing the results of different tracking methods, independent of the initial feature characterization.",
                        "uid": "w-topoinvis-1017",
                        "file_name": "",
                        "time_stamp": "2022-10-17T21:45:00Z",
                        "time_start": "2022-10-17T21:45:00Z",
                        "time_end": "2022-10-17T21:48:00Z",
                        "paper_type": "workshop",
                        "keywords": [
                            "Human-centered computing\u2014Visualization\u2014Visualization design and evaluation methods; Human-centered computing\u2014Visualization\u2014Visualization application domains\u2014Scientific visualization"
                        ],
                        "has_image": "1",
                        "has_video": "235",
                        "paper_award": "",
                        "image_caption": "An example of a time-dependent 2D benchmark dataset for evaluating feature tracking in scalar fields. Four points are propagated through a vector field with a sink in the center during fifteen timesteps. In the upper row, the spatial embeddings of the points are shown with the scalar field and iso-contours. The scalar values in the field are calculated by taking the max of the Gaussian functions each point carries. Below the embeddings is the tracking graph of the four features, which merge in multiple steps into one feature.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "w-topoinvis-1011",
                        "session_id": "w-topoinvis-2",
                        "type": "Virtual Presentation (pre-recorded)",
                        "title": "Subject-Specific Brain Activity Analysis in fMRI Data Using Merge Trees",
                        "contributors": [
                            "Farhan Rasheed"
                        ],
                        "authors": [
                            "Farhan Rasheed",
                            "Daniel J\u00f6nsson",
                            "Emma Nilsson",
                            "Talha Bin Masood",
                            "Ingrid Hotz"
                        ],
                        "abstract": "We present a method for detecting patterns in time-varying functional magnetic resonance imaging (fMRI) data based on topological analysis. The oxygenated blood flow measured by fMRI is widely used as an indicator of brain activity. The signal is, however, prone to noise from various sources. Random brain activity, physiological noise, and noise from the scanner can reach a strength comparable to the signal itself. Thus, extracting the underlying signal is a challenging process typically approached by applying statistical methods. The goal of this work is to investigate the possibilities of recovering information from the signal using topological feature vectors directly based on the raw signal without medical domain priors. We utilize merge trees to define a robust feature vector capturing key features within a time step of fMRI data. We demonstrate how such a concise feature vector representation can be utilized for exploring the temporal development of brain activations, connectivity between these activations, and their relation to cognitive tasks.",
                        "uid": "w-topoinvis-1011",
                        "file_name": "w-topoinvis-1011_Rasheed_Presentation.mp4",
                        "time_stamp": "2022-10-17T21:48:00Z",
                        "time_start": "2022-10-17T21:48:00Z",
                        "time_end": "2022-10-17T21:52:00Z",
                        "paper_type": "workshop",
                        "keywords": [
                            "fMRI data analysis, data abstraction, temporal data, feature detection, merge tree, computational topology-based techniques"
                        ],
                        "has_image": "1",
                        "has_video": "220",
                        "paper_award": "",
                        "image_caption": "Illustration of the visualization components used to analyze the dynamic neural activity.\n(A) Scatter plot obtained by reducing the dimensionality of time-dependent feature vector.\n(B) An overview of the deviation of the activity level from the reference brain for each time point is provided through a statistical summary.\n(C) Merge tree based regions extraction from one subject's reference brain.\n(D) Chord diagram highlighting the connection between activation regions.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "w-topoinvis-1011",
                        "session_id": "w-topoinvis-2",
                        "type": "Virtual Q+A",
                        "title": "Subject-Specific Brain Activity Analysis in fMRI Data Using Merge Trees (Q+A)",
                        "contributors": [
                            "Farhan Rasheed"
                        ],
                        "authors": [],
                        "abstract": "We present a method for detecting patterns in time-varying functional magnetic resonance imaging (fMRI) data based on topological analysis. The oxygenated blood flow measured by fMRI is widely used as an indicator of brain activity. The signal is, however, prone to noise from various sources. Random brain activity, physiological noise, and noise from the scanner can reach a strength comparable to the signal itself. Thus, extracting the underlying signal is a challenging process typically approached by applying statistical methods. The goal of this work is to investigate the possibilities of recovering information from the signal using topological feature vectors directly based on the raw signal without medical domain priors. We utilize merge trees to define a robust feature vector capturing key features within a time step of fMRI data. We demonstrate how such a concise feature vector representation can be utilized for exploring the temporal development of brain activations, connectivity between these activations, and their relation to cognitive tasks.",
                        "uid": "w-topoinvis-1011",
                        "file_name": "",
                        "time_stamp": "2022-10-17T21:52:00Z",
                        "time_start": "2022-10-17T21:52:00Z",
                        "time_end": "2022-10-17T21:55:00Z",
                        "paper_type": "workshop",
                        "keywords": [
                            "fMRI data analysis, data abstraction, temporal data, feature detection, merge tree, computational topology-based techniques"
                        ],
                        "has_image": "1",
                        "has_video": "220",
                        "paper_award": "",
                        "image_caption": "Illustration of the visualization components used to analyze the dynamic neural activity.\n(A) Scatter plot obtained by reducing the dimensionality of time-dependent feature vector.\n(B) An overview of the deviation of the activity level from the reference brain for each time point is provided through a statistical summary.\n(C) Merge tree based regions extraction from one subject's reference brain.\n(D) Chord diagram highlighting the connection between activation regions.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "w-topoinvis-prog-3",
                        "session_id": "w-topoinvis-2",
                        "type": "In Person Presentation",
                        "title": "Lightning Talk by Raghavendra Sridharamurthy",
                        "contributors": [
                            "Raghavendra Sridharamurthy"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "",
                        "file_name": "",
                        "time_stamp": "2022-10-17T21:55:00Z",
                        "time_start": "2022-10-17T21:55:00Z",
                        "time_end": "2022-10-17T21:56:00Z",
                        "paper_type": "",
                        "keywords": [],
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "w-topoinvis-prog-4",
                        "session_id": "w-topoinvis-2",
                        "type": "In Person Presentation",
                        "title": "Lightning Talk by Tananun Songdechakraiwut",
                        "contributors": [
                            "Tananun Songdechakraiwut"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "",
                        "file_name": "",
                        "time_stamp": "2022-10-17T21:56:00Z",
                        "time_start": "2022-10-17T21:56:00Z",
                        "time_end": "2022-10-17T21:57:00Z",
                        "paper_type": "",
                        "keywords": [],
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "w-topoinvis-prog-5",
                        "session_id": "w-topoinvis-2",
                        "type": "In Person Presentation",
                        "title": "Lightning Talk by Hyeon Jeon",
                        "contributors": [
                            "Hyeon Jeon"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "",
                        "file_name": "",
                        "time_stamp": "2022-10-17T21:57:00Z",
                        "time_start": "2022-10-17T21:57:00Z",
                        "time_end": "2022-10-17T21:58:00Z",
                        "paper_type": "",
                        "keywords": [],
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "w-topoinvis-prog-6",
                        "session_id": "w-topoinvis-2",
                        "type": "In Person Other",
                        "title": "Best Paper Awards + Closing",
                        "contributors": [
                            "Paul Rosen",
                            "Julien Tierny"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "",
                        "file_name": "",
                        "time_stamp": "2022-10-17T21:58:00Z",
                        "time_start": "2022-10-17T21:58:00Z",
                        "time_end": "2022-10-17T22:00:00Z",
                        "paper_type": "",
                        "keywords": [],
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    }
                ]
            }
        ]
    },
    "w-altvis": {
        "event": "alt.VIS 2022",
        "long_name": "alt.VIS 2022",
        "event_type": "Workshop",
        "event_prefix": "w-altvis",
        "event_description": "",
        "event_url": "",
        "organizers": [
            "Lonni Besan\u00e7on",
            "Andrew McNutt",
            "Arnaud Prouzeau",
            "Jane Adams",
            "Derya Akbaba",
            "Charles Perin"
        ],
        "sessions": [
            {
                "title": "alt.VIS: Opening, Clairvoyance and Experiential Papers",
                "session_id": "w-altvis-1",
                "event_prefix": "w-altvis",
                "track": "ok7",
                "livestream_id": "ok7-sun",
                "session_image": "w-altvis-1.png",
                "chair": [
                    "Lonni Besan\u00e7on",
                    "Andrew McNutt",
                    "Arnaud Prouzeau",
                    "Jane Adams",
                    "Derya Akbaba",
                    "Charles Perin"
                ],
                "organizers": [],
                "time_start": "2022-10-16T14:00:00Z",
                "time_end": "2022-10-16T15:15:00Z",
                "discord_category": "",
                "discord_channel": "oklahoma-st-7",
                "discord_channel_id": "1024600937903431680",
                "discord_link": "https://discord.com/channels/978320435554947082/1024600937903431680",
                "slido_link": "https://app.sli.do/event/i28UETZaiRWr1XwX6eKjcf?section=f9f02524-f855-45fb-8ca6-c3e7e92e31fc",
                "youtube_url": "https://youtu.be/j4TNizTE5NI",
                "youtube_id": "j4TNizTE5NI",
                "zoom_meeting": "",
                "zoom_password": "",
                "zoom_link": "",
                "ff_link": "",
                "ff_playlist": "",
                "time_slots": [
                    {
                        "slot_id": "w-altvis-prog-1",
                        "session_id": "w-altvis-1",
                        "type": "In Person Other",
                        "title": "alt.vis welcome",
                        "contributors": [
                            "Jane Adams"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "",
                        "file_name": "",
                        "time_stamp": "2022-10-16T14:00:00Z",
                        "time_start": "2022-10-16T14:00:00Z",
                        "time_end": "2022-10-16T14:10:00Z",
                        "paper_type": "",
                        "keywords": [],
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "w-altvis-prog-2",
                        "session_id": "w-altvis-1",
                        "type": "Virtual Presentation (pre-recorded)",
                        "title": "Beyond the Walled Garden: A Visual Essay in Five Chapters",
                        "contributors": [
                            "Jo Wood"
                        ],
                        "authors": [
                            "Jo Wood"
                        ],
                        "abstract": "",
                        "uid": "",
                        "file_name": "wood-beyondthewalledgarden.mov",
                        "time_stamp": "2022-10-16T14:10:00Z",
                        "time_start": "2022-10-16T14:10:00Z",
                        "time_end": "2022-10-16T14:18:00Z",
                        "paper_type": "",
                        "keywords": [],
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "w-altvis-prog-3",
                        "session_id": "w-altvis-1",
                        "type": "Virtual Q+A",
                        "title": "Beyond the Walled Garden: A Visual Essay in Five Chapters (Q+A)",
                        "contributors": [
                            "Jo Wood"
                        ],
                        "authors": [
                            "Jo Wood"
                        ],
                        "abstract": "",
                        "uid": "",
                        "file_name": "",
                        "time_stamp": "2022-10-16T14:18:00Z",
                        "time_start": "2022-10-16T14:18:00Z",
                        "time_end": "2022-10-16T14:23:00Z",
                        "paper_type": "",
                        "keywords": [],
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "w-altvis-prog-4",
                        "session_id": "w-altvis-1",
                        "type": "Virtual Presentation (live)",
                        "title": "Ride Your Data: Raise your Arms, Scream, and Experience your Data from a Roller Coaster Cart",
                        "contributors": [
                            "Vincent Casamayou"
                        ],
                        "authors": [
                            "Vincent Casamayou",
                            "Yvonne Jansen",
                            "Pierre Dragicevic",
                            "Arnaud Prouzeau"
                        ],
                        "abstract": "",
                        "uid": "",
                        "file_name": "",
                        "time_stamp": "2022-10-16T14:23:00Z",
                        "time_start": "2022-10-16T14:23:00Z",
                        "time_end": "2022-10-16T14:31:00Z",
                        "paper_type": "",
                        "keywords": [],
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "w-altvis-prog-5",
                        "session_id": "w-altvis-1",
                        "type": "Virtual Q+A",
                        "title": "Ride Your Data: Raise your Arms, Scream, and Experience your Data from a Roller Coaster Cart (Q+A)",
                        "contributors": [
                            "Vincent Casamayou"
                        ],
                        "authors": [
                            "Vincent Casamayou",
                            "Yvonne Jansen",
                            "Pierre Dragicevic",
                            "Arnaud Prouzeau"
                        ],
                        "abstract": "",
                        "uid": "",
                        "file_name": "",
                        "time_stamp": "2022-10-16T14:31:00Z",
                        "time_start": "2022-10-16T14:31:00Z",
                        "time_end": "2022-10-16T14:36:00Z",
                        "paper_type": "",
                        "keywords": [],
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "w-altvis-prog-6",
                        "session_id": "w-altvis-1",
                        "type": "In Person Presentation",
                        "title": "DyStopia: Into a potential future of IEEE VIS under Plan S",
                        "contributors": [
                            "Lonni Besan\u00e7on"
                        ],
                        "authors": [
                            "Lonni Besan\u00e7on",
                            "Cody Dunne"
                        ],
                        "abstract": "",
                        "uid": "",
                        "file_name": "",
                        "time_stamp": "2022-10-16T14:36:00Z",
                        "time_start": "2022-10-16T14:36:00Z",
                        "time_end": "2022-10-16T14:44:00Z",
                        "paper_type": "",
                        "keywords": [],
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "w-altvis-prog-7",
                        "session_id": "w-altvis-1",
                        "type": "In Person Q+A",
                        "title": "DyStopia: Into a potential future of IEEE VIS under Plan S A (Q+A)",
                        "contributors": [
                            "Lonni Besan\u00e7on"
                        ],
                        "authors": [
                            "Lonni Besan\u00e7on",
                            "Cody Dunne"
                        ],
                        "abstract": "",
                        "uid": "",
                        "file_name": "",
                        "time_stamp": "2022-10-16T14:44:00Z",
                        "time_start": "2022-10-16T14:44:00Z",
                        "time_end": "2022-10-16T14:49:00Z",
                        "paper_type": "",
                        "keywords": [],
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "w-altvis-prog-8",
                        "session_id": "w-altvis-1",
                        "type": "Virtual Presentation (live)",
                        "title": "I Learn to Diffuse, or Data Alchemy 101: a Mnemonic Manifesto",
                        "contributors": [
                            "Victor Schetinger"
                        ],
                        "authors": [
                            "Victor Schetinger",
                            "Velitchko Filipov",
                            "Ignacio P\u00e9rez-Messina",
                            "Ethan Smith",
                            "Rodrigo Oliveira de Oliveira"
                        ],
                        "abstract": "",
                        "uid": "",
                        "file_name": "",
                        "time_stamp": "2022-10-16T14:49:00Z",
                        "time_start": "2022-10-16T14:49:00Z",
                        "time_end": "2022-10-16T14:57:00Z",
                        "paper_type": "",
                        "keywords": [],
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "w-altvis-prog-9",
                        "session_id": "w-altvis-1",
                        "type": "Virtual Q+A",
                        "title": "I Learn to Diffuse, or Data Alchemy 101: a Mnemonic Manifesto (Q+A)",
                        "contributors": [
                            "Victor Schetinger"
                        ],
                        "authors": [
                            "Victor Schetinger",
                            "Velitchko Filipov",
                            "Ignacio P\u00e9rez-Messina",
                            "Ethan Smith",
                            "Rodrigo Oliveira de Oliveira"
                        ],
                        "abstract": "",
                        "uid": "",
                        "file_name": "",
                        "time_stamp": "2022-10-16T14:57:00Z",
                        "time_start": "2022-10-16T14:57:00Z",
                        "time_end": "2022-10-16T15:02:00Z",
                        "paper_type": "",
                        "keywords": [],
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "w-altvis-prog-10",
                        "session_id": "w-altvis-1",
                        "type": "Virtual Presentation (live)",
                        "title": "Me-ifestos for Visualization Empowerment in Teaching (and Learning?)",
                        "contributors": [
                            "Tatiana Losev"
                        ],
                        "authors": [
                            "Jan Aerts",
                            "Wolfgang Aigner",
                            "Benjamin Bach",
                            "Fearn Bishop",
                            "Magdalena Boucher",
                            "Peter C.-H. Cheng",
                            "Alexandra Diehl",
                            "Jason Dykes",
                            "Sarah Hayes",
                            "Uta Hinrichs",
                            "Samuel Huron",
                            "Christoph Kinkeldey",
                            "Andy Kirk",
                            "S\u00f8ren Knudsen",
                            "Doris Kosminsky",
                            "Tatiana Losev",
                            "Areti Manataki",
                            "Andrew Manches",
                            "Isabel Meirelles",
                            "Luiz Morais",
                            "Till Nagel",
                            "Rebecca Noonan",
                            "Georgia Panagiotidou",
                            "Laura Pelchmann",
                            "Fateme Rajabiyazdi",
                            "Christina Stoiber",
                            "Tatiana von Landesberger",
                            "Jagoda Walny",
                            "Wesley Willett"
                        ],
                        "abstract": "",
                        "uid": "",
                        "file_name": "",
                        "time_stamp": "2022-10-16T15:02:00Z",
                        "time_start": "2022-10-16T15:02:00Z",
                        "time_end": "2022-10-16T15:10:00Z",
                        "paper_type": "",
                        "keywords": [],
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "w-altvis-prog-11",
                        "session_id": "w-altvis-1",
                        "type": "Virtual Q+A",
                        "title": "Me-ifestos for Visualization Empowerment in Teaching (and Learning?) (Q+A)",
                        "contributors": [
                            "Tatiana Losev"
                        ],
                        "authors": [
                            "Jan Aerts",
                            "Wolfgang Aigner",
                            "Benjamin Bach",
                            "Fearn Bishop",
                            "Magdalena Boucher",
                            "Peter C.-H. Cheng",
                            "Alexandra Diehl",
                            "Jason Dykes",
                            "Sarah Hayes",
                            "Uta Hinrichs",
                            "Samuel Huron",
                            "Christoph Kinkeldey",
                            "Andy Kirk",
                            "S\u00f8ren Knudsen",
                            "Doris Kosminsky",
                            "Tatiana Losev",
                            "Areti Manataki",
                            "Andrew Manches",
                            "Isabel Meirelles",
                            "Luiz Morais",
                            "Till Nagel",
                            "Rebecca Noonan",
                            "Georgia Panagiotidou",
                            "Laura Pelchmann",
                            "Fateme Rajabiyazdi",
                            "Christina Stoiber",
                            "Tatiana von Landesberger",
                            "Jagoda Walny",
                            "Wesley Willett"
                        ],
                        "abstract": "",
                        "uid": "",
                        "file_name": "",
                        "time_stamp": "2022-10-16T15:10:00Z",
                        "time_start": "2022-10-16T15:10:00Z",
                        "time_end": "2022-10-16T15:15:00Z",
                        "paper_type": "",
                        "keywords": [],
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    }
                ]
            },
            {
                "title": "alt.VIS: Algorithmic Papers and Closing",
                "session_id": "w-altvis-2",
                "event_prefix": "w-altvis",
                "track": "ok7",
                "livestream_id": "ok7-sun",
                "session_image": "w-altvis-2.png",
                "chair": [
                    "Lonni Besan\u00e7on",
                    "Andrew McNutt",
                    "Arnaud Prouzeau",
                    "Jane Adams",
                    "Derya Akbaba",
                    "Charles Perin"
                ],
                "organizers": [],
                "time_start": "2022-10-16T15:45:00Z",
                "time_end": "2022-10-16T17:00:00Z",
                "discord_category": "",
                "discord_channel": "oklahoma-st-7",
                "discord_channel_id": "1024600937903431680",
                "discord_link": "https://discord.com/channels/978320435554947082/1024600937903431680",
                "slido_link": "https://app.sli.do/event/i28UETZaiRWr1XwX6eKjcf?section=f9f02524-f855-45fb-8ca6-c3e7e92e31fc",
                "youtube_url": "https://youtu.be/j4TNizTE5NI",
                "youtube_id": "j4TNizTE5NI",
                "zoom_meeting": "",
                "zoom_password": "",
                "zoom_link": "",
                "ff_link": "",
                "ff_playlist": "",
                "time_slots": [
                    {
                        "slot_id": "w-altvis-prog-12",
                        "session_id": "w-altvis-2",
                        "type": "In Person Presentation",
                        "title": "The worst graph layout algorithm ever",
                        "contributors": [
                            "Sara Di Bartolomeo"
                        ],
                        "authors": [
                            "Sara Di Bartolomeo",
                            "Mat\u011bj Lang",
                            "Cody Dunne"
                        ],
                        "abstract": "",
                        "uid": "",
                        "file_name": "",
                        "time_stamp": "2022-10-16T15:45:00Z",
                        "time_start": "2022-10-16T15:45:00Z",
                        "time_end": "2022-10-16T15:53:00Z",
                        "paper_type": "",
                        "keywords": [],
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "w-altvis-prog-13",
                        "session_id": "w-altvis-2",
                        "type": "In Person Q+A",
                        "title": "The worst graph layout algorithm ever Q+A",
                        "contributors": [
                            "Sara Di Bartolomeo"
                        ],
                        "authors": [
                            "Sara Di Bartolomeo",
                            "Mat\u011bj Lang",
                            "Cody Dunne"
                        ],
                        "abstract": "",
                        "uid": "",
                        "file_name": "",
                        "time_stamp": "2022-10-16T15:53:00Z",
                        "time_start": "2022-10-16T15:53:00Z",
                        "time_end": "2022-10-16T15:58:00Z",
                        "paper_type": "",
                        "keywords": [],
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "w-altvis-prog-14",
                        "session_id": "w-altvis-2",
                        "type": "In Person Presentation",
                        "title": "* (This Name Can Be Automatically Generated)",
                        "contributors": [
                            "Mat\u011bj Lang"
                        ],
                        "authors": [
                            "Mat\u011bj Lang",
                            "Filip Kiraa Op\u00e1len\u00fd",
                            "Palko Ulbrich",
                            "Lev Nikolajevi\u010d"
                        ],
                        "abstract": "",
                        "uid": "",
                        "file_name": "",
                        "time_stamp": "2022-10-16T15:58:00Z",
                        "time_start": "2022-10-16T15:58:00Z",
                        "time_end": "2022-10-16T16:06:00Z",
                        "paper_type": "",
                        "keywords": [],
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "w-altvis-prog-15",
                        "session_id": "w-altvis-2",
                        "type": "In Person Q+A",
                        "title": "* (This Name Can Be Automatically Generated) Q+A",
                        "contributors": [
                            "Mat\u011bj Lang"
                        ],
                        "authors": [
                            "Mat\u011bj Lang",
                            "Filip Kiraa Op\u00e1len\u00fd",
                            "Palko Ulbrich",
                            "Lev Nikolajevi\u010d"
                        ],
                        "abstract": "",
                        "uid": "",
                        "file_name": "",
                        "time_stamp": "2022-10-16T16:06:00Z",
                        "time_start": "2022-10-16T16:06:00Z",
                        "time_end": "2022-10-16T16:11:00Z",
                        "paper_type": "",
                        "keywords": [],
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "w-altvis-prog-16",
                        "session_id": "w-altvis-2",
                        "type": "Virtual Presentation (live)",
                        "title": "Ready Player Viz: Player-Created Strategic Visualizations for Video Games",
                        "contributors": [
                            "Jane Adams"
                        ],
                        "authors": [
                            "Jane Adams",
                            "Michael Scot Davinroy"
                        ],
                        "abstract": "",
                        "uid": "",
                        "file_name": "",
                        "time_stamp": "2022-10-16T16:11:00Z",
                        "time_start": "2022-10-16T16:11:00Z",
                        "time_end": "2022-10-16T16:19:00Z",
                        "paper_type": "",
                        "keywords": [],
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "w-altvis-prog-17",
                        "session_id": "w-altvis-2",
                        "type": "Virtual Q+A",
                        "title": "Ready Player Viz: Player-Created Strategic Visualizations for Video Games Q+A",
                        "contributors": [
                            "Jane Adams"
                        ],
                        "authors": [
                            "Jane Adams",
                            "Michael Scot Davinroy"
                        ],
                        "abstract": "",
                        "uid": "",
                        "file_name": "",
                        "time_stamp": "2022-10-16T16:19:00Z",
                        "time_start": "2022-10-16T16:19:00Z",
                        "time_end": "2022-10-16T16:24:00Z",
                        "paper_type": "",
                        "keywords": [],
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "w-altvis-prog-18",
                        "session_id": "w-altvis-2",
                        "type": "In Person Other",
                        "title": "alt.vis closing",
                        "contributors": [
                            "Lonni Besan\u00e7on"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "",
                        "file_name": "",
                        "time_stamp": "2022-10-16T16:24:00Z",
                        "time_start": "2022-10-16T16:24:00Z",
                        "time_end": "2022-10-16T16:40:00Z",
                        "paper_type": "",
                        "keywords": [],
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    }
                ]
            }
        ]
    },
    "t-design": {
        "event": "Visualization Analysis and Design",
        "long_name": "Visualization Analysis and Design",
        "event_type": "Tutorial",
        "event_prefix": "t-design",
        "event_description": "",
        "event_url": "",
        "organizers": [
            "Tamara Munzner"
        ],
        "sessions": [
            {
                "title": "Visualization Analysis and Design 1",
                "session_id": "t-design-1",
                "event_prefix": "t-design",
                "track": "ok7",
                "livestream_id": "ok7-sun",
                "session_image": "t-design-1.png",
                "chair": [
                    "Tamara Munzner"
                ],
                "organizers": [],
                "time_start": "2022-10-16T19:00:00Z",
                "time_end": "2022-10-16T20:15:00Z",
                "discord_category": "",
                "discord_channel": "oklahoma-st-7",
                "discord_channel_id": "1024600937903431680",
                "discord_link": "https://discord.com/channels/978320435554947082/1024600937903431680",
                "slido_link": "https://app.sli.do/event/i28UETZaiRWr1XwX6eKjcf?section=f9f02524-f855-45fb-8ca6-c3e7e92e31fc",
                "youtube_url": "https://youtu.be/j4TNizTE5NI",
                "youtube_id": "j4TNizTE5NI",
                "zoom_meeting": "",
                "zoom_password": "",
                "zoom_link": "",
                "ff_link": "",
                "ff_playlist": "",
                "time_slots": [
                    {
                        "slot_id": "t-design-prog-1",
                        "session_id": "t-design-1",
                        "type": "In Person Presentation",
                        "title": "Analysis: What, Why, How",
                        "contributors": [
                            "Tamara Munzner"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "",
                        "file_name": "",
                        "time_stamp": "2022-10-16T19:00:00Z",
                        "time_start": "2022-10-16T19:00:00Z",
                        "time_end": "2022-10-16T19:30:00Z",
                        "paper_type": "",
                        "keywords": [],
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "t-design-prog-2",
                        "session_id": "t-design-1",
                        "type": "In Person Presentation",
                        "title": "Marks and Channels",
                        "contributors": [
                            "Tamara Munzner"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "",
                        "file_name": "",
                        "time_stamp": "2022-10-16T19:30:00Z",
                        "time_start": "2022-10-16T19:30:00Z",
                        "time_end": "2022-10-16T19:49:00Z",
                        "paper_type": "",
                        "keywords": [],
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "t-design-prog-3",
                        "session_id": "t-design-1",
                        "type": "In Person Presentation",
                        "title": "Arrange Tables",
                        "contributors": [
                            "Tamara Munzner"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "",
                        "file_name": "",
                        "time_stamp": "2022-10-16T19:49:00Z",
                        "time_start": "2022-10-16T19:49:00Z",
                        "time_end": "2022-10-16T20:14:00Z",
                        "paper_type": "",
                        "keywords": [],
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "t-design-prog-4",
                        "session_id": "t-design-1",
                        "type": "In Person Presentation",
                        "title": "Arrange Spatial Data",
                        "contributors": [
                            "Tamara Munzner"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "",
                        "file_name": "",
                        "time_stamp": "2022-10-16T20:14:00Z",
                        "time_start": "2022-10-16T20:14:00Z",
                        "time_end": "2022-10-16T20:15:00Z",
                        "paper_type": "",
                        "keywords": [],
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    }
                ]
            },
            {
                "title": "Visualization Analysis and Design 2",
                "session_id": "t-design-2",
                "event_prefix": "t-design",
                "track": "ok7",
                "livestream_id": "ok7-sun",
                "session_image": "t-design-2.png",
                "chair": [
                    "Tamara Munzner"
                ],
                "organizers": [],
                "time_start": "2022-10-16T20:45:00Z",
                "time_end": "2022-10-16T22:00:00Z",
                "discord_category": "",
                "discord_channel": "oklahoma-st-7",
                "discord_channel_id": "1024600937903431680",
                "discord_link": "https://discord.com/channels/978320435554947082/1024600937903431680",
                "slido_link": "https://app.sli.do/event/i28UETZaiRWr1XwX6eKjcf?section=f9f02524-f855-45fb-8ca6-c3e7e92e31fc",
                "youtube_url": "https://youtu.be/j4TNizTE5NI",
                "youtube_id": "j4TNizTE5NI",
                "zoom_meeting": "",
                "zoom_password": "",
                "zoom_link": "",
                "ff_link": "",
                "ff_playlist": "",
                "time_slots": [
                    {
                        "slot_id": "t-design-prog-5",
                        "session_id": "t-design-2",
                        "type": "In Person Presentation",
                        "title": "Arrange Networks and Tree",
                        "contributors": [
                            "Tamara Munzner"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "",
                        "file_name": "",
                        "time_stamp": "2022-10-16T20:45:00Z",
                        "time_start": "2022-10-16T20:45:00Z",
                        "time_end": "2022-10-16T21:11:00Z",
                        "paper_type": "",
                        "keywords": [],
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "t-design-prog-6",
                        "session_id": "t-design-2",
                        "type": "In Person Presentation",
                        "title": "Map Color and Other Channels",
                        "contributors": [
                            "Tamara Munzner"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "",
                        "file_name": "",
                        "time_stamp": "2022-10-16T21:11:00Z",
                        "time_start": "2022-10-16T21:11:00Z",
                        "time_end": "2022-10-16T21:26:00Z",
                        "paper_type": "",
                        "keywords": [],
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "t-design-prog-7",
                        "session_id": "t-design-2",
                        "type": "In Person Presentation",
                        "title": "Manipulate: Change, Select, Navigate",
                        "contributors": [
                            "Tamara Munzner"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "",
                        "file_name": "",
                        "time_stamp": "2022-10-16T21:26:00Z",
                        "time_start": "2022-10-16T21:26:00Z",
                        "time_end": "2022-10-16T21:38:00Z",
                        "paper_type": "",
                        "keywords": [],
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "t-design-prog-8",
                        "session_id": "t-design-2",
                        "type": "In Person Presentation",
                        "title": "Facet: Juxtapose, Partition, Superimpose",
                        "contributors": [
                            "Tamara Munzner"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "",
                        "file_name": "",
                        "time_stamp": "2022-10-16T21:38:00Z",
                        "time_start": "2022-10-16T21:38:00Z",
                        "time_end": "2022-10-16T21:58:00Z",
                        "paper_type": "",
                        "keywords": [],
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "t-design-prog-9",
                        "session_id": "t-design-2",
                        "type": "In Person Presentation",
                        "title": "Reduce: Filter, Aggregate",
                        "contributors": [
                            "Tamara Munzner"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "",
                        "file_name": "",
                        "time_stamp": "2022-10-16T21:58:00Z",
                        "time_start": "2022-10-16T21:58:00Z",
                        "time_end": "2022-10-16T22:00:00Z",
                        "paper_type": "",
                        "keywords": [],
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    }
                ]
            }
        ]
    },
    "w-testvis": {
        "event": "TestVis: Workshop on Visualization in Testing of Hardware, Software, and Manufacturing",
        "long_name": "TestVis: Workshop on Visualization in Testing of Hardware, Software, and Manufacturing",
        "event_type": "Workshop",
        "event_prefix": "w-testvis",
        "event_description": "",
        "event_url": "",
        "organizers": [
            "Katherine Isaacs",
            "Steffen Koch",
            "Timo Ropinski",
            "Stefan Wagner",
            "Daniel Weiskopf"
        ],
        "sessions": [
            {
                "title": "TestVis: Session 1",
                "session_id": "w-testvis-1",
                "event_prefix": "w-testvis",
                "track": "ok7",
                "livestream_id": "ok7-mon",
                "session_image": "w-testvis-1.png",
                "chair": [
                    "Katherine Isaacs",
                    "Steffen Koch",
                    "Timo Ropinski",
                    "Stefan Wagner",
                    "Daniel Weiskopf"
                ],
                "organizers": [],
                "time_start": "2022-10-17T14:00:00Z",
                "time_end": "2022-10-17T15:15:00Z",
                "discord_category": "",
                "discord_channel": "oklahoma-st-7",
                "discord_channel_id": "1024600937903431680",
                "discord_link": "https://discord.com/channels/978320435554947082/1024600937903431680",
                "slido_link": "https://app.sli.do/event/i28UETZaiRWr1XwX6eKjcf?section=f9f02524-f855-45fb-8ca6-c3e7e92e31fc",
                "youtube_url": "https://youtu.be/bk81gtt_ZNY",
                "youtube_id": "bk81gtt_ZNY",
                "zoom_meeting": "",
                "zoom_password": "",
                "zoom_link": "",
                "ff_link": "",
                "ff_playlist": "",
                "time_slots": [
                    {
                        "slot_id": "w-testvis-prog-1",
                        "session_id": "w-testvis-1",
                        "type": "In Person Presentation",
                        "title": "Opening",
                        "contributors": [
                            "Organizers"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "",
                        "file_name": "",
                        "time_stamp": "2022-10-17T14:00:00Z",
                        "time_start": "2022-10-17T14:00:00Z",
                        "time_end": "2022-10-17T14:05:00Z",
                        "paper_type": "",
                        "keywords": [],
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "w-testvis-prog-2",
                        "session_id": "w-testvis-1",
                        "type": "Virtual Presentation (live)",
                        "title": "Keynote: On Applying Visual Analytics For Improved Semiconductor Manufacturing and Test",
                        "contributors": [
                            "Matthias Sauer"
                        ],
                        "authors": [
                            "Matthias Sauer"
                        ],
                        "abstract": "",
                        "uid": "",
                        "file_name": "",
                        "time_stamp": "2022-10-17T14:05:00Z",
                        "time_start": "2022-10-17T14:05:00Z",
                        "time_end": "2022-10-17T14:30:00Z",
                        "paper_type": "",
                        "keywords": [],
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "w-testvis-prog-3",
                        "session_id": "w-testvis-1",
                        "type": "Virtual Q+A",
                        "title": "Keynote: On Applying Visual Analytics For Improved Semiconductor Manufacturing and Test (Q+A)",
                        "contributors": [
                            "Matthias Sauer"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "",
                        "file_name": "",
                        "time_stamp": "2022-10-17T14:30:00Z",
                        "time_start": "2022-10-17T14:30:00Z",
                        "time_end": "2022-10-17T14:35:00Z",
                        "paper_type": "",
                        "keywords": [],
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "w-testvis-1002-pres",
                        "session_id": "w-testvis-1",
                        "type": "Virtual Presentation (pre-recorded)",
                        "title": "Visual Exploration of Rheological Test Results from Soft Materials (Q+A)",
                        "contributors": [
                            "Jonas Madsen",
                            "Lasse Sode"
                        ],
                        "authors": [
                            "Jonas Madsen",
                            "Lasse Sode",
                            "Julie Frost Dahl",
                            "Milena Corredig",
                            "Hans-J\u00f6rg Schulz"
                        ],
                        "abstract": "",
                        "uid": "w-testvis-1002",
                        "file_name": "w-testvis-1002_Madsen_Presentation.mp4",
                        "time_stamp": "2022-10-17T14:35:00Z",
                        "time_start": "2022-10-17T14:35:00Z",
                        "time_end": "2022-10-17T14:50:00Z",
                        "paper_type": "workshop",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "1007",
                        "paper_award": "",
                        "image_caption": "This Pipkin plot shows the viscoelastic behavior of a material under study across three samples and for different strain rates. The stress for each rheological test condition is shown as a Lissajous curve (blue) in the elastic perspective. This means the \"slimmer\" the Lissajous curves are, the more elastic the material behaves. And the \"bulkier\" the Lissajous curves become, the less elastic contribution can be observed in the viscoelastic material response. The VAOS visualization software combines this plot with a number of others to support the comparative visual analysis of rheological material properties.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "w-testvis-1002-qa",
                        "session_id": "w-testvis-1",
                        "type": "Virtual Q+A",
                        "title": "Visual Exploration of Rheological Test Results from Soft Materials",
                        "contributors": [
                            "Jonas Madsen",
                            "Lasse Sode"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "w-testvis-1002",
                        "file_name": "",
                        "time_stamp": "2022-10-17T14:50:00Z",
                        "time_start": "2022-10-17T14:50:00Z",
                        "time_end": "2022-10-17T14:55:00Z",
                        "paper_type": "workshop",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "1007",
                        "paper_award": "",
                        "image_caption": "This Pipkin plot shows the viscoelastic behavior of a material under study across three samples and for different strain rates. The stress for each rheological test condition is shown as a Lissajous curve (blue) in the elastic perspective. This means the \"slimmer\" the Lissajous curves are, the more elastic the material behaves. And the \"bulkier\" the Lissajous curves become, the less elastic contribution can be observed in the viscoelastic material response. The VAOS visualization software combines this plot with a number of others to support the comparative visual analysis of rheological material properties.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "w-testvis-1000-pres",
                        "session_id": "w-testvis-1",
                        "type": "Virtual Presentation (pre-recorded)",
                        "title": "Interactive Analysis of Post-Silicon Validation Data",
                        "contributors": [
                            "Andr\u00e9s Lalama"
                        ],
                        "authors": [
                            "Andr\u00e9s Lalama",
                            "Johannes Knittel",
                            "Steffen Koch",
                            "Daniel Weiskopf",
                            "Thomas Ertl",
                            "Sarah Rottacker",
                            "Rapha\u00ebl Latty",
                            "Jochen Rivoir"
                        ],
                        "abstract": "",
                        "uid": "w-testvis-1000",
                        "file_name": "w-testvis-1000_Lalama_Presentation.mp4",
                        "time_stamp": "2022-10-17T14:55:00Z",
                        "time_start": "2022-10-17T14:55:00Z",
                        "time_end": "2022-10-17T15:10:00Z",
                        "paper_type": "workshop",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "882",
                        "paper_award": "",
                        "image_caption": "Overview visualization of a post-silicon validation dataset. Based on a chosen target error attribute (here: JTotal), validation engineers can define thresholds on the right side of the interface to divide test cases into passed, neutral, or failed",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "w-testvis-1000-qa",
                        "session_id": "w-testvis-1",
                        "type": "Virtual Q+A",
                        "title": "Interactive Analysis of Post-Silicon Validation Data (Q+A)",
                        "contributors": [
                            "Andr\u00e9s Lalama"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "w-testvis-1000",
                        "file_name": "",
                        "time_stamp": "2022-10-17T15:10:00Z",
                        "time_start": "2022-10-17T15:10:00Z",
                        "time_end": "2022-10-17T15:15:00Z",
                        "paper_type": "workshop",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "882",
                        "paper_award": "",
                        "image_caption": "Overview visualization of a post-silicon validation dataset. Based on a chosen target error attribute (here: JTotal), validation engineers can define thresholds on the right side of the interface to divide test cases into passed, neutral, or failed",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "",
                        "ff_id": ""
                    }
                ]
            },
            {
                "title": "TestVis: Session 2",
                "session_id": "w-testvis-2",
                "event_prefix": "w-testvis",
                "track": "ok7",
                "livestream_id": "ok7-mon",
                "session_image": "w-testvis-2.png",
                "chair": [
                    "Katherine Isaacs",
                    "Steffen Koch",
                    "Timo Ropinski",
                    "Stefan Wagner",
                    "Daniel Weiskopf"
                ],
                "organizers": [],
                "time_start": "2022-10-17T15:45:00Z",
                "time_end": "2022-10-17T17:00:00Z",
                "discord_category": "",
                "discord_channel": "oklahoma-st-7",
                "discord_channel_id": "1024600937903431680",
                "discord_link": "https://discord.com/channels/978320435554947082/1024600937903431680",
                "slido_link": "https://app.sli.do/event/i28UETZaiRWr1XwX6eKjcf?section=f9f02524-f855-45fb-8ca6-c3e7e92e31fc",
                "youtube_url": "https://youtu.be/bk81gtt_ZNY",
                "youtube_id": "bk81gtt_ZNY",
                "zoom_meeting": "",
                "zoom_password": "",
                "zoom_link": "",
                "ff_link": "",
                "ff_playlist": "",
                "time_slots": [
                    {
                        "slot_id": "w-testvis-prog-4",
                        "session_id": "w-testvis-2",
                        "type": "Virtual Presentation (live)",
                        "title": "Keynote: Using Visualization to Empower Observability",
                        "contributors": [
                            "Danyel Fisher"
                        ],
                        "authors": [
                            "Danyel Fisher"
                        ],
                        "abstract": "",
                        "uid": "",
                        "file_name": "",
                        "time_stamp": "2022-10-17T15:45:00Z",
                        "time_start": "2022-10-17T15:45:00Z",
                        "time_end": "2022-10-17T16:10:00Z",
                        "paper_type": "",
                        "keywords": [],
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "w-testvis-prog-5",
                        "session_id": "w-testvis-2",
                        "type": "Virtual Q+A",
                        "title": "Keynote: Using Visualization to Empower Observability (Q+A)",
                        "contributors": [
                            "Danyel Fisher"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "",
                        "file_name": "",
                        "time_stamp": "2022-10-17T16:10:00Z",
                        "time_start": "2022-10-17T16:10:00Z",
                        "time_end": "2022-10-17T16:15:00Z",
                        "paper_type": "",
                        "keywords": [],
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "w-testvis-1001-pres",
                        "session_id": "w-testvis-2",
                        "type": "Virtual Presentation (pre-recorded)",
                        "title": "Test Intelligence: How Modern Analyses and Visualizations in Teamscale Support Software Testing",
                        "contributors": [
                            "Jakob Rott"
                        ],
                        "authors": [
                            "Jakob Rott"
                        ],
                        "abstract": "",
                        "uid": "w-testvis-1001",
                        "file_name": "w-testvis-1001_Rott_Presentation.mp4",
                        "time_stamp": "2022-10-17T16:15:00Z",
                        "time_start": "2022-10-17T16:15:00Z",
                        "time_end": "2022-10-17T16:30:00Z",
                        "paper_type": "workshop",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "1016",
                        "paper_award": "",
                        "image_caption": "Frequency of bug-fixes in files (blue) compared to the line coverage (green). Find striking areas in your codebase and take countermeasures!",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "w-testvis-1001-qa",
                        "session_id": "w-testvis-2",
                        "type": "Virtual Q+A",
                        "title": "Test Intelligence: How Modern Analyses and Visualizations in Teamscale Support Software Testing (Q+A)",
                        "contributors": [
                            "Jakob Rott"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "w-testvis-1001",
                        "file_name": "",
                        "time_stamp": "2022-10-17T16:30:00Z",
                        "time_start": "2022-10-17T16:30:00Z",
                        "time_end": "2022-10-17T16:35:00Z",
                        "paper_type": "workshop",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "1016",
                        "paper_award": "",
                        "image_caption": "Frequency of bug-fixes in files (blue) compared to the line coverage (green). Find striking areas in your codebase and take countermeasures!",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "w-testvis-1004-pres",
                        "session_id": "w-testvis-2",
                        "type": "Virtual Presentation (pre-recorded)",
                        "title": "FLoAT: Framework for Workflow Analysis, Visualization and Transformation",
                        "contributors": [
                            "John Jacobson III"
                        ],
                        "authors": [
                            "John Jacobson III",
                            "Michael Bentley",
                            "Cayden Lund",
                            "Ganesh Gopalakrishnan",
                            "Ignacio Laguna",
                            "Gregory L. Lee"
                        ],
                        "abstract": "",
                        "uid": "w-testvis-1004",
                        "file_name": "w-testvis-1004_Jacobson_Presentation.mp4",
                        "time_stamp": "2022-10-17T16:35:00Z",
                        "time_start": "2022-10-17T16:35:00Z",
                        "time_end": "2022-10-17T16:50:00Z",
                        "paper_type": "workshop",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "1048",
                        "paper_award": "",
                        "image_caption": "FLoAT",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "w-testvis-1004-qa",
                        "session_id": "w-testvis-2",
                        "type": "Virtual Q+A",
                        "title": "FLoAT: Framework for Workflow Analysis, Visualization and Transformation (Q+A)",
                        "contributors": [
                            "John Jacobson III"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "w-testvis-1004",
                        "file_name": "",
                        "time_stamp": "2022-10-17T16:50:00Z",
                        "time_start": "2022-10-17T16:50:00Z",
                        "time_end": "2022-10-17T16:55:00Z",
                        "paper_type": "workshop",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "1048",
                        "paper_award": "",
                        "image_caption": "FLoAT",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "w-testvis-prog-6",
                        "session_id": "w-testvis-2",
                        "type": "In Person Other",
                        "title": "Closing",
                        "contributors": [
                            "Organizers"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "",
                        "file_name": "",
                        "time_stamp": "2022-10-17T16:55:00Z",
                        "time_start": "2022-10-17T16:55:00Z",
                        "time_end": "2022-10-17T17:00:00Z",
                        "paper_type": "",
                        "keywords": [],
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    }
                ]
            }
        ]
    },
    "w-visguides": {
        "event": "VisGuides: 4th IEEE Workshop on Visualization Guidelines Visualization Guidelines in Research, Design, and Education",
        "long_name": "VisGuides: 4th IEEE Workshop on Visualization Guidelines Visualization Guidelines in Research, Design, and Education",
        "event_type": "Workshop",
        "event_prefix": "w-visguides",
        "event_description": "",
        "event_url": "",
        "organizers": [
            "Benjamin Bach",
            "Alfie Abdul-Rahman",
            "Alexandra Diehl"
        ],
        "sessions": [
            {
                "title": "VisGuides: Session 1",
                "session_id": "w-visguides-1",
                "event_prefix": "w-visguides",
                "track": "ok7",
                "livestream_id": "ok7-mon",
                "session_image": "w-visguides-1.png",
                "chair": [
                    "Benjamin Bach",
                    "Alfie Abdul-Rahman",
                    "Alexandra Diehl"
                ],
                "organizers": [],
                "time_start": "2022-10-17T19:00:00Z",
                "time_end": "2022-10-17T20:15:00Z",
                "discord_category": "",
                "discord_channel": "oklahoma-st-7",
                "discord_channel_id": "1024600937903431680",
                "discord_link": "https://discord.com/channels/978320435554947082/1024600937903431680",
                "slido_link": "https://app.sli.do/event/i28UETZaiRWr1XwX6eKjcf?section=f9f02524-f855-45fb-8ca6-c3e7e92e31fc",
                "youtube_url": "https://youtu.be/bk81gtt_ZNY",
                "youtube_id": "bk81gtt_ZNY",
                "zoom_meeting": "",
                "zoom_password": "",
                "zoom_link": "",
                "ff_link": "",
                "ff_playlist": "",
                "time_slots": [
                    {
                        "slot_id": "w-visguides-prog-1",
                        "session_id": "w-visguides-1",
                        "type": "In Person Other",
                        "title": "Opening Presentation",
                        "contributors": [
                            "Benjamin Bach"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "",
                        "file_name": "",
                        "time_stamp": "2022-10-17T19:00:00Z",
                        "time_start": "2022-10-17T19:00:00Z",
                        "time_end": "2022-10-17T19:05:00Z",
                        "paper_type": "",
                        "keywords": [],
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "w-visguides-prog-2",
                        "session_id": "w-visguides-1",
                        "type": "Virtual Presentation (live)",
                        "title": "Keynote presentation by Prof. Sheelagh Carpendale",
                        "contributors": [
                            "Sheelagh Carpendale"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "",
                        "file_name": "",
                        "time_stamp": "2022-10-17T19:05:00Z",
                        "time_start": "2022-10-17T19:05:00Z",
                        "time_end": "2022-10-17T19:20:00Z",
                        "paper_type": "",
                        "keywords": [],
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "w-visguides-prog-3",
                        "session_id": "w-visguides-1",
                        "type": "Virtual Q+A",
                        "title": "Keynote Q&A",
                        "contributors": [
                            "Sheelagh Carpendale"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "",
                        "file_name": "",
                        "time_stamp": "2022-10-17T19:20:00Z",
                        "time_start": "2022-10-17T19:20:00Z",
                        "time_end": "2022-10-17T19:30:00Z",
                        "paper_type": "",
                        "keywords": [],
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "w-visguides-1006-pres",
                        "session_id": "w-visguides-1",
                        "type": "Virtual Presentation (pre-recorded)",
                        "title": "Supporting Domain Characterization in Visualization Design Studies With the Critical Decision Method",
                        "contributors": [
                            "Lena Cibulski"
                        ],
                        "authors": [
                            "Lena Cibulski",
                            "Evanthia Dimara",
                            "Setia Hermawati",
                            "J\u00f6rn Kohlhammer"
                        ],
                        "abstract": "",
                        "uid": "w-visguides-1006",
                        "file_name": "w-visguides-1006_Cibulski_Presentation.mp4",
                        "time_stamp": "2022-10-17T19:30:00Z",
                        "time_start": "2022-10-17T19:30:00Z",
                        "time_end": "2022-10-17T19:35:00Z",
                        "paper_type": "workshop",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "304",
                        "paper_award": "",
                        "image_caption": "In the domain characterization phase of a visualization design study, the Critical Decision Method provides five actionable steps to carve out domain knowledge by visiting a real-world problem multiple times.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "w-visguides-1009-pres",
                        "session_id": "w-visguides-1",
                        "type": "Virtual Presentation (pre-recorded)",
                        "title": "How should we design violin plots?",
                        "contributors": [
                            "Elena Molina"
                        ],
                        "authors": [
                            "Pere-Pau V\u00e1zquez",
                            "Laia Viale Herrando",
                            "Elena Molina Lopez"
                        ],
                        "abstract": "",
                        "uid": "w-visguides-1009",
                        "file_name": "w-visguides-1009_Molina_Presentation.mp4",
                        "time_stamp": "2022-10-17T19:35:00Z",
                        "time_start": "2022-10-17T19:35:00Z",
                        "time_end": "2022-10-17T19:40:00Z",
                        "paper_type": "workshop",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "250",
                        "paper_award": "",
                        "image_caption": "Violin plots are an extension of the boxplot design with a \u201cdensity trace\u201d as a line. \nTheir goal is to analyze distributions when individual data is less important. \nSometimes they are combined with other representations such as boxplots or beeswarms. \nHowever, users have difficulty comparing distributions, and the usefulness of these combinations has not been tested. \nTherefore, we have carried out an experiment to clarify how the different configurations affect judgments over values encoded.\nOur aim is to extract advice for the best design of these graphics.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "w-visguides-1011-pres",
                        "session_id": "w-visguides-1",
                        "type": "Virtual Presentation (pre-recorded)",
                        "title": "Guiding visualization practice with questions: 10qviz.org",
                        "contributors": [
                            "Arzu \u00c7\u00f6ltekin"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "",
                        "file_name": "",
                        "time_stamp": "2022-10-17T19:40:00Z",
                        "time_start": "2022-10-17T19:40:00Z",
                        "time_end": "2022-10-17T19:45:00Z",
                        "paper_type": "",
                        "keywords": [],
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "w-visguides-prog-4",
                        "session_id": "w-visguides-1",
                        "type": "Virtual Q+A",
                        "title": "Paper Q&A",
                        "contributors": [
                            "Lena Cibulski",
                            "Arzu \u00c7\u00f6ltekin",
                            "Elena Molina"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "",
                        "file_name": "",
                        "time_stamp": "2022-10-17T19:45:00Z",
                        "time_start": "2022-10-17T19:45:00Z",
                        "time_end": "2022-10-17T19:50:00Z",
                        "paper_type": "",
                        "keywords": [],
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "w-visguides-1005-pres",
                        "session_id": "w-visguides-1",
                        "type": "In Person Presentation",
                        "title": "Semantic Color Mapping: A Pipeline for Assigning Meaningful Colors to Text",
                        "contributors": [
                            "Yannick Metz"
                        ],
                        "authors": [
                            "Mennatallah El-Assady",
                            "Rebecca Kehlbeck",
                            "Yannick Metz",
                            "Rita Sevastjanova",
                            "Fabian Sperrle",
                            "Thilo Spinner",
                            "Udo Schlegel"
                        ],
                        "abstract": "",
                        "uid": "w-visguides-1005",
                        "file_name": "w-visguides-1005_Assady_Presentation.mp4",
                        "time_stamp": "2022-10-17T19:50:00Z",
                        "time_start": "2022-10-17T19:50:00Z",
                        "time_end": "2022-10-17T19:55:00Z",
                        "paper_type": "workshop",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "209",
                        "paper_award": "",
                        "image_caption": "Our proposed semantic color mapping pipeline",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "w-visguides-1007-pres",
                        "session_id": "w-visguides-1",
                        "type": "In Person Presentation",
                        "title": "Reflections and Considerations on Running Creative Visualization Learning Activities",
                        "contributors": [
                            "Jonathan Roberts"
                        ],
                        "authors": [
                            "Jonathan C Roberts",
                            "Benjamin Bach",
                            "Magdalena Boucher",
                            "Fanny Chevalier",
                            "Alexandra Diehl",
                            "Uta Hinrichs",
                            "Samuel Huron",
                            "Andy D Kirk",
                            "S\u00f8ren Knudsen",
                            "Isabel Meirelles",
                            "Rebecca Noonan",
                            "Laura Pelchmann",
                            "Fateme Rajabiyazdi",
                            "Christina Stoiber"
                        ],
                        "abstract": "",
                        "uid": "w-visguides-1007",
                        "file_name": "",
                        "time_stamp": "2022-10-17T19:55:00Z",
                        "time_start": "2022-10-17T19:55:00Z",
                        "time_end": "2022-10-17T20:00:00Z",
                        "paper_type": "workshop",
                        "keywords": [],
                        "has_image": "0",
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "w-visguides-1006-pres",
                        "session_id": "w-visguides-1",
                        "type": "In Person Presentation",
                        "title": "[Missing Title]",
                        "contributors": [
                            "Edward He",
                            "Daniel Tolessa"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "",
                        "file_name": "",
                        "time_stamp": "2022-10-17T20:00:00Z",
                        "time_start": "2022-10-17T20:00:00Z",
                        "time_end": "2022-10-17T20:05:00Z",
                        "paper_type": "",
                        "keywords": [],
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "w-visguides-1012-pres",
                        "session_id": "w-visguides-1",
                        "type": "In Person Presentation",
                        "title": "Considering the Role of Guidelines in Visualization Design Practice",
                        "contributors": [
                            "Paul Parsons"
                        ],
                        "authors": [
                            "Paul Parsons",
                            "Prakash Chandra Shukla"
                        ],
                        "abstract": "",
                        "uid": "w-visguides-1012",
                        "file_name": "",
                        "time_stamp": "2022-10-17T20:05:00Z",
                        "time_start": "2022-10-17T20:05:00Z",
                        "time_end": "2022-10-17T20:10:00Z",
                        "paper_type": "workshop",
                        "keywords": [],
                        "has_image": "0",
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "w-visguides-prog-5",
                        "session_id": "w-visguides-1",
                        "type": "In Person Q+A",
                        "title": "Paper Q&A",
                        "contributors": [
                            "Yannick Metz",
                            "Jonathan Roberts",
                            "Edward He",
                            "Daniel Tolessa",
                            "Paul Parsons"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "",
                        "file_name": "",
                        "time_stamp": "2022-10-17T20:10:00Z",
                        "time_start": "2022-10-17T20:10:00Z",
                        "time_end": "2022-10-17T20:15:00Z",
                        "paper_type": "",
                        "keywords": [],
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    }
                ]
            },
            {
                "title": "VisGuides: Session 2",
                "session_id": "w-visguides-2",
                "event_prefix": "w-visguides",
                "track": "ok7",
                "livestream_id": "ok7-mon",
                "session_image": "w-visguides-2.png",
                "chair": [
                    "Benjamin Bach",
                    "Alfie Abdul-Rahman",
                    "Alexandra Diehl"
                ],
                "organizers": [],
                "time_start": "2022-10-17T20:45:00Z",
                "time_end": "2022-10-17T22:00:00Z",
                "discord_category": "",
                "discord_channel": "oklahoma-st-7",
                "discord_channel_id": "1024600937903431680",
                "discord_link": "https://discord.com/channels/978320435554947082/1024600937903431680",
                "slido_link": "https://app.sli.do/event/i28UETZaiRWr1XwX6eKjcf?section=f9f02524-f855-45fb-8ca6-c3e7e92e31fc",
                "youtube_url": "https://youtu.be/bk81gtt_ZNY",
                "youtube_id": "bk81gtt_ZNY",
                "zoom_meeting": "",
                "zoom_password": "",
                "zoom_link": "",
                "ff_link": "",
                "ff_playlist": "",
                "time_slots": [
                    {
                        "slot_id": "w-visguides-prog-6",
                        "session_id": "w-visguides-2",
                        "type": "In Person Other",
                        "title": "Breakout sessions",
                        "contributors": [
                            "Benjamin Bach"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "",
                        "file_name": "",
                        "time_stamp": "2022-10-17T20:45:00Z",
                        "time_start": "2022-10-17T20:45:00Z",
                        "time_end": "2022-10-17T21:30:00Z",
                        "paper_type": "",
                        "keywords": [],
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "w-visguides-prog-7",
                        "session_id": "w-visguides-2",
                        "type": "In Person Other",
                        "title": "Presentations",
                        "contributors": [
                            "Benjamin Bach"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "",
                        "file_name": "",
                        "time_stamp": "2022-10-17T21:30:00Z",
                        "time_start": "2022-10-17T21:30:00Z",
                        "time_end": "2022-10-17T22:00:00Z",
                        "paper_type": "",
                        "keywords": [],
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    }
                ]
            }
        ]
    },
    "w-trex": {
        "event": "TREX: Workshop on TRust and EXpertise in Visualization",
        "long_name": "TREX: Workshop on TRust and EXpertise in Visualization",
        "event_type": "Workshop",
        "event_prefix": "w-trex",
        "event_description": "",
        "event_url": "",
        "organizers": [
            "Mahsan Nourani",
            "Eric Ragan",
            "Alireza Karduni",
            "Cindy Xiong",
            "Brittany Davis Pierson"
        ],
        "sessions": [
            {
                "title": "TREX: Session 1",
                "session_id": "w-trex-1",
                "event_prefix": "w-trex",
                "track": "ok8",
                "livestream_id": "ok8-sun",
                "session_image": "w-trex-1.png",
                "chair": [
                    "Mahsan Nourani",
                    "Eric Ragan",
                    "Alireza Karduni",
                    "Cindy Xiong",
                    "Brittany Davis Pierson"
                ],
                "organizers": [],
                "time_start": "2022-10-16T14:00:00Z",
                "time_end": "2022-10-16T15:15:00Z",
                "discord_category": "",
                "discord_channel": "oklahoma-st-8",
                "discord_channel_id": "1024600961295065128",
                "discord_link": "https://discord.com/channels/978320435554947082/1024600961295065128",
                "slido_link": "https://app.sli.do/event/i28UETZaiRWr1XwX6eKjcf?section=ae89e1c6-05b5-4dce-9468-cedd5098ebbe",
                "youtube_url": "https://youtu.be/nAzQZ6u21kE",
                "youtube_id": "nAzQZ6u21kE",
                "zoom_meeting": "",
                "zoom_password": "",
                "zoom_link": "",
                "ff_link": "",
                "ff_playlist": "",
                "time_slots": [
                    {
                        "slot_id": "w-trex-prog-1",
                        "session_id": "w-trex-1",
                        "type": "In Person Other",
                        "title": "Welcome",
                        "contributors": [
                            "Event Chairs"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "",
                        "file_name": "",
                        "time_stamp": "2022-10-16T14:00:00Z",
                        "time_start": "2022-10-16T14:00:00Z",
                        "time_end": "2022-10-16T14:10:00Z",
                        "paper_type": "",
                        "keywords": [],
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "w-trex-8894-pres",
                        "session_id": "w-trex-1",
                        "type": "Virtual Presentation (pre-recorded)",
                        "title": "Trustworthy Visual Analytics in Clinical Gait Analysis: A Case Study for Patients with Cerebral Palsy  ",
                        "contributors": [
                            "Alexander Rind"
                        ],
                        "authors": [
                            "Alexander Rind",
                            "Djordje Slijepcevic",
                            "Matthias Zeppelzauer",
                            "Fabian Unglaube",
                            "Andreas Kranzl",
                            "Brian Horsak"
                        ],
                        "abstract": "",
                        "uid": "w-trex-8894",
                        "file_name": "w-trex-8894_Rind_Presentation.mp4",
                        "time_stamp": "2022-10-16T14:10:00Z",
                        "time_start": "2022-10-16T14:10:00Z",
                        "time_end": "2022-10-16T14:15:00Z",
                        "paper_type": "workshop",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "298",
                        "paper_award": "",
                        "image_caption": "Clinical gait analysis utilizes 3D motion tracking to record complex time series datasets of patients while they are walking in a gait laboratory. gaitXplorer is a visual analytics approach that classifies cerebral palsy-related gait patterns using a CNN, generates relevance scores using Grad-CAM, and presents the data, the predicted class, and the relevance scores in a visual interface. This case study investigates how clinical gait analysts trust the automated classifications and explanations presented by gaitXplorer.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "w-trex-8894-qa",
                        "session_id": "w-trex-1",
                        "type": "Virtual Q+A",
                        "title": "Trustworthy Visual Analytics in Clinical Gait Analysis: A Case Study for Patients with Cerebral Palsy (Q+A)",
                        "contributors": [
                            "Alexander Rind"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "w-trex-8894",
                        "file_name": "",
                        "time_stamp": "2022-10-16T14:15:00Z",
                        "time_start": "2022-10-16T14:15:00Z",
                        "time_end": "2022-10-16T14:17:00Z",
                        "paper_type": "workshop",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "298",
                        "paper_award": "",
                        "image_caption": "Clinical gait analysis utilizes 3D motion tracking to record complex time series datasets of patients while they are walking in a gait laboratory. gaitXplorer is a visual analytics approach that classifies cerebral palsy-related gait patterns using a CNN, generates relevance scores using Grad-CAM, and presents the data, the predicted class, and the relevance scores in a visual interface. This case study investigates how clinical gait analysts trust the automated classifications and explanations presented by gaitXplorer.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "w-trex-3846-pres",
                        "session_id": "w-trex-1",
                        "type": "In Person Presentation",
                        "title": "Understanding Systematic Miscalibration in Machine Learning Classifiers  ",
                        "contributors": [
                            "Markelle Kelly"
                        ],
                        "authors": [
                            "Markelle Kelly",
                            "Padhraic Smyth"
                        ],
                        "abstract": "",
                        "uid": "w-trex-3846",
                        "file_name": "w-trex-3846_Kelly_Presentation.mp4",
                        "time_stamp": "2022-10-16T14:17:00Z",
                        "time_start": "2022-10-16T14:17:00Z",
                        "time_end": "2022-10-16T14:22:00Z",
                        "paper_type": "workshop",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "495",
                        "paper_award": "",
                        "image_caption": "A 2D plot with the variable \"Age\" on the x-axis and \"% Error\" on the y-axis. The plot includes two curves, with error bars, labeled as the \"Actual Empirical Error\" (in blue) and the \"Model's Predicted Error\" (in red). The red curve is significantly (by over five percentage points) higher than the blue curve around age 40, but much lower than the blue curve around age 60.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "w-trex-3846-qa",
                        "session_id": "w-trex-1",
                        "type": "In Person Q+A",
                        "title": "Understanding Systematic Miscalibration in Machine Learning Classifiers (Q+A)",
                        "contributors": [
                            "Markelle Kelly"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "w-trex-3846",
                        "file_name": "",
                        "time_stamp": "2022-10-16T14:22:00Z",
                        "time_start": "2022-10-16T14:22:00Z",
                        "time_end": "2022-10-16T14:24:00Z",
                        "paper_type": "workshop",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "495",
                        "paper_award": "",
                        "image_caption": "A 2D plot with the variable \"Age\" on the x-axis and \"% Error\" on the y-axis. The plot includes two curves, with error bars, labeled as the \"Actual Empirical Error\" (in blue) and the \"Model's Predicted Error\" (in red). The red curve is significantly (by over five percentage points) higher than the blue curve around age 40, but much lower than the blue curve around age 60.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "w-trex-6700-pres",
                        "session_id": "w-trex-1",
                        "type": "Virtual Presentation (pre-recorded)",
                        "title": "How Do Algorithmic Fairness Metrics Align with Human Judgement? A Mixed-Initiative System for Contextualized Fairness Assessment  ",
                        "contributors": [
                            "Rares Constantin"
                        ],
                        "authors": [
                            "Rares Constantin",
                            "Moritz D\u00fcck",
                            "Anton Alexandrov",
                            "Patrik Matosevic",
                            "Daphna Keidar",
                            "Mennatallah El-Assady"
                        ],
                        "abstract": "",
                        "uid": "w-trex-6700",
                        "file_name": "w-trex-6700_Constantin_Presentation.mp4",
                        "time_stamp": "2022-10-16T14:24:00Z",
                        "time_start": "2022-10-16T14:24:00Z",
                        "time_end": "2022-10-16T14:29:00Z",
                        "paper_type": "workshop",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "379",
                        "paper_award": "",
                        "image_caption": "The simplified pipeline of FairAlign, a visual analytics platform for contextualized fairness assessment. A workflow example would be as follows: The laypeople sign up, choose an annotation dashboard and start taking decisions regarding algorithmic fairness, based on the provided visualizations of data and model's predictions. After the annotation process is finalized, data scientists and machine learning experts can login and analyze the values of the predefined fairness metrics, along with the aggregated results obtained from the human evaluation.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "w-trex-6700-qa",
                        "session_id": "w-trex-1",
                        "type": "Virtual Q+A",
                        "title": "How Do Algorithmic Fairness Metrics Align with Human Judgement? A Mixed-Initiative System for Contextualized Fairness Assessment (Q+A)",
                        "contributors": [
                            "Rares Constantin"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "w-trex-6700",
                        "file_name": "",
                        "time_stamp": "2022-10-16T14:29:00Z",
                        "time_start": "2022-10-16T14:29:00Z",
                        "time_end": "2022-10-16T14:31:00Z",
                        "paper_type": "workshop",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "379",
                        "paper_award": "",
                        "image_caption": "The simplified pipeline of FairAlign, a visual analytics platform for contextualized fairness assessment. A workflow example would be as follows: The laypeople sign up, choose an annotation dashboard and start taking decisions regarding algorithmic fairness, based on the provided visualizations of data and model's predictions. After the annotation process is finalized, data scientists and machine learning experts can login and analyze the values of the predefined fairness metrics, along with the aggregated results obtained from the human evaluation.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "w-trex-9231-pres",
                        "session_id": "w-trex-1",
                        "type": "Virtual Presentation (pre-recorded)",
                        "title": "Kicking Analysts Out of the Meeting Room: Supporting Future Data-driven Decision Making with Intelligent Interactive Visualization Systems  ",
                        "contributors": [
                            "Yi Han"
                        ],
                        "authors": [
                            "Yi Han"
                        ],
                        "abstract": "",
                        "uid": "w-trex-9231",
                        "file_name": "w-trex-9231_Han_Presentation.mp4",
                        "time_stamp": "2022-10-16T14:31:00Z",
                        "time_start": "2022-10-16T14:31:00Z",
                        "time_end": "2022-10-16T14:36:00Z",
                        "paper_type": "workshop",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "289",
                        "paper_award": "",
                        "image_caption": "In the future, decision team members could use a visualization system themselves to conduct analyses and generate visualizations for data-driven decisions. However, three gaps need to be filled first before this future can be realized.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "w-trex-9231-qa",
                        "session_id": "w-trex-1",
                        "type": "Virtual Q+A",
                        "title": "Kicking Analysts Out of the Meeting Room: Supporting Future Data-driven Decision Making with Intelligent Interactive Visualization Systems (Q+A)",
                        "contributors": [
                            "Yi Han"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "w-trex-9231",
                        "file_name": "",
                        "time_stamp": "2022-10-16T14:36:00Z",
                        "time_start": "2022-10-16T14:36:00Z",
                        "time_end": "2022-10-16T14:38:00Z",
                        "paper_type": "workshop",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "289",
                        "paper_award": "",
                        "image_caption": "In the future, decision team members could use a visualization system themselves to conduct analyses and generate visualizations for data-driven decisions. However, three gaps need to be filled first before this future can be realized.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "w-trex-1111-pres",
                        "session_id": "w-trex-1",
                        "type": "Virtual Presentation (live)",
                        "title": "Standardized Process Models for Applying Artificial Intelligence to High-Risk Decision-Making: A Pediatric Neuro-Oncology Perspective  ",
                        "contributors": [
                            "Eric Prince"
                        ],
                        "authors": [
                            "Eric Prince",
                            "Todd Hankinson",
                            "Carsten G\u00f6rg"
                        ],
                        "abstract": "",
                        "uid": "w-trex-1111",
                        "file_name": "",
                        "time_stamp": "2022-10-16T14:38:00Z",
                        "time_start": "2022-10-16T14:38:00Z",
                        "time_end": "2022-10-16T14:43:00Z",
                        "paper_type": "workshop",
                        "keywords": [],
                        "has_image": "0",
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "w-trex-1111-qa",
                        "session_id": "w-trex-1",
                        "type": "Virtual Q+A",
                        "title": "Standardized Process Models for Applying Artificial Intelligence to High-Risk Decision-Making: A Pediatric Neuro-Oncology Perspective (Q+A)",
                        "contributors": [
                            "Eric Prince"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "w-trex-1111",
                        "file_name": "",
                        "time_stamp": "2022-10-16T14:43:00Z",
                        "time_start": "2022-10-16T14:43:00Z",
                        "time_end": "2022-10-16T14:45:00Z",
                        "paper_type": "workshop",
                        "keywords": [],
                        "has_image": "0",
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "w-trex-5208-pres",
                        "session_id": "w-trex-1",
                        "type": "Virtual Presentation (pre-recorded)",
                        "title": "Exploring Effectiveness of Explanations for Appropriate Trust: Lessons from Cognitive Psychology  ",
                        "contributors": [
                            "Ruben S. Verhagen",
                            "Siddharth Mehrotra"
                        ],
                        "authors": [
                            "Ruben Verhagen",
                            "Siddharth Mehrotra",
                            "Mark Neerincx",
                            "Catholijn Jonker",
                            "Myrthe Tielman"
                        ],
                        "abstract": "",
                        "uid": "w-trex-5208",
                        "file_name": "w-trex-5208_Verhagen_Presentation.mp4",
                        "time_stamp": "2022-10-16T14:45:00Z",
                        "time_start": "2022-10-16T14:45:00Z",
                        "time_end": "2022-10-16T14:50:00Z",
                        "paper_type": "workshop",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "326",
                        "paper_award": "",
                        "image_caption": "Four identified components from cognitive psychology for designing effective AI explanations are Perception, Intent, Semantics and User & Context.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "w-trex-5208-qa",
                        "session_id": "w-trex-1",
                        "type": "Virtual Q+A",
                        "title": "Exploring Effectiveness of Explanations for Appropriate Trust: Lessons from Cognitive Psychology (Q+A)",
                        "contributors": [
                            "Ruben S. Verhagen",
                            "Siddharth Mehrotra"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "w-trex-5208",
                        "file_name": "",
                        "time_stamp": "2022-10-16T14:50:00Z",
                        "time_start": "2022-10-16T14:50:00Z",
                        "time_end": "2022-10-16T14:52:00Z",
                        "paper_type": "workshop",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "326",
                        "paper_award": "",
                        "image_caption": "Four identified components from cognitive psychology for designing effective AI explanations are Perception, Intent, Semantics and User & Context.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "w-trex-prog-2",
                        "session_id": "w-trex-1",
                        "type": "Mixed Panel (virtual/in person)",
                        "title": "Breakout Sessions",
                        "contributors": [
                            "Event Chairs + attendees"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "",
                        "file_name": "",
                        "time_stamp": "2022-10-16T14:52:00Z",
                        "time_start": "2022-10-16T14:52:00Z",
                        "time_end": "2022-10-16T15:12:00Z",
                        "paper_type": "",
                        "keywords": [],
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "w-trex-prog-3",
                        "session_id": "w-trex-1",
                        "type": "In Person Other",
                        "title": "Session close",
                        "contributors": [
                            "Event Chairs"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "",
                        "file_name": "",
                        "time_stamp": "2022-10-16T15:12:00Z",
                        "time_start": "2022-10-16T15:12:00Z",
                        "time_end": "2022-10-16T15:15:00Z",
                        "paper_type": "",
                        "keywords": [],
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    }
                ]
            },
            {
                "title": "TREX: Session 2",
                "session_id": "w-trex-2",
                "event_prefix": "w-trex",
                "track": "ok8",
                "livestream_id": "ok8-sun",
                "session_image": "w-trex-2.png",
                "chair": [
                    "Mahsan Nourani",
                    "Eric Ragan",
                    "Alireza Karduni",
                    "Cindy Xiong",
                    "Brittany Davis Pierson"
                ],
                "organizers": [],
                "time_start": "2022-10-16T15:45:00Z",
                "time_end": "2022-10-16T17:00:00Z",
                "discord_category": "",
                "discord_channel": "oklahoma-st-8",
                "discord_channel_id": "1024600961295065128",
                "discord_link": "https://discord.com/channels/978320435554947082/1024600961295065128",
                "slido_link": "https://app.sli.do/event/i28UETZaiRWr1XwX6eKjcf?section=ae89e1c6-05b5-4dce-9468-cedd5098ebbe",
                "youtube_url": "https://youtu.be/nAzQZ6u21kE",
                "youtube_id": "nAzQZ6u21kE",
                "zoom_meeting": "",
                "zoom_password": "",
                "zoom_link": "",
                "ff_link": "",
                "ff_playlist": "",
                "time_slots": [
                    {
                        "slot_id": "w-trex-prog-4",
                        "session_id": "w-trex-2",
                        "type": "In Person Other",
                        "title": "Welcome Back",
                        "contributors": [
                            "Event Chairs"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "",
                        "file_name": "",
                        "time_stamp": "2022-10-16T15:45:00Z",
                        "time_start": "2022-10-16T15:45:00Z",
                        "time_end": "2022-10-16T15:50:00Z",
                        "paper_type": "",
                        "keywords": [],
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "w-trex-prog-5",
                        "session_id": "w-trex-2",
                        "type": "In Person Presentation",
                        "title": "Keynote Talk: When does the data \u201cspeak for itself\u201d? Toward formal models of persuasive data visualization",
                        "contributors": [
                            "Doug Markant"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "",
                        "file_name": "",
                        "time_stamp": "2022-10-16T15:50:00Z",
                        "time_start": "2022-10-16T15:50:00Z",
                        "time_end": "2022-10-16T16:16:00Z",
                        "paper_type": "",
                        "keywords": [],
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "w-trex-prog-6",
                        "session_id": "w-trex-2",
                        "type": "In Person Q+A",
                        "title": "Keynote Talk: When does the data \u201cspeak for itself\u201d? Toward formal models of persuasive data visualization (Q+A)",
                        "contributors": [
                            "Doug Markant"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "",
                        "file_name": "",
                        "time_stamp": "2022-10-16T16:16:00Z",
                        "time_start": "2022-10-16T16:16:00Z",
                        "time_end": "2022-10-16T16:24:00Z",
                        "paper_type": "",
                        "keywords": [],
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "w-trex-6504-pres",
                        "session_id": "w-trex-2",
                        "type": "In Person Presentation",
                        "title": "Perception of Skill in Visual Problem Solving: An Analysis of Interactive Behaviors, Personality Traits, and the Dunning-Kruger Effect  ",
                        "contributors": [
                            "Bonnie Chen"
                        ],
                        "authors": [
                            "Bonnie Chen",
                            "Emily Wall"
                        ],
                        "abstract": "",
                        "uid": "w-trex-6504",
                        "file_name": "w-trex-6504_Chen_Presentation.mp4",
                        "time_stamp": "2022-10-16T16:24:00Z",
                        "time_start": "2022-10-16T16:24:00Z",
                        "time_end": "2022-10-16T16:29:00Z",
                        "paper_type": "workshop",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "400",
                        "paper_award": "",
                        "image_caption": "Figure 1 shows perceived performance as a function of actual performance measured by movement counts. \nX-axis represents four quartiles divided by move counts; Y-axis represents percentile ranking.\n\n\nFigure 2 shows the correlation between personality traits and perceived/actual move count. \nWe detected a statistically non-significant positive correlation between conscientiousness traits and actual move count. \nThere also exists a weak negative correlation between neuroticism traits and perceived move counts.\n\nFigure 3 shows movement path triggered by different personality traits.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "w-trex-6504-qa",
                        "session_id": "w-trex-2",
                        "type": "In Person Q+A",
                        "title": "Perception of Skill in Visual Problem Solving: An Analysis of Interactive Behaviors, Personality Traits, and the Dunning-Kruger Effect (Q+A)",
                        "contributors": [
                            "Bonnie Chen"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "w-trex-6504",
                        "file_name": "",
                        "time_stamp": "2022-10-16T16:29:00Z",
                        "time_start": "2022-10-16T16:29:00Z",
                        "time_end": "2022-10-16T16:31:00Z",
                        "paper_type": "workshop",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "400",
                        "paper_award": "",
                        "image_caption": "Figure 1 shows perceived performance as a function of actual performance measured by movement counts. \nX-axis represents four quartiles divided by move counts; Y-axis represents percentile ranking.\n\n\nFigure 2 shows the correlation between personality traits and perceived/actual move count. \nWe detected a statistically non-significant positive correlation between conscientiousness traits and actual move count. \nThere also exists a weak negative correlation between neuroticism traits and perceived move counts.\n\nFigure 3 shows movement path triggered by different personality traits.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "w-trex-2365-pres",
                        "session_id": "w-trex-2",
                        "type": "In Person Presentation",
                        "title": "Using Processing Fluency as a Metric of Trust in Scatterplot Visualizations  ",
                        "contributors": [
                            "Hamza Elhamdadi"
                        ],
                        "authors": [
                            "Hamza Elhamdadi",
                            "Lace Padilla",
                            "Cindy Xiong"
                        ],
                        "abstract": "",
                        "uid": "w-trex-2365",
                        "file_name": "",
                        "time_stamp": "2022-10-16T16:31:00Z",
                        "time_start": "2022-10-16T16:31:00Z",
                        "time_end": "2022-10-16T16:36:00Z",
                        "paper_type": "workshop",
                        "keywords": [],
                        "has_image": "0",
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "w-trex-2365-qa",
                        "session_id": "w-trex-2",
                        "type": "In Person Q+A",
                        "title": "Using Processing Fluency as a Metric of Trust in Scatterplot Visualizations (Q+A)",
                        "contributors": [
                            "Hamza Elhamdadi"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "w-trex-2365",
                        "file_name": "",
                        "time_stamp": "2022-10-16T16:36:00Z",
                        "time_start": "2022-10-16T16:36:00Z",
                        "time_end": "2022-10-16T16:38:00Z",
                        "paper_type": "workshop",
                        "keywords": [],
                        "has_image": "0",
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "w-trex-6077-pres",
                        "session_id": "w-trex-2",
                        "type": "Virtual Presentation (live)",
                        "title": "Trust Calibration as a Function of the Propogation of Uncertainty in Knowledge Generation: A Survey  ",
                        "contributors": [
                            "Joshua Bolay"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "",
                        "file_name": "",
                        "time_stamp": "2022-10-16T16:38:00Z",
                        "time_start": "2022-10-16T16:38:00Z",
                        "time_end": "2022-10-16T16:43:00Z",
                        "paper_type": "",
                        "keywords": [],
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "w-trex-6077-qa",
                        "session_id": "w-trex-2",
                        "type": "Virtual Q+A",
                        "title": "Trust Calibration as a Function of the Propogation of Uncertainty in Knowledge Generation: A Survey (Q+A)",
                        "contributors": [
                            "Joshua Bolay"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "",
                        "file_name": "",
                        "time_stamp": "2022-10-16T16:43:00Z",
                        "time_start": "2022-10-16T16:43:00Z",
                        "time_end": "2022-10-16T16:45:00Z",
                        "paper_type": "",
                        "keywords": [],
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "w-trex-2027-pres",
                        "session_id": "w-trex-2",
                        "type": "Virtual Presentation (pre-recorded)",
                        "title": "Data Provenance Visualization in Brazilian Public Health Dashboards  ",
                        "contributors": [
                            "Johne Marcus Jarske"
                        ],
                        "authors": [
                            "Johne Marcus Jarske",
                            "Lucia Filgueiras",
                            "Leandro Manuel Velloso",
                            "T\u00e2nia Let\u00edcia Let\u00edcia dos Santos",
                            "Jorge Rady de Almeida J\u00fanior"
                        ],
                        "abstract": "",
                        "uid": "w-trex-2027",
                        "file_name": "w-trex-2027_Jarske_Presentation.mp4",
                        "time_stamp": "2022-10-16T16:45:00Z",
                        "time_start": "2022-10-16T16:45:00Z",
                        "time_end": "2022-10-16T16:50:00Z",
                        "paper_type": "workshop",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "281",
                        "paper_award": "",
                        "image_caption": "To be useful, provenance data needs to be visualized by the user, preferably in a user-friendly way.\nProvenance visualization is still an issue, especially for the lay public.\n\nMany visualization approaches require a high level of visual literacy such as the diagrams proposed by W3C-PROV.\nDashboard users need an easy to interpret visualizations and a more focused set of provenance metadata that enables users to receive precisely what they need to be more confident about the origin of the data.\n\n\nWe propose a three-layer provenance approach to show the most valuable provenance metadata to the user: those artifacts delivered to the client browser (datasets and visualizations); data artifacts produced on the server side (the last frontier of data joins and transformations aiming to deliver useful information to the user), and, \nfinally, the data owner, who is responsible for the maintenance of the data).\n\nShowing this thee-layer provenance we expect to provide the most important provenance metadata to the user and enable a user-friendly provenance visualization based on iconography and automatically generated infographics.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "w-trex-2027-qa",
                        "session_id": "w-trex-2",
                        "type": "Virtual Q+A",
                        "title": "Data Provenance Visualization in Brazilian Public Health Dashboards (Q+A)",
                        "contributors": [
                            "Johne Marcus Jarske"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "w-trex-2027",
                        "file_name": "",
                        "time_stamp": "2022-10-16T16:50:00Z",
                        "time_start": "2022-10-16T16:50:00Z",
                        "time_end": "2022-10-16T16:52:00Z",
                        "paper_type": "workshop",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "281",
                        "paper_award": "",
                        "image_caption": "To be useful, provenance data needs to be visualized by the user, preferably in a user-friendly way.\nProvenance visualization is still an issue, especially for the lay public.\n\nMany visualization approaches require a high level of visual literacy such as the diagrams proposed by W3C-PROV.\nDashboard users need an easy to interpret visualizations and a more focused set of provenance metadata that enables users to receive precisely what they need to be more confident about the origin of the data.\n\n\nWe propose a three-layer provenance approach to show the most valuable provenance metadata to the user: those artifacts delivered to the client browser (datasets and visualizations); data artifacts produced on the server side (the last frontier of data joins and transformations aiming to deliver useful information to the user), and, \nfinally, the data owner, who is responsible for the maintenance of the data).\n\nShowing this thee-layer provenance we expect to provide the most important provenance metadata to the user and enable a user-friendly provenance visualization based on iconography and automatically generated infographics.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "w-trex-prog-7",
                        "session_id": "w-trex-2",
                        "type": "In Person Other",
                        "title": "Discussion and Session Closing",
                        "contributors": [
                            "Event Chairs"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "",
                        "file_name": "",
                        "time_stamp": "2022-10-16T16:52:00Z",
                        "time_start": "2022-10-16T16:52:00Z",
                        "time_end": "2022-10-16T17:00:00Z",
                        "paper_type": "",
                        "keywords": [],
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    }
                ]
            }
        ]
    },
    "w-vis4good": {
        "event": "Visualization for Social Good 2022",
        "long_name": "Visualization for Social Good 2022",
        "event_type": "Workshop",
        "event_prefix": "w-vis4good",
        "event_description": "",
        "event_url": "",
        "organizers": [
            "Leilani Battle",
            "Michelle Borkin",
            "Lane Harrison",
            "Narges Mahyar",
            "Emily Wall"
        ],
        "sessions": [
            {
                "title": "Vis4Good: Opening + Keynote + 2 Papers",
                "session_id": "w-vis4good-1",
                "event_prefix": "w-vis4good",
                "track": "ok8",
                "livestream_id": "ok8-sun",
                "session_image": "w-vis4good-1.png",
                "chair": [
                    "Leilani Battle",
                    "Michelle Borkin",
                    "Lane Harrison",
                    "Narges Mahyar",
                    "Emily Wall"
                ],
                "organizers": [],
                "time_start": "2022-10-16T19:00:00Z",
                "time_end": "2022-10-16T20:15:00Z",
                "discord_category": "",
                "discord_channel": "oklahoma-st-8",
                "discord_channel_id": "1024600961295065128",
                "discord_link": "https://discord.com/channels/978320435554947082/1024600961295065128",
                "slido_link": "https://app.sli.do/event/i28UETZaiRWr1XwX6eKjcf?section=ae89e1c6-05b5-4dce-9468-cedd5098ebbe",
                "youtube_url": "https://youtu.be/nAzQZ6u21kE",
                "youtube_id": "nAzQZ6u21kE",
                "zoom_meeting": "",
                "zoom_password": "",
                "zoom_link": "",
                "ff_link": "",
                "ff_playlist": "",
                "time_slots": [
                    {
                        "slot_id": "w-vis4good-prog-1",
                        "session_id": "w-vis4good-1",
                        "type": "In Person Other",
                        "title": "Welcome",
                        "contributors": [
                            "Leilani Battle"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "",
                        "file_name": "",
                        "time_stamp": "2022-10-16T19:00:00Z",
                        "time_start": "2022-10-16T19:00:00Z",
                        "time_end": "2022-10-16T19:15:00Z",
                        "paper_type": "",
                        "keywords": [],
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "w-vis4good-4497-pres",
                        "session_id": "w-vis4good-1",
                        "type": "In Person Presentation",
                        "title": "Paper | Additional Perspectives on Data Equity",
                        "contributors": [
                            "Jonathan Schwabish"
                        ],
                        "authors": [
                            "Jonathan Schwabish",
                            "Alice Feng"
                        ],
                        "abstract": "",
                        "uid": "w-vis4good-4497",
                        "file_name": "",
                        "time_stamp": "2022-10-16T19:15:00Z",
                        "time_start": "2022-10-16T19:15:00Z",
                        "time_end": "2022-10-16T19:22:00Z",
                        "paper_type": "workshop",
                        "keywords": [],
                        "has_image": "0",
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "w-vis4good-4497-qa",
                        "session_id": "w-vis4good-1",
                        "type": "In Person Q+A",
                        "title": "QA | Additional Perspectives on Data Equity",
                        "contributors": [
                            "Jonathan Schwabish"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "w-vis4good-4497",
                        "file_name": "",
                        "time_stamp": "2022-10-16T19:22:00Z",
                        "time_start": "2022-10-16T19:22:00Z",
                        "time_end": "2022-10-16T19:25:00Z",
                        "paper_type": "workshop",
                        "keywords": [],
                        "has_image": "0",
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "w-vis4good-4801-pres",
                        "session_id": "w-vis4good-1",
                        "type": "In Person Presentation",
                        "title": "Paper | Exploring and Explaining Climate Change: Exploranation as a Visualization Pedagogy for Societal Action",
                        "contributors": [
                            "Lonni Besan\u00e7on"
                        ],
                        "authors": [
                            "Lonni Besan\u00e7on",
                            "Konrad Sch\u00f6nborn",
                            "Erik Sund\u00e9n",
                            "Yin He",
                            "Samuel Rising",
                            "Peteer Westerdahl",
                            "Patric Ljung",
                            "Josef Widestr\u00f6m",
                            "Charles Hansen",
                            "Anders Ynnerman"
                        ],
                        "abstract": "",
                        "uid": "w-vis4good-4801",
                        "file_name": "w-vis4good-4801_Besancon_Presentation.mp4",
                        "time_stamp": "2022-10-16T19:25:00Z",
                        "time_start": "2022-10-16T19:25:00Z",
                        "time_end": "2022-10-16T19:32:00Z",
                        "paper_type": "workshop",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "360",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "w-vis4good-4801-qa",
                        "session_id": "w-vis4good-1",
                        "type": "In Person Q+A",
                        "title": "QA | Exploring and Explaining Climate Change: Exploranation as a Visualization Pedagogy for Societal Action",
                        "contributors": [
                            "Lonni Besan\u00e7on"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "w-vis4good-4801",
                        "file_name": "",
                        "time_stamp": "2022-10-16T19:32:00Z",
                        "time_start": "2022-10-16T19:32:00Z",
                        "time_end": "2022-10-16T19:35:00Z",
                        "paper_type": "workshop",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "360",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "w-vis4good-9035-pres",
                        "session_id": "w-vis4good-1",
                        "type": "In Person Presentation",
                        "title": "Paper | Envisioning Situated Visualizations of Environmental Footprints in an Urban Environment",
                        "contributors": [
                            "Morgane Koval"
                        ],
                        "authors": [
                            "Yvonne Jansen",
                            "Federica Bucchieri",
                            "Pierre Dragicevic",
                            "Martin Hachet",
                            "Morgane Koval",
                            "L\u00e9ana Petiot",
                            "Arnaud Prouzeau",
                            "Dieter Schmalstieg",
                            "Lijie Yao",
                            "Petra Isenberg"
                        ],
                        "abstract": "",
                        "uid": "w-vis4good-9035",
                        "file_name": "w-vis4good-9035_Isenberg_Presentation.mp4",
                        "time_stamp": "2022-10-16T19:35:00Z",
                        "time_start": "2022-10-16T19:35:00Z",
                        "time_end": "2022-10-16T19:42:00Z",
                        "paper_type": "workshop",
                        "keywords": [],
                        "has_image": "0",
                        "has_video": "413",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "w-vis4good-9035-qa",
                        "session_id": "w-vis4good-1",
                        "type": "In Person Q+A",
                        "title": "QA | Envisioning Situated Visualizations of Environmental Footprints in an Urban Environment",
                        "contributors": [
                            "Morgane Koval"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "w-vis4good-9035",
                        "file_name": "",
                        "time_stamp": "2022-10-16T19:42:00Z",
                        "time_start": "2022-10-16T19:42:00Z",
                        "time_end": "2022-10-16T19:45:00Z",
                        "paper_type": "workshop",
                        "keywords": [],
                        "has_image": "0",
                        "has_video": "413",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "w-vis4good-1760-pres",
                        "session_id": "w-vis4good-1",
                        "type": "Virtual Presentation (live)",
                        "title": "Paper | Can data visualizations change minds? Identifying mechanisms of elaborative thinking and persuasion",
                        "contributors": [
                            "Douglas Markant"
                        ],
                        "authors": [
                            "Douglas Markant",
                            "Milad Rogha",
                            "Alireza Karduni",
                            "Ryan Wesslen",
                            "Wenwen Dou"
                        ],
                        "abstract": "",
                        "uid": "w-vis4good-1760",
                        "file_name": "",
                        "time_stamp": "2022-10-16T19:45:00Z",
                        "time_start": "2022-10-16T19:45:00Z",
                        "time_end": "2022-10-16T19:52:00Z",
                        "paper_type": "workshop",
                        "keywords": [],
                        "has_image": "0",
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "w-vis4good-1760-qa",
                        "session_id": "w-vis4good-1",
                        "type": "Virtual Q+A",
                        "title": "QA | Can data visualizations change minds? Identifying mechanisms of elaborative thinking and persuasion",
                        "contributors": [
                            "Douglas Markant"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "w-vis4good-1760",
                        "file_name": "",
                        "time_stamp": "2022-10-16T19:52:00Z",
                        "time_start": "2022-10-16T19:52:00Z",
                        "time_end": "2022-10-16T19:55:00Z",
                        "paper_type": "workshop",
                        "keywords": [],
                        "has_image": "0",
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "w-vis4good-3322-pres",
                        "session_id": "w-vis4good-1",
                        "type": "Virtual Presentation (live)",
                        "title": "Paper | Representing Marginalized Populations: Challenges in Anthropographics",
                        "contributors": [
                            "Priya Dhawka"
                        ],
                        "authors": [
                            "Priya Dhawka",
                            "Helen Ai He",
                            "Wesley Willett"
                        ],
                        "abstract": "",
                        "uid": "w-vis4good-3322",
                        "file_name": "w-vis4good-3322_Dhawka_Presentation.mp4",
                        "time_stamp": "2022-10-16T19:55:00Z",
                        "time_start": "2022-10-16T19:55:00Z",
                        "time_end": "2022-10-16T20:02:00Z",
                        "paper_type": "workshop",
                        "keywords": [],
                        "has_image": "0",
                        "has_video": "431",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "w-vis4good-3322-qa",
                        "session_id": "w-vis4good-1",
                        "type": "Virtual Q+A",
                        "title": "QA | Representing Marginalized Populations: Challenges in Anthropographics",
                        "contributors": [
                            "Priya Dhawka"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "w-vis4good-3322",
                        "file_name": "",
                        "time_stamp": "2022-10-16T20:02:00Z",
                        "time_start": "2022-10-16T20:02:00Z",
                        "time_end": "2022-10-16T20:05:00Z",
                        "paper_type": "workshop",
                        "keywords": [],
                        "has_image": "0",
                        "has_video": "431",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "w-vis4good-6296-pres",
                        "session_id": "w-vis4good-1",
                        "type": "Virtual Presentation (live)",
                        "title": "Paper | Data Bricks Space Mission: Teaching Kids about Data with Physicalization",
                        "contributors": [
                            "Lorenzo Ambrosini"
                        ],
                        "authors": [
                            "Lorenzo Ambrosini",
                            "Miriah Meyer"
                        ],
                        "abstract": "",
                        "uid": "w-vis4good-6296",
                        "file_name": "",
                        "time_stamp": "2022-10-16T20:05:00Z",
                        "time_start": "2022-10-16T20:05:00Z",
                        "time_end": "2022-10-16T20:12:00Z",
                        "paper_type": "workshop",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "The Data Bricks Space Mission is a role-playing physicalization activity for kids aged 10-12 years to engage them with the abstract concept of data. The activity's toolkit includes Lego bricks to represent data and build a visualization and the teachers' manual to guide students through the activity.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "w-vis4good-6296-qa",
                        "session_id": "w-vis4good-1",
                        "type": "Virtual Q+A",
                        "title": "QA | Data Bricks Space Mission: Teaching Kids about Data with Physicalization",
                        "contributors": [
                            "Lorenzo Ambrosini"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "w-vis4good-6296",
                        "file_name": "",
                        "time_stamp": "2022-10-16T20:12:00Z",
                        "time_start": "2022-10-16T20:12:00Z",
                        "time_end": "2022-10-16T20:15:00Z",
                        "paper_type": "workshop",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "The Data Bricks Space Mission is a role-playing physicalization activity for kids aged 10-12 years to engage them with the abstract concept of data. The activity's toolkit includes Lego bricks to represent data and build a visualization and the teachers' manual to guide students through the activity.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "",
                        "ff_id": ""
                    }
                ]
            },
            {
                "title": "Vis4Good: 7 Papers + 1 Poster + Closing",
                "session_id": "w-vis4good-2",
                "event_prefix": "w-vis4good",
                "track": "ok8",
                "livestream_id": "ok8-sun",
                "session_image": "w-vis4good-2.png",
                "chair": [
                    "Leilani Battle",
                    "Michelle Borkin",
                    "Lane Harrison",
                    "Narges Mahyar",
                    "Emily Wall"
                ],
                "organizers": [],
                "time_start": "2022-10-16T20:45:00Z",
                "time_end": "2022-10-16T22:00:00Z",
                "discord_category": "",
                "discord_channel": "oklahoma-st-8",
                "discord_channel_id": "1024600961295065128",
                "discord_link": "https://discord.com/channels/978320435554947082/1024600961295065128",
                "slido_link": "https://app.sli.do/event/i28UETZaiRWr1XwX6eKjcf?section=ae89e1c6-05b5-4dce-9468-cedd5098ebbe",
                "youtube_url": "https://youtu.be/nAzQZ6u21kE",
                "youtube_id": "nAzQZ6u21kE",
                "zoom_meeting": "",
                "zoom_password": "",
                "zoom_link": "",
                "ff_link": "",
                "ff_playlist": "",
                "time_slots": [
                    {
                        "slot_id": "w-vis4good-2457-pres",
                        "session_id": "w-vis4good-2",
                        "type": "Virtual Presentation (pre-recorded)",
                        "title": "Paper | Ten Challenges and Explainable Analogs of growth functions and distributions for statistical literacy and fluency",
                        "contributors": [
                            "Georges Hattab"
                        ],
                        "authors": [
                            "Georges Hattab"
                        ],
                        "abstract": "",
                        "uid": "w-vis4good-2457",
                        "file_name": "w-vis4good-2457_Hattab_Presentation.mp4",
                        "time_stamp": "2022-10-16T20:45:00Z",
                        "time_start": "2022-10-16T20:45:00Z",
                        "time_end": "2022-10-16T20:52:00Z",
                        "paper_type": "workshop",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "451",
                        "paper_award": "",
                        "image_caption": "The Linear Function. The first challenge of the Ten Challenges and Explainable Analogs of Growth Functions and Distributions for Statistical Literacy and Fluency.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "w-vis4good-2457-qa",
                        "session_id": "w-vis4good-2",
                        "type": "Virtual Q+A",
                        "title": "QA | Ten Challenges and Explainable Analogs of growth functions and distributions for statistical literacy and fluency",
                        "contributors": [
                            "Georges Hattab"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "w-vis4good-2457",
                        "file_name": "",
                        "time_stamp": "2022-10-16T20:52:00Z",
                        "time_start": "2022-10-16T20:52:00Z",
                        "time_end": "2022-10-16T20:55:00Z",
                        "paper_type": "workshop",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "451",
                        "paper_award": "",
                        "image_caption": "The Linear Function. The first challenge of the Ten Challenges and Explainable Analogs of Growth Functions and Distributions for Statistical Literacy and Fluency.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "w-vis4good-prog-4",
                        "session_id": "w-vis4good-2",
                        "type": "Virtual Presentation (live)",
                        "title": "Poster Talk | Teaching Data Visualization for Social Impact",
                        "contributors": [
                            "Nil Tuzcu"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "",
                        "file_name": "",
                        "time_stamp": "2022-10-16T20:55:00Z",
                        "time_start": "2022-10-16T20:55:00Z",
                        "time_end": "2022-10-16T21:02:00Z",
                        "paper_type": "",
                        "keywords": [],
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "w-vis4good-prog-6",
                        "session_id": "w-vis4good-2",
                        "type": "In Person Other",
                        "title": "Break-out Session: Brainstorming Visualization for Social Good",
                        "contributors": [
                            "Narges Mayhar"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "",
                        "file_name": "",
                        "time_stamp": "2022-10-16T21:02:00Z",
                        "time_start": "2022-10-16T21:02:00Z",
                        "time_end": "2022-10-16T21:59:00Z",
                        "paper_type": "",
                        "keywords": [],
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "w-vis4good-prog-5",
                        "session_id": "w-vis4good-2",
                        "type": "In Person Other",
                        "title": "Closing remarks",
                        "contributors": [
                            "Leilani Battle"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "",
                        "file_name": "",
                        "time_stamp": "2022-10-16T21:59:00Z",
                        "time_start": "2022-10-16T21:59:00Z",
                        "time_end": "2022-10-16T22:00:00Z",
                        "paper_type": "",
                        "keywords": [],
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    }
                ]
            }
        ]
    },
    "w-viscomm": {
        "event": "VisComm: Fifth Workshop on Visualization for Communication",
        "long_name": "VisComm: Fifth Workshop on Visualization for Communication",
        "event_type": "Workshop",
        "event_prefix": "w-viscomm",
        "event_description": "",
        "event_url": "",
        "organizers": [
            "Barbara Millet",
            "Jonathan Schwabish",
            "Alvitta Ottley",
            "Alice Feng"
        ],
        "sessions": [
            {
                "title": "VisComm: Opening, Presentations, and Late Breaking Work",
                "session_id": "w-viscomm-1",
                "event_prefix": "w-viscomm",
                "track": "ok8",
                "livestream_id": "ok8-mon",
                "session_image": "w-viscomm-1.png",
                "chair": [
                    "Barbara Millet",
                    "Jonathan Schwabish",
                    "Alvitta Ottley",
                    "Alice Feng"
                ],
                "organizers": [],
                "time_start": "2022-10-17T14:00:00Z",
                "time_end": "2022-10-17T15:15:00Z",
                "discord_category": "",
                "discord_channel": "oklahoma-st-8",
                "discord_channel_id": "1024600961295065128",
                "discord_link": "https://discord.com/channels/978320435554947082/1024600961295065128",
                "slido_link": "https://app.sli.do/event/i28UETZaiRWr1XwX6eKjcf?section=ae89e1c6-05b5-4dce-9468-cedd5098ebbe",
                "youtube_url": "https://youtu.be/IK4R_GvDs94",
                "youtube_id": "IK4R_GvDs94",
                "zoom_meeting": "",
                "zoom_password": "",
                "zoom_link": "",
                "ff_link": "",
                "ff_playlist": "",
                "time_slots": [
                    {
                        "slot_id": "w-viscomm-prog-1",
                        "session_id": "w-viscomm-1",
                        "type": "In Person Other",
                        "title": "Opening",
                        "contributors": [
                            "Jonathon Schwabish"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "",
                        "file_name": "",
                        "time_stamp": "2022-10-17T14:00:00Z",
                        "time_start": "2022-10-17T14:00:00Z",
                        "time_end": "2022-10-17T14:05:00Z",
                        "paper_type": "",
                        "keywords": [],
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "w-viscomm-full-1006-pres",
                        "session_id": "w-viscomm-1",
                        "type": "Virtual Presentation (pre-recorded)",
                        "title": "How Do Captions Affect Visualization Reading?",
                        "contributors": [
                            "Shelly Cheng",
                            "Hazel Zhu/Barbara"
                        ],
                        "authors": [
                            "Hazel Zhu",
                            "Shelly Cheng",
                            "eugene Wu"
                        ],
                        "abstract": "",
                        "uid": "w-viscomm-1006",
                        "file_name": "w-viscomm-1006_Zhu_Presentation.mp4",
                        "time_stamp": "2022-10-17T14:05:00Z",
                        "time_start": "2022-10-17T14:05:00Z",
                        "time_end": "2022-10-17T14:14:00Z",
                        "paper_type": "workshop",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "540",
                        "paper_award": "",
                        "image_caption": "This figure shows the percentage of takeaways mentioning primary or secondary features in single and multi-line charts when captions vary in two perspectives: the captioned feature and the semantic level. In the upper-left quadrant, the green bars are higher than the yellow bars, indicating statistical level captions highlight primary features in single-line charts more effectively. In the other three quadrants, especially the lower two quadrants, the yellow bars are higher than the green bars. This result shows that perceptual level captions increase the memorability of medium or low salience features, such as primary and secondary features in multi-line charts.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "w-viscomm-full-1006-qa",
                        "session_id": "w-viscomm-1",
                        "type": "Virtual Q+A",
                        "title": "How Do Captions Affect Visualization Reading? (Q+A)",
                        "contributors": [
                            "Shelly Cheng",
                            "Hazel Zhu/Barbara"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "w-viscomm-1006",
                        "file_name": "",
                        "time_stamp": "2022-10-17T14:14:00Z",
                        "time_start": "2022-10-17T14:14:00Z",
                        "time_end": "2022-10-17T14:17:00Z",
                        "paper_type": "workshop",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "540",
                        "paper_award": "",
                        "image_caption": "This figure shows the percentage of takeaways mentioning primary or secondary features in single and multi-line charts when captions vary in two perspectives: the captioned feature and the semantic level. In the upper-left quadrant, the green bars are higher than the yellow bars, indicating statistical level captions highlight primary features in single-line charts more effectively. In the other three quadrants, especially the lower two quadrants, the yellow bars are higher than the green bars. This result shows that perceptual level captions increase the memorability of medium or low salience features, such as primary and secondary features in multi-line charts.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "w-viscomm-full-1007-pres",
                        "session_id": "w-viscomm-1",
                        "type": "Virtual Presentation (pre-recorded)",
                        "title": "Scoping the Future of Visualization Literacy: A Review",
                        "contributors": [
                            "Mara Solen"
                        ],
                        "authors": [
                            "Mara Solen"
                        ],
                        "abstract": "",
                        "uid": "w-viscomm-1007",
                        "file_name": "w-viscomm-1007_Solen_Presentation.mp4",
                        "time_stamp": "2022-10-17T14:17:00Z",
                        "time_start": "2022-10-17T14:17:00Z",
                        "time_end": "2022-10-17T14:26:00Z",
                        "paper_type": "workshop",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "524",
                        "paper_award": "",
                        "image_caption": "A diagram showing the process of this work. It begins as a survey of 20 papers then branches off to thematic analysis which results in a definition for visualization literacy and an analysis of the participant pools of the studies included in this review that results in a set of demographic gaps. Below, there are two images, one is a screenshot of an educational visualization game and one is a picture of a museum exhibit displaying an interactive visualization tool.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "w-viscomm-full-1007-qa",
                        "session_id": "w-viscomm-1",
                        "type": "Virtual Q+A",
                        "title": "Scoping the Future of Visualization Literacy: A Review (Q+A)",
                        "contributors": [
                            "Mara Solen"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "w-viscomm-1007",
                        "file_name": "",
                        "time_stamp": "2022-10-17T14:26:00Z",
                        "time_start": "2022-10-17T14:26:00Z",
                        "time_end": "2022-10-17T14:29:00Z",
                        "paper_type": "workshop",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "524",
                        "paper_award": "",
                        "image_caption": "A diagram showing the process of this work. It begins as a survey of 20 papers then branches off to thematic analysis which results in a definition for visualization literacy and an analysis of the participant pools of the studies included in this review that results in a set of demographic gaps. Below, there are two images, one is a screenshot of an educational visualization game and one is a picture of a museum exhibit displaying an interactive visualization tool.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "w-viscomm-full-1014-pres",
                        "session_id": "w-viscomm-1",
                        "type": "In Person Presentation",
                        "title": "Negotiating visualization minimalism: a preliminary analysis of Twitter conversations",
                        "contributors": [
                            "Prakash Shukla"
                        ],
                        "authors": [
                            "Prakash Chandra Shukla",
                            "Paul Parsons"
                        ],
                        "abstract": "",
                        "uid": "w-viscomm-1014",
                        "file_name": "",
                        "time_stamp": "2022-10-17T14:29:00Z",
                        "time_start": "2022-10-17T14:29:00Z",
                        "time_end": "2022-10-17T14:38:00Z",
                        "paper_type": "workshop",
                        "keywords": [],
                        "has_image": "0",
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "w-viscomm-full-1014-qa",
                        "session_id": "w-viscomm-1",
                        "type": "In Person Q+A",
                        "title": "Negotiating visualization minimalism: a preliminary analysis of Twitter conversations (Q+A)",
                        "contributors": [
                            "Prakash Shukla"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "w-viscomm-1014",
                        "file_name": "",
                        "time_stamp": "2022-10-17T14:38:00Z",
                        "time_start": "2022-10-17T14:38:00Z",
                        "time_end": "2022-10-17T14:41:00Z",
                        "paper_type": "workshop",
                        "keywords": [],
                        "has_image": "0",
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "w-viscomm-full-1013-pres",
                        "session_id": "w-viscomm-1",
                        "type": "In Person Presentation",
                        "title": "User Engagement with COVID-19 Visualizations on Twitter",
                        "contributors": [
                            "Saugat Pandey"
                        ],
                        "authors": [
                            "Robert Kasumba",
                            "Saugat Pandey",
                            "Vishesh Patel",
                            "Micah Wolfson",
                            "Alvitta Ottley"
                        ],
                        "abstract": "",
                        "uid": "w-viscomm-1013",
                        "file_name": "w-viscomm-1013_Pandey_Presentation.mp4",
                        "time_stamp": "2022-10-17T14:41:00Z",
                        "time_start": "2022-10-17T14:41:00Z",
                        "time_end": "2022-10-17T14:50:00Z",
                        "paper_type": "workshop",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "453",
                        "paper_award": "",
                        "image_caption": "Comparing user engagement between the tweets with and without visualization.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "w-viscomm-full-1013-qa",
                        "session_id": "w-viscomm-1",
                        "type": "In Person Q+A",
                        "title": "User Engagement with COVID-19 Visualizations on Twitter (Q+A)",
                        "contributors": [
                            "Saugat Pandey"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "w-viscomm-1013",
                        "file_name": "",
                        "time_stamp": "2022-10-17T14:50:00Z",
                        "time_start": "2022-10-17T14:50:00Z",
                        "time_end": "2022-10-17T14:53:00Z",
                        "paper_type": "workshop",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "453",
                        "paper_award": "",
                        "image_caption": "Comparing user engagement between the tweets with and without visualization.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "w-viscomm-full-1005-pres",
                        "session_id": "w-viscomm-1",
                        "type": "In Person Presentation",
                        "title": "A Qualitative Evaluation and Taxonomy of Student Annotations on Bar Charts",
                        "contributors": [
                            "Md Dilshadur Rahman"
                        ],
                        "authors": [
                            "Md Dilshadur Rahman",
                            "Ghulam Jilani Quadri",
                            "Paul Rosen"
                        ],
                        "abstract": "",
                        "uid": "w-viscomm-1005",
                        "file_name": "w-viscomm-1005_Rahman_Presentation.mp4",
                        "time_stamp": "2022-10-17T14:53:00Z",
                        "time_start": "2022-10-17T14:53:00Z",
                        "time_end": "2022-10-17T15:02:00Z",
                        "paper_type": "workshop",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "422",
                        "paper_award": "",
                        "image_caption": "Three examples of annotated bar chart submissions. (a) One student used rectangles for filtering, text on bars for\nretrieving values and pointing out the extrema, and a legend for filtering important dates. (b) A second student annotated the chart with ellipses, rectangles, and lines for filtering, and text for filtering and as an identifier. (c) A final student used rectangular shapes and highlights for filtering, texts as an identifier, a trend line for finding extrema, and a legend for filtering.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "w-viscomm-full-1005-qa",
                        "session_id": "w-viscomm-1",
                        "type": "In Person Q+A",
                        "title": "A Qualitative Evaluation and Taxonomy of Student Annotations on Bar Charts (Q+A)",
                        "contributors": [
                            "Md Dilshadur Rahman"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "w-viscomm-1005",
                        "file_name": "",
                        "time_stamp": "2022-10-17T15:02:00Z",
                        "time_start": "2022-10-17T15:02:00Z",
                        "time_end": "2022-10-17T15:05:00Z",
                        "paper_type": "workshop",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "422",
                        "paper_award": "",
                        "image_caption": "Three examples of annotated bar chart submissions. (a) One student used rectangles for filtering, text on bars for\nretrieving values and pointing out the extrema, and a legend for filtering important dates. (b) A second student annotated the chart with ellipses, rectangles, and lines for filtering, and text for filtering and as an identifier. (c) A final student used rectangular shapes and highlights for filtering, texts as an identifier, a trend line for finding extrema, and a legend for filtering.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "w-viscomm-prog-2",
                        "session_id": "w-viscomm-1",
                        "type": "Virtual Presentation (live)",
                        "title": "[LBW] Persuasive Interactive Data Visualizations: Perspective Matters",
                        "contributors": [
                            "Lynne Cotter"
                        ],
                        "authors": [
                            "Lynne M. Cotter"
                        ],
                        "abstract": "",
                        "uid": "",
                        "file_name": "",
                        "time_stamp": "2022-10-17T15:05:00Z",
                        "time_start": "2022-10-17T15:05:00Z",
                        "time_end": "2022-10-17T15:08:00Z",
                        "paper_type": "",
                        "keywords": [],
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "w-viscomm-prog-3",
                        "session_id": "w-viscomm-1",
                        "type": "Virtual Q+A",
                        "title": "[LBW] Persuasive Interactive Data Visualizations: Perspective Matters (Q+A)",
                        "contributors": [
                            "Lynne Cotter"
                        ],
                        "authors": [
                            "Lynne M. Cotter"
                        ],
                        "abstract": "",
                        "uid": "",
                        "file_name": "",
                        "time_stamp": "2022-10-17T15:08:00Z",
                        "time_start": "2022-10-17T15:08:00Z",
                        "time_end": "2022-10-17T15:10:00Z",
                        "paper_type": "",
                        "keywords": [],
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "w-viscomm-prog-4",
                        "session_id": "w-viscomm-1",
                        "type": "In Person Presentation",
                        "title": "[LBW] Visual Communication of Potential Anomalies with Boundary Lines in ISS Mission Control",
                        "contributors": [
                            "Zixu Zhang"
                        ],
                        "authors": [
                            "Paul C. Parsons",
                            "Zixu Zhang"
                        ],
                        "abstract": "",
                        "uid": "",
                        "file_name": "",
                        "time_stamp": "2022-10-17T15:10:00Z",
                        "time_start": "2022-10-17T15:10:00Z",
                        "time_end": "2022-10-17T15:13:00Z",
                        "paper_type": "",
                        "keywords": [],
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "w-viscomm-prog-5",
                        "session_id": "w-viscomm-1",
                        "type": "In Person Q+A",
                        "title": "[LBW] Visual Communication of Potential Anomalies with Boundary Lines in ISS Mission Control (Q+A)",
                        "contributors": [
                            "Zixu Zhang"
                        ],
                        "authors": [
                            "Paul C. Parsons",
                            "Zixu Zhang"
                        ],
                        "abstract": "",
                        "uid": "",
                        "file_name": "",
                        "time_stamp": "2022-10-17T15:13:00Z",
                        "time_start": "2022-10-17T15:13:00Z",
                        "time_end": "2022-10-17T15:15:00Z",
                        "paper_type": "",
                        "keywords": [],
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    }
                ]
            },
            {
                "title": "VisComm: Panel and Closing",
                "session_id": "w-viscomm-2",
                "event_prefix": "w-viscomm",
                "track": "ok8",
                "livestream_id": "ok8-mon",
                "session_image": "w-viscomm-2.png",
                "chair": [
                    "Barbara Millet",
                    "Jonathan Schwabish",
                    "Alvitta Ottley",
                    "Alice Feng"
                ],
                "organizers": [],
                "time_start": "2022-10-17T15:45:00Z",
                "time_end": "2022-10-17T17:00:00Z",
                "discord_category": "",
                "discord_channel": "oklahoma-st-8",
                "discord_channel_id": "1024600961295065128",
                "discord_link": "https://discord.com/channels/978320435554947082/1024600961295065128",
                "slido_link": "https://app.sli.do/event/i28UETZaiRWr1XwX6eKjcf?section=ae89e1c6-05b5-4dce-9468-cedd5098ebbe",
                "youtube_url": "https://youtu.be/IK4R_GvDs94",
                "youtube_id": "IK4R_GvDs94",
                "zoom_meeting": "",
                "zoom_password": "",
                "zoom_link": "",
                "ff_link": "",
                "ff_playlist": "",
                "time_slots": [
                    {
                        "slot_id": "w-viscomm-prog-6",
                        "session_id": "w-viscomm-2",
                        "type": "Mixed Panel (virtual/in person)",
                        "title": "Panel",
                        "contributors": [
                            "Alvitta Ottley",
                            "Mike Bostock",
                            "Jessica Hullman",
                            "Sheelagh Carpendale",
                            "Duncan Geere"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "",
                        "file_name": "The Future of Visualization.mp4",
                        "time_stamp": "2022-10-17T15:45:00Z",
                        "time_start": "2022-10-17T15:45:00Z",
                        "time_end": "2022-10-17T16:45:00Z",
                        "paper_type": "",
                        "keywords": [],
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "w-viscomm-prog-7",
                        "session_id": "w-viscomm-2",
                        "type": "In Person Other",
                        "title": "Closing",
                        "contributors": [
                            "Alvitta Ottley"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "",
                        "file_name": "",
                        "time_stamp": "2022-10-17T16:45:00Z",
                        "time_start": "2022-10-17T16:45:00Z",
                        "time_end": "2022-10-17T17:00:00Z",
                        "paper_type": "",
                        "keywords": [],
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    }
                ]
            }
        ]
    },
    "w-vis4climate": {
        "event": "Viz4Climate: Workshop on High Impact Techniques for Visual Climate Science Communication",
        "long_name": "Viz4Climate: Workshop on High Impact Techniques for Visual Climate Science Communication",
        "event_type": "Workshop",
        "event_prefix": "w-vis4climate",
        "event_description": "",
        "event_url": "",
        "organizers": [
            "Helen-Nicole Kostis",
            "Mark SubbaRao",
            "Marlen Promann"
        ],
        "sessions": [
            {
                "title": "Vis4Climate: Opening, Keynote & Enlightening Session",
                "session_id": "w-vis4climate-1",
                "event_prefix": "w-vis4climate",
                "track": "ok8",
                "livestream_id": "ok8-mon",
                "session_image": "w-vis4climate-1.png",
                "chair": [
                    "Helen-Nicole Kostis",
                    "Mark SubbaRao",
                    "Marlen Promann"
                ],
                "organizers": [],
                "time_start": "2022-10-17T19:00:00Z",
                "time_end": "2022-10-17T20:15:00Z",
                "discord_category": "",
                "discord_channel": "oklahoma-st-8",
                "discord_channel_id": "1024600961295065128",
                "discord_link": "https://discord.com/channels/978320435554947082/1024600961295065128",
                "slido_link": "https://app.sli.do/event/i28UETZaiRWr1XwX6eKjcf?section=ae89e1c6-05b5-4dce-9468-cedd5098ebbe",
                "youtube_url": "https://youtu.be/IK4R_GvDs94",
                "youtube_id": "IK4R_GvDs94",
                "zoom_meeting": "",
                "zoom_password": "",
                "zoom_link": "",
                "ff_link": "",
                "ff_playlist": "",
                "time_slots": [
                    {
                        "slot_id": "w-vis4climate-prog-1",
                        "session_id": "w-vis4climate-1",
                        "type": "In Person Other",
                        "title": "Welcome & Keynote Introduction",
                        "contributors": [
                            "Helen-Nicole Kostis",
                            "Mark Subbarao",
                            "Andrew Christensen"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "",
                        "file_name": "",
                        "time_stamp": "2022-10-17T19:00:00Z",
                        "time_start": "2022-10-17T19:00:00Z",
                        "time_end": "2022-10-17T19:05:00Z",
                        "paper_type": "",
                        "keywords": [],
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "w-vis4climate-prog-2",
                        "session_id": "w-vis4climate-1",
                        "type": "In Person Other",
                        "title": "Keynote Presentation",
                        "contributors": [
                            "Ed Hawkins"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "",
                        "file_name": "",
                        "time_stamp": "2022-10-17T19:05:00Z",
                        "time_start": "2022-10-17T19:05:00Z",
                        "time_end": "2022-10-17T19:23:00Z",
                        "paper_type": "",
                        "keywords": [],
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "w-vis4climate-prog-3",
                        "session_id": "w-vis4climate-1",
                        "type": "Virtual Q+A",
                        "title": "Keynote Q+A",
                        "contributors": [
                            "Helen-Nicole Kostis",
                            "Mark Subbarao",
                            "Ed Hawkins"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "",
                        "file_name": "",
                        "time_stamp": "2022-10-17T19:23:00Z",
                        "time_start": "2022-10-17T19:23:00Z",
                        "time_end": "2022-10-17T19:30:00Z",
                        "paper_type": "",
                        "keywords": [],
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "w-vis4climate-prog-4",
                        "session_id": "w-vis4climate-1",
                        "type": "In Person Other",
                        "title": "Opening of Enlightening Show, discussion virtual",
                        "contributors": [
                            "Helen-Nicole Kostis",
                            "Mark Subbarao"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "",
                        "file_name": "",
                        "time_stamp": "2022-10-17T19:30:00Z",
                        "time_start": "2022-10-17T19:30:00Z",
                        "time_end": "2022-10-17T19:35:00Z",
                        "paper_type": "",
                        "keywords": [],
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "w-vis4climate-1012-pres",
                        "session_id": "w-vis4climate-1",
                        "type": "In Person Presentation",
                        "title": "Presentation by Francesca Samsel",
                        "contributors": [
                            "Francesca Samsel"
                        ],
                        "authors": [
                            "Francesca Samsel",
                            "Jung Who Nam",
                            "Greg Abram",
                            "Mark Petersen"
                        ],
                        "abstract": "",
                        "uid": "w-vis4climate-1012",
                        "file_name": "w-vis4climate-1012_Samsel_Presentation.mp4",
                        "time_stamp": "2022-10-17T19:35:00Z",
                        "time_start": "2022-10-17T19:35:00Z",
                        "time_end": "2022-10-17T19:38:00Z",
                        "paper_type": "workshop",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "181",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "w-vis4climate-1023-pres",
                        "session_id": "w-vis4climate-1",
                        "type": "In Person Presentation",
                        "title": "Presentation by Helen-Nicole Kostis",
                        "contributors": [
                            "Helen-Nicole Kostis"
                        ],
                        "authors": [
                            "AJ Christensen",
                            "Helen-Nicole Kostis",
                            "Mark SubbaRao",
                            "Greg Shirah",
                            "Horace Mitchell"
                        ],
                        "abstract": "",
                        "uid": "w-vis4climate-1023",
                        "file_name": "w-vis4climate-1023_Kostis_Presentation.mp4",
                        "time_stamp": "2022-10-17T19:38:00Z",
                        "time_start": "2022-10-17T19:38:00Z",
                        "time_end": "2022-10-17T19:41:00Z",
                        "paper_type": "workshop",
                        "keywords": [],
                        "has_image": "0",
                        "has_video": "131",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "w-vis4climate-1018-pres",
                        "session_id": "w-vis4climate-1",
                        "type": "In Person Presentation",
                        "title": "Presentation by Foroozan Danshzand",
                        "contributors": [
                            "Foroozan Danshzand"
                        ],
                        "authors": [
                            "Foroozan Daneshzand",
                            "Charles Perin",
                            "Sheelagh Carpendale"
                        ],
                        "abstract": "",
                        "uid": "w-vis4climate-1018",
                        "file_name": "w-vis4climate-1018_1018_Presentation.mp4",
                        "time_stamp": "2022-10-17T19:41:00Z",
                        "time_start": "2022-10-17T19:41:00Z",
                        "time_end": "2022-10-17T19:44:00Z",
                        "paper_type": "workshop",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "181",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "w-vis4climate-1021-pres",
                        "session_id": "w-vis4climate-1",
                        "type": "In Person Presentation",
                        "title": "Presentation by Mark Subbarao",
                        "contributors": [
                            "Mark Subbarao"
                        ],
                        "authors": [
                            "Mark SubbaRao"
                        ],
                        "abstract": "",
                        "uid": "w-vis4climate-1021",
                        "file_name": "w-vis4climate-1021_Subbarao_Presentation.mp4",
                        "time_stamp": "2022-10-17T19:44:00Z",
                        "time_start": "2022-10-17T19:44:00Z",
                        "time_end": "2022-10-17T19:49:00Z",
                        "paper_type": "workshop",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "119",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "w-vis4climate-1025-pres",
                        "session_id": "w-vis4climate-1",
                        "type": "Virtual Presentation (pre-recorded)",
                        "title": "Presentation by Jason Leigh",
                        "contributors": [
                            "Jason Leigh"
                        ],
                        "authors": [
                            "Jason Leigh",
                            "Mahdi Belcaid",
                            "Ryan Theriot",
                            "Nurit Kirshenbaum",
                            "Roderick S Tabalba Jr.",
                            "Michael L. Rogers",
                            "Eva Moralez Peres",
                            "Kari Noe"
                        ],
                        "abstract": "",
                        "uid": "w-vis4climate-1025",
                        "file_name": "w-vis4climate-1025_Leigh_Presentation.mp4",
                        "time_stamp": "2022-10-17T19:49:00Z",
                        "time_start": "2022-10-17T19:49:00Z",
                        "time_end": "2022-10-17T19:52:00Z",
                        "paper_type": "workshop",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "97",
                        "paper_award": "",
                        "image_caption": "Physicalization of scenarios for transitioning Hawaii to 100% renewable energy generation by 2045.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "w-vis4climate-1009-pres",
                        "session_id": "w-vis4climate-1",
                        "type": "Virtual Presentation (pre-recorded)",
                        "title": "Presentation by Everardo Gonzalez",
                        "contributors": [
                            "Everardo Gonzalez"
                        ],
                        "authors": [
                            "Valentin Buck",
                            "Flemming St\u00e4bler",
                            "Everardo Gonz\u00e1lez",
                            "Christian Scharun"
                        ],
                        "abstract": "",
                        "uid": "w-vis4climate-1009",
                        "file_name": "w-vis4climate-1009_Gonzalez_Presentation.mp4",
                        "time_stamp": "2022-10-17T19:52:00Z",
                        "time_start": "2022-10-17T19:52:00Z",
                        "time_end": "2022-10-17T19:55:00Z",
                        "paper_type": "workshop",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "130",
                        "paper_award": "",
                        "image_caption": "The Digital Earth Viewer displaying the location of unaccounted gas platforms in the North Sea along with atmospheric methane traces.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "w-vis4climate-1019-pres",
                        "session_id": "w-vis4climate-1",
                        "type": "Virtual Presentation (pre-recorded)",
                        "title": "Presentation by Kalina Borkiewicz",
                        "contributors": [
                            "Kalina Borkiewicz"
                        ],
                        "authors": [
                            "Kalina Borkiewicz",
                            "Stuart Levy",
                            "Jeffrey D Carpenter",
                            "Donna Cox",
                            "Robert Patterson",
                            "AJ Christensen"
                        ],
                        "abstract": "",
                        "uid": "w-vis4climate-1019",
                        "file_name": "w-vis4climate-1019_Borkiewicz_Presentation.mp4",
                        "time_stamp": "2022-10-17T19:55:00Z",
                        "time_start": "2022-10-17T19:55:00Z",
                        "time_end": "2022-10-17T19:58:00Z",
                        "paper_type": "workshop",
                        "keywords": [],
                        "has_image": "0",
                        "has_video": "216",
                        "paper_award": "",
                        "image_caption": "Cinematic scientific visualization of collapsing Vavilov ice cap, from the film \"Atlas of a Changing Earth\". This 3D topology is based on data from the ArcticDEM project, and the color comes from Landsat satellite photography. The visualization is by the Advanced Visualization Lab, National Center for Supercomputing Applications, University of Illinois at Urbana-Champaign.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "w-vis4climate-1006-pres",
                        "session_id": "w-vis4climate-1",
                        "type": "Virtual Presentation (pre-recorded)",
                        "title": "Presentation by Daniel Sauter",
                        "contributors": [
                            "Daniel Sauter"
                        ],
                        "authors": [
                            "Daniel Sauter",
                            "Timon McPhearson",
                            "Xinyue Peng",
                            "Christopher Kennedy"
                        ],
                        "abstract": "",
                        "uid": "w-vis4climate-1006",
                        "file_name": "w-vis4climate-1006_Sauter_Presentation.mp4",
                        "time_stamp": "2022-10-17T19:58:00Z",
                        "time_start": "2022-10-17T19:58:00Z",
                        "time_end": "2022-10-17T20:01:00Z",
                        "paper_type": "workshop",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "163",
                        "paper_award": "",
                        "image_caption": "Ocellus XR: Table-top augmented reality view of heat risk in New York City. (Sauter, D., McPhearson, T., Peng, X. E., Wu, Y., Fan, P., Bowe, E., Outwater, J., & Kennedy, C.)",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "w-vis4climate-1027-pres",
                        "session_id": "w-vis4climate-1",
                        "type": "Virtual Presentation (pre-recorded)",
                        "title": "Presentation by Marta Fereira",
                        "contributors": [
                            "Marta Fereira"
                        ],
                        "authors": [
                            "Marta Ferreira",
                            "Valentina Nisi",
                            "Nuno Jardim Nunes"
                        ],
                        "abstract": "",
                        "uid": "w-vis4climate-1027",
                        "file_name": "w-vis4climate-1027_Ferreira_Presentation.mp4",
                        "time_stamp": "2022-10-17T20:01:00Z",
                        "time_start": "2022-10-17T20:01:00Z",
                        "time_end": "2022-10-17T20:04:00Z",
                        "paper_type": "workshop",
                        "keywords": [],
                        "has_image": "0",
                        "has_video": "184",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "w-vis4climate-1031-pres",
                        "session_id": "w-vis4climate-1",
                        "type": "Virtual Presentation (pre-recorded)",
                        "title": "Presentation by Antoine Bertin",
                        "contributors": [
                            "Antoine Bertin"
                        ],
                        "authors": [
                            "Antoine Bertin"
                        ],
                        "abstract": "",
                        "uid": "w-vis4climate-1031",
                        "file_name": "w-vis4climate-1031_333hz_Presentation.mp4",
                        "time_stamp": "2022-10-17T20:04:00Z",
                        "time_start": "2022-10-17T20:04:00Z",
                        "time_end": "2022-10-17T20:07:00Z",
                        "paper_type": "workshop",
                        "keywords": [],
                        "has_image": "0",
                        "has_video": "78",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "w-vis4climate-1004-pres",
                        "session_id": "w-vis4climate-1",
                        "type": "Virtual Presentation (pre-recorded)",
                        "title": "Presentation by Kel Elkins",
                        "contributors": [
                            "Kel Elkins"
                        ],
                        "authors": [
                            "Kel Elkins"
                        ],
                        "abstract": "",
                        "uid": "w-vis4climate-1004",
                        "file_name": "w-vis4climate-1004_Elkins_Presentation.mp4",
                        "time_stamp": "2022-10-17T20:07:00Z",
                        "time_start": "2022-10-17T20:07:00Z",
                        "time_end": "2022-10-17T20:10:00Z",
                        "paper_type": "workshop",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "168",
                        "paper_award": "",
                        "image_caption": "A global view of GEDI vegetation height data",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "w-vis4climate-1026-pres",
                        "session_id": "w-vis4climate-1",
                        "type": "Virtual Presentation (pre-recorded)",
                        "title": "Presentation by Jason Haga",
                        "contributors": [
                            "Jason Haga"
                        ],
                        "authors": [
                            "Jason Haga",
                            "Jason Leigh",
                            "Mores Prachyabrued"
                        ],
                        "abstract": "",
                        "uid": "w-vis4climate-1026",
                        "file_name": "w-vis4climate-1026_Haga_Presentation.mp4",
                        "time_stamp": "2022-10-17T20:10:00Z",
                        "time_start": "2022-10-17T20:10:00Z",
                        "time_end": "2022-10-17T20:13:00Z",
                        "paper_type": "workshop",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "179",
                        "paper_award": "",
                        "image_caption": "Landslides are devastating events in Southeast Asia, causing extensive damage and loss of life. Understanding and responding to these events remains a significant challenge. To address this, we initiated the LandSAGE program with the goal of bringing visualization tools, expertise, and capacity for landslide disaster management in Southeast Asia.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "w-vis4climate-1033-pres",
                        "session_id": "w-vis4climate-1",
                        "type": "Virtual Presentation (pre-recorded)",
                        "title": "Presentation by Cristina Tarquini",
                        "contributors": [
                            "Cristina Tarquini"
                        ],
                        "authors": [
                            "Cristina Tarquini"
                        ],
                        "abstract": "",
                        "uid": "w-vis4climate-1033",
                        "file_name": "w-vis4climate-1033_Tarquini_Presentation.mp4",
                        "time_stamp": "2022-10-17T20:13:00Z",
                        "time_start": "2022-10-17T20:13:00Z",
                        "time_end": "2022-10-17T20:15:00Z",
                        "paper_type": "workshop",
                        "keywords": [],
                        "has_image": "0",
                        "has_video": "181",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "",
                        "ff_id": ""
                    }
                ]
            },
            {
                "title": "Vis4Climate: Panel, Co-Creation & Closing Remarks",
                "session_id": "w-vis4climate-2",
                "event_prefix": "w-vis4climate",
                "track": "ok8",
                "livestream_id": "ok8-mon",
                "session_image": "w-vis4climate-2.png",
                "chair": [
                    "Helen-Nicole Kostis",
                    "Mark SubbaRao",
                    "Marlen Promann"
                ],
                "organizers": [],
                "time_start": "2022-10-17T20:45:00Z",
                "time_end": "2022-10-17T22:00:00Z",
                "discord_category": "",
                "discord_channel": "oklahoma-st-8",
                "discord_channel_id": "1024600961295065128",
                "discord_link": "https://discord.com/channels/978320435554947082/1024600961295065128",
                "slido_link": "https://app.sli.do/event/i28UETZaiRWr1XwX6eKjcf?section=ae89e1c6-05b5-4dce-9468-cedd5098ebbe",
                "youtube_url": "https://youtu.be/IK4R_GvDs94",
                "youtube_id": "IK4R_GvDs94",
                "zoom_meeting": "",
                "zoom_password": "",
                "zoom_link": "",
                "ff_link": "",
                "ff_playlist": "",
                "time_slots": [
                    {
                        "slot_id": "w-vis4climate-prog-5",
                        "session_id": "w-vis4climate-2",
                        "type": "In Person Other",
                        "title": "Introduction by Mark Subbarao (Moderator)",
                        "contributors": [
                            "Mark Subbarao"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "",
                        "file_name": "",
                        "time_stamp": "2022-10-17T20:45:00Z",
                        "time_start": "2022-10-17T20:45:00Z",
                        "time_end": "2022-10-17T20:47:00Z",
                        "paper_type": "",
                        "keywords": [],
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "w-vis4climate-prog-6",
                        "session_id": "w-vis4climate-2",
                        "type": "In Person Presentation",
                        "title": "Presentation by Min Chen",
                        "contributors": [
                            "Min Chen"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "",
                        "file_name": "",
                        "time_stamp": "2022-10-17T20:47:00Z",
                        "time_start": "2022-10-17T20:47:00Z",
                        "time_end": "2022-10-17T20:54:00Z",
                        "paper_type": "",
                        "keywords": [],
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "w-vis4climate-prog-7",
                        "session_id": "w-vis4climate-2",
                        "type": "Virtual Presentation (pre-recorded)",
                        "title": "Presentation by Zack Labe",
                        "contributors": [
                            "Zack Labe"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "",
                        "file_name": "vis4climate-Panel_Labe_7minPresentation.mp4",
                        "time_stamp": "2022-10-17T20:54:00Z",
                        "time_start": "2022-10-17T20:54:00Z",
                        "time_end": "2022-10-17T21:01:00Z",
                        "paper_type": "",
                        "keywords": [],
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "w-vis4climate-prog-8",
                        "session_id": "w-vis4climate-2",
                        "type": "Virtual Presentation (pre-recorded)",
                        "title": "Presentation by Andrea Polli",
                        "contributors": [
                            "Andrea Polli"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "",
                        "file_name": "vis4climate-Panel_Polli_7minPresentation.mp4",
                        "time_stamp": "2022-10-17T21:01:00Z",
                        "time_start": "2022-10-17T21:01:00Z",
                        "time_end": "2022-10-17T21:08:00Z",
                        "paper_type": "",
                        "keywords": [],
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "w-vis4climate-prog-9",
                        "session_id": "w-vis4climate-2",
                        "type": "Virtual Presentation (live)",
                        "title": "Presentation by Reyhaneh Maktoufi",
                        "contributors": [
                            "Reyhaneh Maktoufi"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "",
                        "file_name": "",
                        "time_stamp": "2022-10-17T21:08:00Z",
                        "time_start": "2022-10-17T21:08:00Z",
                        "time_end": "2022-10-17T21:15:00Z",
                        "paper_type": "",
                        "keywords": [],
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "w-vis4climate-prog-10",
                        "session_id": "w-vis4climate-2",
                        "type": "Mixed Panel (virtual/in person)",
                        "title": "Panel Discussion & Q+A (questions from audience)",
                        "contributors": [
                            "Mark Subbarao",
                            "Min Chen",
                            "Zack Labe",
                            "Andrea Polli",
                            "Reyhaneh Maktoufi"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "",
                        "file_name": "",
                        "time_stamp": "2022-10-17T21:15:00Z",
                        "time_start": "2022-10-17T21:15:00Z",
                        "time_end": "2022-10-17T21:35:00Z",
                        "paper_type": "",
                        "keywords": [],
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "w-vis4climate-prog-11",
                        "session_id": "w-vis4climate-2",
                        "type": "Mixed Panel (virtual/in person)",
                        "title": "Discussion",
                        "contributors": [
                            "Helen-Nicole Kostis",
                            "Andrew Christensen"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "",
                        "file_name": "",
                        "time_stamp": "2022-10-17T21:35:00Z",
                        "time_start": "2022-10-17T21:35:00Z",
                        "time_end": "2022-10-17T21:50:00Z",
                        "paper_type": "",
                        "keywords": [],
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "w-vis4climate-prog-12",
                        "session_id": "w-vis4climate-2",
                        "type": "In Person Presentation",
                        "title": "Summary",
                        "contributors": [
                            "Helen-Nicole Kostis"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "",
                        "file_name": "",
                        "time_stamp": "2022-10-17T21:50:00Z",
                        "time_start": "2022-10-17T21:50:00Z",
                        "time_end": "2022-10-17T22:00:00Z",
                        "paper_type": "",
                        "keywords": [],
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    }
                ]
            }
        ]
    },
    "t-nlp4vis": {
        "event": "NLP4Vis: Natural Language Processing for Information Visualization",
        "long_name": "NLP4Vis: Natural Language Processing for Information Visualization",
        "event_type": "Tutorial",
        "event_prefix": "t-nlp4vis",
        "event_description": "",
        "event_url": "",
        "organizers": [
            "Enamul Hoque",
            "Shafiq Joty"
        ],
        "sessions": [
            {
                "title": "NLP4Vis: Natural Language Processing for Information Visualization 1",
                "session_id": "t-nlp4vis-1",
                "event_prefix": "t-nlp4vis",
                "track": "pinon",
                "livestream_id": "pinon-sun",
                "session_image": "t-nlp4vis-1.png",
                "chair": [
                    "Enamul Hoque",
                    "Shafiq Joty"
                ],
                "organizers": [],
                "time_start": "2022-10-16T14:00:00Z",
                "time_end": "2022-10-16T15:15:00Z",
                "discord_category": "",
                "discord_channel": "pinon",
                "discord_channel_id": "1024601027644760074",
                "discord_link": "https://discord.com/channels/978320435554947082/1024601027644760074",
                "slido_link": "https://app.sli.do/event/i28UETZaiRWr1XwX6eKjcf?section=bbc002b7-d3ae-48bc-b2d3-0624a7bfed06",
                "youtube_url": "https://youtu.be/-wvdw_VQBfk",
                "youtube_id": "-wvdw_VQBfk",
                "zoom_meeting": "",
                "zoom_password": "",
                "zoom_link": "",
                "ff_link": "",
                "ff_playlist": "",
                "time_slots": [
                    {
                        "slot_id": "t-nlp4vis-prog-1",
                        "session_id": "t-nlp4vis-1",
                        "type": "Mixed Panel (virtual/in person)",
                        "title": "Overview",
                        "contributors": [
                            "Enamul Hoque",
                            "Shafiq Joty"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "",
                        "file_name": "",
                        "time_stamp": "2022-10-16T14:00:00Z",
                        "time_start": "2022-10-16T14:00:00Z",
                        "time_end": "2022-10-16T14:15:00Z",
                        "paper_type": "",
                        "keywords": [],
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "t-nlp4vis-prog-2",
                        "session_id": "t-nlp4vis-1",
                        "type": "In Person Presentation",
                        "title": "Basics of NLP",
                        "contributors": [
                            "Enamul Hoque"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "",
                        "file_name": "",
                        "time_stamp": "2022-10-16T14:15:00Z",
                        "time_start": "2022-10-16T14:15:00Z",
                        "time_end": "2022-10-16T14:45:00Z",
                        "paper_type": "",
                        "keywords": [],
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "t-nlp4vis-prog-3",
                        "session_id": "t-nlp4vis-1",
                        "type": "Virtual Presentation (live)",
                        "title": "Deep Learning for NLP",
                        "contributors": [
                            "Shafiq Joty"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "",
                        "file_name": "",
                        "time_stamp": "2022-10-16T14:45:00Z",
                        "time_start": "2022-10-16T14:45:00Z",
                        "time_end": "2022-10-16T15:10:00Z",
                        "paper_type": "",
                        "keywords": [],
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "t-nlp4vis-prog-4",
                        "session_id": "t-nlp4vis-1",
                        "type": "Mixed Panel (virtual/in person)",
                        "title": "Q+A",
                        "contributors": [
                            "Enamul Hoque",
                            "Shafiq Joty"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "",
                        "file_name": "",
                        "time_stamp": "2022-10-16T15:10:00Z",
                        "time_start": "2022-10-16T15:10:00Z",
                        "time_end": "2022-10-16T15:15:00Z",
                        "paper_type": "",
                        "keywords": [],
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    }
                ]
            },
            {
                "title": "NLP4Vis: Natural Language Processing for Information Visualization 2",
                "session_id": "t-nlp4vis-2",
                "event_prefix": "t-nlp4vis",
                "track": "pinon",
                "livestream_id": "pinon-sun",
                "session_image": "t-nlp4vis-2.png",
                "chair": [
                    "Enamul Hoque",
                    "Shafiq Joty"
                ],
                "organizers": [],
                "time_start": "2022-10-16T15:45:00Z",
                "time_end": "2022-10-16T17:00:00Z",
                "discord_category": "",
                "discord_channel": "pinon",
                "discord_channel_id": "1024601027644760074",
                "discord_link": "https://discord.com/channels/978320435554947082/1024601027644760074",
                "slido_link": "https://app.sli.do/event/i28UETZaiRWr1XwX6eKjcf?section=bbc002b7-d3ae-48bc-b2d3-0624a7bfed06",
                "youtube_url": "https://youtu.be/-wvdw_VQBfk",
                "youtube_id": "-wvdw_VQBfk",
                "zoom_meeting": "",
                "zoom_password": "",
                "zoom_link": "",
                "ff_link": "",
                "ff_playlist": "",
                "time_slots": [
                    {
                        "slot_id": "t-nlp4vis-prog-5",
                        "session_id": "t-nlp4vis-2",
                        "type": "Virtual Presentation (live)",
                        "title": "Deep Learning for NLP (continues)",
                        "contributors": [
                            "Shafiq Joty"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "",
                        "file_name": "",
                        "time_stamp": "2022-10-16T15:45:00Z",
                        "time_start": "2022-10-16T15:45:00Z",
                        "time_end": "2022-10-16T16:10:00Z",
                        "paper_type": "",
                        "keywords": [],
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "t-nlp4vis-prog-6",
                        "session_id": "t-nlp4vis-2",
                        "type": "In Person Presentation",
                        "title": "NLP + Vis Applications",
                        "contributors": [
                            "Enamul Hoque"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "",
                        "file_name": "",
                        "time_stamp": "2022-10-16T16:10:00Z",
                        "time_start": "2022-10-16T16:10:00Z",
                        "time_end": "2022-10-16T16:35:00Z",
                        "paper_type": "",
                        "keywords": [],
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "t-nlp4vis-prog-7",
                        "session_id": "t-nlp4vis-2",
                        "type": "Mixed Panel (virtual/in person)",
                        "title": "Future Challenges",
                        "contributors": [
                            "Enamul Hoque",
                            "Shafiq Joty"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "",
                        "file_name": "",
                        "time_stamp": "2022-10-16T16:35:00Z",
                        "time_start": "2022-10-16T16:35:00Z",
                        "time_end": "2022-10-16T16:50:00Z",
                        "paper_type": "",
                        "keywords": [],
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "t-nlp4vis-prog-8",
                        "session_id": "t-nlp4vis-2",
                        "type": "Mixed Panel (virtual/in person)",
                        "title": "Q+A",
                        "contributors": [
                            "Enamul Hoque",
                            "Shafiq Joty"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "",
                        "file_name": "",
                        "time_stamp": "2022-10-16T16:50:00Z",
                        "time_start": "2022-10-16T16:50:00Z",
                        "time_end": "2022-10-16T17:00:00Z",
                        "paper_type": "",
                        "keywords": [],
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    }
                ]
            }
        ]
    },
    "w-nlvis": {
        "event": "NLVIZ Workshop: Exploring Research Opportunities for Natural Language, Text, and Data Visualization",
        "long_name": "NLVIZ Workshop: Exploring Research Opportunities for Natural Language, Text, and Data Visualization",
        "event_type": "Workshop",
        "event_prefix": "w-nlvis",
        "event_description": "",
        "event_url": "",
        "organizers": [
            "Vidya Setlur",
            "Arjun Srinivasan"
        ],
        "sessions": [
            {
                "title": "NLVIZ: Opening, Keynote and Paper Session I",
                "session_id": "w-nlvis-1",
                "event_prefix": "w-nlvis",
                "track": "pinon",
                "livestream_id": "pinon-sun",
                "session_image": "w-nlvis-1.png",
                "chair": [
                    "Vidya Setlur",
                    "Arjun Srinivasan"
                ],
                "organizers": [],
                "time_start": "2022-10-16T19:00:00Z",
                "time_end": "2022-10-16T20:15:00Z",
                "discord_category": "",
                "discord_channel": "pinon",
                "discord_channel_id": "1024601027644760074",
                "discord_link": "https://discord.com/channels/978320435554947082/1024601027644760074",
                "slido_link": "https://app.sli.do/event/i28UETZaiRWr1XwX6eKjcf?section=bbc002b7-d3ae-48bc-b2d3-0624a7bfed06",
                "youtube_url": "https://youtu.be/-wvdw_VQBfk",
                "youtube_id": "-wvdw_VQBfk",
                "zoom_meeting": "",
                "zoom_password": "",
                "zoom_link": "",
                "ff_link": "",
                "ff_playlist": "",
                "time_slots": [
                    {
                        "slot_id": "w-nlvis-prog-1",
                        "session_id": "w-nlvis-1",
                        "type": "Virtual Presentation (live)",
                        "title": "Opening + Keynote",
                        "contributors": [
                            "Amit Prakash"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "",
                        "file_name": "",
                        "time_stamp": "2022-10-16T19:00:00Z",
                        "time_start": "2022-10-16T19:00:00Z",
                        "time_end": "2022-10-16T19:50:00Z",
                        "paper_type": "",
                        "keywords": [],
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "w-nlvis-prog-2",
                        "session_id": "w-nlvis-1",
                        "type": "Virtual Q+A",
                        "title": "Keynote Q+A",
                        "contributors": [
                            "Amit Prakash"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "",
                        "file_name": "",
                        "time_stamp": "2022-10-16T19:50:00Z",
                        "time_start": "2022-10-16T19:50:00Z",
                        "time_end": "2022-10-16T19:55:00Z",
                        "paper_type": "",
                        "keywords": [],
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "w-nlvis-1003",
                        "session_id": "w-nlvis-1",
                        "type": "In Person Presentation",
                        "title": "Why More Text is (Often) Better: Themes from Reader Preferences for Integration of Charts and Text",
                        "contributors": [
                            "Chase Stokes"
                        ],
                        "authors": [
                            "Chase Stokes",
                            "Marti Hearst"
                        ],
                        "abstract": "",
                        "uid": "w-nlvis-1003",
                        "file_name": "w-nlvis-1003_Stokes_Presentation.mp4",
                        "time_stamp": "2022-10-16T19:55:00Z",
                        "time_start": "2022-10-16T19:55:00Z",
                        "time_end": "2022-10-16T20:03:00Z",
                        "paper_type": "workshop",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "450",
                        "paper_award": "",
                        "image_caption": "Key codes for the creation of the three main themes are listed. Schema refers to the broader classifications of codes, which were used to create the themes. The total count of participants who mentioned a code at least once is shown, followed by a breakdown of these counts by stimulus variant. The darker the shade of blue, the higher the count. Not all participants who mentioned a code did so for all variants; some did so for more than one variant. Six stimuli were examined in this study, with varying amounts of text: Chart-Only (CO), Chart-Title (CT), Chart-Title-Annotation-1 (CTA1), Chart-Title-Annotation-2 (CTA2), Chart-Title-Annotation+ (CTA+), and Text-Only (TO).",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "w-nlvis-1014",
                        "session_id": "w-nlvis-1",
                        "type": "In Person Presentation",
                        "title": "Using Large Language Models to Generate Engaging Captions for Data Visualizations",
                        "contributors": [
                            "Klaus Mueller"
                        ],
                        "authors": [
                            "Ashley Liew",
                            "Klaus Mueller"
                        ],
                        "abstract": "",
                        "uid": "w-nlvis-1014",
                        "file_name": "",
                        "time_stamp": "2022-10-16T20:03:00Z",
                        "time_start": "2022-10-16T20:03:00Z",
                        "time_end": "2022-10-16T20:15:00Z",
                        "paper_type": "workshop",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "Creating compelling captions for data visualizations has been a longstanding challenge. Visualization researchers are typically untrained in journalistic reporting and hence the captions placed below data visualizations tend to be not overly engaging and rather just stick to basic data observations. We explore the opportunities offered by the newly emerging crop of large language models (LLM) which use sophisticated deep learning to produce human-like prose. We ask, can these powerful software devices be purposed to produce engaging captions for generic data visualizations like a scatterplot. We report on first experiments using the popular LLM GPT-3 and deliver some promising results.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "",
                        "ff_id": ""
                    }
                ]
            },
            {
                "title": "NLVIZ: Paper Session II, Discussion and Closing",
                "session_id": "w-nlvis-2",
                "event_prefix": "w-nlvis",
                "track": "pinon",
                "livestream_id": "pinon-sun",
                "session_image": "w-nlvis-2.png",
                "chair": [
                    "Vidya Setlur",
                    "Arjun Srinivasan"
                ],
                "organizers": [],
                "time_start": "2022-10-16T20:45:00Z",
                "time_end": "2022-10-16T22:00:00Z",
                "discord_category": "",
                "discord_channel": "pinon",
                "discord_channel_id": "1024601027644760074",
                "discord_link": "https://discord.com/channels/978320435554947082/1024601027644760074",
                "slido_link": "https://app.sli.do/event/i28UETZaiRWr1XwX6eKjcf?section=bbc002b7-d3ae-48bc-b2d3-0624a7bfed06",
                "youtube_url": "https://youtu.be/-wvdw_VQBfk",
                "youtube_id": "-wvdw_VQBfk",
                "zoom_meeting": "",
                "zoom_password": "",
                "zoom_link": "",
                "ff_link": "",
                "ff_playlist": "",
                "time_slots": [
                    {
                        "slot_id": "w-nlvis-1009",
                        "session_id": "w-nlvis-2",
                        "type": "Virtual Presentation (pre-recorded)",
                        "title": "Visualizing Suicide Risk Prediction in Clinical Language Models",
                        "contributors": [
                            "Filip Dabek"
                        ],
                        "authors": [
                            "Filip Dabek",
                            "Tim Oates",
                            "Peter Hoover",
                            "Jesus Caban"
                        ],
                        "abstract": "",
                        "uid": "w-nlvis-1009",
                        "file_name": "",
                        "time_stamp": "2022-10-16T20:45:00Z",
                        "time_start": "2022-10-16T20:45:00Z",
                        "time_end": "2022-10-16T20:53:00Z",
                        "paper_type": "workshop",
                        "keywords": [],
                        "has_image": "0",
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "w-nlvis-1006",
                        "session_id": "w-nlvis-2",
                        "type": "In Person Presentation",
                        "title": "Summarizing text to embed qualitative data into visualizations",
                        "contributors": [
                            "Richard Brath"
                        ],
                        "authors": [
                            "Richard Brath"
                        ],
                        "abstract": "",
                        "uid": "w-nlvis-1006",
                        "file_name": "",
                        "time_stamp": "2022-10-16T20:53:00Z",
                        "time_start": "2022-10-16T20:53:00Z",
                        "time_end": "2022-10-16T21:01:00Z",
                        "paper_type": "workshop",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "Qualitative data can be conveyed with text passages. Fitting longer passages into visualization requires space to place the text inside the visualization and appropriate text to fit the space available. NLP summarization can be used to fit text into layouts.",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "w-nlvis-1005",
                        "session_id": "w-nlvis-2",
                        "type": "In Person Presentation",
                        "title": "WordStream Maker: A Lightweight End-to-end Visualization Platform for Qualitative Time-series Data",
                        "contributors": [
                            "Huyen N. Nguyen"
                        ],
                        "authors": [
                            "Huyen N. Nguyen",
                            "Tommy Dang",
                            "Kathleen A. Bowe"
                        ],
                        "abstract": "",
                        "uid": "w-nlvis-1005",
                        "file_name": "",
                        "time_stamp": "2022-10-16T21:01:00Z",
                        "time_start": "2022-10-16T21:01:00Z",
                        "time_end": "2022-10-16T21:09:00Z",
                        "paper_type": "workshop",
                        "keywords": [],
                        "has_image": "0",
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "w-nlvis-1004",
                        "session_id": "w-nlvis-2",
                        "type": "In Person Presentation",
                        "title": "Towards Interactively Contextualizing Natural Language Input in Data Visualization Tools",
                        "contributors": [
                            "Marcel Ruoff"
                        ],
                        "authors": [
                            "Marcel Ruoff",
                            "Brad Myers",
                            "Alexander Maedche"
                        ],
                        "abstract": "",
                        "uid": "w-nlvis-1004",
                        "file_name": "",
                        "time_stamp": "2022-10-16T21:09:00Z",
                        "time_start": "2022-10-16T21:09:00Z",
                        "time_end": "2022-10-16T21:17:00Z",
                        "paper_type": "workshop",
                        "keywords": [],
                        "has_image": "0",
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "w-nlvis-1011",
                        "session_id": "w-nlvis-2",
                        "type": "In Person Presentation",
                        "title": "NL2INTERFACE: Interactive Visualization Interface Generation from Natural Language Queries",
                        "contributors": [
                            "Yiru Chen"
                        ],
                        "authors": [
                            "Yiru Chen",
                            "Ryan Li",
                            "Austin Mac",
                            "Tianbao Xie",
                            "Tao Yu",
                            "eugene Wu"
                        ],
                        "abstract": "",
                        "uid": "w-nlvis-1011",
                        "file_name": "",
                        "time_stamp": "2022-10-16T21:17:00Z",
                        "time_start": "2022-10-16T21:17:00Z",
                        "time_end": "2022-10-16T21:25:00Z",
                        "paper_type": "workshop",
                        "keywords": [],
                        "has_image": "0",
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "w-nlvis-1015",
                        "session_id": "w-nlvis-2",
                        "type": "In Person Presentation",
                        "title": "Speech-based Data Exploration for Diabetes Management among Older Adults",
                        "contributors": [
                            "Alark Joshi"
                        ],
                        "authors": [
                            "Vedangi Bhavsar",
                            "Utkarsha Nehe",
                            "Aditya Mandke",
                            "Srushti Sardeshmukh",
                            "Alark Joshi"
                        ],
                        "abstract": "",
                        "uid": "w-nlvis-1015",
                        "file_name": "",
                        "time_stamp": "2022-10-16T21:25:00Z",
                        "time_start": "2022-10-16T21:25:00Z",
                        "time_end": "2022-10-16T21:33:00Z",
                        "paper_type": "workshop",
                        "keywords": [],
                        "has_image": "0",
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": true,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "w-nlvis-prog-3",
                        "session_id": "w-nlvis-2",
                        "type": "In Person Other",
                        "title": "Discussion and Closing",
                        "contributors": [
                            "Vidya Setlur",
                            "Arjun Srinivasan"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "",
                        "file_name": "",
                        "time_stamp": "2022-10-16T21:33:00Z",
                        "time_start": "2022-10-16T21:33:00Z",
                        "time_end": "2022-10-16T22:00:00Z",
                        "paper_type": "",
                        "keywords": [],
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    }
                ]
            }
        ]
    },
    "t-bayesian": {
        "event": "Visualization in Bayesian workflow",
        "long_name": "Visualization in Bayesian workflow",
        "event_type": "Tutorial",
        "event_prefix": "t-bayesian",
        "event_description": "",
        "event_url": "",
        "organizers": [
            "Clinton Brownley"
        ],
        "sessions": [
            {
                "title": "Visualization in Bayesian workflow 1",
                "session_id": "t-bayesian-1",
                "event_prefix": "t-bayesian",
                "track": "pinon",
                "livestream_id": "pinon-mon",
                "session_image": "t-bayesian-1.png",
                "chair": [
                    "Clinton Brownley"
                ],
                "organizers": [],
                "time_start": "2022-10-17T14:00:00Z",
                "time_end": "2022-10-17T15:15:00Z",
                "discord_category": "",
                "discord_channel": "pinon",
                "discord_channel_id": "1024601027644760074",
                "discord_link": "https://discord.com/channels/978320435554947082/1024601027644760074",
                "slido_link": "https://app.sli.do/event/i28UETZaiRWr1XwX6eKjcf?section=bbc002b7-d3ae-48bc-b2d3-0624a7bfed06",
                "youtube_url": "https://youtu.be/LQ2fEmm1Lug",
                "youtube_id": "LQ2fEmm1Lug",
                "zoom_meeting": "",
                "zoom_password": "",
                "zoom_link": "",
                "ff_link": "",
                "ff_playlist": "",
                "time_slots": [
                    {
                        "slot_id": "t-bayesian-prog-1",
                        "session_id": "t-bayesian-1",
                        "type": "In Person Presentation",
                        "title": "PowerPoint Presentation",
                        "contributors": [
                            "Clinton Brownley"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "",
                        "file_name": "",
                        "time_stamp": "2022-10-17T14:00:00Z",
                        "time_start": "2022-10-17T14:00:00Z",
                        "time_end": "2022-10-17T14:30:00Z",
                        "paper_type": "",
                        "keywords": [],
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "t-bayesian-prog-2",
                        "session_id": "t-bayesian-1",
                        "type": "In Person Presentation",
                        "title": "PowerPoint Presentation",
                        "contributors": [
                            "Clinton Brownley"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "",
                        "file_name": "",
                        "time_stamp": "2022-10-17T14:30:00Z",
                        "time_start": "2022-10-17T14:30:00Z",
                        "time_end": "2022-10-17T15:00:00Z",
                        "paper_type": "",
                        "keywords": [],
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "t-bayesian-prog-3",
                        "session_id": "t-bayesian-1",
                        "type": "In Person Other",
                        "title": "Start Hands On Tutorial 1",
                        "contributors": [
                            "Clinton Brownley"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "",
                        "file_name": "",
                        "time_stamp": "2022-10-17T15:00:00Z",
                        "time_start": "2022-10-17T15:00:00Z",
                        "time_end": "2022-10-17T15:15:00Z",
                        "paper_type": "",
                        "keywords": [],
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    }
                ]
            },
            {
                "title": "Visualization in Bayesian workflow 2",
                "session_id": "t-bayesian-2",
                "event_prefix": "t-bayesian",
                "track": "pinon",
                "livestream_id": "pinon-mon",
                "session_image": "t-bayesian-2.png",
                "chair": [
                    "Clinton Brownley"
                ],
                "organizers": [],
                "time_start": "2022-10-17T15:45:00Z",
                "time_end": "2022-10-17T17:00:00Z",
                "discord_category": "",
                "discord_channel": "pinon",
                "discord_channel_id": "1024601027644760074",
                "discord_link": "https://discord.com/channels/978320435554947082/1024601027644760074",
                "slido_link": "https://app.sli.do/event/i28UETZaiRWr1XwX6eKjcf?section=bbc002b7-d3ae-48bc-b2d3-0624a7bfed06",
                "youtube_url": "https://youtu.be/LQ2fEmm1Lug",
                "youtube_id": "LQ2fEmm1Lug",
                "zoom_meeting": "",
                "zoom_password": "",
                "zoom_link": "",
                "ff_link": "",
                "ff_playlist": "",
                "time_slots": [
                    {
                        "slot_id": "t-bayesian-prog-4",
                        "session_id": "t-bayesian-2",
                        "type": "In Person Other",
                        "title": "Hands On Tutorial 1",
                        "contributors": [
                            "Clinton Brownley"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "",
                        "file_name": "",
                        "time_stamp": "2022-10-17T15:45:00Z",
                        "time_start": "2022-10-17T15:45:00Z",
                        "time_end": "2022-10-17T16:20:00Z",
                        "paper_type": "",
                        "keywords": [],
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "t-bayesian-prog-5",
                        "session_id": "t-bayesian-2",
                        "type": "In Person Other",
                        "title": "Hands On Tutorial 2",
                        "contributors": [
                            "Clinton Brownley"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "",
                        "file_name": "",
                        "time_stamp": "2022-10-17T16:20:00Z",
                        "time_start": "2022-10-17T16:20:00Z",
                        "time_end": "2022-10-17T17:00:00Z",
                        "paper_type": "",
                        "keywords": [],
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    }
                ]
            }
        ]
    },
    "t-vtk": {
        "event": "VTK-m \u2013 A ToolKit for Scientific Visualization on Many-Core Processors",
        "long_name": "VTK-m \u2013 A ToolKit for Scientific Visualization on Many-Core Processors",
        "event_type": "Tutorial",
        "event_prefix": "t-vtk",
        "event_description": "",
        "event_url": "",
        "organizers": [
            "Tushar Athawale",
            "Kenneth Moreland",
            "David Pugmire",
            "Silvio Rizzi",
            "Mark Bolstad"
        ],
        "sessions": [
            {
                "title": "VTK-m \u2013 A ToolKit for Scientific Visualization on Many-Core Processors 1",
                "session_id": "t-vtk-1",
                "event_prefix": "t-vtk",
                "track": "pinon",
                "livestream_id": "pinon-mon",
                "session_image": "t-vtk-1.png",
                "chair": [
                    "Tushar Athawale",
                    "Kenneth Moreland",
                    "David Pugmire",
                    "Silvio Rizzi",
                    "Mark Bolstad"
                ],
                "organizers": [],
                "time_start": "2022-10-17T19:00:00Z",
                "time_end": "2022-10-17T20:15:00Z",
                "discord_category": "",
                "discord_channel": "pinon",
                "discord_channel_id": "1024601027644760074",
                "discord_link": "https://discord.com/channels/978320435554947082/1024601027644760074",
                "slido_link": "https://app.sli.do/event/i28UETZaiRWr1XwX6eKjcf?section=bbc002b7-d3ae-48bc-b2d3-0624a7bfed06",
                "youtube_url": "https://youtu.be/LQ2fEmm1Lug",
                "youtube_id": "LQ2fEmm1Lug",
                "zoom_meeting": "",
                "zoom_password": "",
                "zoom_link": "",
                "ff_link": "",
                "ff_playlist": "",
                "time_slots": [
                    {
                        "slot_id": "t-vtk-prog-1",
                        "session_id": "t-vtk-1",
                        "type": "Virtual Presentation (live)",
                        "title": "Introduction/software install",
                        "contributors": [
                            "Tushar Athawale"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "",
                        "file_name": "",
                        "time_stamp": "2022-10-17T19:00:00Z",
                        "time_start": "2022-10-17T19:00:00Z",
                        "time_end": "2022-10-17T19:25:00Z",
                        "paper_type": "",
                        "keywords": [],
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "t-vtk-prog-2",
                        "session_id": "t-vtk-1",
                        "type": "In Person Presentation",
                        "title": "Concepts behind VTK-m usage",
                        "contributors": [
                            "David Pugmire"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "",
                        "file_name": "",
                        "time_stamp": "2022-10-17T19:25:00Z",
                        "time_start": "2022-10-17T19:25:00Z",
                        "time_end": "2022-10-17T19:50:00Z",
                        "paper_type": "",
                        "keywords": [],
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "t-vtk-prog-3",
                        "session_id": "t-vtk-1",
                        "type": "In Person Presentation",
                        "title": "Using VTK-m (concepts + hands on)",
                        "contributors": [
                            "Silvio Rizzi"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "",
                        "file_name": "",
                        "time_stamp": "2022-10-17T19:50:00Z",
                        "time_start": "2022-10-17T19:50:00Z",
                        "time_end": "2022-10-17T20:15:00Z",
                        "paper_type": "",
                        "keywords": [],
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "t-vtk-prog-1",
                        "session_id": "t-vtk-1",
                        "type": "Virtual Presentation (live)",
                        "title": "Introduction/software install",
                        "contributors": [
                            "Tushar Athawale"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "",
                        "file_name": "",
                        "time_stamp": "2022-10-17T19:00:00Z",
                        "time_start": "2022-10-17T19:00:00Z",
                        "time_end": "2022-10-17T19:25:00Z",
                        "paper_type": "",
                        "keywords": [],
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "t-vtk-prog-2",
                        "session_id": "t-vtk-1",
                        "type": "In Person Presentation",
                        "title": "Concepts behind VTK-m usage",
                        "contributors": [
                            "David Pugmire"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "",
                        "file_name": "",
                        "time_stamp": "2022-10-17T19:25:00Z",
                        "time_start": "2022-10-17T19:25:00Z",
                        "time_end": "2022-10-17T19:50:00Z",
                        "paper_type": "",
                        "keywords": [],
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "t-vtk-prog-3",
                        "session_id": "t-vtk-1",
                        "type": "In Person Presentation",
                        "title": "Using VTK-m (concepts + hands on)",
                        "contributors": [
                            "Silvio Rizzi"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "",
                        "file_name": "",
                        "time_stamp": "2022-10-17T19:50:00Z",
                        "time_start": "2022-10-17T19:50:00Z",
                        "time_end": "2022-10-17T20:15:00Z",
                        "paper_type": "",
                        "keywords": [],
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    }
                ]
            },
            {
                "title": "VTK-m \u2013 A ToolKit for Scientific Visualization on Many-Core Processors 2",
                "session_id": "t-vtk-2",
                "event_prefix": "t-vtk",
                "track": "pinon",
                "livestream_id": "pinon-mon",
                "session_image": "t-vtk-2.png",
                "chair": [
                    "Tushar Athawale",
                    "Kenneth Moreland",
                    "David Pugmire",
                    "Silvio Rizzi",
                    "Mark Bolstad"
                ],
                "organizers": [],
                "time_start": "2022-10-17T20:45:00Z",
                "time_end": "2022-10-17T22:00:00Z",
                "discord_category": "",
                "discord_channel": "pinon",
                "discord_channel_id": "1024601027644760074",
                "discord_link": "https://discord.com/channels/978320435554947082/1024601027644760074",
                "slido_link": "https://app.sli.do/event/i28UETZaiRWr1XwX6eKjcf?section=bbc002b7-d3ae-48bc-b2d3-0624a7bfed06",
                "youtube_url": "https://youtu.be/LQ2fEmm1Lug",
                "youtube_id": "LQ2fEmm1Lug",
                "zoom_meeting": "",
                "zoom_password": "",
                "zoom_link": "",
                "ff_link": "",
                "ff_playlist": "",
                "time_slots": [
                    {
                        "slot_id": "t-vtk-prog-4",
                        "session_id": "t-vtk-2",
                        "type": "In Person Presentation",
                        "title": "VTK-m development (concepts + hands on)",
                        "contributors": [
                            "Ken Moreland"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "",
                        "file_name": "",
                        "time_stamp": "2022-10-17T20:45:00Z",
                        "time_start": "2022-10-17T20:45:00Z",
                        "time_end": "2022-10-17T21:20:00Z",
                        "paper_type": "",
                        "keywords": [],
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "t-vtk-prog-5",
                        "session_id": "t-vtk-2",
                        "type": "In Person Presentation",
                        "title": "VTK-m examples (hands on)",
                        "contributors": [
                            "Mark Bolstad"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "",
                        "file_name": "",
                        "time_stamp": "2022-10-17T21:20:00Z",
                        "time_start": "2022-10-17T21:20:00Z",
                        "time_end": "2022-10-17T21:55:00Z",
                        "paper_type": "",
                        "keywords": [],
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "t-vtk-prog-6",
                        "session_id": "t-vtk-2",
                        "type": "In Person Presentation",
                        "title": "Upcoming activities",
                        "contributors": [
                            "Ken Moreland"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "",
                        "file_name": "",
                        "time_stamp": "2022-10-17T21:55:00Z",
                        "time_start": "2022-10-17T21:55:00Z",
                        "time_end": "2022-10-17T22:00:00Z",
                        "paper_type": "",
                        "keywords": [],
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "t-vtk-prog-4",
                        "session_id": "t-vtk-2",
                        "type": "In Person Presentation",
                        "title": "VTK-m development (concepts + hands on)",
                        "contributors": [
                            "Ken Moreland"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "",
                        "file_name": "",
                        "time_stamp": "2022-10-17T20:45:00Z",
                        "time_start": "2022-10-17T20:45:00Z",
                        "time_end": "2022-10-17T21:20:00Z",
                        "paper_type": "",
                        "keywords": [],
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "t-vtk-prog-5",
                        "session_id": "t-vtk-2",
                        "type": "In Person Presentation",
                        "title": "VTK-m examples (hands on)",
                        "contributors": [
                            "Mark Bolstad"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "",
                        "file_name": "",
                        "time_stamp": "2022-10-17T21:20:00Z",
                        "time_start": "2022-10-17T21:20:00Z",
                        "time_end": "2022-10-17T21:55:00Z",
                        "paper_type": "",
                        "keywords": [],
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "t-vtk-prog-6",
                        "session_id": "t-vtk-2",
                        "type": "In Person Presentation",
                        "title": "Upcoming activities",
                        "contributors": [
                            "Ken Moreland"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "",
                        "file_name": "",
                        "time_stamp": "2022-10-17T21:55:00Z",
                        "time_start": "2022-10-17T21:55:00Z",
                        "time_end": "2022-10-17T22:00:00Z",
                        "paper_type": "",
                        "keywords": [],
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    }
                ]
            }
        ]
    },
    "t-color": {
        "event": "Color Scheming in Visualization",
        "long_name": "Color Scheming in Visualization",
        "event_type": "Tutorial",
        "event_prefix": "t-color",
        "event_description": "",
        "event_url": "",
        "organizers": [
            "Theresa-Marie Rhyne"
        ],
        "sessions": [
            {
                "title": "Color Scheming in Visualization 1",
                "session_id": "t-color-1",
                "event_prefix": "t-color",
                "track": "mistletoe",
                "livestream_id": "mistletoe-sun",
                "session_image": "t-color-1.png",
                "chair": [
                    "Theresa-Marie Rhyne"
                ],
                "organizers": [],
                "time_start": "2022-10-16T14:00:00Z",
                "time_end": "2022-10-16T15:15:00Z",
                "discord_category": "",
                "discord_channel": "mistletoe",
                "discord_channel_id": "1024601055700471839",
                "discord_link": "https://discord.com/channels/978320435554947082/1024601055700471839",
                "slido_link": "https://app.sli.do/event/i28UETZaiRWr1XwX6eKjcf?section=05d57c7a-397b-4e16-a41d-7f06e8e64c37",
                "youtube_url": "https://youtu.be/_evorVC17Yg",
                "youtube_id": "_evorVC17Yg",
                "zoom_meeting": "",
                "zoom_password": "",
                "zoom_link": "",
                "ff_link": "",
                "ff_playlist": "",
                "time_slots": [
                    {
                        "slot_id": "t-color-prog-1",
                        "session_id": "t-color-1",
                        "type": "In Person Presentation",
                        "title": "Fundamentals of Colorizing Data Visualizations",
                        "contributors": [
                            "Theresa-Marie Rhyne"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "",
                        "file_name": "",
                        "time_stamp": "2022-10-16T14:00:00Z",
                        "time_start": "2022-10-16T14:00:00Z",
                        "time_end": "2022-10-16T15:15:00Z",
                        "paper_type": "",
                        "keywords": [],
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    }
                ]
            },
            {
                "title": "Color Scheming in Visualization 2",
                "session_id": "t-color-2",
                "event_prefix": "t-color",
                "track": "mistletoe",
                "livestream_id": "mistletoe-sun",
                "session_image": "t-color-2.png",
                "chair": [
                    "Theresa-Marie Rhyne"
                ],
                "organizers": [],
                "time_start": "2022-10-16T15:45:00Z",
                "time_end": "2022-10-16T17:00:00Z",
                "discord_category": "",
                "discord_channel": "mistletoe",
                "discord_channel_id": "1024601055700471839",
                "discord_link": "https://discord.com/channels/978320435554947082/1024601055700471839",
                "slido_link": "https://app.sli.do/event/i28UETZaiRWr1XwX6eKjcf?section=05d57c7a-397b-4e16-a41d-7f06e8e64c37",
                "youtube_url": "https://youtu.be/_evorVC17Yg",
                "youtube_id": "_evorVC17Yg",
                "zoom_meeting": "",
                "zoom_password": "",
                "zoom_link": "",
                "ff_link": "",
                "ff_playlist": "",
                "time_slots": [
                    {
                        "slot_id": "t-color-prog-2",
                        "session_id": "t-color-2",
                        "type": "In Person Presentation",
                        "title": "Tools and Case Studies for Coloriizing Data Vis.",
                        "contributors": [
                            "Theresa-Marie Rhyne"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "",
                        "file_name": "",
                        "time_stamp": "2022-10-16T15:45:00Z",
                        "time_start": "2022-10-16T15:45:00Z",
                        "time_end": "2022-10-16T17:00:00Z",
                        "paper_type": "",
                        "keywords": [],
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    }
                ]
            }
        ]
    },
    "t-sports": {
        "event": "Sports Data Analysis and Visualization",
        "long_name": "Sports Data Analysis and Visualization",
        "event_type": "Tutorial",
        "event_prefix": "t-sports",
        "event_description": "",
        "event_url": "",
        "organizers": [
            "Romain Vuillemot"
        ],
        "sessions": [
            {
                "title": "Sports Data Analysis and Visualization 1",
                "session_id": "t-sports-1",
                "event_prefix": "t-sports",
                "track": "mistletoe",
                "livestream_id": "mistletoe-sun",
                "session_image": "t-sports-1.png",
                "chair": [
                    "Romain Vuillemot"
                ],
                "organizers": [],
                "time_start": "2022-10-16T19:00:00Z",
                "time_end": "2022-10-16T20:15:00Z",
                "discord_category": "",
                "discord_channel": "mistletoe",
                "discord_channel_id": "1024601055700471839",
                "discord_link": "https://discord.com/channels/978320435554947082/1024601055700471839",
                "slido_link": "https://app.sli.do/event/i28UETZaiRWr1XwX6eKjcf?section=05d57c7a-397b-4e16-a41d-7f06e8e64c37",
                "youtube_url": "https://youtu.be/_evorVC17Yg",
                "youtube_id": "_evorVC17Yg",
                "zoom_meeting": "",
                "zoom_password": "",
                "zoom_link": "",
                "ff_link": "",
                "ff_playlist": "",
                "time_slots": [
                    {
                        "slot_id": "t-sports-prog-1",
                        "session_id": "t-sports-1",
                        "type": "Virtual Presentation (live)",
                        "title": "Welcome/Introduction",
                        "contributors": [
                            "Romain Vuillemot"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "",
                        "file_name": "",
                        "time_stamp": "2022-10-16T19:00:00Z",
                        "time_start": "2022-10-16T19:00:00Z",
                        "time_end": "2022-10-16T19:05:00Z",
                        "paper_type": "",
                        "keywords": [],
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "t-sports-prog-2",
                        "session_id": "t-sports-1",
                        "type": "Virtual Presentation (live)",
                        "title": "Breaking down sports visualizations",
                        "contributors": [
                            "Romain Vuillemot"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "",
                        "file_name": "",
                        "time_stamp": "2022-10-16T19:05:00Z",
                        "time_start": "2022-10-16T19:05:00Z",
                        "time_end": "2022-10-16T19:25:00Z",
                        "paper_type": "",
                        "keywords": [],
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "t-sports-prog-3",
                        "session_id": "t-sports-1",
                        "type": "Virtual Presentation (live)",
                        "title": "Event-based sports visualizations",
                        "contributors": [
                            "Romain Vuillemot"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "",
                        "file_name": "",
                        "time_stamp": "2022-10-16T19:25:00Z",
                        "time_start": "2022-10-16T19:25:00Z",
                        "time_end": "2022-10-16T19:45:00Z",
                        "paper_type": "",
                        "keywords": [],
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "t-sports-prog-4",
                        "session_id": "t-sports-1",
                        "type": "Virtual Presentation (live)",
                        "title": "Tracking sports visualizations",
                        "contributors": [
                            "Romain Vuillemot"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "",
                        "file_name": "",
                        "time_stamp": "2022-10-16T19:45:00Z",
                        "time_start": "2022-10-16T19:45:00Z",
                        "time_end": "2022-10-16T20:05:00Z",
                        "paper_type": "",
                        "keywords": [],
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "t-sports-prog-5",
                        "session_id": "t-sports-1",
                        "type": "Virtual Presentation (live)",
                        "title": "Meta-data visualizations",
                        "contributors": [
                            "Romain Vuillemot"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "",
                        "file_name": "",
                        "time_stamp": "2022-10-16T20:05:00Z",
                        "time_start": "2022-10-16T20:05:00Z",
                        "time_end": "2022-10-16T20:15:00Z",
                        "paper_type": "",
                        "keywords": [],
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    }
                ]
            },
            {
                "title": "Sports Data Analysis and Visualization 2",
                "session_id": "t-sports-2",
                "event_prefix": "t-sports",
                "track": "mistletoe",
                "livestream_id": "mistletoe-sun",
                "session_image": "t-sports-2.png",
                "chair": [
                    "Romain Vuillemot"
                ],
                "organizers": [],
                "time_start": "2022-10-16T20:45:00Z",
                "time_end": "2022-10-16T22:00:00Z",
                "discord_category": "",
                "discord_channel": "mistletoe",
                "discord_channel_id": "1024601055700471839",
                "discord_link": "https://discord.com/channels/978320435554947082/1024601055700471839",
                "slido_link": "https://app.sli.do/event/i28UETZaiRWr1XwX6eKjcf?section=05d57c7a-397b-4e16-a41d-7f06e8e64c37",
                "youtube_url": "https://youtu.be/_evorVC17Yg",
                "youtube_id": "_evorVC17Yg",
                "zoom_meeting": "",
                "zoom_password": "",
                "zoom_link": "",
                "ff_link": "",
                "ff_playlist": "",
                "time_slots": [
                    {
                        "slot_id": "t-sports-prog-6",
                        "session_id": "t-sports-2",
                        "type": "Virtual Presentation (live)",
                        "title": "Case study: performance sports visualizations",
                        "contributors": [
                            "Romain Vuillemot"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "",
                        "file_name": "",
                        "time_stamp": "2022-10-16T20:45:00Z",
                        "time_start": "2022-10-16T20:45:00Z",
                        "time_end": "2022-10-16T21:15:00Z",
                        "paper_type": "",
                        "keywords": [],
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "t-sports-prog-7",
                        "session_id": "t-sports-2",
                        "type": "Virtual Presentation (live)",
                        "title": "Case study: tactical sports visualizations",
                        "contributors": [
                            "Romain Vuillemot"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "",
                        "file_name": "",
                        "time_stamp": "2022-10-16T21:15:00Z",
                        "time_start": "2022-10-16T21:15:00Z",
                        "time_end": "2022-10-16T21:30:00Z",
                        "paper_type": "",
                        "keywords": [],
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "t-sports-prog-8",
                        "session_id": "t-sports-2",
                        "type": "Virtual Q+A",
                        "title": "Case study: opposition sports visualizations",
                        "contributors": [
                            "Romain Vuillemot"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "",
                        "file_name": "",
                        "time_stamp": "2022-10-16T21:30:00Z",
                        "time_start": "2022-10-16T21:30:00Z",
                        "time_end": "2022-10-16T21:59:00Z",
                        "paper_type": "",
                        "keywords": [],
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "t-sports-prog-9",
                        "session_id": "t-sports-2",
                        "type": "Virtual Q+A",
                        "title": "Wrap up and QA",
                        "contributors": [
                            "Romain Vuillemot"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "",
                        "file_name": "",
                        "time_stamp": "2022-10-16T21:59:00Z",
                        "time_start": "2022-10-16T21:59:00Z",
                        "time_end": "2022-10-16T22:00:00Z",
                        "paper_type": "",
                        "keywords": [],
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    }
                ]
            }
        ]
    },
    "t-riemannian": {
        "event": "Riemannian Geometry for Scientific Visualization",
        "long_name": "Riemannian Geometry for Scientific Visualization",
        "event_type": "Tutorial",
        "event_prefix": "t-riemannian",
        "event_description": "",
        "event_url": "",
        "organizers": [
            "Markus Hadwiger",
            "Thomas Theussl",
            "Peter Rautek"
        ],
        "sessions": [
            {
                "title": "Riemannian Geometry for Scientific Visualization 1",
                "session_id": "t-riemannian-1",
                "event_prefix": "t-riemannian",
                "track": "mistletoe",
                "livestream_id": "mistletoe-mon",
                "session_image": "t-riemannian-1.png",
                "chair": [
                    "Markus Hadwiger",
                    "Thomas Theussl",
                    "Peter Rautek"
                ],
                "organizers": [],
                "time_start": "2022-10-17T14:00:00Z",
                "time_end": "2022-10-17T15:15:00Z",
                "discord_category": "",
                "discord_channel": "mistletoe",
                "discord_channel_id": "1024601055700471839",
                "discord_link": "https://discord.com/channels/978320435554947082/1024601055700471839",
                "slido_link": "https://app.sli.do/event/i28UETZaiRWr1XwX6eKjcf?section=05d57c7a-397b-4e16-a41d-7f06e8e64c37",
                "youtube_url": "https://youtu.be/dmY-jhL0Oc8",
                "youtube_id": "dmY-jhL0Oc8",
                "zoom_meeting": "",
                "zoom_password": "",
                "zoom_link": "",
                "ff_link": "",
                "ff_playlist": "",
                "time_slots": [
                    {
                        "slot_id": "t-riemannian-prog-1",
                        "session_id": "t-riemannian-1",
                        "type": "In Person Presentation",
                        "title": "Opening Presentation",
                        "contributors": [
                            "Markus Hadwiger",
                            "Peter Rautek"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "",
                        "file_name": "",
                        "time_stamp": "2022-10-17T14:00:00Z",
                        "time_start": "2022-10-17T14:00:00Z",
                        "time_end": "2022-10-17T14:05:00Z",
                        "paper_type": "",
                        "keywords": [],
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "t-riemannian-prog-2",
                        "session_id": "t-riemannian-1",
                        "type": "In Person Presentation",
                        "title": "Manifolds, coordinate charts, vector fields",
                        "contributors": [
                            "Markus Hadwiger"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "",
                        "file_name": "",
                        "time_stamp": "2022-10-17T14:05:00Z",
                        "time_start": "2022-10-17T14:05:00Z",
                        "time_end": "2022-10-17T14:45:00Z",
                        "paper_type": "",
                        "keywords": [],
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "t-riemannian-prog-3",
                        "session_id": "t-riemannian-1",
                        "type": "In Person Presentation",
                        "title": "Tensor fields and differential forms",
                        "contributors": [
                            "Peter Rautek"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "",
                        "file_name": "",
                        "time_stamp": "2022-10-17T14:45:00Z",
                        "time_start": "2022-10-17T14:45:00Z",
                        "time_end": "2022-10-17T15:10:00Z",
                        "paper_type": "",
                        "keywords": [],
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "t-riemannian-prog-4",
                        "session_id": "t-riemannian-1",
                        "type": "In Person Q+A",
                        "title": "Part 1 Q&A",
                        "contributors": [
                            "Markus Hadwiger",
                            "Peter Rautek"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "",
                        "file_name": "",
                        "time_stamp": "2022-10-17T15:10:00Z",
                        "time_start": "2022-10-17T15:10:00Z",
                        "time_end": "2022-10-17T15:15:00Z",
                        "paper_type": "",
                        "keywords": [],
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    }
                ]
            },
            {
                "title": "Riemannian Geometry for Scientific Visualization 2",
                "session_id": "t-riemannian-2",
                "event_prefix": "t-riemannian",
                "track": "mistletoe",
                "livestream_id": "mistletoe-mon",
                "session_image": "t-riemannian-2.png",
                "chair": [
                    "Markus Hadwiger",
                    "Thomas Theussl",
                    "Peter Rautek"
                ],
                "organizers": [],
                "time_start": "2022-10-17T15:45:00Z",
                "time_end": "2022-10-17T17:00:00Z",
                "discord_category": "",
                "discord_channel": "mistletoe",
                "discord_channel_id": "1024601055700471839",
                "discord_link": "https://discord.com/channels/978320435554947082/1024601055700471839",
                "slido_link": "https://app.sli.do/event/i28UETZaiRWr1XwX6eKjcf?section=05d57c7a-397b-4e16-a41d-7f06e8e64c37",
                "youtube_url": "https://youtu.be/dmY-jhL0Oc8",
                "youtube_id": "dmY-jhL0Oc8",
                "zoom_meeting": "",
                "zoom_password": "",
                "zoom_link": "",
                "ff_link": "",
                "ff_playlist": "",
                "time_slots": [
                    {
                        "slot_id": "t-riemannian-prog-5",
                        "session_id": "t-riemannian-2",
                        "type": "In Person Presentation",
                        "title": "Metrics, connections and covariant derivatives, Lie derivatives",
                        "contributors": [
                            "Peter Rautek"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "",
                        "file_name": "",
                        "time_stamp": "2022-10-17T15:45:00Z",
                        "time_start": "2022-10-17T15:45:00Z",
                        "time_end": "2022-10-17T16:20:00Z",
                        "paper_type": "",
                        "keywords": [],
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "t-riemannian-prog-6",
                        "session_id": "t-riemannian-2",
                        "type": "In Person Presentation",
                        "title": "Smooth maps between manifolds; Lie groups and algebras",
                        "contributors": [
                            "Markus Hadwiger"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "",
                        "file_name": "",
                        "time_stamp": "2022-10-17T16:20:00Z",
                        "time_start": "2022-10-17T16:20:00Z",
                        "time_end": "2022-10-17T16:55:00Z",
                        "paper_type": "",
                        "keywords": [],
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    },
                    {
                        "slot_id": "t-riemannian-prog-7",
                        "session_id": "t-riemannian-2",
                        "type": "In Person Q+A",
                        "title": "Part 2 Q&A",
                        "contributors": [
                            "Markus Hadwiger",
                            "Peter Rautek"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "",
                        "file_name": "",
                        "time_stamp": "2022-10-17T16:55:00Z",
                        "time_start": "2022-10-17T16:55:00Z",
                        "time_end": "2022-10-17T17:00:00Z",
                        "paper_type": "",
                        "keywords": [],
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    }
                ]
            }
        ]
    },
    "t-geo": {
        "event": "Analyze and Visualize Large Scale Geospatial Data with H3 and HexTile",
        "long_name": "Analyze and Visualize Large Scale Geospatial Data with H3 and HexTile",
        "event_type": "Tutorial",
        "event_prefix": "t-geo",
        "event_description": "",
        "event_url": "",
        "organizers": [
            "Shan He"
        ],
        "sessions": [
            {
                "title": "Analyze and Visualize Large Scale Geospatial Data with H3 and HexTile 1",
                "session_id": "t-geo-1",
                "event_prefix": "t-geo",
                "track": "mistletoe",
                "livestream_id": "mistletoe-mon",
                "session_image": "t-geo-1.png",
                "chair": [
                    "Shan He"
                ],
                "organizers": [],
                "time_start": "2022-10-17T19:00:00Z",
                "time_end": "2022-10-17T20:15:00Z",
                "discord_category": "",
                "discord_channel": "mistletoe",
                "discord_channel_id": "1024601055700471839",
                "discord_link": "https://discord.com/channels/978320435554947082/1024601055700471839",
                "slido_link": "https://app.sli.do/event/i28UETZaiRWr1XwX6eKjcf?section=05d57c7a-397b-4e16-a41d-7f06e8e64c37",
                "youtube_url": "https://youtu.be/dmY-jhL0Oc8",
                "youtube_id": "dmY-jhL0Oc8",
                "zoom_meeting": "",
                "zoom_password": "",
                "zoom_link": "",
                "ff_link": "",
                "ff_playlist": "",
                "time_slots": [
                    {
                        "slot_id": "t-geo-prog-1",
                        "session_id": "t-geo-1",
                        "type": "In Person Presentation",
                        "title": "Analyze and Visualize Large Scale Geospatial Data with H3 and HexTile 1",
                        "contributors": [
                            "Shan He"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "",
                        "file_name": "",
                        "time_stamp": "2022-10-17T19:00:00Z",
                        "time_start": "2022-10-17T19:00:00Z",
                        "time_end": "2022-10-17T20:15:00Z",
                        "paper_type": "",
                        "keywords": [],
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    }
                ]
            },
            {
                "title": "Analyze and Visualize Large Scale Geospatial Data with H3 and HexTile 2",
                "session_id": "t-geo-2",
                "event_prefix": "t-geo",
                "track": "mistletoe",
                "livestream_id": "mistletoe-mon",
                "session_image": "t-geo-2.png",
                "chair": [
                    "Shan He"
                ],
                "organizers": [],
                "time_start": "2022-10-17T20:45:00Z",
                "time_end": "2022-10-17T22:00:00Z",
                "discord_category": "",
                "discord_channel": "mistletoe",
                "discord_channel_id": "1024601055700471839",
                "discord_link": "https://discord.com/channels/978320435554947082/1024601055700471839",
                "slido_link": "https://app.sli.do/event/i28UETZaiRWr1XwX6eKjcf?section=05d57c7a-397b-4e16-a41d-7f06e8e64c37",
                "youtube_url": "https://youtu.be/dmY-jhL0Oc8",
                "youtube_id": "dmY-jhL0Oc8",
                "zoom_meeting": "",
                "zoom_password": "",
                "zoom_link": "",
                "ff_link": "",
                "ff_playlist": "",
                "time_slots": [
                    {
                        "slot_id": "t-geo-prog-2",
                        "session_id": "t-geo-2",
                        "type": "In Person Presentation",
                        "title": "Analyze and Visualize Large Scale Geospatial Data with H3 and HexTile 2",
                        "contributors": [
                            "Shan He"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "",
                        "file_name": "",
                        "time_stamp": "2022-10-17T20:45:00Z",
                        "time_start": "2022-10-17T20:45:00Z",
                        "time_end": "2022-10-17T22:00:00Z",
                        "paper_type": "",
                        "keywords": [],
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": ""
                    }
                ]
            }
        ]
    }
}