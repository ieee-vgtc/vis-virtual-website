{
    "conf": {
        "event": "Conference Events",
        "long_name": "Conference Events",
        "event_type": "VIS",
        "event_prefix": "conf",
        "event_description": "",
        "event_url": "",
        "organizers": [],
        "sessions": [
            {
                "title": "VIS Welcome (8:30am-8:45am)| VGTC Awards (8:45am-9:30am)| Test of Time Awards (9:30am-10:15am)",
                "session_id": "conf1",
                "event_prefix": "conf",
                "track": "ok4",
                "livestream_id": "ok4-tue",
                "session_image": "conf1.png",
                "chair": [],
                "organizers": [],
                "time_start": "2022-10-18T13:30:00Z",
                "time_end": "2022-10-18T15:15:00Z",
                "discord_category": "",
                "discord_channel": "",
                "discord_channel_id": "",
                "discord_link": "",
                "slido_link": "",
                "youtube_url": "",
                "zoom_meeting": "",
                "zoom_password": "",
                "zoom_link": "",
                "ff_link": "",
                "ff_playlist": "",
                "time_slots": []
            },
            {
                "title": "Industry Keynote (2:00pm-2:15pm)| VIS Keynote (2:15pm-3:15pm)",
                "session_id": "conf2",
                "event_prefix": "conf",
                "track": "ok4",
                "livestream_id": "ok4-tue",
                "session_image": "conf2.png",
                "chair": [],
                "organizers": [],
                "time_start": "2022-10-18T19:00:00Z",
                "time_end": "2022-10-18T20:15:00Z",
                "discord_category": "",
                "discord_channel": "",
                "discord_channel_id": "",
                "discord_link": "",
                "slido_link": "",
                "youtube_url": "",
                "zoom_meeting": "",
                "zoom_password": "",
                "zoom_link": "",
                "ff_link": "",
                "ff_playlist": "",
                "time_slots": []
            },
            {
                "title": "VIS Supporters Forum| Featured Sponsor Talks (TBD)| Supporters Panel (TBD)",
                "session_id": "conf3",
                "event_prefix": "conf",
                "track": "ok4",
                "livestream_id": "ok4-tue",
                "session_image": "conf3.png",
                "chair": [],
                "organizers": [],
                "time_start": "2022-10-18T20:45:00Z",
                "time_end": "2022-10-18T22:00:00Z",
                "discord_category": "",
                "discord_channel": "",
                "discord_channel_id": "",
                "discord_link": "",
                "slido_link": "",
                "youtube_url": "",
                "zoom_meeting": "",
                "zoom_password": "",
                "zoom_link": "",
                "ff_link": "",
                "ff_playlist": "",
                "time_slots": []
            },
            {
                "title": "VIS Capstone (10:45am-11:45am)| VIS Closing (11:45am-12:00pm)",
                "session_id": "conf4",
                "event_prefix": "conf",
                "track": "ok4",
                "livestream_id": "ok4-fri",
                "session_image": "conf4.png",
                "chair": [],
                "organizers": [],
                "time_start": "2022-10-21T15:45:00Z",
                "time_end": "2022-10-21T17:00:00Z",
                "discord_category": "",
                "discord_channel": "",
                "discord_channel_id": "",
                "discord_link": "",
                "slido_link": "",
                "youtube_url": "",
                "zoom_meeting": "",
                "zoom_password": "",
                "zoom_link": "",
                "ff_link": "",
                "ff_playlist": "",
                "time_slots": []
            },
            {
                "title": "VISAP Opening Reception (6:00pm-8:00pm)| -- Art Exhibit Location: Automobile Alley",
                "session_id": "conf5",
                "event_prefix": "conf",
                "track": "various",
                "livestream_id": "various-wed",
                "session_image": "conf5.png",
                "chair": [],
                "organizers": [],
                "time_start": "2022-10-18T23:00:00Z",
                "time_end": "2022-10-19T01:00:00Z",
                "discord_category": "",
                "discord_channel": "",
                "discord_channel_id": "",
                "discord_link": "",
                "slido_link": "",
                "youtube_url": "",
                "zoom_meeting": "",
                "zoom_password": "",
                "zoom_link": "",
                "ff_link": "",
                "ff_playlist": "",
                "time_slots": []
            },
            {
                "title": "VIS combined Poster Session (5:30pm-6:45pm)| -- includes all Workshops/Associated Event Posters| -- Exhibit Hall Location: OK Station 2+3",
                "session_id": "conf6",
                "event_prefix": "conf",
                "track": "various",
                "livestream_id": "various-thu",
                "session_image": "conf6.png",
                "chair": [],
                "organizers": [],
                "time_start": "2022-10-19T22:30:00Z",
                "time_end": "2022-10-20T23:45:00Z",
                "discord_category": "",
                "discord_channel": "",
                "discord_channel_id": "",
                "discord_link": "",
                "slido_link": "",
                "youtube_url": "",
                "zoom_meeting": "",
                "zoom_password": "",
                "zoom_link": "",
                "ff_link": "",
                "ff_playlist": "",
                "time_slots": []
            },
            {
                "title": "VIS Banquet (6:45pm-10:00pm)| -- Location: First Americans Museum| -- Shuttles depart starting at 6:30pm",
                "session_id": "conf7",
                "event_prefix": "conf",
                "track": "various",
                "livestream_id": "various-thu",
                "session_image": "conf7.png",
                "chair": [],
                "organizers": [],
                "time_start": "2022-10-20T23:30:00Z",
                "time_end": "2022-10-20T04:00:00Z",
                "discord_category": "",
                "discord_channel": "",
                "discord_channel_id": "",
                "discord_link": "",
                "slido_link": "",
                "youtube_url": "",
                "zoom_meeting": "",
                "zoom_password": "",
                "zoom_link": "",
                "ff_link": "",
                "ff_playlist": "",
                "time_slots": []
            },
            {
                "title": "OU Open House (6:00pm-9:00pm)| -- Location: off site, shuttles depart 5:45pm",
                "session_id": "conf8",
                "event_prefix": "conf",
                "track": "various",
                "livestream_id": "various-fri",
                "session_image": "conf8.png",
                "chair": [],
                "organizers": [],
                "time_start": "2022-10-20T23:00:00Z",
                "time_end": "2022-10-21T02:00:00Z",
                "discord_category": "",
                "discord_channel": "",
                "discord_channel_id": "",
                "discord_link": "",
                "slido_link": "",
                "youtube_url": "",
                "zoom_meeting": "",
                "zoom_password": "",
                "zoom_link": "",
                "ff_link": "",
                "ff_playlist": "",
                "time_slots": []
            },
            {
                "title": "Supporters Exhibits, Networking + Hiring, Posters",
                "session_id": "ex1",
                "event_prefix": "conf",
                "track": "various",
                "livestream_id": "various-tue",
                "session_image": "ex1.png",
                "chair": [],
                "organizers": [],
                "time_start": "2022-10-18T13:30:00Z",
                "time_end": "2022-10-18T15:15:00Z",
                "discord_category": "",
                "discord_channel": "",
                "discord_channel_id": "",
                "discord_link": "",
                "slido_link": "",
                "youtube_url": "",
                "zoom_meeting": "",
                "zoom_password": "",
                "zoom_link": "",
                "ff_link": "",
                "ff_playlist": "",
                "time_slots": []
            },
            {
                "title": "Supporters Exhibits, Networking + Hiring, Posters",
                "session_id": "ex2",
                "event_prefix": "conf",
                "track": "various",
                "livestream_id": "various-wed",
                "session_image": "ex2.png",
                "chair": [],
                "organizers": [],
                "time_start": "2022-10-19T14:00:00Z",
                "time_end": "2022-10-19T15:15:00Z",
                "discord_category": "",
                "discord_channel": "",
                "discord_channel_id": "",
                "discord_link": "",
                "slido_link": "",
                "youtube_url": "",
                "zoom_meeting": "",
                "zoom_password": "",
                "zoom_link": "",
                "ff_link": "",
                "ff_playlist": "",
                "time_slots": []
            },
            {
                "title": "Supporters Exhibits, Networking + Hiring, Posters",
                "session_id": "ex3",
                "event_prefix": "conf",
                "track": "various",
                "livestream_id": "various-thu",
                "session_image": "ex3.png",
                "chair": [],
                "organizers": [],
                "time_start": "2022-10-20T14:00:00Z",
                "time_end": "2022-10-20T15:15:00Z",
                "discord_category": "",
                "discord_channel": "",
                "discord_channel_id": "",
                "discord_link": "",
                "slido_link": "",
                "youtube_url": "",
                "zoom_meeting": "",
                "zoom_password": "",
                "zoom_link": "",
                "ff_link": "",
                "ff_playlist": "",
                "time_slots": []
            }
        ]
    },
    "v-full": {
        "event": "VIS Full Papers",
        "long_name": "VIS Full Papers",
        "event_type": "Paper Presentations",
        "event_prefix": "v-full",
        "event_description": "",
        "event_url": "",
        "organizers": [],
        "sessions": [
            {
                "title": "VIS Opening (10:45am-11:03am)| Best Papers (11:03am-12:00pm)",
                "session_id": "full1",
                "event_prefix": "v-full",
                "track": "ok4",
                "livestream_id": "ok4-tue",
                "session_image": "full1.png",
                "chair": [
                    "Tamara Munzner"
                ],
                "organizers": [],
                "time_start": "2022-10-18T15:45:00Z",
                "time_end": "2022-10-18T17:00:00Z",
                "discord_category": "",
                "discord_channel": "",
                "discord_channel_id": "",
                "discord_link": "",
                "slido_link": "",
                "youtube_url": "",
                "zoom_meeting": "",
                "zoom_password": "",
                "zoom_link": "",
                "ff_link": "",
                "ff_playlist": "",
                "time_slots": [
                    {
                        "slot_id": "v-full-1240-pres",
                        "session_id": "full1",
                        "type": "In Person Presentation",
                        "title": "Affective Learning Objectives for Communicative Visualizations",
                        "contributors": [
                            "Elsie Lee-Robbins"
                        ],
                        "authors": [
                            "Elsie Lee-Robbins",
                            "Eytan Adar"
                        ],
                        "abstract": "",
                        "uid": "v-full-1240",
                        "file_name": "",
                        "time_stamp": "2022-10-18T16:00:00Z",
                        "time_start": "2022-10-18T16:00:00Z",
                        "time_end": "2022-10-18T16:13:00Z",
                        "paper_type": "full",
                        "keywords": "",
                        "has_image": "1",
                        "has_video": "607",
                        "paper_award": "",
                        "image_caption": "Three examples of learning objectives and how they connect to the affective learning objectives taxonomy (left) and the example visualization from the Economist (right).",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-full-1240-qa",
                        "session_id": "full1",
                        "type": "In Person Q+A",
                        "title": "Affective Learning Objectives for Communicative Visualizations (Q+A)",
                        "contributors": [
                            "Elsie Lee-Robbins"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "v-full-1240",
                        "file_name": "",
                        "time_stamp": "2022-10-18T16:13:00Z",
                        "time_start": "2022-10-18T16:13:00Z",
                        "time_end": "2022-10-18T16:15:00Z",
                        "paper_type": "full",
                        "keywords": "",
                        "has_image": "1",
                        "has_video": "607",
                        "paper_award": "",
                        "image_caption": "Three examples of learning objectives and how they connect to the affective learning objectives taxonomy (left) and the example visualization from the Economist (right).",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-full-1636-pres",
                        "session_id": "full1",
                        "type": "In Person Presentation",
                        "title": "The Goldilocks Zone: Balancing Trust and Task-Based Performance in Multiple COVID-19 Forecast Visualization Choices",
                        "contributors": [
                            "Dr. Lace Padilla"
                        ],
                        "authors": [
                            "Lace Padilla",
                            "Racquel Fygenson",
                            "Spencer C. Castro",
                            "Enrico Bertini"
                        ],
                        "abstract": "",
                        "uid": "v-full-1636",
                        "file_name": "",
                        "time_stamp": "2022-10-18T16:15:00Z",
                        "time_start": "2022-10-18T16:15:00Z",
                        "time_end": "2022-10-18T16:28:00Z",
                        "paper_type": "full",
                        "keywords": "",
                        "has_image": "1",
                        "has_video": "940",
                        "paper_award": "",
                        "image_caption": "Multiple forecast visualizations (MFVs) used in Experiments 1 and 2 showing COVID-19 mortality forecasts for November 13, 2021 in the US. Each line depicts a different group\u2019s forecast, and the experiments examined the impact of the number of forecasts shown on trust and predictions of the COVID-19 trends. Each participant was shown the 16 stimuli in one row of this figure in a randomized order.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-full-1636-qa",
                        "session_id": "full1",
                        "type": "In Person Q+A",
                        "title": "The Goldilocks Zone: Balancing Trust and Task-Based Performance in Multiple COVID-19 Forecast Visualization Choices (Q+A)",
                        "contributors": [
                            "Dr. Lace Padilla"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "v-full-1636",
                        "file_name": "",
                        "time_stamp": "2022-10-18T16:28:00Z",
                        "time_start": "2022-10-18T16:28:00Z",
                        "time_end": "2022-10-18T16:30:00Z",
                        "paper_type": "full",
                        "keywords": "",
                        "has_image": "1",
                        "has_video": "940",
                        "paper_award": "",
                        "image_caption": "Multiple forecast visualizations (MFVs) used in Experiments 1 and 2 showing COVID-19 mortality forecasts for November 13, 2021 in the US. Each line depicts a different group\u2019s forecast, and the experiments examined the impact of the number of forecasts shown on trust and predictions of the COVID-19 trends. Each participant was shown the 16 stimuli in one row of this figure in a randomized order.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-full-1075-pres",
                        "session_id": "full1",
                        "type": "In Person Presentation",
                        "title": "Uncertainty-Aware Multidimensional Scaling",
                        "contributors": [
                            "David H\u00e4gele"
                        ],
                        "authors": [
                            "David H\u00e4gele",
                            "Tim Krake",
                            "Daniel Weiskopf"
                        ],
                        "abstract": "",
                        "uid": "v-full-1075",
                        "file_name": "",
                        "time_stamp": "2022-10-18T16:30:00Z",
                        "time_start": "2022-10-18T16:30:00Z",
                        "time_end": "2022-10-18T16:43:00Z",
                        "paper_type": "full",
                        "keywords": "",
                        "has_image": "1",
                        "has_video": "490",
                        "paper_award": "",
                        "image_caption": "Uncertainty-Aware Multidimensional Scaling applied to a set of 6-dimensional distributions. The distributions describe the variance in stats (Hit Points, Attack, Defense, Sp. Att., Sp. Def., Speed) for different Pokemon. The plot shows the projected probability densities with isolines for different quantiles.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-full-1075-qa",
                        "session_id": "full1",
                        "type": "In Person Q+A",
                        "title": "Uncertainty-Aware Multidimensional Scaling (Q+A)",
                        "contributors": [
                            "David H\u00e4gele"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "v-full-1075",
                        "file_name": "",
                        "time_stamp": "2022-10-18T16:43:00Z",
                        "time_start": "2022-10-18T16:43:00Z",
                        "time_end": "2022-10-18T16:45:00Z",
                        "paper_type": "full",
                        "keywords": "",
                        "has_image": "1",
                        "has_video": "490",
                        "paper_award": "",
                        "image_caption": "Uncertainty-Aware Multidimensional Scaling applied to a set of 6-dimensional distributions. The distributions describe the variance in stats (Hit Points, Attack, Defense, Sp. Att., Sp. Def., Speed) for different Pokemon. The plot shows the projected probability densities with isolines for different quantiles.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-short-1097-pres",
                        "session_id": "full1",
                        "type": "In Person Presentation",
                        "title": "Exploring D3 Implementation Challenges on Stack Overflow",
                        "contributors": [
                            "Leilani Battle"
                        ],
                        "authors": [
                            "Leilani Battle",
                            "Danni Feng",
                            "Kelli Webber"
                        ],
                        "abstract": "Visualization languages help to standardize the process of designing effective visualizations, one of the most prominent being D3. However, few researchers have analyzed at scale how users incorporate these languages into existing visualization programming processes, i.e., implementation workflows. In this paper, we present a new method for evaluating visualization languages. Our method emphasizes the experiences of users as observed through the online communities that have sprouted to facilitate public discussion and support around visualization languages. We demonstrate our method by analyzing D3 implementation workflows and challenges discussed on Stack Overflow. Our results show how the visualization community may be limiting its understanding of users' visualization implementation challenges by ignoring the larger context in which languages such as D3 are used. Based on our findings, we suggest new research directions to enhance the user experience with visualization languages. All our data and code are available at: https://osf.io/fup48/.",
                        "uid": "v-short-1097",
                        "file_name": "",
                        "time_stamp": "2022-10-18T16:45:00Z",
                        "time_start": "2022-10-18T16:45:00Z",
                        "time_end": "2022-10-18T16:58:00Z",
                        "paper_type": "short",
                        "keywords": "Web mining, visualization language evaluation",
                        "has_image": "1",
                        "has_video": "550",
                        "paper_award": "",
                        "image_caption": "By analyzing how and why D3 users post on Stack Overflow, we can understand how they program D3 visualizations and what challenges they encounter during the implementation process. Here are examples of visualizations and interactions that D3 users have discussed on Stack Overflow, as well as reference images they have used for inspiration.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-short-1097-qa",
                        "session_id": "full1",
                        "type": "In Person Q+A",
                        "title": "Exploring D3 Implementation Challenges on Stack Overflow (Q+A)",
                        "contributors": [
                            "Leilani Battle"
                        ],
                        "authors": [],
                        "abstract": "Visualization languages help to standardize the process of designing effective visualizations, one of the most prominent being D3. However, few researchers have analyzed at scale how users incorporate these languages into existing visualization programming processes, i.e., implementation workflows. In this paper, we present a new method for evaluating visualization languages. Our method emphasizes the experiences of users as observed through the online communities that have sprouted to facilitate public discussion and support around visualization languages. We demonstrate our method by analyzing D3 implementation workflows and challenges discussed on Stack Overflow. Our results show how the visualization community may be limiting its understanding of users' visualization implementation challenges by ignoring the larger context in which languages such as D3 are used. Based on our findings, we suggest new research directions to enhance the user experience with visualization languages. All our data and code are available at: https://osf.io/fup48/.",
                        "uid": "v-short-1097",
                        "file_name": "",
                        "time_stamp": "2022-10-18T16:58:00Z",
                        "time_start": "2022-10-18T16:58:00Z",
                        "time_end": "2022-10-18T17:00:00Z",
                        "paper_type": "short",
                        "keywords": "Web mining, visualization language evaluation",
                        "has_image": "1",
                        "has_video": "550",
                        "paper_award": "",
                        "image_caption": "By analyzing how and why D3 users post on Stack Overflow, we can understand how they program D3 visualizations and what challenges they encounter during the implementation process. Here are examples of visualizations and interactions that D3 users have discussed on Stack Overflow, as well as reference images they have used for inspiration.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    }
                ]
            },
            {
                "title": "Visualization Opportunities",
                "session_id": "full2",
                "event_prefix": "v-full",
                "track": "ok1",
                "livestream_id": "ok1-wed",
                "session_image": "full2.png",
                "chair": [
                    "Melanie Tory"
                ],
                "organizers": [],
                "time_start": "2022-10-19T15:45:00Z",
                "time_end": "2022-10-19T17:00:00Z",
                "discord_category": "",
                "discord_channel": "",
                "discord_channel_id": "",
                "discord_link": "",
                "slido_link": "",
                "youtube_url": "",
                "zoom_meeting": "",
                "zoom_password": "",
                "zoom_link": "",
                "ff_link": "",
                "ff_playlist": "",
                "time_slots": [
                    {
                        "slot_id": "full2-opening",
                        "session_id": "full2",
                        "type": "In Person Other",
                        "title": "Session Opening",
                        "contributors": [
                            "Melanie Tory"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "",
                        "file_name": "",
                        "time_stamp": "2022-10-19T15:45:00Z",
                        "time_start": "2022-10-19T15:45:00Z",
                        "time_end": "2022-10-19T15:45:00Z",
                        "paper_type": "",
                        "keywords": "",
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-full-1467-pres",
                        "session_id": "full2",
                        "type": "In Person Presentation",
                        "title": "KiriPhys: Exploring New Data Physicalization Opportunities",
                        "contributors": [
                            "Foroozan Daneshzand"
                        ],
                        "authors": [
                            "Foroozan Daneshzand",
                            "Charles Perin",
                            "Sheelagh Carpendale"
                        ],
                        "abstract": "",
                        "uid": "v-full-1467",
                        "file_name": "",
                        "time_stamp": "2022-10-19T15:45:00Z",
                        "time_start": "2022-10-19T15:45:00Z",
                        "time_end": "2022-10-19T15:55:00Z",
                        "paper_type": "full",
                        "keywords": "",
                        "has_image": "1",
                        "has_video": "567",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-full-1467-qa",
                        "session_id": "full2",
                        "type": "In Person Q+A",
                        "title": "KiriPhys: Exploring New Data Physicalization Opportunities (Q+A)",
                        "contributors": [
                            "Foroozan Daneshzand"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "v-full-1467",
                        "file_name": "",
                        "time_stamp": "2022-10-19T15:55:00Z",
                        "time_start": "2022-10-19T15:55:00Z",
                        "time_end": "2022-10-19T15:57:00Z",
                        "paper_type": "full",
                        "keywords": "",
                        "has_image": "1",
                        "has_video": "567",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-full-1197-pres",
                        "session_id": "full2",
                        "type": "Virtual Presentation (pre-recorded)",
                        "title": "Transtopia: Supporting Expressive and Faithful Pictograph Design with Visual Style Transfer",
                        "contributors": [
                            "Yang Shi"
                        ],
                        "authors": [
                            "Yang Shi",
                            "Pei Liu",
                            "Siji Chen",
                            "Mengdi Sun",
                            "Nan Cao"
                        ],
                        "abstract": "",
                        "uid": "v-full-1197",
                        "file_name": "",
                        "time_stamp": "2022-10-19T15:57:00Z",
                        "time_start": "2022-10-19T15:57:00Z",
                        "time_end": "2022-10-19T16:07:00Z",
                        "paper_type": "full",
                        "keywords": "",
                        "has_image": "1",
                        "has_video": "436",
                        "paper_award": "",
                        "image_caption": "Vistylist is a design support tool that disentangles the visual style of a source pictorial visualization from its content and transfers the visual style to one or more intended pictorial visualizations. The user interface of Vistylist consists of four main components: (1) users upload their source visualizations to the Design Panel (A) to extract visual elements, (2) users then upload their datasets to the Data Panel (B) to create field-channel mappings, (3) users can view the results on the Canvas (C), where they can adjust icons using the Icon Panel (D), and (4) the Suggestion Panel (E) suggested alternative designs.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-full-1197-qa",
                        "session_id": "full2",
                        "type": "Virtual Q+A",
                        "title": "Transtopia: Supporting Expressive and Faithful Pictograph Design with Visual Style Transfer (Q+A)",
                        "contributors": [
                            "Yang Shi"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "v-full-1197",
                        "file_name": "",
                        "time_stamp": "2022-10-19T16:07:00Z",
                        "time_start": "2022-10-19T16:07:00Z",
                        "time_end": "2022-10-19T16:09:00Z",
                        "paper_type": "full",
                        "keywords": "",
                        "has_image": "1",
                        "has_video": "436",
                        "paper_award": "",
                        "image_caption": "Vistylist is a design support tool that disentangles the visual style of a source pictorial visualization from its content and transfers the visual style to one or more intended pictorial visualizations. The user interface of Vistylist consists of four main components: (1) users upload their source visualizations to the Design Panel (A) to extract visual elements, (2) users then upload their datasets to the Data Panel (B) to create field-channel mappings, (3) users can view the results on the Canvas (C), where they can adjust icons using the Icon Panel (D), and (4) the Suggestion Panel (E) suggested alternative designs.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-full-1221-pres",
                        "session_id": "full2",
                        "type": "Virtual Presentation (pre-recorded)",
                        "title": "Self-Supervised Color-Concept Association via Image Colorization",
                        "contributors": [
                            "Ruizhen Hu",
                            "Ziqi Ye"
                        ],
                        "authors": [
                            "Ruizhen Hu",
                            "Ziqi Ye",
                            "Bin Chen",
                            "Oliver van Kaick",
                            "Hui Huang"
                        ],
                        "abstract": "",
                        "uid": "v-full-1221",
                        "file_name": "",
                        "time_stamp": "2022-10-19T16:09:00Z",
                        "time_start": "2022-10-19T16:09:00Z",
                        "time_end": "2022-10-19T16:19:00Z",
                        "paper_type": "full",
                        "keywords": "",
                        "has_image": "1",
                        "has_video": "617",
                        "paper_award": "",
                        "image_caption": "We introduce a self-supervised method for automatically extracting color-concept associations from natural images. We apply a colorization neural network to predict color distributions for input images. The distributions are transformed into ratings for a color library, which are then aggregated across multiple images of a concept.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-full-1221-qa",
                        "session_id": "full2",
                        "type": "Virtual Q+A",
                        "title": "Self-Supervised Color-Concept Association via Image Colorization (Q+A)",
                        "contributors": [
                            "Ruizhen Hu",
                            "Ziqi Ye"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "v-full-1221",
                        "file_name": "",
                        "time_stamp": "2022-10-19T16:19:00Z",
                        "time_start": "2022-10-19T16:19:00Z",
                        "time_end": "2022-10-19T16:21:00Z",
                        "paper_type": "full",
                        "keywords": "",
                        "has_image": "1",
                        "has_video": "617",
                        "paper_award": "",
                        "image_caption": "We introduce a self-supervised method for automatically extracting color-concept associations from natural images. We apply a colorization neural network to predict color distributions for input images. The distributions are transformed into ratings for a color library, which are then aggregated across multiple images of a concept.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-full-1616-pres",
                        "session_id": "full2",
                        "type": "In Person Presentation",
                        "title": "Cultivating Visualization Literacy for Children through Curiosity and Play",
                        "contributors": [
                            "S. Sandra Bae"
                        ],
                        "authors": [
                            "S. Sandra Bae",
                            "Rishi Vanukuru",
                            "Ruhan Yang",
                            "Peter Gyory",
                            "Ran Zhou",
                            "Ellen Yi-Luen Do",
                            "Danielle Albers Szafir"
                        ],
                        "abstract": "",
                        "uid": "v-full-1616",
                        "file_name": "",
                        "time_stamp": "2022-10-19T16:21:00Z",
                        "time_start": "2022-10-19T16:21:00Z",
                        "time_end": "2022-10-19T16:31:00Z",
                        "paper_type": "full",
                        "keywords": "",
                        "has_image": "1",
                        "has_video": "718",
                        "paper_award": "",
                        "image_caption": "Data is Yours. A toolkit to help children personalize visualizations made out of everyday materials and accumulate playful learning experiences.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-full-1616-qa",
                        "session_id": "full2",
                        "type": "In Person Q+A",
                        "title": "Cultivating Visualization Literacy for Children through Curiosity and Play (Q+A)",
                        "contributors": [
                            "S. Sandra Bae"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "v-full-1616",
                        "file_name": "",
                        "time_stamp": "2022-10-19T16:31:00Z",
                        "time_start": "2022-10-19T16:31:00Z",
                        "time_end": "2022-10-19T16:33:00Z",
                        "paper_type": "full",
                        "keywords": "",
                        "has_image": "1",
                        "has_video": "718",
                        "paper_award": "",
                        "image_caption": "Data is Yours. A toolkit to help children personalize visualizations made out of everyday materials and accumulate playful learning experiences.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-full-1237-pres",
                        "session_id": "full2",
                        "type": "In Person Presentation",
                        "title": "Roboviz: A Game-Centered Project for Information Visualization Education",
                        "contributors": [
                            "Eytan Adar"
                        ],
                        "authors": [
                            "Eytan Adar",
                            "Elsie Lee-Robbins"
                        ],
                        "abstract": "",
                        "uid": "v-full-1237",
                        "file_name": "",
                        "time_stamp": "2022-10-19T16:33:00Z",
                        "time_start": "2022-10-19T16:33:00Z",
                        "time_end": "2022-10-19T16:43:00Z",
                        "paper_type": "full",
                        "keywords": "",
                        "has_image": "1",
                        "has_video": "659",
                        "paper_award": "",
                        "image_caption": "A couple of robots from our Roboviz project documentation (left) and a set of projects produced by students (right). The top three visualizations (A, B, C) were different ways to visualize robot \"productivity\"--a key way to earn points in the game. The bottom three (D, E, F) are example visualizations of the network and hierarchical data from one team's dashboard. Each reflected a different possible strategy to earn points in the game.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-full-1237-qa",
                        "session_id": "full2",
                        "type": "In Person Q+A",
                        "title": "Roboviz: A Game-Centered Project for Information Visualization Education (Q+A)",
                        "contributors": [
                            "Eytan Adar"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "v-full-1237",
                        "file_name": "",
                        "time_stamp": "2022-10-19T16:43:00Z",
                        "time_start": "2022-10-19T16:43:00Z",
                        "time_end": "2022-10-19T16:45:00Z",
                        "paper_type": "full",
                        "keywords": "",
                        "has_image": "1",
                        "has_video": "659",
                        "paper_award": "",
                        "image_caption": "A couple of robots from our Roboviz project documentation (left) and a set of projects produced by students (right). The top three visualizations (A, B, C) were different ways to visualize robot \"productivity\"--a key way to earn points in the game. The bottom three (D, E, F) are example visualizations of the network and hierarchical data from one team's dashboard. Each reflected a different possible strategy to earn points in the game.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-full-1621-pres",
                        "session_id": "full2",
                        "type": "In Person Presentation",
                        "title": "Relaxed Dot Plots: Faithful Visualization of Samples and Their Distribution",
                        "contributors": [
                            "Nils Rodrigues"
                        ],
                        "authors": [
                            "Nils Rodrigues",
                            "Christoph Schulz",
                            "S\u00f6ren D\u00f6ring",
                            "Daniel Baumgartner",
                            "Tim Krake",
                            "Daniel Weiskopf"
                        ],
                        "abstract": "",
                        "uid": "v-full-1621",
                        "file_name": "",
                        "time_stamp": "2022-10-19T16:45:00Z",
                        "time_start": "2022-10-19T16:45:00Z",
                        "time_end": "2022-10-19T16:55:00Z",
                        "paper_type": "full",
                        "keywords": "",
                        "has_image": "1",
                        "has_video": "533",
                        "paper_award": "",
                        "image_caption": "Relaxed dot plot of the percentage of renewables in electricity production. Dots represent countries as indicated by flag. Outlines show the corresponding continent: Africa (blue), America (green), Asia (red), Europe (yellow), Oceania (black). Root scaling shrinks dots in areas with high value frequency. The technique represents subtle differences in values more faithfully than traditional dot plots through relaxation of stacks.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-full-1621-qa",
                        "session_id": "full2",
                        "type": "In Person Q+A",
                        "title": "Relaxed Dot Plots: Faithful Visualization of Samples and Their Distribution (Q+A)",
                        "contributors": [
                            "Nils Rodrigues"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "v-full-1621",
                        "file_name": "",
                        "time_stamp": "2022-10-19T16:55:00Z",
                        "time_start": "2022-10-19T16:55:00Z",
                        "time_end": "2022-10-19T16:57:00Z",
                        "paper_type": "full",
                        "keywords": "",
                        "has_image": "1",
                        "has_video": "533",
                        "paper_award": "",
                        "image_caption": "Relaxed dot plot of the percentage of renewables in electricity production. Dots represent countries as indicated by flag. Outlines show the corresponding continent: Africa (blue), America (green), Asia (red), Europe (yellow), Oceania (black). Root scaling shrinks dots in areas with high value frequency. The technique represents subtle differences in values more faithfully than traditional dot plots through relaxation of stacks.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    }
                ]
            },
            {
                "title": "Dealing with Scale, Space, and Dimension",
                "session_id": "full3",
                "event_prefix": "v-full",
                "track": "ok6",
                "livestream_id": "ok6-wed",
                "session_image": "full3.png",
                "chair": [
                    "Joshua A. Levine"
                ],
                "organizers": [],
                "time_start": "2022-10-19T20:45:00Z",
                "time_end": "2022-10-19T22:00:00Z",
                "discord_category": "",
                "discord_channel": "",
                "discord_channel_id": "",
                "discord_link": "",
                "slido_link": "",
                "youtube_url": "",
                "zoom_meeting": "",
                "zoom_password": "",
                "zoom_link": "",
                "ff_link": "",
                "ff_playlist": "",
                "time_slots": [
                    {
                        "slot_id": "full3-opening",
                        "session_id": "full3",
                        "type": "In Person Other",
                        "title": "Session Opening",
                        "contributors": [
                            "Joshua A. Levine"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "",
                        "file_name": "",
                        "time_stamp": "2022-10-19T20:45:00Z",
                        "time_start": "2022-10-19T20:45:00Z",
                        "time_end": "2022-10-19T20:45:00Z",
                        "paper_type": "",
                        "keywords": "",
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-full-1406-pres",
                        "session_id": "full3",
                        "type": "In Person Presentation",
                        "title": "Multivariate Probabilistic Range Queries for Scalable Interactive 3D Visualization",
                        "contributors": [
                            "Markus Hadwiger"
                        ],
                        "authors": [
                            "Amani Waleed Ageeli",
                            "Alberto Jaspe-Villanueva",
                            "Ronell Sicat",
                            "Florian Mannuss",
                            "Peter Rautek",
                            "Markus Hadwiger"
                        ],
                        "abstract": "",
                        "uid": "v-full-1406",
                        "file_name": "",
                        "time_stamp": "2022-10-19T20:45:00Z",
                        "time_start": "2022-10-19T20:45:00Z",
                        "time_end": "2022-10-19T20:55:00Z",
                        "paper_type": "full",
                        "keywords": "",
                        "has_image": "1",
                        "has_video": "629",
                        "paper_award": "",
                        "image_caption": "Large-scale scientific data, such as climate simulations, often comprise a large number of attributes for each data sample, like temperature, pressure, humidity, and many more. Interactive visualization and analysis require filtering according to any desired combination of attributes, which is challenging for large data and many attributes. We propose a flexible probabilistic framework for multivariate range queries that decouples all attribute dimensions via projection, allowing any subset of attributes to be queried with full efficiency. Moreover, our approach is output-sensitive, mainly scaling with the cardinality of the query result rather than with the input data size.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-full-1406-qa",
                        "session_id": "full3",
                        "type": "In Person Q+A",
                        "title": "Multivariate Probabilistic Range Queries for Scalable Interactive 3D Visualization (Q+A)",
                        "contributors": [
                            "Markus Hadwiger"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "v-full-1406",
                        "file_name": "",
                        "time_stamp": "2022-10-19T20:55:00Z",
                        "time_start": "2022-10-19T20:55:00Z",
                        "time_end": "2022-10-19T20:57:00Z",
                        "paper_type": "full",
                        "keywords": "",
                        "has_image": "1",
                        "has_video": "629",
                        "paper_award": "",
                        "image_caption": "Large-scale scientific data, such as climate simulations, often comprise a large number of attributes for each data sample, like temperature, pressure, humidity, and many more. Interactive visualization and analysis require filtering according to any desired combination of attributes, which is challenging for large data and many attributes. We propose a flexible probabilistic framework for multivariate range queries that decouples all attribute dimensions via projection, allowing any subset of attributes to be queried with full efficiency. Moreover, our approach is output-sensitive, mainly scaling with the cardinality of the query result rather than with the input data size.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-full-1394-pres",
                        "session_id": "full3",
                        "type": "Virtual Presentation (pre-recorded)",
                        "title": "Dual Space Coupling Model Guided Overlap-free Scatterplot",
                        "contributors": [
                            "zeyu li"
                        ],
                        "authors": [
                            "zeyu li",
                            "RuiZhi Shi",
                            "Yan Liu",
                            "Shizhuo Long",
                            "Ziheng Guo",
                            "Shichao Jia",
                            "Jiawan Zhang"
                        ],
                        "abstract": "",
                        "uid": "v-full-1394",
                        "file_name": "",
                        "time_stamp": "2022-10-19T20:57:00Z",
                        "time_start": "2022-10-19T20:57:00Z",
                        "time_end": "2022-10-19T21:07:00Z",
                        "paper_type": "full",
                        "keywords": "",
                        "has_image": "0",
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-full-1394-qa",
                        "session_id": "full3",
                        "type": "Virtual Q+A",
                        "title": "Dual Space Coupling Model Guided Overlap-free Scatterplot (Q+A)",
                        "contributors": [
                            "zeyu li"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "v-full-1394",
                        "file_name": "",
                        "time_stamp": "2022-10-19T21:07:00Z",
                        "time_start": "2022-10-19T21:07:00Z",
                        "time_end": "2022-10-19T21:09:00Z",
                        "paper_type": "full",
                        "keywords": "",
                        "has_image": "0",
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-full-1124-pres",
                        "session_id": "full3",
                        "type": "In Person Presentation",
                        "title": "Measuring Effects of Spatial Visualization and Domain on Visualization Task Performance: A Comparative Study",
                        "contributors": [
                            "Sara Tandon"
                        ],
                        "authors": [
                            "Sara Tandon",
                            "Alfie Abdul-Rahman",
                            "Rita Borgo"
                        ],
                        "abstract": "",
                        "uid": "v-full-1124",
                        "file_name": "",
                        "time_stamp": "2022-10-19T21:09:00Z",
                        "time_start": "2022-10-19T21:09:00Z",
                        "time_end": "2022-10-19T21:19:00Z",
                        "paper_type": "full",
                        "keywords": "",
                        "has_image": "1",
                        "has_video": "747",
                        "paper_award": "",
                        "image_caption": "This study builds on current research in domain-specific visualization and cognition to address if domain and spatial visualization ability combine to affect performance on information visualization tasks. \nWe measure spatial visualization and visual task performance between those with tertiary education and professional profile in business, law & political science, and math & computer science. \nStimuli consisted of bar chart layouts rotated along Cartesian and polar coordinates to assess performance on spatially rotated data. \nOur results can advance inclusive practices in visualization design and add to knowledge in domain-specific visual research that can empower designers across disciplines to create effective visualizations.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-full-1124-qa",
                        "session_id": "full3",
                        "type": "In Person Q+A",
                        "title": "Measuring Effects of Spatial Visualization and Domain on Visualization Task Performance: A Comparative Study (Q+A)",
                        "contributors": [
                            "Sara Tandon"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "v-full-1124",
                        "file_name": "",
                        "time_stamp": "2022-10-19T21:19:00Z",
                        "time_start": "2022-10-19T21:19:00Z",
                        "time_end": "2022-10-19T21:21:00Z",
                        "paper_type": "full",
                        "keywords": "",
                        "has_image": "1",
                        "has_video": "747",
                        "paper_award": "",
                        "image_caption": "This study builds on current research in domain-specific visualization and cognition to address if domain and spatial visualization ability combine to affect performance on information visualization tasks. \nWe measure spatial visualization and visual task performance between those with tertiary education and professional profile in business, law & political science, and math & computer science. \nStimuli consisted of bar chart layouts rotated along Cartesian and polar coordinates to assess performance on spatially rotated data. \nOur results can advance inclusive practices in visualization design and add to knowledge in domain-specific visual research that can empower designers across disciplines to create effective visualizations.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-tvcg-9735308-pres",
                        "session_id": "full3",
                        "type": "In Person Presentation",
                        "title": "Local Latent Representation based on Geometric Convolution for Particle Data Feature Exploration",
                        "contributors": [
                            "Li, Haoyu",
                            "Haoyu Li"
                        ],
                        "authors": [
                            "Haoyu Li",
                            "Han-Wei Shen"
                        ],
                        "abstract": "Feature related particle data analysis plays an important role in many scientific applications such as fluid simulations, cosmology simulations and molecular dynamics. Compared to conventional methods that use hand-crafted feature descriptors, some recent studies focus on transforming the data into a new latent space, where features are easier to be identified, compared and extracted. However, it is challenging to transform particle data into latent representations, since the convolution neural networks used in prior studies require the data presented in regular grids. In this paper, we adopt Geometric Convolution, a neural network building block designed for 3D point clouds, to create latent representations for scientific particle data. These latent representations capture both the particle positions and their physical attributes in the local neighborhood so that features can be extracted by clustering in the latent space, and tracked by applying tracking algorithms such as mean-shift. We validate the extracted features and tracking results from our approach using datasets from three applications and show that they are comparable to the methods that define hand-crafted features for each specific dataset.",
                        "uid": "v-tvcg-9735308",
                        "file_name": "",
                        "time_stamp": "2022-10-19T21:21:00Z",
                        "time_start": "2022-10-19T21:21:00Z",
                        "time_end": "2022-10-19T21:31:00Z",
                        "paper_type": "full",
                        "keywords": "Data transformation, Particle data, Feature extraction and tracking, Deep learning",
                        "has_image": "1",
                        "has_video": "591",
                        "paper_award": "",
                        "image_caption": "We present a particle data feature analysis method using geometric convolution based neural networks. Feature descriptor is defined using geometric convolution and automatedly learned using an autoencoder architecture. The bottom left figure shows an overview workflow. With our designed feature exploration tool (top left figure), we demonstrate our approach has comparable feature extraction quality (left figure) and does not require tedious feature descriptor crafting or costly data resampling to regular grids.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-tvcg-9735308-qa",
                        "session_id": "full3",
                        "type": "In Person Q+A",
                        "title": "Local Latent Representation based on Geometric Convolution for Particle Data Feature Exploration (Q+A)",
                        "contributors": [
                            "Li, Haoyu",
                            "Haoyu Li"
                        ],
                        "authors": [],
                        "abstract": "Feature related particle data analysis plays an important role in many scientific applications such as fluid simulations, cosmology simulations and molecular dynamics. Compared to conventional methods that use hand-crafted feature descriptors, some recent studies focus on transforming the data into a new latent space, where features are easier to be identified, compared and extracted. However, it is challenging to transform particle data into latent representations, since the convolution neural networks used in prior studies require the data presented in regular grids. In this paper, we adopt Geometric Convolution, a neural network building block designed for 3D point clouds, to create latent representations for scientific particle data. These latent representations capture both the particle positions and their physical attributes in the local neighborhood so that features can be extracted by clustering in the latent space, and tracked by applying tracking algorithms such as mean-shift. We validate the extracted features and tracking results from our approach using datasets from three applications and show that they are comparable to the methods that define hand-crafted features for each specific dataset.",
                        "uid": "v-tvcg-9735308",
                        "file_name": "",
                        "time_stamp": "2022-10-19T21:31:00Z",
                        "time_start": "2022-10-19T21:31:00Z",
                        "time_end": "2022-10-19T21:33:00Z",
                        "paper_type": "full",
                        "keywords": "Data transformation, Particle data, Feature extraction and tracking, Deep learning",
                        "has_image": "1",
                        "has_video": "591",
                        "paper_award": "",
                        "image_caption": "We present a particle data feature analysis method using geometric convolution based neural networks. Feature descriptor is defined using geometric convolution and automatedly learned using an autoencoder architecture. The bottom left figure shows an overview workflow. With our designed feature exploration tool (top left figure), we demonstrate our approach has comparable feature extraction quality (left figure) and does not require tedious feature descriptor crafting or costly data resampling to regular grids.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-tvcg-9765327-pres",
                        "session_id": "full3",
                        "type": "In Person Presentation",
                        "title": "Graphical Enhancements for Effective Exemplar Identification in Contextual Data Visualizations",
                        "contributors": [
                            "Shenghui Cheng",
                            "Xinyu Zhang"
                        ],
                        "authors": [
                            "Xinyu Zhang",
                            "Shenghui Cheng",
                            "Klaus Mueller"
                        ],
                        "abstract": "An exemplar is an entity that represents a desirable instance in a multi-attribute configuration space. It offers certain strengths in some of its attributes without unduly compromising the strengths in other attributes. Exemplars are frequently sought after in real life applications, such as systems engineering, investment banking, drug advisory, product marketing and many others. We study a specific method for the visualization of multi-attribute configuration spaces, the Data Context Map (DCM), for its capacity in enabling users to identify proper exemplars. The DCM produces a 2D embedding where users can view the data objects in the context of the data attributes. We ask whether certain graphical enhancements can aid users to gain a better understanding of the attribute-wise tradeoffs and so select better exemplar sets.  We conducted several user studies for three different graphical designs, namely iso-contour, value-shaded topographic rendering and terrain topographic rendering, and compare these with a baseline DCM display. As a benchmark we use an exemplar set generated via Pareto optimization which has similar goals but unlike humans can operate in the native high-dimensional data space. Our study finds that the two topographic maps are statistically superior to both the iso-contour and the DCM baseline display.",
                        "uid": "v-tvcg-9765327",
                        "file_name": "",
                        "time_stamp": "2022-10-19T21:33:00Z",
                        "time_start": "2022-10-19T21:33:00Z",
                        "time_end": "2022-10-19T21:43:00Z",
                        "paper_type": "full",
                        "keywords": "High-dimensional data, multivariate data, contextual displays, exemplar generation, decision support, configuration space",
                        "has_image": "1",
                        "has_video": "688",
                        "paper_award": "",
                        "image_caption": "These are graphical enhancements on data context map tested by (a)wine dataset on iso-contour map, (b)university dataset on value-shaded map and (c)car dataset on terrain map. Points on paths are exemplars recommended by our Pareto guided searching algorithm and stacked bar charts show their attributes.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-tvcg-9765327-qa",
                        "session_id": "full3",
                        "type": "In Person Q+A",
                        "title": "Graphical Enhancements for Effective Exemplar Identification in Contextual Data Visualizations (Q+A)",
                        "contributors": [
                            "Shenghui Cheng",
                            "Xinyu Zhang"
                        ],
                        "authors": [],
                        "abstract": "An exemplar is an entity that represents a desirable instance in a multi-attribute configuration space. It offers certain strengths in some of its attributes without unduly compromising the strengths in other attributes. Exemplars are frequently sought after in real life applications, such as systems engineering, investment banking, drug advisory, product marketing and many others. We study a specific method for the visualization of multi-attribute configuration spaces, the Data Context Map (DCM), for its capacity in enabling users to identify proper exemplars. The DCM produces a 2D embedding where users can view the data objects in the context of the data attributes. We ask whether certain graphical enhancements can aid users to gain a better understanding of the attribute-wise tradeoffs and so select better exemplar sets.  We conducted several user studies for three different graphical designs, namely iso-contour, value-shaded topographic rendering and terrain topographic rendering, and compare these with a baseline DCM display. As a benchmark we use an exemplar set generated via Pareto optimization which has similar goals but unlike humans can operate in the native high-dimensional data space. Our study finds that the two topographic maps are statistically superior to both the iso-contour and the DCM baseline display.",
                        "uid": "v-tvcg-9765327",
                        "file_name": "",
                        "time_stamp": "2022-10-19T21:43:00Z",
                        "time_start": "2022-10-19T21:43:00Z",
                        "time_end": "2022-10-19T21:45:00Z",
                        "paper_type": "full",
                        "keywords": "High-dimensional data, multivariate data, contextual displays, exemplar generation, decision support, configuration space",
                        "has_image": "1",
                        "has_video": "688",
                        "paper_award": "",
                        "image_caption": "These are graphical enhancements on data context map tested by (a)wine dataset on iso-contour map, (b)university dataset on value-shaded map and (c)car dataset on terrain map. Points on paths are exemplars recommended by our Pareto guided searching algorithm and stacked bar charts show their attributes.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-tvcg-9514468-pres",
                        "session_id": "full3",
                        "type": "In Person Presentation",
                        "title": "Effectiveness Error: Measuring and Improving RadViz Visual Effectiveness",
                        "contributors": [
                            "Marco Angelini"
                        ],
                        "authors": [
                            "Marco Angelini",
                            "Graziano Blasilli",
                            "Simone Lenti",
                            "Alessia Palleschi",
                            "Giuseppe Santucci"
                        ],
                        "abstract": "RadViz contributes to multidimensional analysis by using 2D points for encoding data elements and interpreting them alongthe original data dimensions. For these characteristics it is used in different application domains, like clustering, anomaly detection, and software visualization. However, it is likely that using the dimension arrangement that comes with the data will produce a plot that leads users to make inaccurate conclusions about points values and data distribution. This paper attacks this problem without altering the original RadViz design: it defines, for both a single point and a set of points, the metric of effectiveness error, and uses it to define the objective function of a dimension arrangement strategy, arguing that minimizing it increases the overall RadViz visual quality. This paper investigated the intuition that reducing the effectiveness error is beneficial for other well-known RadViz problems, like points clumping toward the center, many-to-one plotting of non-proportional points, and cluster separation. It presents an algorithm that reduces to zero the effectiveness error for a single point and a heuristic that approximates the dimension arrangement minimizing the effectiveness error for an arbitrary set of points. A set of experiments based on 21 real datasets has been performed, with the goals of analyzing the advantages of reducing the effectiveness error, comparing the proposed dimension arrangement strategy with other related proposals, and investigating the heuristic accuracy. The Effectiveness Error metric, the algorithm, and the heuristic presented in this paper have been made available in a d3.jsplugin at  https://aware-diag-sapienza.github.io/d3-radviz.",
                        "uid": "v-tvcg-9514468",
                        "file_name": "",
                        "time_stamp": "2022-10-19T21:45:00Z",
                        "time_start": "2022-10-19T21:45:00Z",
                        "time_end": "2022-10-19T21:55:00Z",
                        "paper_type": "full",
                        "keywords": "Dimensionality reduction, RadViz, dimension arrangement, visual quality metrics.",
                        "has_image": "1",
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "Two RadViz charts show the CSM dataset using two different Dimension Arrangements out of about 20 million, highlighting the points' Effectiveness Error (EE).\n\nThe original dataset DA on the left shows quite a high EE for the points, suggesting inaccurate conclusions: the data distribution is led by Budget, Gross, and Screens.\n\nThe RadViz on the right uses the arrangement generated by the Effectiveness Error Minimization Heuristic proposed in this paper. Most of the points have an EE close to zero, suggesting that the current DA produces correct insights: the data distribution is leaded by Sentiment and Ratings.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-tvcg-9514468-qa",
                        "session_id": "full3",
                        "type": "In Person Q+A",
                        "title": "Effectiveness Error: Measuring and Improving RadViz Visual Effectiveness (Q+A)",
                        "contributors": [
                            "Marco Angelini"
                        ],
                        "authors": [],
                        "abstract": "RadViz contributes to multidimensional analysis by using 2D points for encoding data elements and interpreting them alongthe original data dimensions. For these characteristics it is used in different application domains, like clustering, anomaly detection, and software visualization. However, it is likely that using the dimension arrangement that comes with the data will produce a plot that leads users to make inaccurate conclusions about points values and data distribution. This paper attacks this problem without altering the original RadViz design: it defines, for both a single point and a set of points, the metric of effectiveness error, and uses it to define the objective function of a dimension arrangement strategy, arguing that minimizing it increases the overall RadViz visual quality. This paper investigated the intuition that reducing the effectiveness error is beneficial for other well-known RadViz problems, like points clumping toward the center, many-to-one plotting of non-proportional points, and cluster separation. It presents an algorithm that reduces to zero the effectiveness error for a single point and a heuristic that approximates the dimension arrangement minimizing the effectiveness error for an arbitrary set of points. A set of experiments based on 21 real datasets has been performed, with the goals of analyzing the advantages of reducing the effectiveness error, comparing the proposed dimension arrangement strategy with other related proposals, and investigating the heuristic accuracy. The Effectiveness Error metric, the algorithm, and the heuristic presented in this paper have been made available in a d3.jsplugin at  https://aware-diag-sapienza.github.io/d3-radviz.",
                        "uid": "v-tvcg-9514468",
                        "file_name": "",
                        "time_stamp": "2022-10-19T21:55:00Z",
                        "time_start": "2022-10-19T21:55:00Z",
                        "time_end": "2022-10-19T21:57:00Z",
                        "paper_type": "full",
                        "keywords": "Dimensionality reduction, RadViz, dimension arrangement, visual quality metrics.",
                        "has_image": "1",
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "Two RadViz charts show the CSM dataset using two different Dimension Arrangements out of about 20 million, highlighting the points' Effectiveness Error (EE).\n\nThe original dataset DA on the left shows quite a high EE for the points, suggesting inaccurate conclusions: the data distribution is led by Budget, Gross, and Screens.\n\nThe RadViz on the right uses the arrangement generated by the Effectiveness Error Minimization Heuristic proposed in this paper. Most of the points have an EE close to zero, suggesting that the current DA produces correct insights: the data distribution is leaded by Sentiment and Ratings.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    }
                ]
            },
            {
                "title": "Text, Language, and Image Data",
                "session_id": "full4",
                "event_prefix": "v-full",
                "track": "ok4",
                "livestream_id": "ok4-thu",
                "session_image": "full4.png",
                "chair": [
                    "Steffen Koch"
                ],
                "organizers": [],
                "time_start": "2022-10-20T20:45:00Z",
                "time_end": "2022-10-20T22:00:00Z",
                "discord_category": "",
                "discord_channel": "",
                "discord_channel_id": "",
                "discord_link": "",
                "slido_link": "",
                "youtube_url": "",
                "zoom_meeting": "",
                "zoom_password": "",
                "zoom_link": "",
                "ff_link": "",
                "ff_playlist": "",
                "time_slots": [
                    {
                        "slot_id": "full4-opening",
                        "session_id": "full4",
                        "type": "In Person Other",
                        "title": "Session Opening",
                        "contributors": [
                            "Steffen Koch"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "",
                        "file_name": "",
                        "time_stamp": "2022-10-20T20:45:00Z",
                        "time_start": "2022-10-20T20:45:00Z",
                        "time_end": "2022-10-20T20:45:00Z",
                        "paper_type": "",
                        "keywords": "",
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-tvcg-9801603-pres",
                        "session_id": "full4",
                        "type": "Virtual Presentation (pre-recorded)",
                        "title": "A Unified Understanding of Deep NLP Models for Text Classification",
                        "contributors": [
                            "Weikai Yang",
                            "Zhen Li"
                        ],
                        "authors": [
                            "Zhen Li",
                            "Xiting Wang",
                            "Weikai Yang",
                            "Jing Wu",
                            "Zhengyan Zhang",
                            "Zhiyuan Liu",
                            "Maosong Sun",
                            "Hui Zhang",
                            "Shixia Liu"
                        ],
                        "abstract": "The rapid development of deep natural language processing (NLP) models for text classification has led to an urgent need for a unified understanding of these models proposed individually. Existing methods cannot meet the need for understanding different models in one framework due to the lack of a unified measure for explaining both low-level (e.g., words) and high-level (e.g., phrases) features. We have developed a visual analysis tool, DeepNLPVis, to enable a unified understanding of NLP models for text classification. The key idea is a mutual information-based measure, which provides quantitative explanations on how each layer of a model maintains the information of input words in a sample. We model the intra- and inter-word information at each layer measuring the importance of a word to the final prediction as well as the relationships between words, such as the formation of phrases. A multi-level visualization, which consists of a corpus-level, a sample-level, and a word-level visualization, supports the analysis from the overall training set to individual samples. Two case studies on classification tasks and comparison between models demonstrate that DeepNLPVis can help users effectively identify potential problems caused by samples and model architectures and then make informed improvements.",
                        "uid": "v-tvcg-9801603",
                        "file_name": "",
                        "time_stamp": "2022-10-20T20:45:00Z",
                        "time_start": "2022-10-20T20:45:00Z",
                        "time_end": "2022-10-20T20:55:00Z",
                        "paper_type": "full",
                        "keywords": "Explainable AI, visual debugging, visual analytics, deep NLP model, information-based interpretation",
                        "has_image": "1",
                        "has_video": "561",
                        "paper_award": "",
                        "image_caption": "DeepNLPVis for analyzing the BERT model on news classification: (a) class view for showing the overall model performance; (b) distribution view for identifying samples and words of interest; (c) word contribution of selected samples; (d) sample list; (e) information flow for analyzing a sample by its intra- and inter-word information.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-tvcg-9801603-qa",
                        "session_id": "full4",
                        "type": "Virtual Q+A",
                        "title": "A Unified Understanding of Deep NLP Models for Text Classification (Q+A)",
                        "contributors": [
                            "Weikai Yang",
                            "Zhen Li"
                        ],
                        "authors": [],
                        "abstract": "The rapid development of deep natural language processing (NLP) models for text classification has led to an urgent need for a unified understanding of these models proposed individually. Existing methods cannot meet the need for understanding different models in one framework due to the lack of a unified measure for explaining both low-level (e.g., words) and high-level (e.g., phrases) features. We have developed a visual analysis tool, DeepNLPVis, to enable a unified understanding of NLP models for text classification. The key idea is a mutual information-based measure, which provides quantitative explanations on how each layer of a model maintains the information of input words in a sample. We model the intra- and inter-word information at each layer measuring the importance of a word to the final prediction as well as the relationships between words, such as the formation of phrases. A multi-level visualization, which consists of a corpus-level, a sample-level, and a word-level visualization, supports the analysis from the overall training set to individual samples. Two case studies on classification tasks and comparison between models demonstrate that DeepNLPVis can help users effectively identify potential problems caused by samples and model architectures and then make informed improvements.",
                        "uid": "v-tvcg-9801603",
                        "file_name": "",
                        "time_stamp": "2022-10-20T20:55:00Z",
                        "time_start": "2022-10-20T20:55:00Z",
                        "time_end": "2022-10-20T20:57:00Z",
                        "paper_type": "full",
                        "keywords": "Explainable AI, visual debugging, visual analytics, deep NLP model, information-based interpretation",
                        "has_image": "1",
                        "has_video": "561",
                        "paper_award": "",
                        "image_caption": "DeepNLPVis for analyzing the BERT model on news classification: (a) class view for showing the overall model performance; (b) distribution view for identifying samples and words of interest; (c) word contribution of selected samples; (d) sample list; (e) information flow for analyzing a sample by its intra- and inter-word information.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-tvcg-9801527-pres",
                        "session_id": "full4",
                        "type": "In Person Presentation",
                        "title": "EBBE-Text: Explaining Neural Networks by Exploring Text Classification Decision Boundaries",
                        "contributors": [
                            "Alexis Delaforge"
                        ],
                        "authors": [
                            "Alexis Delaforge",
                            "J\u00e9r\u00f4me Az\u00e9",
                            "Sandra Bringay",
                            "Caroline Mollevi",
                            "Arnaud Sallaberry",
                            "Maximilien Servajean"
                        ],
                        "abstract": "While neural networks (NN) have been successfully applied to many NLP tasks, the way they function is often difficult to interpret. In this article, we focus on binary text classification via NNs and propose a new tool, which includes a visualization of the decision boundary and the distances of data elements to this boundary. This tool increases the interpretability of NN. Our approach uses two innovative views: (1) an overview of the text representation space and (2) a local view allowing data exploration around the decision boundary for various localities of this representation space. These views are integrated into a visual platform, EBBE-Text, which also contains state-of-the-art visualizations of NN representation spaces and several kinds of information obtained from the classification process. The various views are linked through numerous interactive functionalities that enable easy exploration of texts and classification results via the various complementary views. A user study shows the effectiveness of the visual encoding and a case study illustrates the benefits of using our tool for the analysis of the classifications obtained with several recent NNs and two datasets.",
                        "uid": "v-tvcg-9801527",
                        "file_name": "",
                        "time_stamp": "2022-10-20T20:57:00Z",
                        "time_start": "2022-10-20T20:57:00Z",
                        "time_end": "2022-10-20T21:07:00Z",
                        "paper_type": "full",
                        "keywords": "Artificial neural networks, Data visualization, Computational modeling, Natural language processing, Predictive models, Task analysis, Deep learning, Binary text classification, decision boundary, deep learning, interpretability, neural networks, representation space, visual analytics",
                        "has_image": "1",
                        "has_video": "554",
                        "paper_award": "",
                        "image_caption": "Locality view of EBBE-Text. From left to right and top to bottom: boundary sub-view, path text list, top 10 word list, text representation spaces resulting from dimension reduction, confusion matrix, input text form, classify command. With these features, EBBE-Text is used to explain neural network predictions for a binary text classification task.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-tvcg-9801527-qa",
                        "session_id": "full4",
                        "type": "In Person Q+A",
                        "title": "EBBE-Text: Explaining Neural Networks by Exploring Text Classification Decision Boundaries (Q+A)",
                        "contributors": [
                            "Alexis Delaforge"
                        ],
                        "authors": [],
                        "abstract": "While neural networks (NN) have been successfully applied to many NLP tasks, the way they function is often difficult to interpret. In this article, we focus on binary text classification via NNs and propose a new tool, which includes a visualization of the decision boundary and the distances of data elements to this boundary. This tool increases the interpretability of NN. Our approach uses two innovative views: (1) an overview of the text representation space and (2) a local view allowing data exploration around the decision boundary for various localities of this representation space. These views are integrated into a visual platform, EBBE-Text, which also contains state-of-the-art visualizations of NN representation spaces and several kinds of information obtained from the classification process. The various views are linked through numerous interactive functionalities that enable easy exploration of texts and classification results via the various complementary views. A user study shows the effectiveness of the visual encoding and a case study illustrates the benefits of using our tool for the analysis of the classifications obtained with several recent NNs and two datasets.",
                        "uid": "v-tvcg-9801527",
                        "file_name": "",
                        "time_stamp": "2022-10-20T21:07:00Z",
                        "time_start": "2022-10-20T21:07:00Z",
                        "time_end": "2022-10-20T21:09:00Z",
                        "paper_type": "full",
                        "keywords": "Artificial neural networks, Data visualization, Computational modeling, Natural language processing, Predictive models, Task analysis, Deep learning, Binary text classification, decision boundary, deep learning, interpretability, neural networks, representation space, visual analytics",
                        "has_image": "1",
                        "has_video": "554",
                        "paper_award": "",
                        "image_caption": "Locality view of EBBE-Text. From left to right and top to bottom: boundary sub-view, path text list, top 10 word list, text representation spaces resulting from dimension reduction, confusion matrix, input text form, classify command. With these features, EBBE-Text is used to explain neural network predictions for a binary text classification task.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-tvcg-9716779-pres",
                        "session_id": "full4",
                        "type": "In Person Presentation",
                        "title": "LegalVis: Exploring and Inferring Precedent Citations in Legal Documents",
                        "contributors": [
                            "Jean Roberto",
                            "Lucas Resck"
                        ],
                        "authors": [
                            "Lucas E. Resck",
                            "Jean R. Ponciano",
                            "Luis Gustavo Nonato",
                            "Jorge Poco"
                        ],
                        "abstract": "To reduce the number of pending cases and conflicting rulings in the Brazilian Judiciary, the National Congress amended the Constitution, allowing the Brazilian Supreme Court (STF) to create binding precedents (BPs), i.e., a set of understandings that both Executive and lower Judiciary branches must follow. The STF's justices frequently cite the 58 existing BPs in their decisions, and it is of primary relevance that judicial experts could identify and analyze such citations. To assist in this problem, we propose LegalVis, a web-based visual analytics system designed to support the analysis of legal documents that cite or could potentially cite a BP. We model the problem of identifying potential citations (i.e., non-explicit) as a classification problem. However, a simple score is not enough to explain the results; that is why we use an interpretability machine learning method to explain the reason behind each identified citation. For a compelling visual exploration of documents and BPs, LegalVis comprises three interactive visual components: the first presents an overview of the data showing temporal patterns, the second allows filtering and grouping relevant documents by topic, and the last one shows a document's text aiming to interpret the model's output by pointing out which paragraphs are likely to mention the BP, even if not explicitly specified. We evaluated our identification model and obtained an accuracy of 96%; we also made a quantitative and qualitative analysis of the results. The usefulness and effectiveness of LegalVis were evaluated through two usage scenarios and feedback from six domain experts.",
                        "uid": "v-tvcg-9716779",
                        "file_name": "",
                        "time_stamp": "2022-10-20T21:09:00Z",
                        "time_start": "2022-10-20T21:09:00Z",
                        "time_end": "2022-10-20T21:19:00Z",
                        "paper_type": "full",
                        "keywords": "Legal Documents, Visual Analytics, Brazilian Legal System, Natural Language Processing",
                        "has_image": "1",
                        "has_video": "523",
                        "paper_award": "",
                        "image_caption": "LegalVis is a web-based visual analytic system designed to assist judicial experts in analyzing Brazilian legal documents that cite or could potentially cite binding precedents. LegalVis first identifies potential citations by implementing a machine learning model based on classification. Then, an interpretability mechanism provides reliable explanations for the model's decisions. All this information becomes accessible through the three provided linked views: (a) Global View, which presents an overview of the data showing temporal patterns; (b) Paragraph Similarities View, which allows filtering and grouping of relevant documents; and (c) Document Reader, which shows a document\u2019s text while highlighting relevant paragraphs.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-tvcg-9716779-qa",
                        "session_id": "full4",
                        "type": "In Person Q+A",
                        "title": "LegalVis: Exploring and Inferring Precedent Citations in Legal Documents (Q+A)",
                        "contributors": [
                            "Jean Roberto",
                            "Lucas Resck"
                        ],
                        "authors": [],
                        "abstract": "To reduce the number of pending cases and conflicting rulings in the Brazilian Judiciary, the National Congress amended the Constitution, allowing the Brazilian Supreme Court (STF) to create binding precedents (BPs), i.e., a set of understandings that both Executive and lower Judiciary branches must follow. The STF's justices frequently cite the 58 existing BPs in their decisions, and it is of primary relevance that judicial experts could identify and analyze such citations. To assist in this problem, we propose LegalVis, a web-based visual analytics system designed to support the analysis of legal documents that cite or could potentially cite a BP. We model the problem of identifying potential citations (i.e., non-explicit) as a classification problem. However, a simple score is not enough to explain the results; that is why we use an interpretability machine learning method to explain the reason behind each identified citation. For a compelling visual exploration of documents and BPs, LegalVis comprises three interactive visual components: the first presents an overview of the data showing temporal patterns, the second allows filtering and grouping relevant documents by topic, and the last one shows a document's text aiming to interpret the model's output by pointing out which paragraphs are likely to mention the BP, even if not explicitly specified. We evaluated our identification model and obtained an accuracy of 96%; we also made a quantitative and qualitative analysis of the results. The usefulness and effectiveness of LegalVis were evaluated through two usage scenarios and feedback from six domain experts.",
                        "uid": "v-tvcg-9716779",
                        "file_name": "",
                        "time_stamp": "2022-10-20T21:19:00Z",
                        "time_start": "2022-10-20T21:19:00Z",
                        "time_end": "2022-10-20T21:21:00Z",
                        "paper_type": "full",
                        "keywords": "Legal Documents, Visual Analytics, Brazilian Legal System, Natural Language Processing",
                        "has_image": "1",
                        "has_video": "523",
                        "paper_award": "",
                        "image_caption": "LegalVis is a web-based visual analytic system designed to assist judicial experts in analyzing Brazilian legal documents that cite or could potentially cite binding precedents. LegalVis first identifies potential citations by implementing a machine learning model based on classification. Then, an interpretability mechanism provides reliable explanations for the model's decisions. All this information becomes accessible through the three provided linked views: (a) Global View, which presents an overview of the data showing temporal patterns; (b) Paragraph Similarities View, which allows filtering and grouping of relevant documents; and (c) Document Reader, which shows a document\u2019s text while highlighting relevant paragraphs.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-tvcg-9488285-pres",
                        "session_id": "full4",
                        "type": "Virtual Presentation (pre-recorded)",
                        "title": "DeHumor: Visual Analytics for Decomposing Humor",
                        "contributors": [
                            "Xingbo Wang"
                        ],
                        "authors": [
                            "Xingbo Wang",
                            "Yao Ming",
                            "Tongshuang Wu",
                            "Haipeng Zeng",
                            "Yong Wang",
                            "Huamin Qu"
                        ],
                        "abstract": "Despite being a critical communication skill, grasping humor is challenging\u2014a successful use of humor requires a mixture of both engaging content build-up and an appropriate vocal delivery (e.g., pause). Prior studies on computational humor emphasize the textual and audio features immediately next to the punchline, yet overlooking longer-term context setup. Moreover, the theories are usually too abstract for understanding each concrete humor snippet. To fill in the gap, we develop DeHumor, a visual analytical system for analyzing humorous behaviors in public speaking. To intuitively reveal the building blocks of each concrete example, DeHumor decomposes each humorous video into multimodal features and provides inline annotations of them on the video script. In particular, to better capture the build-ups, we introduce content repetition as a complement to features introduced in theories of computational humor and visualize them in a context linking graph. To help users locate the punchlines that have the desired features to learn, we summarize the content (with keywords) and humor feature statistics on an augmented time matrix. With case studies on stand-up comedy shows and TED talks, we show that DeHumor is able to highlight various building blocks of humor examples. In addition, expert interviews with communication coaches and humor researchers demonstrate the effectiveness of DeHumor for multimodal humor analysis of speech content and vocal delivery.",
                        "uid": "v-tvcg-9488285",
                        "file_name": "",
                        "time_stamp": "2022-10-20T21:21:00Z",
                        "time_start": "2022-10-20T21:21:00Z",
                        "time_end": "2022-10-20T21:31:00Z",
                        "paper_type": "full",
                        "keywords": "Humor, Context, Multimodal Features, Visualization",
                        "has_image": "1",
                        "has_video": "649",
                        "paper_award": "",
                        "image_caption": "Humor\u2014a critical communication skill\u2014requires a mixture of both engaging content build-ups and an appropriate vocal delivery. In this paper, we present DeHumor, a visual analytics system to decompose humor into a set of quantifiable verbal and vocal features. DeHumor helps domain experts systematically explore humorous behaviors from speech level, context level, and sentence level.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-tvcg-9488285-qa",
                        "session_id": "full4",
                        "type": "Virtual Q+A",
                        "title": "DeHumor: Visual Analytics for Decomposing Humor (Q+A)",
                        "contributors": [
                            "Xingbo Wang"
                        ],
                        "authors": [],
                        "abstract": "Despite being a critical communication skill, grasping humor is challenging\u2014a successful use of humor requires a mixture of both engaging content build-up and an appropriate vocal delivery (e.g., pause). Prior studies on computational humor emphasize the textual and audio features immediately next to the punchline, yet overlooking longer-term context setup. Moreover, the theories are usually too abstract for understanding each concrete humor snippet. To fill in the gap, we develop DeHumor, a visual analytical system for analyzing humorous behaviors in public speaking. To intuitively reveal the building blocks of each concrete example, DeHumor decomposes each humorous video into multimodal features and provides inline annotations of them on the video script. In particular, to better capture the build-ups, we introduce content repetition as a complement to features introduced in theories of computational humor and visualize them in a context linking graph. To help users locate the punchlines that have the desired features to learn, we summarize the content (with keywords) and humor feature statistics on an augmented time matrix. With case studies on stand-up comedy shows and TED talks, we show that DeHumor is able to highlight various building blocks of humor examples. In addition, expert interviews with communication coaches and humor researchers demonstrate the effectiveness of DeHumor for multimodal humor analysis of speech content and vocal delivery.",
                        "uid": "v-tvcg-9488285",
                        "file_name": "",
                        "time_stamp": "2022-10-20T21:31:00Z",
                        "time_start": "2022-10-20T21:31:00Z",
                        "time_end": "2022-10-20T21:33:00Z",
                        "paper_type": "full",
                        "keywords": "Humor, Context, Multimodal Features, Visualization",
                        "has_image": "1",
                        "has_video": "649",
                        "paper_award": "",
                        "image_caption": "Humor\u2014a critical communication skill\u2014requires a mixture of both engaging content build-ups and an appropriate vocal delivery. In this paper, we present DeHumor, a visual analytics system to decompose humor into a set of quantifiable verbal and vocal features. DeHumor helps domain experts systematically explore humorous behaviors from speech level, context level, and sentence level.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-full-1651-pres",
                        "session_id": "full4",
                        "type": "In Person Presentation",
                        "title": "Interactive and visual prompt engineering for ad-hoc task adaptation with large language models",
                        "contributors": [
                            "Hendrik Strobelt"
                        ],
                        "authors": [
                            "Hendrik Strobelt",
                            "Albert Webson",
                            "Victor Sanh",
                            "Benjamin Hoover",
                            "Johanna Beyer",
                            "Hanspeter Pfister",
                            "Alexander Rush"
                        ],
                        "abstract": "",
                        "uid": "v-full-1651",
                        "file_name": "",
                        "time_stamp": "2022-10-20T21:33:00Z",
                        "time_start": "2022-10-20T21:33:00Z",
                        "time_end": "2022-10-20T21:43:00Z",
                        "paper_type": "full",
                        "keywords": "",
                        "has_image": "1",
                        "has_video": "385",
                        "paper_award": "",
                        "image_caption": "Example of PromptIDE's interface to explore variations of different prompts. Each variation is tested against up to twenty data examples and represented as a template card (a). For each variation, rich detail can be tracked by using the detail stripes (b). If performance and qualitative detail are convincing, a user can collect the prompt in the prompt cart (c).",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-full-1651-qa",
                        "session_id": "full4",
                        "type": "In Person Q+A",
                        "title": "Interactive and visual prompt engineering for ad-hoc task adaptation with large language models (Q+A)",
                        "contributors": [
                            "Hendrik Strobelt"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "v-full-1651",
                        "file_name": "",
                        "time_stamp": "2022-10-20T21:43:00Z",
                        "time_start": "2022-10-20T21:43:00Z",
                        "time_end": "2022-10-20T21:45:00Z",
                        "paper_type": "full",
                        "keywords": "",
                        "has_image": "1",
                        "has_video": "385",
                        "paper_award": "",
                        "image_caption": "Example of PromptIDE's interface to explore variations of different prompts. Each variation is tested against up to twenty data examples and represented as a template card (a). For each variation, rich detail can be tracked by using the detail stripes (b). If performance and qualitative detail are convincing, a user can collect the prompt in the prompt cart (c).",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-tvcg-9765476-pres",
                        "session_id": "full4",
                        "type": "In Person Presentation",
                        "title": "A Hybrid In Situ Approach for Cost Efficient Image Database Generation",
                        "contributors": [
                            "Steffen Frey"
                        ],
                        "authors": [
                            "Valentin Bruder",
                            "Matthew Larsen",
                            "Thomas Ertl",
                            "Hank Childs",
                            "Steffen Frey"
                        ],
                        "abstract": "The visualization of results while the simulation is running is increasingly common in extreme scale computing environments. We present a novel approach for in situ generation of image databases to achieve cost savings on supercomputers. Our approach, a hybrid between traditional inline and in transit techniques, dynamically distributes visualization tasks between simulation nodes and visualization nodes, using probing as a basis to estimate rendering cost. Our hybrid design differs from previous works in that it creates opportunities to minimize idle time from four fundamental types of inefficiency: variability, limited scalability, overhead, and rightsizing. We demonstrate our results by comparing our method against both inline and in transit methods for a variety of configurations, including two simulation codes and a scaling study that goes above 19K cores. Our findings show that our approach is superior in many configurations. As in situ visualization becomes increasingly ubiquitous, we believe our technique could lead to significant amounts of reclaimed cycles on supercomputers.",
                        "uid": "v-tvcg-9765476",
                        "file_name": "",
                        "time_stamp": "2022-10-20T21:45:00Z",
                        "time_start": "2022-10-20T21:45:00Z",
                        "time_end": "2022-10-20T21:55:00Z",
                        "paper_type": "full",
                        "keywords": "Visualization, High performance computing, In situ",
                        "has_image": "1",
                        "has_video": "527",
                        "paper_award": "",
                        "image_caption": "Our hybrid in situ approach dynamically distributes visualization tasks based on predicted render times. It combines characteristics from traditional in situ techniques: it employs dedicated visualization nodes such as in transit in situ, and simulation nodes can also take over rendering tasks like in inline in situ. Dynamic task distribution allows to address various inefficiencies (variability, scalability, rightsizing and overhead) and save precious supercomputer resources.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-tvcg-9765476-qa",
                        "session_id": "full4",
                        "type": "In Person Q+A",
                        "title": "A Hybrid In Situ Approach for Cost Efficient Image Database Generation (Q+A)",
                        "contributors": [
                            "Steffen Frey"
                        ],
                        "authors": [],
                        "abstract": "The visualization of results while the simulation is running is increasingly common in extreme scale computing environments. We present a novel approach for in situ generation of image databases to achieve cost savings on supercomputers. Our approach, a hybrid between traditional inline and in transit techniques, dynamically distributes visualization tasks between simulation nodes and visualization nodes, using probing as a basis to estimate rendering cost. Our hybrid design differs from previous works in that it creates opportunities to minimize idle time from four fundamental types of inefficiency: variability, limited scalability, overhead, and rightsizing. We demonstrate our results by comparing our method against both inline and in transit methods for a variety of configurations, including two simulation codes and a scaling study that goes above 19K cores. Our findings show that our approach is superior in many configurations. As in situ visualization becomes increasingly ubiquitous, we believe our technique could lead to significant amounts of reclaimed cycles on supercomputers.",
                        "uid": "v-tvcg-9765476",
                        "file_name": "",
                        "time_stamp": "2022-10-20T21:55:00Z",
                        "time_start": "2022-10-20T21:55:00Z",
                        "time_end": "2022-10-20T21:57:00Z",
                        "paper_type": "full",
                        "keywords": "Visualization, High performance computing, In situ",
                        "has_image": "1",
                        "has_video": "527",
                        "paper_award": "",
                        "image_caption": "Our hybrid in situ approach dynamically distributes visualization tasks based on predicted render times. It combines characteristics from traditional in situ techniques: it employs dedicated visualization nodes such as in transit in situ, and simulation nodes can also take over rendering tasks like in inline in situ. Dynamic task distribution allows to address various inefficiencies (variability, scalability, rightsizing and overhead) and save precious supercomputer resources.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    }
                ]
            },
            {
                "title": "Transforming Tabular Data and Grammars",
                "session_id": "full5",
                "event_prefix": "v-full",
                "track": "ok5",
                "livestream_id": "ok5-wed",
                "session_image": "full5.png",
                "chair": [
                    "Alexander Lex"
                ],
                "organizers": [],
                "time_start": "2022-10-19T14:00:00Z",
                "time_end": "2022-10-19T15:15:00Z",
                "discord_category": "",
                "discord_channel": "",
                "discord_channel_id": "",
                "discord_link": "",
                "slido_link": "",
                "youtube_url": "",
                "zoom_meeting": "",
                "zoom_password": "",
                "zoom_link": "",
                "ff_link": "",
                "ff_playlist": "",
                "time_slots": [
                    {
                        "slot_id": "full5-opening",
                        "session_id": "full5",
                        "type": "In Person Other",
                        "title": "Session Opening",
                        "contributors": [
                            "Alexander Lex"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "",
                        "file_name": "",
                        "time_stamp": "2022-10-19T14:00:00Z",
                        "time_start": "2022-10-19T14:00:00Z",
                        "time_end": "2022-10-19T14:00:00Z",
                        "paper_type": "",
                        "keywords": "",
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-full-1346-pres",
                        "session_id": "full5",
                        "type": "Virtual Presentation (pre-recorded)",
                        "title": "Revealing the Semantics of Data Wrangling Script With COMANTICS",
                        "contributors": [
                            "Kai Xiong"
                        ],
                        "authors": [
                            "Kai Xiong",
                            "Zhongsu Luo",
                            "Siwei Fu",
                            "Yongheng Wang",
                            "Mingliang Xu",
                            "Yingcai Wu"
                        ],
                        "abstract": "",
                        "uid": "v-full-1346",
                        "file_name": "",
                        "time_stamp": "2022-10-19T14:00:00Z",
                        "time_start": "2022-10-19T14:00:00Z",
                        "time_end": "2022-10-19T14:10:00Z",
                        "paper_type": "full",
                        "keywords": "",
                        "has_image": "1",
                        "has_video": "601",
                        "paper_award": "",
                        "image_caption": "COMANTICS is a three-step pipeline that automatically detects the semantics of data wrangling scripts by inferring the types of data transformations with their parameters. COMANTICS first generates intermediate input and output tables for each line of code and detects changes between them. Then, it identifies the transformation type through characteristic-based and CNN-based components. Last, it infers parameters for the transformation by employing a \u201cslot filling\u201d strategy.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-full-1346-qa",
                        "session_id": "full5",
                        "type": "Virtual Q+A",
                        "title": "Revealing the Semantics of Data Wrangling Script With COMANTICS (Q+A)",
                        "contributors": [
                            "Kai Xiong"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "v-full-1346",
                        "file_name": "",
                        "time_stamp": "2022-10-19T14:10:00Z",
                        "time_start": "2022-10-19T14:10:00Z",
                        "time_end": "2022-10-19T14:12:00Z",
                        "paper_type": "full",
                        "keywords": "",
                        "has_image": "1",
                        "has_video": "601",
                        "paper_award": "",
                        "image_caption": "COMANTICS is a three-step pipeline that automatically detects the semantics of data wrangling scripts by inferring the types of data transformations with their parameters. COMANTICS first generates intermediate input and output tables for each line of code and detects changes between them. Then, it identifies the transformation type through characteristic-based and CNN-based components. Last, it infers parameters for the transformation by employing a \u201cslot filling\u201d strategy.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-tvcg-9693232-pres",
                        "session_id": "full5",
                        "type": "Virtual Presentation (pre-recorded)",
                        "title": "Visualizing the Scripts of Data Wrangling with SOMNUS",
                        "contributors": [
                            "Kai Xiong,"
                        ],
                        "authors": [
                            "Kai Xiong",
                            "Siwei Fu",
                            "Guoming Ding",
                            "Zhongsu Luo",
                            "Rong Yu",
                            "Wei Chen",
                            "Hujun Bao",
                            "Yingcai Wu"
                        ],
                        "abstract": "Data workers use various scripting languages for data transformation, such as SAS, R, and Python. However, understanding intricate code pieces requires advanced programming skills, which hinders data workers from grasping the idea of data transformation at ease. Program visualization is beneficial for debugging and education and has the potential to illustrate transformations intuitively and interactively. In this paper, we explore visualization design for demonstrating the semantics of code pieces in the context of data transformation. First, to depict individual data transformations, we structure a design space by two primary dimensions, i.e., key parameters to encode and possible visual channels to be mapped. Then, we derive a collection of 23 glyphs that visualize the semantics of transformations. Next, we design a pipeline, named Somnus, that provides an overview of the creation and evolution of data tables using a provenance graph. At the same time, it allows detailed investigation of individual transformations. User feedback on Somnus is positive. Our study participants achieved better accuracy with less time using Somnus, and preferred it over carefully-crafted textual description. Further, we provide two example applications to demonstrate the utility and versatility of Somnus.",
                        "uid": "v-tvcg-9693232",
                        "file_name": "",
                        "time_stamp": "2022-10-19T14:12:00Z",
                        "time_start": "2022-10-19T14:12:00Z",
                        "time_end": "2022-10-19T14:22:00Z",
                        "paper_type": "full",
                        "keywords": "Program understanding, data transformation, visualization design",
                        "has_image": "1",
                        "has_video": "600",
                        "paper_award": "",
                        "image_caption": "SOMNUS is a pipeline to visualize the semantics of code pieces in the context of data transformation. SOMNUS accepts a data wrangling script with its source tables as input and results in a glyph-based provenance graph where nodes represent tables and edges denote data transformations. We implement SOMNUS as a web-based system, which can help data scientists validate the procedure of data transformation. We also apply SOMNUS to explain scripts generated by MORPHEUS to reveal intermediate data transformations given source and target tables.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-tvcg-9693232-qa",
                        "session_id": "full5",
                        "type": "Virtual Q+A",
                        "title": "Visualizing the Scripts of Data Wrangling with SOMNUS (Q+A)",
                        "contributors": [
                            "Kai Xiong,"
                        ],
                        "authors": [],
                        "abstract": "Data workers use various scripting languages for data transformation, such as SAS, R, and Python. However, understanding intricate code pieces requires advanced programming skills, which hinders data workers from grasping the idea of data transformation at ease. Program visualization is beneficial for debugging and education and has the potential to illustrate transformations intuitively and interactively. In this paper, we explore visualization design for demonstrating the semantics of code pieces in the context of data transformation. First, to depict individual data transformations, we structure a design space by two primary dimensions, i.e., key parameters to encode and possible visual channels to be mapped. Then, we derive a collection of 23 glyphs that visualize the semantics of transformations. Next, we design a pipeline, named Somnus, that provides an overview of the creation and evolution of data tables using a provenance graph. At the same time, it allows detailed investigation of individual transformations. User feedback on Somnus is positive. Our study participants achieved better accuracy with less time using Somnus, and preferred it over carefully-crafted textual description. Further, we provide two example applications to demonstrate the utility and versatility of Somnus.",
                        "uid": "v-tvcg-9693232",
                        "file_name": "",
                        "time_stamp": "2022-10-19T14:22:00Z",
                        "time_start": "2022-10-19T14:22:00Z",
                        "time_end": "2022-10-19T14:24:00Z",
                        "paper_type": "full",
                        "keywords": "Program understanding, data transformation, visualization design",
                        "has_image": "1",
                        "has_video": "600",
                        "paper_award": "",
                        "image_caption": "SOMNUS is a pipeline to visualize the semantics of code pieces in the context of data transformation. SOMNUS accepts a data wrangling script with its source tables as input and results in a glyph-based provenance graph where nodes represent tables and edges denote data transformations. We implement SOMNUS as a web-based system, which can help data scientists validate the procedure of data transformation. We also apply SOMNUS to explain scripts generated by MORPHEUS to reveal intermediate data transformations given source and target tables.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-full-1171-pres",
                        "session_id": "full5",
                        "type": "In Person Presentation",
                        "title": "Rigel: Transforming Tabular Data By Declarative Mapping",
                        "contributors": [
                            "Ran Chen"
                        ],
                        "authors": [
                            "Ran Chen",
                            "Di Weng",
                            "Yanwei Huang",
                            "Xinhuan Shu",
                            "Jiayi Zhou",
                            "Guodao Sun",
                            "Yingcai Wu"
                        ],
                        "abstract": "",
                        "uid": "v-full-1171",
                        "file_name": "",
                        "time_stamp": "2022-10-19T14:24:00Z",
                        "time_start": "2022-10-19T14:24:00Z",
                        "time_end": "2022-10-19T14:34:00Z",
                        "paper_type": "full",
                        "keywords": "",
                        "has_image": "1",
                        "has_video": "593",
                        "paper_award": "",
                        "image_caption": "Rigel\u2019s user interface: Rigel allows users to construct declarative mappings by dragging or typing values from input data into a spreadsheet. Rigel recommends possible mappings from data to row, column, and cell channels to declaratively compose user-desired tables.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-full-1171-qa",
                        "session_id": "full5",
                        "type": "In Person Q+A",
                        "title": "Rigel: Transforming Tabular Data By Declarative Mapping (Q+A)",
                        "contributors": [
                            "Ran Chen"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "v-full-1171",
                        "file_name": "",
                        "time_stamp": "2022-10-19T14:34:00Z",
                        "time_start": "2022-10-19T14:34:00Z",
                        "time_end": "2022-10-19T14:36:00Z",
                        "paper_type": "full",
                        "keywords": "",
                        "has_image": "1",
                        "has_video": "593",
                        "paper_award": "",
                        "image_caption": "Rigel\u2019s user interface: Rigel allows users to construct declarative mappings by dragging or typing values from input data into a spreadsheet. Rigel recommends possible mappings from data to row, column, and cell channels to declaratively compose user-desired tables.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-full-1413-pres",
                        "session_id": "full5",
                        "type": "Virtual Presentation (pre-recorded)",
                        "title": "HiTailor: Interactive Transformation and Visualization for Hierarchical Tabular Data",
                        "contributors": [
                            "Guozheng Li",
                            "Runfei Li"
                        ],
                        "authors": [
                            "Guozheng Li",
                            "Runfei Li",
                            "Zicheng Wang",
                            "Chi Harold Liu",
                            "Min Lu",
                            "Guoren Wang"
                        ],
                        "abstract": "",
                        "uid": "v-full-1413",
                        "file_name": "",
                        "time_stamp": "2022-10-19T14:36:00Z",
                        "time_start": "2022-10-19T14:36:00Z",
                        "time_end": "2022-10-19T14:46:00Z",
                        "paper_type": "full",
                        "keywords": "",
                        "has_image": "1",
                        "has_video": "550",
                        "paper_award": "",
                        "image_caption": "The user interface of the HiTailor prototype system. (a) Transformation operator panel; (b) Tabular visualization panel; (c) Visualization template panel; (d) Visualization configuration panel.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-full-1413-qa",
                        "session_id": "full5",
                        "type": "Virtual Q+A",
                        "title": "HiTailor: Interactive Transformation and Visualization for Hierarchical Tabular Data (Q+A)",
                        "contributors": [
                            "Guozheng Li",
                            "Runfei Li"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "v-full-1413",
                        "file_name": "",
                        "time_stamp": "2022-10-19T14:46:00Z",
                        "time_start": "2022-10-19T14:46:00Z",
                        "time_end": "2022-10-19T14:48:00Z",
                        "paper_type": "full",
                        "keywords": "",
                        "has_image": "1",
                        "has_video": "550",
                        "paper_award": "",
                        "image_caption": "The user interface of the HiTailor prototype system. (a) Transformation operator panel; (b) Tabular visualization panel; (c) Visualization template panel; (d) Visualization configuration panel.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-full-1138-pres",
                        "session_id": "full5",
                        "type": "In Person Presentation",
                        "title": "Animated Vega-Lite: A Unified Grammar of Interactive and Animated Visualizations",
                        "contributors": [
                            "Jonathan Zong"
                        ],
                        "authors": [
                            "Jonathan Zong",
                            "Josh M. Pollock",
                            "Dylan Wootton",
                            "Arvind Satyanarayan"
                        ],
                        "abstract": "",
                        "uid": "v-full-1138",
                        "file_name": "",
                        "time_stamp": "2022-10-19T14:48:00Z",
                        "time_start": "2022-10-19T14:48:00Z",
                        "time_end": "2022-10-19T14:58:00Z",
                        "paper_type": "full",
                        "keywords": "",
                        "has_image": "1",
                        "has_video": "734",
                        "paper_award": "",
                        "image_caption": "An analyst\u2019s workflow with Animated Vega-Lite. A) Static visualization of bird migrations. B) Adding interaction to hover over a migration path and view a tooltip. C) Switching from static lines to animated circle marks. D) Adding animated path trails for the previous 5 days. E) Adding an interactive slider to scrub through the animation.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-full-1138-qa",
                        "session_id": "full5",
                        "type": "In Person Q+A",
                        "title": "Animated Vega-Lite: A Unified Grammar of Interactive and Animated Visualizations (Q+A)",
                        "contributors": [
                            "Jonathan Zong"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "v-full-1138",
                        "file_name": "",
                        "time_stamp": "2022-10-19T14:58:00Z",
                        "time_start": "2022-10-19T14:58:00Z",
                        "time_end": "2022-10-19T15:00:00Z",
                        "paper_type": "full",
                        "keywords": "",
                        "has_image": "1",
                        "has_video": "734",
                        "paper_award": "",
                        "image_caption": "An analyst\u2019s workflow with Animated Vega-Lite. A) Static visualization of bird migrations. B) Adding interaction to hover over a migration path and view a tooltip. C) Switching from static lines to animated circle marks. D) Adding animated path trails for the previous 5 days. E) Adding an interactive slider to scrub through the animation.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-full-1045-pres",
                        "session_id": "full5",
                        "type": "In Person Presentation",
                        "title": "No Grammar to Rule Them All: A Survey of JSON-style DSLs for Visualization",
                        "contributors": [
                            "Andrew M McNutt"
                        ],
                        "authors": [
                            "Andrew M McNutt"
                        ],
                        "abstract": "",
                        "uid": "v-full-1045",
                        "file_name": "",
                        "time_stamp": "2022-10-19T15:00:00Z",
                        "time_start": "2022-10-19T15:00:00Z",
                        "time_end": "2022-10-19T15:10:00Z",
                        "paper_type": "full",
                        "keywords": "",
                        "has_image": "1",
                        "has_video": "574",
                        "paper_award": "",
                        "image_caption": "How are JSON-style domain specific languages for visualization designed and implemented? In this paper we answer that question by analyzing 57 different languages. We identify five key concerns relating to the targeted domain, the underlying model, the composition of languages, affordances relating to the targeted end user, and other implementation practicalities.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-full-1045-qa",
                        "session_id": "full5",
                        "type": "In Person Q+A",
                        "title": "No Grammar to Rule Them All: A Survey of JSON-style DSLs for Visualization (Q+A)",
                        "contributors": [
                            "Andrew M McNutt"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "v-full-1045",
                        "file_name": "",
                        "time_stamp": "2022-10-19T15:10:00Z",
                        "time_start": "2022-10-19T15:10:00Z",
                        "time_end": "2022-10-19T15:12:00Z",
                        "paper_type": "full",
                        "keywords": "",
                        "has_image": "1",
                        "has_video": "574",
                        "paper_award": "",
                        "image_caption": "How are JSON-style domain specific languages for visualization designed and implemented? In this paper we answer that question by analyzing 57 different languages. We identify five key concerns relating to the targeted domain, the underlying model, the composition of languages, affordances relating to the targeted end user, and other implementation practicalities.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    }
                ]
            },
            {
                "title": "Spatial Data",
                "session_id": "full6",
                "event_prefix": "v-full",
                "track": "ok6",
                "livestream_id": "ok6-fri",
                "session_image": "full6.png",
                "chair": [
                    "Holger Theisel"
                ],
                "organizers": [],
                "time_start": "2022-10-21T14:00:00Z",
                "time_end": "2022-10-21T15:15:00Z",
                "discord_category": "",
                "discord_channel": "",
                "discord_channel_id": "",
                "discord_link": "",
                "slido_link": "",
                "youtube_url": "",
                "zoom_meeting": "",
                "zoom_password": "",
                "zoom_link": "",
                "ff_link": "",
                "ff_playlist": "",
                "time_slots": [
                    {
                        "slot_id": "full6-opening",
                        "session_id": "full6",
                        "type": "In Person Other",
                        "title": "Session Opening",
                        "contributors": [
                            "Holger Theisel"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "",
                        "file_name": "",
                        "time_stamp": "2022-10-21T14:00:00Z",
                        "time_start": "2022-10-21T14:00:00Z",
                        "time_end": "2022-10-21T14:00:00Z",
                        "paper_type": "",
                        "keywords": "",
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-tvcg-9576578-pres",
                        "session_id": "full6",
                        "type": "In Person Presentation",
                        "title": "Real-Time Visualization of Large-Scale Geological Models with Nonlinear Feature-Preserving  Levels of Detail",
                        "contributors": [
                            "Ronell Sicat"
                        ],
                        "authors": [
                            "Ronell Sicat",
                            "Mohamed Ibrahim",
                            "Amani Ageeli",
                            "Florian Mannuss",
                            "Peter Rautek",
                            "Markus Hadwiger"
                        ],
                        "abstract": "The rapidly growing size and complexity of 3D geological models has increased the need for level-of-detail techniques and compact encodings to facilitate interactive visualization. For large-scale hexahedral meshes, state-of-the-art approaches often employ wavelet schemes for level of detail as well as for data compression. Here, wavelet transforms serve two  purposes: (1) they achieve substantial compression for data reduction; and (2) the multiresolution encoding provides levels of detail for visualization. However, in coarser detail levels, important geometric features, such as geological faults, often get too smoothed out or lost, due to linear translation invariant filtering. The same is true for attribute features, such as discontinuities in porosity or permeability.We present a novel, integrated approach addressing both purposes above, while preserving critical data features of both model geometry and its attributes. Our first major contribution is that we completely decouple the computation of levels of detail from data compression, and perform nonlinear filtering in a high-dimensional data space jointly representing the geological model geometry with its attributes. Computing detail levels in this space enables us to jointly preserve features in both geometry and attributes. While designed in a general way, our framework specifically employs joint bilateral filters, computed efficiently on a high-dimensional permutohedral grid. For data compression, after the computation of all detail levels, each level is separately encoded with a standard wavelet transform. Our second major contribution is a compact GPU data structure for the encoded mesh and attributes that enables direct real-time GPU visualization without prior decoding.",
                        "uid": "v-tvcg-9576578",
                        "file_name": "",
                        "time_stamp": "2022-10-21T14:00:00Z",
                        "time_start": "2022-10-21T14:00:00Z",
                        "time_end": "2022-10-21T14:10:00Z",
                        "paper_type": "full",
                        "keywords": "Geological model visualization, Structured hexahedral meshes, Multiresolution representations and visualization, GPU data structures and rendering",
                        "has_image": "1",
                        "has_video": "528",
                        "paper_award": "",
                        "image_caption": "Coarse resolution levels of hexahedral meshes with attributes computed using subsampling (b,c) can suffer from degradation of small details and faults (magenta annotations: faults disappear and turn into slanted cells). HexaShrink (e,f) is able to maintain faults but attribute edges can bleed out (red annotations) and the mesh and attribute spatial positions are misaligned (orange annotations). Our approach (a,d) considers the mesh and attribute features jointly so that even regions with homogenous or flat attributes but with non-homogenous mesh (cyan boxes) or other way around (green boxes) will be rendered with adaptively fine-detailed geometry.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-tvcg-9576578-qa",
                        "session_id": "full6",
                        "type": "In Person Q+A",
                        "title": "Real-Time Visualization of Large-Scale Geological Models with Nonlinear Feature-Preserving  Levels of Detail (Q+A)",
                        "contributors": [
                            "Ronell Sicat"
                        ],
                        "authors": [],
                        "abstract": "The rapidly growing size and complexity of 3D geological models has increased the need for level-of-detail techniques and compact encodings to facilitate interactive visualization. For large-scale hexahedral meshes, state-of-the-art approaches often employ wavelet schemes for level of detail as well as for data compression. Here, wavelet transforms serve two  purposes: (1) they achieve substantial compression for data reduction; and (2) the multiresolution encoding provides levels of detail for visualization. However, in coarser detail levels, important geometric features, such as geological faults, often get too smoothed out or lost, due to linear translation invariant filtering. The same is true for attribute features, such as discontinuities in porosity or permeability.We present a novel, integrated approach addressing both purposes above, while preserving critical data features of both model geometry and its attributes. Our first major contribution is that we completely decouple the computation of levels of detail from data compression, and perform nonlinear filtering in a high-dimensional data space jointly representing the geological model geometry with its attributes. Computing detail levels in this space enables us to jointly preserve features in both geometry and attributes. While designed in a general way, our framework specifically employs joint bilateral filters, computed efficiently on a high-dimensional permutohedral grid. For data compression, after the computation of all detail levels, each level is separately encoded with a standard wavelet transform. Our second major contribution is a compact GPU data structure for the encoded mesh and attributes that enables direct real-time GPU visualization without prior decoding.",
                        "uid": "v-tvcg-9576578",
                        "file_name": "",
                        "time_stamp": "2022-10-21T14:10:00Z",
                        "time_start": "2022-10-21T14:10:00Z",
                        "time_end": "2022-10-21T14:12:00Z",
                        "paper_type": "full",
                        "keywords": "Geological model visualization, Structured hexahedral meshes, Multiresolution representations and visualization, GPU data structures and rendering",
                        "has_image": "1",
                        "has_video": "528",
                        "paper_award": "",
                        "image_caption": "Coarse resolution levels of hexahedral meshes with attributes computed using subsampling (b,c) can suffer from degradation of small details and faults (magenta annotations: faults disappear and turn into slanted cells). HexaShrink (e,f) is able to maintain faults but attribute edges can bleed out (red annotations) and the mesh and attribute spatial positions are misaligned (orange annotations). Our approach (a,d) considers the mesh and attribute features jointly so that even regions with homogenous or flat attributes but with non-homogenous mesh (cyan boxes) or other way around (green boxes) will be rendered with adaptively fine-detailed geometry.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-tvcg-9736650-pres",
                        "session_id": "full6",
                        "type": "In Person Presentation",
                        "title": "CosmoVis: An Interactive Visual Analysis Tool for Exploring Hydrodynamic Cosmological Simulations",
                        "contributors": [
                            "Angus G. Forbes",
                            "David Abramov"
                        ],
                        "authors": [
                            "David Abramov",
                            "Joseph N. Burchett",
                            "Oskar Elek",
                            "Cameron Hummels",
                            "J. Xavier Prochaska",
                            "Angus G. Forbes"
                        ],
                        "abstract": "We introduce CosmoVis, an open source web-based visualization tool for the interactive analysis of massive hydrodynamic cosmological simulation data. CosmoVis was designed in close collaboration with astrophysicists to enable researchers and citizen scientists to share and explore these datasets, and to use them to investigate a range of scientific questions. CosmoVis visualizes many key gas, dark matter, and stellar attributes extracted from the source simulations, which typically consist of complex data structures multiple terabytes in size, often requiring extensive data wrangling. CosmoVis introduces a range of features to facilitate real-time analysis of these simulations, including the use of \"virtual skewers,\" simulated analogues of absorption line spectroscopy that act as spectral probes piercing the volume of gaseous cosmic medium. We explain how such synthetic spectra can be used to gain insight into the source datasets and to make functional comparisons with observational data. Furthermore, we identify the main analysis tasks that CosmoVis enables and present implementation details of the software interface and the client-server architecture. We conclude by providing details of three contemporary scientific use cases that were conducted by domain experts using the software and by documenting expert feedback from astrophysicists at different career levels.",
                        "uid": "v-tvcg-9736650",
                        "file_name": "",
                        "time_stamp": "2022-10-21T14:12:00Z",
                        "time_start": "2022-10-21T14:12:00Z",
                        "time_end": "2022-10-21T14:22:00Z",
                        "paper_type": "full",
                        "keywords": "Astrovis, astrographics, cosmological simulations, astronomy, astrophysics, virtual spectrography",
                        "has_image": "1",
                        "has_video": "139",
                        "paper_award": "",
                        "image_caption": "Cosmological simulations are rendered volumetrically within the CosmoVis software, illustrating the various types of properties within them. Here, a user interactively places a \u201cvirtual skewer\u201d within the EAGLE 12 Mpc simulation, as shown in the leftmost panel (temperature). Skewers are used to generate absorption line spectra, ion column densities, and other properties. The remaining three panels display gas density, temperature, and metallicity fields within the cross-section from the first panel, where the same skewer runs through each panel. Plots displaying thermodynamical properties of the gas intercepted by the skewer are shown in the lower-right corner of the last three panels.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-tvcg-9736650-qa",
                        "session_id": "full6",
                        "type": "In Person Q+A",
                        "title": "CosmoVis: An Interactive Visual Analysis Tool for Exploring Hydrodynamic Cosmological Simulations (Q+A)",
                        "contributors": [
                            "Angus G. Forbes",
                            "David Abramov"
                        ],
                        "authors": [],
                        "abstract": "We introduce CosmoVis, an open source web-based visualization tool for the interactive analysis of massive hydrodynamic cosmological simulation data. CosmoVis was designed in close collaboration with astrophysicists to enable researchers and citizen scientists to share and explore these datasets, and to use them to investigate a range of scientific questions. CosmoVis visualizes many key gas, dark matter, and stellar attributes extracted from the source simulations, which typically consist of complex data structures multiple terabytes in size, often requiring extensive data wrangling. CosmoVis introduces a range of features to facilitate real-time analysis of these simulations, including the use of \"virtual skewers,\" simulated analogues of absorption line spectroscopy that act as spectral probes piercing the volume of gaseous cosmic medium. We explain how such synthetic spectra can be used to gain insight into the source datasets and to make functional comparisons with observational data. Furthermore, we identify the main analysis tasks that CosmoVis enables and present implementation details of the software interface and the client-server architecture. We conclude by providing details of three contemporary scientific use cases that were conducted by domain experts using the software and by documenting expert feedback from astrophysicists at different career levels.",
                        "uid": "v-tvcg-9736650",
                        "file_name": "",
                        "time_stamp": "2022-10-21T14:22:00Z",
                        "time_start": "2022-10-21T14:22:00Z",
                        "time_end": "2022-10-21T14:24:00Z",
                        "paper_type": "full",
                        "keywords": "Astrovis, astrographics, cosmological simulations, astronomy, astrophysics, virtual spectrography",
                        "has_image": "1",
                        "has_video": "139",
                        "paper_award": "",
                        "image_caption": "Cosmological simulations are rendered volumetrically within the CosmoVis software, illustrating the various types of properties within them. Here, a user interactively places a \u201cvirtual skewer\u201d within the EAGLE 12 Mpc simulation, as shown in the leftmost panel (temperature). Skewers are used to generate absorption line spectra, ion column densities, and other properties. The remaining three panels display gas density, temperature, and metallicity fields within the cross-section from the first panel, where the same skewer runs through each panel. Plots displaying thermodynamical properties of the gas intercepted by the skewer are shown in the lower-right corner of the last three panels.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-tvcg-9599597-pres",
                        "session_id": "full6",
                        "type": "Virtual Presentation (pre-recorded)",
                        "title": "Dynamic Mode Decomposition for Large-Scale Coherent Structure Extraction in Shear Flows",
                        "contributors": [
                            "Duong Nguyen"
                        ],
                        "authors": [
                            "Duong Nguyen",
                            "Panruo Wu",
                            "Rodolfo Ostilla Monico",
                            "Guoning Chen"
                        ],
                        "abstract": "Large-scale structures have been observed in many shear flows which are the fluid generated between two surfaces moving with different velocity. A better understanding of the physics of the structures (especially large-scale structures) in shear flows will help explain a diverse range of physical phenomena and improve our capability of modeling more complex turbulence flows. Many efforts have been made in order to capture such structures; however, conventional methods have their limitations, such as arbitrariness in parameter choice or specificity to certain setups. To address this challenge, we propose to use Multi-Resolution Dynamic Mode Decomposition (mrDMD), for large-scale structure extraction in shear flows. In particular, we show that the slow-motion DMD modes are able to reveal large-scale structures in shear flows that also have slow dynamics. In most cases, we find that the slowest DMD mode and its reconstructed flow can sufficiently capture the large-scale dynamics in the shear flows, which leads to a parameter-free strategy for large-scale structure extraction. Effective visualization of the large-scale structures can then be produced with the aid of the slowest DMD mode. To speed up the computation of mrDMD, we provide a fast GPU-based implementation. We also apply our method to some non-shear flows that need not behave quasi-linearly to demonstrate the limitation of our strategy of using the slowest DMD mode. For non-shear flows, we show that multiple modes from different levels of mrDMD may be needed to sufficiently characterize the flow behavior.",
                        "uid": "v-tvcg-9599597",
                        "file_name": "",
                        "time_stamp": "2022-10-21T14:24:00Z",
                        "time_start": "2022-10-21T14:24:00Z",
                        "time_end": "2022-10-21T14:34:00Z",
                        "paper_type": "full",
                        "keywords": "Flow visualization, Shear Flows, Dynamic Mode Decomposition",
                        "has_image": "1",
                        "has_video": "690",
                        "paper_award": "",
                        "image_caption": "Large-scale coherent structures extraction for shear flows based on Dynamic Mode Decomposition",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-tvcg-9599597-qa",
                        "session_id": "full6",
                        "type": "Virtual Q+A",
                        "title": "Dynamic Mode Decomposition for Large-Scale Coherent Structure Extraction in Shear Flows (Q+A)",
                        "contributors": [
                            "Duong Nguyen"
                        ],
                        "authors": [],
                        "abstract": "Large-scale structures have been observed in many shear flows which are the fluid generated between two surfaces moving with different velocity. A better understanding of the physics of the structures (especially large-scale structures) in shear flows will help explain a diverse range of physical phenomena and improve our capability of modeling more complex turbulence flows. Many efforts have been made in order to capture such structures; however, conventional methods have their limitations, such as arbitrariness in parameter choice or specificity to certain setups. To address this challenge, we propose to use Multi-Resolution Dynamic Mode Decomposition (mrDMD), for large-scale structure extraction in shear flows. In particular, we show that the slow-motion DMD modes are able to reveal large-scale structures in shear flows that also have slow dynamics. In most cases, we find that the slowest DMD mode and its reconstructed flow can sufficiently capture the large-scale dynamics in the shear flows, which leads to a parameter-free strategy for large-scale structure extraction. Effective visualization of the large-scale structures can then be produced with the aid of the slowest DMD mode. To speed up the computation of mrDMD, we provide a fast GPU-based implementation. We also apply our method to some non-shear flows that need not behave quasi-linearly to demonstrate the limitation of our strategy of using the slowest DMD mode. For non-shear flows, we show that multiple modes from different levels of mrDMD may be needed to sufficiently characterize the flow behavior.",
                        "uid": "v-tvcg-9599597",
                        "file_name": "",
                        "time_stamp": "2022-10-21T14:34:00Z",
                        "time_start": "2022-10-21T14:34:00Z",
                        "time_end": "2022-10-21T14:36:00Z",
                        "paper_type": "full",
                        "keywords": "Flow visualization, Shear Flows, Dynamic Mode Decomposition",
                        "has_image": "1",
                        "has_video": "690",
                        "paper_award": "",
                        "image_caption": "Large-scale coherent structures extraction for shear flows based on Dynamic Mode Decomposition",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-full-1239-pres",
                        "session_id": "full6",
                        "type": "Virtual Presentation (pre-recorded)",
                        "title": "A Comparison of Spatiotemporal Visualizations for 3D Urban Analytics",
                        "contributors": [
                            "Roberta Mota",
                            "Roberta Cabral Ramos Mota"
                        ],
                        "authors": [
                            "Roberta Mota",
                            "Fabio Miranda",
                            "Julio Daniel Silva",
                            "Marius Horga",
                            "Marcos Lage",
                            "Luis Ceferino",
                            "Usman Raza Alim",
                            "Ehud Sharlin",
                            "Nivan Ferreira"
                        ],
                        "abstract": "",
                        "uid": "v-full-1239",
                        "file_name": "",
                        "time_stamp": "2022-10-21T14:36:00Z",
                        "time_start": "2022-10-21T14:36:00Z",
                        "time_end": "2022-10-21T14:46:00Z",
                        "paper_type": "full",
                        "keywords": "",
                        "has_image": "1",
                        "has_video": "617",
                        "paper_award": "",
                        "image_caption": "The four spatiotemporal visualizations compared in our study. From left to right: Embedded View, Linked View, Temporal Juxtaposition, and Spatial Juxtaposition. In this example, the visualizations encode the values of an artificially-created temporal attribute of the building facade with four time steps. In each visualization, three rectangular regions with colored outlines are placed along the building surface. When comparing their attribute values, the blue region has the minimum average attribute value between [t1,t2] time steps, whereas the red and black regions have the maximum average values in t3 and t4, respectively.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-full-1239-qa",
                        "session_id": "full6",
                        "type": "Virtual Q+A",
                        "title": "A Comparison of Spatiotemporal Visualizations for 3D Urban Analytics (Q+A)",
                        "contributors": [
                            "Roberta Mota",
                            "Roberta Cabral Ramos Mota"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "v-full-1239",
                        "file_name": "",
                        "time_stamp": "2022-10-21T14:46:00Z",
                        "time_start": "2022-10-21T14:46:00Z",
                        "time_end": "2022-10-21T14:48:00Z",
                        "paper_type": "full",
                        "keywords": "",
                        "has_image": "1",
                        "has_video": "617",
                        "paper_award": "",
                        "image_caption": "The four spatiotemporal visualizations compared in our study. From left to right: Embedded View, Linked View, Temporal Juxtaposition, and Spatial Juxtaposition. In this example, the visualizations encode the values of an artificially-created temporal attribute of the building facade with four time steps. In each visualization, three rectangular regions with colored outlines are placed along the building surface. When comparing their attribute values, the blue region has the minimum average attribute value between [t1,t2] time steps, whereas the red and black regions have the maximum average values in t3 and t4, respectively.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-tvcg-9750868-pres",
                        "session_id": "full6",
                        "type": "In Person Presentation",
                        "title": "EpiMob: Interactive Visual Analytics of Citywide Human Mobility Restrictions for Epidemic Control",
                        "contributors": [
                            "Chuang Yang"
                        ],
                        "authors": [
                            "Chuang Yang",
                            "Zhiwen Zhang",
                            "Zipei Fan",
                            "Renhe Jiang",
                            "Quanjun Chen",
                            "Xuan Song",
                            "Ryosuke Shibasaki."
                        ],
                        "abstract": "The outbreak of coronavirus disease (COVID-19) has swept across more than 180 countries and territories since late January 2020. As a worldwide emergency response, governments have implemented various measures and policies, such as self-quarantine, travel restrictions, work from home, and regional lockdown, to control the spread of the epidemic. These countermeasures seek to restrict human mobility because COVID-19 is a highly contagious disease that is spread by human-to-human transmission. Medical experts and policymakers have expressed the urgency to effectively evaluate the outcome of human restriction policies with the aid of big data and information technology. Thus, based on big human mobility data and city POI data, an interactive visual analytics system called Epidemic Mobility (EpiMob) was designed in this study. The system interactively simulates the changes in human mobility and infection status in response to the implementation of a certain restriction policy or a combination of policies (e.g., regional lockdown, telecommuting, screening). Users can conveniently designate the spatial and temporal ranges for different mobility restriction policies. Then, the results reflecting the infection situation under different policies are dynamically displayed and can be flexibly compared and analyzed in depth. Multiple case studies consisting of interviews with domain experts were conducted in the largest metropolitan area of Japan (i.e., Greater Tokyo Area) to demonstrate that the system can provide insight into the effects of different human mobility restriction policies for epidemic control, through measurements and comparisons.",
                        "uid": "v-tvcg-9750868",
                        "file_name": "",
                        "time_stamp": "2022-10-21T14:48:00Z",
                        "time_start": "2022-10-21T14:48:00Z",
                        "time_end": "2022-10-21T14:58:00Z",
                        "paper_type": "full",
                        "keywords": "Human mobility simulation, epidemic control, visual analytics, interactive system, big trajectory data",
                        "has_image": "1",
                        "has_video": "603",
                        "paper_award": "",
                        "image_caption": "EpiMob\u2013an interactive visual analytics system for simulating and evaluating the effects of mobility restriction policies for epidemic control. In Panel A the user is enabled to specify the mobility restriction policies, including A1\u2014regional lockdown, A2\u2014screening, and A3\u2014telecommuting, and can also set the essential epidemic parameters to adapt to different diseases and local environments (A4, A5). The simulation results are listed in the overview panel B. By clicking the in-depth analysis button of a simulation result, users can further analyze its spatial propagation features (panel D). The user can also perform a comparative analysis (panel C).",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-tvcg-9750868-qa",
                        "session_id": "full6",
                        "type": "In Person Q+A",
                        "title": "EpiMob: Interactive Visual Analytics of Citywide Human Mobility Restrictions for Epidemic Control (Q+A)",
                        "contributors": [
                            "Chuang Yang"
                        ],
                        "authors": [],
                        "abstract": "The outbreak of coronavirus disease (COVID-19) has swept across more than 180 countries and territories since late January 2020. As a worldwide emergency response, governments have implemented various measures and policies, such as self-quarantine, travel restrictions, work from home, and regional lockdown, to control the spread of the epidemic. These countermeasures seek to restrict human mobility because COVID-19 is a highly contagious disease that is spread by human-to-human transmission. Medical experts and policymakers have expressed the urgency to effectively evaluate the outcome of human restriction policies with the aid of big data and information technology. Thus, based on big human mobility data and city POI data, an interactive visual analytics system called Epidemic Mobility (EpiMob) was designed in this study. The system interactively simulates the changes in human mobility and infection status in response to the implementation of a certain restriction policy or a combination of policies (e.g., regional lockdown, telecommuting, screening). Users can conveniently designate the spatial and temporal ranges for different mobility restriction policies. Then, the results reflecting the infection situation under different policies are dynamically displayed and can be flexibly compared and analyzed in depth. Multiple case studies consisting of interviews with domain experts were conducted in the largest metropolitan area of Japan (i.e., Greater Tokyo Area) to demonstrate that the system can provide insight into the effects of different human mobility restriction policies for epidemic control, through measurements and comparisons.",
                        "uid": "v-tvcg-9750868",
                        "file_name": "",
                        "time_stamp": "2022-10-21T14:58:00Z",
                        "time_start": "2022-10-21T14:58:00Z",
                        "time_end": "2022-10-21T15:00:00Z",
                        "paper_type": "full",
                        "keywords": "Human mobility simulation, epidemic control, visual analytics, interactive system, big trajectory data",
                        "has_image": "1",
                        "has_video": "603",
                        "paper_award": "",
                        "image_caption": "EpiMob\u2013an interactive visual analytics system for simulating and evaluating the effects of mobility restriction policies for epidemic control. In Panel A the user is enabled to specify the mobility restriction policies, including A1\u2014regional lockdown, A2\u2014screening, and A3\u2014telecommuting, and can also set the essential epidemic parameters to adapt to different diseases and local environments (A4, A5). The simulation results are listed in the overview panel B. By clicking the in-depth analysis button of a simulation result, users can further analyze its spatial propagation features (panel D). The user can also perform a comparative analysis (panel C).",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-full-1291-pres",
                        "session_id": "full6",
                        "type": "Virtual Presentation (pre-recorded)",
                        "title": "On-Tube Attribute Visualization for Multivariate Trajectory Data",
                        "contributors": [
                            "Benjamin Russig"
                        ],
                        "authors": [
                            "Benjamin Russig",
                            "David Gro\u00df",
                            "Raimund Dachselt",
                            "Stefan Gumhold"
                        ],
                        "abstract": "",
                        "uid": "v-full-1291",
                        "file_name": "",
                        "time_stamp": "2022-10-21T15:00:00Z",
                        "time_start": "2022-10-21T15:00:00Z",
                        "time_end": "2022-10-21T15:10:00Z",
                        "paper_type": "full",
                        "keywords": "",
                        "has_image": "1",
                        "has_video": "655",
                        "paper_award": "",
                        "image_caption": "Various on-tube visualizations of vehicle GPS trajectories (top row) and stream lines (bottom row), visualizing many additional attributes right at the spatial locations they were sampled at. Procedural visualization primitives visible in these examples include circle, triangle, rectangle and sign glyphs, line plots and star coordinates, as well as a normal-mapped grid aiding perception of relative sizes.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-full-1291-qa",
                        "session_id": "full6",
                        "type": "Virtual Q+A",
                        "title": "On-Tube Attribute Visualization for Multivariate Trajectory Data (Q+A)",
                        "contributors": [
                            "Benjamin Russig"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "v-full-1291",
                        "file_name": "",
                        "time_stamp": "2022-10-21T15:10:00Z",
                        "time_start": "2022-10-21T15:10:00Z",
                        "time_end": "2022-10-21T15:12:00Z",
                        "paper_type": "full",
                        "keywords": "",
                        "has_image": "1",
                        "has_video": "655",
                        "paper_award": "",
                        "image_caption": "Various on-tube visualizations of vehicle GPS trajectories (top row) and stream lines (bottom row), visualizing many additional attributes right at the spatial locations they were sampled at. Procedural visualization primitives visible in these examples include circle, triangle, rectangle and sign glyphs, line plots and star coordinates, as well as a normal-mapped grid aiding perception of relative sizes.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    }
                ]
            },
            {
                "title": "Uncertainty",
                "session_id": "full7",
                "event_prefix": "v-full",
                "track": "ok5",
                "livestream_id": "ok5-wed",
                "session_image": "full7.png",
                "chair": [
                    "Kristi Potter"
                ],
                "organizers": [],
                "time_start": "2022-10-19T20:45:00Z",
                "time_end": "2022-10-19T22:00:00Z",
                "discord_category": "",
                "discord_channel": "",
                "discord_channel_id": "",
                "discord_link": "",
                "slido_link": "",
                "youtube_url": "",
                "zoom_meeting": "",
                "zoom_password": "",
                "zoom_link": "",
                "ff_link": "",
                "ff_playlist": "",
                "time_slots": [
                    {
                        "slot_id": "full7-opening",
                        "session_id": "full7",
                        "type": "In Person Other",
                        "title": "Session Opening",
                        "contributors": [
                            "Kristi Potter"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "",
                        "file_name": "",
                        "time_stamp": "2022-10-19T20:45:00Z",
                        "time_start": "2022-10-19T20:45:00Z",
                        "time_end": "2022-10-19T20:45:00Z",
                        "paper_type": "",
                        "keywords": "",
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-full-1307-pres",
                        "session_id": "full7",
                        "type": "Virtual Presentation (pre-recorded)",
                        "title": "Fiber Uncertainty Visualization for Bivariate Data With Parametric and Nonparametric Noise Models",
                        "contributors": [
                            "Dr. Tushar M. Athawale"
                        ],
                        "authors": [
                            "Tushar M. Athawale",
                            "Chris R. Johnson",
                            "Sudhanshu Sane",
                            "David Pugmire"
                        ],
                        "abstract": "",
                        "uid": "v-full-1307",
                        "file_name": "",
                        "time_stamp": "2022-10-19T20:45:00Z",
                        "time_start": "2022-10-19T20:45:00Z",
                        "time_end": "2022-10-19T20:55:00Z",
                        "paper_type": "full",
                        "keywords": "",
                        "has_image": "1",
                        "has_video": "660",
                        "paper_award": "",
                        "image_caption": "Mean, parametric, and nonparametric noise models for uncertainty analysis of vortical features of the Red Sea ensemble dataset. Fiber positions are visualized for a trait corresponding to anticyclonic (negative Z component of curl) vortices, as indicated by the cyan polygon in image (a). The parametric noise model (c) suggests multiple high-probability vortices inside the magenta box, whereas the mean (b) and nonparametric (d) statistical models suggest a relatively low number of vortices inside the magenta box. Such inconsistency inside the magenta box across the three statistical models indicates the need for further eddy analysis in the same region.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-full-1307-qa",
                        "session_id": "full7",
                        "type": "Virtual Q+A",
                        "title": "Fiber Uncertainty Visualization for Bivariate Data With Parametric and Nonparametric Noise Models (Q+A)",
                        "contributors": [
                            "Dr. Tushar M. Athawale"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "v-full-1307",
                        "file_name": "",
                        "time_stamp": "2022-10-19T20:55:00Z",
                        "time_start": "2022-10-19T20:55:00Z",
                        "time_end": "2022-10-19T20:57:00Z",
                        "paper_type": "full",
                        "keywords": "",
                        "has_image": "1",
                        "has_video": "660",
                        "paper_award": "",
                        "image_caption": "Mean, parametric, and nonparametric noise models for uncertainty analysis of vortical features of the Red Sea ensemble dataset. Fiber positions are visualized for a trait corresponding to anticyclonic (negative Z component of curl) vortices, as indicated by the cyan polygon in image (a). The parametric noise model (c) suggests multiple high-probability vortices inside the magenta box, whereas the mean (b) and nonparametric (d) statistical models suggest a relatively low number of vortices inside the magenta box. Such inconsistency inside the magenta box across the three statistical models indicates the need for further eddy analysis in the same region.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-full-1121-pres",
                        "session_id": "full7",
                        "type": "Virtual Presentation (pre-recorded)",
                        "title": "Evaluating the Use of Uncertainty Visualisations for Imputations of Missing Values in Scatterplots",
                        "contributors": [
                            "Abhraneel Sarma"
                        ],
                        "authors": [
                            "Abhraneel Sarma",
                            "Shunan Guo",
                            "Jane Hoffswell",
                            "Ryan Rossi",
                            "Fan Du",
                            "Eunyee Koh",
                            "Matthew Kay"
                        ],
                        "abstract": "",
                        "uid": "v-full-1121",
                        "file_name": "",
                        "time_stamp": "2022-10-19T20:57:00Z",
                        "time_start": "2022-10-19T20:57:00Z",
                        "time_end": "2022-10-19T21:07:00Z",
                        "paper_type": "full",
                        "keywords": "",
                        "has_image": "1",
                        "has_video": "542",
                        "paper_award": "",
                        "image_caption": "Datasets can often have missing values (A). Not accounting for missing values of a dataset can lead to incorrect conclusions (B). For example, the figure on the middle-left shows how the trend line can vary when considering only observed data compared to the actual ground truth trend line (which is unknowable). However, imputing missing values can provide the necessary information required for correct inference (C), as the estimate with the imputed dataset is close to the ground truth estimate. In this study, we explore how different ways of representing imputations of a dataset with missing values (D) impact analysts performance on two visual analysis tasks (E)?",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-full-1121-qa",
                        "session_id": "full7",
                        "type": "Virtual Q+A",
                        "title": "Evaluating the Use of Uncertainty Visualisations for Imputations of Missing Values in Scatterplots (Q+A)",
                        "contributors": [
                            "Abhraneel Sarma"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "v-full-1121",
                        "file_name": "",
                        "time_stamp": "2022-10-19T21:07:00Z",
                        "time_start": "2022-10-19T21:07:00Z",
                        "time_end": "2022-10-19T21:09:00Z",
                        "paper_type": "full",
                        "keywords": "",
                        "has_image": "1",
                        "has_video": "542",
                        "paper_award": "",
                        "image_caption": "Datasets can often have missing values (A). Not accounting for missing values of a dataset can lead to incorrect conclusions (B). For example, the figure on the middle-left shows how the trend line can vary when considering only observed data compared to the actual ground truth trend line (which is unknowable). However, imputing missing values can provide the necessary information required for correct inference (C), as the estimate with the imputed dataset is close to the ground truth estimate. In this study, we explore how different ways of representing imputations of a dataset with missing values (D) impact analysts performance on two visual analysis tasks (E)?",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-full-1062-pres",
                        "session_id": "full7",
                        "type": "In Person Presentation",
                        "title": "Dispersion vs Disparity: Hiding Uncertainty Can Encourage Stereotyping When Visualizing Social Outcomes",
                        "contributors": [
                            "Cindy Xiong",
                            "Eli Holder"
                        ],
                        "authors": [
                            "Eli Holder",
                            "Cindy Xiong"
                        ],
                        "abstract": "",
                        "uid": "v-full-1062",
                        "file_name": "",
                        "time_stamp": "2022-10-19T21:09:00Z",
                        "time_start": "2022-10-19T21:09:00Z",
                        "time_end": "2022-10-19T21:19:00Z",
                        "paper_type": "full",
                        "keywords": "",
                        "has_image": "1",
                        "has_video": "675",
                        "paper_award": "",
                        "image_caption": "In the United States, social outcomes like health, wealth, and education vary widely between\u00a0race, gender, and other groups.\nVisualizing disparities can help raise awareness.\nBut conventional chart design\u00a0choices can actually reinforce harmful stereotypes about\u00a0the people being visualized.\nThrough a series of experiments, we\u00a0find that charts that hide variability increase stereotyping, whereas charts that\u00a0emphasize variability decrease stereotyping.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-full-1062-qa",
                        "session_id": "full7",
                        "type": "In Person Q+A",
                        "title": "Dispersion vs Disparity: Hiding Uncertainty Can Encourage Stereotyping When Visualizing Social Outcomes (Q+A)",
                        "contributors": [
                            "Cindy Xiong",
                            "Eli Holder"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "v-full-1062",
                        "file_name": "",
                        "time_stamp": "2022-10-19T21:19:00Z",
                        "time_start": "2022-10-19T21:19:00Z",
                        "time_end": "2022-10-19T21:21:00Z",
                        "paper_type": "full",
                        "keywords": "",
                        "has_image": "1",
                        "has_video": "675",
                        "paper_award": "",
                        "image_caption": "In the United States, social outcomes like health, wealth, and education vary widely between\u00a0race, gender, and other groups.\nVisualizing disparities can help raise awareness.\nBut conventional chart design\u00a0choices can actually reinforce harmful stereotypes about\u00a0the people being visualized.\nThrough a series of experiments, we\u00a0find that charts that hide variability increase stereotyping, whereas charts that\u00a0emphasize variability decrease stereotyping.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-full-1269-pres",
                        "session_id": "full7",
                        "type": "In Person Presentation",
                        "title": "Communicating Uncertainty in Digital Humanities Visualization Research",
                        "contributors": [
                            "Georgia Panagiotidou"
                        ],
                        "authors": [
                            "Georgia Panagiotidou",
                            "Houda Lamqaddam",
                            "Jeroen Poblome",
                            "Koenraad Brosens",
                            "Katrien Verbert",
                            "Andrew Vande Moere"
                        ],
                        "abstract": "",
                        "uid": "v-full-1269",
                        "file_name": "",
                        "time_stamp": "2022-10-19T21:21:00Z",
                        "time_start": "2022-10-19T21:21:00Z",
                        "time_end": "2022-10-19T21:31:00Z",
                        "paper_type": "full",
                        "keywords": "",
                        "has_image": "1",
                        "has_video": "524",
                        "paper_award": "",
                        "image_caption": "By mapping how visualizations in the digital humanities encoded uncertainty, we identified four approaches that varied in terms of explicitness and customization.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-full-1269-qa",
                        "session_id": "full7",
                        "type": "In Person Q+A",
                        "title": "Communicating Uncertainty in Digital Humanities Visualization Research (Q+A)",
                        "contributors": [
                            "Georgia Panagiotidou"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "v-full-1269",
                        "file_name": "",
                        "time_stamp": "2022-10-19T21:31:00Z",
                        "time_start": "2022-10-19T21:31:00Z",
                        "time_end": "2022-10-19T21:33:00Z",
                        "paper_type": "full",
                        "keywords": "",
                        "has_image": "1",
                        "has_video": "524",
                        "paper_award": "",
                        "image_caption": "By mapping how visualizations in the digital humanities encoded uncertainty, we identified four approaches that varied in terms of explicitness and customization.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-tvcg-9566799-pres",
                        "session_id": "full7",
                        "type": "In Person Presentation",
                        "title": "Fuzzy Spreadsheet: Understanding and Exploring Uncertainties in Tabular Calculations",
                        "contributors": [
                            "Vaishali Dhanoa"
                        ],
                        "authors": [
                            "Vaishali Dhanoa",
                            "Conny Walchshofer",
                            "Andreas Hinterreiter",
                            "Eduard Gr\u00f6ller",
                            "Marc Streit"
                        ],
                        "abstract": "Spreadsheet-based tools provide a simple yet effective way of calculating values, which makes them the number-one choice for building and formalizing simple models for budget planning and many other applications. A cell in a spreadsheet holds one specific value and gives a discrete, over precise view of the underlying model. Therefore, spreadsheets are of limited use when investigating the inherent uncertainties of such models and answering what-if questions. Existing extensions typically require a complex modeling process that cannot easily be embedded in a tabular layout. In Fuzzy Spreadsheet, a cell can hold and display a distribution of values. This integrated uncertainty-handling immediately conveys sensitivity and robustness information. The fuzzification of the cells enables calculations not only with precise values but also with distributions, and probabilities. We conservatively added and carefully crafted visuals to maintain the look and feel of a traditional spreadsheet while facilitating what-if analyses. Given a user-specified reference cell, Fuzzy Spreadsheet automatically extracts and visualizes contextually relevant information, such as impact, uncertainty, and degree of neighborhood, for the selected and related cells. To evaluate its usability and the perceived mental effort required, we conducted a user study. The results show that our approach outperforms traditional spreadsheets in terms of answer correctness, response time, and perceived mental effort in almost all tasks tested.",
                        "uid": "v-tvcg-9566799",
                        "file_name": "",
                        "time_stamp": "2022-10-19T21:33:00Z",
                        "time_start": "2022-10-19T21:33:00Z",
                        "time_end": "2022-10-19T21:43:00Z",
                        "paper_type": "full",
                        "keywords": "Uncertainty visualization, tabular data, spreadsheet augmentation",
                        "has_image": "1",
                        "has_video": "533",
                        "paper_award": "",
                        "image_caption": "Conference  planning  usage  scenario  with  Fuzzy  Spreadsheet  encodings.  The  user  selects  as  a  reference  cell  (A)  and  controls  thevisualization using the side panel. In the what-if analysis mode, they reduce the value of Catering 2021 to zero (B) and analyze its effect onthe focus cell (C). More detailed information for the focus cell is shown in the side panel (right)",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-tvcg-9566799-qa",
                        "session_id": "full7",
                        "type": "In Person Q+A",
                        "title": "Fuzzy Spreadsheet: Understanding and Exploring Uncertainties in Tabular Calculations (Q+A)",
                        "contributors": [
                            "Vaishali Dhanoa"
                        ],
                        "authors": [],
                        "abstract": "Spreadsheet-based tools provide a simple yet effective way of calculating values, which makes them the number-one choice for building and formalizing simple models for budget planning and many other applications. A cell in a spreadsheet holds one specific value and gives a discrete, over precise view of the underlying model. Therefore, spreadsheets are of limited use when investigating the inherent uncertainties of such models and answering what-if questions. Existing extensions typically require a complex modeling process that cannot easily be embedded in a tabular layout. In Fuzzy Spreadsheet, a cell can hold and display a distribution of values. This integrated uncertainty-handling immediately conveys sensitivity and robustness information. The fuzzification of the cells enables calculations not only with precise values but also with distributions, and probabilities. We conservatively added and carefully crafted visuals to maintain the look and feel of a traditional spreadsheet while facilitating what-if analyses. Given a user-specified reference cell, Fuzzy Spreadsheet automatically extracts and visualizes contextually relevant information, such as impact, uncertainty, and degree of neighborhood, for the selected and related cells. To evaluate its usability and the perceived mental effort required, we conducted a user study. The results show that our approach outperforms traditional spreadsheets in terms of answer correctness, response time, and perceived mental effort in almost all tasks tested.",
                        "uid": "v-tvcg-9566799",
                        "file_name": "",
                        "time_stamp": "2022-10-19T21:43:00Z",
                        "time_start": "2022-10-19T21:43:00Z",
                        "time_end": "2022-10-19T21:45:00Z",
                        "paper_type": "full",
                        "keywords": "Uncertainty visualization, tabular data, spreadsheet augmentation",
                        "has_image": "1",
                        "has_video": "533",
                        "paper_award": "",
                        "image_caption": "Conference  planning  usage  scenario  with  Fuzzy  Spreadsheet  encodings.  The  user  selects  as  a  reference  cell  (A)  and  controls  thevisualization using the side panel. In the what-if analysis mode, they reduce the value of Catering 2021 to zero (B) and analyze its effect onthe focus cell (C). More detailed information for the focus cell is shown in the side panel (right)",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    }
                ]
            },
            {
                "title": "Understanding and Modeling How People Respond to Visualizations",
                "session_id": "full8",
                "event_prefix": "v-full",
                "track": "ok6",
                "livestream_id": "ok6-wed",
                "session_image": "full8.png",
                "chair": [
                    "Carolina Nobre"
                ],
                "organizers": [],
                "time_start": "2022-10-19T15:45:00Z",
                "time_end": "2022-10-19T17:00:00Z",
                "discord_category": "",
                "discord_channel": "",
                "discord_channel_id": "",
                "discord_link": "",
                "slido_link": "",
                "youtube_url": "",
                "zoom_meeting": "",
                "zoom_password": "",
                "zoom_link": "",
                "ff_link": "",
                "ff_playlist": "",
                "time_slots": [
                    {
                        "slot_id": "full8-opening",
                        "session_id": "full8",
                        "type": "In Person Other",
                        "title": "Session Opening",
                        "contributors": [
                            "Carolina Nobre"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "",
                        "file_name": "",
                        "time_stamp": "2022-10-19T15:45:00Z",
                        "time_start": "2022-10-19T15:45:00Z",
                        "time_end": "2022-10-19T15:45:00Z",
                        "paper_type": "",
                        "keywords": "",
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-tvcg-9492011-pres",
                        "session_id": "full8",
                        "type": "In Person Presentation",
                        "title": "A Survey of Perception-Based Visualization Studies by Task",
                        "contributors": [
                            "Ghulam Jilani Quadri"
                        ],
                        "authors": [
                            "Ghulam Jilani Quadri",
                            "Paul Rosen"
                        ],
                        "abstract": "Knowledge of human perception has long been incorporated into visualizations to enhance their quality and effectiveness. The last decade, in particular, has shown an increase in perception-based visualization research studies. With all of this recent progress, the visualization community lacks a comprehensive guide to contextualize their results. In this report, we provide a systematic and comprehensive review of research studies on perception related to visualization. This survey reviews perception-focused visualization studies since 1980 and summarizes their research developments focusing on low-level tasks, further breaking techniques down by visual encoding and visualization type. In particular, we focus on how perception is used to evaluate the effectiveness of visualizations, to help readers understand and apply the principles of perception of their visualization designs through a task-optimized approach. We concluded our report with a summary of the weaknesses and open research questions in the area.",
                        "uid": "v-tvcg-9492011",
                        "file_name": "",
                        "time_stamp": "2022-10-19T15:45:00Z",
                        "time_start": "2022-10-19T15:45:00Z",
                        "time_end": "2022-10-19T15:55:00Z",
                        "paper_type": "full",
                        "keywords": "Visualization, perception, graphical perception, visual analytics tasks, evaluation, survey.",
                        "has_image": "1",
                        "has_video": "530",
                        "paper_award": "",
                        "image_caption": "The image shows a web resource as an interactive list of the papers we have reviewed and identified in our survey. The interactive web resources can be utilized to identify and filter the selected perception-based studies on visualization types, visual encoding, low-level visual tasks, authors, and venue. Please check our paper for more detail.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-tvcg-9492011-qa",
                        "session_id": "full8",
                        "type": "In Person Q+A",
                        "title": "A Survey of Perception-Based Visualization Studies by Task (Q+A)",
                        "contributors": [
                            "Ghulam Jilani Quadri"
                        ],
                        "authors": [],
                        "abstract": "Knowledge of human perception has long been incorporated into visualizations to enhance their quality and effectiveness. The last decade, in particular, has shown an increase in perception-based visualization research studies. With all of this recent progress, the visualization community lacks a comprehensive guide to contextualize their results. In this report, we provide a systematic and comprehensive review of research studies on perception related to visualization. This survey reviews perception-focused visualization studies since 1980 and summarizes their research developments focusing on low-level tasks, further breaking techniques down by visual encoding and visualization type. In particular, we focus on how perception is used to evaluate the effectiveness of visualizations, to help readers understand and apply the principles of perception of their visualization designs through a task-optimized approach. We concluded our report with a summary of the weaknesses and open research questions in the area.",
                        "uid": "v-tvcg-9492011",
                        "file_name": "",
                        "time_stamp": "2022-10-19T15:55:00Z",
                        "time_start": "2022-10-19T15:55:00Z",
                        "time_end": "2022-10-19T15:57:00Z",
                        "paper_type": "full",
                        "keywords": "Visualization, perception, graphical perception, visual analytics tasks, evaluation, survey.",
                        "has_image": "1",
                        "has_video": "530",
                        "paper_award": "",
                        "image_caption": "The image shows a web resource as an interactive list of the papers we have reviewed and identified in our survey. The interactive web resources can be utilized to identify and filter the selected perception-based studies on visualization types, visual encoding, low-level visual tasks, authors, and venue. Please check our paper for more detail.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-full-1219-pres",
                        "session_id": "full8",
                        "type": "Virtual Presentation (pre-recorded)",
                        "title": "BeauVis: A Validated Scale for Measuring the Aesthetic Pleasure of Visual Representations",
                        "contributors": [
                            "Tingying He"
                        ],
                        "authors": [
                            "Tingying He",
                            "Petra Isenberg",
                            "Raimund Dachselt",
                            "Tobias Isenberg"
                        ],
                        "abstract": "",
                        "uid": "v-full-1219",
                        "file_name": "",
                        "time_stamp": "2022-10-19T15:57:00Z",
                        "time_start": "2022-10-19T15:57:00Z",
                        "time_end": "2022-10-19T16:07:00Z",
                        "paper_type": "full",
                        "keywords": "",
                        "has_image": "1",
                        "has_video": "530",
                        "paper_award": "",
                        "image_caption": "One participant\u2019s data on our BeauVis scale in its recommended version.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-full-1219-qa",
                        "session_id": "full8",
                        "type": "Virtual Q+A",
                        "title": "BeauVis: A Validated Scale for Measuring the Aesthetic Pleasure of Visual Representations (Q+A)",
                        "contributors": [
                            "Tingying He"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "v-full-1219",
                        "file_name": "",
                        "time_stamp": "2022-10-19T16:07:00Z",
                        "time_start": "2022-10-19T16:07:00Z",
                        "time_end": "2022-10-19T16:09:00Z",
                        "paper_type": "full",
                        "keywords": "",
                        "has_image": "1",
                        "has_video": "530",
                        "paper_award": "",
                        "image_caption": "One participant\u2019s data on our BeauVis scale in its recommended version.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-full-1217-pres",
                        "session_id": "full8",
                        "type": "In Person Presentation",
                        "title": "Photosensitive Accessibility for Interactive Data Visualizations",
                        "contributors": [
                            "Laura South"
                        ],
                        "authors": [
                            "Laura South",
                            "Michelle A. Borkin"
                        ],
                        "abstract": "",
                        "uid": "v-full-1217",
                        "file_name": "",
                        "time_stamp": "2022-10-19T16:09:00Z",
                        "time_start": "2022-10-19T16:09:00Z",
                        "time_end": "2022-10-19T16:19:00Z",
                        "paper_type": "full",
                        "keywords": "",
                        "has_image": "1",
                        "has_video": "557",
                        "paper_award": "",
                        "image_caption": "Three visualizations with navigation (A), filtering (B), and selection (C) interaction mechanisms from our dataset of 375 online D3 visualizations annotated for photosensitive accessibility. Each of the three visualizations in this figure are inaccessible because they are capable of producing flickering sequences that could induce seizures when viewed by people with photosensitive epilepsy.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-full-1217-qa",
                        "session_id": "full8",
                        "type": "In Person Q+A",
                        "title": "Photosensitive Accessibility for Interactive Data Visualizations (Q+A)",
                        "contributors": [
                            "Laura South"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "v-full-1217",
                        "file_name": "",
                        "time_stamp": "2022-10-19T16:19:00Z",
                        "time_start": "2022-10-19T16:19:00Z",
                        "time_end": "2022-10-19T16:21:00Z",
                        "paper_type": "full",
                        "keywords": "",
                        "has_image": "1",
                        "has_video": "557",
                        "paper_award": "",
                        "image_caption": "Three visualizations with navigation (A), filtering (B), and selection (C) interaction mechanisms from our dataset of 375 online D3 visualizations annotated for photosensitive accessibility. Each of the three visualizations in this figure are inaccessible because they are capable of producing flickering sequences that could induce seizures when viewed by people with photosensitive epilepsy.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-full-1115-pres",
                        "session_id": "full8",
                        "type": "In Person Presentation",
                        "title": "Extending assignment inference to colormap data visualizations",
                        "contributors": [
                            "Melissa A. Schoenlein"
                        ],
                        "authors": [
                            "Melissa A. Schoenlein",
                            "Johnny Campos",
                            "Kevin Lande",
                            "Laurent Lessard",
                            "Karen Schloss"
                        ],
                        "abstract": "",
                        "uid": "v-full-1115",
                        "file_name": "",
                        "time_stamp": "2022-10-19T16:21:00Z",
                        "time_start": "2022-10-19T16:21:00Z",
                        "time_end": "2022-10-19T16:31:00Z",
                        "paper_type": "full",
                        "keywords": "",
                        "has_image": "1",
                        "has_video": "654",
                        "paper_award": "",
                        "image_caption": "In this study, participants inferred which region of colormaps (left/right) represented more of a domain concept (e.g., ocean water). Inferences can be predicted by simulating assignment inference using a weighted combination of multiple (sometimes competing) sources of \u201cmerit\u201d: direct associations and relational associations (dark-is-more bias). This study bridges our understanding how direct and relational associations combine to influence inferences about the meanings of colors in visualizations. Our findings can be translated directly to design visualizations that align with people\u2019s expectations about the meanings of colors, thereby facilitating visual communication.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-full-1115-qa",
                        "session_id": "full8",
                        "type": "In Person Q+A",
                        "title": "Extending assignment inference to colormap data visualizations (Q+A)",
                        "contributors": [
                            "Melissa A. Schoenlein"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "v-full-1115",
                        "file_name": "",
                        "time_stamp": "2022-10-19T16:31:00Z",
                        "time_start": "2022-10-19T16:31:00Z",
                        "time_end": "2022-10-19T16:33:00Z",
                        "paper_type": "full",
                        "keywords": "",
                        "has_image": "1",
                        "has_video": "654",
                        "paper_award": "",
                        "image_caption": "In this study, participants inferred which region of colormaps (left/right) represented more of a domain concept (e.g., ocean water). Inferences can be predicted by simulating assignment inference using a weighted combination of multiple (sometimes competing) sources of \u201cmerit\u201d: direct associations and relational associations (dark-is-more bias). This study bridges our understanding how direct and relational associations combine to influence inferences about the meanings of colors in visualizations. Our findings can be translated directly to design visualizations that align with people\u2019s expectations about the meanings of colors, thereby facilitating visual communication.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-full-1448-pres",
                        "session_id": "full8",
                        "type": "In Person Presentation",
                        "title": "A Scanner Deeply: Predicting Human Eye Movement on Visualizations Using Crowdsourced Data Collection",
                        "contributors": [
                            "Sungbok Shin"
                        ],
                        "authors": [
                            "Sungbok Shin",
                            "Sunghyo Chung",
                            "Sanghyun Hong",
                            "Niklas Elmqvist"
                        ],
                        "abstract": "",
                        "uid": "v-full-1448",
                        "file_name": "",
                        "time_stamp": "2022-10-19T16:33:00Z",
                        "time_start": "2022-10-19T16:33:00Z",
                        "time_end": "2022-10-19T16:43:00Z",
                        "paper_type": "full",
                        "keywords": "",
                        "has_image": "1",
                        "has_video": "501",
                        "paper_award": "",
                        "image_caption": "Sungbok Shin is a Ph.D. Student at the dept. of Computer in the University of Maryland, College Park, United States. He gained his B.S. as a dual degree of Computer Science and Engineering and Mathematics and his M.Eng. in Computer Science and Engineering in Korea University, Seoul, South Korea. Sungbok is interested in understanding how existing machine learning models (e.g., neural networks) learn humans' perceptions while looking at visualizations. He is also interested in developing applications that would facilitate and can improve chart designs.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-full-1448-qa",
                        "session_id": "full8",
                        "type": "In Person Q+A",
                        "title": "A Scanner Deeply: Predicting Human Eye Movement on Visualizations Using Crowdsourced Data Collection (Q+A)",
                        "contributors": [
                            "Sungbok Shin"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "v-full-1448",
                        "file_name": "",
                        "time_stamp": "2022-10-19T16:43:00Z",
                        "time_start": "2022-10-19T16:43:00Z",
                        "time_end": "2022-10-19T16:45:00Z",
                        "paper_type": "full",
                        "keywords": "",
                        "has_image": "1",
                        "has_video": "501",
                        "paper_award": "",
                        "image_caption": "Sungbok Shin is a Ph.D. Student at the dept. of Computer in the University of Maryland, College Park, United States. He gained his B.S. as a dual degree of Computer Science and Engineering and Mathematics and his M.Eng. in Computer Science and Engineering in Korea University, Seoul, South Korea. Sungbok is interested in understanding how existing machine learning models (e.g., neural networks) learn humans' perceptions while looking at visualizations. He is also interested in developing applications that would facilitate and can improve chart designs.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-full-1254-pres",
                        "session_id": "full8",
                        "type": "In Person Presentation",
                        "title": "Studying Early Decision Making with Progressive Bar Charts",
                        "contributors": [
                            "Ameya B Patil"
                        ],
                        "authors": [
                            "Ameya B Patil",
                            "Ga\u00eblle Richer",
                            "Dominik Moritz",
                            "Christopher Jermaine",
                            "Jean-Daniel Fekete"
                        ],
                        "abstract": "",
                        "uid": "v-full-1254",
                        "file_name": "",
                        "time_stamp": "2022-10-19T16:45:00Z",
                        "time_start": "2022-10-19T16:45:00Z",
                        "time_end": "2022-10-19T16:55:00Z",
                        "paper_type": "full",
                        "keywords": "",
                        "has_image": "1",
                        "has_video": "447",
                        "paper_award": "",
                        "image_caption": "Four progressive bar chart designs showing the means for four columns of a data table loaded progressively, updated every second: (BASELINE) baseline bar chart, (CI) bar chart with confidence intervals (95%), (HISTORY) bar chart with near-history, and (HISTORY+CI) bar chart with near-history and confidence intervals. All the bar charts represent the same data at one step of the progression. For our proposed new designs (HISTORY & HISTORY+CI), opacity and position along X-axis encode the recency of updates; opaque bars on the right represent more recent updates, and transparent bars on the left represent older updates.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-full-1254-qa",
                        "session_id": "full8",
                        "type": "In Person Q+A",
                        "title": "Studying Early Decision Making with Progressive Bar Charts (Q+A)",
                        "contributors": [
                            "Ameya B Patil"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "v-full-1254",
                        "file_name": "",
                        "time_stamp": "2022-10-19T16:55:00Z",
                        "time_start": "2022-10-19T16:55:00Z",
                        "time_end": "2022-10-19T16:57:00Z",
                        "paper_type": "full",
                        "keywords": "",
                        "has_image": "1",
                        "has_video": "447",
                        "paper_award": "",
                        "image_caption": "Four progressive bar chart designs showing the means for four columns of a data table loaded progressively, updated every second: (BASELINE) baseline bar chart, (CI) bar chart with confidence intervals (95%), (HISTORY) bar chart with near-history, and (HISTORY+CI) bar chart with near-history and confidence intervals. All the bar charts represent the same data at one step of the progression. For our proposed new designs (HISTORY & HISTORY+CI), opacity and position along X-axis encode the recency of updates; opaque bars on the right represent more recent updates, and transparent bars on the left represent older updates.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    }
                ]
            },
            {
                "title": "Interpreting Machine Learning",
                "session_id": "full9",
                "event_prefix": "v-full",
                "track": "ok4",
                "livestream_id": "ok4-thu",
                "session_image": "full9.png",
                "chair": [
                    "Adam Perer"
                ],
                "organizers": [],
                "time_start": "2022-10-20T15:45:00Z",
                "time_end": "2022-10-20T17:00:00Z",
                "discord_category": "",
                "discord_channel": "",
                "discord_channel_id": "",
                "discord_link": "",
                "slido_link": "",
                "youtube_url": "",
                "zoom_meeting": "",
                "zoom_password": "",
                "zoom_link": "",
                "ff_link": "",
                "ff_playlist": "",
                "time_slots": [
                    {
                        "slot_id": "full9-opening",
                        "session_id": "full9",
                        "type": "In Person Other",
                        "title": "Session Opening",
                        "contributors": [
                            "Adam Perer"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "",
                        "file_name": "",
                        "time_stamp": "2022-10-20T15:45:00Z",
                        "time_start": "2022-10-20T15:45:00Z",
                        "time_end": "2022-10-20T15:45:00Z",
                        "paper_type": "",
                        "keywords": "",
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-tvcg-9695246-pres",
                        "session_id": "full9",
                        "type": "In Person Presentation",
                        "title": "StrategyAtlas: Strategy Analysis for Machine Learning Interpretability",
                        "contributors": [
                            "Collaris, Dennis"
                        ],
                        "authors": [
                            "Dennis Collaris",
                            "Jarke J. van Wijk"
                        ],
                        "abstract": "Businesses in high-risk environments have been reluctant to adopt modern machine learning approaches due to their complex and uninterpretable nature. Most current solutions provide local, instance-level explanations, but this is insufficient for understanding the model as a whole. In this work, we show that strategy clusters (i.e., groups of data instances that are treated distinctly by the model) can be used to understand the global behavior of a complex ML model. To support effective exploration and understanding of these clusters, we introduce StrategyAtlas, a system designed to analyze and explain model strategies. Furthermore, it supports multiple ways to utilize these strategies for simplifying and improving the reference model. In collaboration with a large insurance company, we present a use case in automatic insurance acceptance, and show how professional data scientists were enabled to understand a complex model and improve the production model based on these insights.",
                        "uid": "v-tvcg-9695246",
                        "file_name": "",
                        "time_stamp": "2022-10-20T15:45:00Z",
                        "time_start": "2022-10-20T15:45:00Z",
                        "time_end": "2022-10-20T15:55:00Z",
                        "paper_type": "full",
                        "keywords": "Machine learning, Visual analytics, Explainable AI",
                        "has_image": "1",
                        "has_video": "640",
                        "paper_award": "",
                        "image_caption": "StrategyAtlas: a visual analytics system designed to identify and interpret different model strategies, to help data scientists understand complex machine learning models.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-tvcg-9695246-qa",
                        "session_id": "full9",
                        "type": "In Person Q+A",
                        "title": "StrategyAtlas: Strategy Analysis for Machine Learning Interpretability (Q+A)",
                        "contributors": [
                            "Collaris, Dennis"
                        ],
                        "authors": [],
                        "abstract": "Businesses in high-risk environments have been reluctant to adopt modern machine learning approaches due to their complex and uninterpretable nature. Most current solutions provide local, instance-level explanations, but this is insufficient for understanding the model as a whole. In this work, we show that strategy clusters (i.e., groups of data instances that are treated distinctly by the model) can be used to understand the global behavior of a complex ML model. To support effective exploration and understanding of these clusters, we introduce StrategyAtlas, a system designed to analyze and explain model strategies. Furthermore, it supports multiple ways to utilize these strategies for simplifying and improving the reference model. In collaboration with a large insurance company, we present a use case in automatic insurance acceptance, and show how professional data scientists were enabled to understand a complex model and improve the production model based on these insights.",
                        "uid": "v-tvcg-9695246",
                        "file_name": "",
                        "time_stamp": "2022-10-20T15:55:00Z",
                        "time_start": "2022-10-20T15:55:00Z",
                        "time_end": "2022-10-20T15:57:00Z",
                        "paper_type": "full",
                        "keywords": "Machine learning, Visual analytics, Explainable AI",
                        "has_image": "1",
                        "has_video": "640",
                        "paper_award": "",
                        "image_caption": "StrategyAtlas: a visual analytics system designed to identify and interpret different model strategies, to help data scientists understand complex machine learning models.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-full-1696-pres",
                        "session_id": "full9",
                        "type": "In Person Presentation",
                        "title": "VDL-Surrogate: A View-Dependent Latent-based Model for Parameter Space Exploration of Ensemble Simulations",
                        "contributors": [
                            "Neng Shi"
                        ],
                        "authors": [
                            "Neng Shi",
                            "Jiayi Xu",
                            "Haoyu Li",
                            "Hanqi Guo",
                            "Jonathan Woodring",
                            "Han-Wei Shen"
                        ],
                        "abstract": "",
                        "uid": "v-full-1696",
                        "file_name": "",
                        "time_stamp": "2022-10-20T15:57:00Z",
                        "time_start": "2022-10-20T15:57:00Z",
                        "time_end": "2022-10-20T16:07:00Z",
                        "paper_type": "full",
                        "keywords": "",
                        "has_image": "1",
                        "has_video": "599",
                        "paper_award": "",
                        "image_caption": "We propose VDL-Surrogate, a view-dependent neural-network-latent-based surrogate model for parameter space exploration of ensemble simulations that allows high-resolution visualizations and user-specified visual mappings. We use view-dependent latent representations to replace the raw data as the input and output of the visualization surrogate to train the surrogate with limited GPU memory. In the inference stage, given a new input simulation parameter, we first predict the latent representation and then decode the representation to data space. The image compares the visualization results generated using VDL-Surrogate with the ground truth images on cosmology (Nyx) and ocean (MPAS-Ocean) simulations.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-full-1696-qa",
                        "session_id": "full9",
                        "type": "In Person Q+A",
                        "title": "VDL-Surrogate: A View-Dependent Latent-based Model for Parameter Space Exploration of Ensemble Simulations (Q+A)",
                        "contributors": [
                            "Neng Shi"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "v-full-1696",
                        "file_name": "",
                        "time_stamp": "2022-10-20T16:07:00Z",
                        "time_start": "2022-10-20T16:07:00Z",
                        "time_end": "2022-10-20T16:09:00Z",
                        "paper_type": "full",
                        "keywords": "",
                        "has_image": "1",
                        "has_video": "599",
                        "paper_award": "",
                        "image_caption": "We propose VDL-Surrogate, a view-dependent neural-network-latent-based surrogate model for parameter space exploration of ensemble simulations that allows high-resolution visualizations and user-specified visual mappings. We use view-dependent latent representations to replace the raw data as the input and output of the visualization surrogate to train the surrogate with limited GPU memory. In the inference stage, given a new input simulation parameter, we first predict the latent representation and then decode the representation to data space. The image compares the visualization results generated using VDL-Surrogate with the ground truth images on cosmology (Nyx) and ocean (MPAS-Ocean) simulations.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-full-1074-pres",
                        "session_id": "full9",
                        "type": "In Person Presentation",
                        "title": "ConceptExplainer: Understanding the Mental Model of Deep Learning Algorithms via Interactive Concept-based Explanations",
                        "contributors": [
                            "Jinbin Huang"
                        ],
                        "authors": [
                            "Jinbin Huang",
                            "Aditi Mishra",
                            "Bum Chul Kwon",
                            "Chris Bryan"
                        ],
                        "abstract": "",
                        "uid": "v-full-1074",
                        "file_name": "",
                        "time_stamp": "2022-10-20T16:09:00Z",
                        "time_start": "2022-10-20T16:09:00Z",
                        "time_end": "2022-10-20T16:19:00Z",
                        "paper_type": "full",
                        "keywords": "",
                        "has_image": "1",
                        "has_video": "667",
                        "paper_award": "",
                        "image_caption": "Traditional deep learning interpretability methods which are suitable for model users cannot explain network behaviors at the global level and are inflexible at providing fine-grained explanations. As a solution, concept-based explanations are gaining attention due to their human intuitiveness and their flexibility to describe both global and local model behaviors. Concepts are groups of similarly meaningful pixels that express a notion, embedded within the network\u2019s latent space and have commonly been hand-generated, but have recently been discovered by automated approaches. Unfortunately, the magnitude and diversity of discovered concepts makes it difficult to navigate and make sense of the concept space. Visual analytics can serve a valuable role in bridging these gaps by enabling structured navigation and exploration of the concept space to provide concept-based insights of model behavior to users. To this end, we design, develop, and validate ConceptExplainer, a visual analytics system that enables people to interactively probe and explore the concept space to explain model behavior at the instance/class/global level. The system was developed via iterative prototyping to address a number of design challenges that model users face in interpreting the behavior of deep learning models. Via a rigorous user study, we validate how ConceptExplainer supports these challenges. Likewise, we conduct a series of usage scenarios to demonstrate how the system supports the interactive analysis of model behavior across a variety of tasks and explanation granularities, such as identifying concepts that are important to classification, identifying bias in training data, and understanding how concepts can be shared across diverse and seemingly dissimilar classes.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-full-1074-qa",
                        "session_id": "full9",
                        "type": "In Person Q+A",
                        "title": "ConceptExplainer: Understanding the Mental Model of Deep Learning Algorithms via Interactive Concept-based Explanations (Q+A)",
                        "contributors": [
                            "Jinbin Huang"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "v-full-1074",
                        "file_name": "",
                        "time_stamp": "2022-10-20T16:19:00Z",
                        "time_start": "2022-10-20T16:19:00Z",
                        "time_end": "2022-10-20T16:21:00Z",
                        "paper_type": "full",
                        "keywords": "",
                        "has_image": "1",
                        "has_video": "667",
                        "paper_award": "",
                        "image_caption": "Traditional deep learning interpretability methods which are suitable for model users cannot explain network behaviors at the global level and are inflexible at providing fine-grained explanations. As a solution, concept-based explanations are gaining attention due to their human intuitiveness and their flexibility to describe both global and local model behaviors. Concepts are groups of similarly meaningful pixels that express a notion, embedded within the network\u2019s latent space and have commonly been hand-generated, but have recently been discovered by automated approaches. Unfortunately, the magnitude and diversity of discovered concepts makes it difficult to navigate and make sense of the concept space. Visual analytics can serve a valuable role in bridging these gaps by enabling structured navigation and exploration of the concept space to provide concept-based insights of model behavior to users. To this end, we design, develop, and validate ConceptExplainer, a visual analytics system that enables people to interactively probe and explore the concept space to explain model behavior at the instance/class/global level. The system was developed via iterative prototyping to address a number of design challenges that model users face in interpreting the behavior of deep learning models. Via a rigorous user study, we validate how ConceptExplainer supports these challenges. Likewise, we conduct a series of usage scenarios to demonstrate how the system supports the interactive analysis of model behavior across a variety of tasks and explanation granularities, such as identifying concepts that are important to classification, identifying bias in training data, and understanding how concepts can be shared across diverse and seemingly dissimilar classes.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-full-1320-pres",
                        "session_id": "full9",
                        "type": "In Person Presentation",
                        "title": "SliceTeller : A Data Slice-Driven Approach for Machine Learning Model Validation",
                        "contributors": [
                            "Jorge H Piazentin Ono",
                            "Xiaoyu Zhang"
                        ],
                        "authors": [
                            "Xiaoyu Zhang",
                            "Jorge H Piazentin Ono",
                            "Huan Song",
                            "Liang Gou",
                            "Kwan-Liu Ma",
                            "Liu Ren"
                        ],
                        "abstract": "",
                        "uid": "v-full-1320",
                        "file_name": "",
                        "time_stamp": "2022-10-20T16:21:00Z",
                        "time_start": "2022-10-20T16:21:00Z",
                        "time_end": "2022-10-20T16:31:00Z",
                        "paper_type": "full",
                        "keywords": "",
                        "has_image": "1",
                        "has_video": "590",
                        "paper_award": "",
                        "image_caption": "SliceTeller applied to the comparison of two machine learning models (ResNet50 and GroupDRO) for hair color classification (gray hair, not gray hair), trained on the CelebFaces Attributes Dataset (CelebA). (A) Slice Matrix: The data slices (represented as rows), slice descriptions (encoded as columns), and slice metrics (Support and Accuracy). Slices are sorted by model accuracy. (A - Tooltip) Confusion matrix for Slice 1. (B) Estimated effects of optimizing the model for two data slices (Slices 1 and 2, highlighted in blue). (C) Accuracy comparison between the two models, ResNet50 and GroupDRO. (D) Slice Detail View containing image samples from a data slice. (E) Slice Detail View containing the comparison of two data slices using the MatrixScape visualization. (F) System menu, containing options for model selection, effect estimation of focusing on a slice during model training, and data slice summarization.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-full-1320-qa",
                        "session_id": "full9",
                        "type": "In Person Q+A",
                        "title": "SliceTeller : A Data Slice-Driven Approach for Machine Learning Model Validation (Q+A)",
                        "contributors": [
                            "Jorge H Piazentin Ono",
                            "Xiaoyu Zhang"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "v-full-1320",
                        "file_name": "",
                        "time_stamp": "2022-10-20T16:31:00Z",
                        "time_start": "2022-10-20T16:31:00Z",
                        "time_end": "2022-10-20T16:33:00Z",
                        "paper_type": "full",
                        "keywords": "",
                        "has_image": "1",
                        "has_video": "590",
                        "paper_award": "",
                        "image_caption": "SliceTeller applied to the comparison of two machine learning models (ResNet50 and GroupDRO) for hair color classification (gray hair, not gray hair), trained on the CelebFaces Attributes Dataset (CelebA). (A) Slice Matrix: The data slices (represented as rows), slice descriptions (encoded as columns), and slice metrics (Support and Accuracy). Slices are sorted by model accuracy. (A - Tooltip) Confusion matrix for Slice 1. (B) Estimated effects of optimizing the model for two data slices (Slices 1 and 2, highlighted in blue). (C) Accuracy comparison between the two models, ResNet50 and GroupDRO. (D) Slice Detail View containing image samples from a data slice. (E) Slice Detail View containing the comparison of two data slices using the MatrixScape visualization. (F) System menu, containing options for model selection, effect estimation of focusing on a slice during model training, and data slice summarization.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-full-1657-pres",
                        "session_id": "full9",
                        "type": "In Person Presentation",
                        "title": "Calibrate: Interactive Analysis of Probabilistic Model Output",
                        "contributors": [
                            "Peter Xenopoulos"
                        ],
                        "authors": [
                            "Peter Xenopoulos",
                            "Jo\u00e3o Rulff",
                            "Luis Gustavo Nonato",
                            "Brian Barr",
                            "Claudio Silva"
                        ],
                        "abstract": "",
                        "uid": "v-full-1657",
                        "file_name": "",
                        "time_stamp": "2022-10-20T16:33:00Z",
                        "time_start": "2022-10-20T16:33:00Z",
                        "time_end": "2022-10-20T16:43:00Z",
                        "paper_type": "full",
                        "keywords": "",
                        "has_image": "1",
                        "has_video": "504",
                        "paper_award": "",
                        "image_caption": "Calibrate allows users to interactively analyze model calibration in Jupyter Notebooks. The system allows for easy creation of conventional and learned reliability diagrams. The reliability diagrams are coupled with a prediction histogram to provide better context for the users. The diagrams also allow for brushing to analyze prediction regions. When a selection is made, the instance view, shown below the reliability diagram, is updated. Users can also create and analyze subsets of the data by brushing on feature distributions.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-full-1657-qa",
                        "session_id": "full9",
                        "type": "In Person Q+A",
                        "title": "Calibrate: Interactive Analysis of Probabilistic Model Output (Q+A)",
                        "contributors": [
                            "Peter Xenopoulos"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "v-full-1657",
                        "file_name": "",
                        "time_stamp": "2022-10-20T16:43:00Z",
                        "time_start": "2022-10-20T16:43:00Z",
                        "time_end": "2022-10-20T16:45:00Z",
                        "paper_type": "full",
                        "keywords": "",
                        "has_image": "1",
                        "has_video": "504",
                        "paper_award": "",
                        "image_caption": "Calibrate allows users to interactively analyze model calibration in Jupyter Notebooks. The system allows for easy creation of conventional and learned reliability diagrams. The reliability diagrams are coupled with a prediction histogram to provide better context for the users. The diagrams also allow for brushing to analyze prediction regions. When a selection is made, the instance view, shown below the reliability diagram, is updated. Users can also create and analyze subsets of the data by brushing on feature distributions.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-full-1319-pres",
                        "session_id": "full9",
                        "type": "In Person Presentation",
                        "title": "Visualizing Ensemble Predictions of Music Mood",
                        "contributors": [
                            "Mr Zelin Ye"
                        ],
                        "authors": [
                            "Zelin Ye",
                            "Min Chen"
                        ],
                        "abstract": "",
                        "uid": "v-full-1319",
                        "file_name": "",
                        "time_stamp": "2022-10-20T16:45:00Z",
                        "time_start": "2022-10-20T16:45:00Z",
                        "time_end": "2022-10-20T16:55:00Z",
                        "paper_type": "full",
                        "keywords": "",
                        "has_image": "1",
                        "has_video": "597",
                        "paper_award": "",
                        "image_caption": "An ensemble of 210 machine learning (ML) models are visualized by the dual-flux ThemeRiver, \naccompanied with the music score for bar 30~34 of Bach: Paritita V in G, BWV829: Gigue.\nThe upper, lower flux and the dashed line depict the dominant mood, the other moods in the descending\norder, and the 50% threshold over time.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-full-1319-qa",
                        "session_id": "full9",
                        "type": "In Person Q+A",
                        "title": "Visualizing Ensemble Predictions of Music Mood (Q+A)",
                        "contributors": [
                            "Mr Zelin Ye"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "v-full-1319",
                        "file_name": "",
                        "time_stamp": "2022-10-20T16:55:00Z",
                        "time_start": "2022-10-20T16:55:00Z",
                        "time_end": "2022-10-20T16:57:00Z",
                        "paper_type": "full",
                        "keywords": "",
                        "has_image": "1",
                        "has_video": "597",
                        "paper_award": "",
                        "image_caption": "An ensemble of 210 machine learning (ML) models are visualized by the dual-flux ThemeRiver, \naccompanied with the music score for bar 30~34 of Bach: Paritita V in G, BWV829: Gigue.\nThe upper, lower flux and the dashed line depict the dominant mood, the other moods in the descending\norder, and the 50% threshold over time.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    }
                ]
            },
            {
                "title": "Storytelling",
                "session_id": "full10",
                "event_prefix": "v-full",
                "track": "ok4",
                "livestream_id": "ok4-thu",
                "session_image": "full10.png",
                "chair": [
                    "Robert Laramee"
                ],
                "organizers": [],
                "time_start": "2022-10-20T19:00:00Z",
                "time_end": "2022-10-20T20:15:00Z",
                "discord_category": "",
                "discord_channel": "",
                "discord_channel_id": "",
                "discord_link": "",
                "slido_link": "",
                "youtube_url": "",
                "zoom_meeting": "",
                "zoom_password": "",
                "zoom_link": "",
                "ff_link": "",
                "ff_playlist": "",
                "time_slots": [
                    {
                        "slot_id": "full10-opening",
                        "session_id": "full10",
                        "type": "In Person Other",
                        "title": "Session Opening",
                        "contributors": [
                            "Robert Laramee"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "",
                        "file_name": "",
                        "time_stamp": "2022-10-20T19:00:00Z",
                        "time_start": "2022-10-20T19:00:00Z",
                        "time_end": "2022-10-20T19:00:00Z",
                        "paper_type": "",
                        "keywords": "",
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-full-1198-pres",
                        "session_id": "full10",
                        "type": "Virtual Presentation (pre-recorded)",
                        "title": "Breaking the Fourth Wall of Data Stories through Interaction",
                        "contributors": [
                            "Yang Shi",
                            "Tian Gao"
                        ],
                        "authors": [
                            "Yang Shi",
                            "Tian Gao",
                            "Xiaohan Jiao",
                            "Nan Cao"
                        ],
                        "abstract": "",
                        "uid": "v-full-1198",
                        "file_name": "",
                        "time_stamp": "2022-10-20T19:00:00Z",
                        "time_start": "2022-10-20T19:00:00Z",
                        "time_end": "2022-10-20T19:13:00Z",
                        "paper_type": "full",
                        "keywords": "",
                        "has_image": "1",
                        "has_video": "478",
                        "paper_award": "",
                        "image_caption": "We explored data stories that are combined with the narrative device, breaking the fourth wall (BTFW). We collected 58 high-quality data stories from a range of online sources and coded them from two perspectives, including the input of BTFW interaction provided by readers and the output of BTFW interaction generated by data stories to respond to the input. As a result, six design patterns were identified: (C1) Golden Hook, (C2) Kaleidoscope, (C3) Simulator, (C4) Spotlight, (C5) Magic Mirror, and (C6) Touchstone.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-full-1198-qa",
                        "session_id": "full10",
                        "type": "Virtual Q+A",
                        "title": "Breaking the Fourth Wall of Data Stories through Interaction (Q+A)",
                        "contributors": [
                            "Yang Shi",
                            "Tian Gao"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "v-full-1198",
                        "file_name": "",
                        "time_stamp": "2022-10-20T19:13:00Z",
                        "time_start": "2022-10-20T19:13:00Z",
                        "time_end": "2022-10-20T19:15:00Z",
                        "paper_type": "full",
                        "keywords": "",
                        "has_image": "1",
                        "has_video": "478",
                        "paper_award": "",
                        "image_caption": "We explored data stories that are combined with the narrative device, breaking the fourth wall (BTFW). We collected 58 high-quality data stories from a range of online sources and coded them from two perspectives, including the input of BTFW interaction provided by readers and the output of BTFW interaction generated by data stories to respond to the input. As a result, six design patterns were identified: (C1) Golden Hook, (C2) Kaleidoscope, (C3) Simulator, (C4) Spotlight, (C5) Magic Mirror, and (C6) Touchstone.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-full-1495-pres",
                        "session_id": "full10",
                        "type": "Virtual Presentation (pre-recorded)",
                        "title": "Erato: Cooperative Data Story Editing via Fact Interpolation",
                        "contributors": [
                            "Prof. Nan Cao",
                            "Mengdi Sun"
                        ],
                        "authors": [
                            "Mengdi Sun",
                            "Ligan Cai",
                            "Weiwei Cui",
                            "Yanqiu Wu",
                            "Yang Shi",
                            "Nan Cao"
                        ],
                        "abstract": "",
                        "uid": "v-full-1495",
                        "file_name": "",
                        "time_stamp": "2022-10-20T19:15:00Z",
                        "time_start": "2022-10-20T19:15:00Z",
                        "time_end": "2022-10-20T19:28:00Z",
                        "paper_type": "full",
                        "keywords": "",
                        "has_image": "1",
                        "has_video": "540",
                        "paper_award": "",
                        "image_caption": "A data story about natural disasters. The data facts in black were created as keyframes of the story, while the facts in red were generated based on our interpolation algorithm to fill the content gap. The story first illustrates an overall situation of global natural disasters and gradually focuses on the situation in China. Finally, it reveals that floods have the most pernicious impact on China. The corresponding interpolation process is also shown under the data story. The searching path is marked in red and the nodes with yellow borders are the final selected interpolation results.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-full-1495-qa",
                        "session_id": "full10",
                        "type": "Virtual Q+A",
                        "title": "Erato: Cooperative Data Story Editing via Fact Interpolation (Q+A)",
                        "contributors": [
                            "Prof. Nan Cao",
                            "Mengdi Sun"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "v-full-1495",
                        "file_name": "",
                        "time_stamp": "2022-10-20T19:28:00Z",
                        "time_start": "2022-10-20T19:28:00Z",
                        "time_end": "2022-10-20T19:30:00Z",
                        "paper_type": "full",
                        "keywords": "",
                        "has_image": "1",
                        "has_video": "540",
                        "paper_award": "",
                        "image_caption": "A data story about natural disasters. The data facts in black were created as keyframes of the story, while the facts in red were generated based on our interpolation algorithm to fill the content gap. The story first illustrates an overall situation of global natural disasters and gradually focuses on the situation in China. Finally, it reveals that floods have the most pernicious impact on China. The corresponding interpolation process is also shown under the data story. The searching path is marked in red and the nodes with yellow borders are the final selected interpolation results.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-full-1180-pres",
                        "session_id": "full10",
                        "type": "Virtual Presentation (pre-recorded)",
                        "title": "Geo-Storylines: Integrating Maps into Storyline Visualizations",
                        "contributors": [
                            "Vanessa Pe\u00f1a-Araya"
                        ],
                        "authors": [
                            "Golina Hulstein",
                            "Vanessa Pe\u00f1a-Araya",
                            "Anastasia Bezerianos"
                        ],
                        "abstract": "",
                        "uid": "v-full-1180",
                        "file_name": "",
                        "time_stamp": "2022-10-20T19:30:00Z",
                        "time_start": "2022-10-20T19:30:00Z",
                        "time_end": "2022-10-20T19:43:00Z",
                        "paper_type": "full",
                        "keywords": "",
                        "has_image": "1",
                        "has_video": "498",
                        "paper_award": "",
                        "image_caption": "Three Geo-Storyline designs showing the geo-temporal evolution of the relationships between people. (A) Coordinated Views include a map on the left and a unique Storyline timeline on the right. While scrolling, links appear between the relationship nearest to the map and the associated locations. (B) In Map Glyphs each relationship is represented by a map with the associated locations drawn in orange. (C) Time Glyphs are composed by a map on the left and a scrollable list of Storyline glyphs on the right. Each Storyline glyph contains all the relationships associated with one location linked by a gray line.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-full-1180-qa",
                        "session_id": "full10",
                        "type": "Virtual Q+A",
                        "title": "Geo-Storylines: Integrating Maps into Storyline Visualizations (Q+A)",
                        "contributors": [
                            "Vanessa Pe\u00f1a-Araya"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "v-full-1180",
                        "file_name": "",
                        "time_stamp": "2022-10-20T19:43:00Z",
                        "time_start": "2022-10-20T19:43:00Z",
                        "time_end": "2022-10-20T19:45:00Z",
                        "paper_type": "full",
                        "keywords": "",
                        "has_image": "1",
                        "has_video": "498",
                        "paper_award": "",
                        "image_caption": "Three Geo-Storyline designs showing the geo-temporal evolution of the relationships between people. (A) Coordinated Views include a map on the left and a unique Storyline timeline on the right. While scrolling, links appear between the relationship nearest to the map and the associated locations. (B) In Map Glyphs each relationship is represented by a map with the associated locations drawn in orange. (C) Time Glyphs are composed by a map on the left and a scrollable list of Storyline glyphs on the right. Each Storyline glyph contains all the relationships associated with one location linked by a gray line.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-tvcg-9695173-pres",
                        "session_id": "full10",
                        "type": "In Person Presentation",
                        "title": "Roslingifier: Semi-Automated Storytelling for Animated Scatterplots",
                        "contributors": [
                            "Sungahn Ko"
                        ],
                        "authors": [
                            "Minjeong Shin",
                            "Joohee Kim",
                            "Yunha Han",
                            "Lexing Xie",
                            "Mitchell Whitelaw",
                            "Bum Chul Kwon",
                            "Sungahn Ko",
                            "Niklas Elmqvist"
                        ],
                        "abstract": "We present Roslingifier, a data-driven storytelling method for animated scatterplots. Like its namesake, Hans Rosling (1948--2017), a professor of public health and a spellbinding public speaker, Roslingifier turns a sequence of entities changing over time---such as countries and continents with their demographic data---into an engaging narrative telling the story of the data. This data-driven storytelling method with an in-person presenter is a new genre of storytelling technique and has never been studied before. In this paper, we aim to define a design space for this new genre---data presentation---and provide a semi-automated authoring tool for helping presenters create quality presentations. From an in-depth analysis of video clips of presentations using interactive visualizations, we derive three specific techniques to achieve this: natural language narratives, visual effects that highlight events, and temporal branching that changes playback time of the animation. Our implementation of the Roslingifier method is capable of identifying and clustering significant movements, automatically generating visual highlighting and a narrative for playback, and enabling the user to customize. From two user studies, we show that Roslingifier allows users to effectively create engaging data stories and the system features help both presenters and viewers find diverse insights.",
                        "uid": "v-tvcg-9695173",
                        "file_name": "",
                        "time_stamp": "2022-10-20T19:45:00Z",
                        "time_start": "2022-10-20T19:45:00Z",
                        "time_end": "2022-10-20T19:58:00Z",
                        "paper_type": "full",
                        "keywords": "Data-driven storytelling, narrative visualization, Hans Rosling, Gapminder, Trendalyzer",
                        "has_image": "1",
                        "has_video": "548",
                        "paper_award": "",
                        "image_caption": "In this paper, we define a design space for the new genre, data presentation, as the use of interactive data visualization to support in-person presentations. From an in-depth analysis of video clips of presentations using interactive visualizations, we derive specific presentation techniques and implement a semi-automated authoring tool for helping presenters create quality presentations. Our implementation of the Roslingifier method is capable of, identifying and clustering significant movements, automatically generating visual highlighting and a narrative for playback, and enabling the user to customize.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-tvcg-9695173-qa",
                        "session_id": "full10",
                        "type": "In Person Q+A",
                        "title": "Roslingifier: Semi-Automated Storytelling for Animated Scatterplots (Q+A)",
                        "contributors": [
                            "Sungahn Ko"
                        ],
                        "authors": [],
                        "abstract": "We present Roslingifier, a data-driven storytelling method for animated scatterplots. Like its namesake, Hans Rosling (1948--2017), a professor of public health and a spellbinding public speaker, Roslingifier turns a sequence of entities changing over time---such as countries and continents with their demographic data---into an engaging narrative telling the story of the data. This data-driven storytelling method with an in-person presenter is a new genre of storytelling technique and has never been studied before. In this paper, we aim to define a design space for this new genre---data presentation---and provide a semi-automated authoring tool for helping presenters create quality presentations. From an in-depth analysis of video clips of presentations using interactive visualizations, we derive three specific techniques to achieve this: natural language narratives, visual effects that highlight events, and temporal branching that changes playback time of the animation. Our implementation of the Roslingifier method is capable of identifying and clustering significant movements, automatically generating visual highlighting and a narrative for playback, and enabling the user to customize. From two user studies, we show that Roslingifier allows users to effectively create engaging data stories and the system features help both presenters and viewers find diverse insights.",
                        "uid": "v-tvcg-9695173",
                        "file_name": "",
                        "time_stamp": "2022-10-20T19:58:00Z",
                        "time_start": "2022-10-20T19:58:00Z",
                        "time_end": "2022-10-20T20:00:00Z",
                        "paper_type": "full",
                        "keywords": "Data-driven storytelling, narrative visualization, Hans Rosling, Gapminder, Trendalyzer",
                        "has_image": "1",
                        "has_video": "548",
                        "paper_award": "",
                        "image_caption": "In this paper, we define a design space for the new genre, data presentation, as the use of interactive data visualization to support in-person presentations. From an in-depth analysis of video clips of presentations using interactive visualizations, we derive specific presentation techniques and implement a semi-automated authoring tool for helping presenters create quality presentations. Our implementation of the Roslingifier method is capable of, identifying and clustering significant movements, automatically generating visual highlighting and a narrative for playback, and enabling the user to customize.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-tvcg-9645360-pres",
                        "session_id": "full10",
                        "type": "In Person Presentation",
                        "title": "Nanotilus: Generator of Immersive Guided-Tours in Crowded 3D Environments",
                        "contributors": [
                            "Ruwayda Alharbi"
                        ],
                        "authors": [
                            "Ruwayda Alharbi",
                            "Ond\u02c7rej Strnad",
                            "Laura R. Luidolt",
                            "Manuela Waldner",
                            "David Kou\u02c7ril",
                            "Ciril Bohak",
                            "Tobias Klein",
                            "Eduard Groller",
                            "Ivan Viola"
                        ],
                        "abstract": "Immersive virtual reality environments are gaining popularity for studying and exploring crowded three-dimensional structures. When reaching very high structural densities, the natural depiction of the scene produces impenetrable clutter and requires visibility and occlusion management strategies for exploration and orientation. Strategies developed to address the crowdedness in desktop applications, however, inhibit the feeling of immersion. They result in nonimmersive, desktop-style outside-in viewing in virtual reality. This paper proposes Nanotilus---a new visibility and guidance approach for very dense environments that generates an endoscopic inside-out experience instead of outside-in viewing, preserving the immersive aspect of virtual reality. The approach consists of two novel, tightly coupled mechanisms that control scene sparsification simultaneously with camera path planning. The sparsification strategy is localized around the camera and is realized as a multi-scale, multi-shell, variety-preserving technique. When Nanotilus dives into the structures to capture internal details residing on multiple scales, it guides the camera using depth-based path planning. In addition to sparsification and path planning, we complete the tour generation with an animation controller, textual annotation, and text-to-visualization conversion. We demonstrate the generated guided tours on mesoscopic biological models -- SARS-CoV-2 and HIV. We evaluate the Nanotilus experience with a baseline outside-in sparsification and navigational technique in a formal user study with 29 participants. While users can maintain a better overview using the outside-in sparsification, the study confirms our hypothesis that Nanotilus leads to stronger engagement and immersion.",
                        "uid": "v-tvcg-9645360",
                        "file_name": "",
                        "time_stamp": "2022-10-20T20:00:00Z",
                        "time_start": "2022-10-20T20:00:00Z",
                        "time_end": "2022-10-20T20:13:00Z",
                        "paper_type": "full",
                        "keywords": "VR immersive, Visibility management, Path planning, Storytelling, Visualization",
                        "has_image": "1",
                        "has_video": "498",
                        "paper_award": "",
                        "image_caption": "Nanotilus shells enters the center of the HIV model and sparsifies several protein instances to reveal the internal part of the model. A grayscale model with highlighted proteins inside the Nanotilus shells is combined with the model\u2019s final depiction.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-tvcg-9645360-qa",
                        "session_id": "full10",
                        "type": "In Person Q+A",
                        "title": "Nanotilus: Generator of Immersive Guided-Tours in Crowded 3D Environments (Q+A)",
                        "contributors": [
                            "Ruwayda Alharbi"
                        ],
                        "authors": [],
                        "abstract": "Immersive virtual reality environments are gaining popularity for studying and exploring crowded three-dimensional structures. When reaching very high structural densities, the natural depiction of the scene produces impenetrable clutter and requires visibility and occlusion management strategies for exploration and orientation. Strategies developed to address the crowdedness in desktop applications, however, inhibit the feeling of immersion. They result in nonimmersive, desktop-style outside-in viewing in virtual reality. This paper proposes Nanotilus---a new visibility and guidance approach for very dense environments that generates an endoscopic inside-out experience instead of outside-in viewing, preserving the immersive aspect of virtual reality. The approach consists of two novel, tightly coupled mechanisms that control scene sparsification simultaneously with camera path planning. The sparsification strategy is localized around the camera and is realized as a multi-scale, multi-shell, variety-preserving technique. When Nanotilus dives into the structures to capture internal details residing on multiple scales, it guides the camera using depth-based path planning. In addition to sparsification and path planning, we complete the tour generation with an animation controller, textual annotation, and text-to-visualization conversion. We demonstrate the generated guided tours on mesoscopic biological models -- SARS-CoV-2 and HIV. We evaluate the Nanotilus experience with a baseline outside-in sparsification and navigational technique in a formal user study with 29 participants. While users can maintain a better overview using the outside-in sparsification, the study confirms our hypothesis that Nanotilus leads to stronger engagement and immersion.",
                        "uid": "v-tvcg-9645360",
                        "file_name": "",
                        "time_stamp": "2022-10-20T20:13:00Z",
                        "time_start": "2022-10-20T20:13:00Z",
                        "time_end": "2022-10-20T20:15:00Z",
                        "paper_type": "full",
                        "keywords": "VR immersive, Visibility management, Path planning, Storytelling, Visualization",
                        "has_image": "1",
                        "has_video": "498",
                        "paper_award": "",
                        "image_caption": "Nanotilus shells enters the center of the HIV model and sparsifies several protein instances to reveal the internal part of the model. A grayscale model with highlighted proteins inside the Nanotilus shells is combined with the model\u2019s final depiction.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-full-1103-pres",
                        "session_id": "full10",
                        "type": "In Person Presentation",
                        "title": "How Do Viewers Synthesize Conflicting Information from Data Visualizations?",
                        "contributors": [
                            "Mr. Prateek Mantri",
                            "Cindy Xiong"
                        ],
                        "authors": [
                            "Prateek Mantri",
                            "Hariharan Subramonyam",
                            "Audrey Michal",
                            "Cindy Xiong"
                        ],
                        "abstract": "",
                        "uid": "v-full-1103",
                        "file_name": "",
                        "time_stamp": "2022-10-20T20:15:00Z",
                        "time_start": "2022-10-20T20:15:00Z",
                        "time_end": "2022-10-20T20:28:00Z",
                        "paper_type": "full",
                        "keywords": "",
                        "has_image": "1",
                        "has_video": "623",
                        "paper_award": "",
                        "image_caption": "How do visualization readers synthesize conflicting information from multiple visualizations? We conducted a series of experiments in which participants synthesized empirical evidence from a pair of line charts presented sequeentially. We found that participants tended to weigh the positive slope more when the two charts depicted relationships in the opposite direction (e.g., one positive slope and one negative slope).",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-full-1103-qa",
                        "session_id": "full10",
                        "type": "In Person Q+A",
                        "title": "How Do Viewers Synthesize Conflicting Information from Data Visualizations? (Q+A)",
                        "contributors": [
                            "Mr. Prateek Mantri",
                            "Cindy Xiong"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "v-full-1103",
                        "file_name": "",
                        "time_stamp": "2022-10-20T20:28:00Z",
                        "time_start": "2022-10-20T20:28:00Z",
                        "time_end": "2022-10-20T20:30:00Z",
                        "paper_type": "full",
                        "keywords": "",
                        "has_image": "1",
                        "has_video": "623",
                        "paper_award": "",
                        "image_caption": "How do visualization readers synthesize conflicting information from multiple visualizations? We conducted a series of experiments in which participants synthesized empirical evidence from a pair of line charts presented sequeentially. We found that participants tended to weigh the positive slope more when the two charts depicted relationships in the opposite direction (e.g., one positive slope and one negative slope).",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    }
                ]
            },
            {
                "title": "Interactive Dimensionality (High Dimensional Data)",
                "session_id": "full11",
                "event_prefix": "v-full",
                "track": "ok5",
                "livestream_id": "ok5-thu",
                "session_image": "full11.png",
                "chair": [
                    "Lars Linsen"
                ],
                "organizers": [],
                "time_start": "2022-10-20T14:00:00Z",
                "time_end": "2022-10-20T15:15:00Z",
                "discord_category": "",
                "discord_channel": "",
                "discord_channel_id": "",
                "discord_link": "",
                "slido_link": "",
                "youtube_url": "",
                "zoom_meeting": "",
                "zoom_password": "",
                "zoom_link": "",
                "ff_link": "",
                "ff_playlist": "",
                "time_slots": [
                    {
                        "slot_id": "full11-opening",
                        "session_id": "full11",
                        "type": "In Person Other",
                        "title": "Session Opening",
                        "contributors": [
                            "Lars Linsen"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "",
                        "file_name": "",
                        "time_stamp": "2022-10-20T14:00:00Z",
                        "time_start": "2022-10-20T14:00:00Z",
                        "time_end": "2022-10-20T14:00:00Z",
                        "paper_type": "",
                        "keywords": "",
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-full-1525-pres",
                        "session_id": "full11",
                        "type": "Virtual Presentation (pre-recorded)",
                        "title": "RankAxis: Towards a Systematic Combination of Projection and Ranking in Multi-Attribute Data Exploration",
                        "contributors": [
                            "Quan Li"
                        ],
                        "authors": [
                            "Qiangqiang Liu",
                            "Yukun Ren",
                            "Zhihua Zhu",
                            "Dai Li",
                            "Xiaojuan Ma",
                            "Quan Li"
                        ],
                        "abstract": "",
                        "uid": "v-full-1525",
                        "file_name": "",
                        "time_stamp": "2022-10-20T14:00:00Z",
                        "time_start": "2022-10-20T14:00:00Z",
                        "time_end": "2022-10-20T14:13:00Z",
                        "paper_type": "full",
                        "keywords": "",
                        "has_image": "1",
                        "has_video": "544",
                        "paper_award": "",
                        "image_caption": "(A) The data loader facilitates data selection; (B) The interactive projection view shows the projection distribution and guides analysts to explore the projection layout and directional semantics; (C1 \u2013 C5) The ranking tabular view summarizes the attribute contributions to the ranking and supports the deduction of the attribute weights based on user interaction, as well as compares different ranking schemes; (D) The comparative projection view analyzes the distribution of observations generated by different ranking schemes; (E) The ranking projection axis view compares the results of projection and ranking in the same context.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-full-1525-qa",
                        "session_id": "full11",
                        "type": "Virtual Q+A",
                        "title": "RankAxis: Towards a Systematic Combination of Projection and Ranking in Multi-Attribute Data Exploration (Q+A)",
                        "contributors": [
                            "Quan Li"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "v-full-1525",
                        "file_name": "",
                        "time_stamp": "2022-10-20T14:13:00Z",
                        "time_start": "2022-10-20T14:13:00Z",
                        "time_end": "2022-10-20T14:15:00Z",
                        "paper_type": "full",
                        "keywords": "",
                        "has_image": "1",
                        "has_video": "544",
                        "paper_award": "",
                        "image_caption": "(A) The data loader facilitates data selection; (B) The interactive projection view shows the projection distribution and guides analysts to explore the projection layout and directional semantics; (C1 \u2013 C5) The ranking tabular view summarizes the attribute contributions to the ranking and supports the deduction of the attribute weights based on user interaction, as well as compares different ranking schemes; (D) The comparative projection view analyzes the distribution of observations generated by different ranking schemes; (E) The ranking projection axis view compares the results of projection and ranking in the same context.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-full-1631-pres",
                        "session_id": "full11",
                        "type": "In Person Presentation",
                        "title": "PC-Expo: A Metrics-Based Interactive Axes Reordering Method for Parallel Coordinate Displays",
                        "contributors": [
                            "Anjul Kumar Tyagi"
                        ],
                        "authors": [
                            "Anjul Kumar Tyagi",
                            "Tyler Estro",
                            "Geoff Kuenning",
                            "Erez Zadok",
                            "Klaus Mueller"
                        ],
                        "abstract": "",
                        "uid": "v-full-1631",
                        "file_name": "",
                        "time_stamp": "2022-10-20T14:15:00Z",
                        "time_start": "2022-10-20T14:15:00Z",
                        "time_end": "2022-10-20T14:28:00Z",
                        "paper_type": "full",
                        "keywords": "",
                        "has_image": "1",
                        "has_video": "265",
                        "paper_award": "",
                        "image_caption": "PC-Expo is a parallel coordinate axis reordering system supporting real-time local line patter detections. Users can reorder the axis based on many different line properties to represent any story in the data.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-full-1631-qa",
                        "session_id": "full11",
                        "type": "In Person Q+A",
                        "title": "PC-Expo: A Metrics-Based Interactive Axes Reordering Method for Parallel Coordinate Displays (Q+A)",
                        "contributors": [
                            "Anjul Kumar Tyagi"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "v-full-1631",
                        "file_name": "",
                        "time_stamp": "2022-10-20T14:28:00Z",
                        "time_start": "2022-10-20T14:28:00Z",
                        "time_end": "2022-10-20T14:30:00Z",
                        "paper_type": "full",
                        "keywords": "",
                        "has_image": "1",
                        "has_video": "265",
                        "paper_award": "",
                        "image_caption": "PC-Expo is a parallel coordinate axis reordering system supporting real-time local line patter detections. Users can reorder the axis based on many different line properties to represent any story in the data.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-full-1359-pres",
                        "session_id": "full11",
                        "type": "Virtual Presentation (pre-recorded)",
                        "title": "Incorporation of Human Knowledge into Data Embeddings to Improve Pattern Significance and Interpretability",
                        "contributors": [
                            "Jie Li"
                        ],
                        "authors": [
                            "Jie Li",
                            "Chunqi Zhou"
                        ],
                        "abstract": "",
                        "uid": "v-full-1359",
                        "file_name": "",
                        "time_stamp": "2022-10-20T14:30:00Z",
                        "time_start": "2022-10-20T14:30:00Z",
                        "time_end": "2022-10-20T14:43:00Z",
                        "paper_type": "full",
                        "keywords": "",
                        "has_image": "1",
                        "has_video": "571",
                        "paper_award": "",
                        "image_caption": "The user is using the knowledge-based visualization system to analyze the Covid-19 dataset. Visual structures (outliers and clusters) become distinguishable by incorporating human knowledge, making pattern discovery and understanding easy.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-full-1359-qa",
                        "session_id": "full11",
                        "type": "Virtual Q+A",
                        "title": "Incorporation of Human Knowledge into Data Embeddings to Improve Pattern Significance and Interpretability (Q+A)",
                        "contributors": [
                            "Jie Li"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "v-full-1359",
                        "file_name": "",
                        "time_stamp": "2022-10-20T14:43:00Z",
                        "time_start": "2022-10-20T14:43:00Z",
                        "time_end": "2022-10-20T14:45:00Z",
                        "paper_type": "full",
                        "keywords": "",
                        "has_image": "1",
                        "has_video": "571",
                        "paper_award": "",
                        "image_caption": "The user is using the knowledge-based visualization system to analyze the Covid-19 dataset. Visual structures (outliers and clusters) become distinguishable by incorporating human knowledge, making pattern discovery and understanding easy.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-full-1350-pres",
                        "session_id": "full11",
                        "type": "Virtual Presentation (pre-recorded)",
                        "title": "Interactive Visual Cluster Analysis by Contrastive Dimensionality Reduction",
                        "contributors": [
                            "Jiazhi Xia",
                            "Linquan Huang"
                        ],
                        "authors": [
                            "Jiazhi Xia",
                            "Linquan Huang",
                            "Weixing Lin",
                            "Xin Zhao",
                            "Jing Wu",
                            "Yang Chen",
                            "Ying Zhao",
                            "Wei Chen"
                        ],
                        "abstract": "",
                        "uid": "v-full-1350",
                        "file_name": "",
                        "time_stamp": "2022-10-20T14:45:00Z",
                        "time_start": "2022-10-20T14:45:00Z",
                        "time_end": "2022-10-20T14:58:00Z",
                        "paper_type": "full",
                        "keywords": "",
                        "has_image": "1",
                        "has_video": "642",
                        "paper_award": "",
                        "image_caption": "The embedding results by dimensionality reduction techniques. Top: the embedding results of the Indian Food dataset by (a) ISOMAP, (b) t-SNE, (c) UMAP, and (d) CDR (the Contrastive Dimensionality Reduction), respectively. The data points are color-encoded by class labels. Bottom: the interactive analysis of the Animals dataset by CDR. (e) The initial embedding result. (f) A must link is added to merge the butterfly clusters.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-full-1350-qa",
                        "session_id": "full11",
                        "type": "Virtual Q+A",
                        "title": "Interactive Visual Cluster Analysis by Contrastive Dimensionality Reduction (Q+A)",
                        "contributors": [
                            "Jiazhi Xia",
                            "Linquan Huang"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "v-full-1350",
                        "file_name": "",
                        "time_stamp": "2022-10-20T14:58:00Z",
                        "time_start": "2022-10-20T14:58:00Z",
                        "time_end": "2022-10-20T15:00:00Z",
                        "paper_type": "full",
                        "keywords": "",
                        "has_image": "1",
                        "has_video": "642",
                        "paper_award": "",
                        "image_caption": "The embedding results by dimensionality reduction techniques. Top: the embedding results of the Indian Food dataset by (a) ISOMAP, (b) t-SNE, (c) UMAP, and (d) CDR (the Contrastive Dimensionality Reduction), respectively. The data points are color-encoded by class labels. Bottom: the interactive analysis of the Animals dataset by CDR. (e) The initial embedding result. (f) A must link is added to merge the butterfly clusters.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-full-1447-pres",
                        "session_id": "full11",
                        "type": "In Person Presentation",
                        "title": "Predicting User Preferences of Dimensionality Reduction Embedding Quality",
                        "contributors": [
                            "Cristina Morariu"
                        ],
                        "authors": [
                            "Cristina Morariu",
                            "Adrien Bibal",
                            "Rene Cutura",
                            "Benoit Frenay",
                            "Michael Sedlmair"
                        ],
                        "abstract": "",
                        "uid": "v-full-1447",
                        "file_name": "",
                        "time_stamp": "2022-10-20T15:00:00Z",
                        "time_start": "2022-10-20T15:00:00Z",
                        "time_end": "2022-10-20T15:13:00Z",
                        "paper_type": "full",
                        "keywords": "",
                        "has_image": "1",
                        "has_video": "583",
                        "paper_award": "",
                        "image_caption": "Overview of the best embeddings, as predicted by our method for the \"Photography of Flowers\" Dataset. All embeddings we generated are visualized on the metamap on the left hand side, with very good ones displayed against a blue background, while the very good ones displayed against red background. On the left, the top 3 highest rated embeddings are visualized as scatterplots.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-full-1447-qa",
                        "session_id": "full11",
                        "type": "In Person Q+A",
                        "title": "Predicting User Preferences of Dimensionality Reduction Embedding Quality (Q+A)",
                        "contributors": [
                            "Cristina Morariu"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "v-full-1447",
                        "file_name": "",
                        "time_stamp": "2022-10-20T15:13:00Z",
                        "time_start": "2022-10-20T15:13:00Z",
                        "time_end": "2022-10-20T15:15:00Z",
                        "paper_type": "full",
                        "keywords": "",
                        "has_image": "1",
                        "has_video": "583",
                        "paper_award": "",
                        "image_caption": "Overview of the best embeddings, as predicted by our method for the \"Photography of Flowers\" Dataset. All embeddings we generated are visualized on the metamap on the left hand side, with very good ones displayed against a blue background, while the very good ones displayed against red background. On the left, the top 3 highest rated embeddings are visualized as scatterplots.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-tvcg-9382912-pres",
                        "session_id": "full11",
                        "type": "In Person Presentation",
                        "title": "VERTIGo: A Visual Platform for Querying and Exploring Large Multilayer Networks",
                        "contributors": [
                            "Erick Cuenca"
                        ],
                        "authors": [
                            "Erick Cuenca",
                            "Arnaud Sallaberry",
                            "Dino Ienco",
                            "Pascal Poncelet"
                        ],
                        "abstract": "Many real world data can be modeled by a graph with a set of nodes interconnected to each other by multiple relationships. Such a rich graph is called multilayer graph or network. Providing useful visualization tools to support the query process for such graphs is challenging. Although many approaches have addressed the visual query construction, few efforts have been done to provide a contextualized exploration of query results and suggestion strategies to refine the original query. This is due to several issues such as i) the size of the graphs ii) the large number of retrieved results and iii) the way they can be organized to facilitate their exploration. In this paper, we present VERTIGo, a novel visual platform to query, explore and support the analysis of large multilayer graphs. VERTIGo provides coordinated views to navigate and explore the large set of retrieved results at different granularity levels. In addition, the proposed system supports the refinement of the query by visual suggestions to guide the user through the exploration process. Two examples and a user study demonstrate how VERTIGo can be used to perform visual analysis query, exploration, and suggestion) on real world multilayer networks.",
                        "uid": "v-tvcg-9382912",
                        "file_name": "",
                        "time_stamp": "2022-10-20T15:15:00Z",
                        "time_start": "2022-10-20T15:15:00Z",
                        "time_end": "2022-10-20T15:28:00Z",
                        "paper_type": "full",
                        "keywords": "Visual Querying System, Visual Pattern Suggestion, Multilayer Networks",
                        "has_image": "1",
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "The VERTIGo\u2019s user interface being used to explore a co-authorship network. (a) The Query view allows the users to build the query and start the process to retrieve the results, called embeddings. This view also supports query suggestions visualized as pie charts. (b)(c) The Graph view showing either an overview of the embedding locations with a heatmap representation (b) or the embedding relations with Kelp-based diagrams (c). (d) The Embeddings view shows a list of embeddings for a set of selected entities. (e) A histogram allows filtering embeddings by their Minimum Bounding Rectangle values.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-tvcg-9382912-qa",
                        "session_id": "full11",
                        "type": "In Person Q+A",
                        "title": "VERTIGo: A Visual Platform for Querying and Exploring Large Multilayer Networks (Q+A)",
                        "contributors": [
                            "Erick Cuenca"
                        ],
                        "authors": [],
                        "abstract": "Many real world data can be modeled by a graph with a set of nodes interconnected to each other by multiple relationships. Such a rich graph is called multilayer graph or network. Providing useful visualization tools to support the query process for such graphs is challenging. Although many approaches have addressed the visual query construction, few efforts have been done to provide a contextualized exploration of query results and suggestion strategies to refine the original query. This is due to several issues such as i) the size of the graphs ii) the large number of retrieved results and iii) the way they can be organized to facilitate their exploration. In this paper, we present VERTIGo, a novel visual platform to query, explore and support the analysis of large multilayer graphs. VERTIGo provides coordinated views to navigate and explore the large set of retrieved results at different granularity levels. In addition, the proposed system supports the refinement of the query by visual suggestions to guide the user through the exploration process. Two examples and a user study demonstrate how VERTIGo can be used to perform visual analysis query, exploration, and suggestion) on real world multilayer networks.",
                        "uid": "v-tvcg-9382912",
                        "file_name": "",
                        "time_stamp": "2022-10-20T15:28:00Z",
                        "time_start": "2022-10-20T15:28:00Z",
                        "time_end": "2022-10-20T15:30:00Z",
                        "paper_type": "full",
                        "keywords": "Visual Querying System, Visual Pattern Suggestion, Multilayer Networks",
                        "has_image": "1",
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "The VERTIGo\u2019s user interface being used to explore a co-authorship network. (a) The Query view allows the users to build the query and start the process to retrieve the results, called embeddings. This view also supports query suggestions visualized as pie charts. (b)(c) The Graph view showing either an overview of the embedding locations with a heatmap representation (b) or the embedding relations with Kelp-based diagrams (c). (d) The Embeddings view shows a list of embeddings for a set of selected entities. (e) A histogram allows filtering embeddings by their Minimum Bounding Rectangle values.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    }
                ]
            },
            {
                "title": "Natural Language Interaction",
                "session_id": "full12",
                "event_prefix": "v-full",
                "track": "ok4",
                "livestream_id": "ok4-fri",
                "session_image": "full12.png",
                "chair": [
                    "John Stasko"
                ],
                "organizers": [],
                "time_start": "2022-10-21T14:00:00Z",
                "time_end": "2022-10-21T15:15:00Z",
                "discord_category": "",
                "discord_channel": "",
                "discord_channel_id": "",
                "discord_link": "",
                "slido_link": "",
                "youtube_url": "",
                "zoom_meeting": "",
                "zoom_password": "",
                "zoom_link": "",
                "ff_link": "",
                "ff_playlist": "",
                "time_slots": [
                    {
                        "slot_id": "full12-opening",
                        "session_id": "full12",
                        "type": "In Person Other",
                        "title": "Session Opening",
                        "contributors": [
                            "John Stasko"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "",
                        "file_name": "",
                        "time_stamp": "2022-10-21T14:00:00Z",
                        "time_start": "2022-10-21T14:00:00Z",
                        "time_end": "2022-10-21T14:00:00Z",
                        "paper_type": "",
                        "keywords": "",
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-full-1653-pres",
                        "session_id": "full12",
                        "type": "In Person Presentation",
                        "title": "Probablement, Wahrscheinlich, Likely? A Cross-Language Study of How People Verbalize Probabilities in Icon Array Visualizations",
                        "contributors": [
                            "No\u00eblle Rakotondravony"
                        ],
                        "authors": [
                            "No\u00eblle Rakotondravony",
                            "Yiren Ding",
                            "Lane Harrison"
                        ],
                        "abstract": "",
                        "uid": "v-full-1653",
                        "file_name": "",
                        "time_stamp": "2022-10-21T14:00:00Z",
                        "time_start": "2022-10-21T14:00:00Z",
                        "time_end": "2022-10-21T14:13:00Z",
                        "paper_type": "full",
                        "keywords": "",
                        "has_image": "1",
                        "has_video": "504",
                        "paper_award": "",
                        "image_caption": "Two charts showing results from two experiments. \nLeft: Expression-to-Vis experiment. A user icon, an icon array with 100 gray icons, an arrow pointing to a second icon array with 67 icons in orange are above a graph that shows the 95% confidence interval of the values drawn on the icon arrays on the x-axis. The list of probability expressions is on the y-axis. Five 95% confidence intervals representing the range of drawn values in the five languages are displayed for each probability expression.\nRight: Vis-to-Expression experiment. A user icon, an icon array with 35 orange icons, and a list of probability expressions with the expression 'likely' highlighted are above a confidence interval graph that shows the value appearing on the icon array on the x-axis. The list of probability expressions is on the y-axis. Five 95% confidence intervals representing the range of drawn values in the five languages are displayed for each probability expression.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-full-1653-qa",
                        "session_id": "full12",
                        "type": "In Person Q+A",
                        "title": "Probablement, Wahrscheinlich, Likely? A Cross-Language Study of How People Verbalize Probabilities in Icon Array Visualizations (Q+A)",
                        "contributors": [
                            "No\u00eblle Rakotondravony"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "v-full-1653",
                        "file_name": "",
                        "time_stamp": "2022-10-21T14:13:00Z",
                        "time_start": "2022-10-21T14:13:00Z",
                        "time_end": "2022-10-21T14:15:00Z",
                        "paper_type": "full",
                        "keywords": "",
                        "has_image": "1",
                        "has_video": "504",
                        "paper_award": "",
                        "image_caption": "Two charts showing results from two experiments. \nLeft: Expression-to-Vis experiment. A user icon, an icon array with 100 gray icons, an arrow pointing to a second icon array with 67 icons in orange are above a graph that shows the 95% confidence interval of the values drawn on the icon arrays on the x-axis. The list of probability expressions is on the y-axis. Five 95% confidence intervals representing the range of drawn values in the five languages are displayed for each probability expression.\nRight: Vis-to-Expression experiment. A user icon, an icon array with 35 orange icons, and a list of probability expressions with the expression 'likely' highlighted are above a confidence interval graph that shows the value appearing on the icon array on the x-axis. The list of probability expressions is on the y-axis. Five 95% confidence intervals representing the range of drawn values in the five languages are displayed for each probability expression.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-full-1577-pres",
                        "session_id": "full12",
                        "type": "Virtual Presentation (pre-recorded)",
                        "title": "FlowNL: Asking the Flow Data in Natural Languages",
                        "contributors": [
                            "Jun Tao"
                        ],
                        "authors": [
                            "Jieying Huang",
                            "Yang Xi",
                            "Junnan Hu",
                            "Jun Tao"
                        ],
                        "abstract": "",
                        "uid": "v-full-1577",
                        "file_name": "",
                        "time_stamp": "2022-10-21T14:15:00Z",
                        "time_start": "2022-10-21T14:15:00Z",
                        "time_end": "2022-10-21T14:28:00Z",
                        "paper_type": "full",
                        "keywords": "",
                        "has_image": "1",
                        "has_video": "659",
                        "paper_award": "",
                        "image_caption": "Using FLowNL to explore the crayfish dataset. Users can query flow structure in natural languages using the input box (a). They can resolve unknown terms through conversation in the dialog box (b) based on the existing objects (d). FlowNL system will suggest queries based on templates (e) and display the resulting query formula (c). FlowNL visualizes \u201ctiny spiral flow\u201d in orange, \u201cspiral flow\u201d in green, and \u201cupward flow\u201d in purple (f), respectively, based on user queries.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-full-1577-qa",
                        "session_id": "full12",
                        "type": "Virtual Q+A",
                        "title": "FlowNL: Asking the Flow Data in Natural Languages (Q+A)",
                        "contributors": [
                            "Jun Tao"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "v-full-1577",
                        "file_name": "",
                        "time_stamp": "2022-10-21T14:28:00Z",
                        "time_start": "2022-10-21T14:28:00Z",
                        "time_end": "2022-10-21T14:30:00Z",
                        "paper_type": "full",
                        "keywords": "",
                        "has_image": "1",
                        "has_video": "659",
                        "paper_award": "",
                        "image_caption": "Using FLowNL to explore the crayfish dataset. Users can query flow structure in natural languages using the input box (a). They can resolve unknown terms through conversation in the dialog box (b) based on the existing objects (d). FlowNL system will suggest queries based on templates (e) and display the resulting query formula (c). FlowNL visualizes \u201ctiny spiral flow\u201d in orange, \u201cspiral flow\u201d in green, and \u201cupward flow\u201d in purple (f), respectively, based on user queries.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-full-1056-pres",
                        "session_id": "full12",
                        "type": "In Person Presentation",
                        "title": "Comparison Conundrum and the Chamber of Visualizations: An Exploration of How Language Influences Visual Design",
                        "contributors": [
                            "Aimen Gaba"
                        ],
                        "authors": [
                            "Aimen Gaba",
                            "Vidya Setlur",
                            "Arjun Srinivasan",
                            "Jane Hoffswell",
                            "Cindy Xiong"
                        ],
                        "abstract": "",
                        "uid": "v-full-1056",
                        "file_name": "",
                        "time_stamp": "2022-10-21T14:30:00Z",
                        "time_start": "2022-10-21T14:30:00Z",
                        "time_end": "2022-10-21T14:43:00Z",
                        "paper_type": "full",
                        "keywords": "",
                        "has_image": "1",
                        "has_video": "512",
                        "paper_award": "",
                        "image_caption": "Four comparison utterances from our design space with varying cardinalities for the comparison entities (1 - 1, 1 - N, N - M, N) and different levels of concreteness (explicit and implicit). Each of these comparison utterances was included in our online survey in which participants ranked their preference for the different visualization types; the most preferred visualization(s) have a colored border.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-full-1056-qa",
                        "session_id": "full12",
                        "type": "In Person Q+A",
                        "title": "Comparison Conundrum and the Chamber of Visualizations: An Exploration of How Language Influences Visual Design (Q+A)",
                        "contributors": [
                            "Aimen Gaba"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "v-full-1056",
                        "file_name": "",
                        "time_stamp": "2022-10-21T14:43:00Z",
                        "time_start": "2022-10-21T14:43:00Z",
                        "time_end": "2022-10-21T14:45:00Z",
                        "paper_type": "full",
                        "keywords": "",
                        "has_image": "1",
                        "has_video": "512",
                        "paper_award": "",
                        "image_caption": "Four comparison utterances from our design space with varying cardinalities for the comparison entities (1 - 1, 1 - N, N - M, N) and different levels of concreteness (explicit and implicit). Each of these comparison utterances was included in our online survey in which participants ranked their preference for the different visualization types; the most preferred visualization(s) have a colored border.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-tvcg-9615008-pres",
                        "session_id": "full12",
                        "type": "In Person Presentation",
                        "title": "Explaining with Examples: Lessons Learned from Crowdsourced Introductory Description of Information Visualizations",
                        "contributors": [
                            "Leni YANG"
                        ],
                        "authors": [
                            "Leni Yang; Cindy Xiong; Jason K. Wong; Aoyu Wu; Huamin Qu"
                        ],
                        "abstract": "Data visualizations have been increasingly used in oral presentations to communicate data patterns to the general public. Clear verbal introductions of visualizations to explain how to interpret the visually encoded information are essential to convey the takeaways and avoid misunderstandings. We contribute a series of studies to investigate how to effectively introduce visualizations to the audience with varying degrees of visualization literacy. We begin with understanding how people are introducing visualizations. We crowdsource 110 introductions of visualizations and categorize them based on their content and structures. From these crowdsourced introductions, we identify different introduction strategies and generate a set of introductions for evaluation. We conduct experiments to systematically compare the effectiveness of different introduction strategies across four visualizations with 1,080 participants. We find that introductions explaining visual encodings with concrete examples are the most effective. Our study provides both qualitative and quantitative insights into how to construct effective verbal introductions of visualizations in presentations, inspiring further research in data storytelling.",
                        "uid": "v-tvcg-9615008",
                        "file_name": "",
                        "time_stamp": "2022-10-21T14:45:00Z",
                        "time_start": "2022-10-21T14:45:00Z",
                        "time_end": "2022-10-21T14:58:00Z",
                        "paper_type": "full",
                        "keywords": "narrative visualization, oral presentation, introduction",
                        "has_image": "0",
                        "has_video": "582",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-tvcg-9615008-qa",
                        "session_id": "full12",
                        "type": "In Person Q+A",
                        "title": "Explaining with Examples: Lessons Learned from Crowdsourced Introductory Description of Information Visualizations (Q+A)",
                        "contributors": [
                            "Leni YANG"
                        ],
                        "authors": [],
                        "abstract": "Data visualizations have been increasingly used in oral presentations to communicate data patterns to the general public. Clear verbal introductions of visualizations to explain how to interpret the visually encoded information are essential to convey the takeaways and avoid misunderstandings. We contribute a series of studies to investigate how to effectively introduce visualizations to the audience with varying degrees of visualization literacy. We begin with understanding how people are introducing visualizations. We crowdsource 110 introductions of visualizations and categorize them based on their content and structures. From these crowdsourced introductions, we identify different introduction strategies and generate a set of introductions for evaluation. We conduct experiments to systematically compare the effectiveness of different introduction strategies across four visualizations with 1,080 participants. We find that introductions explaining visual encodings with concrete examples are the most effective. Our study provides both qualitative and quantitative insights into how to construct effective verbal introductions of visualizations in presentations, inspiring further research in data storytelling.",
                        "uid": "v-tvcg-9615008",
                        "file_name": "",
                        "time_stamp": "2022-10-21T14:58:00Z",
                        "time_start": "2022-10-21T14:58:00Z",
                        "time_end": "2022-10-21T15:00:00Z",
                        "paper_type": "full",
                        "keywords": "narrative visualization, oral presentation, introduction",
                        "has_image": "0",
                        "has_video": "582",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-full-1260-pres",
                        "session_id": "full12",
                        "type": "Virtual Presentation (pre-recorded)",
                        "title": "Towards Natural Language-Based Visualization Authoring",
                        "contributors": [
                            "Yun Wang"
                        ],
                        "authors": [
                            "Yun Wang",
                            "Zhitao Hou",
                            "Jiaqi Wang",
                            "Tongshuang Wu",
                            "Leixian Shen",
                            "He Huang",
                            "Haidong Zhang",
                            "Dongmei Zhang"
                        ],
                        "abstract": "",
                        "uid": "v-full-1260",
                        "file_name": "",
                        "time_stamp": "2022-10-21T15:00:00Z",
                        "time_start": "2022-10-21T15:00:00Z",
                        "time_end": "2022-10-21T15:13:00Z",
                        "paper_type": "full",
                        "keywords": "",
                        "has_image": "1",
                        "has_video": "489",
                        "paper_award": "",
                        "image_caption": "In our authoring-oriented NLI pipeline, we model users\u2019 editing intents as editing actions, which decouple natural language interpreter and visualization applications as an intermediate layer.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-full-1260-qa",
                        "session_id": "full12",
                        "type": "Virtual Q+A",
                        "title": "Towards Natural Language-Based Visualization Authoring (Q+A)",
                        "contributors": [
                            "Yun Wang"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "v-full-1260",
                        "file_name": "",
                        "time_stamp": "2022-10-21T15:13:00Z",
                        "time_start": "2022-10-21T15:13:00Z",
                        "time_end": "2022-10-21T15:15:00Z",
                        "paper_type": "full",
                        "keywords": "",
                        "has_image": "1",
                        "has_video": "489",
                        "paper_award": "",
                        "image_caption": "In our authoring-oriented NLI pipeline, we model users\u2019 editing intents as editing actions, which decouple natural language interpreter and visualization applications as an intermediate layer.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-full-1024-pres",
                        "session_id": "full12",
                        "type": "In Person Presentation",
                        "title": "Under- and Over-Texting: Exploring the Effect of Quantity and Content on Reader Preferences and Takeaways",
                        "contributors": [
                            "Chase Stokes"
                        ],
                        "authors": [
                            "Chase Stokes",
                            "Vidya Setlur",
                            "Bridget Cogley",
                            "Arvind Satyanarayan",
                            "Marti Hearst"
                        ],
                        "abstract": "",
                        "uid": "v-full-1024",
                        "file_name": "",
                        "time_stamp": "2022-10-21T15:15:00Z",
                        "time_start": "2022-10-21T15:15:00Z",
                        "time_end": "2022-10-21T15:28:00Z",
                        "paper_type": "full",
                        "keywords": "",
                        "has_image": "1",
                        "has_video": "522",
                        "paper_award": "",
                        "image_caption": "A set of information moves from an entirely visual representation to an entirely textual representation. These forms of communication differ importantly in reader preference, as readers most prefer the third set in this sequence: the chart with the most annotations. Readers least preferred the entirely visual and entirely textual representations, although a substantial minority expressed a strong preference for textual representations. Furthermore, the specifics of the text added to the visualizations may influence the readers' takeaways, with certain content leading to similar takeaways and certain positions leading to a higher likelihood for the reader to use the text in their takeaway.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-full-1024-qa",
                        "session_id": "full12",
                        "type": "In Person Q+A",
                        "title": "Under- and Over-Texting: Exploring the Effect of Quantity and Content on Reader Preferences and Takeaways (Q+A)",
                        "contributors": [
                            "Chase Stokes"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "v-full-1024",
                        "file_name": "",
                        "time_stamp": "2022-10-21T15:28:00Z",
                        "time_start": "2022-10-21T15:28:00Z",
                        "time_end": "2022-10-21T15:30:00Z",
                        "paper_type": "full",
                        "keywords": "",
                        "has_image": "1",
                        "has_video": "522",
                        "paper_award": "",
                        "image_caption": "A set of information moves from an entirely visual representation to an entirely textual representation. These forms of communication differ importantly in reader preference, as readers most prefer the third set in this sequence: the chart with the most annotations. Readers least preferred the entirely visual and entirely textual representations, although a substantial minority expressed a strong preference for textual representations. Furthermore, the specifics of the text added to the visualizations may influence the readers' takeaways, with certain content leading to similar takeaways and certain positions leading to a higher likelihood for the reader to use the text in their takeaway.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    }
                ]
            },
            {
                "title": "Reflecting on Academia and our Field",
                "session_id": "full13",
                "event_prefix": "v-full",
                "track": "ok5",
                "livestream_id": "ok5-thu",
                "session_image": "full13.png",
                "chair": [
                    "Petra Isenberg"
                ],
                "organizers": [],
                "time_start": "2022-10-20T19:00:00Z",
                "time_end": "2022-10-20T20:15:00Z",
                "discord_category": "",
                "discord_channel": "",
                "discord_channel_id": "",
                "discord_link": "",
                "slido_link": "",
                "youtube_url": "",
                "zoom_meeting": "",
                "zoom_password": "",
                "zoom_link": "",
                "ff_link": "",
                "ff_playlist": "",
                "time_slots": [
                    {
                        "slot_id": "full13-opening",
                        "session_id": "full13",
                        "type": "In Person Other",
                        "title": "Session Opening",
                        "contributors": [
                            "Petra Isenberg"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "",
                        "file_name": "",
                        "time_stamp": "2022-10-20T19:00:00Z",
                        "time_start": "2022-10-20T19:00:00Z",
                        "time_end": "2022-10-20T19:00:00Z",
                        "paper_type": "",
                        "keywords": "",
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-tvcg-9733942-pres",
                        "session_id": "full13",
                        "type": "In Person Presentation",
                        "title": "Scientometric Analysis of Interdisciplinary Collaboration and Gender Trends in 30 Years of IEEE VIS Publications",
                        "contributors": [
                            "Ali Sarvghad"
                        ],
                        "authors": [
                            "Ali Sarvghad",
                            "Rolando Franqui-Nadal",
                            "Rebecca Reznik-Zellen",
                            "Ria Chawla",
                            "Narges Mahyar"
                        ],
                        "abstract": "We present the results of a scientometric analysis of 30 years of IEEE VIS publications between 1990-2020, in which we conducted a multifaceted analysis of interdisciplinary collaboration and gender composition among authors. To this end, we curated BiblioVIS, a bibliometric dataset that contains rich metadata about IEEE VIS publications, including 3032 papers and 6113 authors. One of the main factors differentiating BiblioVIS from similar datasets is the authors' gender and discipline data, which we inferred through iterative rounds of computational and manual processes. Our analysis shows that, by and large, inter-institutional and interdisciplinary collaboration has been steadily growing over the past 30 years. However, interdisciplinary research was mainly between a few fields, including Computer Science, Engineering and Technology, and Medicine and Health disciplines. Our analysis of gender shows steady growth in women's authorship. Despite this growth, the gender distribution is still highly skewed, with men dominating (~75%) of this space. Our predictive analysis of gender balance shows that if the current trends continue, gender parity in the visualization field will not be reached before the third quarter of the century (~2070). Our primary goal in this work is to call the visualization community's attention to the critical topics of collaboration, diversity, and gender. Our research offers critical insights through the lens of diversity and gender to help accelerate progress towards a more diverse and representative research community.",
                        "uid": "v-tvcg-9733942",
                        "file_name": "",
                        "time_stamp": "2022-10-20T19:00:00Z",
                        "time_start": "2022-10-20T19:00:00Z",
                        "time_end": "2022-10-20T19:13:00Z",
                        "paper_type": "full",
                        "keywords": "Scientometric, IEEE VIS Publications, Gender, Co-authorship, Collaboration, Interdisciplinary, Inter-institutional",
                        "has_image": "0",
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-tvcg-9733942-qa",
                        "session_id": "full13",
                        "type": "In Person Q+A",
                        "title": "Scientometric Analysis of Interdisciplinary Collaboration and Gender Trends in 30 Years of IEEE VIS Publications (Q+A)",
                        "contributors": [
                            "Ali Sarvghad"
                        ],
                        "authors": [],
                        "abstract": "We present the results of a scientometric analysis of 30 years of IEEE VIS publications between 1990-2020, in which we conducted a multifaceted analysis of interdisciplinary collaboration and gender composition among authors. To this end, we curated BiblioVIS, a bibliometric dataset that contains rich metadata about IEEE VIS publications, including 3032 papers and 6113 authors. One of the main factors differentiating BiblioVIS from similar datasets is the authors' gender and discipline data, which we inferred through iterative rounds of computational and manual processes. Our analysis shows that, by and large, inter-institutional and interdisciplinary collaboration has been steadily growing over the past 30 years. However, interdisciplinary research was mainly between a few fields, including Computer Science, Engineering and Technology, and Medicine and Health disciplines. Our analysis of gender shows steady growth in women's authorship. Despite this growth, the gender distribution is still highly skewed, with men dominating (~75%) of this space. Our predictive analysis of gender balance shows that if the current trends continue, gender parity in the visualization field will not be reached before the third quarter of the century (~2070). Our primary goal in this work is to call the visualization community's attention to the critical topics of collaboration, diversity, and gender. Our research offers critical insights through the lens of diversity and gender to help accelerate progress towards a more diverse and representative research community.",
                        "uid": "v-tvcg-9733942",
                        "file_name": "",
                        "time_stamp": "2022-10-20T19:13:00Z",
                        "time_start": "2022-10-20T19:13:00Z",
                        "time_end": "2022-10-20T19:15:00Z",
                        "paper_type": "full",
                        "keywords": "Scientometric, IEEE VIS Publications, Gender, Co-authorship, Collaboration, Interdisciplinary, Inter-institutional",
                        "has_image": "0",
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-full-1364-pres",
                        "session_id": "full13",
                        "type": "In Person Presentation",
                        "title": "Thirty-Two Years of IEEE VIS: Authors, Fields of Study and Citations",
                        "contributors": [
                            "Hongtao Hao"
                        ],
                        "authors": [
                            "Hongtao Hao",
                            "Yumian Cui",
                            "Zhengxiang Wang",
                            "Yea-Seul Kim"
                        ],
                        "abstract": "",
                        "uid": "v-full-1364",
                        "file_name": "",
                        "time_stamp": "2022-10-20T19:15:00Z",
                        "time_start": "2022-10-20T19:15:00Z",
                        "time_end": "2022-10-20T19:28:00Z",
                        "paper_type": "full",
                        "keywords": "",
                        "has_image": "1",
                        "has_video": "470",
                        "paper_award": "",
                        "image_caption": "Fields of study at different levels in 3,240 VIS papers across the past 32 years. From L0 to L3, granularity increases. \u201cTrend\u201d indicates the proportion of papers falling into a field of study against the total number of papers published in that year. The highest proportion for each field of study is highlighted. \u201cTotal\u201d indicates the total number of VIS publications falling into a field of study. One paper may contain more than one field of study at the same level, and one field of study may have multiple parent fields. For example, Pattern Recognition belongs to both Computer Science and Psychology.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-full-1364-qa",
                        "session_id": "full13",
                        "type": "In Person Q+A",
                        "title": "Thirty-Two Years of IEEE VIS: Authors, Fields of Study and Citations (Q+A)",
                        "contributors": [
                            "Hongtao Hao"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "v-full-1364",
                        "file_name": "",
                        "time_stamp": "2022-10-20T19:28:00Z",
                        "time_start": "2022-10-20T19:28:00Z",
                        "time_end": "2022-10-20T19:30:00Z",
                        "paper_type": "full",
                        "keywords": "",
                        "has_image": "1",
                        "has_video": "470",
                        "paper_award": "",
                        "image_caption": "Fields of study at different levels in 3,240 VIS papers across the past 32 years. From L0 to L3, granularity increases. \u201cTrend\u201d indicates the proportion of papers falling into a field of study against the total number of papers published in that year. The highest proportion for each field of study is highlighted. \u201cTotal\u201d indicates the total number of VIS publications falling into a field of study. One paper may contain more than one field of study at the same level, and one field of study may have multiple parent fields. For example, Pattern Recognition belongs to both Computer Science and Psychology.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-tvcg-9747941-pres",
                        "session_id": "full13",
                        "type": "In Person Presentation",
                        "title": "SD^2: Slicing and Dicing Scholarly Data for Interactive Evaluation of Academic Performance",
                        "contributors": [
                            "Zhichun Guo"
                        ],
                        "authors": [
                            "Zhichun Guo",
                            "Jun Tao",
                            "Siming Chen",
                            "Nitesh V. Chawla",
                            "Chaoli Wang"
                        ],
                        "abstract": "Comprehensively evaluating and comparing researchers' academic performance is complicated due to the intrinsic complexity of scholarly data. Different scholarly evaluation tasks often require the publication and citation data to be investigated in various manners. In this paper, we present an interactive visualization framework, SD^2, to enable flexible data partition and composition to support various analysis requirements within a single system. SD^2 features the hierarchical histogram, a novel visual representation for flexibly slicing and dicing the data, allowing different aspects of scholarly performance to be studied and compared. We also leverage the state-of-the-art set visualization technique to select individual researchers or combine multiple scholars for comprehensive visual comparison. We conduct multiple rounds of expert evaluation to study the effectiveness and usability of SD^2 and revise the design and system implementation accordingly. The effectiveness of SD^2 is demonstrated via multiple usage scenarios with each aiming to answer a specific, commonly raised question.",
                        "uid": "v-tvcg-9747941",
                        "file_name": "",
                        "time_stamp": "2022-10-20T19:30:00Z",
                        "time_start": "2022-10-20T19:30:00Z",
                        "time_end": "2022-10-20T19:43:00Z",
                        "paper_type": "full",
                        "keywords": "Scholarly performance, publication, citation, hierarchical histogram, visual analytics.",
                        "has_image": "1",
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "SD^2 consists of three coordinated views: (a) scholar view, (b) publication view, and (c) hierarchical histogram view. The example shows a 2019 ACM Turing Award winner Yoshua Bengio\ufffds papers with his co-authors Ian Goodfellow or Aaron Courville. In (c), the upper histogram shows papers authored by Goodfellow or Courville and the lower histogram shows papers authored by Bengio and Courville.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-tvcg-9747941-qa",
                        "session_id": "full13",
                        "type": "In Person Q+A",
                        "title": "SD^2: Slicing and Dicing Scholarly Data for Interactive Evaluation of Academic Performance (Q+A)",
                        "contributors": [
                            "Zhichun Guo"
                        ],
                        "authors": [],
                        "abstract": "Comprehensively evaluating and comparing researchers' academic performance is complicated due to the intrinsic complexity of scholarly data. Different scholarly evaluation tasks often require the publication and citation data to be investigated in various manners. In this paper, we present an interactive visualization framework, SD^2, to enable flexible data partition and composition to support various analysis requirements within a single system. SD^2 features the hierarchical histogram, a novel visual representation for flexibly slicing and dicing the data, allowing different aspects of scholarly performance to be studied and compared. We also leverage the state-of-the-art set visualization technique to select individual researchers or combine multiple scholars for comprehensive visual comparison. We conduct multiple rounds of expert evaluation to study the effectiveness and usability of SD^2 and revise the design and system implementation accordingly. The effectiveness of SD^2 is demonstrated via multiple usage scenarios with each aiming to answer a specific, commonly raised question.",
                        "uid": "v-tvcg-9747941",
                        "file_name": "",
                        "time_stamp": "2022-10-20T19:43:00Z",
                        "time_start": "2022-10-20T19:43:00Z",
                        "time_end": "2022-10-20T19:45:00Z",
                        "paper_type": "full",
                        "keywords": "Scholarly performance, publication, citation, hierarchical histogram, visual analytics.",
                        "has_image": "1",
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "SD^2 consists of three coordinated views: (a) scholar view, (b) publication view, and (c) hierarchical histogram view. The example shows a 2019 ACM Turing Award winner Yoshua Bengio\ufffds papers with his co-authors Ian Goodfellow or Aaron Courville. In (c), the upper histogram shows papers authored by Goodfellow or Courville and the lower histogram shows papers authored by Bengio and Courville.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-full-1344-pres",
                        "session_id": "full13",
                        "type": "In Person Presentation",
                        "title": "In Defence of Visual Analytics Systems: Replies to Critics",
                        "contributors": [
                            "Aoyu Wu"
                        ],
                        "authors": [
                            "Aoyu Wu",
                            "Dazhen Deng",
                            "Furui Cheng",
                            "Yingcai Wu",
                            "Shixia Liu",
                            "Huamin Qu"
                        ],
                        "abstract": "",
                        "uid": "v-full-1344",
                        "file_name": "",
                        "time_stamp": "2022-10-20T19:45:00Z",
                        "time_start": "2022-10-20T19:45:00Z",
                        "time_end": "2022-10-20T19:58:00Z",
                        "paper_type": "full",
                        "keywords": "",
                        "has_image": "1",
                        "has_video": "551",
                        "paper_award": "",
                        "image_caption": "We contribute a research on visualization research to collect data from researchers to study the underly doing of research. Specifically, we contribute two interview studies. In the first study, we asked researchers \u201cWhat are the criticisms you have received during peer-reviewing?\u201d. In the second interview, we asked researchers how to respond to those criticisms throught the research progress.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-full-1344-qa",
                        "session_id": "full13",
                        "type": "In Person Q+A",
                        "title": "In Defence of Visual Analytics Systems: Replies to Critics (Q+A)",
                        "contributors": [
                            "Aoyu Wu"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "v-full-1344",
                        "file_name": "",
                        "time_stamp": "2022-10-20T19:58:00Z",
                        "time_start": "2022-10-20T19:58:00Z",
                        "time_end": "2022-10-20T20:00:00Z",
                        "paper_type": "full",
                        "keywords": "",
                        "has_image": "1",
                        "has_video": "551",
                        "paper_award": "",
                        "image_caption": "We contribute a research on visualization research to collect data from researchers to study the underly doing of research. Specifically, we contribute two interview studies. In the first study, we asked researchers \u201cWhat are the criticisms you have received during peer-reviewing?\u201d. In the second interview, we asked researchers how to respond to those criticisms throught the research progress.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-full-1152-pres",
                        "session_id": "full13",
                        "type": "In Person Presentation",
                        "title": "Visualization Design Practices in a Crisis: Behind the Scenes with COVID-19 Dashboard Creators",
                        "contributors": [
                            "Yixuan Zhang"
                        ],
                        "authors": [
                            "Yixuan Zhang",
                            "Joseph D Gaggiano",
                            "Yifan Sun",
                            "Neha Kumar",
                            "Clio Andris",
                            "Andrea G Parker"
                        ],
                        "abstract": "",
                        "uid": "v-full-1152",
                        "file_name": "",
                        "time_stamp": "2022-10-20T20:00:00Z",
                        "time_start": "2022-10-20T20:00:00Z",
                        "time_end": "2022-10-20T20:13:00Z",
                        "paper_type": "full",
                        "keywords": "",
                        "has_image": "1",
                        "has_video": "591",
                        "paper_award": "",
                        "image_caption": "{\\rtf1\\ansi\\ansicpg1252\\cocoartf2638\n\\cocoatextscaling0\\cocoaplatform0{\\fonttbl\\f0\\fswiss\\fcharset0 Helvetica;}\n{\\colortbl;\\red255\\green255\\blue255;}\n{\\*\\expandedcolortbl;;}\n\\margl1440\\margr1440\\vieww11520\\viewh8400\\viewkind0\n\\pard\\tx720\\tx1440\\tx2160\\tx2880\\tx3600\\tx4320\\tx5040\\tx5760\\tx6480\\tx7200\\tx7920\\tx8640\\pardirnatural\\partightenfactor0\n\n\\f0\\fs24 \\cf0 Conceptualize visualization as \\'93boundary objects\\'94 to understand design and design guidelines.}",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-full-1152-qa",
                        "session_id": "full13",
                        "type": "In Person Q+A",
                        "title": "Visualization Design Practices in a Crisis: Behind the Scenes with COVID-19 Dashboard Creators (Q+A)",
                        "contributors": [
                            "Yixuan Zhang"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "v-full-1152",
                        "file_name": "",
                        "time_stamp": "2022-10-20T20:13:00Z",
                        "time_start": "2022-10-20T20:13:00Z",
                        "time_end": "2022-10-20T20:15:00Z",
                        "paper_type": "full",
                        "keywords": "",
                        "has_image": "1",
                        "has_video": "591",
                        "paper_award": "",
                        "image_caption": "{\\rtf1\\ansi\\ansicpg1252\\cocoartf2638\n\\cocoatextscaling0\\cocoaplatform0{\\fonttbl\\f0\\fswiss\\fcharset0 Helvetica;}\n{\\colortbl;\\red255\\green255\\blue255;}\n{\\*\\expandedcolortbl;;}\n\\margl1440\\margr1440\\vieww11520\\viewh8400\\viewkind0\n\\pard\\tx720\\tx1440\\tx2160\\tx2880\\tx3600\\tx4320\\tx5040\\tx5760\\tx6480\\tx7200\\tx7920\\tx8640\\pardirnatural\\partightenfactor0\n\n\\f0\\fs24 \\cf0 Conceptualize visualization as \\'93boundary objects\\'94 to understand design and design guidelines.}",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-full-1456-pres",
                        "session_id": "full13",
                        "type": "In Person Presentation",
                        "title": "Understanding how Designers Find and Use Data Visualization Examples",
                        "contributors": [
                            "Hannah K. Bako"
                        ],
                        "authors": [
                            "Hannah K. Bako",
                            "Xinyi Liu",
                            "Leilani Battle",
                            "Zhicheng Liu"
                        ],
                        "abstract": "",
                        "uid": "v-full-1456",
                        "file_name": "",
                        "time_stamp": "2022-10-20T20:15:00Z",
                        "time_start": "2022-10-20T20:15:00Z",
                        "time_end": "2022-10-20T20:28:00Z",
                        "paper_type": "full",
                        "keywords": "",
                        "has_image": "1",
                        "has_video": "600",
                        "paper_award": "",
                        "image_caption": "A graphic showing key themes for a paper titled \"Understanding how designers find and use data visualization examples\". At the top right is a graphic of a young man and lady depicting the two key groups of visualization designers in this study. Below is a collage of images with a magnifying glass depicting the search for examples with the caption \"exploratory vs targeted search\". In the middle is a graphic of a puzzled person trying to evaluate the criteria for selecting examples with the caption \"effectiveness vs aesthetics\". On the far right is a graphic depicting ideas being transferred between examples and visualization designs captioned \" strategies for using examples, Select & Merge, Replicate & Modify, Trial & Error\".",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-full-1456-qa",
                        "session_id": "full13",
                        "type": "In Person Q+A",
                        "title": "Understanding how Designers Find and Use Data Visualization Examples (Q+A)",
                        "contributors": [
                            "Hannah K. Bako"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "v-full-1456",
                        "file_name": "",
                        "time_stamp": "2022-10-20T20:28:00Z",
                        "time_start": "2022-10-20T20:28:00Z",
                        "time_end": "2022-10-20T20:30:00Z",
                        "paper_type": "full",
                        "keywords": "",
                        "has_image": "1",
                        "has_video": "600",
                        "paper_award": "",
                        "image_caption": "A graphic showing key themes for a paper titled \"Understanding how designers find and use data visualization examples\". At the top right is a graphic of a young man and lady depicting the two key groups of visualization designers in this study. Below is a collage of images with a magnifying glass depicting the search for examples with the caption \"exploratory vs targeted search\". In the middle is a graphic of a puzzled person trying to evaluate the criteria for selecting examples with the caption \"effectiveness vs aesthetics\". On the far right is a graphic depicting ideas being transferred between examples and visualization designs captioned \" strategies for using examples, Select & Merge, Replicate & Modify, Trial & Error\".",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    }
                ]
            },
            {
                "title": "Infrastructure Management",
                "session_id": "full14",
                "event_prefix": "v-full",
                "track": "mistletoe",
                "livestream_id": "mistletoe-thu",
                "session_image": "full14.png",
                "chair": [
                    "Ross Maciejewski"
                ],
                "organizers": [],
                "time_start": "2022-10-20T20:45:00Z",
                "time_end": "2022-10-20T22:00:00Z",
                "discord_category": "",
                "discord_channel": "",
                "discord_channel_id": "",
                "discord_link": "",
                "slido_link": "",
                "youtube_url": "",
                "zoom_meeting": "",
                "zoom_password": "",
                "zoom_link": "",
                "ff_link": "",
                "ff_playlist": "",
                "time_slots": [
                    {
                        "slot_id": "full14-opening",
                        "session_id": "full14",
                        "type": "In Person Other",
                        "title": "Session Opening",
                        "contributors": [
                            "Ross Maciejewski"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "",
                        "file_name": "",
                        "time_stamp": "2022-10-20T20:45:00Z",
                        "time_start": "2022-10-20T20:45:00Z",
                        "time_end": "2022-10-20T20:45:00Z",
                        "paper_type": "",
                        "keywords": "",
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-full-1006-pres",
                        "session_id": "full14",
                        "type": "In Person Presentation",
                        "title": "A Systematic Review of BGP Visualization Tools: Mapping Techniques to Cyberattack and Anomaly Attributes",
                        "contributors": [
                            "Justin Raynor"
                        ],
                        "authors": [
                            "Justin Raynor",
                            "Tarik Crnovrsanin",
                            "Sara Di Bartolomeo",
                            "Laura South",
                            "David Saffo",
                            "Cody Dunne"
                        ],
                        "abstract": "",
                        "uid": "v-full-1006",
                        "file_name": "",
                        "time_stamp": "2022-10-20T20:45:00Z",
                        "time_start": "2022-10-20T20:45:00Z",
                        "time_end": "2022-10-20T20:58:00Z",
                        "paper_type": "full",
                        "keywords": "",
                        "has_image": "1",
                        "has_video": "645",
                        "paper_award": "",
                        "image_caption": "Parallel timeline of attacks on the BGP system (bottom) and the tools that have been proposed to visualize them (top). The bottom nodes and the corresponding edges are colored categorically by the type of attack. Each tool is connected to the specific attacks that were used to demonstrate or validate the tool design and are colored to show the number of connected attacks. We have also encoded the edges that connect the specific tools to the attacks with color indicating the attack type to help show patterns in how tool development is changing over time and to answer the question of whether or not the tools are evolving with the threat landscape.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-full-1006-qa",
                        "session_id": "full14",
                        "type": "In Person Q+A",
                        "title": "A Systematic Review of BGP Visualization Tools: Mapping Techniques to Cyberattack and Anomaly Attributes (Q+A)",
                        "contributors": [
                            "Justin Raynor"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "v-full-1006",
                        "file_name": "",
                        "time_stamp": "2022-10-20T20:58:00Z",
                        "time_start": "2022-10-20T20:58:00Z",
                        "time_end": "2022-10-20T21:00:00Z",
                        "paper_type": "full",
                        "keywords": "",
                        "has_image": "1",
                        "has_video": "645",
                        "paper_award": "",
                        "image_caption": "Parallel timeline of attacks on the BGP system (bottom) and the tools that have been proposed to visualize them (top). The bottom nodes and the corresponding edges are colored categorically by the type of attack. Each tool is connected to the specific attacks that were used to demonstrate or validate the tool design and are colored to show the number of connected attacks. We have also encoded the edges that connect the specific tools to the attacks with color indicating the attack type to help show patterns in how tool development is changing over time and to answer the question of whether or not the tools are evolving with the threat landscape.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-full-1329-pres",
                        "session_id": "full14",
                        "type": "Virtual Presentation (pre-recorded)",
                        "title": "RISeer: Inspecting the Status and Dynamics of Regional Industrial Structure via Visual Analytics",
                        "contributors": [
                            "Longfei Chen"
                        ],
                        "authors": [
                            "Longfei Chen",
                            "Yang Ouyang",
                            "Haipeng Zhang",
                            "Suting Hong",
                            "Quan Li"
                        ],
                        "abstract": "",
                        "uid": "v-full-1329",
                        "file_name": "",
                        "time_stamp": "2022-10-20T21:00:00Z",
                        "time_start": "2022-10-20T21:00:00Z",
                        "time_end": "2022-10-20T21:13:00Z",
                        "paper_type": "full",
                        "keywords": "",
                        "has_image": "1",
                        "has_video": "657",
                        "paper_award": "",
                        "image_caption": "Regional industrial structure (RIS) refers to the proportional relationship between industrial sectors with different development functions in a specific region, and it is the result of the combination of the spatial layout of the national economy in a specific region. To explore the evolutionary patterns of the RIS, we present RISeer, an interactive visualization system based on interpretable machine learning models and enhanced visualizations. Our work allows users to track evolution of RIS dynamically and supports inspection and comparison between inter-regional enterprise clusters.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-full-1329-qa",
                        "session_id": "full14",
                        "type": "Virtual Q+A",
                        "title": "RISeer: Inspecting the Status and Dynamics of Regional Industrial Structure via Visual Analytics (Q+A)",
                        "contributors": [
                            "Longfei Chen"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "v-full-1329",
                        "file_name": "",
                        "time_stamp": "2022-10-20T21:13:00Z",
                        "time_start": "2022-10-20T21:13:00Z",
                        "time_end": "2022-10-20T21:15:00Z",
                        "paper_type": "full",
                        "keywords": "",
                        "has_image": "1",
                        "has_video": "657",
                        "paper_award": "",
                        "image_caption": "Regional industrial structure (RIS) refers to the proportional relationship between industrial sectors with different development functions in a specific region, and it is the result of the combination of the spatial layout of the national economy in a specific region. To explore the evolutionary patterns of the RIS, we present RISeer, an interactive visualization system based on interpretable machine learning models and enhanced visualizations. Our work allows users to track evolution of RIS dynamically and supports inspection and comparison between inter-regional enterprise clusters.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-full-1161-pres",
                        "session_id": "full14",
                        "type": "In Person Presentation",
                        "title": "PMU Tracker: A Visualization Platform for Egocentric Event Propagation Analysis in the Power Grid",
                        "contributors": [
                            "Anjana Arunkumar"
                        ],
                        "authors": [
                            "Anjana Arunkumar",
                            "Andrea Pinceti",
                            "Lalitha Sankar",
                            "Chris Bryan"
                        ],
                        "abstract": "",
                        "uid": "v-full-1161",
                        "file_name": "",
                        "time_stamp": "2022-10-20T21:15:00Z",
                        "time_start": "2022-10-20T21:15:00Z",
                        "time_end": "2022-10-20T21:28:00Z",
                        "paper_type": "full",
                        "keywords": "",
                        "has_image": "1",
                        "has_video": "584",
                        "paper_award": "",
                        "image_caption": "Phasor Measurement Units (PMUs) provide high volume time-synchronized measurements of voltage and current, enabling real-time monitoring and control in the electric power grid. PMU Tracker is an end-to-end visual analysis and event localization tool that affords streamlined, flexible, and scalable analysis of PMU data. Based on an identified grid event, users can apply spectral analysis for anomaly detection across a set of coordinated visualizations. This is enabled by our development of a novel cluster dendrogram visualization, which affords users the ability to interactively track event propagation from the epicentric PMU(s), i.e., those at or near the center of the event.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-full-1161-qa",
                        "session_id": "full14",
                        "type": "In Person Q+A",
                        "title": "PMU Tracker: A Visualization Platform for Egocentric Event Propagation Analysis in the Power Grid (Q+A)",
                        "contributors": [
                            "Anjana Arunkumar"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "v-full-1161",
                        "file_name": "",
                        "time_stamp": "2022-10-20T21:28:00Z",
                        "time_start": "2022-10-20T21:28:00Z",
                        "time_end": "2022-10-20T21:30:00Z",
                        "paper_type": "full",
                        "keywords": "",
                        "has_image": "1",
                        "has_video": "584",
                        "paper_award": "",
                        "image_caption": "Phasor Measurement Units (PMUs) provide high volume time-synchronized measurements of voltage and current, enabling real-time monitoring and control in the electric power grid. PMU Tracker is an end-to-end visual analysis and event localization tool that affords streamlined, flexible, and scalable analysis of PMU data. Based on an identified grid event, users can apply spectral analysis for anomaly detection across a set of coordinated visualizations. This is enabled by our development of a novel cluster dendrogram visualization, which affords users the ability to interactively track event propagation from the epicentric PMU(s), i.e., those at or near the center of the event.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-full-1143-pres",
                        "session_id": "full14",
                        "type": "Virtual Presentation (pre-recorded)",
                        "title": "ECoalVis: Visual Analysis of Control Strategies in Coal-fired Power Plants",
                        "contributors": [
                            "Shuhan Liu"
                        ],
                        "authors": [
                            "Shuhan Liu",
                            "Di Weng",
                            "Yuan Tian",
                            "Zikun Deng",
                            "Haoran Xu",
                            "Xiangyu Zhu",
                            "Honglei Yin",
                            "Xianyuan Zhan",
                            "Yingcai Wu"
                        ],
                        "abstract": "",
                        "uid": "v-full-1143",
                        "file_name": "",
                        "time_stamp": "2022-10-20T21:30:00Z",
                        "time_start": "2022-10-20T21:30:00Z",
                        "time_end": "2022-10-20T21:43:00Z",
                        "paper_type": "full",
                        "keywords": "",
                        "has_image": "1",
                        "has_video": "580",
                        "paper_award": "",
                        "image_caption": "The user interface of ECoalVis. (A) The filter view reveals the time series of key sensors and allows users to fuzzy query control strategies. (B) The graph view reveals the spatial propagation of control strategy impact across components, units and sensors. (C) The strategy view depicts the temporal cascading of control strategy impact, visualizing the topology of the strategy and the time-lag-aligned time series. (D) The detail view allows users to search for sensors and perform time series operations to find insights from the raw data.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-full-1143-qa",
                        "session_id": "full14",
                        "type": "Virtual Q+A",
                        "title": "ECoalVis: Visual Analysis of Control Strategies in Coal-fired Power Plants (Q+A)",
                        "contributors": [
                            "Shuhan Liu"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "v-full-1143",
                        "file_name": "",
                        "time_stamp": "2022-10-20T21:43:00Z",
                        "time_start": "2022-10-20T21:43:00Z",
                        "time_end": "2022-10-20T21:45:00Z",
                        "paper_type": "full",
                        "keywords": "",
                        "has_image": "1",
                        "has_video": "580",
                        "paper_award": "",
                        "image_caption": "The user interface of ECoalVis. (A) The filter view reveals the time series of key sensors and allows users to fuzzy query control strategies. (B) The graph view reveals the spatial propagation of control strategy impact across components, units and sensors. (C) The strategy view depicts the temporal cascading of control strategy impact, visualizing the topology of the strategy and the time-lag-aligned time series. (D) The detail view allows users to search for sensors and perform time series operations to find insights from the raw data.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-full-1491-pres",
                        "session_id": "full14",
                        "type": "In Person Presentation",
                        "title": "A Visual Analytics System for Improving Traffic Forecasting Models",
                        "contributors": [
                            "Seungmin Jin"
                        ],
                        "authors": [
                            "Seungmin Jin",
                            "Hyunwook Lee",
                            "Cheonbok Park",
                            "Hyeshin Chu",
                            "Yunwon Tae",
                            "Jaegul Choo",
                            "Sungahn Ko"
                        ],
                        "abstract": "",
                        "uid": "v-full-1491",
                        "file_name": "",
                        "time_stamp": "2022-10-20T21:45:00Z",
                        "time_start": "2022-10-20T21:45:00Z",
                        "time_end": "2022-10-20T21:58:00Z",
                        "paper_type": "full",
                        "keywords": "",
                        "has_image": "1",
                        "has_video": "655",
                        "paper_award": "",
                        "image_caption": "(a) filter view, (b) table view, (c) ground-truth&prediction result comparison view, (d) a map with attention curves and clusters, and (e) attention heatmap view. There are 8 matrices in the head-cluster view to show how much each attention head refers to reference roads for making predictions for target roads in a cluster point of view. In each matrix, the column represents the clusters of reference roads, while the row means clusters of target roads in ascending order. The color of each cluster cell represents the intensity of the attention.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-full-1491-qa",
                        "session_id": "full14",
                        "type": "In Person Q+A",
                        "title": "A Visual Analytics System for Improving Traffic Forecasting Models (Q+A)",
                        "contributors": [
                            "Seungmin Jin"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "v-full-1491",
                        "file_name": "",
                        "time_stamp": "2022-10-20T21:58:00Z",
                        "time_start": "2022-10-20T21:58:00Z",
                        "time_end": "2022-10-20T22:00:00Z",
                        "paper_type": "full",
                        "keywords": "",
                        "has_image": "1",
                        "has_video": "655",
                        "paper_award": "",
                        "image_caption": "(a) filter view, (b) table view, (c) ground-truth&prediction result comparison view, (d) a map with attention curves and clusters, and (e) attention heatmap view. There are 8 matrices in the head-cluster view to show how much each attention head refers to reference roads for making predictions for target roads in a cluster point of view. In each matrix, the column represents the clusters of reference roads, while the row means clusters of target roads in ascending order. The color of each cluster cell represents the intensity of the attention.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-tvcg-9632413-pres",
                        "session_id": "full14",
                        "type": "Virtual Presentation (pre-recorded)",
                        "title": "RCMVis: A Visual Analytics System for Route Choice Modeling",
                        "contributors": [
                            "DongHwa Shin"
                        ],
                        "authors": [
                            "DongHwa Shin",
                            "Jaemin Jo",
                            "Bohyoung Kim",
                            "Hyunjoo Song",
                            "Shin-Hyung Cho",
                            "Jinwook Seo"
                        ],
                        "abstract": "We present RCMVis, a visual analytics system to support interactive Route Choice Modeling analysis. It aims to model which characteristics of routes, such as distance and the number of traffic lights, affect travelers' route choice behaviors and how much they affect the choice during their trips. Through close collaboration with domain experts, we designed a visual analytics framework for Route Choice Modeling. The framework supports three interactive analysis stages: exploration, modeling, and reasoning. In the exploration stage, we help analysts interactively explore trip data from multiple origin-destination (OD) pairs and choose a subset of data they want to focus on. To this end, we provide coordinated multiple OD views with different foci that allow analysts to inspect, rank, and compare OD pairs in terms of their multidimensional attributes. In the modeling stage, we integrate a k-medoids clustering method and a path-size logit model into our system to enable analysts to model route choice behaviors from trips with support for feature selection, hyperparameter tuning, and model comparison. Finally, in the reasoning stage, we help analysts rationalize and refine the model by selectively inspecting the trips that strongly support the modeling result. For evaluation, we conducted a case study and interviews with domain experts. The domain experts discovered unexpected insights from numerous modeling results, allowing them to explore the hyperparameter space more effectively to gain better results. In addition, they gained OD- and road-level insights into which data mainly supported the modeling result, enabling further discussion of the model.",
                        "uid": "v-tvcg-9632413",
                        "file_name": "",
                        "time_stamp": "2022-10-20T22:00:00Z",
                        "time_start": "2022-10-20T22:00:00Z",
                        "time_end": "2022-10-20T22:13:00Z",
                        "paper_type": "full",
                        "keywords": "route choice modeling, urban planning, trajectory data, origin-destination, visual analytics",
                        "has_image": "1",
                        "has_video": "535",
                        "paper_award": "",
                        "image_caption": "We present RCMVis, a visual analytics system to support interactive Route Choice Modeling analysis. It aims to model which characteristics of routes, such as distance and the number of traffic lights, affect travelers' route choice behaviors and how much they affect the choice during their trips. Through close collaboration with domain experts, they could make meaningful discoveries about the data and the models they developed, including geographical distributions of traffic, the hyperparameter space of the models, and data-level insights to help interpret models.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-tvcg-9632413-qa",
                        "session_id": "full14",
                        "type": "Virtual Q+A",
                        "title": "RCMVis: A Visual Analytics System for Route Choice Modeling (Q+A)",
                        "contributors": [
                            "DongHwa Shin"
                        ],
                        "authors": [],
                        "abstract": "We present RCMVis, a visual analytics system to support interactive Route Choice Modeling analysis. It aims to model which characteristics of routes, such as distance and the number of traffic lights, affect travelers' route choice behaviors and how much they affect the choice during their trips. Through close collaboration with domain experts, we designed a visual analytics framework for Route Choice Modeling. The framework supports three interactive analysis stages: exploration, modeling, and reasoning. In the exploration stage, we help analysts interactively explore trip data from multiple origin-destination (OD) pairs and choose a subset of data they want to focus on. To this end, we provide coordinated multiple OD views with different foci that allow analysts to inspect, rank, and compare OD pairs in terms of their multidimensional attributes. In the modeling stage, we integrate a k-medoids clustering method and a path-size logit model into our system to enable analysts to model route choice behaviors from trips with support for feature selection, hyperparameter tuning, and model comparison. Finally, in the reasoning stage, we help analysts rationalize and refine the model by selectively inspecting the trips that strongly support the modeling result. For evaluation, we conducted a case study and interviews with domain experts. The domain experts discovered unexpected insights from numerous modeling results, allowing them to explore the hyperparameter space more effectively to gain better results. In addition, they gained OD- and road-level insights into which data mainly supported the modeling result, enabling further discussion of the model.",
                        "uid": "v-tvcg-9632413",
                        "file_name": "",
                        "time_stamp": "2022-10-20T22:13:00Z",
                        "time_start": "2022-10-20T22:13:00Z",
                        "time_end": "2022-10-20T22:15:00Z",
                        "paper_type": "full",
                        "keywords": "route choice modeling, urban planning, trajectory data, origin-destination, visual analytics",
                        "has_image": "1",
                        "has_video": "535",
                        "paper_award": "",
                        "image_caption": "We present RCMVis, a visual analytics system to support interactive Route Choice Modeling analysis. It aims to model which characteristics of routes, such as distance and the number of traffic lights, affect travelers' route choice behaviors and how much they affect the choice during their trips. Through close collaboration with domain experts, they could make meaningful discoveries about the data and the models they developed, including geographical distributions of traffic, the hyperparameter space of the models, and data-level insights to help interpret models.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    }
                ]
            },
            {
                "title": "Sports Vis",
                "session_id": "full15",
                "event_prefix": "v-full",
                "track": "ok6",
                "livestream_id": "ok6-thu",
                "session_image": "full15.png",
                "chair": [
                    "Johanna Schmidt"
                ],
                "organizers": [],
                "time_start": "2022-10-20T15:45:00Z",
                "time_end": "2022-10-20T17:00:00Z",
                "discord_category": "",
                "discord_channel": "",
                "discord_channel_id": "",
                "discord_link": "",
                "slido_link": "",
                "youtube_url": "",
                "zoom_meeting": "",
                "zoom_password": "",
                "zoom_link": "",
                "ff_link": "",
                "ff_playlist": "",
                "time_slots": [
                    {
                        "slot_id": "full15-opening",
                        "session_id": "full15",
                        "type": "In Person Other",
                        "title": "Session Opening",
                        "contributors": [
                            "Johanna Schmidt"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "",
                        "file_name": "",
                        "time_stamp": "2022-10-20T15:45:00Z",
                        "time_start": "2022-10-20T15:45:00Z",
                        "time_end": "2022-10-20T15:45:00Z",
                        "paper_type": "",
                        "keywords": "",
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-full-1238-pres",
                        "session_id": "full15",
                        "type": "In Person Presentation",
                        "title": "Sporthesia: Augmenting Sports Videos Using Natural Language",
                        "contributors": [
                            "Zhutian Chen"
                        ],
                        "authors": [
                            "Zhutian Chen",
                            "Qisen Yang",
                            "Xiao Xie",
                            "Johanna Beyer",
                            "Haijun Xia",
                            "Yingcai Wu",
                            "Hanspeter Pfister"
                        ],
                        "abstract": "",
                        "uid": "v-full-1238",
                        "file_name": "",
                        "time_stamp": "2022-10-20T15:45:00Z",
                        "time_start": "2022-10-20T15:45:00Z",
                        "time_end": "2022-10-20T15:58:00Z",
                        "paper_type": "full",
                        "keywords": "",
                        "has_image": "1",
                        "has_video": "488",
                        "paper_award": "",
                        "image_caption": "Sporthesia takes raw video footage and commentary text of racket-based sports as input, and outputs an augmented video. To achieve this, three key steps are taken: 1) detecting the visualizable entities in the text, 2) mapping the entities to visualizations, and 3) scheduling the visualizations to play with the raw video.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-full-1238-qa",
                        "session_id": "full15",
                        "type": "In Person Q+A",
                        "title": "Sporthesia: Augmenting Sports Videos Using Natural Language (Q+A)",
                        "contributors": [
                            "Zhutian Chen"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "v-full-1238",
                        "file_name": "",
                        "time_stamp": "2022-10-20T15:58:00Z",
                        "time_start": "2022-10-20T15:58:00Z",
                        "time_end": "2022-10-20T16:00:00Z",
                        "paper_type": "full",
                        "keywords": "",
                        "has_image": "1",
                        "has_video": "488",
                        "paper_award": "",
                        "image_caption": "Sporthesia takes raw video footage and commentary text of racket-based sports as input, and outputs an augmented video. To achieve this, three key steps are taken: 1) detecting the visualizable entities in the text, 2) mapping the entities to visualizations, and 3) scheduling the visualizations to play with the raw video.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-full-1083-pres",
                        "session_id": "full15",
                        "type": "Virtual Presentation (pre-recorded)",
                        "title": "OBTracker: Visual Analytics of Off-ball Movements in Basketball",
                        "contributors": [
                            "Yihong Wu"
                        ],
                        "authors": [
                            "Yihong Wu",
                            "Dazhen Deng",
                            "Xiao Xie",
                            "Moqi He",
                            "Jie Xu",
                            "Hongzeng Zhang",
                            "Hui Zhang",
                            "Yingcai Wu"
                        ],
                        "abstract": "",
                        "uid": "v-full-1083",
                        "file_name": "",
                        "time_stamp": "2022-10-20T16:00:00Z",
                        "time_start": "2022-10-20T16:00:00Z",
                        "time_end": "2022-10-20T16:13:00Z",
                        "paper_type": "full",
                        "keywords": "",
                        "has_image": "1",
                        "has_video": "580",
                        "paper_award": "",
                        "image_caption": "The image shows the system interface of OBTracker. (A) The summary view provides navigation of basketball teams and presents a team\u2019s typical off-ball movement patterns. (B) The player view shows a list of player combinations and their performance when executing a specific type of off-ball movement. (C) The explanation view illustrates why an off-ball movement is effective from the aspects of player positioning and team cooperation.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-full-1083-qa",
                        "session_id": "full15",
                        "type": "Virtual Q+A",
                        "title": "OBTracker: Visual Analytics of Off-ball Movements in Basketball (Q+A)",
                        "contributors": [
                            "Yihong Wu"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "v-full-1083",
                        "file_name": "",
                        "time_stamp": "2022-10-20T16:13:00Z",
                        "time_start": "2022-10-20T16:13:00Z",
                        "time_end": "2022-10-20T16:15:00Z",
                        "paper_type": "full",
                        "keywords": "",
                        "has_image": "1",
                        "has_video": "580",
                        "paper_award": "",
                        "image_caption": "The image shows the system interface of OBTracker. (A) The summary view provides navigation of basketball teams and presents a team\u2019s typical off-ball movement patterns. (B) The player view shows a list of player combinations and their performance when executing a specific type of off-ball movement. (C) The explanation view illustrates why an off-ball movement is effective from the aspects of player positioning and team cooperation.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-tvcg-9802784-pres",
                        "session_id": "full15",
                        "type": "Virtual Presentation (pre-recorded)",
                        "title": "Visualization in Motion: A Research Agenda and Two Evaluations",
                        "contributors": [
                            "Lijie Yao"
                        ],
                        "authors": [
                            "Lijie Yao",
                            "Anastasia Bezerianos",
                            "Romain Vuillemot",
                            "Petra Isenberg"
                        ],
                        "abstract": "We contribute a research agenda for visualization in motion and two experiments to understand how well viewers can read data from moving visualizations. We define visualizations in motion as visual data representations that are used in contexts that exhibit relative motion between a viewer and an entire visualization. Sports analytics, video games, wearable devices, or data physicalizations are example contexts that involve different types of relative motion between a viewer and a visualization. To analyze the opportunities and challenges for designing visualization in motion, we show example scenarios and outline a first research agenda. Motivated primarily by the prevalence of and opportunities for visualizations in sports and video games we started to investigate a small aspect of our research agenda: the impact of two important characteristics of motion---speed and trajectory on a stationary viewer's ability to read data from moving donut and bar charts. We found that increasing speed and trajectory complexity did negatively affect the accuracy of reading values from the charts and that bar charts were more negatively impacted. In practice, however, this impact was small: both charts were still read fairly accurately.",
                        "uid": "v-tvcg-9802784",
                        "file_name": "",
                        "time_stamp": "2022-10-20T16:15:00Z",
                        "time_start": "2022-10-20T16:15:00Z",
                        "time_end": "2022-10-20T16:28:00Z",
                        "paper_type": "full",
                        "keywords": "Situated Visualization, Visualization in Motion, Research Agenda, Empirical Study",
                        "has_image": "1",
                        "has_video": "536",
                        "paper_award": "",
                        "image_caption": "This is the logo of \"Visualization in motion\", as a entire new research direction.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-tvcg-9802784-qa",
                        "session_id": "full15",
                        "type": "Virtual Q+A",
                        "title": "Visualization in Motion: A Research Agenda and Two Evaluations (Q+A)",
                        "contributors": [
                            "Lijie Yao"
                        ],
                        "authors": [],
                        "abstract": "We contribute a research agenda for visualization in motion and two experiments to understand how well viewers can read data from moving visualizations. We define visualizations in motion as visual data representations that are used in contexts that exhibit relative motion between a viewer and an entire visualization. Sports analytics, video games, wearable devices, or data physicalizations are example contexts that involve different types of relative motion between a viewer and a visualization. To analyze the opportunities and challenges for designing visualization in motion, we show example scenarios and outline a first research agenda. Motivated primarily by the prevalence of and opportunities for visualizations in sports and video games we started to investigate a small aspect of our research agenda: the impact of two important characteristics of motion---speed and trajectory on a stationary viewer's ability to read data from moving donut and bar charts. We found that increasing speed and trajectory complexity did negatively affect the accuracy of reading values from the charts and that bar charts were more negatively impacted. In practice, however, this impact was small: both charts were still read fairly accurately.",
                        "uid": "v-tvcg-9802784",
                        "file_name": "",
                        "time_stamp": "2022-10-20T16:28:00Z",
                        "time_start": "2022-10-20T16:28:00Z",
                        "time_end": "2022-10-20T16:30:00Z",
                        "paper_type": "full",
                        "keywords": "Situated Visualization, Visualization in Motion, Research Agenda, Empirical Study",
                        "has_image": "1",
                        "has_video": "536",
                        "paper_award": "",
                        "image_caption": "This is the logo of \"Visualization in motion\", as a entire new research direction.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-full-1041-pres",
                        "session_id": "full15",
                        "type": "Virtual Presentation (pre-recorded)",
                        "title": "RASIPAM: Interactive Pattern Mining of Multivariate Event Sequences in Racket Sports",
                        "contributors": [
                            "Jiang Wu"
                        ],
                        "authors": [
                            "Jiang Wu",
                            "Dongyu Liu",
                            "Ziyang Guo",
                            "Yingcai Wu"
                        ],
                        "abstract": "",
                        "uid": "v-full-1041",
                        "file_name": "",
                        "time_stamp": "2022-10-20T16:30:00Z",
                        "time_start": "2022-10-20T16:30:00Z",
                        "time_end": "2022-10-20T16:43:00Z",
                        "paper_type": "full",
                        "keywords": "",
                        "has_image": "1",
                        "has_video": "555",
                        "paper_award": "",
                        "image_caption": "A screenshot of our system for interactive tactic mining. After domain experts select a player of interest on the control bar (A), the system mines an initial set of tactics and visualizes them in the Projection View (E) and the Tactic View (C). Experts can give suggestions to improve the tactics based on their domain knowledge in the Suggestion Panel (B). The system will refine the tactic set and compare the new tactics with the original ones (C1 and E1). The Rally View (D) shows more details about a chosen tactic.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-full-1041-qa",
                        "session_id": "full15",
                        "type": "Virtual Q+A",
                        "title": "RASIPAM: Interactive Pattern Mining of Multivariate Event Sequences in Racket Sports (Q+A)",
                        "contributors": [
                            "Jiang Wu"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "v-full-1041",
                        "file_name": "",
                        "time_stamp": "2022-10-20T16:43:00Z",
                        "time_start": "2022-10-20T16:43:00Z",
                        "time_end": "2022-10-20T16:45:00Z",
                        "paper_type": "full",
                        "keywords": "",
                        "has_image": "1",
                        "has_video": "555",
                        "paper_award": "",
                        "image_caption": "A screenshot of our system for interactive tactic mining. After domain experts select a player of interest on the control bar (A), the system mines an initial set of tactics and visualizes them in the Projection View (E) and the Tactic View (C). Experts can give suggestions to improve the tactics based on their domain knowledge in the Suggestion Panel (B). The system will refine the tactic set and compare the new tactics with the original ones (C1 and E1). The Rally View (D) shows more details about a chosen tactic.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-full-1031-pres",
                        "session_id": "full15",
                        "type": "Virtual Presentation (pre-recorded)",
                        "title": "Tac-Trainer: A Visual Analytics System for IoT-based Racket Sports Training",
                        "contributors": [
                            "Jiachen Wang"
                        ],
                        "authors": [
                            "Jiachen Wang",
                            "Ji Ma",
                            "Kangping Hu",
                            "Zheng Zhou",
                            "Hui Zhang",
                            "Xiao Xie",
                            "Yingcai Wu"
                        ],
                        "abstract": "",
                        "uid": "v-full-1031",
                        "file_name": "",
                        "time_stamp": "2022-10-20T16:45:00Z",
                        "time_start": "2022-10-20T16:45:00Z",
                        "time_end": "2022-10-20T16:58:00Z",
                        "paper_type": "full",
                        "keywords": "",
                        "has_image": "1",
                        "has_video": "540",
                        "paper_award": "",
                        "image_caption": "Tac-Trainer contains a training view and a suggestion view. The training view visualizes the strokes in a training session through a customized flow (C) consisting of a metadata panel (A), a control panel (B), and a stroke flow (F). The suggestion view provides a list of optimization suggestions (J) for a poorly-performed stroke (G). Each suggestion can be explored in a 3-D coordinate (K). The interface presents the details of Case 1. E is the first training session and D is the second. G is the stroke chosen for optimization. L is the optimization suggestion chosen by the coach.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-full-1031-qa",
                        "session_id": "full15",
                        "type": "Virtual Q+A",
                        "title": "Tac-Trainer: A Visual Analytics System for IoT-based Racket Sports Training (Q+A)",
                        "contributors": [
                            "Jiachen Wang"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "v-full-1031",
                        "file_name": "",
                        "time_stamp": "2022-10-20T16:58:00Z",
                        "time_start": "2022-10-20T16:58:00Z",
                        "time_end": "2022-10-20T17:00:00Z",
                        "paper_type": "full",
                        "keywords": "",
                        "has_image": "1",
                        "has_video": "540",
                        "paper_award": "",
                        "image_caption": "Tac-Trainer contains a training view and a suggestion view. The training view visualizes the strokes in a training session through a customized flow (C) consisting of a metadata panel (A), a control panel (B), and a stroke flow (F). The suggestion view provides a list of optimization suggestions (J) for a poorly-performed stroke (G). Each suggestion can be explored in a 3-D coordinate (K). The interface presents the details of Case 1. E is the first training session and D is the second. G is the stroke chosen for optimization. L is the optimization suggestion chosen by the coach.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-full-1223-pres",
                        "session_id": "full15",
                        "type": "In Person Presentation",
                        "title": "Omnioculars: Embedded Visualization for Augmenting Basketball Game Viewing Experiences",
                        "contributors": [
                            "Tica Lin"
                        ],
                        "authors": [
                            "Tica Lin",
                            "Zhutian Chen",
                            "Yalong Yang",
                            "Daniele Chiappalupi",
                            "Johanna Beyer",
                            "Hanspeter Pfister"
                        ],
                        "abstract": "",
                        "uid": "v-full-1223",
                        "file_name": "",
                        "time_stamp": "2022-10-20T17:00:00Z",
                        "time_start": "2022-10-20T17:00:00Z",
                        "time_end": "2022-10-20T17:13:00Z",
                        "paper_type": "full",
                        "keywords": "",
                        "has_image": "1",
                        "has_video": "626",
                        "paper_award": "",
                        "image_caption": "Our Omnioculars prototype supports a personalized and interactive game viewing experience with embedded visualizations for in-game analysis. We created a simulated basketball game environment to support our design probe of embedded visualizations.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-full-1223-qa",
                        "session_id": "full15",
                        "type": "In Person Q+A",
                        "title": "Omnioculars: Embedded Visualization for Augmenting Basketball Game Viewing Experiences (Q+A)",
                        "contributors": [
                            "Tica Lin"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "v-full-1223",
                        "file_name": "",
                        "time_stamp": "2022-10-20T17:13:00Z",
                        "time_start": "2022-10-20T17:13:00Z",
                        "time_end": "2022-10-20T17:15:00Z",
                        "paper_type": "full",
                        "keywords": "",
                        "has_image": "1",
                        "has_video": "626",
                        "paper_award": "",
                        "image_caption": "Our Omnioculars prototype supports a personalized and interactive game viewing experience with embedded visualizations for in-game analysis. We created a simulated basketball game environment to support our design probe of embedded visualizations.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    }
                ]
            },
            {
                "title": "Visual Analytics of Health Data",
                "session_id": "full16",
                "event_prefix": "v-full",
                "track": "ok5",
                "livestream_id": "ok5-fri",
                "session_image": "full16.png",
                "chair": [
                    "Bum Chul Kwon"
                ],
                "organizers": [],
                "time_start": "2022-10-21T14:00:00Z",
                "time_end": "2022-10-21T15:15:00Z",
                "discord_category": "",
                "discord_channel": "",
                "discord_channel_id": "",
                "discord_link": "",
                "slido_link": "",
                "youtube_url": "",
                "zoom_meeting": "",
                "zoom_password": "",
                "zoom_link": "",
                "ff_link": "",
                "ff_playlist": "",
                "time_slots": [
                    {
                        "slot_id": "full16-opening",
                        "session_id": "full16",
                        "type": "In Person Other",
                        "title": "Session Opening",
                        "contributors": [
                            "Bum Chul Kwon"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "",
                        "file_name": "",
                        "time_stamp": "2022-10-21T14:00:00Z",
                        "time_start": "2022-10-21T14:00:00Z",
                        "time_end": "2022-10-21T14:00:00Z",
                        "paper_type": "",
                        "keywords": "",
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-tvcg-9779066-pres",
                        "session_id": "full16",
                        "type": "In Person Presentation",
                        "title": "ClinicalPath: a Visualization tool to Improve the Evaluation of Electronic Health Records in Clinical Decision-Making",
                        "contributors": [
                            "Claudio Linhares"
                        ],
                        "authors": [
                            "Claudio D. G. Linhares",
                            "Daniel M. Lima",
                            "Jean R. Ponciano",
                            "Mauro M. Olivatto",
                            "Marco A. Gutierrez",
                            "Jorge Poco",
                            "Caetano Traina Jr.",
                            "Agma J. M. Traina"
                        ],
                        "abstract": "Physicians work at a very tight schedule and need decision-making support tools to help on improving and doing their work in a timely and dependable manner. Examining piles of sheets with test results and using systems with little visualization support to provide diagnostics is daunting, but that is still the usual way for the physicians' daily procedure, especially in developing countries. Electronic Health Records systems have been designed to keep the patients' history and reduce the time spent analyzing the patient's data. However, better tools to support decision-making are still needed. In this paper, we propose \\systemname, a visualization tool for users to track a patient's clinical path through a series of tests and data, which can aid in treatments and diagnoses. Our proposal is focused on patient's data analysis, presenting the test results and clinical history longitudinally. Both the visualization design and the system functionality were developed in close collaboration with experts in the medical domain to ensure a right fit of the technical solutions and the real needs of the professionals. We validated the proposed visualization based on case studies and user assessments through tasks based on the physician's daily activities. Our results show that our proposed system improves the physicians' experience in decision-making tasks, made with more confidence and better usage of the physicians' time, allowing them to take other needed care for the patients.",
                        "uid": "v-tvcg-9779066",
                        "file_name": "",
                        "time_stamp": "2022-10-21T14:00:00Z",
                        "time_start": "2022-10-21T14:00:00Z",
                        "time_end": "2022-10-21T14:13:00Z",
                        "paper_type": "full",
                        "keywords": "Information Visualization, Interactive Visualizations, Human-Computer Interaction, Electronic Health Records",
                        "has_image": "1",
                        "has_video": "522",
                        "paper_award": "",
                        "image_caption": "Example of the Clinical path visualization divided into the main features: (A) Categorization of the tests; (B) Timestamp information with color codding, note that the dates follow the date format (dd/MM/yyyy); (C) Color and shape codding (symbols) for the test results; and (D) Line chart with the test result variation over time; (E)  Line chart combining different test results over time; (F) Automatic highlighting relevant changes in tests results over time; (G) Patient global and local information over time; (H) Line chart showing patient information over time.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-tvcg-9779066-qa",
                        "session_id": "full16",
                        "type": "In Person Q+A",
                        "title": "ClinicalPath: a Visualization tool to Improve the Evaluation of Electronic Health Records in Clinical Decision-Making (Q+A)",
                        "contributors": [
                            "Claudio Linhares"
                        ],
                        "authors": [],
                        "abstract": "Physicians work at a very tight schedule and need decision-making support tools to help on improving and doing their work in a timely and dependable manner. Examining piles of sheets with test results and using systems with little visualization support to provide diagnostics is daunting, but that is still the usual way for the physicians' daily procedure, especially in developing countries. Electronic Health Records systems have been designed to keep the patients' history and reduce the time spent analyzing the patient's data. However, better tools to support decision-making are still needed. In this paper, we propose \\systemname, a visualization tool for users to track a patient's clinical path through a series of tests and data, which can aid in treatments and diagnoses. Our proposal is focused on patient's data analysis, presenting the test results and clinical history longitudinally. Both the visualization design and the system functionality were developed in close collaboration with experts in the medical domain to ensure a right fit of the technical solutions and the real needs of the professionals. We validated the proposed visualization based on case studies and user assessments through tasks based on the physician's daily activities. Our results show that our proposed system improves the physicians' experience in decision-making tasks, made with more confidence and better usage of the physicians' time, allowing them to take other needed care for the patients.",
                        "uid": "v-tvcg-9779066",
                        "file_name": "",
                        "time_stamp": "2022-10-21T14:13:00Z",
                        "time_start": "2022-10-21T14:13:00Z",
                        "time_end": "2022-10-21T14:15:00Z",
                        "paper_type": "full",
                        "keywords": "Information Visualization, Interactive Visualizations, Human-Computer Interaction, Electronic Health Records",
                        "has_image": "1",
                        "has_video": "522",
                        "paper_award": "",
                        "image_caption": "Example of the Clinical path visualization divided into the main features: (A) Categorization of the tests; (B) Timestamp information with color codding, note that the dates follow the date format (dd/MM/yyyy); (C) Color and shape codding (symbols) for the test results; and (D) Line chart with the test result variation over time; (E)  Line chart combining different test results over time; (F) Automatic highlighting relevant changes in tests results over time; (G) Patient global and local information over time; (H) Line chart showing patient information over time.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-tvcg-9754243-pres",
                        "session_id": "full16",
                        "type": "Virtual Presentation (pre-recorded)",
                        "title": "Visual Assistance in Development and Validation of Bayesian Networks for Clinical Decision Support",
                        "contributors": [
                            "Juliane M\u00fcller-Sielaff,",
                            "Juliane M\u00fcller-Sielaff"
                        ],
                        "authors": [
                            "Juliane M\u00fcller-Sielaff",
                            "Seyed Behnam Beladi",
                            "Stephanie W. Vrede",
                            "Monique Meuschke",
                            "Peter J.F. Lucas",
                            "Johanna M.A. Pijnenborg",
                            "Steffen Oeltze-Jafra"
                        ],
                        "abstract": "The development and validation of Clinical Decision Support Models (CDSM) based on Bayesian networks (BN) is commonly done in a collaborative work between medical researchers providing the domain expertise and computer scientists developing the decision support model. Although modern tools provide facilities for data-driven model generation, domain experts are required to validate the accuracy of the learned model and to provide expert knowledge for fine-tuning it while computer scientists are needed to integrate this knowledge in the learned model (hybrid modeling approach). This generally time-expensive procedure hampers CDSM generation and updating. To address this problem, we developed a novel interactive visual approach allowing medical researchers with less knowledge in CDSM to develop and validate BNs based on domain specific data mainly independently and thus, diminishing the need for an additional computer scientist. In this context, we abstracted and simplified the common workflow in BN development as well as adjusted the workflow to medical experts' needs. We demonstrate our visual approach with data of endometrial cancer patients and evaluated it with six medical researchers who are domain experts in the gynecological field.",
                        "uid": "v-tvcg-9754243",
                        "file_name": "",
                        "time_stamp": "2022-10-21T14:15:00Z",
                        "time_start": "2022-10-21T14:15:00Z",
                        "time_end": "2022-10-21T14:28:00Z",
                        "paper_type": "full",
                        "keywords": "Bayesian networks, Visual Analysis, Clinical Decision Support, Causal Model Development",
                        "has_image": "1",
                        "has_video": "604",
                        "paper_award": "",
                        "image_caption": "The common development process of Clinical Decision Support Models (CDSM) based on Bayesian networks (BN) requires medical researchers providing the domain expertise and a modeling expert developing the model. This collaborative but generally time-expensive procedure, however, hampers in model generation and updating. Furthermore, because the modelling expert might make design decisions on the experts\u2019 domain, at some stage the medical expert may be confronted with a lack of understanding of the resulting BN model. To address these problems, we gathered the requirements on a visual approach allowing medical researchers to generate and validate CDSMs based on BNs mainly independently.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-tvcg-9754243-qa",
                        "session_id": "full16",
                        "type": "Virtual Q+A",
                        "title": "Visual Assistance in Development and Validation of Bayesian Networks for Clinical Decision Support (Q+A)",
                        "contributors": [
                            "Juliane M\u00fcller-Sielaff,",
                            "Juliane M\u00fcller-Sielaff"
                        ],
                        "authors": [],
                        "abstract": "The development and validation of Clinical Decision Support Models (CDSM) based on Bayesian networks (BN) is commonly done in a collaborative work between medical researchers providing the domain expertise and computer scientists developing the decision support model. Although modern tools provide facilities for data-driven model generation, domain experts are required to validate the accuracy of the learned model and to provide expert knowledge for fine-tuning it while computer scientists are needed to integrate this knowledge in the learned model (hybrid modeling approach). This generally time-expensive procedure hampers CDSM generation and updating. To address this problem, we developed a novel interactive visual approach allowing medical researchers with less knowledge in CDSM to develop and validate BNs based on domain specific data mainly independently and thus, diminishing the need for an additional computer scientist. In this context, we abstracted and simplified the common workflow in BN development as well as adjusted the workflow to medical experts' needs. We demonstrate our visual approach with data of endometrial cancer patients and evaluated it with six medical researchers who are domain experts in the gynecological field.",
                        "uid": "v-tvcg-9754243",
                        "file_name": "",
                        "time_stamp": "2022-10-21T14:28:00Z",
                        "time_start": "2022-10-21T14:28:00Z",
                        "time_end": "2022-10-21T14:30:00Z",
                        "paper_type": "full",
                        "keywords": "Bayesian networks, Visual Analysis, Clinical Decision Support, Causal Model Development",
                        "has_image": "1",
                        "has_video": "604",
                        "paper_award": "",
                        "image_caption": "The common development process of Clinical Decision Support Models (CDSM) based on Bayesian networks (BN) requires medical researchers providing the domain expertise and a modeling expert developing the model. This collaborative but generally time-expensive procedure, however, hampers in model generation and updating. Furthermore, because the modelling expert might make design decisions on the experts\u2019 domain, at some stage the medical expert may be confronted with a lack of understanding of the resulting BN model. To address these problems, we gathered the requirements on a visual approach allowing medical researchers to generate and validate CDSMs based on BNs mainly independently.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-full-1228-pres",
                        "session_id": "full16",
                        "type": "In Person Presentation",
                        "title": "Chartwalk: Navigating large collections of text notes in electronic health records for clinical chart review",
                        "contributors": [
                            "Nicole Sultanum"
                        ],
                        "authors": [
                            "Nicole Sultanum",
                            "Farooq Naeem",
                            "Michael Brudno",
                            "Fanny Chevalier"
                        ],
                        "abstract": "",
                        "uid": "v-full-1228",
                        "file_name": "",
                        "time_stamp": "2022-10-21T14:30:00Z",
                        "time_start": "2022-10-21T14:30:00Z",
                        "time_end": "2022-10-21T14:43:00Z",
                        "paper_type": "full",
                        "keywords": "",
                        "has_image": "1",
                        "has_video": "646",
                        "paper_award": "",
                        "image_caption": "Interface Design for ChartWalk, a text visualization tool to support healthcare workers navigate text collections in patient records for clinical overview.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-full-1228-qa",
                        "session_id": "full16",
                        "type": "In Person Q+A",
                        "title": "Chartwalk: Navigating large collections of text notes in electronic health records for clinical chart review (Q+A)",
                        "contributors": [
                            "Nicole Sultanum"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "v-full-1228",
                        "file_name": "",
                        "time_stamp": "2022-10-21T14:43:00Z",
                        "time_start": "2022-10-21T14:43:00Z",
                        "time_end": "2022-10-21T14:45:00Z",
                        "paper_type": "full",
                        "keywords": "",
                        "has_image": "1",
                        "has_video": "646",
                        "paper_award": "",
                        "image_caption": "Interface Design for ChartWalk, a text visualization tool to support healthcare workers navigate text collections in patient records for clinical overview.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-full-1111-pres",
                        "session_id": "full16",
                        "type": "In Person Presentation",
                        "title": "Development and Evaluation of Two Approaches of Visual Sensitivity Analysis to Support Epidemiological Modelling",
                        "contributors": [
                            "Erik Rydow"
                        ],
                        "authors": [
                            "Erik Rydow",
                            "Rita Borgo",
                            "Hui Fang",
                            "Thomas Torsney-Weir",
                            "Ben Swallow",
                            "Thibaud Porphyre",
                            "Cagatay Turkay",
                            "Min Chen"
                        ],
                        "abstract": "",
                        "uid": "v-full-1111",
                        "file_name": "",
                        "time_stamp": "2022-10-21T14:45:00Z",
                        "time_start": "2022-10-21T14:45:00Z",
                        "time_end": "2022-10-21T14:58:00Z",
                        "paper_type": "full",
                        "keywords": "",
                        "has_image": "1",
                        "has_video": "485",
                        "paper_award": "",
                        "image_caption": "An algorithmic approach to sensitivity analysis typically takes the data of ensemble runs, such as the parameter sets (a) and outputs (b), and estimates the sensitivity of each parameter (c). This algorithmic-centric approach can be assisted by visualization (d) that enriches the basis numerical measures, and be complemented by a visualization-centric and algorithm-assisted approach (e).",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-full-1111-qa",
                        "session_id": "full16",
                        "type": "In Person Q+A",
                        "title": "Development and Evaluation of Two Approaches of Visual Sensitivity Analysis to Support Epidemiological Modelling (Q+A)",
                        "contributors": [
                            "Erik Rydow"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "v-full-1111",
                        "file_name": "",
                        "time_stamp": "2022-10-21T14:58:00Z",
                        "time_start": "2022-10-21T14:58:00Z",
                        "time_end": "2022-10-21T15:00:00Z",
                        "paper_type": "full",
                        "keywords": "",
                        "has_image": "1",
                        "has_video": "485",
                        "paper_award": "",
                        "image_caption": "An algorithmic approach to sensitivity analysis typically takes the data of ensemble runs, such as the parameter sets (a) and outputs (b), and estimates the sensitivity of each parameter (c). This algorithmic-centric approach can be assisted by visualization (d) that enriches the basis numerical measures, and be complemented by a visualization-centric and algorithm-assisted approach (e).",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-tvcg-9721816-pres",
                        "session_id": "full16",
                        "type": "Virtual Presentation (pre-recorded)",
                        "title": "A framework for evaluating dashboards in healthcare",
                        "contributors": [
                            "Mengdie Zhuang"
                        ],
                        "authors": [
                            "Mengdie Zhuang",
                            "David Concannon",
                            "Ed Manley"
                        ],
                        "abstract": "In the era of \u2018information overload\u2019, effective information provision is essential for enabling rapid response and critical decision making. In making sense of diverse information sources, dashboards have become an indispensable tool, providing fast, effective, adaptable, and personalized access to information for professionals and the general public alike. However, these objectives place heavy requirements on dashboards as information systems in usability and effective design. Understanding these issues is challenging given the absence of consistent and comprehensive approaches to dashboard evaluation. In this paper we systematically review literature on dashboard implementation in healthcare, where dashboards have been employed widely, and where there is widespread interest for improving the current state of the art, and subsequently analyse approaches taken towards evaluation. We draw upon consolidated dashboard literature and our own observations to introduce a general definition of dashboards which is more relevant to current trends, together with seven evaluation scenarios - task performance, behaviour change, interaction workflow,  perceived engagement, potential utility, algorithm performance and system implementation. These scenarios distinguish different evaluation purposes which we illustrate through measurements, example studies, and common challenges in evaluation study design. We provide a breakdown of each evaluation scenario, and highlight some of the more subtle questions. We demonstrate the use of the proposed framework by a design study guided by this framework. We conclude by comparing this framework with existing literature,  outlining a number of active discussion points and a set of dashboard evaluation best practices for the academic, clinical and software development communities alike.",
                        "uid": "v-tvcg-9721816",
                        "file_name": "",
                        "time_stamp": "2022-10-21T15:00:00Z",
                        "time_start": "2022-10-21T15:00:00Z",
                        "time_end": "2022-10-21T15:13:00Z",
                        "paper_type": "full",
                        "keywords": "Visualization, Dashboard, Evaluation, Healthcare.",
                        "has_image": "1",
                        "has_video": "641",
                        "paper_award": "",
                        "image_caption": "A Framework: Seven Scenarios for Evaluating Dashboards",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-tvcg-9721816-qa",
                        "session_id": "full16",
                        "type": "Virtual Q+A",
                        "title": "A framework for evaluating dashboards in healthcare (Q+A)",
                        "contributors": [
                            "Mengdie Zhuang"
                        ],
                        "authors": [],
                        "abstract": "In the era of \u2018information overload\u2019, effective information provision is essential for enabling rapid response and critical decision making. In making sense of diverse information sources, dashboards have become an indispensable tool, providing fast, effective, adaptable, and personalized access to information for professionals and the general public alike. However, these objectives place heavy requirements on dashboards as information systems in usability and effective design. Understanding these issues is challenging given the absence of consistent and comprehensive approaches to dashboard evaluation. In this paper we systematically review literature on dashboard implementation in healthcare, where dashboards have been employed widely, and where there is widespread interest for improving the current state of the art, and subsequently analyse approaches taken towards evaluation. We draw upon consolidated dashboard literature and our own observations to introduce a general definition of dashboards which is more relevant to current trends, together with seven evaluation scenarios - task performance, behaviour change, interaction workflow,  perceived engagement, potential utility, algorithm performance and system implementation. These scenarios distinguish different evaluation purposes which we illustrate through measurements, example studies, and common challenges in evaluation study design. We provide a breakdown of each evaluation scenario, and highlight some of the more subtle questions. We demonstrate the use of the proposed framework by a design study guided by this framework. We conclude by comparing this framework with existing literature,  outlining a number of active discussion points and a set of dashboard evaluation best practices for the academic, clinical and software development communities alike.",
                        "uid": "v-tvcg-9721816",
                        "file_name": "",
                        "time_stamp": "2022-10-21T15:13:00Z",
                        "time_start": "2022-10-21T15:13:00Z",
                        "time_end": "2022-10-21T15:15:00Z",
                        "paper_type": "full",
                        "keywords": "Visualization, Dashboard, Evaluation, Healthcare.",
                        "has_image": "1",
                        "has_video": "641",
                        "paper_award": "",
                        "image_caption": "A Framework: Seven Scenarios for Evaluating Dashboards",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-full-1584-pres",
                        "session_id": "full16",
                        "type": "In Person Presentation",
                        "title": "Extending the Nested Model for User-Centric XAI: A Design Study on GNN-based Drug Repurposing",
                        "contributors": [
                            "Qianwen Wang"
                        ],
                        "authors": [
                            "Qianwen Wang",
                            "Kexin Huang",
                            "Payal Chandak",
                            "Marinka Zitnik",
                            "Nils Gehlenborg"
                        ],
                        "abstract": "",
                        "uid": "v-full-1584",
                        "file_name": "",
                        "time_stamp": "2022-10-21T15:15:00Z",
                        "time_start": "2022-10-21T15:15:00Z",
                        "time_end": "2022-10-21T15:28:00Z",
                        "paper_type": "full",
                        "keywords": "",
                        "has_image": "1",
                        "has_video": "542",
                        "paper_award": "",
                        "image_caption": "Previous AI visualization tools usually select one specific explanation before the design study based on its popularity in the ML community without considering how the domain characteristics and user needs may influence the selection and visualization of explanations.\nThis paper presents a design study where we investigated how to select and visualize AI explanations for domain users.\nBuilding on the nested design model of visualization, we incorporate XAI design considerations from a literature review and our collaborators' feedback into the design process.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-full-1584-qa",
                        "session_id": "full16",
                        "type": "In Person Q+A",
                        "title": "Extending the Nested Model for User-Centric XAI: A Design Study on GNN-based Drug Repurposing (Q+A)",
                        "contributors": [
                            "Qianwen Wang"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "v-full-1584",
                        "file_name": "",
                        "time_stamp": "2022-10-21T15:28:00Z",
                        "time_start": "2022-10-21T15:28:00Z",
                        "time_end": "2022-10-21T15:30:00Z",
                        "paper_type": "full",
                        "keywords": "",
                        "has_image": "1",
                        "has_video": "542",
                        "paper_award": "",
                        "image_caption": "Previous AI visualization tools usually select one specific explanation before the design study based on its popularity in the ML community without considering how the domain characteristics and user needs may influence the selection and visualization of explanations.\nThis paper presents a design study where we investigated how to select and visualize AI explanations for domain users.\nBuilding on the nested design model of visualization, we incorporate XAI design considerations from a literature review and our collaborators' feedback into the design process.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    }
                ]
            },
            {
                "title": "ML for VIS",
                "session_id": "full17",
                "event_prefix": "v-full",
                "track": "ok4",
                "livestream_id": "ok4-thu",
                "session_image": "full17.png",
                "chair": [
                    "Daniel J\u00f6nsson"
                ],
                "organizers": [],
                "time_start": "2022-10-20T14:00:00Z",
                "time_end": "2022-10-20T15:15:00Z",
                "discord_category": "",
                "discord_channel": "",
                "discord_channel_id": "",
                "discord_link": "",
                "slido_link": "",
                "youtube_url": "",
                "zoom_meeting": "",
                "zoom_password": "",
                "zoom_link": "",
                "ff_link": "",
                "ff_playlist": "",
                "time_slots": [
                    {
                        "slot_id": "full17-opening",
                        "session_id": "full17",
                        "type": "In Person Other",
                        "title": "Session Opening",
                        "contributors": [
                            "Daniel J\u00f6nsson"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "",
                        "file_name": "",
                        "time_stamp": "2022-10-20T14:00:00Z",
                        "time_start": "2022-10-20T14:00:00Z",
                        "time_end": "2022-10-20T14:00:00Z",
                        "paper_type": "",
                        "keywords": "",
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-tvcg-9495259-pres",
                        "session_id": "full17",
                        "type": "Virtual Presentation (pre-recorded)",
                        "title": "AI4VIS: Survey on Artificial Intelligence Approaches for Data Visualization",
                        "contributors": [
                            "Aoyu Wu"
                        ],
                        "authors": [
                            "Aoyu Wu",
                            "Yun Wang",
                            "Xinhuan Shu",
                            "Dominik Moritz",
                            "Weiwei Cui",
                            "Haidong Zhang",
                            "Dongmei Zhang",
                            "Huamin Qu"
                        ],
                        "abstract": "Visualizations themselves have become a data format. Akin to other data formats such as text and images, visualizations are increasingly created, stored, shared, and (re-)used with artificial intelligence (AI) techniques. In this survey, we probe the underlying vision of formalizing visualizations as an emerging data format and review the recent advance in applying AI techniques to visualization data (AI4VIS). We define visualization data as the digital representations of visualizations in computers and focus on data visualization (e.g., charts and infographics). We build our survey upon a corpus spanning ten different fields in computer science with an eye toward identifying important common interests. Our resulting taxonomy is organized around WHAT is visualization data and its representation, WHY and HOW to apply AI to visualization data. We highlight a set of common tasks that researchers apply to the visualization data and present a detailed discussion of AI approaches developed to accomplish those tasks. Drawing upon our literature review, we discuss several important research questions surrounding the management and exploitation of visualization data, as well as the role of AI in support of those processes.",
                        "uid": "v-tvcg-9495259",
                        "file_name": "",
                        "time_stamp": "2022-10-20T14:00:00Z",
                        "time_start": "2022-10-20T14:00:00Z",
                        "time_end": "2022-10-20T14:13:00Z",
                        "paper_type": "full",
                        "keywords": "Survey; Data Visualization; Artificial Intelligence; Data Format; Machine Learning",
                        "has_image": "1",
                        "has_video": "607",
                        "paper_award": "",
                        "image_caption": "We present a survey on AI for visualizations from a corpus of 98 papers spanning 10 different fields in computer science. We argue that visualizations are becoming a new data format (visualization data) and categorize the surveyed papers from three aspects: (A) What is the visualization data and its representation; (B) Why apply AI to visualization data; and (C) How to apply AI to visualization data, where we outline seven tasks and separately discuss corresponding AI approaches.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-tvcg-9495259-qa",
                        "session_id": "full17",
                        "type": "Virtual Q+A",
                        "title": "AI4VIS: Survey on Artificial Intelligence Approaches for Data Visualization (Q+A)",
                        "contributors": [
                            "Aoyu Wu"
                        ],
                        "authors": [],
                        "abstract": "Visualizations themselves have become a data format. Akin to other data formats such as text and images, visualizations are increasingly created, stored, shared, and (re-)used with artificial intelligence (AI) techniques. In this survey, we probe the underlying vision of formalizing visualizations as an emerging data format and review the recent advance in applying AI techniques to visualization data (AI4VIS). We define visualization data as the digital representations of visualizations in computers and focus on data visualization (e.g., charts and infographics). We build our survey upon a corpus spanning ten different fields in computer science with an eye toward identifying important common interests. Our resulting taxonomy is organized around WHAT is visualization data and its representation, WHY and HOW to apply AI to visualization data. We highlight a set of common tasks that researchers apply to the visualization data and present a detailed discussion of AI approaches developed to accomplish those tasks. Drawing upon our literature review, we discuss several important research questions surrounding the management and exploitation of visualization data, as well as the role of AI in support of those processes.",
                        "uid": "v-tvcg-9495259",
                        "file_name": "",
                        "time_stamp": "2022-10-20T14:13:00Z",
                        "time_start": "2022-10-20T14:13:00Z",
                        "time_end": "2022-10-20T14:15:00Z",
                        "paper_type": "full",
                        "keywords": "Survey; Data Visualization; Artificial Intelligence; Data Format; Machine Learning",
                        "has_image": "1",
                        "has_video": "607",
                        "paper_award": "",
                        "image_caption": "We present a survey on AI for visualizations from a corpus of 98 papers spanning 10 different fields in computer science. We argue that visualizations are becoming a new data format (visualization data) and categorize the surveyed papers from three aspects: (A) What is the visualization data and its representation; (B) Why apply AI to visualization data; and (C) How to apply AI to visualization data, where we outline seven tasks and separately discuss corresponding AI approaches.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-tvcg-9760126-pres",
                        "session_id": "full17",
                        "type": "In Person Presentation",
                        "title": "DL4SciVis: A State-of-the-Art Survey on Deep Learning for Scientific Visualization",
                        "contributors": [
                            "Chaoli Wang"
                        ],
                        "authors": [
                            "Chaoli Wang",
                            "Jun Han"
                        ],
                        "abstract": "Since 2016, we have witnessed the tremendous growth of artificial intelligence+visualization (AI+VIS) research. However, existing survey papers on AI+VIS focus on visual analytics and information visualization, not scientific visualization (SciVis). In this paper, we survey related deep learning (DL) works in SciVis, specifically in the direction of DL4SciVis: designing DL solutions for solving SciVis problems. To stay focused, we primarily consider works that handle scalar and vector field data but exclude mesh data. We classify and discuss these works along six dimensions: domain setting, research task, learning type, network architecture, loss function, and evaluation metric. The paper concludes with a discussion of the remaining gaps to fill along the discussed dimensions and the grand challenges we need to tackle as a community. This state-of-the-art survey guides SciVis researchers in gaining an overview of this emerging topic and points out future directions to grow this research.",
                        "uid": "v-tvcg-9760126",
                        "file_name": "",
                        "time_stamp": "2022-10-20T14:15:00Z",
                        "time_start": "2022-10-20T14:15:00Z",
                        "time_end": "2022-10-20T14:28:00Z",
                        "paper_type": "full",
                        "keywords": "Scientific visualization, deep learning, survey",
                        "has_image": "1",
                        "has_video": "568",
                        "paper_award": "",
                        "image_caption": "Selected visualization results generated from the authors' DL4SciVis works. From left to right: reconstruction of vector field data from representative streamlines (CG&A 2019), learning surface representation via a graph convolutional network (SurfNet, EuroVis 2022), generating super-resolution for vector field data (SSR-VFD, PacificVis 2020), and clustering and selecting stream surfaces via an autoencoder (FlowNet, TVCG 2020).",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-tvcg-9760126-qa",
                        "session_id": "full17",
                        "type": "In Person Q+A",
                        "title": "DL4SciVis: A State-of-the-Art Survey on Deep Learning for Scientific Visualization (Q+A)",
                        "contributors": [
                            "Chaoli Wang"
                        ],
                        "authors": [],
                        "abstract": "Since 2016, we have witnessed the tremendous growth of artificial intelligence+visualization (AI+VIS) research. However, existing survey papers on AI+VIS focus on visual analytics and information visualization, not scientific visualization (SciVis). In this paper, we survey related deep learning (DL) works in SciVis, specifically in the direction of DL4SciVis: designing DL solutions for solving SciVis problems. To stay focused, we primarily consider works that handle scalar and vector field data but exclude mesh data. We classify and discuss these works along six dimensions: domain setting, research task, learning type, network architecture, loss function, and evaluation metric. The paper concludes with a discussion of the remaining gaps to fill along the discussed dimensions and the grand challenges we need to tackle as a community. This state-of-the-art survey guides SciVis researchers in gaining an overview of this emerging topic and points out future directions to grow this research.",
                        "uid": "v-tvcg-9760126",
                        "file_name": "",
                        "time_stamp": "2022-10-20T14:28:00Z",
                        "time_start": "2022-10-20T14:28:00Z",
                        "time_end": "2022-10-20T14:30:00Z",
                        "paper_type": "full",
                        "keywords": "Scientific visualization, deep learning, survey",
                        "has_image": "1",
                        "has_video": "568",
                        "paper_award": "",
                        "image_caption": "Selected visualization results generated from the authors' DL4SciVis works. From left to right: reconstruction of vector field data from representative streamlines (CG&A 2019), learning surface representation via a graph convolutional network (SurfNet, EuroVis 2022), generating super-resolution for vector field data (SSR-VFD, PacificVis 2020), and clustering and selecting stream surfaces via an autoencoder (FlowNet, TVCG 2020).",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-tvcg-9523770-pres",
                        "session_id": "full17",
                        "type": "In Person Presentation",
                        "title": "A Survey on ML4VIS: Applying Machine Learning Advances to Data Visualization",
                        "contributors": [
                            "Wang, Qianwen"
                        ],
                        "authors": [
                            "Qianwen Wang",
                            "Zhutian Chen",
                            "Yong Wang",
                            "Huamin Qu"
                        ],
                        "abstract": "Inspired by the great success of machine learning (ML), researchers have applied ML techniques to visualizations to achieve a better design, development, and evaluation of visualizations. This branch of studies, known as ML4VIS, is gaining increasing research attention in recent years. To successfully adapt ML techniques for visualizations, a structured understanding of the integration of ML4VISis needed. In this paper, we systematically survey ML4VIS studies, aiming to answer two motivating questions: \"what visualization processes can be assisted by ML?\" and \"how ML techniques can be used to solve visualization problems?\" This survey reveals seven main processes where the employment of ML techniques can benefit visualizations: Data Processing4VIS, Data-VIS Mapping, Insight Communication, Style Imitation, VIS Interaction, VIS Reading, and User Profiling. The seven processes are related to existing visualization theoretical models in an ML4VIS pipeline, aiming to illuminate the role of ML-assisted visualization in general visualizations. Meanwhile, the seven processes are mapped into main learning tasks in ML to align the capabilities of ML with the needs in visualization. Current practices and future opportunities of ML4VIS are discussed in the context of the ML4VIS pipeline and the ML-VIS mapping. While more studies are still needed in the area of ML4VIS, we hope this paper can provide a stepping-stone for future exploration. A web-based interactive browser of this survey is available at https://ml4vis.github.io",
                        "uid": "v-tvcg-9523770",
                        "file_name": "",
                        "time_stamp": "2022-10-20T14:30:00Z",
                        "time_start": "2022-10-20T14:30:00Z",
                        "time_end": "2022-10-20T14:43:00Z",
                        "paper_type": "full",
                        "keywords": "ML4VIS, Machine Learning, Data Visualization, Survey",
                        "has_image": "1",
                        "has_video": "623",
                        "paper_award": "",
                        "image_caption": "To successfully adapt ML techniques for visualizations, a structured understanding of the integration of ML4VISis needed. In this paper, we systematically survey 88 ML4VIS studies, aiming to answer two motivating questions: \"what visualization processes can be assisted by ML?\" and \"how ML techniques can be used to solve visualization problems?\"",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-tvcg-9523770-qa",
                        "session_id": "full17",
                        "type": "In Person Q+A",
                        "title": "A Survey on ML4VIS: Applying Machine Learning Advances to Data Visualization (Q+A)",
                        "contributors": [
                            "Wang, Qianwen"
                        ],
                        "authors": [],
                        "abstract": "Inspired by the great success of machine learning (ML), researchers have applied ML techniques to visualizations to achieve a better design, development, and evaluation of visualizations. This branch of studies, known as ML4VIS, is gaining increasing research attention in recent years. To successfully adapt ML techniques for visualizations, a structured understanding of the integration of ML4VISis needed. In this paper, we systematically survey ML4VIS studies, aiming to answer two motivating questions: \"what visualization processes can be assisted by ML?\" and \"how ML techniques can be used to solve visualization problems?\" This survey reveals seven main processes where the employment of ML techniques can benefit visualizations: Data Processing4VIS, Data-VIS Mapping, Insight Communication, Style Imitation, VIS Interaction, VIS Reading, and User Profiling. The seven processes are related to existing visualization theoretical models in an ML4VIS pipeline, aiming to illuminate the role of ML-assisted visualization in general visualizations. Meanwhile, the seven processes are mapped into main learning tasks in ML to align the capabilities of ML with the needs in visualization. Current practices and future opportunities of ML4VIS are discussed in the context of the ML4VIS pipeline and the ML-VIS mapping. While more studies are still needed in the area of ML4VIS, we hope this paper can provide a stepping-stone for future exploration. A web-based interactive browser of this survey is available at https://ml4vis.github.io",
                        "uid": "v-tvcg-9523770",
                        "file_name": "",
                        "time_stamp": "2022-10-20T14:43:00Z",
                        "time_start": "2022-10-20T14:43:00Z",
                        "time_end": "2022-10-20T14:45:00Z",
                        "paper_type": "full",
                        "keywords": "ML4VIS, Machine Learning, Data Visualization, Survey",
                        "has_image": "1",
                        "has_video": "623",
                        "paper_award": "",
                        "image_caption": "To successfully adapt ML techniques for visualizations, a structured understanding of the integration of ML4VISis needed. In this paper, we systematically survey 88 ML4VIS studies, aiming to answer two motivating questions: \"what visualization processes can be assisted by ML?\" and \"how ML techniques can be used to solve visualization problems?\"",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-tvcg-9706326-pres",
                        "session_id": "full17",
                        "type": "Virtual Presentation (pre-recorded)",
                        "title": "Reinforcement Learning for Load-balanced Parallel Particle Tracing",
                        "contributors": [
                            "Jiayi Xu"
                        ],
                        "authors": [
                            "Jiayi Xu",
                            "Hanqi Guo",
                            "Han-Wei Shen",
                            "Mukund Raj",
                            "Skylar W. Wurster",
                            "Tom Peterka"
                        ],
                        "abstract": "We explore an online reinforcement learning (RL) paradigm to dynamically optimize parallel particle tracing performance in distributed-memory systems. Our method combines three novel components: (1) a work donation algorithm, (2) a high-order workload estimation model, and (3) a communication cost model. First, we design an RL-based work donation algorithm. Our algorithm monitors workloads of processes and creates RL agents to donate data blocks and particles from high-workload processes to low-workload processes to minimize program execution time. The agents learn the donation strategy on the fly based on reward and cost functions designed to consider processes' workload changes and data transfer costs of donation actions. Second, we propose a workload estimation model, helping RL agents estimate the workload distribution of processes in future computations. Third, we design a communication cost model that considers both block and particle data exchange costs, helping RL agents make effective decisions with minimized communication costs. We demonstrate that our algorithm adapts to different flow behaviors in large-scale fluid dynamics, ocean, and weather simulation data. Our algorithm improves parallel particle tracing performance in terms of parallel efficiency, load balance, and costs of I/O and communication for evaluations with up to 16,384 processors.",
                        "uid": "v-tvcg-9706326",
                        "file_name": "",
                        "time_stamp": "2022-10-20T14:45:00Z",
                        "time_start": "2022-10-20T14:45:00Z",
                        "time_end": "2022-10-20T14:58:00Z",
                        "paper_type": "full",
                        "keywords": "Distributed and parallel particle tracing, dynamic load balancing, reinforcement learning.",
                        "has_image": "1",
                        "has_video": "604",
                        "paper_award": "",
                        "image_caption": "Reinforcement Learning for Load-balanced Parallel Particle Tracing",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-tvcg-9706326-qa",
                        "session_id": "full17",
                        "type": "Virtual Q+A",
                        "title": "Reinforcement Learning for Load-balanced Parallel Particle Tracing (Q+A)",
                        "contributors": [
                            "Jiayi Xu"
                        ],
                        "authors": [],
                        "abstract": "We explore an online reinforcement learning (RL) paradigm to dynamically optimize parallel particle tracing performance in distributed-memory systems. Our method combines three novel components: (1) a work donation algorithm, (2) a high-order workload estimation model, and (3) a communication cost model. First, we design an RL-based work donation algorithm. Our algorithm monitors workloads of processes and creates RL agents to donate data blocks and particles from high-workload processes to low-workload processes to minimize program execution time. The agents learn the donation strategy on the fly based on reward and cost functions designed to consider processes' workload changes and data transfer costs of donation actions. Second, we propose a workload estimation model, helping RL agents estimate the workload distribution of processes in future computations. Third, we design a communication cost model that considers both block and particle data exchange costs, helping RL agents make effective decisions with minimized communication costs. We demonstrate that our algorithm adapts to different flow behaviors in large-scale fluid dynamics, ocean, and weather simulation data. Our algorithm improves parallel particle tracing performance in terms of parallel efficiency, load balance, and costs of I/O and communication for evaluations with up to 16,384 processors.",
                        "uid": "v-tvcg-9706326",
                        "file_name": "",
                        "time_stamp": "2022-10-20T14:58:00Z",
                        "time_start": "2022-10-20T14:58:00Z",
                        "time_end": "2022-10-20T15:00:00Z",
                        "paper_type": "full",
                        "keywords": "Distributed and parallel particle tracing, dynamic load balancing, reinforcement learning.",
                        "has_image": "1",
                        "has_video": "604",
                        "paper_award": "",
                        "image_caption": "Reinforcement Learning for Load-balanced Parallel Particle Tracing",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-full-1018-pres",
                        "session_id": "full17",
                        "type": "In Person Presentation",
                        "title": "IDL: An Importance-Driven Latent Generation Method for Scientific Data",
                        "contributors": [
                            "JINGYI SHEN"
                        ],
                        "authors": [
                            "JINGYI SHEN",
                            "Haoyu Li",
                            "Jiayi Xu",
                            "Ayan Biswas",
                            "Han-Wei Shen"
                        ],
                        "abstract": "",
                        "uid": "v-full-1018",
                        "file_name": "",
                        "time_stamp": "2022-10-20T15:00:00Z",
                        "time_start": "2022-10-20T15:00:00Z",
                        "time_end": "2022-10-20T15:13:00Z",
                        "paper_type": "full",
                        "keywords": "",
                        "has_image": "1",
                        "has_video": "554",
                        "paper_award": "",
                        "image_caption": "We present an importance-driven latent generation method (IDLat) based on an autoencoder model which tightly relates latent representations to specific data of interest, such as salient regions or features of interest. With a trained model, scientists can flexibly define various importance criteria and obtain different latent representations. We further reduce the latent size through a lossless entropy coding model. In addition, we develop a visual exploration tool for latent space analysis and demonstrate the efficiency of identifying and analyzing feature regions with importance-driven latent representations.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-full-1018-qa",
                        "session_id": "full17",
                        "type": "In Person Q+A",
                        "title": "IDL: An Importance-Driven Latent Generation Method for Scientific Data (Q+A)",
                        "contributors": [
                            "JINGYI SHEN"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "v-full-1018",
                        "file_name": "",
                        "time_stamp": "2022-10-20T15:13:00Z",
                        "time_start": "2022-10-20T15:13:00Z",
                        "time_end": "2022-10-20T15:15:00Z",
                        "paper_type": "full",
                        "keywords": "",
                        "has_image": "1",
                        "has_video": "554",
                        "paper_award": "",
                        "image_caption": "We present an importance-driven latent generation method (IDLat) based on an autoencoder model which tightly relates latent representations to specific data of interest, such as salient regions or features of interest. With a trained model, scientists can flexibly define various importance criteria and obtain different latent representations. We further reduce the latent size through a lossless entropy coding model. In addition, we develop a visual exploration tool for latent space analysis and demonstrate the efficiency of identifying and analyzing feature regions with importance-driven latent representations.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-full-1033-pres",
                        "session_id": "full17",
                        "type": "Virtual Presentation (pre-recorded)",
                        "title": "DashBot: Insight-Driven Dashboard Generation Based on Deep Reinforcement Learning",
                        "contributors": [
                            "Dazhen Deng"
                        ],
                        "authors": [
                            "Dazhen Deng",
                            "Aoyu Wu",
                            "Huamin Qu",
                            "Yingcai Wu"
                        ],
                        "abstract": "",
                        "uid": "v-full-1033",
                        "file_name": "",
                        "time_stamp": "2022-10-20T15:15:00Z",
                        "time_start": "2022-10-20T15:15:00Z",
                        "time_end": "2022-10-20T15:28:00Z",
                        "paper_type": "full",
                        "keywords": "",
                        "has_image": "1",
                        "has_video": "631",
                        "paper_award": "",
                        "image_caption": "Screenshot of the DashBot Interface showing the generation of a dashboard named \u201cInsights about wind in seattle-weather\u201d. The interface consists of a table view (A), a topic list (B), a chart editor (C), a canvas view (D), and a recommendation view (E).",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-full-1033-qa",
                        "session_id": "full17",
                        "type": "Virtual Q+A",
                        "title": "DashBot: Insight-Driven Dashboard Generation Based on Deep Reinforcement Learning (Q+A)",
                        "contributors": [
                            "Dazhen Deng"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "v-full-1033",
                        "file_name": "",
                        "time_stamp": "2022-10-20T15:28:00Z",
                        "time_start": "2022-10-20T15:28:00Z",
                        "time_end": "2022-10-20T15:30:00Z",
                        "paper_type": "full",
                        "keywords": "",
                        "has_image": "1",
                        "has_video": "631",
                        "paper_award": "",
                        "image_caption": "Screenshot of the DashBot Interface showing the generation of a dashboard named \u201cInsights about wind in seattle-weather\u201d. The interface consists of a table view (A), a topic list (B), a chart editor (C), a canvas view (D), and a recommendation view (E).",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    }
                ]
            },
            {
                "title": "VA and ML",
                "session_id": "full18",
                "event_prefix": "v-full",
                "track": "ok4",
                "livestream_id": "ok4-wed",
                "session_image": "full18.png",
                "chair": [
                    "Cagatay Turkay"
                ],
                "organizers": [],
                "time_start": "2022-10-19T14:00:00Z",
                "time_end": "2022-10-19T15:15:00Z",
                "discord_category": "",
                "discord_channel": "",
                "discord_channel_id": "",
                "discord_link": "",
                "slido_link": "",
                "youtube_url": "",
                "zoom_meeting": "",
                "zoom_password": "",
                "zoom_link": "",
                "ff_link": "",
                "ff_playlist": "",
                "time_slots": [
                    {
                        "slot_id": "full18-opening",
                        "session_id": "full18",
                        "type": "In Person Other",
                        "title": "Session Opening",
                        "contributors": [
                            "Cagatay Turkay"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "",
                        "file_name": "",
                        "time_stamp": "2022-10-19T14:00:00Z",
                        "time_start": "2022-10-19T14:00:00Z",
                        "time_end": "2022-10-19T14:00:00Z",
                        "paper_type": "",
                        "keywords": "",
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-tvcg-9497654-pres",
                        "session_id": "full18",
                        "type": "Virtual Presentation (pre-recorded)",
                        "title": "Survey on Visual Analysis of Event Sequence Data",
                        "contributors": [
                            "Yi Guo"
                        ],
                        "authors": [
                            "Yi Guo",
                            "Shunan Guo",
                            "Zhuochen Jin",
                            "Smiti Kaul",
                            "David Gotz",
                            "Nan Cao"
                        ],
                        "abstract": "Event sequence data record series of discrete events in the time order of occurrence. They are commonly observed in a variety of applications ranging from electronic health records to network logs, with the characteristics of large-scale, high-dimensional and heterogeneous. This high complexity of event sequence data makes it difficult for analysts to manually explore and find patterns, resulting in ever-increasing needs for computational and perceptual aids from visual analytics techniques to extract and communicate insights from event sequence datasets. In this paper, we review the state-of-the-art visual analytics approaches, characterize them with our proposed design space, and categorize them based on analytical tasks and applications. From our review of relevant literature, we have also identified several remaining research challenges and future research opportunities.",
                        "uid": "v-tvcg-9497654",
                        "file_name": "",
                        "time_stamp": "2022-10-19T14:00:00Z",
                        "time_start": "2022-10-19T14:00:00Z",
                        "time_end": "2022-10-19T14:13:00Z",
                        "paper_type": "full",
                        "keywords": "Visual Analysis, Event Sequence Data, Visualization",
                        "has_image": "1",
                        "has_video": "528",
                        "paper_award": "",
                        "image_caption": "The design spaces of visual analytics techniques for event sequence data include four dimensions: data scale, automated sequence analysis, visual representation, interaction technique.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-tvcg-9497654-qa",
                        "session_id": "full18",
                        "type": "Virtual Q+A",
                        "title": "Survey on Visual Analysis of Event Sequence Data (Q+A)",
                        "contributors": [
                            "Yi Guo"
                        ],
                        "authors": [],
                        "abstract": "Event sequence data record series of discrete events in the time order of occurrence. They are commonly observed in a variety of applications ranging from electronic health records to network logs, with the characteristics of large-scale, high-dimensional and heterogeneous. This high complexity of event sequence data makes it difficult for analysts to manually explore and find patterns, resulting in ever-increasing needs for computational and perceptual aids from visual analytics techniques to extract and communicate insights from event sequence datasets. In this paper, we review the state-of-the-art visual analytics approaches, characterize them with our proposed design space, and categorize them based on analytical tasks and applications. From our review of relevant literature, we have also identified several remaining research challenges and future research opportunities.",
                        "uid": "v-tvcg-9497654",
                        "file_name": "",
                        "time_stamp": "2022-10-19T14:13:00Z",
                        "time_start": "2022-10-19T14:13:00Z",
                        "time_end": "2022-10-19T14:15:00Z",
                        "paper_type": "full",
                        "keywords": "Visual Analysis, Event Sequence Data, Visualization",
                        "has_image": "1",
                        "has_video": "528",
                        "paper_award": "",
                        "image_caption": "The design spaces of visual analytics techniques for event sequence data include four dimensions: data scale, automated sequence analysis, visual representation, interaction technique.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-tvcg-9664269-pres",
                        "session_id": "full18",
                        "type": "Virtual Presentation (pre-recorded)",
                        "title": "Towards Better Caption Supervision for Object Detection",
                        "contributors": [
                            "Shixia Liu"
                        ],
                        "authors": [
                            "Changjian Chen",
                            "Jing Wu",
                            "Xiaohan Wang",
                            "Shouxing Xiang",
                            "Song-Hai Zhang",
                            "Qifeng Tang",
                            "Shixia Liu"
                        ],
                        "abstract": "As training high-performance object detectors requires expensive bounding box annotations, recent methods resort to free available image captions. However, detectors trained on caption supervision perform poorly because captions are usually noisy and cannot provide precise location information. To tackle this issue, we present a visual analysis method, which tightly integrates caption supervision with object detection to mutually enhance each other. In particular, object labels are first extracted from captions, which are utilized to train the detectors. Then, the label information from images is fed into caption supervision for further improvement. To effectively loop users into the object detection process, a node-link-based set visualization supported by a multi-type relational co-clustering algorithm is developed to explain the relationships between the extracted labels and the images with detected objects. The co-clustering algorithm clusters labels and images simultaneously by utilizing both their representations and their relationships. Quantitative evaluations and a case study are conducted to demonstrate the efficiency and effectiveness of the developed method in improving the performance of object detectors.",
                        "uid": "v-tvcg-9664269",
                        "file_name": "",
                        "time_stamp": "2022-10-19T14:15:00Z",
                        "time_start": "2022-10-19T14:15:00Z",
                        "time_end": "2022-10-19T14:28:00Z",
                        "paper_type": "full",
                        "keywords": "Machine learning, interactive visualization, object detection, caption supervision, co-clustering.",
                        "has_image": "1",
                        "has_video": "488",
                        "paper_award": "",
                        "image_caption": "MutualDetector: (a) a node-link-based set visualization consists of a tree of labels (1), the relationships between the labels and image clusters (2), and a matrix (3) to show the representative images with the detected objects for each cluster; (b) an information panel to show important words, captions, and selected images.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-tvcg-9664269-qa",
                        "session_id": "full18",
                        "type": "Virtual Q+A",
                        "title": "Towards Better Caption Supervision for Object Detection (Q+A)",
                        "contributors": [
                            "Shixia Liu"
                        ],
                        "authors": [],
                        "abstract": "As training high-performance object detectors requires expensive bounding box annotations, recent methods resort to free available image captions. However, detectors trained on caption supervision perform poorly because captions are usually noisy and cannot provide precise location information. To tackle this issue, we present a visual analysis method, which tightly integrates caption supervision with object detection to mutually enhance each other. In particular, object labels are first extracted from captions, which are utilized to train the detectors. Then, the label information from images is fed into caption supervision for further improvement. To effectively loop users into the object detection process, a node-link-based set visualization supported by a multi-type relational co-clustering algorithm is developed to explain the relationships between the extracted labels and the images with detected objects. The co-clustering algorithm clusters labels and images simultaneously by utilizing both their representations and their relationships. Quantitative evaluations and a case study are conducted to demonstrate the efficiency and effectiveness of the developed method in improving the performance of object detectors.",
                        "uid": "v-tvcg-9664269",
                        "file_name": "",
                        "time_stamp": "2022-10-19T14:28:00Z",
                        "time_start": "2022-10-19T14:28:00Z",
                        "time_end": "2022-10-19T14:30:00Z",
                        "paper_type": "full",
                        "keywords": "Machine learning, interactive visualization, object detection, caption supervision, co-clustering.",
                        "has_image": "1",
                        "has_video": "488",
                        "paper_award": "",
                        "image_caption": "MutualDetector: (a) a node-link-based set visualization consists of a tree of labels (1), the relationships between the labels and image clusters (2), and a matrix (3) to show the representative images with the detected objects for each cluster; (b) an information panel to show important words, captions, and selected images.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-tvcg-9672706-pres",
                        "session_id": "full18",
                        "type": "In Person Presentation",
                        "title": "FeatureEnVi: Visual Analytics for Feature Engineering Using Stepwise Selection and Semi-Automatic Extraction Approaches",
                        "contributors": [
                            "Angelos Chatzimparmpas"
                        ],
                        "authors": [
                            "Angelos Chatzimparmpas",
                            "Rafael M. Martins",
                            "Kostiantyn Kucher",
                            "Andreas Kerren"
                        ],
                        "abstract": "The machine learning (ML) life cycle involves a series of iterative steps, from the effective gathering and preparation of the data\u2014including complex feature engineering processes\u2014to the presentation and improvement of results, with various algorithms to choose from in every step. Feature engineering in particular can be very beneficial for ML, leading to numerous improvements such as boosting the predictive results, decreasing computational times, reducing excessive noise, and increasing the transparency behind the decisions taken during the training. Despite that, while several visual analytics tools exist to monitor and control the different stages of the ML life cycle (especially those related to data and algorithms), feature engineering support remains inadequate. In this paper, we present FeatureEnVi, a visual analytics system specifically designed to assist with the feature engineering process. Our proposed system helps users to choose the most important feature, to transform the original features into powerful alternatives, and to experiment with different feature generation combinations. Additionally, data space slicing allows users to explore the impact of features on both local and global scales. FeatureEnVi utilizes multiple automatic feature selection techniques; furthermore, it visually guides users with statistical evidence about the influence of each feature (or subsets of features). The final outcome is the extraction of heavily engineered features, evaluated by multiple validation metrics. The usefulness and applicability of FeatureEnVi are demonstrated with two use cases and a case study. We also report feedback from interviews with two ML experts and a visualization researcher who assessed the effectiveness of our system.",
                        "uid": "v-tvcg-9672706",
                        "file_name": "",
                        "time_stamp": "2022-10-19T14:30:00Z",
                        "time_start": "2022-10-19T14:30:00Z",
                        "time_end": "2022-10-19T14:43:00Z",
                        "paper_type": "full",
                        "keywords": "Feature selection, feature extraction, feature engineering, machine learning, visual analytics, visualization",
                        "has_image": "1",
                        "has_video": "651",
                        "paper_award": "",
                        "image_caption": "Selecting important features, transforming them, and generating new features with FeatureEnVi: (a) the plot for manually slicing the data space and continuously checking the migration of instances; (b) the view for the selection of features according to multiple feature importances; (c) provides an overview of the features with statistical measures for the different groups of instances; (d) the visualization for the detailed exploration of features, their transformation, and comparison between features for feature generation purposes; and (e) the punchcard for tracking the steps of the process and the grouped bar chart for comparing the current versus the best predictive performance.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-tvcg-9672706-qa",
                        "session_id": "full18",
                        "type": "In Person Q+A",
                        "title": "FeatureEnVi: Visual Analytics for Feature Engineering Using Stepwise Selection and Semi-Automatic Extraction Approaches (Q+A)",
                        "contributors": [
                            "Angelos Chatzimparmpas"
                        ],
                        "authors": [],
                        "abstract": "The machine learning (ML) life cycle involves a series of iterative steps, from the effective gathering and preparation of the data\u2014including complex feature engineering processes\u2014to the presentation and improvement of results, with various algorithms to choose from in every step. Feature engineering in particular can be very beneficial for ML, leading to numerous improvements such as boosting the predictive results, decreasing computational times, reducing excessive noise, and increasing the transparency behind the decisions taken during the training. Despite that, while several visual analytics tools exist to monitor and control the different stages of the ML life cycle (especially those related to data and algorithms), feature engineering support remains inadequate. In this paper, we present FeatureEnVi, a visual analytics system specifically designed to assist with the feature engineering process. Our proposed system helps users to choose the most important feature, to transform the original features into powerful alternatives, and to experiment with different feature generation combinations. Additionally, data space slicing allows users to explore the impact of features on both local and global scales. FeatureEnVi utilizes multiple automatic feature selection techniques; furthermore, it visually guides users with statistical evidence about the influence of each feature (or subsets of features). The final outcome is the extraction of heavily engineered features, evaluated by multiple validation metrics. The usefulness and applicability of FeatureEnVi are demonstrated with two use cases and a case study. We also report feedback from interviews with two ML experts and a visualization researcher who assessed the effectiveness of our system.",
                        "uid": "v-tvcg-9672706",
                        "file_name": "",
                        "time_stamp": "2022-10-19T14:43:00Z",
                        "time_start": "2022-10-19T14:43:00Z",
                        "time_end": "2022-10-19T14:45:00Z",
                        "paper_type": "full",
                        "keywords": "Feature selection, feature extraction, feature engineering, machine learning, visual analytics, visualization",
                        "has_image": "1",
                        "has_video": "651",
                        "paper_award": "",
                        "image_caption": "Selecting important features, transforming them, and generating new features with FeatureEnVi: (a) the plot for manually slicing the data space and continuously checking the migration of instances; (b) the view for the selection of features according to multiple feature importances; (c) provides an overview of the features with statistical measures for the different groups of instances; (d) the visualization for the detailed exploration of features, their transformation, and comparison between features for feature generation purposes; and (e) the punchcard for tracking the steps of the process and the grouped bar chart for comparing the current versus the best predictive performance.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-full-1569-pres",
                        "session_id": "full18",
                        "type": "In Person Presentation",
                        "title": "A Design Space for Surfacing Content Recommendations in Visual Analytic Platforms",
                        "contributors": [
                            "David Gotz",
                            "Zhilan Zhou"
                        ],
                        "authors": [
                            "Zhilan Zhou",
                            "Wenyuan Wang",
                            "Mengtian Guo",
                            "Yue Wang",
                            "David Gotz"
                        ],
                        "abstract": "",
                        "uid": "v-full-1569",
                        "file_name": "",
                        "time_stamp": "2022-10-19T14:45:00Z",
                        "time_start": "2022-10-19T14:45:00Z",
                        "time_end": "2022-10-19T14:58:00Z",
                        "paper_type": "full",
                        "keywords": "",
                        "has_image": "1",
                        "has_video": "482",
                        "paper_award": "",
                        "image_caption": "A hypothetical visual analytics platform with an interface includes two tabs: one with visualizations and another with a text-based list. A pop-up window is on top of the interface with content recommendations.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-full-1569-qa",
                        "session_id": "full18",
                        "type": "In Person Q+A",
                        "title": "A Design Space for Surfacing Content Recommendations in Visual Analytic Platforms (Q+A)",
                        "contributors": [
                            "David Gotz",
                            "Zhilan Zhou"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "v-full-1569",
                        "file_name": "",
                        "time_stamp": "2022-10-19T14:58:00Z",
                        "time_start": "2022-10-19T14:58:00Z",
                        "time_end": "2022-10-19T15:00:00Z",
                        "paper_type": "full",
                        "keywords": "",
                        "has_image": "1",
                        "has_video": "482",
                        "paper_award": "",
                        "image_caption": "A hypothetical visual analytics platform with an interface includes two tabs: one with visualizations and another with a text-based list. A pop-up window is on top of the interface with content recommendations.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-full-1452-pres",
                        "session_id": "full18",
                        "type": "In Person Presentation",
                        "title": "Diverse Interaction Recommendation for Public Users Exploring Multi-view Visualization using Deep Learning",
                        "contributors": [
                            "Yixuan Li"
                        ],
                        "authors": [
                            "Yixuan Li",
                            "Yusheng Qi",
                            "Yang Shi",
                            "Qing Chen",
                            "Nan Cao",
                            "Siming Chen"
                        ],
                        "abstract": "",
                        "uid": "v-full-1452",
                        "file_name": "",
                        "time_stamp": "2022-10-19T15:00:00Z",
                        "time_start": "2022-10-19T15:00:00Z",
                        "time_end": "2022-10-19T15:13:00Z",
                        "paper_type": "full",
                        "keywords": "",
                        "has_image": "1",
                        "has_video": "700",
                        "paper_award": "",
                        "image_caption": "In this work, we introduced a method for making real-time, insightful, and diverse interaction recommendations in visualization systems with multiple views and interaction types. The model performs as an end-to-end pipeline with users\u2019 Previous Interaction logs and Visual States as the input and Recommendations of the next step interaction as the output.\nIn the user's workflow, each time the user make one interaction, the system collects the interaction and visual state and then transfers them into numerical vectors. In the predicting workflow, the pre-trained models receive the data and output predicted interaction vectors. After being converted to JSON data, the recommended interactions will be sent back.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-full-1452-qa",
                        "session_id": "full18",
                        "type": "In Person Q+A",
                        "title": "Diverse Interaction Recommendation for Public Users Exploring Multi-view Visualization using Deep Learning (Q+A)",
                        "contributors": [
                            "Yixuan Li"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "v-full-1452",
                        "file_name": "",
                        "time_stamp": "2022-10-19T15:13:00Z",
                        "time_start": "2022-10-19T15:13:00Z",
                        "time_end": "2022-10-19T15:15:00Z",
                        "paper_type": "full",
                        "keywords": "",
                        "has_image": "1",
                        "has_video": "700",
                        "paper_award": "",
                        "image_caption": "In this work, we introduced a method for making real-time, insightful, and diverse interaction recommendations in visualization systems with multiple views and interaction types. The model performs as an end-to-end pipeline with users\u2019 Previous Interaction logs and Visual States as the input and Recommendations of the next step interaction as the output.\nIn the user's workflow, each time the user make one interaction, the system collects the interaction and visual state and then transfers them into numerical vectors. In the predicting workflow, the pre-trained models receive the data and output predicted interaction vectors. After being converted to JSON data, the recommended interactions will be sent back.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-full-1674-pres",
                        "session_id": "full18",
                        "type": "In Person Presentation",
                        "title": "Visinity: Visual Spatial Neighborhood Analysis for Multiplexed Tissue Imaging Data",
                        "contributors": [
                            "Simon Alexander Warchol"
                        ],
                        "authors": [
                            "Simon Alexander Warchol",
                            "Robert Kr\u00fcger",
                            "Ajit Johnson Nirmal",
                            "Giorgio Gaglia",
                            "Jared Jessup Jessup",
                            "Cecily C. Ritch",
                            "John Hoffer",
                            "Jeremy Muhlich",
                            "Megan L Burger",
                            "Tyler Jacks",
                            "Sandro Santagata Santagata",
                            "Peter Sorger",
                            "Hanspeter Pfister"
                        ],
                        "abstract": "",
                        "uid": "v-full-1674",
                        "file_name": "",
                        "time_stamp": "2022-10-19T15:15:00Z",
                        "time_start": "2022-10-19T15:15:00Z",
                        "time_end": "2022-10-19T15:28:00Z",
                        "paper_type": "full",
                        "keywords": "",
                        "has_image": "1",
                        "has_video": "989",
                        "paper_award": "",
                        "image_caption": "Visinity is a visual analytics system for spatial neighborhood analysis across cohorts of gigapixel whole-slide tissue images, developed in direct collaboration with pathologists and cell biologists. Our approach is based on a fast and scalable spatial neighborhood computation and leverages unsupervised learning to quantify, compare, and group cells by their surrounding cellular neighborhood. To test, verify, and refine hypotheses, experts can visually query for spatial neighborhood patterns, determine their presence and statistical significance across specimens, and annotate and visually compare findings. We demonstrate that Visinity can be used to identify biologically relevant patterns in two case studies with biomedical experts.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-full-1674-qa",
                        "session_id": "full18",
                        "type": "In Person Q+A",
                        "title": "Visinity: Visual Spatial Neighborhood Analysis for Multiplexed Tissue Imaging Data (Q+A)",
                        "contributors": [
                            "Simon Alexander Warchol"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "v-full-1674",
                        "file_name": "",
                        "time_stamp": "2022-10-19T15:28:00Z",
                        "time_start": "2022-10-19T15:28:00Z",
                        "time_end": "2022-10-19T15:30:00Z",
                        "paper_type": "full",
                        "keywords": "",
                        "has_image": "1",
                        "has_video": "989",
                        "paper_award": "",
                        "image_caption": "Visinity is a visual analytics system for spatial neighborhood analysis across cohorts of gigapixel whole-slide tissue images, developed in direct collaboration with pathologists and cell biologists. Our approach is based on a fast and scalable spatial neighborhood computation and leverages unsupervised learning to quantify, compare, and group cells by their surrounding cellular neighborhood. To test, verify, and refine hypotheses, experts can visually query for spatial neighborhood patterns, determine their presence and statistical significance across specimens, and annotate and visually compare findings. We demonstrate that Visinity can be used to identify biologically relevant patterns in two case studies with biomedical experts.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    }
                ]
            },
            {
                "title": "VA for ML",
                "session_id": "full19",
                "event_prefix": "v-full",
                "track": "ok4",
                "livestream_id": "ok4-wed",
                "session_image": "full19.png",
                "chair": [
                    "Timo Ropinski"
                ],
                "organizers": [],
                "time_start": "2022-10-19T15:45:00Z",
                "time_end": "2022-10-19T17:00:00Z",
                "discord_category": "",
                "discord_channel": "",
                "discord_channel_id": "",
                "discord_link": "",
                "slido_link": "",
                "youtube_url": "",
                "zoom_meeting": "",
                "zoom_password": "",
                "zoom_link": "",
                "ff_link": "",
                "ff_playlist": "",
                "time_slots": [
                    {
                        "slot_id": "full19-opening",
                        "session_id": "full19",
                        "type": "In Person Other",
                        "title": "Session Opening",
                        "contributors": [
                            "Timo Ropinski"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "",
                        "file_name": "",
                        "time_stamp": "2022-10-19T15:45:00Z",
                        "time_start": "2022-10-19T15:45:00Z",
                        "time_end": "2022-10-19T15:45:00Z",
                        "paper_type": "",
                        "keywords": "",
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-tvcg-9705076-pres",
                        "session_id": "full19",
                        "type": "In Person Presentation",
                        "title": "GNNLens: A Visual Analytics Approach for Prediction Error Diagnosis of Graph Neural Networks",
                        "contributors": [
                            "Zhihua JIN"
                        ],
                        "authors": [
                            "Zhihua Jin",
                            "Yong Wang",
                            "Qianwen Wang",
                            "Yao Ming",
                            "Tengfei Ma",
                            "Huamin Qu"
                        ],
                        "abstract": "Graph Neural Networks (GNNs) aim to extend deep learning techniques to graph data and have achieved significant progress in graph analysis tasks (e.g., node classification) in recent years. However, similar to other deep neural networks like Convolutional Neural Networks (CNNs) and Recurrent Neural Networks (RNNs), GNNs behave like a black box with their details hidden from model developers and users. It is therefore difficult to diagnose possible errors of GNNs. Despite many visual analytics studies being done on CNNs and RNNs, little research has addressed the challenges for GNNs. This paper fills the research gap with an interactive visual analysis tool, GNNLens, to assist model developers and users in understanding and analyzing GNNs. Specifically, Parallel Sets View and Projection View enable users to quickly identify and validate error patterns in the set of wrong predictions; Graph View and Feature Matrix View offer a detailed analysis of individual nodes to assist users in forming hypotheses about the error patterns. Since GNNs jointly model the graph structure and the node features, we reveal the relative influences of the two types of information by comparing the predictions of three models: GNN, Multi-Layer Perceptron (MLP), and GNN Without Using Features (GNNWUF). Two case studies and interviews with domain experts demonstrate the effectiveness of GNNLens in facilitating the understanding of GNN models and their errors.",
                        "uid": "v-tvcg-9705076",
                        "file_name": "",
                        "time_stamp": "2022-10-19T15:45:00Z",
                        "time_start": "2022-10-19T15:45:00Z",
                        "time_end": "2022-10-19T15:58:00Z",
                        "paper_type": "full",
                        "keywords": "Graph Neural Networks, Error Diagnosis, Visualization",
                        "has_image": "1",
                        "has_video": "634",
                        "paper_award": "",
                        "image_caption": "GNNLens is a visual analytics tool that helps model developers and users understand and diagnose GNNs. GNNLens consists of four visualization components. Parallel Sets View (b) and Projection View (c) enable users to quickly identify and validate error patterns in the set of wrong predictions. Graph View (d) and Feature Matrix View (e) offer a detailed analysis of individual nodes to assist users in forming hypotheses about the error patterns. They are linked together to support users to analyze GNN models simultaneously from multiple angles and extract general error patterns in GNN prediction results.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-tvcg-9705076-qa",
                        "session_id": "full19",
                        "type": "In Person Q+A",
                        "title": "GNNLens: A Visual Analytics Approach for Prediction Error Diagnosis of Graph Neural Networks (Q+A)",
                        "contributors": [
                            "Zhihua JIN"
                        ],
                        "authors": [],
                        "abstract": "Graph Neural Networks (GNNs) aim to extend deep learning techniques to graph data and have achieved significant progress in graph analysis tasks (e.g., node classification) in recent years. However, similar to other deep neural networks like Convolutional Neural Networks (CNNs) and Recurrent Neural Networks (RNNs), GNNs behave like a black box with their details hidden from model developers and users. It is therefore difficult to diagnose possible errors of GNNs. Despite many visual analytics studies being done on CNNs and RNNs, little research has addressed the challenges for GNNs. This paper fills the research gap with an interactive visual analysis tool, GNNLens, to assist model developers and users in understanding and analyzing GNNs. Specifically, Parallel Sets View and Projection View enable users to quickly identify and validate error patterns in the set of wrong predictions; Graph View and Feature Matrix View offer a detailed analysis of individual nodes to assist users in forming hypotheses about the error patterns. Since GNNs jointly model the graph structure and the node features, we reveal the relative influences of the two types of information by comparing the predictions of three models: GNN, Multi-Layer Perceptron (MLP), and GNN Without Using Features (GNNWUF). Two case studies and interviews with domain experts demonstrate the effectiveness of GNNLens in facilitating the understanding of GNN models and their errors.",
                        "uid": "v-tvcg-9705076",
                        "file_name": "",
                        "time_stamp": "2022-10-19T15:58:00Z",
                        "time_start": "2022-10-19T15:58:00Z",
                        "time_end": "2022-10-19T16:00:00Z",
                        "paper_type": "full",
                        "keywords": "Graph Neural Networks, Error Diagnosis, Visualization",
                        "has_image": "1",
                        "has_video": "634",
                        "paper_award": "",
                        "image_caption": "GNNLens is a visual analytics tool that helps model developers and users understand and diagnose GNNs. GNNLens consists of four visualization components. Parallel Sets View (b) and Projection View (c) enable users to quickly identify and validate error patterns in the set of wrong predictions. Graph View (d) and Feature Matrix View (e) offer a detailed analysis of individual nodes to assist users in forming hypotheses about the error patterns. They are linked together to support users to analyze GNN models simultaneously from multiple angles and extract general error patterns in GNN prediction results.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-full-1119-pres",
                        "session_id": "full19",
                        "type": "Virtual Presentation (pre-recorded)",
                        "title": "Visual Analysis of Neural Architecture Spaces for Summarizing Design Principles",
                        "contributors": [
                            "Jun Yuan"
                        ],
                        "authors": [
                            "Jun Yuan",
                            "Mengchen Liu",
                            "Fengyuan Tian",
                            "Shixia Liu"
                        ],
                        "abstract": "",
                        "uid": "v-full-1119",
                        "file_name": "",
                        "time_stamp": "2022-10-19T16:00:00Z",
                        "time_start": "2022-10-19T16:00:00Z",
                        "time_end": "2022-10-19T16:13:00Z",
                        "paper_type": "full",
                        "keywords": "",
                        "has_image": "1",
                        "has_video": "510",
                        "paper_award": "",
                        "image_caption": "ArchExplorer: (a) the architecture visualization to show the architecture clusters; (b) three selected sub-clusters after zooming into cluster A; (c) a detailed comparison of the selected architectures.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-full-1119-qa",
                        "session_id": "full19",
                        "type": "Virtual Q+A",
                        "title": "Visual Analysis of Neural Architecture Spaces for Summarizing Design Principles (Q+A)",
                        "contributors": [
                            "Jun Yuan"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "v-full-1119",
                        "file_name": "",
                        "time_stamp": "2022-10-19T16:13:00Z",
                        "time_start": "2022-10-19T16:13:00Z",
                        "time_end": "2022-10-19T16:15:00Z",
                        "paper_type": "full",
                        "keywords": "",
                        "has_image": "1",
                        "has_video": "510",
                        "paper_award": "",
                        "image_caption": "ArchExplorer: (a) the architecture visualization to show the architecture clusters; (b) three selected sub-clusters after zooming into cluster A; (c) a detailed comparison of the selected architectures.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-full-1626-pres",
                        "session_id": "full19",
                        "type": "In Person Presentation",
                        "title": "NAS-Navigator: Visual Steering for Explainable One-Shot Deep Neural Network Synthesis",
                        "contributors": [
                            "Anjul Kumar Tyagi"
                        ],
                        "authors": [
                            "Anjul Kumar Tyagi",
                            "Cong Xie",
                            "Klaus Mueller"
                        ],
                        "abstract": "",
                        "uid": "v-full-1626",
                        "file_name": "",
                        "time_stamp": "2022-10-19T16:15:00Z",
                        "time_start": "2022-10-19T16:15:00Z",
                        "time_end": "2022-10-19T16:28:00Z",
                        "paper_type": "full",
                        "keywords": "",
                        "has_image": "1",
                        "has_video": "314",
                        "paper_award": "",
                        "image_caption": "NAS-Navigator is a human-in-the-loop, full-explainable system for Neural Network Architecture Search. Compared to full automated NAS techniques, NAS-Navigator allows users to control the NAS search and visualize the network architecture search space.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-full-1626-qa",
                        "session_id": "full19",
                        "type": "In Person Q+A",
                        "title": "NAS-Navigator: Visual Steering for Explainable One-Shot Deep Neural Network Synthesis (Q+A)",
                        "contributors": [
                            "Anjul Kumar Tyagi"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "v-full-1626",
                        "file_name": "",
                        "time_stamp": "2022-10-19T16:28:00Z",
                        "time_start": "2022-10-19T16:28:00Z",
                        "time_end": "2022-10-19T16:30:00Z",
                        "paper_type": "full",
                        "keywords": "",
                        "has_image": "1",
                        "has_video": "314",
                        "paper_award": "",
                        "image_caption": "NAS-Navigator is a human-in-the-loop, full-explainable system for Neural Network Architecture Search. Compared to full automated NAS techniques, NAS-Navigator allows users to control the NAS search and visualize the network architecture search space.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-full-1338-pres",
                        "session_id": "full19",
                        "type": "Virtual Presentation (pre-recorded)",
                        "title": "HetVis: A Visual Analysis Approach for Identifying Heterogeneity in Federated Learning",
                        "contributors": [
                            "Xumeng Wang"
                        ],
                        "authors": [
                            "Xumeng Wang",
                            "Wei Chen",
                            "Jiazhi Xia",
                            "Zhen Wen",
                            "Rongchen Zhu",
                            "Tobias Schreck"
                        ],
                        "abstract": "",
                        "uid": "v-full-1338",
                        "file_name": "",
                        "time_stamp": "2022-10-19T16:30:00Z",
                        "time_start": "2022-10-19T16:30:00Z",
                        "time_end": "2022-10-19T16:43:00Z",
                        "paper_type": "full",
                        "keywords": "",
                        "has_image": "1",
                        "has_video": "609",
                        "paper_award": "",
                        "image_caption": "We developed a visual analytics tool, HetVis, for participating clients in federated learning to explore data heterogeneity. We identify data heterogeneity by comparing prediction behaviors of the global federated model and the stand-alone model trained with local data. Then, a context-aware clustering of the inconsistent records is done, to provide a summary of data heterogeneity. To further explore the extracted heterogeneity issues and corresponding impacts, we leverage a suite of novel charts, as shown in the image.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-full-1338-qa",
                        "session_id": "full19",
                        "type": "Virtual Q+A",
                        "title": "HetVis: A Visual Analysis Approach for Identifying Heterogeneity in Federated Learning (Q+A)",
                        "contributors": [
                            "Xumeng Wang"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "v-full-1338",
                        "file_name": "",
                        "time_stamp": "2022-10-19T16:43:00Z",
                        "time_start": "2022-10-19T16:43:00Z",
                        "time_end": "2022-10-19T16:45:00Z",
                        "paper_type": "full",
                        "keywords": "",
                        "has_image": "1",
                        "has_video": "609",
                        "paper_award": "",
                        "image_caption": "We developed a visual analytics tool, HetVis, for participating clients in federated learning to explore data heterogeneity. We identify data heterogeneity by comparing prediction behaviors of the global federated model and the stand-alone model trained with local data. Then, a context-aware clustering of the inconsistent records is done, to provide a summary of data heterogeneity. To further explore the extracted heterogeneity issues and corresponding impacts, we leverage a suite of novel charts, as shown in the image.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-full-1205-pres",
                        "session_id": "full19",
                        "type": "In Person Presentation",
                        "title": "Visual Exploration of Large-Scale Image Datasets for Machine Learning with Treemaps",
                        "contributors": [
                            "Minsuk Kahng",
                            "Donald Bertucci"
                        ],
                        "authors": [
                            "Donald R Bertucci",
                            "Md Montaser Hamid",
                            "Yashwanthi Anand",
                            "Anita Ruangrotsakun",
                            "Delyar Tabatabai",
                            "Melissa Perez",
                            "Minsuk Kahng"
                        ],
                        "abstract": "",
                        "uid": "v-full-1205",
                        "file_name": "",
                        "time_stamp": "2022-10-19T16:45:00Z",
                        "time_start": "2022-10-19T16:45:00Z",
                        "time_end": "2022-10-19T16:58:00Z",
                        "paper_type": "full",
                        "keywords": "",
                        "has_image": "1",
                        "has_video": "331",
                        "paper_award": "",
                        "image_caption": "DendroMap is an interactive visualization system for exploring large-scale image datasets used in machine learning. The initial view of DendroMap shown on the left side of the figure presents an image dataset as nested rectangles of similar images, like treemaps, providing an overview of the entire dataset. A user clicks on a rectangle with plants, insects, and animals to zoom down into the hierarchy of those images. Then, DendroMap expands this portion to take up the space of the entire screen and shows new similar image groups.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-full-1205-qa",
                        "session_id": "full19",
                        "type": "In Person Q+A",
                        "title": "Visual Exploration of Large-Scale Image Datasets for Machine Learning with Treemaps (Q+A)",
                        "contributors": [
                            "Minsuk Kahng",
                            "Donald Bertucci"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "v-full-1205",
                        "file_name": "",
                        "time_stamp": "2022-10-19T16:58:00Z",
                        "time_start": "2022-10-19T16:58:00Z",
                        "time_end": "2022-10-19T17:00:00Z",
                        "paper_type": "full",
                        "keywords": "",
                        "has_image": "1",
                        "has_video": "331",
                        "paper_award": "",
                        "image_caption": "DendroMap is an interactive visualization system for exploring large-scale image datasets used in machine learning. The initial view of DendroMap shown on the left side of the figure presents an image dataset as nested rectangles of similar images, like treemaps, providing an overview of the entire dataset. A user clicks on a rectangle with plants, insects, and animals to zoom down into the hierarchy of those images. Then, DendroMap expands this portion to take up the space of the entire screen and shows new similar image groups.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-tvcg-9795241-pres",
                        "session_id": "full19",
                        "type": "Virtual Presentation (pre-recorded)",
                        "title": "Diagnosing Ensemble Few-Shot Classifiers",
                        "contributors": [
                            "Weikai Yang"
                        ],
                        "authors": [
                            "Weikai Yang",
                            "Xi Ye",
                            "Xingxing Zhang",
                            "Lanxi Xiao",
                            "Jiazhi Xia",
                            "Zhongyuan Wang",
                            "Jun Zhu",
                            "Hanspeter Pfister",
                            "Shixia Liu"
                        ],
                        "abstract": "The base learners and labeled samples (shots) in an ensemble few-shot classifier greatly affect the model performance. When the performance is not satisfactory, it is usually difficult to understand the underlying causes and make improvements. To tackle this issue, we propose a visual analysis method, FSLDiagnotor. Given a set of base learners and a collection of samples with a few shots, we consider two problems: 1) finding a subset of base learners that well predict the sample collections; and 2) replacing the low-quality shots with more representative ones to adequately represent the sample collections. We formulate both problems as sparse subset selection and develop two selection algorithms to recommend appropriate learners and shots, respectively. A matrix visualization and a scatterplot are combined to explain the recommended learners and shots in context and facilitate users in adjusting them. Based on the adjustment, the algorithm updates the recommendation results for another round of improvement. Two case studies are conducted to demonstrate that FSLDiagnotor helps build a few-shot classifier efficiently and increases the accuracy by 12% and 21%, respectively.",
                        "uid": "v-tvcg-9795241",
                        "file_name": "",
                        "time_stamp": "2022-10-19T17:00:00Z",
                        "time_start": "2022-10-19T17:00:00Z",
                        "time_end": "2022-10-19T17:13:00Z",
                        "paper_type": "full",
                        "keywords": "Few-shot learning, ensemble model, subset selection, matrix visualization, scatterplot",
                        "has_image": "1",
                        "has_video": "557",
                        "paper_award": "",
                        "image_caption": "FSLDiagnotor: the learner view (left) compares base learners (rows) with the ensemble model, including the overall difference (circles in the first column) and detailed difference (stacked bars in the other columns); the sample view (right) visualizes the shots and unlabeled samples in context. The image content and label distributions of the samples of interest are displayed below.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-tvcg-9795241-qa",
                        "session_id": "full19",
                        "type": "Virtual Q+A",
                        "title": "Diagnosing Ensemble Few-Shot Classifiers (Q+A)",
                        "contributors": [
                            "Weikai Yang"
                        ],
                        "authors": [],
                        "abstract": "The base learners and labeled samples (shots) in an ensemble few-shot classifier greatly affect the model performance. When the performance is not satisfactory, it is usually difficult to understand the underlying causes and make improvements. To tackle this issue, we propose a visual analysis method, FSLDiagnotor. Given a set of base learners and a collection of samples with a few shots, we consider two problems: 1) finding a subset of base learners that well predict the sample collections; and 2) replacing the low-quality shots with more representative ones to adequately represent the sample collections. We formulate both problems as sparse subset selection and develop two selection algorithms to recommend appropriate learners and shots, respectively. A matrix visualization and a scatterplot are combined to explain the recommended learners and shots in context and facilitate users in adjusting them. Based on the adjustment, the algorithm updates the recommendation results for another round of improvement. Two case studies are conducted to demonstrate that FSLDiagnotor helps build a few-shot classifier efficiently and increases the accuracy by 12% and 21%, respectively.",
                        "uid": "v-tvcg-9795241",
                        "file_name": "",
                        "time_stamp": "2022-10-19T17:13:00Z",
                        "time_start": "2022-10-19T17:13:00Z",
                        "time_end": "2022-10-19T17:15:00Z",
                        "paper_type": "full",
                        "keywords": "Few-shot learning, ensemble model, subset selection, matrix visualization, scatterplot",
                        "has_image": "1",
                        "has_video": "557",
                        "paper_award": "",
                        "image_caption": "FSLDiagnotor: the learner view (left) compares base learners (rows) with the ensemble model, including the overall difference (circles in the first column) and detailed difference (stacked bars in the other columns); the sample view (right) visualizes the shots and unlabeled samples in context. The image content and label distributions of the samples of interest are displayed below.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    }
                ]
            },
            {
                "title": "Neuro/Brain/Medical Data",
                "session_id": "full20",
                "event_prefix": "v-full",
                "track": "ok6",
                "livestream_id": "ok6-thu",
                "session_image": "full20.png",
                "chair": [
                    "Johanna Beyer"
                ],
                "organizers": [],
                "time_start": "2022-10-20T14:00:00Z",
                "time_end": "2022-10-20T15:15:00Z",
                "discord_category": "",
                "discord_channel": "",
                "discord_channel_id": "",
                "discord_link": "",
                "slido_link": "",
                "youtube_url": "",
                "zoom_meeting": "",
                "zoom_password": "",
                "zoom_link": "",
                "ff_link": "",
                "ff_playlist": "",
                "time_slots": [
                    {
                        "slot_id": "full20-opening",
                        "session_id": "full20",
                        "type": "In Person Other",
                        "title": "Session Opening",
                        "contributors": [
                            "Johanna Beyer"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "",
                        "file_name": "",
                        "time_stamp": "2022-10-20T14:00:00Z",
                        "time_start": "2022-10-20T14:00:00Z",
                        "time_end": "2022-10-20T14:00:00Z",
                        "paper_type": "",
                        "keywords": "",
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-tvcg-9492002-pres",
                        "session_id": "full20",
                        "type": "Virtual Presentation (pre-recorded)",
                        "title": "MV2Net: Multi-Variate Multi-View Brain Network Comparison over Uncertain Data",
                        "contributors": [
                            "Lei Shi"
                        ],
                        "authors": [
                            "Lei Shi",
                            "Junnan Hu",
                            "Zhihao Tan",
                            "Jun Tao",
                            "Jiayan Ding",
                            "Yan Jin",
                            "Yanjun Wu",
                            "Paul M. Thompson"
                        ],
                        "abstract": "Visually identifying effective bio-markers from human brain networks poses non-trivial challenges to the field of data visualization and analysis. Existing methods in the literature and neuroscience practice are generally limited to the study of individual connectivity features in the brain (e.g., the strength of neural connection among brain regions). Pairwise comparisons between contrasting subject groups (e.g., the diseased and the healthy controls) are normally performed. The underlying neuroimaging and brain network construction process is assumed to have 100% fidelity. Yet, real-world user requirements on brain network visual comparison lean against these assumptions. In this work, we present MV^2Net, a visual analytics system that tightly integrates multi-variate multi-view visualization for brain network comparison with an interactive wrangling mechanism to deal with data uncertainty. On the analysis side, the system integrates multiple extraction methods on diffusion and geometric connectivity features of brain networks, an anomaly detection algorithm for data quality assessment, single- and multi-connection feature selection methods for bio-marker detection. On the visualization side, novel designs are introduced which optimize network comparisons among contrasting subject groups and related connectivity features. Our design provides level-of-detail comparisons, from juxtaposed and explicit-coding views for subject group comparisons, to high-order composite view for correlation of network comparisons, and to fiber tract detail view for voxel-level comparisons. The proposed techniques are inspired and evaluated in expert studies, as well as through case analyses on diffusion and geometric bio-markers of certain neurology diseases. Results in these experiments demonstrate the effectiveness and superiority of MV^2Net over state-of-the-art approaches.",
                        "uid": "v-tvcg-9492002",
                        "file_name": "",
                        "time_stamp": "2022-10-20T14:00:00Z",
                        "time_start": "2022-10-20T14:00:00Z",
                        "time_end": "2022-10-20T14:10:00Z",
                        "paper_type": "full",
                        "keywords": "brain network, visual comparison, multivariate analysis",
                        "has_image": "1",
                        "has_video": "613",
                        "paper_award": "",
                        "image_caption": "The visualization interface of MV2Net: (a) selection panel for two subject groups under comparison; (b) heatmaps showing data quality in the subject by feature matrices which helps to select high-quality features; (c) brain network view for group-based comparison; (d) high-order composite of multiple comparisons; (e) 3D view of fiber tract details between two selected ROIs.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-tvcg-9492002-qa",
                        "session_id": "full20",
                        "type": "Virtual Q+A",
                        "title": "MV2Net: Multi-Variate Multi-View Brain Network Comparison over Uncertain Data (Q+A)",
                        "contributors": [
                            "Lei Shi"
                        ],
                        "authors": [],
                        "abstract": "Visually identifying effective bio-markers from human brain networks poses non-trivial challenges to the field of data visualization and analysis. Existing methods in the literature and neuroscience practice are generally limited to the study of individual connectivity features in the brain (e.g., the strength of neural connection among brain regions). Pairwise comparisons between contrasting subject groups (e.g., the diseased and the healthy controls) are normally performed. The underlying neuroimaging and brain network construction process is assumed to have 100% fidelity. Yet, real-world user requirements on brain network visual comparison lean against these assumptions. In this work, we present MV^2Net, a visual analytics system that tightly integrates multi-variate multi-view visualization for brain network comparison with an interactive wrangling mechanism to deal with data uncertainty. On the analysis side, the system integrates multiple extraction methods on diffusion and geometric connectivity features of brain networks, an anomaly detection algorithm for data quality assessment, single- and multi-connection feature selection methods for bio-marker detection. On the visualization side, novel designs are introduced which optimize network comparisons among contrasting subject groups and related connectivity features. Our design provides level-of-detail comparisons, from juxtaposed and explicit-coding views for subject group comparisons, to high-order composite view for correlation of network comparisons, and to fiber tract detail view for voxel-level comparisons. The proposed techniques are inspired and evaluated in expert studies, as well as through case analyses on diffusion and geometric bio-markers of certain neurology diseases. Results in these experiments demonstrate the effectiveness and superiority of MV^2Net over state-of-the-art approaches.",
                        "uid": "v-tvcg-9492002",
                        "file_name": "",
                        "time_stamp": "2022-10-20T14:10:00Z",
                        "time_start": "2022-10-20T14:10:00Z",
                        "time_end": "2022-10-20T14:12:00Z",
                        "paper_type": "full",
                        "keywords": "brain network, visual comparison, multivariate analysis",
                        "has_image": "1",
                        "has_video": "613",
                        "paper_award": "",
                        "image_caption": "The visualization interface of MV2Net: (a) selection panel for two subject groups under comparison; (b) heatmaps showing data quality in the subject by feature matrices which helps to select high-quality features; (c) brain network view for group-based comparison; (d) high-order composite of multiple comparisons; (e) 3D view of fiber tract details between two selected ROIs.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-tvcg-9529035-pres",
                        "session_id": "full20",
                        "type": "Virtual Presentation (pre-recorded)",
                        "title": "NeuroConstruct: 3D Reconstruction and Visualization of Neurites in Optical Microscopy Brain Images",
                        "contributors": [
                            "Parmida Ghahremani"
                        ],
                        "authors": [
                            "Parmida Ghahremani",
                            "Saeed Boorboor",
                            "Pooya Mirhosseini",
                            "Chetan Gudisagar",
                            "Mala Ananth",
                            "David Talmage",
                            "Lorna W. Role",
                            "Arie E. Kaufman"
                        ],
                        "abstract": "We introduce NeuroConstruct, a novel end-to-end application for the segmentation, registration, and visualization of brain volumes imaged using wide-field microscopy. NeuroConstruct offers a Segmentation Toolbox with various annotation helper functions that aid experts to effectively and precisely annotate micrometer resolution neurites. It also offers an automatic neurites segmentation using convolutional neuronal networks (CNN) trained by the Toolbox annotations and somas segmentation using thresholding. To visualize neurites in a given volume, NeuroConstruct offers a hybrid rendering by combining iso-surface rendering of high-confidence classified neurites, along with real-time rendering of raw volume using a 2D transfer function for voxel classification score vs. voxel intensity value. For a complete reconstruction of the 3D neurites, we introduce a Registration Toolbox that provides automatic coarse-to-fine alignment of serially sectioned samples. The quantitative and qualitative analysis show that NeuroConstruct outperforms the state-of-the-art in all design aspects. NeuroConstruct was developed as a collaboration between computer scientists and neuroscientists, with an application to the study of cholinergic neurons, which are severely affected in Alzheimer\u2019s disease.",
                        "uid": "v-tvcg-9529035",
                        "file_name": "",
                        "time_stamp": "2022-10-20T14:12:00Z",
                        "time_start": "2022-10-20T14:12:00Z",
                        "time_end": "2022-10-20T14:22:00Z",
                        "paper_type": "full",
                        "keywords": "Wide-field Microscopy,Neuron Morphology,Segmentation,Image Processing and Computer Vision,Computing Methodologies,Registration,Hybrid Volume Rendering,CNN",
                        "has_image": "0",
                        "has_video": "430",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-tvcg-9529035-qa",
                        "session_id": "full20",
                        "type": "Virtual Q+A",
                        "title": "NeuroConstruct: 3D Reconstruction and Visualization of Neurites in Optical Microscopy Brain Images (Q+A)",
                        "contributors": [
                            "Parmida Ghahremani"
                        ],
                        "authors": [],
                        "abstract": "We introduce NeuroConstruct, a novel end-to-end application for the segmentation, registration, and visualization of brain volumes imaged using wide-field microscopy. NeuroConstruct offers a Segmentation Toolbox with various annotation helper functions that aid experts to effectively and precisely annotate micrometer resolution neurites. It also offers an automatic neurites segmentation using convolutional neuronal networks (CNN) trained by the Toolbox annotations and somas segmentation using thresholding. To visualize neurites in a given volume, NeuroConstruct offers a hybrid rendering by combining iso-surface rendering of high-confidence classified neurites, along with real-time rendering of raw volume using a 2D transfer function for voxel classification score vs. voxel intensity value. For a complete reconstruction of the 3D neurites, we introduce a Registration Toolbox that provides automatic coarse-to-fine alignment of serially sectioned samples. The quantitative and qualitative analysis show that NeuroConstruct outperforms the state-of-the-art in all design aspects. NeuroConstruct was developed as a collaboration between computer scientists and neuroscientists, with an application to the study of cholinergic neurons, which are severely affected in Alzheimer\u2019s disease.",
                        "uid": "v-tvcg-9529035",
                        "file_name": "",
                        "time_stamp": "2022-10-20T14:22:00Z",
                        "time_start": "2022-10-20T14:22:00Z",
                        "time_end": "2022-10-20T14:24:00Z",
                        "paper_type": "full",
                        "keywords": "Wide-field Microscopy,Neuron Morphology,Segmentation,Image Processing and Computer Vision,Computing Methodologies,Registration,Hybrid Volume Rendering,CNN",
                        "has_image": "0",
                        "has_video": "430",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-tvcg-9555234-pres",
                        "session_id": "full20",
                        "type": "In Person Presentation",
                        "title": "DXplorer: A Unified Visualization Framework for Interactive Dendritic Spine Analysis using 3D morphological Features",
                        "contributors": [
                            "Won-Ki Jeong",
                            "JunYoung Choi"
                        ],
                        "authors": [
                            "JunYoung Choi",
                            "Sang-Eun Lee",
                            "YeIn Lee",
                            "Eunji Cho",
                            "Sunghoe Chang",
                            "Won-Ki Jeong"
                        ],
                        "abstract": "Dendritic spines are dynamic, submicron-scale protrusions on neuronal dendrites that receive neuronal inputs. Morphological changes in the dendritic spine often reflect alterations in physiological conditions and are indicators of various neuropsychiatric conditions. However, owing to the highly dynamic and heterogeneous nature of spines, accurate measurement and objective analysis of spine morphology are major challenges in neuroscience research. Most conventional approaches for analyzing dendritic spines are based on two-dimensional (2D) images, which barely reflect the actual three-dimensional (3D) shapes. Although some recent studies have attempted to analyze spines with various 3D-based features, it is still difficult to objectively categorize and analyze spines based on 3D morphology. Here, we propose a unified visualization framework for an interactive 3D dendritic spine analysis system, DXplorer, that displays 3D rendering of spines and plots the high-dimensional features extracted from the 3D mesh of spines. With this system, users can perform the clustering of spines interactively and explore and analyze dendritic spines based on high-dimensional features. We propose a series of high-dimensional morphological features extracted from a 3D mesh of dendritic spines. In addition, an interactive machine learning classifier with visual exploration and user feedback using an interactive 3D mesh grid view ensures a more precise classification based on the spine phenotype. A user study and two case studies were conducted to quantitatively verify the performance and usability of the DXplorer. We demonstrate that the system performs the entire analytic process effectively and provides high-quality, accurate, and objective analysis.",
                        "uid": "v-tvcg-9555234",
                        "file_name": "",
                        "time_stamp": "2022-10-20T14:24:00Z",
                        "time_start": "2022-10-20T14:24:00Z",
                        "time_end": "2022-10-20T14:34:00Z",
                        "paper_type": "full",
                        "keywords": "Biomedical and Medical Visualization, Machine Learning, Task and Requirements Analysis, User Interfaces, Intelligence Analysis",
                        "has_image": "1",
                        "has_video": "485",
                        "paper_award": "",
                        "image_caption": "Example of dendritic spine\u2019s 3D mesh with morphological features (upper row) and DXplorer\u2019s user interfaces (bottom row). DXplorer is a unified visual analysis framework for the interactive analysis of dendritic spines using 3D meshes and high-dimensional features. The aim of the proposed system is to provide a unified data processing and visualization workflow for various spine analysis tasks, which will eventually provide insight into the correlation between neuronal functions and the shape of the spine.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-tvcg-9555234-qa",
                        "session_id": "full20",
                        "type": "In Person Q+A",
                        "title": "DXplorer: A Unified Visualization Framework for Interactive Dendritic Spine Analysis using 3D morphological Features (Q+A)",
                        "contributors": [
                            "Won-Ki Jeong",
                            "JunYoung Choi"
                        ],
                        "authors": [],
                        "abstract": "Dendritic spines are dynamic, submicron-scale protrusions on neuronal dendrites that receive neuronal inputs. Morphological changes in the dendritic spine often reflect alterations in physiological conditions and are indicators of various neuropsychiatric conditions. However, owing to the highly dynamic and heterogeneous nature of spines, accurate measurement and objective analysis of spine morphology are major challenges in neuroscience research. Most conventional approaches for analyzing dendritic spines are based on two-dimensional (2D) images, which barely reflect the actual three-dimensional (3D) shapes. Although some recent studies have attempted to analyze spines with various 3D-based features, it is still difficult to objectively categorize and analyze spines based on 3D morphology. Here, we propose a unified visualization framework for an interactive 3D dendritic spine analysis system, DXplorer, that displays 3D rendering of spines and plots the high-dimensional features extracted from the 3D mesh of spines. With this system, users can perform the clustering of spines interactively and explore and analyze dendritic spines based on high-dimensional features. We propose a series of high-dimensional morphological features extracted from a 3D mesh of dendritic spines. In addition, an interactive machine learning classifier with visual exploration and user feedback using an interactive 3D mesh grid view ensures a more precise classification based on the spine phenotype. A user study and two case studies were conducted to quantitatively verify the performance and usability of the DXplorer. We demonstrate that the system performs the entire analytic process effectively and provides high-quality, accurate, and objective analysis.",
                        "uid": "v-tvcg-9555234",
                        "file_name": "",
                        "time_stamp": "2022-10-20T14:34:00Z",
                        "time_start": "2022-10-20T14:34:00Z",
                        "time_end": "2022-10-20T14:36:00Z",
                        "paper_type": "full",
                        "keywords": "Biomedical and Medical Visualization, Machine Learning, Task and Requirements Analysis, User Interfaces, Intelligence Analysis",
                        "has_image": "1",
                        "has_video": "485",
                        "paper_award": "",
                        "image_caption": "Example of dendritic spine\u2019s 3D mesh with morphological features (upper row) and DXplorer\u2019s user interfaces (bottom row). DXplorer is a unified visual analysis framework for the interactive analysis of dendritic spines using 3D meshes and high-dimensional features. The aim of the proposed system is to provide a unified data processing and visualization workflow for various spine analysis tasks, which will eventually provide insight into the correlation between neuronal functions and the shape of the spine.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-tvcg-9665344-pres",
                        "session_id": "full20",
                        "type": "Virtual Presentation (pre-recorded)",
                        "title": "A Predictive Visual Analytics System for Studying Neurodegenerative Disease Based on DTI Fiber Tracts",
                        "contributors": [
                            "Chaoqing Xu"
                        ],
                        "authors": [
                            "Chaoqing Xu",
                            "Tyson Neuroth",
                            "Takanori Fujiwara",
                            "Ronghua Liang",
                            "Kwan-Liu Ma"
                        ],
                        "abstract": "Diffusion tensor imaging (DTI) has been used to study the effects of neurodegenerative diseases on neural pathways, which may lead to more reliable and early diagnosis of these diseases as well as a better understanding of how they affect the brain. We introduce a predictive visual analytics system for studying patient groups based on their labeled DTI fiber tract data and corresponding statistics. The system\u2019s machine-learning-augmented interface guides the user through an organized and holistic analysis space, including the statistical feature space, the physical space, and the space of patients over different groups. We use a custom machine learning pipeline to help narrow down this large analysis space and then explore it pragmatically through a range of linked visualizations. We conduct several case studies using DTI and T1-weighted images from the research database of Parkinson\u2019s Progression Markers Initiative.",
                        "uid": "v-tvcg-9665344",
                        "file_name": "",
                        "time_stamp": "2022-10-20T14:36:00Z",
                        "time_start": "2022-10-20T14:36:00Z",
                        "time_end": "2022-10-20T14:46:00Z",
                        "paper_type": "full",
                        "keywords": "Brain fiber tracts, neurodegenerative disease, machine learning, predictive visual analytics, visualization.",
                        "has_image": "1",
                        "has_video": "565",
                        "paper_award": "",
                        "image_caption": "The interface of the brain fiber visualization system. Right-Top is the cohort selection module and machine learning module. Right-Bot shows the outputs of machine learning, which includes three exploration modules: the feature, region, and subject modules. Left-Bot is the information visualization module that includes a range of views for comparative analysis. Left-Mid is the 3D fiber rendering module that shows selected subjects\u2019 fibers for physiological analysis. The selected feature is mapped to the fibers through color. Left-Top is the timeline view of subjects, which can be clicked to render the fibers corresponding to the clicked time.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-tvcg-9665344-qa",
                        "session_id": "full20",
                        "type": "Virtual Q+A",
                        "title": "A Predictive Visual Analytics System for Studying Neurodegenerative Disease Based on DTI Fiber Tracts (Q+A)",
                        "contributors": [
                            "Chaoqing Xu"
                        ],
                        "authors": [],
                        "abstract": "Diffusion tensor imaging (DTI) has been used to study the effects of neurodegenerative diseases on neural pathways, which may lead to more reliable and early diagnosis of these diseases as well as a better understanding of how they affect the brain. We introduce a predictive visual analytics system for studying patient groups based on their labeled DTI fiber tract data and corresponding statistics. The system\u2019s machine-learning-augmented interface guides the user through an organized and holistic analysis space, including the statistical feature space, the physical space, and the space of patients over different groups. We use a custom machine learning pipeline to help narrow down this large analysis space and then explore it pragmatically through a range of linked visualizations. We conduct several case studies using DTI and T1-weighted images from the research database of Parkinson\u2019s Progression Markers Initiative.",
                        "uid": "v-tvcg-9665344",
                        "file_name": "",
                        "time_stamp": "2022-10-20T14:46:00Z",
                        "time_start": "2022-10-20T14:46:00Z",
                        "time_end": "2022-10-20T14:48:00Z",
                        "paper_type": "full",
                        "keywords": "Brain fiber tracts, neurodegenerative disease, machine learning, predictive visual analytics, visualization.",
                        "has_image": "1",
                        "has_video": "565",
                        "paper_award": "",
                        "image_caption": "The interface of the brain fiber visualization system. Right-Top is the cohort selection module and machine learning module. Right-Bot shows the outputs of machine learning, which includes three exploration modules: the feature, region, and subject modules. Left-Bot is the information visualization module that includes a range of views for comparative analysis. Left-Mid is the 3D fiber rendering module that shows selected subjects\u2019 fibers for physiological analysis. The selected feature is mapped to the fibers through color. Left-Top is the timeline view of subjects, which can be clicked to render the fibers corresponding to the clicked time.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-tvcg-9610985-pres",
                        "session_id": "full20",
                        "type": "In Person Presentation",
                        "title": "NeuRegenerate: A Framework for Visualizing Neurodegeneration",
                        "contributors": [
                            "Saeed Boorboor"
                        ],
                        "authors": [
                            "Saeed Boorboor",
                            "Shawn Mathew",
                            "Mala Ananth",
                            "David Talmage",
                            "Lorna W. Role",
                            "Arie E. Kaufman."
                        ],
                        "abstract": "Recent advances in high-resolution microscopy have allowed scientists to better understand the underlying brain connectivity. However, due to the limitation that biological specimens can only be imaged at a single timepoint, studying changes to neural projections over time is limited to observations gathered using population analysis. In this paper, we introduce NeuRegenerate, a novel end-to-end framework for the prediction and visualization of changes in neural fiber morphology within a subject across specified age-timepoints. To predict projections, we present  neuReGANerator, a deep-learning network based on cycle-consistent generative adversarial network (GAN) that translates features of neuronal structures across age-timepoints for large brain microscopy volumes. We improve the reconstruction quality of the predicted neuronal structures by implementing a density multiplier and a new loss function, called the hallucination loss. Moreover, to alleviate artifacts that occur due to tiling of large input volumes, we introduce a spatial-consistency module in the training pipeline of neuReGANerator. Finally, to visualize the change in projections, predicted using neuReGANerator, NeuRegenerate offers two modes: (i) neuroCompare to simultaneously visualize the difference in the structures of the neuronal projections, from two age domains (using structural view and bounded view), and (ii) em neuroMorph, a vesselness-based morphing technique to interactively visualize the transformation of the structures from one age-timepoint to the other. Our framework is designed specifically for volumes acquired using wide-field microscopy. We demonstrate our framework by visualizing the structural changes within the cholinergic system of the mouse brain between a young and old specimen.",
                        "uid": "v-tvcg-9610985",
                        "file_name": "",
                        "time_stamp": "2022-10-20T14:48:00Z",
                        "time_start": "2022-10-20T14:48:00Z",
                        "time_end": "2022-10-20T14:58:00Z",
                        "paper_type": "full",
                        "keywords": "Neuron visualization, volume visualization, volume transformation, neuroscience, wide-field microscopy, machine learning,",
                        "has_image": "1",
                        "has_video": "603",
                        "paper_award": "",
                        "image_caption": "NeuRegenerate allows neuroscientists to visualize structural changes that occur in an individual specimen's brain, across age: for a diseased mouse data (right-most structure), we are able to predict and reconstruct its healthy neuronal extensions at a younger age (left-most structure). The three in-between structures are generated using our neuroMorph technique that allows users to interactively visualize the neurodegeneration process. The structures in this figure are processed from an input volume imaged using a wide-field microscope, and rendered using our framework's structural mode.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-tvcg-9610985-qa",
                        "session_id": "full20",
                        "type": "In Person Q+A",
                        "title": "NeuRegenerate: A Framework for Visualizing Neurodegeneration (Q+A)",
                        "contributors": [
                            "Saeed Boorboor"
                        ],
                        "authors": [],
                        "abstract": "Recent advances in high-resolution microscopy have allowed scientists to better understand the underlying brain connectivity. However, due to the limitation that biological specimens can only be imaged at a single timepoint, studying changes to neural projections over time is limited to observations gathered using population analysis. In this paper, we introduce NeuRegenerate, a novel end-to-end framework for the prediction and visualization of changes in neural fiber morphology within a subject across specified age-timepoints. To predict projections, we present  neuReGANerator, a deep-learning network based on cycle-consistent generative adversarial network (GAN) that translates features of neuronal structures across age-timepoints for large brain microscopy volumes. We improve the reconstruction quality of the predicted neuronal structures by implementing a density multiplier and a new loss function, called the hallucination loss. Moreover, to alleviate artifacts that occur due to tiling of large input volumes, we introduce a spatial-consistency module in the training pipeline of neuReGANerator. Finally, to visualize the change in projections, predicted using neuReGANerator, NeuRegenerate offers two modes: (i) neuroCompare to simultaneously visualize the difference in the structures of the neuronal projections, from two age domains (using structural view and bounded view), and (ii) em neuroMorph, a vesselness-based morphing technique to interactively visualize the transformation of the structures from one age-timepoint to the other. Our framework is designed specifically for volumes acquired using wide-field microscopy. We demonstrate our framework by visualizing the structural changes within the cholinergic system of the mouse brain between a young and old specimen.",
                        "uid": "v-tvcg-9610985",
                        "file_name": "",
                        "time_stamp": "2022-10-20T14:58:00Z",
                        "time_start": "2022-10-20T14:58:00Z",
                        "time_end": "2022-10-20T15:00:00Z",
                        "paper_type": "full",
                        "keywords": "Neuron visualization, volume visualization, volume transformation, neuroscience, wide-field microscopy, machine learning,",
                        "has_image": "1",
                        "has_video": "603",
                        "paper_award": "",
                        "image_caption": "NeuRegenerate allows neuroscientists to visualize structural changes that occur in an individual specimen's brain, across age: for a diseased mouse data (right-most structure), we are able to predict and reconstruct its healthy neuronal extensions at a younger age (left-most structure). The three in-between structures are generated using our neuroMorph technique that allows users to interactively visualize the neurodegeneration process. The structures in this figure are processed from an input volume imaged using a wide-field microscope, and rendered using our framework's structural mode.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-tvcg-9645173-pres",
                        "session_id": "full20",
                        "type": "Virtual Presentation (pre-recorded)",
                        "title": "GUCCI - Guided Cardiac Cohort Investigation of Blood Flow Data",
                        "contributors": [
                            "Monique Meuschke"
                        ],
                        "authors": [
                            "Monique Meuschke",
                            "Uli Niemann",
                            "Benjamin Behrendt",
                            "Matthias Gutberlet",
                            "Bernhard Preim",
                            "Kai Lawonn"
                        ],
                        "abstract": "We present the framework GUCCI (Guided Cardiac Cohort Investigation), which provides a guided visual analytics workflow to analyze cohort-based measured blood flow data in the aorta. In the past, many specialized techniques have been developed for the visual exploration of such data sets for a better understanding of the influence of morphological and hemodynamic conditions on cardiovascular diseases. However, there is a lack of dedicated techniques that allow visual comparison of multiple datasets and defined cohorts, which is essential to characterize pathologies. GUCCI offers visual analytics techniques and novel visualization methods to guide the user through the comparison of predefined cohorts, such as healthy volunteers and patients with a pathologically altered aorta.  The combination of overview and glyph-based depictions together with statistical cohort-specific information allows investigating differences and similarities of the time-dependent data. Our framework was evaluated in a qualitative user study with three radiologists specialized in cardiac imaging and two experts in medical blood flow visualization. They were able to discover cohort-specific characteristics, which supports the derivation of standard values as well as the assessment of pathology-related severity and the need for treatment.",
                        "uid": "v-tvcg-9645173",
                        "file_name": "",
                        "time_stamp": "2022-10-20T15:00:00Z",
                        "time_start": "2022-10-20T15:00:00Z",
                        "time_end": "2022-10-20T15:10:00Z",
                        "paper_type": "full",
                        "keywords": "Medical Visualization, Cohort Analysis, Measured Blood Flow Data, Cardiac Diseases",
                        "has_image": "1",
                        "has_video": "588",
                        "paper_award": "",
                        "image_caption": "Workflow of GUCCI for a cohort-based visual analysis of aortic flow data. First, a subset of relevant features is selected, which serves as\ninput for the cohort definition. Then, visual analysis of the cohorts is supported with different visualization techniques.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-tvcg-9645173-qa",
                        "session_id": "full20",
                        "type": "Virtual Q+A",
                        "title": "GUCCI - Guided Cardiac Cohort Investigation of Blood Flow Data (Q+A)",
                        "contributors": [
                            "Monique Meuschke"
                        ],
                        "authors": [],
                        "abstract": "We present the framework GUCCI (Guided Cardiac Cohort Investigation), which provides a guided visual analytics workflow to analyze cohort-based measured blood flow data in the aorta. In the past, many specialized techniques have been developed for the visual exploration of such data sets for a better understanding of the influence of morphological and hemodynamic conditions on cardiovascular diseases. However, there is a lack of dedicated techniques that allow visual comparison of multiple datasets and defined cohorts, which is essential to characterize pathologies. GUCCI offers visual analytics techniques and novel visualization methods to guide the user through the comparison of predefined cohorts, such as healthy volunteers and patients with a pathologically altered aorta.  The combination of overview and glyph-based depictions together with statistical cohort-specific information allows investigating differences and similarities of the time-dependent data. Our framework was evaluated in a qualitative user study with three radiologists specialized in cardiac imaging and two experts in medical blood flow visualization. They were able to discover cohort-specific characteristics, which supports the derivation of standard values as well as the assessment of pathology-related severity and the need for treatment.",
                        "uid": "v-tvcg-9645173",
                        "file_name": "",
                        "time_stamp": "2022-10-20T15:10:00Z",
                        "time_start": "2022-10-20T15:10:00Z",
                        "time_end": "2022-10-20T15:12:00Z",
                        "paper_type": "full",
                        "keywords": "Medical Visualization, Cohort Analysis, Measured Blood Flow Data, Cardiac Diseases",
                        "has_image": "1",
                        "has_video": "588",
                        "paper_award": "",
                        "image_caption": "Workflow of GUCCI for a cohort-based visual analysis of aortic flow data. First, a subset of relevant features is selected, which serves as\ninput for the cohort definition. Then, visual analysis of the cohorts is supported with different visualization techniques.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    }
                ]
            },
            {
                "title": "Temporal Data",
                "session_id": "full21",
                "event_prefix": "v-full",
                "track": "ok6",
                "livestream_id": "ok6-wed",
                "session_image": "full21.png",
                "chair": [
                    "Wolfgang Aigner"
                ],
                "organizers": [],
                "time_start": "2022-10-19T14:00:00Z",
                "time_end": "2022-10-19T15:15:00Z",
                "discord_category": "",
                "discord_channel": "",
                "discord_channel_id": "",
                "discord_link": "",
                "slido_link": "",
                "youtube_url": "",
                "zoom_meeting": "",
                "zoom_password": "",
                "zoom_link": "",
                "ff_link": "",
                "ff_playlist": "",
                "time_slots": [
                    {
                        "slot_id": "full21-opening",
                        "session_id": "full21",
                        "type": "In Person Other",
                        "title": "Session Opening",
                        "contributors": [
                            "Wolfgang Aigner"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "",
                        "file_name": "",
                        "time_stamp": "2022-10-19T14:00:00Z",
                        "time_start": "2022-10-19T14:00:00Z",
                        "time_end": "2022-10-19T14:00:00Z",
                        "paper_type": "",
                        "keywords": "",
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-full-1380-pres",
                        "session_id": "full21",
                        "type": "Virtual Presentation (pre-recorded)",
                        "title": "Visualizing the Passage of Time with Video Temporal Pyramids",
                        "contributors": [
                            "Melissa E Swift",
                            "Melissa Swift"
                        ],
                        "authors": [
                            "Melissa E Swift",
                            "Wyatt Ayers",
                            "Sophie Pallanck",
                            "Scott Wehrwein"
                        ],
                        "abstract": "",
                        "uid": "v-full-1380",
                        "file_name": "",
                        "time_stamp": "2022-10-19T14:00:00Z",
                        "time_start": "2022-10-19T14:00:00Z",
                        "time_end": "2022-10-19T14:10:00Z",
                        "paper_type": "full",
                        "keywords": "",
                        "has_image": "1",
                        "has_video": "480",
                        "paper_award": "",
                        "image_caption": "Given months or years of video footage from a fixed scene (such as a recorded webcam), our approach builds a Video Temporal Pyramid consisting of different length shorter videos, each distilling the events from a particular timescale such as seasonal changes at the 90-day timescale and human activity at the shorter 1-second timescale. Our videos are an improvement over standard timelapse videos because they avoid aliasing and temporal discontinuities and can be upsampled for a smooth viewing experience. Additionally, our Video Spectrogram tool visualizes the whole pyramid in order to help with navigation and discovery of patterns and anomalies.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-full-1380-qa",
                        "session_id": "full21",
                        "type": "Virtual Q+A",
                        "title": "Visualizing the Passage of Time with Video Temporal Pyramids (Q+A)",
                        "contributors": [
                            "Melissa E Swift",
                            "Melissa Swift"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "v-full-1380",
                        "file_name": "",
                        "time_stamp": "2022-10-19T14:10:00Z",
                        "time_start": "2022-10-19T14:10:00Z",
                        "time_end": "2022-10-19T14:12:00Z",
                        "paper_type": "full",
                        "keywords": "",
                        "has_image": "1",
                        "has_video": "480",
                        "paper_award": "",
                        "image_caption": "Given months or years of video footage from a fixed scene (such as a recorded webcam), our approach builds a Video Temporal Pyramid consisting of different length shorter videos, each distilling the events from a particular timescale such as seasonal changes at the 90-day timescale and human activity at the shorter 1-second timescale. Our videos are an improvement over standard timelapse videos because they avoid aliasing and temporal discontinuities and can be upsampled for a smooth viewing experience. Additionally, our Video Spectrogram tool visualizes the whole pyramid in order to help with navigation and discovery of patterns and anomalies.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-full-1289-pres",
                        "session_id": "full21",
                        "type": "In Person Presentation",
                        "title": "Constrained Dynamic Mode Decomposition",
                        "contributors": [
                            "Tim Krake"
                        ],
                        "authors": [
                            "Tim Krake",
                            "Daniel Kl\u00f6tzl",
                            "Bernhard Eberhardt",
                            "Daniel Weiskopf"
                        ],
                        "abstract": "",
                        "uid": "v-full-1289",
                        "file_name": "",
                        "time_stamp": "2022-10-19T14:12:00Z",
                        "time_start": "2022-10-19T14:12:00Z",
                        "time_end": "2022-10-19T14:22:00Z",
                        "paper_type": "full",
                        "keywords": "",
                        "has_image": "1",
                        "has_video": "480",
                        "paper_award": "",
                        "image_caption": "The decomposition of time series into seasonal and trend patterns can be performed with Dynamic Mode Decomposition. The extracted DMD components capture these patterns only in an approximate fashion, although knowledge about time scales or specific periodicities is often present. Our technique, constrained Dynamic Mode Decomposition, enables incorporation of user-defined frequencies into DMD, which makes an explorative investigation of time series possible.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-full-1289-qa",
                        "session_id": "full21",
                        "type": "In Person Q+A",
                        "title": "Constrained Dynamic Mode Decomposition (Q+A)",
                        "contributors": [
                            "Tim Krake"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "v-full-1289",
                        "file_name": "",
                        "time_stamp": "2022-10-19T14:22:00Z",
                        "time_start": "2022-10-19T14:22:00Z",
                        "time_end": "2022-10-19T14:24:00Z",
                        "paper_type": "full",
                        "keywords": "",
                        "has_image": "1",
                        "has_video": "480",
                        "paper_award": "",
                        "image_caption": "The decomposition of time series into seasonal and trend patterns can be performed with Dynamic Mode Decomposition. The extracted DMD components capture these patterns only in an approximate fashion, although knowledge about time scales or specific periodicities is often present. Our technique, constrained Dynamic Mode Decomposition, enables incorporation of user-defined frequencies into DMD, which makes an explorative investigation of time series possible.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-full-1163-pres",
                        "session_id": "full21",
                        "type": "Virtual Presentation (pre-recorded)",
                        "title": "SizePairs: Achieving Stable and Balanced Temporal Treemaps using Hierarchical Size-based Paring",
                        "contributors": [
                            "Yunhai Wang",
                            "Chang Han"
                        ],
                        "authors": [
                            "Chang Han",
                            "Anyi Li",
                            "Jaemin Jo",
                            "Bongshin Lee",
                            "Oliver Deussen",
                            "Yunhai Wang"
                        ],
                        "abstract": "",
                        "uid": "v-full-1163",
                        "file_name": "",
                        "time_stamp": "2022-10-19T14:24:00Z",
                        "time_start": "2022-10-19T14:24:00Z",
                        "time_end": "2022-10-19T14:34:00Z",
                        "paper_type": "full",
                        "keywords": "",
                        "has_image": "1",
                        "has_video": "573",
                        "paper_award": "",
                        "image_caption": "Comparing different temporal treemap methods using three time steps (t = 0, t = 10, and t = 20) of the WorldBankHIV data: (a) Hilbert treemap method; (b) Local moves method; (c) Greedy insertion treemaps. (d) SizePairs; and (e) SizePairs with local moves. To reflect the stability of each method, we assign colors to each rectangle at the first time step and then apply the color scheme to the other time steps. SizePairs is more stable and maintains better aspect ratios, and SizePairs in combination with local moves (e) has even better aspect ratios than SizePairs alone (d).",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-full-1163-qa",
                        "session_id": "full21",
                        "type": "Virtual Q+A",
                        "title": "SizePairs: Achieving Stable and Balanced Temporal Treemaps using Hierarchical Size-based Paring (Q+A)",
                        "contributors": [
                            "Yunhai Wang",
                            "Chang Han"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "v-full-1163",
                        "file_name": "",
                        "time_stamp": "2022-10-19T14:34:00Z",
                        "time_start": "2022-10-19T14:34:00Z",
                        "time_end": "2022-10-19T14:36:00Z",
                        "paper_type": "full",
                        "keywords": "",
                        "has_image": "1",
                        "has_video": "573",
                        "paper_award": "",
                        "image_caption": "Comparing different temporal treemap methods using three time steps (t = 0, t = 10, and t = 20) of the WorldBankHIV data: (a) Hilbert treemap method; (b) Local moves method; (c) Greedy insertion treemaps. (d) SizePairs; and (e) SizePairs with local moves. To reflect the stability of each method, we assign colors to each rectangle at the first time step and then apply the color scheme to the other time steps. SizePairs is more stable and maintains better aspect ratios, and SizePairs in combination with local moves (e) has even better aspect ratios than SizePairs alone (d).",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-full-1230-pres",
                        "session_id": "full21",
                        "type": "In Person Presentation",
                        "title": "LargeNetVis: visual exploration of large temporal networks based on community taxonomies",
                        "contributors": [
                            "Claudio Linhares",
                            "Jean Ponciano"
                        ],
                        "authors": [
                            "Claudio Linhares",
                            "Jean Roberto Ponciano",
                            "Diogenes Pedro",
                            "Luis Rocha",
                            "Agma Traina",
                            "Jorge Poco"
                        ],
                        "abstract": "",
                        "uid": "v-full-1230",
                        "file_name": "",
                        "time_stamp": "2022-10-19T14:36:00Z",
                        "time_start": "2022-10-19T14:36:00Z",
                        "time_end": "2022-10-19T14:46:00Z",
                        "paper_type": "full",
                        "keywords": "",
                        "has_image": "1",
                        "has_video": "576",
                        "paper_award": "",
                        "image_caption": "LargeNetVis is a web-based visual analytics system designed to support the visual exploration of temporal networks with up to a few thousand nodes and timestamps. It enhances the analysis by leveraging the network community structure and three taxonomies. LargeNetVis contains four linked views. While the Taxonomy Matrix (A) and Global View (B) enable global-level analysis, the node-link diagram (C) and the Temporal Activity Map (D) enable structural and temporal local-level analysis, respectively. The system also provides a panel with the network, community, or node numerical information (E). We demonstrate usefulness through usage scenarios and a user study with 14 participants.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-full-1230-qa",
                        "session_id": "full21",
                        "type": "In Person Q+A",
                        "title": "LargeNetVis: visual exploration of large temporal networks based on community taxonomies (Q+A)",
                        "contributors": [
                            "Claudio Linhares",
                            "Jean Ponciano"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "v-full-1230",
                        "file_name": "",
                        "time_stamp": "2022-10-19T14:46:00Z",
                        "time_start": "2022-10-19T14:46:00Z",
                        "time_end": "2022-10-19T14:48:00Z",
                        "paper_type": "full",
                        "keywords": "",
                        "has_image": "1",
                        "has_video": "576",
                        "paper_award": "",
                        "image_caption": "LargeNetVis is a web-based visual analytics system designed to support the visual exploration of temporal networks with up to a few thousand nodes and timestamps. It enhances the analysis by leveraging the network community structure and three taxonomies. LargeNetVis contains four linked views. While the Taxonomy Matrix (A) and Global View (B) enable global-level analysis, the node-link diagram (C) and the Temporal Activity Map (D) enable structural and temporal local-level analysis, respectively. The system also provides a panel with the network, community, or node numerical information (E). We demonstrate usefulness through usage scenarios and a user study with 14 participants.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-full-1069-pres",
                        "session_id": "full21",
                        "type": "Virtual Presentation (pre-recorded)",
                        "title": "ASTF: Visual Abstractions of Time-Varying Patterns in Radio Signals",
                        "contributors": [
                            "Ying Zhao"
                        ],
                        "authors": [
                            "Ying Zhao",
                            "Luhao Ge",
                            "Huixuan Xie",
                            "Genghuai Bai",
                            "Zhao Zhang",
                            "Qiang Wei",
                            "Yun Lin",
                            "Yuchao Liu",
                            "Fangfang Zhou"
                        ],
                        "abstract": "",
                        "uid": "v-full-1069",
                        "file_name": "",
                        "time_stamp": "2022-10-19T14:48:00Z",
                        "time_start": "2022-10-19T14:48:00Z",
                        "time_end": "2022-10-19T14:58:00Z",
                        "paper_type": "full",
                        "keywords": "",
                        "has_image": "1",
                        "has_video": "600",
                        "paper_award": "",
                        "image_caption": "Visual abstractions of time-varying patterns in radio signals using abstracted signal time-frequency (ASTF) diagram.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-full-1069-qa",
                        "session_id": "full21",
                        "type": "Virtual Q+A",
                        "title": "ASTF: Visual Abstractions of Time-Varying Patterns in Radio Signals (Q+A)",
                        "contributors": [
                            "Ying Zhao"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "v-full-1069",
                        "file_name": "",
                        "time_stamp": "2022-10-19T14:58:00Z",
                        "time_start": "2022-10-19T14:58:00Z",
                        "time_end": "2022-10-19T15:00:00Z",
                        "paper_type": "full",
                        "keywords": "",
                        "has_image": "1",
                        "has_video": "600",
                        "paper_award": "",
                        "image_caption": "Visual abstractions of time-varying patterns in radio signals using abstracted signal time-frequency (ASTF) diagram.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-full-1544-pres",
                        "session_id": "full21",
                        "type": "Virtual Presentation (pre-recorded)",
                        "title": "RankFIRST: Visual Analysis for Factor Investment by Ranking Stock Timeseries",
                        "contributors": [
                            "Lei Shi",
                            "Huijie Guo"
                        ],
                        "authors": [
                            "Huijie Guo",
                            "Meijun Liu",
                            "Bowen Yang",
                            "Ye Sun",
                            "Lei Shi",
                            "Huamin Qu"
                        ],
                        "abstract": "",
                        "uid": "v-full-1544",
                        "file_name": "",
                        "time_stamp": "2022-10-19T15:00:00Z",
                        "time_start": "2022-10-19T15:00:00Z",
                        "time_end": "2022-10-19T15:10:00Z",
                        "paper_type": "full",
                        "keywords": "",
                        "has_image": "1",
                        "has_video": "578",
                        "paper_award": "",
                        "image_caption": "The visualization interface of RankFIRST: (a) configuration panel for investment time, stock pool, and ranking strategy; (b) factor view showing the factor return time series for selection; (c) factor treemap of aggregated factor returns; (d) ranked stock list; (e) stock view showing stock time series by the firework chart for portfolio construction; (f) portfolio view depicting its return time series in the past (red curve); (g) backtest view on the actual portfolio performance in the next 3 months (red bars).",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-full-1544-qa",
                        "session_id": "full21",
                        "type": "Virtual Q+A",
                        "title": "RankFIRST: Visual Analysis for Factor Investment by Ranking Stock Timeseries (Q+A)",
                        "contributors": [
                            "Lei Shi",
                            "Huijie Guo"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "v-full-1544",
                        "file_name": "",
                        "time_stamp": "2022-10-19T15:10:00Z",
                        "time_start": "2022-10-19T15:10:00Z",
                        "time_end": "2022-10-19T15:12:00Z",
                        "paper_type": "full",
                        "keywords": "",
                        "has_image": "1",
                        "has_video": "578",
                        "paper_award": "",
                        "image_caption": "The visualization interface of RankFIRST: (a) configuration panel for investment time, stock pool, and ranking strategy; (b) factor view showing the factor return time series for selection; (c) factor treemap of aggregated factor returns; (d) ranked stock list; (e) stock view showing stock time series by the firework chart for portfolio construction; (f) portfolio view depicting its return time series in the past (red curve); (g) backtest view on the actual portfolio performance in the next 3 months (red bars).",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    }
                ]
            },
            {
                "title": "Comparisons",
                "session_id": "full22",
                "event_prefix": "v-full",
                "track": "ok1",
                "livestream_id": "ok1-fri",
                "session_image": "full22.png",
                "chair": [
                    "Peter Lindstrom"
                ],
                "organizers": [],
                "time_start": "2022-10-21T14:00:00Z",
                "time_end": "2022-10-21T15:15:00Z",
                "discord_category": "",
                "discord_channel": "",
                "discord_channel_id": "",
                "discord_link": "",
                "slido_link": "",
                "youtube_url": "",
                "zoom_meeting": "",
                "zoom_password": "",
                "zoom_link": "",
                "ff_link": "",
                "ff_playlist": "",
                "time_slots": [
                    {
                        "slot_id": "full22-opening",
                        "session_id": "full22",
                        "type": "In Person Other",
                        "title": "Session Opening",
                        "contributors": [
                            "Peter Lindstrom"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "",
                        "file_name": "",
                        "time_stamp": "2022-10-21T14:00:00Z",
                        "time_start": "2022-10-21T14:00:00Z",
                        "time_end": "2022-10-21T14:00:00Z",
                        "paper_type": "",
                        "keywords": "",
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-tvcg-9744472-pres",
                        "session_id": "full22",
                        "type": "In Person Presentation",
                        "title": "Geometry-Aware Merge Tree Comparisons for Time-Varying Data with Interleaving Distances",
                        "contributors": [
                            "Lin Yan"
                        ],
                        "authors": [
                            "Lin Yan",
                            "Talha Bin Masood",
                            "Farhan Rasheed",
                            "Ingrid Hotz",
                            "Bei Wang"
                        ],
                        "abstract": "Merge trees, a type of topological descriptor, serve to identify and summarize the topological characteristics associated with scalar fields. They have great potential for analyzing and visualizing time-varying data. First, they give compressed and topology-preserving representations of data instances. Second, their comparisons provide a basis for studying the relations among data instances, such as their distributions, clusters, outliers, and periodicities. A number of comparative measures have been developed for merge trees. However, these measures are often computationally expensive since they implicitly consider all possible correspondences between critical points of the merge trees. In this paper, we perform geometry-aware comparisons of merge trees using labeled interleaving distances. The main idea is to decouple the computation of a comparative measure into two steps: a labeling step that generates a correspondence between the critical points of two merge trees, and a comparison step that computes distances between a pair of labeled merge trees by encoding them as matrices. We show that our approach is general, computationally efficient, and practically useful. Our framework makes it possible to integrate geometric information of the data domain in the labeling process. At the same time, the framework reduces the computational complexity since not all possible correspondences have to be considered. We demonstrate via experiments that such geometry-aware merge tree comparisons help to detect transitions, clusters, and periodicities of time-varying datasets, as well as to diagnose and highlight the topological changes between adjacent data instances.",
                        "uid": "v-tvcg-9744472",
                        "file_name": "",
                        "time_stamp": "2022-10-21T14:00:00Z",
                        "time_start": "2022-10-21T14:00:00Z",
                        "time_end": "2022-10-21T14:10:00Z",
                        "paper_type": "full",
                        "keywords": "Merge trees, merge tree metrics, topological data analysis, topology in visualization",
                        "has_image": "1",
                        "has_video": "585",
                        "paper_award": "",
                        "image_caption": "In this paper, we perform geometry-aware comparisons of merge trees using labeled interleaving distances. The main idea is to decouple the computation of a comparative measure into two steps: a labeling step that generates a correspondence between the critical points of two merge trees, and a comparison step that computes distances between a pair of labeled merge trees by encoding them as matrices. We demonstrate via experiments that such geometry-aware merge tree comparisons help to detect transitions, clusters, and periodicities of time-varying datasets.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-tvcg-9744472-qa",
                        "session_id": "full22",
                        "type": "In Person Q+A",
                        "title": "Geometry-Aware Merge Tree Comparisons for Time-Varying Data with Interleaving Distances (Q+A)",
                        "contributors": [
                            "Lin Yan"
                        ],
                        "authors": [],
                        "abstract": "Merge trees, a type of topological descriptor, serve to identify and summarize the topological characteristics associated with scalar fields. They have great potential for analyzing and visualizing time-varying data. First, they give compressed and topology-preserving representations of data instances. Second, their comparisons provide a basis for studying the relations among data instances, such as their distributions, clusters, outliers, and periodicities. A number of comparative measures have been developed for merge trees. However, these measures are often computationally expensive since they implicitly consider all possible correspondences between critical points of the merge trees. In this paper, we perform geometry-aware comparisons of merge trees using labeled interleaving distances. The main idea is to decouple the computation of a comparative measure into two steps: a labeling step that generates a correspondence between the critical points of two merge trees, and a comparison step that computes distances between a pair of labeled merge trees by encoding them as matrices. We show that our approach is general, computationally efficient, and practically useful. Our framework makes it possible to integrate geometric information of the data domain in the labeling process. At the same time, the framework reduces the computational complexity since not all possible correspondences have to be considered. We demonstrate via experiments that such geometry-aware merge tree comparisons help to detect transitions, clusters, and periodicities of time-varying datasets, as well as to diagnose and highlight the topological changes between adjacent data instances.",
                        "uid": "v-tvcg-9744472",
                        "file_name": "",
                        "time_stamp": "2022-10-21T14:10:00Z",
                        "time_start": "2022-10-21T14:10:00Z",
                        "time_end": "2022-10-21T14:12:00Z",
                        "paper_type": "full",
                        "keywords": "Merge trees, merge tree metrics, topological data analysis, topology in visualization",
                        "has_image": "1",
                        "has_video": "585",
                        "paper_award": "",
                        "image_caption": "In this paper, we perform geometry-aware comparisons of merge trees using labeled interleaving distances. The main idea is to decouple the computation of a comparative measure into two steps: a labeling step that generates a correspondence between the critical points of two merge trees, and a comparison step that computes distances between a pair of labeled merge trees by encoding them as matrices. We demonstrate via experiments that such geometry-aware merge tree comparisons help to detect transitions, clusters, and periodicities of time-varying datasets.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-tvcg-9468958-pres",
                        "session_id": "full22",
                        "type": "Virtual Presentation (pre-recorded)",
                        "title": "Interpretable Anomaly Detection in Event Sequences via Sequence Matching and Visual Comparison",
                        "contributors": [
                            "Shunan Guo"
                        ],
                        "authors": [
                            "Shunan Guo",
                            "Zhuochen Jin",
                            "Qing Chen",
                            "David Gotz",
                            "Hongyuan Zha",
                            "Nan Cao"
                        ],
                        "abstract": "Anomaly detection is a common analytical task that aims to identify rare cases that differ from the typical cases that make up the majority of a dataset. When analyzing event sequence data, the task of anomaly detection can be complex because the sequential and temporal nature of such data results in diverse definitions and flexible forms of anomalies. This, in turn, increases the difficulty in interpreting detected anomalies. In this paper, we propose a visual analytic approach for detecting anomalous sequences in an event sequence dataset via an unsupervised anomaly detection algorithm based on Variational AutoEncoders. We further compare the anomalous sequences with their reconstructions and with the normal sequences through a sequence matching algorithm to identify event anomalies. A visual analytics system is developed to support interactive exploration and interpretations of anomalies through novel visualization designs that facilitate the comparison between anomalous sequences and normal sequences. Finally, we quantitatively evaluate the performance of our anomaly detection algorithm, demonstrate the effectiveness of our system through case studies, and report feedback collected from study participants.",
                        "uid": "v-tvcg-9468958",
                        "file_name": "",
                        "time_stamp": "2022-10-21T14:12:00Z",
                        "time_start": "2022-10-21T14:12:00Z",
                        "time_end": "2022-10-21T14:22:00Z",
                        "paper_type": "full",
                        "keywords": "Anomaly detection, Data models, Data visualizations, Task analysis, Sequences, Heart, Diabetes",
                        "has_image": "1",
                        "has_video": "670",
                        "paper_award": "",
                        "image_caption": "The user interface of the visual analytics system, which consists of seven key views to support comparison-based visual anomaly detection: (1) anomaly overview, (2) similarity view, (3) reconstruction view, (4) anomalous sequence view, (5) normal sequence view with two variants(5A, 5B) in cluster mode and sequence mode, respectively, (6) anomalous record view and (7) similar record list.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-tvcg-9468958-qa",
                        "session_id": "full22",
                        "type": "Virtual Q+A",
                        "title": "Interpretable Anomaly Detection in Event Sequences via Sequence Matching and Visual Comparison (Q+A)",
                        "contributors": [
                            "Shunan Guo"
                        ],
                        "authors": [],
                        "abstract": "Anomaly detection is a common analytical task that aims to identify rare cases that differ from the typical cases that make up the majority of a dataset. When analyzing event sequence data, the task of anomaly detection can be complex because the sequential and temporal nature of such data results in diverse definitions and flexible forms of anomalies. This, in turn, increases the difficulty in interpreting detected anomalies. In this paper, we propose a visual analytic approach for detecting anomalous sequences in an event sequence dataset via an unsupervised anomaly detection algorithm based on Variational AutoEncoders. We further compare the anomalous sequences with their reconstructions and with the normal sequences through a sequence matching algorithm to identify event anomalies. A visual analytics system is developed to support interactive exploration and interpretations of anomalies through novel visualization designs that facilitate the comparison between anomalous sequences and normal sequences. Finally, we quantitatively evaluate the performance of our anomaly detection algorithm, demonstrate the effectiveness of our system through case studies, and report feedback collected from study participants.",
                        "uid": "v-tvcg-9468958",
                        "file_name": "",
                        "time_stamp": "2022-10-21T14:22:00Z",
                        "time_start": "2022-10-21T14:22:00Z",
                        "time_end": "2022-10-21T14:24:00Z",
                        "paper_type": "full",
                        "keywords": "Anomaly detection, Data models, Data visualizations, Task analysis, Sequences, Heart, Diabetes",
                        "has_image": "1",
                        "has_video": "670",
                        "paper_award": "",
                        "image_caption": "The user interface of the visual analytics system, which consists of seven key views to support comparison-based visual anomaly detection: (1) anomaly overview, (2) similarity view, (3) reconstruction view, (4) anomalous sequence view, (5) normal sequence view with two variants(5A, 5B) in cluster mode and sequence mode, respectively, (6) anomalous record view and (7) similar record list.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-tvcg-9585392-pres",
                        "session_id": "full22",
                        "type": "In Person Presentation",
                        "title": "Comparative Analysis of Merge Trees using Local Tree Edit Distance",
                        "contributors": [
                            "Vijay Natarajan",
                            "Raghavendra Sridharamurthy"
                        ],
                        "authors": [
                            "Raghavendra Sridharamurthy",
                            "Vijay Natarajan"
                        ],
                        "abstract": "Comparative analysis of scalar fields is an important problem with various applications including feature-directed visualization and feature tracking in time-varying data. Comparing topological structures that are abstract and succinct representations of the scalar fields lead to faster and meaningful comparison. While there are many distance or similarity measures to compare topological structures in a global context, there are no known measures for comparing topological structures locally. While the global measures have many applications, they do not directly lend themselves to fine-grained analysis across multiple scales. We define a local variant of the tree edit distance and apply it towards local comparative analysis of merge trees with support for finer analysis. We also present experimental results on time-varying scalar fields, 3D cryo-electron microscopy data, and other synthetic data sets to show the utility of this approach in applications like symmetry detection and feature tracking.",
                        "uid": "v-tvcg-9585392",
                        "file_name": "",
                        "time_stamp": "2022-10-21T14:24:00Z",
                        "time_start": "2022-10-21T14:24:00Z",
                        "time_end": "2022-10-21T14:34:00Z",
                        "paper_type": "full",
                        "keywords": "Merge tree, scalar field, local distance measure, persistence, edit distance, symmetry detection, feature tracking.",
                        "has_image": "1",
                        "has_video": "454",
                        "paper_award": "",
                        "image_caption": "Local Merge Tree Edit Distance (LMTED) helps in local comparison of scalar fields and facilitates lot of applications like symmetry detection, feature tracking, analysis of effects of topological compression.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-tvcg-9585392-qa",
                        "session_id": "full22",
                        "type": "In Person Q+A",
                        "title": "Comparative Analysis of Merge Trees using Local Tree Edit Distance (Q+A)",
                        "contributors": [
                            "Vijay Natarajan",
                            "Raghavendra Sridharamurthy"
                        ],
                        "authors": [],
                        "abstract": "Comparative analysis of scalar fields is an important problem with various applications including feature-directed visualization and feature tracking in time-varying data. Comparing topological structures that are abstract and succinct representations of the scalar fields lead to faster and meaningful comparison. While there are many distance or similarity measures to compare topological structures in a global context, there are no known measures for comparing topological structures locally. While the global measures have many applications, they do not directly lend themselves to fine-grained analysis across multiple scales. We define a local variant of the tree edit distance and apply it towards local comparative analysis of merge trees with support for finer analysis. We also present experimental results on time-varying scalar fields, 3D cryo-electron microscopy data, and other synthetic data sets to show the utility of this approach in applications like symmetry detection and feature tracking.",
                        "uid": "v-tvcg-9585392",
                        "file_name": "",
                        "time_stamp": "2022-10-21T14:34:00Z",
                        "time_start": "2022-10-21T14:34:00Z",
                        "time_end": "2022-10-21T14:36:00Z",
                        "paper_type": "full",
                        "keywords": "Merge tree, scalar field, local distance measure, persistence, edit distance, symmetry detection, feature tracking.",
                        "has_image": "1",
                        "has_video": "454",
                        "paper_award": "",
                        "image_caption": "Local Merge Tree Edit Distance (LMTED) helps in local comparison of scalar fields and facilitates lot of applications like symmetry detection, feature tracking, analysis of effects of topological compression.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-tvcg-9729550-pres",
                        "session_id": "full22",
                        "type": "In Person Presentation",
                        "title": "Visual Exploration of Relationships and Structure in Low-Dimensional Embeddings",
                        "contributors": [
                            "Klaus Eckelt"
                        ],
                        "authors": [
                            "Klaus Eckelt",
                            "Andreas Hinterreiter",
                            "Patrick Adelberger",
                            "Conny Walchshofer",
                            "Vaishali Dhanoa",
                            "Christina Humer",
                            "Moritz Heckmann",
                            "Christian A. Steinparz",
                            "Marc Streit"
                        ],
                        "abstract": "In this work, we propose an interactive visual approach for the exploration and formation of structural relationships in embeddings of high-dimensional data. These structural relationships, such as item sequences, associations of items with groups, and hierarchies between groups of items, are defining properties of many real-world datasets. Nevertheless, most existing methods for the visual exploration of embeddings treat these structures as second-class citizens or do not take them into account at all. In our proposed analysis workflow, users explore enriched scatterplots of the embedding, in which relationships between items and\\slash or groups are visually highlighted. The original high-dimensional data for single items, groups of items, or differences between connected items and groups are accessible through additional summary visualizations. We carefully tailored these summary and difference visualizations to the various data types and semantic contexts. During their exploratory analysis, users can externalize their insights by setting up additional groups and relationships between items and\\slash or groups. We demonstrate the utility and potential impact of our approach by means of two use cases and multiple examples from various domains.",
                        "uid": "v-tvcg-9729550",
                        "file_name": "",
                        "time_stamp": "2022-10-21T14:36:00Z",
                        "time_start": "2022-10-21T14:36:00Z",
                        "time_end": "2022-10-21T14:46:00Z",
                        "paper_type": "full",
                        "keywords": "Dimensionality reduction, projection, visual analytics, layout enrichment, aggregation, comparison",
                        "has_image": "1",
                        "has_video": "556",
                        "paper_award": "",
                        "image_caption": "The figure shows an embedding of 450 chess games. Each game consists of multiple states; one for each player move. The states of a game are connected in order to follow the course of the game. Groups of game states are represented by diamond symbols. Group A represents the start of all chess games, which splits up into groups for three different openings: Zukertort (B, green), English (C, pink), and Queen's Pawn (D, orange) opening. Groups E-I contain middlegame moves, and group J endgame moves. In the comparison pane on the right, summaries and differences of middlegame groups are shown.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-tvcg-9729550-qa",
                        "session_id": "full22",
                        "type": "In Person Q+A",
                        "title": "Visual Exploration of Relationships and Structure in Low-Dimensional Embeddings (Q+A)",
                        "contributors": [
                            "Klaus Eckelt"
                        ],
                        "authors": [],
                        "abstract": "In this work, we propose an interactive visual approach for the exploration and formation of structural relationships in embeddings of high-dimensional data. These structural relationships, such as item sequences, associations of items with groups, and hierarchies between groups of items, are defining properties of many real-world datasets. Nevertheless, most existing methods for the visual exploration of embeddings treat these structures as second-class citizens or do not take them into account at all. In our proposed analysis workflow, users explore enriched scatterplots of the embedding, in which relationships between items and\\slash or groups are visually highlighted. The original high-dimensional data for single items, groups of items, or differences between connected items and groups are accessible through additional summary visualizations. We carefully tailored these summary and difference visualizations to the various data types and semantic contexts. During their exploratory analysis, users can externalize their insights by setting up additional groups and relationships between items and\\slash or groups. We demonstrate the utility and potential impact of our approach by means of two use cases and multiple examples from various domains.",
                        "uid": "v-tvcg-9729550",
                        "file_name": "",
                        "time_stamp": "2022-10-21T14:46:00Z",
                        "time_start": "2022-10-21T14:46:00Z",
                        "time_end": "2022-10-21T14:48:00Z",
                        "paper_type": "full",
                        "keywords": "Dimensionality reduction, projection, visual analytics, layout enrichment, aggregation, comparison",
                        "has_image": "1",
                        "has_video": "556",
                        "paper_award": "",
                        "image_caption": "The figure shows an embedding of 450 chess games. Each game consists of multiple states; one for each player move. The states of a game are connected in order to follow the course of the game. Groups of game states are represented by diamond symbols. Group A represents the start of all chess games, which splits up into groups for three different openings: Zukertort (B, green), English (C, pink), and Queen's Pawn (D, orange) opening. Groups E-I contain middlegame moves, and group J endgame moves. In the comparison pane on the right, summaries and differences of middlegame groups are shown.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-tvcg-9716867-pres",
                        "session_id": "full22",
                        "type": "Virtual Presentation (pre-recorded)",
                        "title": "View Composition Algebra for Ad Hoc Comparison",
                        "contributors": [
                            "Eugene Wu"
                        ],
                        "authors": [
                            "Eugene Wu"
                        ],
                        "abstract": "Comparison is a core task in visual analysis.  Although there are numerous guidelines to help users design effective visualizations to aid known comparison tasks, there are few techniques available when users want to make ad hoc comparisons between marks, trends, or charts during data exploration and visual analysis.  For instance, to compare voting count maps from different years, two stock trends in a line chart, or a scatterplot of country GDPs with a textual summary of the average GDP.  Ideally, users can directly select the comparison targets and compare them, however what elements of a visualization should be candidate targets, which combinations of targets are safe to compare, and what comparison operations make sense?  This paper proposes a conceptual model that lets users compose combinations of values, marks, legend elements, and charts using a set of composition operators that summarize, compute differences, merge, and model their operands.  We further define a View Composition Algebra (VCA) that is compatible with datacube-based visualizations, derive an interaction design based on this algebra that supports ad hoc visual comparisons, and illustrate its utility through several use cases.",
                        "uid": "v-tvcg-9716867",
                        "file_name": "",
                        "time_stamp": "2022-10-21T14:48:00Z",
                        "time_start": "2022-10-21T14:48:00Z",
                        "time_end": "2022-10-21T14:58:00Z",
                        "paper_type": "full",
                        "keywords": "Visualization, Algebra, Comparison, Databases",
                        "has_image": "1",
                        "has_video": "573",
                        "paper_award": "",
                        "image_caption": "View Composition Algebra for Ad-hoc Comparisons",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-tvcg-9716867-qa",
                        "session_id": "full22",
                        "type": "Virtual Q+A",
                        "title": "View Composition Algebra for Ad Hoc Comparison (Q+A)",
                        "contributors": [
                            "Eugene Wu"
                        ],
                        "authors": [],
                        "abstract": "Comparison is a core task in visual analysis.  Although there are numerous guidelines to help users design effective visualizations to aid known comparison tasks, there are few techniques available when users want to make ad hoc comparisons between marks, trends, or charts during data exploration and visual analysis.  For instance, to compare voting count maps from different years, two stock trends in a line chart, or a scatterplot of country GDPs with a textual summary of the average GDP.  Ideally, users can directly select the comparison targets and compare them, however what elements of a visualization should be candidate targets, which combinations of targets are safe to compare, and what comparison operations make sense?  This paper proposes a conceptual model that lets users compose combinations of values, marks, legend elements, and charts using a set of composition operators that summarize, compute differences, merge, and model their operands.  We further define a View Composition Algebra (VCA) that is compatible with datacube-based visualizations, derive an interaction design based on this algebra that supports ad hoc visual comparisons, and illustrate its utility through several use cases.",
                        "uid": "v-tvcg-9716867",
                        "file_name": "",
                        "time_stamp": "2022-10-21T14:58:00Z",
                        "time_start": "2022-10-21T14:58:00Z",
                        "time_end": "2022-10-21T15:00:00Z",
                        "paper_type": "full",
                        "keywords": "Visualization, Algebra, Comparison, Databases",
                        "has_image": "1",
                        "has_video": "573",
                        "paper_award": "",
                        "image_caption": "View Composition Algebra for Ad-hoc Comparisons",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-full-1607-pres",
                        "session_id": "full22",
                        "type": "Virtual Presentation (pre-recorded)",
                        "title": "Visual Comparison of Language Model Adaptation",
                        "contributors": [
                            "Rita Sevastjanova"
                        ],
                        "authors": [
                            "Rita Sevastjanova",
                            "Eren Cakmak",
                            "Shauli Ravfogel",
                            "Ryan Cotterell",
                            "Mennatallah El-Assady"
                        ],
                        "abstract": "",
                        "uid": "v-full-1607",
                        "file_name": "",
                        "time_stamp": "2022-10-21T15:00:00Z",
                        "time_start": "2022-10-21T15:00:00Z",
                        "time_end": "2022-10-21T15:10:00Z",
                        "paper_type": "full",
                        "keywords": "",
                        "has_image": "1",
                        "has_video": "599",
                        "paper_award": "",
                        "image_caption": "We present a workspace that enables the evaluation and comparison of adapters \u2013 lightweight alternatives for language model fine-tuning. After data pre-processing (e.g., embedding extraction), users can select pre-trained adapters, create explanations, and explore model differences through three types of visualizations: Concept Embedding Similarity, Concept Embedding Projection, and Concept Prediction Similarity. The explanations are provided for single models as well as model comparisons. Here: contrary to\nthe rotten-tomatoes sentiment classifier, the context-0 embeddings of the sst-2 sentiment classifier strongly encode the two polarities of human qualities.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-full-1607-qa",
                        "session_id": "full22",
                        "type": "Virtual Q+A",
                        "title": "Visual Comparison of Language Model Adaptation (Q+A)",
                        "contributors": [
                            "Rita Sevastjanova"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "v-full-1607",
                        "file_name": "",
                        "time_stamp": "2022-10-21T15:10:00Z",
                        "time_start": "2022-10-21T15:10:00Z",
                        "time_end": "2022-10-21T15:12:00Z",
                        "paper_type": "full",
                        "keywords": "",
                        "has_image": "1",
                        "has_video": "599",
                        "paper_award": "",
                        "image_caption": "We present a workspace that enables the evaluation and comparison of adapters \u2013 lightweight alternatives for language model fine-tuning. After data pre-processing (e.g., embedding extraction), users can select pre-trained adapters, create explanations, and explore model differences through three types of visualizations: Concept Embedding Similarity, Concept Embedding Projection, and Concept Prediction Similarity. The explanations are provided for single models as well as model comparisons. Here: contrary to\nthe rotten-tomatoes sentiment classifier, the context-0 embeddings of the sst-2 sentiment classifier strongly encode the two polarities of human qualities.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    }
                ]
            },
            {
                "title": "Topology",
                "session_id": "full23",
                "event_prefix": "v-full",
                "track": "ok5",
                "livestream_id": "ok5-thu",
                "session_image": "full23.png",
                "chair": [
                    "Julien Tierny"
                ],
                "organizers": [],
                "time_start": "2022-10-20T20:45:00Z",
                "time_end": "2022-10-20T22:00:00Z",
                "discord_category": "",
                "discord_channel": "",
                "discord_channel_id": "",
                "discord_link": "",
                "slido_link": "",
                "youtube_url": "",
                "zoom_meeting": "",
                "zoom_password": "",
                "zoom_link": "",
                "ff_link": "",
                "ff_playlist": "",
                "time_slots": [
                    {
                        "slot_id": "full23-opening",
                        "session_id": "full23",
                        "type": "In Person Other",
                        "title": "Session Opening",
                        "contributors": [
                            "Julien Tierny"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "",
                        "file_name": "",
                        "time_stamp": "2022-10-20T20:45:00Z",
                        "time_start": "2022-10-20T20:45:00Z",
                        "time_end": "2022-10-20T20:45:00Z",
                        "paper_type": "",
                        "keywords": "",
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-full-1051-pres",
                        "session_id": "full23",
                        "type": "In Person Presentation",
                        "title": "Temporal Merge Tree Maps: A Topology-Based Static Visualization for Temporal Scalar Data",
                        "contributors": [
                            "Wiebke K\u00f6pp"
                        ],
                        "authors": [
                            "Wiebke K\u00f6pp",
                            "Tino Weinkauf"
                        ],
                        "abstract": "",
                        "uid": "v-full-1051",
                        "file_name": "",
                        "time_stamp": "2022-10-20T20:45:00Z",
                        "time_start": "2022-10-20T20:45:00Z",
                        "time_end": "2022-10-20T20:55:00Z",
                        "paper_type": "full",
                        "keywords": "",
                        "has_image": "1",
                        "has_video": "1325",
                        "paper_award": "",
                        "image_caption": "A temporal merge tree map (top) is a static visualization of a time-dependent scalar field (bottom). Our method uses augmented merge trees to map the data samples of each time step to a vertical slice. An optimization scheme is employed to achieve a temporally coherent mapping. The shown Storms data set represents storm activity over Europe in December 1999 and contains 744 time steps, arranged from left to right. Cyclones can easily be identified as dark blue lines and compared with each other while still being shown in the context of the entire data.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-full-1051-qa",
                        "session_id": "full23",
                        "type": "In Person Q+A",
                        "title": "Temporal Merge Tree Maps: A Topology-Based Static Visualization for Temporal Scalar Data (Q+A)",
                        "contributors": [
                            "Wiebke K\u00f6pp"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "v-full-1051",
                        "file_name": "",
                        "time_stamp": "2022-10-20T20:55:00Z",
                        "time_start": "2022-10-20T20:55:00Z",
                        "time_end": "2022-10-20T20:57:00Z",
                        "paper_type": "full",
                        "keywords": "",
                        "has_image": "1",
                        "has_video": "1325",
                        "paper_award": "",
                        "image_caption": "A temporal merge tree map (top) is a static visualization of a time-dependent scalar field (bottom). Our method uses augmented merge trees to map the data samples of each time step to a vertical slice. An optimization scheme is employed to achieve a temporally coherent mapping. The shown Storms data set represents storm activity over Europe in December 1999 and contains 744 time steps, arranged from left to right. Cyclones can easily be identified as dark blue lines and compared with each other while still being shown in the context of the entire data.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-tvcg-9583888-pres",
                        "session_id": "full23",
                        "type": "In Person Presentation",
                        "title": "TopoCluster: A Localized Data Structure for Topology-based Visualization",
                        "contributors": [
                            "Guoxi Liu"
                        ],
                        "authors": [
                            "Guoxi Liu",
                            "Federico Iuricich",
                            "Riccardo Fellegara",
                            "Leila De Floriani"
                        ],
                        "abstract": "Unstructured data are collections of points with irregular topology, often represented through simplicial meshes, such as triangle and tetrahedral meshes. Whenever possible such representations are avoided in visualization since they are computationally demanding if compared with regular grids. In this work, we aim at simplifying the encoding and processing of simplicial meshes. The paper proposes TopoCluster, a new localized data structure for tetrahedral meshes. TopoCluster provides efficient computation of the connectivity of the mesh elements with a low memory footprint. The key idea of TopoCluster is to subdivide the simplicial mesh into clusters. Then, the connectivity information is computed locally for each cluster and discarded when it is no longer needed. We define two instances of TopoCluster. The first instance prioritizes time efficiency and provides only modest savings in memory, while the second instance drastically reduces memory consumption up to an order of magnitude with respect to comparable data structures. Thanks to the simple interface provided by TopoCluster, we have been able to integrate both data structures into the existing Topological Toolkit (TTK) framework. As a result, users can run any plugin of TTK using TopoCluster without changing a single line of code.",
                        "uid": "v-tvcg-9583888",
                        "file_name": "",
                        "time_stamp": "2022-10-20T20:57:00Z",
                        "time_start": "2022-10-20T20:57:00Z",
                        "time_end": "2022-10-20T21:07:00Z",
                        "paper_type": "full",
                        "keywords": "Data visualization, data structures, topological data analysis, simplicial meshes, tetrahedral meshes",
                        "has_image": "1",
                        "has_video": "493",
                        "paper_award": "",
                        "image_caption": "An example of the TopoCluster data structure showing the localized computation of topological relations on the dragon dataset and the performance comparison with other data structures.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-tvcg-9583888-qa",
                        "session_id": "full23",
                        "type": "In Person Q+A",
                        "title": "TopoCluster: A Localized Data Structure for Topology-based Visualization (Q+A)",
                        "contributors": [
                            "Guoxi Liu"
                        ],
                        "authors": [],
                        "abstract": "Unstructured data are collections of points with irregular topology, often represented through simplicial meshes, such as triangle and tetrahedral meshes. Whenever possible such representations are avoided in visualization since they are computationally demanding if compared with regular grids. In this work, we aim at simplifying the encoding and processing of simplicial meshes. The paper proposes TopoCluster, a new localized data structure for tetrahedral meshes. TopoCluster provides efficient computation of the connectivity of the mesh elements with a low memory footprint. The key idea of TopoCluster is to subdivide the simplicial mesh into clusters. Then, the connectivity information is computed locally for each cluster and discarded when it is no longer needed. We define two instances of TopoCluster. The first instance prioritizes time efficiency and provides only modest savings in memory, while the second instance drastically reduces memory consumption up to an order of magnitude with respect to comparable data structures. Thanks to the simple interface provided by TopoCluster, we have been able to integrate both data structures into the existing Topological Toolkit (TTK) framework. As a result, users can run any plugin of TTK using TopoCluster without changing a single line of code.",
                        "uid": "v-tvcg-9583888",
                        "file_name": "",
                        "time_stamp": "2022-10-20T21:07:00Z",
                        "time_start": "2022-10-20T21:07:00Z",
                        "time_end": "2022-10-20T21:09:00Z",
                        "paper_type": "full",
                        "keywords": "Data visualization, data structures, topological data analysis, simplicial meshes, tetrahedral meshes",
                        "has_image": "1",
                        "has_video": "493",
                        "paper_award": "",
                        "image_caption": "An example of the TopoCluster data structure showing the localized computation of topological relations on the dragon dataset and the performance comparison with other data structures.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-full-1233-pres",
                        "session_id": "full23",
                        "type": "Virtual Presentation (pre-recorded)",
                        "title": "Computing a Stable Distance on Merge Trees",
                        "contributors": [
                            "Brian C Bollen"
                        ],
                        "authors": [
                            "Brian C Bollen",
                            "Joshua A Levine",
                            "Pasindu P. Tennakoon"
                        ],
                        "abstract": "",
                        "uid": "v-full-1233",
                        "file_name": "",
                        "time_stamp": "2022-10-20T21:09:00Z",
                        "time_start": "2022-10-20T21:09:00Z",
                        "time_end": "2022-10-20T21:19:00Z",
                        "paper_type": "full",
                        "keywords": "",
                        "has_image": "1",
                        "has_video": "581",
                        "paper_award": "",
                        "image_caption": "We construct a novel distance between merge tree which exhibit two properties that are desirable for the analysis of scalar field data -- stability and discriminativity. We provide an accompanying computation to this distance and an accompanying proof that persistence simplification can provide an approximation to this distance while reducing computation time drastically. We achieve this by drawing inspriation from the well-known graph-edit distance problem, the definition and construction of the bottleneck distance, and the stability handling of the universal distance on merge trees.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-full-1233-qa",
                        "session_id": "full23",
                        "type": "Virtual Q+A",
                        "title": "Computing a Stable Distance on Merge Trees (Q+A)",
                        "contributors": [
                            "Brian C Bollen"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "v-full-1233",
                        "file_name": "",
                        "time_stamp": "2022-10-20T21:19:00Z",
                        "time_start": "2022-10-20T21:19:00Z",
                        "time_end": "2022-10-20T21:21:00Z",
                        "paper_type": "full",
                        "keywords": "",
                        "has_image": "1",
                        "has_video": "581",
                        "paper_award": "",
                        "image_caption": "We construct a novel distance between merge tree which exhibit two properties that are desirable for the analysis of scalar field data -- stability and discriminativity. We provide an accompanying computation to this distance and an accompanying proof that persistence simplification can provide an approximation to this distance while reducing computation time drastically. We achieve this by drawing inspriation from the well-known graph-edit distance problem, the definition and construction of the bottleneck distance, and the stability handling of the universal distance on merge trees.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-tvcg-9721603-pres",
                        "session_id": "full23",
                        "type": "In Person Presentation",
                        "title": "Topological Simplifications of Hypergraphs",
                        "contributors": [
                            "Youjia Zhou"
                        ],
                        "authors": [
                            "Youjia Zhou",
                            "Archit Rathore",
                            "Emilie Purvine",
                            "Bei Wang"
                        ],
                        "abstract": "We study hypergraph visualization via its topological simplification. We explore both vertex simplification and hyperedge simplification of hypergraphs using tools from topological data analysis. In particular, we transform a hypergraph into its graph representations known as the line graph and clique expansion. A topological simplification of such a graph representation induces a simplification of the hypergraph. In simplifying a hypergraph, we allow vertices to be combined if they belong to almost the same set of hyperedges, and hyperedges to be merged if they share almost the same set of vertices. Our proposed approaches are general, mathematically justifiable, and put vertex simplification and hyperedge simplification in a unifying framework.",
                        "uid": "v-tvcg-9721603",
                        "file_name": "",
                        "time_stamp": "2022-10-20T21:21:00Z",
                        "time_start": "2022-10-20T21:21:00Z",
                        "time_end": "2022-10-20T21:31:00Z",
                        "paper_type": "full",
                        "keywords": "Hypergraph simplification, hypergraph visualization, graph simplification, topological data analysis",
                        "has_image": "1",
                        "has_video": "881",
                        "paper_award": "",
                        "image_caption": "We propose a topological simplification framework for hypergraphs, and provide an interactive tool that allows users to explore the simplification framework and apply vertex and hyperedge simplifications to gain insights from their own datasets. The image demonstrates the original hypergraph (a) from an active Domain Name System (DNS) dataset and the simplified hypergraph (c) after hyperedge simplification.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-tvcg-9721603-qa",
                        "session_id": "full23",
                        "type": "In Person Q+A",
                        "title": "Topological Simplifications of Hypergraphs (Q+A)",
                        "contributors": [
                            "Youjia Zhou"
                        ],
                        "authors": [],
                        "abstract": "We study hypergraph visualization via its topological simplification. We explore both vertex simplification and hyperedge simplification of hypergraphs using tools from topological data analysis. In particular, we transform a hypergraph into its graph representations known as the line graph and clique expansion. A topological simplification of such a graph representation induces a simplification of the hypergraph. In simplifying a hypergraph, we allow vertices to be combined if they belong to almost the same set of hyperedges, and hyperedges to be merged if they share almost the same set of vertices. Our proposed approaches are general, mathematically justifiable, and put vertex simplification and hyperedge simplification in a unifying framework.",
                        "uid": "v-tvcg-9721603",
                        "file_name": "",
                        "time_stamp": "2022-10-20T21:31:00Z",
                        "time_start": "2022-10-20T21:31:00Z",
                        "time_end": "2022-10-20T21:33:00Z",
                        "paper_type": "full",
                        "keywords": "Hypergraph simplification, hypergraph visualization, graph simplification, topological data analysis",
                        "has_image": "1",
                        "has_video": "881",
                        "paper_award": "",
                        "image_caption": "We propose a topological simplification framework for hypergraphs, and provide an interactive tool that allows users to explore the simplification framework and apply vertex and hyperedge simplifications to gain insights from their own datasets. The image demonstrates the original hypergraph (a) from an active Domain Name System (DNS) dataset and the simplified hypergraph (c) after hyperedge simplification.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-tvcg-9531544-pres",
                        "session_id": "full23",
                        "type": "In Person Presentation",
                        "title": "Persistence cycles for visual exploration of persistent homology",
                        "contributors": [
                            "Federico Iuricich"
                        ],
                        "authors": [
                            "Federico Iuricich"
                        ],
                        "abstract": "Persistent homology is a fundamental tool in topological data analysis used for the most diverse applications. Information captured by persistent homology is commonly visualized using scatter plots representations. Despite being widely adopted, such a visualization technique limits user understanding and is prone to misinterpretation. This paper proposes a new approach for the efficient computation of persistence cycles, a geometric representation of the features captured by persistent homology. We illustrate the importance of rendering persistence cycles when analyzing scalar fields, and we discuss the advantages that our approach provides compared to other techniques in topology-based visualization. We provide an efficient implementation of our approach based on discrete Morse theory, as a new module for the Topology Toolkit. We show that our implementation has comparable performance with respect to state-of-the-art toolboxes while providing a better framework for visually analyzing persistent homology information.",
                        "uid": "v-tvcg-9531544",
                        "file_name": "",
                        "time_stamp": "2022-10-20T21:33:00Z",
                        "time_start": "2022-10-20T21:33:00Z",
                        "time_end": "2022-10-20T21:43:00Z",
                        "paper_type": "full",
                        "keywords": "Persistent homology, Topological Data Analysis, Scalar fields",
                        "has_image": "1",
                        "has_video": "669",
                        "paper_award": "",
                        "image_caption": "Example of persistence 1-cycles computed on the Silicium dataset. Each persistence 1-cycle can be found in the correspondence of a pair of 1-saddle 2-saddle critical points and identifies a tunnel/handle in the dataset.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-tvcg-9531544-qa",
                        "session_id": "full23",
                        "type": "In Person Q+A",
                        "title": "Persistence cycles for visual exploration of persistent homology (Q+A)",
                        "contributors": [
                            "Federico Iuricich"
                        ],
                        "authors": [],
                        "abstract": "Persistent homology is a fundamental tool in topological data analysis used for the most diverse applications. Information captured by persistent homology is commonly visualized using scatter plots representations. Despite being widely adopted, such a visualization technique limits user understanding and is prone to misinterpretation. This paper proposes a new approach for the efficient computation of persistence cycles, a geometric representation of the features captured by persistent homology. We illustrate the importance of rendering persistence cycles when analyzing scalar fields, and we discuss the advantages that our approach provides compared to other techniques in topology-based visualization. We provide an efficient implementation of our approach based on discrete Morse theory, as a new module for the Topology Toolkit. We show that our implementation has comparable performance with respect to state-of-the-art toolboxes while providing a better framework for visually analyzing persistent homology information.",
                        "uid": "v-tvcg-9531544",
                        "file_name": "",
                        "time_stamp": "2022-10-20T21:43:00Z",
                        "time_start": "2022-10-20T21:43:00Z",
                        "time_end": "2022-10-20T21:45:00Z",
                        "paper_type": "full",
                        "keywords": "Persistent homology, Topological Data Analysis, Scalar fields",
                        "has_image": "1",
                        "has_video": "669",
                        "paper_award": "",
                        "image_caption": "Example of persistence 1-cycles computed on the Silicium dataset. Each persistence 1-cycle can be found in the correspondence of a pair of 1-saddle 2-saddle critical points and identifies a tunnel/handle in the dataset.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-tvcg-9721643-pres",
                        "session_id": "full23",
                        "type": "Virtual Presentation (pre-recorded)",
                        "title": "Geometry-Aware Planar Embedding of Treelike Structures",
                        "contributors": [
                            "Ping Hu"
                        ],
                        "authors": [
                            "Ping Hu",
                            "Saeed Boorboor",
                            "Joseph Marino",
                            "Arie E. Kaufman"
                        ],
                        "abstract": "The growing complexity of spatial and structural information in 3D data makes data inspection and visualization a challenging task. We describe a method to create a planar embedding of 3D treelike structures using their skeleton representations. Our method maintains the original geometry, without overlaps, to the best extent possible, allowing exploration of the topology within a single view. We present a novel camera view generation method which maximizes the visible geometric attributes (segment shape and relative placement between segments). Camera views are created for individual segments and are used to determine local bending angles at each node by projecting them to 2D. The final embedding is generated by minimizing an energy function (the weights of which are user adjustable) based on branch length and the 2D angles, while avoiding intersections. The user can also interactively modify segment placement within the 2D embedding, and the overall embedding will update accordingly. A global to local interactive exploration is provided using hierarchical camera views that are created for subtrees within the structure. We evaluate our method both qualitatively and quantitatively and demonstrate our results by constructing planar visualizations of line data (traced neurons) and volume data (CT vascular and bronchial data).",
                        "uid": "v-tvcg-9721643",
                        "file_name": "",
                        "time_stamp": "2022-10-20T21:45:00Z",
                        "time_start": "2022-10-20T21:45:00Z",
                        "time_end": "2022-10-20T21:55:00Z",
                        "paper_type": "full",
                        "keywords": "I.3.5 Computational Geometry and Object Modeling < I.3 Computer Graphics < I Computing Methodologies, I.6 Simulation, Modeling, and Visualization < I Computing Methodologies",
                        "has_image": "1",
                        "has_video": "661",
                        "paper_award": "",
                        "image_caption": "Our planar embedding method for the cranial blood vessels. Left: Illustration of three camera view fields focusing on a certain subtree structure. The blue curves represent a navigation example which connects the three hierarchical views. Right: Planar embedding result using our energy function that preserves the global shape of the object along with its local morphology, while avoiding intersections. The color represents the blood vessel radius of the segment with respect to the average vessel radius of the entire structure.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-tvcg-9721643-qa",
                        "session_id": "full23",
                        "type": "Virtual Q+A",
                        "title": "Geometry-Aware Planar Embedding of Treelike Structures (Q+A)",
                        "contributors": [
                            "Ping Hu"
                        ],
                        "authors": [],
                        "abstract": "The growing complexity of spatial and structural information in 3D data makes data inspection and visualization a challenging task. We describe a method to create a planar embedding of 3D treelike structures using their skeleton representations. Our method maintains the original geometry, without overlaps, to the best extent possible, allowing exploration of the topology within a single view. We present a novel camera view generation method which maximizes the visible geometric attributes (segment shape and relative placement between segments). Camera views are created for individual segments and are used to determine local bending angles at each node by projecting them to 2D. The final embedding is generated by minimizing an energy function (the weights of which are user adjustable) based on branch length and the 2D angles, while avoiding intersections. The user can also interactively modify segment placement within the 2D embedding, and the overall embedding will update accordingly. A global to local interactive exploration is provided using hierarchical camera views that are created for subtrees within the structure. We evaluate our method both qualitatively and quantitatively and demonstrate our results by constructing planar visualizations of line data (traced neurons) and volume data (CT vascular and bronchial data).",
                        "uid": "v-tvcg-9721643",
                        "file_name": "",
                        "time_stamp": "2022-10-20T21:55:00Z",
                        "time_start": "2022-10-20T21:55:00Z",
                        "time_end": "2022-10-20T21:57:00Z",
                        "paper_type": "full",
                        "keywords": "I.3.5 Computational Geometry and Object Modeling < I.3 Computer Graphics < I Computing Methodologies, I.6 Simulation, Modeling, and Visualization < I Computing Methodologies",
                        "has_image": "1",
                        "has_video": "661",
                        "paper_award": "",
                        "image_caption": "Our planar embedding method for the cranial blood vessels. Left: Illustration of three camera view fields focusing on a certain subtree structure. The blue curves represent a navigation example which connects the three hierarchical views. Right: Planar embedding result using our energy function that preserves the global shape of the object along with its local morphology, while avoiding intersections. The color represents the blood vessel radius of the segment with respect to the average vessel radius of the entire structure.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    }
                ]
            },
            {
                "title": "Graphs and Networks",
                "session_id": "full24",
                "event_prefix": "v-full",
                "track": "ok5",
                "livestream_id": "ok5-thu",
                "session_image": "full24.png",
                "chair": [
                    "Andreas Kerren"
                ],
                "organizers": [],
                "time_start": "2022-10-20T15:45:00Z",
                "time_end": "2022-10-20T17:00:00Z",
                "discord_category": "",
                "discord_channel": "",
                "discord_channel_id": "",
                "discord_link": "",
                "slido_link": "",
                "youtube_url": "",
                "zoom_meeting": "",
                "zoom_password": "",
                "zoom_link": "",
                "ff_link": "",
                "ff_playlist": "",
                "time_slots": [
                    {
                        "slot_id": "full24-opening",
                        "session_id": "full24",
                        "type": "In Person Other",
                        "title": "Session Opening",
                        "contributors": [
                            "Andreas Kerren"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "",
                        "file_name": "",
                        "time_stamp": "2022-10-20T15:45:00Z",
                        "time_start": "2022-10-20T15:45:00Z",
                        "time_end": "2022-10-20T15:45:00Z",
                        "paper_type": "",
                        "keywords": "",
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-full-1551-pres",
                        "session_id": "full24",
                        "type": "In Person Presentation",
                        "title": "MosaicSets: Embedding Set Systems into Grid Graphs",
                        "contributors": [
                            "Markus Wallinger",
                            "Peter Rottmann"
                        ],
                        "authors": [
                            "Peter Rottmann",
                            "Markus Wallinger",
                            "Annika Bonerath",
                            "Sven Gedicke",
                            "Martin N\u00f6llenburg",
                            "Jan-Henrik Haunert"
                        ],
                        "abstract": "",
                        "uid": "v-full-1551",
                        "file_name": "",
                        "time_stamp": "2022-10-20T15:45:00Z",
                        "time_start": "2022-10-20T15:45:00Z",
                        "time_end": "2022-10-20T15:55:00Z",
                        "paper_type": "full",
                        "keywords": "",
                        "has_image": "1",
                        "has_video": "474",
                        "paper_award": "",
                        "image_caption": "We present MosaicSets: a novel approach to create Euler-like diagrams from non-spatial set systems such that each element occupies one cell of a regular hexagonal or square grid. The main challenge is to find an assignment of the elements to the grid cells such that each set constitutes a contiguous region. In fact, this task is NP-hard. Thus, we provide an approach using integer linear programming. In the given figure, we visualize the research groups of the Agricultural Faculty of the University of Bonn with MosaicSets once with a hexagonal grid and once with a square grid.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-full-1551-qa",
                        "session_id": "full24",
                        "type": "In Person Q+A",
                        "title": "MosaicSets: Embedding Set Systems into Grid Graphs (Q+A)",
                        "contributors": [
                            "Markus Wallinger",
                            "Peter Rottmann"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "v-full-1551",
                        "file_name": "",
                        "time_stamp": "2022-10-20T15:55:00Z",
                        "time_start": "2022-10-20T15:55:00Z",
                        "time_end": "2022-10-20T15:57:00Z",
                        "paper_type": "full",
                        "keywords": "",
                        "has_image": "1",
                        "has_video": "474",
                        "paper_award": "",
                        "image_caption": "We present MosaicSets: a novel approach to create Euler-like diagrams from non-spatial set systems such that each element occupies one cell of a regular hexagonal or square grid. The main challenge is to find an assignment of the elements to the grid cells such that each set constitutes a contiguous region. In fact, this task is NP-hard. Thus, we provide an approach using integer linear programming. In the given figure, we visualize the research groups of the Agricultural Faculty of the University of Bonn with MosaicSets once with a hexagonal grid and once with a square grid.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-full-1165-pres",
                        "session_id": "full24",
                        "type": "Virtual Presentation (pre-recorded)",
                        "title": "Taurus: Towards A Unified Force Representation and Universal Solver for Graph Layout",
                        "contributors": [
                            "Yunhai Wang",
                            "Mingliang Xue"
                        ],
                        "authors": [
                            "Mingliang Xue",
                            "Zhi Wang",
                            "Fahai Zhong",
                            "Yong Wang",
                            "Mingliang Xu",
                            "Oliver Deussen",
                            "Yunhai Wang"
                        ],
                        "abstract": "",
                        "uid": "v-full-1165",
                        "file_name": "",
                        "time_stamp": "2022-10-20T15:57:00Z",
                        "time_start": "2022-10-20T15:57:00Z",
                        "time_end": "2022-10-20T16:07:00Z",
                        "paper_type": "full",
                        "keywords": "",
                        "has_image": "1",
                        "has_video": "657",
                        "paper_award": "",
                        "image_caption": "In this paper, we present a general framework, which we call Taurus, Towards A Unified force Representation and Universal Solver for graph layout, that offers a unified view for understanding and comparing most of the popular graph layout algorithms. (A) It is based on a unified force representation, which formulates most existing techniques as a combination of quotient-based forces that combine power functions of graph-theoretical and Euclidean distances. (B) This representation enables us to compare the strengths and weaknesses of existing techniques, while facilitating the development of new methods. Based on this, we propose a new balanced stress model (BSM) that is able to layout graphs in superior quality. (C) To demonstrate the effectiveness of our framework, we compare the implementation of each layout method under our framework with its original or existing implementation. The results show that all methods implemented in Taurus (bottom) are similar to or even better than the original implementations (top), with a largely reduced runtime. (D) We release an open-source package, which facilitates easy comparison of different graph layout methods for any graph input as well as effectively creating customized graph layout techniques.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-full-1165-qa",
                        "session_id": "full24",
                        "type": "Virtual Q+A",
                        "title": "Taurus: Towards A Unified Force Representation and Universal Solver for Graph Layout (Q+A)",
                        "contributors": [
                            "Yunhai Wang",
                            "Mingliang Xue"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "v-full-1165",
                        "file_name": "",
                        "time_stamp": "2022-10-20T16:07:00Z",
                        "time_start": "2022-10-20T16:07:00Z",
                        "time_end": "2022-10-20T16:09:00Z",
                        "paper_type": "full",
                        "keywords": "",
                        "has_image": "1",
                        "has_video": "657",
                        "paper_award": "",
                        "image_caption": "In this paper, we present a general framework, which we call Taurus, Towards A Unified force Representation and Universal Solver for graph layout, that offers a unified view for understanding and comparing most of the popular graph layout algorithms. (A) It is based on a unified force representation, which formulates most existing techniques as a combination of quotient-based forces that combine power functions of graph-theoretical and Euclidean distances. (B) This representation enables us to compare the strengths and weaknesses of existing techniques, while facilitating the development of new methods. Based on this, we propose a new balanced stress model (BSM) that is able to layout graphs in superior quality. (C) To demonstrate the effectiveness of our framework, we compare the implementation of each layout method under our framework with its original or existing implementation. The results show that all methods implemented in Taurus (bottom) are similar to or even better than the original implementations (top), with a largely reduced runtime. (D) We release an open-source package, which facilitates easy comparison of different graph layout methods for any graph input as well as effectively creating customized graph layout techniques.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-tvcg-9705082-pres",
                        "session_id": "full24",
                        "type": "Virtual Presentation (pre-recorded)",
                        "title": "Visualizing Graph Neural Networks with CorGIE: Corresponding a Graph to Its Embedding",
                        "contributors": [
                            "Zipeng Liu"
                        ],
                        "authors": [
                            "Zipeng Liu",
                            "Yang Wang",
                            "J\u00fcrgen Bernard",
                            "Tamara Munzner"
                        ],
                        "abstract": "Graph neural networks (GNNs) are a class of powerful machine learning tools that model node relations for making predictions of nodes or links. GNN developers rely on quantitative metrics of the predictions to evaluate a GNN, but similar to many other neural networks, it is difficult for them to understand if the GNN truly learns characteristics of a graph as expected. We propose an approach to corresponding an input graph to its node embedding (aka latent space), a common component of GNNs that is later used for prediction. We abstract the data and tasks, and develop an interactive multi-view interface called CorGIE to instantiate the abstraction. As the key function in CorGIE, we propose the K-hop graph layout to show topological neighbors in hops and their clustering structure. To evaluate the functionality and usability of CorGIE, we present how to use CorGIE in two usage scenarios, and conduct a case study with five GNN experts.",
                        "uid": "v-tvcg-9705082",
                        "file_name": "",
                        "time_stamp": "2022-10-20T16:09:00Z",
                        "time_start": "2022-10-20T16:09:00Z",
                        "time_end": "2022-10-20T16:19:00Z",
                        "paper_type": "full",
                        "keywords": "Visualization for machine learning, graph neural network, graph layout",
                        "has_image": "1",
                        "has_video": "600",
                        "paper_award": "",
                        "image_caption": "Full screenshot of CorGIE interface on the Movie dataset, with two focal groups of user nodes.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-tvcg-9705082-qa",
                        "session_id": "full24",
                        "type": "Virtual Q+A",
                        "title": "Visualizing Graph Neural Networks with CorGIE: Corresponding a Graph to Its Embedding (Q+A)",
                        "contributors": [
                            "Zipeng Liu"
                        ],
                        "authors": [],
                        "abstract": "Graph neural networks (GNNs) are a class of powerful machine learning tools that model node relations for making predictions of nodes or links. GNN developers rely on quantitative metrics of the predictions to evaluate a GNN, but similar to many other neural networks, it is difficult for them to understand if the GNN truly learns characteristics of a graph as expected. We propose an approach to corresponding an input graph to its node embedding (aka latent space), a common component of GNNs that is later used for prediction. We abstract the data and tasks, and develop an interactive multi-view interface called CorGIE to instantiate the abstraction. As the key function in CorGIE, we propose the K-hop graph layout to show topological neighbors in hops and their clustering structure. To evaluate the functionality and usability of CorGIE, we present how to use CorGIE in two usage scenarios, and conduct a case study with five GNN experts.",
                        "uid": "v-tvcg-9705082",
                        "file_name": "",
                        "time_stamp": "2022-10-20T16:19:00Z",
                        "time_start": "2022-10-20T16:19:00Z",
                        "time_end": "2022-10-20T16:21:00Z",
                        "paper_type": "full",
                        "keywords": "Visualization for machine learning, graph neural network, graph layout",
                        "has_image": "1",
                        "has_video": "600",
                        "paper_award": "",
                        "image_caption": "Full screenshot of CorGIE interface on the Movie dataset, with two focal groups of user nodes.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-full-1541-pres",
                        "session_id": "full24",
                        "type": "In Person Presentation",
                        "title": "Comparative Evaluation of Bipartite, Node-Link, and Matrix-Based Network Representations",
                        "contributors": [
                            "Moataz Abdelaal"
                        ],
                        "authors": [
                            "Moataz Abdelaal",
                            "Nathan D Schiele",
                            "Katrin Angerbauer",
                            "Kuno Kurzhals",
                            "Michael Sedlmair",
                            "Daniel Weiskopf"
                        ],
                        "abstract": "",
                        "uid": "v-full-1541",
                        "file_name": "",
                        "time_stamp": "2022-10-20T16:21:00Z",
                        "time_start": "2022-10-20T16:21:00Z",
                        "time_end": "2022-10-20T16:31:00Z",
                        "paper_type": "full",
                        "keywords": "",
                        "has_image": "1",
                        "has_video": "1347",
                        "paper_award": "",
                        "image_caption": "Three visualization techniques for the representation of large networks. Node-link diagrams and adjacency matrices are\ncommon in visualization. Bipartite layouts have been proposed as an alternative for solving different tasks. We compare all three\ntechniques with respect to different network properties and tasks.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-full-1541-qa",
                        "session_id": "full24",
                        "type": "In Person Q+A",
                        "title": "Comparative Evaluation of Bipartite, Node-Link, and Matrix-Based Network Representations (Q+A)",
                        "contributors": [
                            "Moataz Abdelaal"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "v-full-1541",
                        "file_name": "",
                        "time_stamp": "2022-10-20T16:31:00Z",
                        "time_start": "2022-10-20T16:31:00Z",
                        "time_end": "2022-10-20T16:33:00Z",
                        "paper_type": "full",
                        "keywords": "",
                        "has_image": "1",
                        "has_video": "1347",
                        "paper_award": "",
                        "image_caption": "Three visualization techniques for the representation of large networks. Node-link diagrams and adjacency matrices are\ncommon in visualization. Bipartite layouts have been proposed as an alternative for solving different tasks. We compare all three\ntechniques with respect to different network properties and tasks.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-full-1154-pres",
                        "session_id": "full24",
                        "type": "In Person Presentation",
                        "title": "Understanding Barriers to Network Exploration with Visualization: A Report from the Trenches",
                        "contributors": [
                            "Mashael AlKadi"
                        ],
                        "authors": [
                            "Mashael AlKadi",
                            "Vanessa Serrano",
                            "James Scott-Brown",
                            "Uta Hinrichs",
                            "Catherine Plaisant",
                            "Jean-Daniel Fekete",
                            "Benjamin Bach"
                        ],
                        "abstract": "",
                        "uid": "v-full-1154",
                        "file_name": "",
                        "time_stamp": "2022-10-20T16:33:00Z",
                        "time_start": "2022-10-20T16:33:00Z",
                        "time_end": "2022-10-20T16:43:00Z",
                        "paper_type": "full",
                        "keywords": "",
                        "has_image": "1",
                        "has_video": "582",
                        "paper_award": "",
                        "image_caption": "Visual Network Exploration Model",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-full-1154-qa",
                        "session_id": "full24",
                        "type": "In Person Q+A",
                        "title": "Understanding Barriers to Network Exploration with Visualization: A Report from the Trenches (Q+A)",
                        "contributors": [
                            "Mashael AlKadi"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "v-full-1154",
                        "file_name": "",
                        "time_stamp": "2022-10-20T16:43:00Z",
                        "time_start": "2022-10-20T16:43:00Z",
                        "time_end": "2022-10-20T16:45:00Z",
                        "paper_type": "full",
                        "keywords": "",
                        "has_image": "1",
                        "has_video": "582",
                        "paper_award": "",
                        "image_caption": "Visual Network Exploration Model",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-tvcg-9720180-pres",
                        "session_id": "full24",
                        "type": "Virtual Presentation (pre-recorded)",
                        "title": "VividGraph: Learning to Extract and Redesign Network Graphs from Visualization Images",
                        "contributors": [
                            "Sicheng Song"
                        ],
                        "authors": [
                            "Sicheng Song",
                            "Chenhui Li",
                            "Yujing Sun",
                            "Changbo Wang"
                        ],
                        "abstract": "Network graphs are common visualization charts. They often appear in the form of bitmaps in papers, web pages, magazine prints, and designer sketches. People often want to modify network graphs because of their poor design, but it is difficult to obtain their underlying data. In this paper, we present VividGraph, a pipeline for automatically extracting and redesigning network graphs from static images. We propose using convolutional neural networks to solve the problem of network graph data extraction. Our method is robust to hand-drawn graphs, blurred graph images, and large graph images. We also present a network graph classification module to make it effective for directed graphs. We propose two evaluation methods to demonstrate the effectiveness of our approach. It can be used to quickly transform designer sketches, extract underlying data from existing network graphs, and interactively redesign poorly designed network graphs.",
                        "uid": "v-tvcg-9720180",
                        "file_name": "",
                        "time_stamp": "2022-10-20T16:45:00Z",
                        "time_start": "2022-10-20T16:45:00Z",
                        "time_end": "2022-10-20T16:55:00Z",
                        "paper_type": "full",
                        "keywords": "Information visualization , Network graph , Data extraction , Chart recognition , Semantic segmentation , Redesign",
                        "has_image": "1",
                        "has_video": "547",
                        "paper_award": "",
                        "image_caption": "VividGraph can be used in many practical applications. The input is a bitmap. Through our semantic segmentation and connection algorithm, we can obtain its underlying data. Using the extracted data, we can reconstruct the vector of the graph and redesign the chart, such as recoloring, re-layout, and data modification.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-tvcg-9720180-qa",
                        "session_id": "full24",
                        "type": "Virtual Q+A",
                        "title": "VividGraph: Learning to Extract and Redesign Network Graphs from Visualization Images (Q+A)",
                        "contributors": [
                            "Sicheng Song"
                        ],
                        "authors": [],
                        "abstract": "Network graphs are common visualization charts. They often appear in the form of bitmaps in papers, web pages, magazine prints, and designer sketches. People often want to modify network graphs because of their poor design, but it is difficult to obtain their underlying data. In this paper, we present VividGraph, a pipeline for automatically extracting and redesigning network graphs from static images. We propose using convolutional neural networks to solve the problem of network graph data extraction. Our method is robust to hand-drawn graphs, blurred graph images, and large graph images. We also present a network graph classification module to make it effective for directed graphs. We propose two evaluation methods to demonstrate the effectiveness of our approach. It can be used to quickly transform designer sketches, extract underlying data from existing network graphs, and interactively redesign poorly designed network graphs.",
                        "uid": "v-tvcg-9720180",
                        "file_name": "",
                        "time_stamp": "2022-10-20T16:55:00Z",
                        "time_start": "2022-10-20T16:55:00Z",
                        "time_end": "2022-10-20T16:57:00Z",
                        "paper_type": "full",
                        "keywords": "Information visualization , Network graph , Data extraction , Chart recognition , Semantic segmentation , Redesign",
                        "has_image": "1",
                        "has_video": "547",
                        "paper_award": "",
                        "image_caption": "VividGraph can be used in many practical applications. The input is a bitmap. Through our semantic segmentation and connection algorithm, we can obtain its underlying data. Using the extracted data, we can reconstruct the vector of the graph and redesign the chart, such as recoloring, re-layout, and data modification.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    }
                ]
            },
            {
                "title": "Visualization Design",
                "session_id": "full25",
                "event_prefix": "v-full",
                "track": "ok5",
                "livestream_id": "ok5-wed",
                "session_image": "full25.png",
                "chair": [
                    "Miriah Meyer"
                ],
                "organizers": [],
                "time_start": "2022-10-19T15:45:00Z",
                "time_end": "2022-10-19T17:00:00Z",
                "discord_category": "",
                "discord_channel": "",
                "discord_channel_id": "",
                "discord_link": "",
                "slido_link": "",
                "youtube_url": "",
                "zoom_meeting": "",
                "zoom_password": "",
                "zoom_link": "",
                "ff_link": "",
                "ff_playlist": "",
                "time_slots": [
                    {
                        "slot_id": "full25-opening",
                        "session_id": "full25",
                        "type": "In Person Other",
                        "title": "Session Opening",
                        "contributors": [
                            "Miriah Meyer"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "",
                        "file_name": "",
                        "time_stamp": "2022-10-19T15:45:00Z",
                        "time_start": "2022-10-19T15:45:00Z",
                        "time_end": "2022-10-19T15:45:00Z",
                        "paper_type": "",
                        "keywords": "",
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-tvcg-9508898-pres",
                        "session_id": "full25",
                        "type": "Virtual Presentation (pre-recorded)",
                        "title": "Towards Systematic Design Considerations for Visualizing Cross-View Data Relationships",
                        "contributors": [
                            "Maoyuan Sun"
                        ],
                        "authors": [
                            "Maoyuan Sun",
                            "Akhil Namburi",
                            "David Koop",
                            "Jian Zhao",
                            "Tianyi Li",
                            "Haeyong Chung"
                        ],
                        "abstract": "Due to the scale of data and the complexity of analysis tasks, insight discovery often requires coordinating multiple visualizations (views), with each view displaying different parts of data or the same data from different perspectives. For example, to analyze car sales records, a marketing analyst uses a line chart to visualize the trend of car sales, a scatterplot to inspect the price and horsepower of different cars, and a matrix to compare the transaction amounts in types of deals. To explore related information across multiple views, current visual analysis tools heavily rely on brushing and linking techniques, which may require a significant amount of user effort (e.g., many trial-and-error attempts). There may be other efficient and effective ways of displaying cross-view data relationships to support data analysis with multiple views, but currently there are no guidelines to address this design challenge. In this paper, we present systematic design considerations for visualizing cross-view data relationships, which leverages descriptive aspects of relationships and usable visual context of multi-view visualizations. We discuss pros and cons of different designs for showing cross-view data relationships, and provide a set of recommendations for helping practitioners make design decisions.",
                        "uid": "v-tvcg-9508898",
                        "file_name": "",
                        "time_stamp": "2022-10-19T15:45:00Z",
                        "time_start": "2022-10-19T15:45:00Z",
                        "time_end": "2022-10-19T15:55:00Z",
                        "paper_type": "full",
                        "keywords": "Visualization, Data visualization, Task analysis, Organizations, Systematics, Periodic structures, Computer science",
                        "has_image": "1",
                        "has_video": "540",
                        "paper_award": "",
                        "image_caption": "A descriptive framework of data relationships with four aspects.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-tvcg-9508898-qa",
                        "session_id": "full25",
                        "type": "Virtual Q+A",
                        "title": "Towards Systematic Design Considerations for Visualizing Cross-View Data Relationships (Q+A)",
                        "contributors": [
                            "Maoyuan Sun"
                        ],
                        "authors": [],
                        "abstract": "Due to the scale of data and the complexity of analysis tasks, insight discovery often requires coordinating multiple visualizations (views), with each view displaying different parts of data or the same data from different perspectives. For example, to analyze car sales records, a marketing analyst uses a line chart to visualize the trend of car sales, a scatterplot to inspect the price and horsepower of different cars, and a matrix to compare the transaction amounts in types of deals. To explore related information across multiple views, current visual analysis tools heavily rely on brushing and linking techniques, which may require a significant amount of user effort (e.g., many trial-and-error attempts). There may be other efficient and effective ways of displaying cross-view data relationships to support data analysis with multiple views, but currently there are no guidelines to address this design challenge. In this paper, we present systematic design considerations for visualizing cross-view data relationships, which leverages descriptive aspects of relationships and usable visual context of multi-view visualizations. We discuss pros and cons of different designs for showing cross-view data relationships, and provide a set of recommendations for helping practitioners make design decisions.",
                        "uid": "v-tvcg-9508898",
                        "file_name": "",
                        "time_stamp": "2022-10-19T15:55:00Z",
                        "time_start": "2022-10-19T15:55:00Z",
                        "time_end": "2022-10-19T15:57:00Z",
                        "paper_type": "full",
                        "keywords": "Visualization, Data visualization, Task analysis, Organizations, Systematics, Periodic structures, Computer science",
                        "has_image": "1",
                        "has_video": "540",
                        "paper_award": "",
                        "image_caption": "A descriptive framework of data relationships with four aspects.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-tvcg-9465643-pres",
                        "session_id": "full25",
                        "type": "Virtual Presentation (pre-recorded)",
                        "title": "Designing with Pictographs: Envision Topics without Sacrificing Understanding",
                        "contributors": [
                            "Alyx Burns"
                        ],
                        "authors": [
                            "Alyxander Burns",
                            "Cindy Xiong",
                            "Steven Franconeri",
                            "Alberto Cairo",
                            "Narges Mahyar"
                        ],
                        "abstract": "Past studies have shown that when a visualization uses pictographs to encode data, they have a positive effect on memory, engagement, and assessment of risk. However, little is known about how pictographs affect one's ability to understand a visualization, beyond memory for values and trends. We conducted two crowdsourced experiments to compare the effectiveness of using pictographs when showing part-to-whole relationships. In Experiment 1, we compared pictograph arrays to more traditional bar and pie charts. We tested participants' ability to generate high-level insights following Bloom's taxonomy of educational objectives via 6 free-response questions. We found that accuracy for extracting information and generating insights did not differ overall between the two versions. To explore the motivating differences between the designs, we conducted a second experiment where participants compared charts containing pictograph arrays to more traditional charts on 5 metrics and explained their reasoning. We found that some participants preferred the way that pictographs allowed them to envision the topic more easily, while others preferred traditional bar and pie charts because they seem less cluttered and faster to read. These results suggest that, at least in simple visualizations depicting part-to-whole relationships, the choice of using pictographs has little influence on sensemaking and insight extraction. When deciding whether to use pictograph arrays, designers should consider visual appeal, perceived comprehension time, ease of envisioning the topic, and clutteredness.",
                        "uid": "v-tvcg-9465643",
                        "file_name": "",
                        "time_stamp": "2022-10-19T15:57:00Z",
                        "time_start": "2022-10-19T15:57:00Z",
                        "time_end": "2022-10-19T16:07:00Z",
                        "paper_type": "full",
                        "keywords": "Infographics, pictographs, design, graph comprehension, understanding, casual sensemaking",
                        "has_image": "1",
                        "has_video": "496",
                        "paper_award": "",
                        "image_caption": "What impact do pictographs have on the ways that the general public makes sense of visualizations? In this paper, we conducted two crowdsourced experiments with 6 pairs of real-world visualizations to compare the effectiveness of using pictographs instead of more traditional abstract shapes when showing part-to-whole relationships.  Our results suggest that pictographs have little influence on sensemaking and insight extraction, but influence perceived visual appeal, comprehension time, ease of envisioning the topic, and clutteredness.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-tvcg-9465643-qa",
                        "session_id": "full25",
                        "type": "Virtual Q+A",
                        "title": "Designing with Pictographs: Envision Topics without Sacrificing Understanding (Q+A)",
                        "contributors": [
                            "Alyx Burns"
                        ],
                        "authors": [],
                        "abstract": "Past studies have shown that when a visualization uses pictographs to encode data, they have a positive effect on memory, engagement, and assessment of risk. However, little is known about how pictographs affect one's ability to understand a visualization, beyond memory for values and trends. We conducted two crowdsourced experiments to compare the effectiveness of using pictographs when showing part-to-whole relationships. In Experiment 1, we compared pictograph arrays to more traditional bar and pie charts. We tested participants' ability to generate high-level insights following Bloom's taxonomy of educational objectives via 6 free-response questions. We found that accuracy for extracting information and generating insights did not differ overall between the two versions. To explore the motivating differences between the designs, we conducted a second experiment where participants compared charts containing pictograph arrays to more traditional charts on 5 metrics and explained their reasoning. We found that some participants preferred the way that pictographs allowed them to envision the topic more easily, while others preferred traditional bar and pie charts because they seem less cluttered and faster to read. These results suggest that, at least in simple visualizations depicting part-to-whole relationships, the choice of using pictographs has little influence on sensemaking and insight extraction. When deciding whether to use pictograph arrays, designers should consider visual appeal, perceived comprehension time, ease of envisioning the topic, and clutteredness.",
                        "uid": "v-tvcg-9465643",
                        "file_name": "",
                        "time_stamp": "2022-10-19T16:07:00Z",
                        "time_start": "2022-10-19T16:07:00Z",
                        "time_end": "2022-10-19T16:09:00Z",
                        "paper_type": "full",
                        "keywords": "Infographics, pictographs, design, graph comprehension, understanding, casual sensemaking",
                        "has_image": "1",
                        "has_video": "496",
                        "paper_award": "",
                        "image_caption": "What impact do pictographs have on the ways that the general public makes sense of visualizations? In this paper, we conducted two crowdsourced experiments with 6 pairs of real-world visualizations to compare the effectiveness of using pictographs instead of more traditional abstract shapes when showing part-to-whole relationships.  Our results suggest that pictographs have little influence on sensemaking and insight extraction, but influence perceived visual appeal, comprehension time, ease of envisioning the topic, and clutteredness.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-tvcg-9737134-pres",
                        "session_id": "full25",
                        "type": "In Person Presentation",
                        "title": "Visualizing Higher-Order 3D Tensors by Multipole Lines",
                        "contributors": [
                            "Chiara Hergl"
                        ],
                        "authors": [
                            "Chiara Hergl",
                            "Thomas Nagel",
                            "Gerik Scheuermann"
                        ],
                        "abstract": "Physics, medicine, earth sciences, mechanical engineering, geo-engineering, bio-engineering and many more application areas use tensorial data. For example, tensors are used in formulating the balance equations of charge, mass, momentum, or energy as well as the constitutive relations that complement them. Some of these tensors (i.e. stiffness tensor, strain gradient, photo-elastic tensor) are of order higher than two. Currently, there are nearly no visualization techniques for such data beyond glyphs. An important reason for this is the limit of currently used tensor decomposition techniques. In this article, we propose to use the deviatoric decomposition to draw lines describing tensors of arbitrary order in three dimensions. The deviatoric decomposition splits a three-dimensional tensor of any order with any type of index symmetry into totally symmetric, traceless tensors. These tensors, called deviators, can be described by a unique set of directions (called multipoles by J.~C. Maxwell) and scalars. These multipoles allow the definition of multipole lines which can be computed in a similar fashion to tensor lines and allow a line-based visualization of three-dimensional tensors of any order. We give examples for the visualization of symmetric, second-order tensor fields as well as fourth-order tensor fields. To allow an interpretation of the multipole lines, we analyze the connection between the multipoles and the eigenvectors/eigenvalues in the second-order case. For the fourth-order stiffness tensor, we prove relations between multipoles and important physical quantities such as shear moduli as well as the eigenvectors of the second-order right Cauchy-Green tensor.",
                        "uid": "v-tvcg-9737134",
                        "file_name": "",
                        "time_stamp": "2022-10-19T16:09:00Z",
                        "time_start": "2022-10-19T16:09:00Z",
                        "time_end": "2022-10-19T16:19:00Z",
                        "paper_type": "full",
                        "keywords": "tensor algebra, higher-order tensor, line-based, deviatoric decomposition, anisotropy",
                        "has_image": "1",
                        "has_video": "714",
                        "paper_award": "",
                        "image_caption": "Stiffness tensor visualization of an indentation test using a soft biological material. The fourth-order three-dimensional tensor is decomposed into so called deviators. Each deviator is represented by multipole lines. The figure depicts the multipole lines corresponding to the second-order deviator describing the asymmetric part.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-tvcg-9737134-qa",
                        "session_id": "full25",
                        "type": "In Person Q+A",
                        "title": "Visualizing Higher-Order 3D Tensors by Multipole Lines (Q+A)",
                        "contributors": [
                            "Chiara Hergl"
                        ],
                        "authors": [],
                        "abstract": "Physics, medicine, earth sciences, mechanical engineering, geo-engineering, bio-engineering and many more application areas use tensorial data. For example, tensors are used in formulating the balance equations of charge, mass, momentum, or energy as well as the constitutive relations that complement them. Some of these tensors (i.e. stiffness tensor, strain gradient, photo-elastic tensor) are of order higher than two. Currently, there are nearly no visualization techniques for such data beyond glyphs. An important reason for this is the limit of currently used tensor decomposition techniques. In this article, we propose to use the deviatoric decomposition to draw lines describing tensors of arbitrary order in three dimensions. The deviatoric decomposition splits a three-dimensional tensor of any order with any type of index symmetry into totally symmetric, traceless tensors. These tensors, called deviators, can be described by a unique set of directions (called multipoles by J.~C. Maxwell) and scalars. These multipoles allow the definition of multipole lines which can be computed in a similar fashion to tensor lines and allow a line-based visualization of three-dimensional tensors of any order. We give examples for the visualization of symmetric, second-order tensor fields as well as fourth-order tensor fields. To allow an interpretation of the multipole lines, we analyze the connection between the multipoles and the eigenvectors/eigenvalues in the second-order case. For the fourth-order stiffness tensor, we prove relations between multipoles and important physical quantities such as shear moduli as well as the eigenvectors of the second-order right Cauchy-Green tensor.",
                        "uid": "v-tvcg-9737134",
                        "file_name": "",
                        "time_stamp": "2022-10-19T16:19:00Z",
                        "time_start": "2022-10-19T16:19:00Z",
                        "time_end": "2022-10-19T16:21:00Z",
                        "paper_type": "full",
                        "keywords": "tensor algebra, higher-order tensor, line-based, deviatoric decomposition, anisotropy",
                        "has_image": "1",
                        "has_video": "714",
                        "paper_award": "",
                        "image_caption": "Stiffness tensor visualization of an indentation test using a soft biological material. The fourth-order three-dimensional tensor is decomposed into so called deviators. Each deviator is represented by multipole lines. The figure depicts the multipole lines corresponding to the second-order deviator describing the asymmetric part.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-full-1335-pres",
                        "session_id": "full25",
                        "type": "Virtual Presentation (pre-recorded)",
                        "title": "MetaGlyph: Automatic Generation of Metaphoric Glyph-based Visualization",
                        "contributors": [
                            "Lu Ying"
                        ],
                        "authors": [
                            "Lu Ying",
                            "Yuchen Yang",
                            "Xinhuan Shu",
                            "Dazhen Deng",
                            "Tan Tang",
                            "Lingyun Yu",
                            "Yingcai Wu"
                        ],
                        "abstract": "",
                        "uid": "v-full-1335",
                        "file_name": "",
                        "time_stamp": "2022-10-19T16:21:00Z",
                        "time_start": "2022-10-19T16:21:00Z",
                        "time_end": "2022-10-19T16:31:00Z",
                        "paper_type": "full",
                        "keywords": "",
                        "has_image": "1",
                        "has_video": "546",
                        "paper_award": "",
                        "image_caption": "Example MGVs generated by MetaGlyph: (a) several attributes of different pokemons; (b) forest area changes in different countries from 1995 to 2020; (c) display of various CDs; (e) multiple hotels\u2019 information arranged by price (x) and rate (y); (f) chocolates in different ratings placed on a map; (g) disparate mushrooms. The legends in (a1)(b1)(f1) are also generated by MetaGlyph. The center boxes with yellow backgrounds illustrate the data mappings for each glyph. Encoding channels are represented in icons and (d) shows the annotation. Dashed arrows are used to associate visual elements with the resulting visualizations.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-full-1335-qa",
                        "session_id": "full25",
                        "type": "Virtual Q+A",
                        "title": "MetaGlyph: Automatic Generation of Metaphoric Glyph-based Visualization (Q+A)",
                        "contributors": [
                            "Lu Ying"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "v-full-1335",
                        "file_name": "",
                        "time_stamp": "2022-10-19T16:31:00Z",
                        "time_start": "2022-10-19T16:31:00Z",
                        "time_end": "2022-10-19T16:33:00Z",
                        "paper_type": "full",
                        "keywords": "",
                        "has_image": "1",
                        "has_video": "546",
                        "paper_award": "",
                        "image_caption": "Example MGVs generated by MetaGlyph: (a) several attributes of different pokemons; (b) forest area changes in different countries from 1995 to 2020; (c) display of various CDs; (e) multiple hotels\u2019 information arranged by price (x) and rate (y); (f) chocolates in different ratings placed on a map; (g) disparate mushrooms. The legends in (a1)(b1)(f1) are also generated by MetaGlyph. The center boxes with yellow backgrounds illustrate the data mappings for each glyph. Encoding channels are represented in icons and (d) shows the annotation. Dashed arrows are used to associate visual elements with the resulting visualizations.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-full-1554-pres",
                        "session_id": "full25",
                        "type": "In Person Presentation",
                        "title": "Dashboard Design Patterns",
                        "contributors": [
                            "Benjamin Bach"
                        ],
                        "authors": [
                            "Benjamin Bach",
                            "Euan Freeman",
                            "Alfie Abdul-Rahman",
                            "Cagatay Turkay",
                            "Saiful Khan",
                            "Yulei Fan",
                            "Min Chen"
                        ],
                        "abstract": "",
                        "uid": "v-full-1554",
                        "file_name": "",
                        "time_stamp": "2022-10-19T16:33:00Z",
                        "time_start": "2022-10-19T16:33:00Z",
                        "time_end": "2022-10-19T16:43:00Z",
                        "paper_type": "full",
                        "keywords": "",
                        "has_image": "1",
                        "has_video": "853",
                        "paper_award": "",
                        "image_caption": "Dashboard design patterns grouped into eight categories. More info on our website https://dashboarddesignpatterns.github.io.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-full-1554-qa",
                        "session_id": "full25",
                        "type": "In Person Q+A",
                        "title": "Dashboard Design Patterns (Q+A)",
                        "contributors": [
                            "Benjamin Bach"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "v-full-1554",
                        "file_name": "",
                        "time_stamp": "2022-10-19T16:43:00Z",
                        "time_start": "2022-10-19T16:43:00Z",
                        "time_end": "2022-10-19T16:45:00Z",
                        "paper_type": "full",
                        "keywords": "",
                        "has_image": "1",
                        "has_video": "853",
                        "paper_award": "",
                        "image_caption": "Dashboard design patterns grouped into eight categories. More info on our website https://dashboarddesignpatterns.github.io.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-full-1206-pres",
                        "session_id": "full25",
                        "type": "Virtual Presentation (pre-recorded)",
                        "title": "A Representative Design Space of Multiclass Contour Visualization",
                        "contributors": [
                            "Sihang Li"
                        ],
                        "authors": [
                            "Sihang Li",
                            "Jiacheng Yu",
                            "Mingxuan Li",
                            "Le Liu",
                            "Xiaolong (Luke) Zhang",
                            "Xiaoru Yuan"
                        ],
                        "abstract": "",
                        "uid": "v-full-1206",
                        "file_name": "",
                        "time_stamp": "2022-10-19T16:45:00Z",
                        "time_start": "2022-10-19T16:45:00Z",
                        "time_end": "2022-10-19T16:55:00Z",
                        "paper_type": "full",
                        "keywords": "",
                        "has_image": "1",
                        "has_video": "562",
                        "paper_award": "",
                        "image_caption": "Variations of three design parameters under our proposed multiclass contour visualization framework. The contours are drawn using our declarative DSL. Through a user study, it can be learned that different choices of these three parameters affect the user's ability to complete different tasks.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-full-1206-qa",
                        "session_id": "full25",
                        "type": "Virtual Q+A",
                        "title": "A Representative Design Space of Multiclass Contour Visualization (Q+A)",
                        "contributors": [
                            "Sihang Li"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "v-full-1206",
                        "file_name": "",
                        "time_stamp": "2022-10-19T16:55:00Z",
                        "time_start": "2022-10-19T16:55:00Z",
                        "time_end": "2022-10-19T16:57:00Z",
                        "paper_type": "full",
                        "keywords": "",
                        "has_image": "1",
                        "has_video": "562",
                        "paper_award": "",
                        "image_caption": "Variations of three design parameters under our proposed multiclass contour visualization framework. The contours are drawn using our declarative DSL. Through a user study, it can be learned that different choices of these three parameters affect the user's ability to complete different tasks.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    }
                ]
            },
            {
                "title": "Immersive Analytics and Situated Visualization",
                "session_id": "full26",
                "event_prefix": "v-full",
                "track": "ok4",
                "livestream_id": "ok4-wed",
                "session_image": "full26.png",
                "chair": [
                    "Danielle Szafir"
                ],
                "organizers": [],
                "time_start": "2022-10-19T19:00:00Z",
                "time_end": "2022-10-19T20:15:00Z",
                "discord_category": "",
                "discord_channel": "",
                "discord_channel_id": "",
                "discord_link": "",
                "slido_link": "",
                "youtube_url": "",
                "zoom_meeting": "",
                "zoom_password": "",
                "zoom_link": "",
                "ff_link": "",
                "ff_playlist": "",
                "time_slots": [
                    {
                        "slot_id": "full26-opening",
                        "session_id": "full26",
                        "type": "In Person Other",
                        "title": "Session Opening",
                        "contributors": [
                            "Danielle Szafir"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "",
                        "file_name": "",
                        "time_stamp": "2022-10-19T19:00:00Z",
                        "time_start": "2022-10-19T19:00:00Z",
                        "time_end": "2022-10-19T19:00:00Z",
                        "paper_type": "",
                        "keywords": "",
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-full-1101-pres",
                        "session_id": "full26",
                        "type": "In Person Presentation",
                        "title": "Exploring Interactions with Printed Data Visualizations in Augmented Reality",
                        "contributors": [
                            "Wai Tong"
                        ],
                        "authors": [
                            "Wai Tong",
                            "Zhutian Chen",
                            "Meng Xia",
                            "Leo Yu-Ho Lo",
                            "Linping Yuan",
                            "Benjamin Bach",
                            "Huamin Qu"
                        ],
                        "abstract": "",
                        "uid": "v-full-1101",
                        "file_name": "",
                        "time_stamp": "2022-10-19T19:00:00Z",
                        "time_start": "2022-10-19T19:00:00Z",
                        "time_end": "2022-10-19T19:10:00Z",
                        "paper_type": "full",
                        "keywords": "",
                        "has_image": "1",
                        "has_video": "547",
                        "paper_award": "",
                        "image_caption": "We investigate the possibility of interacting with printed visualizations in Augmented Reality. Suppose a student receives (a) a leaflet about university ranking and wants to analyze three universities' ranking histories of interest. Examples of interactions with (b) digital content overlaid: (c) tilt the paper to rescale the y-axis, (d) move (translate) to zoom (e) unfold to show two charts side by side and link them, (f) point to select elements and highlight them in the other chart.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-full-1101-qa",
                        "session_id": "full26",
                        "type": "In Person Q+A",
                        "title": "Exploring Interactions with Printed Data Visualizations in Augmented Reality (Q+A)",
                        "contributors": [
                            "Wai Tong"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "v-full-1101",
                        "file_name": "",
                        "time_stamp": "2022-10-19T19:10:00Z",
                        "time_start": "2022-10-19T19:10:00Z",
                        "time_end": "2022-10-19T19:12:00Z",
                        "paper_type": "full",
                        "keywords": "",
                        "has_image": "1",
                        "has_video": "547",
                        "paper_award": "",
                        "image_caption": "We investigate the possibility of interacting with printed visualizations in Augmented Reality. Suppose a student receives (a) a leaflet about university ranking and wants to analyze three universities' ranking histories of interest. Examples of interactions with (b) digital content overlaid: (c) tilt the paper to rescale the y-axis, (d) move (translate) to zoom (e) unfold to show two charts side by side and link them, (f) point to select elements and highlight them in the other chart.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-tvcg-9729627-pres",
                        "session_id": "full26",
                        "type": "In Person Presentation",
                        "title": "RagRug: A Toolkit for Situated Analytics",
                        "contributors": [
                            "Philipp Fleck"
                        ],
                        "authors": [
                            "Philipp Fleck",
                            "Aimee Sousa Calepso",
                            "Sebastian Hubenschmid,Michael Sedlmair",
                            "Dieter Schmalstieg"
                        ],
                        "abstract": "We present RagRug, an open-source toolkit for situated analytics. The  , abilities of RagRug go beyond previous immersiveanalytics toolkits by  , focusing on specific requirements emerging when using augmented  , reality (AR) rather than virtual reality. RagRugcombines state of the  , art visual encoding capabilities with a comprehensive physical-virtual  , model, which lets application developerssystematically describe the  , physical objects in the real world and their role in AR. We connect AR  , visualizations with data streams fromthe Internet of Things using  , distributed dataflow. To this end, we use reactive programming  , patterns so that visualizations becomecontext-aware, i.e., they adapt  , to events coming in from the environment. The resulting authoring  , system is low-code; it emphasisesdescribing the physical and the  , virtual world and the dataflow between the elements contained therein.  , We describe the technicaldesign and implementation of RagRug, and  , report on five example applications illustrating the toolkit\u2019s  , abilities.",
                        "uid": "v-tvcg-9729627",
                        "file_name": "",
                        "time_stamp": "2022-10-19T19:12:00Z",
                        "time_start": "2022-10-19T19:12:00Z",
                        "time_end": "2022-10-19T19:22:00Z",
                        "paper_type": "full",
                        "keywords": "Augmented Reality, Visualization, Visual Analytics, Immersive Analytics, Situated Analytics",
                        "has_image": "1",
                        "has_video": "467",
                        "paper_award": "",
                        "image_caption": "We present RagRug, an open-source toolkit for situated analytics. The abilities of RagRug go beyond previous immersive analytics toolkits by focusing on specific requirements emerging when using augmented reality (AR) rather than virtual reality. RagRug combines state of the art visual encoding capabilities with a comprehensive physical-virtual model, which lets application developers systematically describe the physical objects in the real world and their role in AR. We connect AR visualization with data streams from the Internet of Things using distributed dataflow. Github: https://github.com/philfleck/ragrug",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-tvcg-9729627-qa",
                        "session_id": "full26",
                        "type": "In Person Q+A",
                        "title": "RagRug: A Toolkit for Situated Analytics (Q+A)",
                        "contributors": [
                            "Philipp Fleck"
                        ],
                        "authors": [],
                        "abstract": "We present RagRug, an open-source toolkit for situated analytics. The  , abilities of RagRug go beyond previous immersiveanalytics toolkits by  , focusing on specific requirements emerging when using augmented  , reality (AR) rather than virtual reality. RagRugcombines state of the  , art visual encoding capabilities with a comprehensive physical-virtual  , model, which lets application developerssystematically describe the  , physical objects in the real world and their role in AR. We connect AR  , visualizations with data streams fromthe Internet of Things using  , distributed dataflow. To this end, we use reactive programming  , patterns so that visualizations becomecontext-aware, i.e., they adapt  , to events coming in from the environment. The resulting authoring  , system is low-code; it emphasisesdescribing the physical and the  , virtual world and the dataflow between the elements contained therein.  , We describe the technicaldesign and implementation of RagRug, and  , report on five example applications illustrating the toolkit\u2019s  , abilities.",
                        "uid": "v-tvcg-9729627",
                        "file_name": "",
                        "time_stamp": "2022-10-19T19:22:00Z",
                        "time_start": "2022-10-19T19:22:00Z",
                        "time_end": "2022-10-19T19:24:00Z",
                        "paper_type": "full",
                        "keywords": "Augmented Reality, Visualization, Visual Analytics, Immersive Analytics, Situated Analytics",
                        "has_image": "1",
                        "has_video": "467",
                        "paper_award": "",
                        "image_caption": "We present RagRug, an open-source toolkit for situated analytics. The abilities of RagRug go beyond previous immersive analytics toolkits by focusing on specific requirements emerging when using augmented reality (AR) rather than virtual reality. RagRug combines state of the art visual encoding capabilities with a comprehensive physical-virtual model, which lets application developers systematically describe the physical objects in the real world and their role in AR. We connect AR visualization with data streams from the Internet of Things using distributed dataflow. Github: https://github.com/philfleck/ragrug",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-full-1272-pres",
                        "session_id": "full26",
                        "type": "Virtual Presentation (pre-recorded)",
                        "title": "PuzzleFixer: A Visual Reassembly System for Immersive Fragments Restoration",
                        "contributors": [
                            "Shuainan Ye"
                        ],
                        "authors": [
                            "Shuainan Ye",
                            "Zhutian Chen",
                            "Xiangtong Chu",
                            "Kang Li",
                            "Juntong Luo",
                            "Yi Li",
                            "Guohua Geng",
                            "Yingcai Wu"
                        ],
                        "abstract": "",
                        "uid": "v-full-1272",
                        "file_name": "",
                        "time_stamp": "2022-10-19T19:24:00Z",
                        "time_start": "2022-10-19T19:24:00Z",
                        "time_end": "2022-10-19T19:34:00Z",
                        "paper_type": "full",
                        "keywords": "",
                        "has_image": "1",
                        "has_video": "544",
                        "paper_award": "",
                        "image_caption": "A reassembly example. (a) The skeletons and shapes show that the right part of g1 is poorly aligned. (b) The clusters reveal that other fragments can be matched at bottom or left side of the target group. (c) The candidate in the selected cluster has the same appearance but different match relations. (d) Refine the alignments between groups. (e) Visual cues are shown in real-time to indicate relations. (f) Force cue is added on f2 and f3 to help rotate the f3 to align. (g) The reassembled stele after finishing the workflow.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-full-1272-qa",
                        "session_id": "full26",
                        "type": "Virtual Q+A",
                        "title": "PuzzleFixer: A Visual Reassembly System for Immersive Fragments Restoration (Q+A)",
                        "contributors": [
                            "Shuainan Ye"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "v-full-1272",
                        "file_name": "",
                        "time_stamp": "2022-10-19T19:34:00Z",
                        "time_start": "2022-10-19T19:34:00Z",
                        "time_end": "2022-10-19T19:36:00Z",
                        "paper_type": "full",
                        "keywords": "",
                        "has_image": "1",
                        "has_video": "544",
                        "paper_award": "",
                        "image_caption": "A reassembly example. (a) The skeletons and shapes show that the right part of g1 is poorly aligned. (b) The clusters reveal that other fragments can be matched at bottom or left side of the target group. (c) The candidate in the selected cluster has the same appearance but different match relations. (d) Refine the alignments between groups. (e) Visual cues are shown in real-time to indicate relations. (f) Force cue is added on f2 and f3 to help rotate the f3 to align. (g) The reassembled stele after finishing the workflow.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-full-1382-pres",
                        "session_id": "full26",
                        "type": "Virtual Presentation (pre-recorded)",
                        "title": "Effects of View Layout on Situated Analytics for Multiple Representations in Immersive Visualizaiton",
                        "contributors": [
                            "Zhen Wen"
                        ],
                        "authors": [
                            "Zhen Wen",
                            "Wei Zeng",
                            "Luoxuan Weng",
                            "Yihan Liu",
                            "Mingliang Xu",
                            "Wei Chen"
                        ],
                        "abstract": "",
                        "uid": "v-full-1382",
                        "file_name": "",
                        "time_stamp": "2022-10-19T19:36:00Z",
                        "time_start": "2022-10-19T19:36:00Z",
                        "time_end": "2022-10-19T19:46:00Z",
                        "paper_type": "full",
                        "keywords": "",
                        "has_image": "1",
                        "has_video": "546",
                        "paper_award": "",
                        "image_caption": "In this paper, we present an in-depth study on the effects of view layout on situated analytics for multiple-view representations in immersive visualization. We distill a list of design requirements for effective view layout\nthat promotes situatedness and analytics simultaneously. To fulfill the requirements, we leverage cylindrical reference frame and propose a force-directed approach to automatically position multiple-view representations in immersive visualization.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-full-1382-qa",
                        "session_id": "full26",
                        "type": "Virtual Q+A",
                        "title": "Effects of View Layout on Situated Analytics for Multiple Representations in Immersive Visualizaiton (Q+A)",
                        "contributors": [
                            "Zhen Wen"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "v-full-1382",
                        "file_name": "",
                        "time_stamp": "2022-10-19T19:46:00Z",
                        "time_start": "2022-10-19T19:46:00Z",
                        "time_end": "2022-10-19T19:48:00Z",
                        "paper_type": "full",
                        "keywords": "",
                        "has_image": "1",
                        "has_video": "546",
                        "paper_award": "",
                        "image_caption": "In this paper, we present an in-depth study on the effects of view layout on situated analytics for multiple-view representations in immersive visualization. We distill a list of design requirements for effective view layout\nthat promotes situatedness and analytics simultaneously. To fulfill the requirements, we leverage cylindrical reference frame and propose a force-directed approach to automatically position multiple-view representations in immersive visualization.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-full-1523-pres",
                        "session_id": "full26",
                        "type": "Virtual Presentation (pre-recorded)",
                        "title": "RoboHapalytics: A Robot Assisted Haptic Controller for Immersive Analytics",
                        "contributors": [
                            "shaozhang dai"
                        ],
                        "authors": [
                            "Shaozhang Dai",
                            "Tim Dwyer",
                            "Barrett Ens",
                            "Jim Smiley",
                            "Lonni Besan\u00e7on"
                        ],
                        "abstract": "",
                        "uid": "v-full-1523",
                        "file_name": "",
                        "time_stamp": "2022-10-19T19:48:00Z",
                        "time_start": "2022-10-19T19:48:00Z",
                        "time_end": "2022-10-19T19:58:00Z",
                        "paper_type": "full",
                        "keywords": "",
                        "has_image": "1",
                        "has_video": "538",
                        "paper_award": "",
                        "image_caption": "RoboHapalytics: A Robot Assisted Haptic Controller for Immersive Analytics [2022 VIS]",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-full-1523-qa",
                        "session_id": "full26",
                        "type": "Virtual Q+A",
                        "title": "RoboHapalytics: A Robot Assisted Haptic Controller for Immersive Analytics (Q+A)",
                        "contributors": [
                            "shaozhang dai"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "v-full-1523",
                        "file_name": "",
                        "time_stamp": "2022-10-19T19:58:00Z",
                        "time_start": "2022-10-19T19:58:00Z",
                        "time_end": "2022-10-19T20:00:00Z",
                        "paper_type": "full",
                        "keywords": "",
                        "has_image": "1",
                        "has_video": "538",
                        "paper_award": "",
                        "image_caption": "RoboHapalytics: A Robot Assisted Haptic Controller for Immersive Analytics [2022 VIS]",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-tvcg-9645242-pres",
                        "session_id": "full26",
                        "type": "In Person Presentation",
                        "title": "Labeling Out-of-View Objects in Immersive Analytics to Support Situated Visual Searching",
                        "contributors": [
                            "Tica Lin"
                        ],
                        "authors": [
                            "Tica Lin",
                            "Yalong Yang",
                            "Johanna Beyer",
                            "Hanspeter Pfister"
                        ],
                        "abstract": "Augmented Reality (AR) embeds digital information into objects of the physical world. Data can be shown in-situ, thereby enabling real-time visual comparisons and object search in real-life user tasks, such as comparing products and looking up scores in a sports game. While there have been studies on designing AR interfaces for situated information retrieval, there has only been limited research on AR object labeling for visual search tasks in the spatial environment. In this paper, we identify and categorize different design aspects in AR label design and report on a formal user study on labels for out-of-view objects to support visual search tasks in AR. We design three visualization techniques for out-of-view object labeling in AR, which respectively encode the relative physical position (height-encoded), the rotational direction (angle-encoded), and the label values (value-encoded) of the objects. We further implement two traditional in-view object labeling techniques, where labels are placed either next to the respective objects (situated) or at the edge of the AR FoV (boundary). We evaluate these five different label conditions in three visual search tasks for static objects. Our study shows that out-of-view object labels are beneficial when searching for objects outside the FoV, spatial orientation, and when comparing multiple spatially sparse objects. Angle-encoded labels with directional cues of the surrounding objects have the overall best performance with the highest user satisfaction. We discuss the implications of our findings for future immersive AR interface design.",
                        "uid": "v-tvcg-9645242",
                        "file_name": "",
                        "time_stamp": "2022-10-19T20:00:00Z",
                        "time_start": "2022-10-19T20:00:00Z",
                        "time_end": "2022-10-19T20:10:00Z",
                        "paper_type": "full",
                        "keywords": "Object Labeling, Mixed / Augmented Reality, Immersive Analytics, Situated Analytics, Data Visualization",
                        "has_image": "1",
                        "has_video": "262",
                        "paper_award": "",
                        "image_caption": "AR label design for objects outside the field-of-view (FOV). (a) Our user study uses a VR HMD to simulate consistent AR conditions. (b) The user is surrounded by spatially sparse objects and can see labels for in-view and out-of-view objects on the AR screen. Labels for out-of-view objects are placed on the boundary to support embodied navigation. (c) A simulated grocery shopping experience with AR labels.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-tvcg-9645242-qa",
                        "session_id": "full26",
                        "type": "In Person Q+A",
                        "title": "Labeling Out-of-View Objects in Immersive Analytics to Support Situated Visual Searching (Q+A)",
                        "contributors": [
                            "Tica Lin"
                        ],
                        "authors": [],
                        "abstract": "Augmented Reality (AR) embeds digital information into objects of the physical world. Data can be shown in-situ, thereby enabling real-time visual comparisons and object search in real-life user tasks, such as comparing products and looking up scores in a sports game. While there have been studies on designing AR interfaces for situated information retrieval, there has only been limited research on AR object labeling for visual search tasks in the spatial environment. In this paper, we identify and categorize different design aspects in AR label design and report on a formal user study on labels for out-of-view objects to support visual search tasks in AR. We design three visualization techniques for out-of-view object labeling in AR, which respectively encode the relative physical position (height-encoded), the rotational direction (angle-encoded), and the label values (value-encoded) of the objects. We further implement two traditional in-view object labeling techniques, where labels are placed either next to the respective objects (situated) or at the edge of the AR FoV (boundary). We evaluate these five different label conditions in three visual search tasks for static objects. Our study shows that out-of-view object labels are beneficial when searching for objects outside the FoV, spatial orientation, and when comparing multiple spatially sparse objects. Angle-encoded labels with directional cues of the surrounding objects have the overall best performance with the highest user satisfaction. We discuss the implications of our findings for future immersive AR interface design.",
                        "uid": "v-tvcg-9645242",
                        "file_name": "",
                        "time_stamp": "2022-10-19T20:10:00Z",
                        "time_start": "2022-10-19T20:10:00Z",
                        "time_end": "2022-10-19T20:12:00Z",
                        "paper_type": "full",
                        "keywords": "Object Labeling, Mixed / Augmented Reality, Immersive Analytics, Situated Analytics, Data Visualization",
                        "has_image": "1",
                        "has_video": "262",
                        "paper_award": "",
                        "image_caption": "AR label design for objects outside the field-of-view (FOV). (a) Our user study uses a VR HMD to simulate consistent AR conditions. (b) The user is surrounded by spatially sparse objects and can see labels for in-view and out-of-view objects on the AR screen. Labels for out-of-view objects are placed on the boundary to support embodied navigation. (c) A simulated grocery shopping experience with AR labels.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    }
                ]
            },
            {
                "title": "Questioning Data and Data Bias",
                "session_id": "full27",
                "event_prefix": "v-full",
                "track": "ok5",
                "livestream_id": "ok5-wed",
                "session_image": "full27.png",
                "chair": [
                    "Emily Wall"
                ],
                "organizers": [],
                "time_start": "2022-10-19T19:00:00Z",
                "time_end": "2022-10-19T20:15:00Z",
                "discord_category": "",
                "discord_channel": "",
                "discord_channel_id": "",
                "discord_link": "",
                "slido_link": "",
                "youtube_url": "",
                "zoom_meeting": "",
                "zoom_password": "",
                "zoom_link": "",
                "ff_link": "",
                "ff_playlist": "",
                "time_slots": [
                    {
                        "slot_id": "full27-opening",
                        "session_id": "full27",
                        "type": "In Person Other",
                        "title": "Session Opening",
                        "contributors": [
                            "Emily Wall"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "",
                        "file_name": "",
                        "time_stamp": "2022-10-19T19:00:00Z",
                        "time_start": "2022-10-19T19:00:00Z",
                        "time_end": "2022-10-19T19:00:00Z",
                        "paper_type": "",
                        "keywords": "",
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-full-1351-pres",
                        "session_id": "full27",
                        "type": "Virtual Presentation (pre-recorded)",
                        "title": "VACSEN: A Visualization Approach for Noise Awareness in Quantum Computing",
                        "contributors": [
                            "Shaolun Ruan"
                        ],
                        "authors": [
                            "Shaolun Ruan",
                            "Yong Wang",
                            "Weiwen Jiang",
                            "Ying Mao",
                            "Qiang Guan"
                        ],
                        "abstract": "",
                        "uid": "v-full-1351",
                        "file_name": "",
                        "time_stamp": "2022-10-19T19:00:00Z",
                        "time_start": "2022-10-19T19:00:00Z",
                        "time_end": "2022-10-19T19:10:00Z",
                        "paper_type": "full",
                        "keywords": "",
                        "has_image": "1",
                        "has_video": "601",
                        "paper_award": "",
                        "image_caption": "The severe and inevitable noise issues are the key challenges to achieving the real quantum advantages in today\u2019s quantum computers. We propose VACSEN, a visualization approach for noise awareness in quantum computing. VACSEN supports a real-time noise awareness of quantum computers and compiled circuits, leading to a better circuit execution with higher fidelity.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-full-1351-qa",
                        "session_id": "full27",
                        "type": "Virtual Q+A",
                        "title": "VACSEN: A Visualization Approach for Noise Awareness in Quantum Computing (Q+A)",
                        "contributors": [
                            "Shaolun Ruan"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "v-full-1351",
                        "file_name": "",
                        "time_stamp": "2022-10-19T19:10:00Z",
                        "time_start": "2022-10-19T19:10:00Z",
                        "time_end": "2022-10-19T19:12:00Z",
                        "paper_type": "full",
                        "keywords": "",
                        "has_image": "1",
                        "has_video": "601",
                        "paper_award": "",
                        "image_caption": "The severe and inevitable noise issues are the key challenges to achieving the real quantum advantages in today\u2019s quantum computers. We propose VACSEN, a visualization approach for noise awareness in quantum computing. VACSEN supports a real-time noise awareness of quantum computers and compiled circuits, leading to a better circuit execution with higher fidelity.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-tvcg-9676662-pres",
                        "session_id": "full27",
                        "type": "In Person Presentation",
                        "title": "Do you believe your (social media) data? A personal story on location data biases, errors, and plausibility as well as their visualization",
                        "contributors": [
                            "Tobias Isenberg"
                        ],
                        "authors": [
                            "Tobias Isenberg",
                            "Zujany Salazar",
                            "Rafael Blanco",
                            "Catherine Plaisant"
                        ],
                        "abstract": "We present a case study on a journey about a personal data collection of carnivorous plant species habitats, and the resulting scientific exploration of location data biases, data errors, location hiding, and data plausibility. While initially driven by personal interest, our work led to the analysis and development of various means for visualizing threats to insight from geo-tagged social media data. In the course of this endeavor we analyzed local and global geographic distributions and their inaccuracies. We also contribute Motion Plausibility Profiles---a new means for visualizing how believable a specific contributor\u2019s location data is or if it was likely manipulated. We then compared our own repurposed social media dataset with data from a dedicated citizen science project. Compared to biases and errors in the literature on traditional citizen science data, with our visualizations we could also identify some new types or show new aspects for known ones. Moreover, we demonstrate several types of errors and biases for repurposed social media data.",
                        "uid": "v-tvcg-9676662",
                        "file_name": "",
                        "time_stamp": "2022-10-19T19:12:00Z",
                        "time_start": "2022-10-19T19:12:00Z",
                        "time_end": "2022-10-19T19:22:00Z",
                        "paper_type": "full",
                        "keywords": "Social media data; Flickr; Panoramio; iNaturalist; data bias; data error; data plausibility; data obfuscation; citizen science",
                        "has_image": "1",
                        "has_video": "581",
                        "paper_award": "",
                        "image_caption": "In our paper we describe a journey diven by a personal dataset, in which we discovered and then analyzed various sources of data errors and data bias, This investigation led us to new visual data representation we call Motion Plausibility Profiles, which allow us to analyze a person's history of posting geo-located images on social media. The image shows the Motion Plausibility Profile of a Flickr user, which indicates that they posted only one or two images at with plausible coordinate data in 2008, that starting from 2010 they apparently individually manipulated the geo-locations of the posted images for each image individually as evident in the implausible (color-coded) speeds between image locations (including some that would require jet airplaine travel between photo sites), and from sometime in 2013 they assigned the exact same geo-location for all of several images that they posted for a given day (indicated in blue).",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-tvcg-9676662-qa",
                        "session_id": "full27",
                        "type": "In Person Q+A",
                        "title": "Do you believe your (social media) data? A personal story on location data biases, errors, and plausibility as well as their visualization (Q+A)",
                        "contributors": [
                            "Tobias Isenberg"
                        ],
                        "authors": [],
                        "abstract": "We present a case study on a journey about a personal data collection of carnivorous plant species habitats, and the resulting scientific exploration of location data biases, data errors, location hiding, and data plausibility. While initially driven by personal interest, our work led to the analysis and development of various means for visualizing threats to insight from geo-tagged social media data. In the course of this endeavor we analyzed local and global geographic distributions and their inaccuracies. We also contribute Motion Plausibility Profiles---a new means for visualizing how believable a specific contributor\u2019s location data is or if it was likely manipulated. We then compared our own repurposed social media dataset with data from a dedicated citizen science project. Compared to biases and errors in the literature on traditional citizen science data, with our visualizations we could also identify some new types or show new aspects for known ones. Moreover, we demonstrate several types of errors and biases for repurposed social media data.",
                        "uid": "v-tvcg-9676662",
                        "file_name": "",
                        "time_stamp": "2022-10-19T19:22:00Z",
                        "time_start": "2022-10-19T19:22:00Z",
                        "time_end": "2022-10-19T19:24:00Z",
                        "paper_type": "full",
                        "keywords": "Social media data; Flickr; Panoramio; iNaturalist; data bias; data error; data plausibility; data obfuscation; citizen science",
                        "has_image": "1",
                        "has_video": "581",
                        "paper_award": "",
                        "image_caption": "In our paper we describe a journey diven by a personal dataset, in which we discovered and then analyzed various sources of data errors and data bias, This investigation led us to new visual data representation we call Motion Plausibility Profiles, which allow us to analyze a person's history of posting geo-located images on social media. The image shows the Motion Plausibility Profile of a Flickr user, which indicates that they posted only one or two images at with plausible coordinate data in 2008, that starting from 2010 they apparently individually manipulated the geo-locations of the posted images for each image individually as evident in the implausible (color-coded) speeds between image locations (including some that would require jet airplaine travel between photo sites), and from sometime in 2013 they assigned the exact same geo-location for all of several images that they posted for a given day (indicated in blue).",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-full-1050-pres",
                        "session_id": "full27",
                        "type": "In Person Presentation",
                        "title": "D-BIAS: A Causality-Based Human-in-the-Loop System for Tackling Algorithmic Bias",
                        "contributors": [
                            "Bhavya Ghai"
                        ],
                        "authors": [
                            "Bhavya Ghai",
                            "Klaus Mueller"
                        ],
                        "abstract": "",
                        "uid": "v-full-1050",
                        "file_name": "",
                        "time_stamp": "2022-10-19T19:24:00Z",
                        "time_start": "2022-10-19T19:24:00Z",
                        "time_end": "2022-10-19T19:34:00Z",
                        "paper_type": "full",
                        "keywords": "",
                        "has_image": "1",
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "The visual interface of the D-BIAS tool. (A) The Generator panel: used to create the causal network and download the debiased dataset (B) The Causal Network view: shows the causal relations between the attributes of the data, allows user to inject their prior in the system (C) The Evaluation panel: used to choose the sensitive variable, the ML model and displays different evaluation metrics. This snapshot pertains to the Adult Income dataset post-debiasing.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-full-1050-qa",
                        "session_id": "full27",
                        "type": "In Person Q+A",
                        "title": "D-BIAS: A Causality-Based Human-in-the-Loop System for Tackling Algorithmic Bias (Q+A)",
                        "contributors": [
                            "Bhavya Ghai"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "v-full-1050",
                        "file_name": "",
                        "time_stamp": "2022-10-19T19:34:00Z",
                        "time_start": "2022-10-19T19:34:00Z",
                        "time_end": "2022-10-19T19:36:00Z",
                        "paper_type": "full",
                        "keywords": "",
                        "has_image": "1",
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "The visual interface of the D-BIAS tool. (A) The Generator panel: used to create the causal network and download the debiased dataset (B) The Causal Network view: shows the causal relations between the attributes of the data, allows user to inject their prior in the system (C) The Evaluation panel: used to choose the sensitive variable, the ML model and displays different evaluation metrics. This snapshot pertains to the Adult Income dataset post-debiasing.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-full-1618-pres",
                        "session_id": "full27",
                        "type": "In Person Presentation",
                        "title": "A Unified Comparison of User Modeling Techniques for Information Relevance and Exploration Bias",
                        "contributors": [
                            "Sunwoo Ha"
                        ],
                        "authors": [
                            "Sunwoo Ha",
                            "Shayan Monadjemi",
                            "Alvitta Ottley",
                            "Roman Garnett"
                        ],
                        "abstract": "",
                        "uid": "v-full-1618",
                        "file_name": "",
                        "time_stamp": "2022-10-19T19:36:00Z",
                        "time_start": "2022-10-19T19:36:00Z",
                        "time_end": "2022-10-19T19:46:00Z",
                        "paper_type": "full",
                        "keywords": "",
                        "has_image": "1",
                        "has_video": "515",
                        "paper_award": "",
                        "image_caption": "In our paper, we present a unified comparison of techniques that predicts the user's next data interaction and detect their exploration bias.The performance of eight previously proposed techniques are evaluated with four unique user study interaction logs.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-full-1618-qa",
                        "session_id": "full27",
                        "type": "In Person Q+A",
                        "title": "A Unified Comparison of User Modeling Techniques for Information Relevance and Exploration Bias (Q+A)",
                        "contributors": [
                            "Sunwoo Ha"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "v-full-1618",
                        "file_name": "",
                        "time_stamp": "2022-10-19T19:46:00Z",
                        "time_start": "2022-10-19T19:46:00Z",
                        "time_end": "2022-10-19T19:48:00Z",
                        "paper_type": "full",
                        "keywords": "",
                        "has_image": "1",
                        "has_video": "515",
                        "paper_award": "",
                        "image_caption": "In our paper, we present a unified comparison of techniques that predicts the user's next data interaction and detect their exploration bias.The performance of eight previously proposed techniques are evaluated with four unique user study interaction logs.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-full-1151-pres",
                        "session_id": "full27",
                        "type": "In Person Presentation",
                        "title": "Seeing What You Believe or Believing What You See? Belief Biases Correlation Estimation",
                        "contributors": [
                            "Cindy Xiong"
                        ],
                        "authors": [
                            "Cindy Xiong",
                            "Chase Stokes",
                            "Yea-Seul Kim",
                            "Steven Franconeri"
                        ],
                        "abstract": "",
                        "uid": "v-full-1151",
                        "file_name": "",
                        "time_stamp": "2022-10-19T19:48:00Z",
                        "time_start": "2022-10-19T19:48:00Z",
                        "time_end": "2022-10-19T19:58:00Z",
                        "paper_type": "full",
                        "keywords": "",
                        "has_image": "1",
                        "has_video": "671",
                        "paper_award": "",
                        "image_caption": "Your new data have arrived. \nWith the intent of showing a relationship between the strength of \nenvironmental regulations in a region and that region's air quality, \nyou plot your data values in a scatterplot. \nAt first, the correlation is barely noticeable. \nBut you then notice a small set of outliers. \nIf you ignore those, the correlation looks as strong as you expected.\nPeople might induce themselves to see a correlation as weak or strong, \ndepending on existing beliefs about data.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-full-1151-qa",
                        "session_id": "full27",
                        "type": "In Person Q+A",
                        "title": "Seeing What You Believe or Believing What You See? Belief Biases Correlation Estimation (Q+A)",
                        "contributors": [
                            "Cindy Xiong"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "v-full-1151",
                        "file_name": "",
                        "time_stamp": "2022-10-19T19:58:00Z",
                        "time_start": "2022-10-19T19:58:00Z",
                        "time_end": "2022-10-19T20:00:00Z",
                        "paper_type": "full",
                        "keywords": "",
                        "has_image": "1",
                        "has_video": "671",
                        "paper_award": "",
                        "image_caption": "Your new data have arrived. \nWith the intent of showing a relationship between the strength of \nenvironmental regulations in a region and that region's air quality, \nyou plot your data values in a scatterplot. \nAt first, the correlation is barely noticeable. \nBut you then notice a small set of outliers. \nIf you ignore those, the correlation looks as strong as you expected.\nPeople might induce themselves to see a correlation as weak or strong, \ndepending on existing beliefs about data.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-full-1155-pres",
                        "session_id": "full27",
                        "type": "In Person Presentation",
                        "title": "Data Hunches: Incorporating Personal Knowledge into Visualizations",
                        "contributors": [
                            "Haihan Lin"
                        ],
                        "authors": [
                            "Haihan Lin",
                            "Derya Akbaba",
                            "Miriah Meyer",
                            "Alexander Lex"
                        ],
                        "abstract": "",
                        "uid": "v-full-1155",
                        "file_name": "",
                        "time_stamp": "2022-10-19T20:00:00Z",
                        "time_start": "2022-10-19T20:00:00Z",
                        "time_end": "2022-10-19T20:10:00Z",
                        "paper_type": "full",
                        "keywords": "",
                        "has_image": "1",
                        "has_video": "647",
                        "paper_award": "",
                        "image_caption": "Three rectangle bars placed as a horizontal bar chart, with name Germany, New Zealand, and Norway. The Germany bar has five sketchy lines vertically placed. Shorter sketchy zigzag line filled bars are placed on top of New Zealand and Norway. And on the New Zealand bar, there is a thumb-up icon with number 4 next to it, and a thumb-down icon with number 1 next to it.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-full-1155-qa",
                        "session_id": "full27",
                        "type": "In Person Q+A",
                        "title": "Data Hunches: Incorporating Personal Knowledge into Visualizations (Q+A)",
                        "contributors": [
                            "Haihan Lin"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "v-full-1155",
                        "file_name": "",
                        "time_stamp": "2022-10-19T20:10:00Z",
                        "time_start": "2022-10-19T20:10:00Z",
                        "time_end": "2022-10-19T20:12:00Z",
                        "paper_type": "full",
                        "keywords": "",
                        "has_image": "1",
                        "has_video": "647",
                        "paper_award": "",
                        "image_caption": "Three rectangle bars placed as a horizontal bar chart, with name Germany, New Zealand, and Norway. The Germany bar has five sketchy lines vertically placed. Shorter sketchy zigzag line filled bars are placed on top of New Zealand and Norway. And on the New Zealand bar, there is a thumb-up icon with number 4 next to it, and a thumb-down icon with number 1 next to it.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    }
                ]
            },
            {
                "title": "DNA/Genome and Molecular Data/Vis",
                "session_id": "full28",
                "event_prefix": "v-full",
                "track": "ok1",
                "livestream_id": "ok1-wed",
                "session_image": "full28.png",
                "chair": [
                    "Michael Krone"
                ],
                "organizers": [],
                "time_start": "2022-10-19T20:45:00Z",
                "time_end": "2022-10-19T22:00:00Z",
                "discord_category": "",
                "discord_channel": "",
                "discord_channel_id": "",
                "discord_link": "",
                "slido_link": "",
                "youtube_url": "",
                "zoom_meeting": "",
                "zoom_password": "",
                "zoom_link": "",
                "ff_link": "",
                "ff_playlist": "",
                "time_slots": [
                    {
                        "slot_id": "full28-opening",
                        "session_id": "full28",
                        "type": "In Person Other",
                        "title": "Session Opening",
                        "contributors": [
                            "Michael Krone"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "",
                        "file_name": "",
                        "time_stamp": "2022-10-19T20:45:00Z",
                        "time_start": "2022-10-19T20:45:00Z",
                        "time_end": "2022-10-19T20:45:00Z",
                        "paper_type": "",
                        "keywords": "",
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-full-1193-pres",
                        "session_id": "full28",
                        "type": "In Person Presentation",
                        "title": "Multi-View Design Patterns and Responsive Visualization for Genomics Data",
                        "contributors": [
                            "Sehi L'Yi"
                        ],
                        "authors": [
                            "Sehi L'Yi",
                            "Nils Gehlenborg"
                        ],
                        "abstract": "",
                        "uid": "v-full-1193",
                        "file_name": "",
                        "time_stamp": "2022-10-19T20:45:00Z",
                        "time_start": "2022-10-19T20:45:00Z",
                        "time_end": "2022-10-19T20:55:00Z",
                        "paper_type": "full",
                        "keywords": "",
                        "has_image": "1",
                        "has_video": "626",
                        "paper_award": "",
                        "image_caption": "Through a survey, we classify typical view composition patterns of genomics visualizations, such as vertically long, horizontally wide, circular, and cross-shaped compositions. We then identify their usability issues in different resolutions that stem from the composition patterns, as well as approaches to address the issues and to make genomics visualizations responsive.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-full-1193-qa",
                        "session_id": "full28",
                        "type": "In Person Q+A",
                        "title": "Multi-View Design Patterns and Responsive Visualization for Genomics Data (Q+A)",
                        "contributors": [
                            "Sehi L'Yi"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "v-full-1193",
                        "file_name": "",
                        "time_stamp": "2022-10-19T20:55:00Z",
                        "time_start": "2022-10-19T20:55:00Z",
                        "time_end": "2022-10-19T20:57:00Z",
                        "paper_type": "full",
                        "keywords": "",
                        "has_image": "1",
                        "has_video": "626",
                        "paper_award": "",
                        "image_caption": "Through a survey, we classify typical view composition patterns of genomics visualizations, such as vertically long, horizontally wide, circular, and cross-shaped compositions. We then identify their usability issues in different resolutions that stem from the composition patterns, as well as approaches to address the issues and to make genomics visualizations responsive.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-tvcg-9523759-pres",
                        "session_id": "full28",
                        "type": "Virtual Presentation (pre-recorded)",
                        "title": "Vivern \u2013 A Virtual Environment for Multiscale Visualization and Modeling of DNA Nanostructures",
                        "contributors": [
                            "David Ku\u0165\u00e1k"
                        ],
                        "authors": [
                            "David Ku\u0165\u00e1k",
                            "Matias Nicol\u00e1s Selzer",
                            "Jan By\u0161ka",
                            "Mar\u00eda Luj\u00e1n Ganuza",
                            "Ivan Bari\u0161i\u0107",
                            "Barbora Kozl\u00edkov\u00e1",
                            "Haichao Miao"
                        ],
                        "abstract": "DNA nanostructures offer promising applications, particularly in the biomedical domain, as they can be used for targeted drug delivery, construction of nanorobots, or as a basis for molecular motors. One of the most prominent techniques for assembling these structures is DNA origami. Nowadays, desktop applications are used for the in silico design of such structures. However, as such structures are often spatially complex, their assembly and analysis are complicated. Since virtual reality was proven to be advantageous for such spatial-related tasks and there are no existing VR solutions focused on this domain, we propose Vivern, a VR application that allows domain experts to design and visually examine DNA origami nanostructures. Our approach presents different abstracted visual representations of the nanostructures, various color schemes, and an ability to place several DNA nanostructures and proteins in one environment, thus allowing for the detailed analysis of complex assemblies. We also present two novel examination tools, the Magic Scale Lens and the DNA Untwister, that allow the experts to visually embed different representations into local regions to preserve the context and support detailed investigation. To showcase the capabilities of our solution, prototypes of novel nanodevices conceptualized by our collaborating experts, such as DNA-protein hybrid structures and DNA origami superstructures, are presented. Finally, the results of two rounds of evaluations are summarized. They demonstrate the advantages of our solution, especially for scenarios where current desktop tools are very limited, while also presenting possible future research directions.",
                        "uid": "v-tvcg-9523759",
                        "file_name": "",
                        "time_stamp": "2022-10-19T20:57:00Z",
                        "time_start": "2022-10-19T20:57:00Z",
                        "time_end": "2022-10-19T21:07:00Z",
                        "paper_type": "full",
                        "keywords": "Virtual reality, abstraction, DNA origami, nanostructures, visualization, focus+context, interaction, in silico modeling, nanotechnology, multiscale, magic scale lens",
                        "has_image": "1",
                        "has_video": "650",
                        "paper_award": "",
                        "image_caption": "Screenshot of the Vivern application capturing the user in action. The scene contains two DNA origami structures. One in the back is visualized in single-strand scale,\nwhile the structure in front of the user is currently being modified. This structure started in a double-strand scale and the user used the provided focus+context techniques to embed additional scales, namely single-stranded one and then nucleotide one.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-tvcg-9523759-qa",
                        "session_id": "full28",
                        "type": "Virtual Q+A",
                        "title": "Vivern \u2013 A Virtual Environment for Multiscale Visualization and Modeling of DNA Nanostructures (Q+A)",
                        "contributors": [
                            "David Ku\u0165\u00e1k"
                        ],
                        "authors": [],
                        "abstract": "DNA nanostructures offer promising applications, particularly in the biomedical domain, as they can be used for targeted drug delivery, construction of nanorobots, or as a basis for molecular motors. One of the most prominent techniques for assembling these structures is DNA origami. Nowadays, desktop applications are used for the in silico design of such structures. However, as such structures are often spatially complex, their assembly and analysis are complicated. Since virtual reality was proven to be advantageous for such spatial-related tasks and there are no existing VR solutions focused on this domain, we propose Vivern, a VR application that allows domain experts to design and visually examine DNA origami nanostructures. Our approach presents different abstracted visual representations of the nanostructures, various color schemes, and an ability to place several DNA nanostructures and proteins in one environment, thus allowing for the detailed analysis of complex assemblies. We also present two novel examination tools, the Magic Scale Lens and the DNA Untwister, that allow the experts to visually embed different representations into local regions to preserve the context and support detailed investigation. To showcase the capabilities of our solution, prototypes of novel nanodevices conceptualized by our collaborating experts, such as DNA-protein hybrid structures and DNA origami superstructures, are presented. Finally, the results of two rounds of evaluations are summarized. They demonstrate the advantages of our solution, especially for scenarios where current desktop tools are very limited, while also presenting possible future research directions.",
                        "uid": "v-tvcg-9523759",
                        "file_name": "",
                        "time_stamp": "2022-10-19T21:07:00Z",
                        "time_start": "2022-10-19T21:07:00Z",
                        "time_end": "2022-10-19T21:09:00Z",
                        "paper_type": "full",
                        "keywords": "Virtual reality, abstraction, DNA origami, nanostructures, visualization, focus+context, interaction, in silico modeling, nanotechnology, multiscale, magic scale lens",
                        "has_image": "1",
                        "has_video": "650",
                        "paper_award": "",
                        "image_caption": "Screenshot of the Vivern application capturing the user in action. The scene contains two DNA origami structures. One in the back is visualized in single-strand scale,\nwhile the structure in front of the user is currently being modified. This structure started in a double-strand scale and the user used the provided focus+context techniques to embed additional scales, namely single-stranded one and then nucleotide one.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-full-1212-pres",
                        "session_id": "full28",
                        "type": "In Person Presentation",
                        "title": "GenoREC: A Recommendation System for Interactive Genomics Data Visualization",
                        "contributors": [
                            "Aditeya Pandey"
                        ],
                        "authors": [
                            "Aditeya Pandey",
                            "Sehi L'Yi",
                            "Qianwen Wang",
                            "Michelle A. Borkin",
                            "Nils Gehlenborg"
                        ],
                        "abstract": "",
                        "uid": "v-full-1212",
                        "file_name": "",
                        "time_stamp": "2022-10-19T21:09:00Z",
                        "time_start": "2022-10-19T21:09:00Z",
                        "time_end": "2022-10-19T21:19:00Z",
                        "paper_type": "full",
                        "keywords": "",
                        "has_image": "1",
                        "has_video": "929",
                        "paper_award": "",
                        "image_caption": "GenoREC\u2019s user interface depicting the user input panel and recommended visualisations  from the system.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-full-1212-qa",
                        "session_id": "full28",
                        "type": "In Person Q+A",
                        "title": "GenoREC: A Recommendation System for Interactive Genomics Data Visualization (Q+A)",
                        "contributors": [
                            "Aditeya Pandey"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "v-full-1212",
                        "file_name": "",
                        "time_stamp": "2022-10-19T21:19:00Z",
                        "time_start": "2022-10-19T21:19:00Z",
                        "time_end": "2022-10-19T21:21:00Z",
                        "paper_type": "full",
                        "keywords": "",
                        "has_image": "1",
                        "has_video": "929",
                        "paper_award": "",
                        "image_caption": "GenoREC\u2019s user interface depicting the user input panel and recommended visualisations  from the system.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-full-1175-pres",
                        "session_id": "full28",
                        "type": "In Person Presentation",
                        "title": "sMolBoxes: Dataflow Model for Molecular Dynamics Exploration",
                        "contributors": [
                            "Pavol Ulbrich"
                        ],
                        "authors": [
                            "Pavol Ulbrich",
                            "Manuela Waldner",
                            "Katar\u00edna Furmanov\u00e1",
                            "S\u00e9rgio M. Marques",
                            "David Bedn\u00e1\u0159",
                            "Barbora Kozlikova",
                            "Jan By\u0161ka"
                        ],
                        "abstract": "",
                        "uid": "v-full-1175",
                        "file_name": "",
                        "time_stamp": "2022-10-19T21:21:00Z",
                        "time_start": "2022-10-19T21:21:00Z",
                        "time_end": "2022-10-19T21:31:00Z",
                        "paper_type": "full",
                        "keywords": "",
                        "has_image": "1",
                        "has_video": "386",
                        "paper_award": "",
                        "image_caption": "An overview of the sMolBoxes interface for molecular dynamics exploration, consisting of the 3D View, the Canvas with sMolBox Nodes, and the detailed bigBox View for navigation through the 3D animation.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-full-1175-qa",
                        "session_id": "full28",
                        "type": "In Person Q+A",
                        "title": "sMolBoxes: Dataflow Model for Molecular Dynamics Exploration (Q+A)",
                        "contributors": [
                            "Pavol Ulbrich"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "v-full-1175",
                        "file_name": "",
                        "time_stamp": "2022-10-19T21:31:00Z",
                        "time_start": "2022-10-19T21:31:00Z",
                        "time_end": "2022-10-19T21:33:00Z",
                        "paper_type": "full",
                        "keywords": "",
                        "has_image": "1",
                        "has_video": "386",
                        "paper_award": "",
                        "image_caption": "An overview of the sMolBoxes interface for molecular dynamics exploration, consisting of the 3D View, the Canvas with sMolBox Nodes, and the detailed bigBox View for navigation through the 3D animation.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-tvcg-9627526-pres",
                        "session_id": "full28",
                        "type": "In Person Presentation",
                        "title": "Molecumentary: Adaptable Narrated Documentaries Using Molecular Visualization",
                        "contributors": [
                            "David Kouril",
                            "Ond\u0159ej Strnad"
                        ],
                        "authors": [
                            "David Kou\u0159il",
                            "Ond\u0159ej Strnad",
                            "Peter Mindek",
                            "Sarkis Halladjian",
                            "Tobias Isenberg",
                            "M. Eduard Gr\u00f6ller",
                            "Ivan Viola"
                        ],
                        "abstract": "We present a method for producing documentary-style content using real-time scientific visualization. We introduce molecumentaries, i.e., molecular documentaries featuring structural models from molecular biology, created through adaptable methods instead of the rigid traditional production pipeline. Our work is motivated by the rapid evolution of scientific visualization and it potential in science dissemination. Without some form of explanation or guidance, however, novices and lay-persons often find it difficult to gain insights from the visualization itself. We integrate such knowledge using the verbal channel and provide it along an engaging visual presentation. To realize the synthesis of a molecumentary, we provide technical solutions along two major production steps: (1) preparing a story structure and (2) turning the story into a concrete narrative. In the first step, we compile information about the model from heterogeneous sources into a story graph. We combine local knowledge with external sources to complete the story graph and enrich the final result. In the second step, we synthesize a narrative, i.e., story elements presented in sequence, using the story graph. We then traverse the story graph and generate a virtual tour, using automated camera and visualization transitions. We turn texts written by domain experts into verbal representations using text-to-speech functionality and provide them as a commentary. Using the described framework, we synthesize fly-throughs with descriptions: automatic ones that mimic a manually authored documentary or semi-automatic ones which guide the documentary narrative solely through curated textual input.",
                        "uid": "v-tvcg-9627526",
                        "file_name": "",
                        "time_stamp": "2022-10-19T21:33:00Z",
                        "time_start": "2022-10-19T21:33:00Z",
                        "time_end": "2022-10-19T21:43:00Z",
                        "paper_type": "full",
                        "keywords": "Virtual tour, audio, biological data, storytelling, illustrative visualization.",
                        "has_image": "1",
                        "has_video": "612",
                        "paper_award": "",
                        "image_caption": "A snapshot of a molecumentary: Our framework generates guided tours that describe complex molecular models, such as the HIV pictured here.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-tvcg-9627526-qa",
                        "session_id": "full28",
                        "type": "In Person Q+A",
                        "title": "Molecumentary: Adaptable Narrated Documentaries Using Molecular Visualization (Q+A)",
                        "contributors": [
                            "David Kouril",
                            "Ond\u0159ej Strnad"
                        ],
                        "authors": [],
                        "abstract": "We present a method for producing documentary-style content using real-time scientific visualization. We introduce molecumentaries, i.e., molecular documentaries featuring structural models from molecular biology, created through adaptable methods instead of the rigid traditional production pipeline. Our work is motivated by the rapid evolution of scientific visualization and it potential in science dissemination. Without some form of explanation or guidance, however, novices and lay-persons often find it difficult to gain insights from the visualization itself. We integrate such knowledge using the verbal channel and provide it along an engaging visual presentation. To realize the synthesis of a molecumentary, we provide technical solutions along two major production steps: (1) preparing a story structure and (2) turning the story into a concrete narrative. In the first step, we compile information about the model from heterogeneous sources into a story graph. We combine local knowledge with external sources to complete the story graph and enrich the final result. In the second step, we synthesize a narrative, i.e., story elements presented in sequence, using the story graph. We then traverse the story graph and generate a virtual tour, using automated camera and visualization transitions. We turn texts written by domain experts into verbal representations using text-to-speech functionality and provide them as a commentary. Using the described framework, we synthesize fly-throughs with descriptions: automatic ones that mimic a manually authored documentary or semi-automatic ones which guide the documentary narrative solely through curated textual input.",
                        "uid": "v-tvcg-9627526",
                        "file_name": "",
                        "time_stamp": "2022-10-19T21:43:00Z",
                        "time_start": "2022-10-19T21:43:00Z",
                        "time_end": "2022-10-19T21:45:00Z",
                        "paper_type": "full",
                        "keywords": "Virtual tour, audio, biological data, storytelling, illustrative visualization.",
                        "has_image": "1",
                        "has_video": "612",
                        "paper_award": "",
                        "image_caption": "A snapshot of a molecumentary: Our framework generates guided tours that describe complex molecular models, such as the HIV pictured here.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-full-1578-pres",
                        "session_id": "full28",
                        "type": "Virtual Presentation (pre-recorded)",
                        "title": "Polyphony: an Interactive Transfer Learning Framework for Single-Cell Data Analysis",
                        "contributors": [
                            "Furui Cheng"
                        ],
                        "authors": [
                            "Furui Cheng",
                            "Mark S Keller",
                            "Huamin Qu",
                            "Nils Gehlenborg",
                            "Qianwen Wang"
                        ],
                        "abstract": "",
                        "uid": "v-full-1578",
                        "file_name": "",
                        "time_stamp": "2022-10-19T21:45:00Z",
                        "time_start": "2022-10-19T21:45:00Z",
                        "time_end": "2022-10-19T21:55:00Z",
                        "paper_type": "full",
                        "keywords": "",
                        "has_image": "1",
                        "has_video": "588",
                        "paper_award": "",
                        "image_caption": "The interface of Polyphony contains three views: the comparison view, the anchor set view, and the marker view. The comparison view provides an overview of the joint embedding space and offers users interactions to inspect, delete, and add anchors. The anchor set view orders the anchors in a table, supporting inspection and comparing different anchors. The marker view shows the significant genes for the query and reference cells from a focal anchor.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-full-1578-qa",
                        "session_id": "full28",
                        "type": "Virtual Q+A",
                        "title": "Polyphony: an Interactive Transfer Learning Framework for Single-Cell Data Analysis (Q+A)",
                        "contributors": [
                            "Furui Cheng"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "v-full-1578",
                        "file_name": "",
                        "time_stamp": "2022-10-19T21:55:00Z",
                        "time_start": "2022-10-19T21:55:00Z",
                        "time_end": "2022-10-19T21:57:00Z",
                        "paper_type": "full",
                        "keywords": "",
                        "has_image": "1",
                        "has_video": "588",
                        "paper_award": "",
                        "image_caption": "The interface of Polyphony contains three views: the comparison view, the anchor set view, and the marker view. The comparison view provides an overview of the joint embedding space and offers users interactions to inspect, delete, and add anchors. The anchor set view orders the anchors in a table, supporting inspection and comparing different anchors. The marker view shows the significant genes for the query and reference cells from a focal anchor.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    }
                ]
            },
            {
                "title": "Digital Humanities, e-Commerce, and Engineering",
                "session_id": "full29",
                "event_prefix": "v-full",
                "track": "ok1",
                "livestream_id": "ok1-thu",
                "session_image": "full29.png",
                "chair": [
                    "Enrico Bertini"
                ],
                "organizers": [],
                "time_start": "2022-10-20T15:45:00Z",
                "time_end": "2022-10-20T17:00:00Z",
                "discord_category": "",
                "discord_channel": "",
                "discord_channel_id": "",
                "discord_link": "",
                "slido_link": "",
                "youtube_url": "",
                "zoom_meeting": "",
                "zoom_password": "",
                "zoom_link": "",
                "ff_link": "",
                "ff_playlist": "",
                "time_slots": [
                    {
                        "slot_id": "full29-opening",
                        "session_id": "full29",
                        "type": "In Person Other",
                        "title": "Session Opening",
                        "contributors": [
                            "Enrico Bertini"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "",
                        "file_name": "",
                        "time_stamp": "2022-10-20T15:45:00Z",
                        "time_start": "2022-10-20T15:45:00Z",
                        "time_end": "2022-10-20T15:45:00Z",
                        "paper_type": "",
                        "keywords": "",
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-full-1426-pres",
                        "session_id": "full29",
                        "type": "In Person Presentation",
                        "title": "CohortVA: A Visual Analytic System for Progressive Exploration of Cohorts based on Historical Data",
                        "contributors": [
                            "Wei Zhang",
                            "Jason Wong"
                        ],
                        "authors": [
                            "Wei Zhang",
                            "Jason Kamkwai Wong",
                            "Xumeng Wang",
                            "Youcheng Gong",
                            "Rongchen Zhu",
                            "Kai Liu",
                            "Zihan Yan",
                            "Siwei Tan",
                            "Huamin Qu",
                            "Siming Chen",
                            "Wei Chen"
                        ],
                        "abstract": "",
                        "uid": "v-full-1426",
                        "file_name": "",
                        "time_stamp": "2022-10-20T15:45:00Z",
                        "time_start": "2022-10-20T15:45:00Z",
                        "time_end": "2022-10-20T15:55:00Z",
                        "paper_type": "full",
                        "keywords": "",
                        "has_image": "1",
                        "has_video": "540",
                        "paper_award": "",
                        "image_caption": "We present CohortVA, an interactive visual analytic system for historians to identify and explore historical cohorts. Given an initial group of figures, the cohort identification model extracts their common features and identifies potential cohorts to improve the research efficiency. CohortVA\u2019s visual interface provides various supporting information for historians to cross-check those results, fostering trust in the system and a deeper understanding of cohorts. Case studies and historian interviews demonstrate our system\u2019s usefulness and effectiveness. We summarized the learned lessons and design implications, which we believe will guide system designers in dealing with historical data and working with historians.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-full-1426-qa",
                        "session_id": "full29",
                        "type": "In Person Q+A",
                        "title": "CohortVA: A Visual Analytic System for Progressive Exploration of Cohorts based on Historical Data (Q+A)",
                        "contributors": [
                            "Wei Zhang",
                            "Jason Wong"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "v-full-1426",
                        "file_name": "",
                        "time_stamp": "2022-10-20T15:55:00Z",
                        "time_start": "2022-10-20T15:55:00Z",
                        "time_end": "2022-10-20T15:57:00Z",
                        "paper_type": "full",
                        "keywords": "",
                        "has_image": "1",
                        "has_video": "540",
                        "paper_award": "",
                        "image_caption": "We present CohortVA, an interactive visual analytic system for historians to identify and explore historical cohorts. Given an initial group of figures, the cohort identification model extracts their common features and identifies potential cohorts to improve the research efficiency. CohortVA\u2019s visual interface provides various supporting information for historians to cross-check those results, fostering trust in the system and a deeper understanding of cohorts. Case studies and historian interviews demonstrate our system\u2019s usefulness and effectiveness. We summarized the learned lessons and design implications, which we believe will guide system designers in dealing with historical data and working with historians.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-full-1421-pres",
                        "session_id": "full29",
                        "type": "In Person Presentation",
                        "title": "PromotionLens: Inspecting Promotion Strategies of Online E-commerce via Visual Analytics",
                        "contributors": [
                            "Chenyang Zhang"
                        ],
                        "authors": [
                            "Chenyang Zhang",
                            "Xiyuan Wang",
                            "Chuyi Zhao",
                            "Yijing Ren",
                            "Tianyu Zhang",
                            "Zhenhui Peng",
                            "Xiaomeng Fan",
                            "Xiaojuan Ma",
                            "Quan Li"
                        ],
                        "abstract": "",
                        "uid": "v-full-1421",
                        "file_name": "",
                        "time_stamp": "2022-10-20T15:57:00Z",
                        "time_start": "2022-10-20T15:57:00Z",
                        "time_end": "2022-10-20T16:07:00Z",
                        "paper_type": "full",
                        "keywords": "",
                        "has_image": "1",
                        "has_video": "619",
                        "paper_award": "",
                        "image_caption": "Promotions are commonly used by e-commerce merchants to boost sales. The efficacy of different promotion strategies can help sellers adapt their offering to customer demand in order to survive and thrive. We present PromotionLens, a visual analytics system for exploring, comparing, and modeling the impact of various promotional strategies. Our approach combines representative multivariant time-series forecasting models and well-designed visualizations to demonstrate and explain the impact of sales and promotional factors, and to support \u201cwhat-if\u201d analysis of promotions. Two case studies, expert feedback, and a qualitative user study demonstrate the efficacy of PromotionLens.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-full-1421-qa",
                        "session_id": "full29",
                        "type": "In Person Q+A",
                        "title": "PromotionLens: Inspecting Promotion Strategies of Online E-commerce via Visual Analytics (Q+A)",
                        "contributors": [
                            "Chenyang Zhang"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "v-full-1421",
                        "file_name": "",
                        "time_stamp": "2022-10-20T16:07:00Z",
                        "time_start": "2022-10-20T16:07:00Z",
                        "time_end": "2022-10-20T16:09:00Z",
                        "paper_type": "full",
                        "keywords": "",
                        "has_image": "1",
                        "has_video": "619",
                        "paper_award": "",
                        "image_caption": "Promotions are commonly used by e-commerce merchants to boost sales. The efficacy of different promotion strategies can help sellers adapt their offering to customer demand in order to survive and thrive. We present PromotionLens, a visual analytics system for exploring, comparing, and modeling the impact of various promotional strategies. Our approach combines representative multivariant time-series forecasting models and well-designed visualizations to demonstrate and explain the impact of sales and promotional factors, and to support \u201cwhat-if\u201d analysis of promotions. Two case studies, expert feedback, and a qualitative user study demonstrate the efficacy of PromotionLens.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-full-1547-pres",
                        "session_id": "full29",
                        "type": "In Person Presentation",
                        "title": "Interactive Visual Analysis of Structure-borne Noise Data",
                        "contributors": [
                            "Kresimir Matkovic",
                            "Rainer Splechtna"
                        ],
                        "authors": [
                            "Rainer Splechtna",
                            "Denis Gracanin",
                            "Goran Todorovic",
                            "Stanislav Goja",
                            "Boris Bedic",
                            "Helwig Hauser",
                            "Kresimir Matkovic"
                        ],
                        "abstract": "",
                        "uid": "v-full-1547",
                        "file_name": "",
                        "time_stamp": "2022-10-20T16:09:00Z",
                        "time_start": "2022-10-20T16:09:00Z",
                        "time_end": "2022-10-20T16:19:00Z",
                        "paper_type": "full",
                        "keywords": "",
                        "has_image": "1",
                        "has_video": "574",
                        "paper_award": "",
                        "image_caption": "The coordinated multiple views solution for interactive visual analysis of structure-borne noise simulation data. The drill-down view consists of three panes. The overview matrix pane provides aggregated values for individual engine parts across the whole frequency range. The harmonics pane and the frequency band details pane provide more information on data in the frequency domain. The linked 3D views show the three most critical harmonics for the selected frequency band. Additional views, like the box-plot view shown here, can be added on demand during exploratory analysis sessions.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-full-1547-qa",
                        "session_id": "full29",
                        "type": "In Person Q+A",
                        "title": "Interactive Visual Analysis of Structure-borne Noise Data (Q+A)",
                        "contributors": [
                            "Kresimir Matkovic",
                            "Rainer Splechtna"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "v-full-1547",
                        "file_name": "",
                        "time_stamp": "2022-10-20T16:19:00Z",
                        "time_start": "2022-10-20T16:19:00Z",
                        "time_end": "2022-10-20T16:21:00Z",
                        "paper_type": "full",
                        "keywords": "",
                        "has_image": "1",
                        "has_video": "574",
                        "paper_award": "",
                        "image_caption": "The coordinated multiple views solution for interactive visual analysis of structure-borne noise simulation data. The drill-down view consists of three panes. The overview matrix pane provides aggregated values for individual engine parts across the whole frequency range. The harmonics pane and the frequency band details pane provide more information on data in the frequency domain. The linked 3D views show the three most critical harmonics for the selected frequency band. Additional views, like the box-plot view shown here, can be added on demand during exploratory analysis sessions.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-full-1312-pres",
                        "session_id": "full29",
                        "type": "In Person Presentation",
                        "title": "Traveler: Navigating Task Parallel Traces for Performance Analysis",
                        "contributors": [
                            "Sayef Azad Sakin"
                        ],
                        "authors": [
                            "Sayef Azad Sakin",
                            "Alex Bigelow",
                            "Mohammad Tohid",
                            "Connor Scully-Allison",
                            "Carlos Scheidegger",
                            "Steven Robert Brandt",
                            "Christopher P. Taylor",
                            "Kevin A. Huck",
                            "Hartmut Kaiser",
                            "Katherine E. Isaacs"
                        ],
                        "abstract": "",
                        "uid": "v-full-1312",
                        "file_name": "",
                        "time_stamp": "2022-10-20T16:21:00Z",
                        "time_start": "2022-10-20T16:21:00Z",
                        "time_end": "2022-10-20T16:31:00Z",
                        "paper_type": "full",
                        "keywords": "",
                        "has_image": "1",
                        "has_video": "552",
                        "paper_award": "",
                        "image_caption": "Understanding the behavior of software in execution is a key step in fixing performance issues. Execution traces contain a historical record of per-thread events collected while the software was running. Visual analysis of traces is difficult due to the size and complexity of the data. We present Traveler, multi-view coordinated visualization for visual exploration of execution traces. Traveler provides diverse and hierarchical ways of navigating trace data to manage and interpret both the vast scale differences and relationships between asynchronously scheduled computing tasks.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-full-1312-qa",
                        "session_id": "full29",
                        "type": "In Person Q+A",
                        "title": "Traveler: Navigating Task Parallel Traces for Performance Analysis (Q+A)",
                        "contributors": [
                            "Sayef Azad Sakin"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "v-full-1312",
                        "file_name": "",
                        "time_stamp": "2022-10-20T16:31:00Z",
                        "time_start": "2022-10-20T16:31:00Z",
                        "time_end": "2022-10-20T16:33:00Z",
                        "paper_type": "full",
                        "keywords": "",
                        "has_image": "1",
                        "has_video": "552",
                        "paper_award": "",
                        "image_caption": "Understanding the behavior of software in execution is a key step in fixing performance issues. Execution traces contain a historical record of per-thread events collected while the software was running. Visual analysis of traces is difficult due to the size and complexity of the data. We present Traveler, multi-view coordinated visualization for visual exploration of execution traces. Traveler provides diverse and hierarchical ways of navigating trace data to manage and interpret both the vast scale differences and relationships between asynchronously scheduled computing tasks.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-full-1617-pres",
                        "session_id": "full29",
                        "type": "In Person Presentation",
                        "title": "Visual Analysis and Detection of Contrails in Aircraft Engine Simulations",
                        "contributors": [
                            "Md Nafiul Alam Nipu"
                        ],
                        "authors": [
                            "Md Nafiul Alam Nipu",
                            "Carla Gabriela Floricel",
                            "Negar Naghash Zadeh",
                            "Roberto Paoli",
                            "G. Elisabeta Marai"
                        ],
                        "abstract": "",
                        "uid": "v-full-1617",
                        "file_name": "",
                        "time_stamp": "2022-10-20T16:33:00Z",
                        "time_start": "2022-10-20T16:33:00Z",
                        "time_end": "2022-10-20T16:43:00Z",
                        "paper_type": "full",
                        "keywords": "",
                        "has_image": "1",
                        "has_video": "481",
                        "paper_award": "",
                        "image_caption": "A visualization system for the analysis of contrails generated by airplane engines.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-full-1617-qa",
                        "session_id": "full29",
                        "type": "In Person Q+A",
                        "title": "Visual Analysis and Detection of Contrails in Aircraft Engine Simulations (Q+A)",
                        "contributors": [
                            "Md Nafiul Alam Nipu"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "v-full-1617",
                        "file_name": "",
                        "time_stamp": "2022-10-20T16:43:00Z",
                        "time_start": "2022-10-20T16:43:00Z",
                        "time_end": "2022-10-20T16:45:00Z",
                        "paper_type": "full",
                        "keywords": "",
                        "has_image": "1",
                        "has_video": "481",
                        "paper_award": "",
                        "image_caption": "A visualization system for the analysis of contrails generated by airplane engines.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-full-1478-pres",
                        "session_id": "full29",
                        "type": "Virtual Presentation (pre-recorded)",
                        "title": "DPVisCreator: Incorporating visual constraints to privacy-preserving visualization via differential privacy",
                        "contributors": [
                            "Jiehui Zhou"
                        ],
                        "authors": [
                            "Jiehui Zhou",
                            "Xumeng Wang",
                            "Jason Kamkwai Wong",
                            "Huanliang Wang",
                            "Zhongwei Wang",
                            "Xiaoyu Yang",
                            "Xiaoran Yan",
                            "Haozhe Feng",
                            "Huamin Qu",
                            "Haochao Ying",
                            "Wei Chen"
                        ],
                        "abstract": "",
                        "uid": "v-full-1478",
                        "file_name": "",
                        "time_stamp": "2022-10-20T16:45:00Z",
                        "time_start": "2022-10-20T16:45:00Z",
                        "time_end": "2022-10-20T16:55:00Z",
                        "paper_type": "full",
                        "keywords": "",
                        "has_image": "1",
                        "has_video": "522",
                        "paper_award": "",
                        "image_caption": "The overview of our visual analytics approach for generating privacy-preserving visualizations. Users can select patterns of interest from visualizations of tabular data and specify importance weights. The PriVis model considers these pattern constraints and privacy budgets to generate privacy-preserving schemes. Users can evaluate the results based on the visualization charts and utility metrics and return for iterative adjustments.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-full-1478-qa",
                        "session_id": "full29",
                        "type": "Virtual Q+A",
                        "title": "DPVisCreator: Incorporating visual constraints to privacy-preserving visualization via differential privacy (Q+A)",
                        "contributors": [
                            "Jiehui Zhou"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "v-full-1478",
                        "file_name": "",
                        "time_stamp": "2022-10-20T16:55:00Z",
                        "time_start": "2022-10-20T16:55:00Z",
                        "time_end": "2022-10-20T16:57:00Z",
                        "paper_type": "full",
                        "keywords": "",
                        "has_image": "1",
                        "has_video": "522",
                        "paper_award": "",
                        "image_caption": "The overview of our visual analytics approach for generating privacy-preserving visualizations. Users can select patterns of interest from visualizations of tabular data and specify importance weights. The PriVis model considers these pattern constraints and privacy budgets to generate privacy-preserving schemes. Users can evaluate the results based on the visualization charts and utility metrics and return for iterative adjustments.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    }
                ]
            },
            {
                "title": "Decision Making and Reasoning",
                "session_id": "full30",
                "event_prefix": "v-full",
                "track": "ok1",
                "livestream_id": "ok1-wed",
                "session_image": "full30.png",
                "chair": [
                    "Lace Padilla"
                ],
                "organizers": [],
                "time_start": "2022-10-19T14:00:00Z",
                "time_end": "2022-10-19T15:15:00Z",
                "discord_category": "",
                "discord_channel": "",
                "discord_channel_id": "",
                "discord_link": "",
                "slido_link": "",
                "youtube_url": "",
                "zoom_meeting": "",
                "zoom_password": "",
                "zoom_link": "",
                "ff_link": "",
                "ff_playlist": "",
                "time_slots": [
                    {
                        "slot_id": "full30-opening",
                        "session_id": "full30",
                        "type": "In Person Other",
                        "title": "Session Opening",
                        "contributors": [
                            "Lace Padilla"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "",
                        "file_name": "",
                        "time_stamp": "2022-10-19T14:00:00Z",
                        "time_start": "2022-10-19T14:00:00Z",
                        "time_end": "2022-10-19T14:00:00Z",
                        "paper_type": "",
                        "keywords": "",
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-full-1568-pres",
                        "session_id": "full30",
                        "type": "Virtual Presentation (pre-recorded)",
                        "title": "ErgoExplorer: Interactive Ergonomic Risk Assessment from Video Collections",
                        "contributors": [
                            "Claudio Delrieux"
                        ],
                        "authors": [
                            "Manlio Miguel Massiris Fernandez",
                            "Sanjin Rados",
                            "Eduard Gr\u00f6ller",
                            "Claudio Delrieux",
                            "Kresimir Matkovic"
                        ],
                        "abstract": "",
                        "uid": "v-full-1568",
                        "file_name": "",
                        "time_stamp": "2022-10-19T14:00:00Z",
                        "time_start": "2022-10-19T14:00:00Z",
                        "time_end": "2022-10-19T14:10:00Z",
                        "paper_type": "full",
                        "keywords": "",
                        "has_image": "1",
                        "has_video": "455",
                        "paper_award": "",
                        "image_caption": "Ergonomic risk assessment is now, due to an increased awareness, carried out more often than in the past. The conventional risk assessment evaluation is based on expert-assisted observation of the workplaces and manually filling in score tables. With ErgoExplorer it is possible to explore and analyze time dependent ergonomic scores from automatically recorded observations encompassing very long periods. A dashboard as used in one of the analysis sessions is shown here. Ergonomists can understand how to mitigate ergonomic risk by using coordinated multiple views: (a) ErgoView, (b) ErgoTimeline,(c) scatterplot matrix, and (d) parallel coordinates.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-full-1568-qa",
                        "session_id": "full30",
                        "type": "Virtual Q+A",
                        "title": "ErgoExplorer: Interactive Ergonomic Risk Assessment from Video Collections (Q+A)",
                        "contributors": [
                            "Claudio Delrieux"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "v-full-1568",
                        "file_name": "",
                        "time_stamp": "2022-10-19T14:10:00Z",
                        "time_start": "2022-10-19T14:10:00Z",
                        "time_end": "2022-10-19T14:12:00Z",
                        "paper_type": "full",
                        "keywords": "",
                        "has_image": "1",
                        "has_video": "455",
                        "paper_award": "",
                        "image_caption": "Ergonomic risk assessment is now, due to an increased awareness, carried out more often than in the past. The conventional risk assessment evaluation is based on expert-assisted observation of the workplaces and manually filling in score tables. With ErgoExplorer it is possible to explore and analyze time dependent ergonomic scores from automatically recorded observations encompassing very long periods. A dashboard as used in one of the analysis sessions is shown here. Ergonomists can understand how to mitigate ergonomic risk by using coordinated multiple views: (a) ErgoView, (b) ErgoTimeline,(c) scatterplot matrix, and (d) parallel coordinates.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-full-1614-pres",
                        "session_id": "full30",
                        "type": "In Person Presentation",
                        "title": "TrafficVis: Visualizing Organized Activity and Spatio-Temporal Patterns for Detecting Human Trafficking",
                        "contributors": [
                            "Catalina Vajiac"
                        ],
                        "authors": [
                            "Catalina Vajiac",
                            "Duen Horng Chau",
                            "Andreas Olligschlaeger",
                            "Rebecca Mackenzie",
                            "Pratheeksha Nair",
                            "Meng-Chieh Lee",
                            "Yifei Li",
                            "Namyong Park",
                            "Reihaneh Rabbany",
                            "Christos Faloutsos"
                        ],
                        "abstract": "",
                        "uid": "v-full-1614",
                        "file_name": "",
                        "time_stamp": "2022-10-19T14:12:00Z",
                        "time_start": "2022-10-19T14:12:00Z",
                        "time_end": "2022-10-19T14:22:00Z",
                        "paper_type": "full",
                        "keywords": "",
                        "has_image": "1",
                        "has_video": "617",
                        "paper_award": "",
                        "image_caption": "Analyzing online escort ads using TrafficVis: we show one meta-cluster, i.e.  micro (text) clusters connected using metadata, on real data. Some text blurred for privacy. 1. Human trafficking domain expert uses Micro-cluster panel to drill down to specific micro-cluster data and associated ads. 2-3. Expert uses Timeline panel and Map panel to investigate metadata, noticing inconsistent posting time and regional geographic spread, ruling out spam and scam. 4.  Expert uses Text panel to quickly find telling signals; differences between ads in a micro-cluster are highlighted. 5. Finally, the expert confidently labels the meta-cluster for each modus operandi (M.O.), deciding on benign (at-will sex worker), with a small chance of trafficking.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-full-1614-qa",
                        "session_id": "full30",
                        "type": "In Person Q+A",
                        "title": "TrafficVis: Visualizing Organized Activity and Spatio-Temporal Patterns for Detecting Human Trafficking (Q+A)",
                        "contributors": [
                            "Catalina Vajiac"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "v-full-1614",
                        "file_name": "",
                        "time_stamp": "2022-10-19T14:22:00Z",
                        "time_start": "2022-10-19T14:22:00Z",
                        "time_end": "2022-10-19T14:24:00Z",
                        "paper_type": "full",
                        "keywords": "",
                        "has_image": "1",
                        "has_video": "617",
                        "paper_award": "",
                        "image_caption": "Analyzing online escort ads using TrafficVis: we show one meta-cluster, i.e.  micro (text) clusters connected using metadata, on real data. Some text blurred for privacy. 1. Human trafficking domain expert uses Micro-cluster panel to drill down to specific micro-cluster data and associated ads. 2-3. Expert uses Timeline panel and Map panel to investigate metadata, noticing inconsistent posting time and regional geographic spread, ruling out spam and scam. 4.  Expert uses Text panel to quickly find telling signals; differences between ads in a micro-cluster are highlighted. 5. Finally, the expert confidently labels the meta-cluster for each modus operandi (M.O.), deciding on benign (at-will sex worker), with a small chance of trafficking.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-tvcg-9507307-pres",
                        "session_id": "full30",
                        "type": "In Person Presentation",
                        "title": "Outcome-Explorer: A Causality Guided Interactive Visual Interface for Interpretable Algorithmic Decision Making",
                        "contributors": [
                            "Md Naimul Hoque"
                        ],
                        "authors": [
                            "Hoque",
                            "Md Naimul\uff0cMueller",
                            "Klaus"
                        ],
                        "abstract": "The widespread adoption of algorithmic decision-making systems has brought about the necessity to interpret the reasoning behind these decisions. The majority of these systems are complex black box models, and auxiliary models are often used to approximate and then explain their behavior. However, recent research suggests that such explanations are not overly accessible to lay users with no specific expertise in machine learning and this can lead to an incorrect interpretation of the underlying model. In this paper, we show that a predictive and interactive model based on causality is inherently interpretable, does not require any auxiliary model, and allows both expert and non-expert users to understand the model comprehensively. To demonstrate our method we developed Outcome Explorer, a causality guided interactive interface, and evaluated it by conducting think-aloud sessions with three expert users and a user study with 18 non-expert users. All three expert users found our tool to be comprehensive in supporting their explanation needs while the non-expert users were able to understand the inner workings of a model easily.",
                        "uid": "v-tvcg-9507307",
                        "file_name": "",
                        "time_stamp": "2022-10-19T14:24:00Z",
                        "time_start": "2022-10-19T14:24:00Z",
                        "time_end": "2022-10-19T14:34:00Z",
                        "paper_type": "full",
                        "keywords": "Explainable AI, Causality, Visual Analytics, Human-Computer Interaction.",
                        "has_image": "1",
                        "has_video": "507",
                        "paper_award": "",
                        "image_caption": "An image containing the title of the paper, author list, and a screenshot of the interface proposed in the paper.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-tvcg-9507307-qa",
                        "session_id": "full30",
                        "type": "In Person Q+A",
                        "title": "Outcome-Explorer: A Causality Guided Interactive Visual Interface for Interpretable Algorithmic Decision Making (Q+A)",
                        "contributors": [
                            "Md Naimul Hoque"
                        ],
                        "authors": [],
                        "abstract": "The widespread adoption of algorithmic decision-making systems has brought about the necessity to interpret the reasoning behind these decisions. The majority of these systems are complex black box models, and auxiliary models are often used to approximate and then explain their behavior. However, recent research suggests that such explanations are not overly accessible to lay users with no specific expertise in machine learning and this can lead to an incorrect interpretation of the underlying model. In this paper, we show that a predictive and interactive model based on causality is inherently interpretable, does not require any auxiliary model, and allows both expert and non-expert users to understand the model comprehensively. To demonstrate our method we developed Outcome Explorer, a causality guided interactive interface, and evaluated it by conducting think-aloud sessions with three expert users and a user study with 18 non-expert users. All three expert users found our tool to be comprehensive in supporting their explanation needs while the non-expert users were able to understand the inner workings of a model easily.",
                        "uid": "v-tvcg-9507307",
                        "file_name": "",
                        "time_stamp": "2022-10-19T14:34:00Z",
                        "time_start": "2022-10-19T14:34:00Z",
                        "time_end": "2022-10-19T14:36:00Z",
                        "paper_type": "full",
                        "keywords": "Explainable AI, Causality, Visual Analytics, Human-Computer Interaction.",
                        "has_image": "1",
                        "has_video": "507",
                        "paper_award": "",
                        "image_caption": "An image containing the title of the paper, author list, and a screenshot of the interface proposed in the paper.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-full-1400-pres",
                        "session_id": "full30",
                        "type": "Virtual Presentation (pre-recorded)",
                        "title": "MedChemLens: An Interactive Visual Tool to Support Direction Selection in Interdisciplinary Experimental Research of Medicinal Chemistry",
                        "contributors": [
                            "Chuhan Shi"
                        ],
                        "authors": [
                            "Chuhan Shi",
                            "Fei Nie",
                            "Yicheng Hu",
                            "Yige Xu",
                            "Lei Chen",
                            "Xiaojuan Ma",
                            "Qiong Luo"
                        ],
                        "abstract": "",
                        "uid": "v-full-1400",
                        "file_name": "",
                        "time_stamp": "2022-10-19T14:36:00Z",
                        "time_start": "2022-10-19T14:36:00Z",
                        "time_end": "2022-10-19T14:46:00Z",
                        "paper_type": "full",
                        "keywords": "",
                        "has_image": "1",
                        "has_video": "516",
                        "paper_award": "",
                        "image_caption": "MedChemLens: The Drug Target Search view allows users to search drug targets. The Signaling Pathway view presents the signaling pathways of the targets under search. The Overview shows the overall distributions of the existing drug compound research. The Publication Trend view displays the number of publications over time related to the targets. The Detail View consists of the Chemistry panel, which summarizes the drug compounds proposed in chemical publications; the Pharmacology panel, which displays the molecular feature values of the compounds tested in pharmacological assays; and the Clinical Pharmacy panel, which visualizes the clinical trial progress of the compounds.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-full-1400-qa",
                        "session_id": "full30",
                        "type": "Virtual Q+A",
                        "title": "MedChemLens: An Interactive Visual Tool to Support Direction Selection in Interdisciplinary Experimental Research of Medicinal Chemistry (Q+A)",
                        "contributors": [
                            "Chuhan Shi"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "v-full-1400",
                        "file_name": "",
                        "time_stamp": "2022-10-19T14:46:00Z",
                        "time_start": "2022-10-19T14:46:00Z",
                        "time_end": "2022-10-19T14:48:00Z",
                        "paper_type": "full",
                        "keywords": "",
                        "has_image": "1",
                        "has_video": "516",
                        "paper_award": "",
                        "image_caption": "MedChemLens: The Drug Target Search view allows users to search drug targets. The Signaling Pathway view presents the signaling pathways of the targets under search. The Overview shows the overall distributions of the existing drug compound research. The Publication Trend view displays the number of publications over time related to the targets. The Detail View consists of the Chemistry panel, which summarizes the drug compounds proposed in chemical publications; the Pharmacology panel, which displays the molecular feature values of the compounds tested in pharmacological assays; and the Clinical Pharmacy panel, which visualizes the clinical trial progress of the compounds.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-tvcg-9761750-pres",
                        "session_id": "full30",
                        "type": "Virtual Presentation (pre-recorded)",
                        "title": "GestureLens: Visual Analysis of Gestures in Presentation Videos",
                        "contributors": [
                            "Haipeng Zeng"
                        ],
                        "authors": [
                            "Haipeng Zeng",
                            "Xingbo Wang",
                            "Yong Wang",
                            "Aoyu Wu",
                            "Ting Chuen Pong",
                            "Huamin Qu"
                        ],
                        "abstract": "Appropriate gestures can enhance message delivery and audience engagement in both daily communication and public presentations. In this paper, we contribute a visual analytic approach that assists professional public speaking coaches in improving their practice of gesture training through analyzing presentation videos. Manually checking and exploring gesture usage in the presentation videos is often tedious and time-consuming. There lacks an efficient method to help users conduct gesture exploration, which is challenging due to the intrinsically temporal evolution of gestures and their complex correlation to speech content. In this paper, we propose GestureLens, a visual analytics system to facilitate gesture-based and content-based exploration of gesture usage in presentation videos. Specifically, the exploration view enables users to obtain a quick overview of the spatial and temporal distributions of gestures. The dynamic hand movements are firstly aggregated through a heatmap in the gesture space for uncovering spatial patterns, and then decomposed into two mutually perpendicular timelines for revealing temporal patterns. The relation view allows users to explicitly explore the correlation between speech content and gestures by enabling linked analysis and intuitive glyph designs. The video view and dynamic view show the context and overall dynamic movement of the selected gestures, respectively. Two usage scenarios and expert interviews with professional presentation coaches demonstrate the effectiveness and usefulness of GestureLens in facilitating gesture exploration and analysis of presentation videos.",
                        "uid": "v-tvcg-9761750",
                        "file_name": "",
                        "time_stamp": "2022-10-19T14:48:00Z",
                        "time_start": "2022-10-19T14:48:00Z",
                        "time_end": "2022-10-19T14:58:00Z",
                        "paper_type": "full",
                        "keywords": "Gesture, hand movements, presentation video analysis, visual analysis.",
                        "has_image": "1",
                        "has_video": "621",
                        "paper_award": "",
                        "image_caption": "Appropriate gestures can enhance message delivery and audience engagement in both daily communication and public presentations. In this paper, we contribute a visual analytic approach that assists professional public speaking coaches in improving their practice of gesture training through analyzing presentation videos. We propose GestureLens, a visual analytics system to facilitate gesture-based and content-based exploration of gesture usage in presentation videos. Two usage scenarios and expert interviews with professional presentation coaches demonstrate the effectiveness and usefulness of GestureLens in facilitating gesture exploration and analysis of presentation videos.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-tvcg-9761750-qa",
                        "session_id": "full30",
                        "type": "Virtual Q+A",
                        "title": "GestureLens: Visual Analysis of Gestures in Presentation Videos (Q+A)",
                        "contributors": [
                            "Haipeng Zeng"
                        ],
                        "authors": [],
                        "abstract": "Appropriate gestures can enhance message delivery and audience engagement in both daily communication and public presentations. In this paper, we contribute a visual analytic approach that assists professional public speaking coaches in improving their practice of gesture training through analyzing presentation videos. Manually checking and exploring gesture usage in the presentation videos is often tedious and time-consuming. There lacks an efficient method to help users conduct gesture exploration, which is challenging due to the intrinsically temporal evolution of gestures and their complex correlation to speech content. In this paper, we propose GestureLens, a visual analytics system to facilitate gesture-based and content-based exploration of gesture usage in presentation videos. Specifically, the exploration view enables users to obtain a quick overview of the spatial and temporal distributions of gestures. The dynamic hand movements are firstly aggregated through a heatmap in the gesture space for uncovering spatial patterns, and then decomposed into two mutually perpendicular timelines for revealing temporal patterns. The relation view allows users to explicitly explore the correlation between speech content and gestures by enabling linked analysis and intuitive glyph designs. The video view and dynamic view show the context and overall dynamic movement of the selected gestures, respectively. Two usage scenarios and expert interviews with professional presentation coaches demonstrate the effectiveness and usefulness of GestureLens in facilitating gesture exploration and analysis of presentation videos.",
                        "uid": "v-tvcg-9761750",
                        "file_name": "",
                        "time_stamp": "2022-10-19T14:58:00Z",
                        "time_start": "2022-10-19T14:58:00Z",
                        "time_end": "2022-10-19T15:00:00Z",
                        "paper_type": "full",
                        "keywords": "Gesture, hand movements, presentation video analysis, visual analysis.",
                        "has_image": "1",
                        "has_video": "621",
                        "paper_award": "",
                        "image_caption": "Appropriate gestures can enhance message delivery and audience engagement in both daily communication and public presentations. In this paper, we contribute a visual analytic approach that assists professional public speaking coaches in improving their practice of gesture training through analyzing presentation videos. We propose GestureLens, a visual analytics system to facilitate gesture-based and content-based exploration of gesture usage in presentation videos. Two usage scenarios and expert interviews with professional presentation coaches demonstrate the effectiveness and usefulness of GestureLens in facilitating gesture exploration and analysis of presentation videos.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-full-1667-pres",
                        "session_id": "full30",
                        "type": "In Person Presentation",
                        "title": "Visual Concept Programming: A Visual Analytics Approach to Injecting Human Intelligence at Scale",
                        "contributors": [
                            "Wenbin He"
                        ],
                        "authors": [
                            "Md Naimul Hoque",
                            "Wenbin He",
                            "Shekar Arvind Kumar",
                            "Liang Gou",
                            "Liu Ren"
                        ],
                        "abstract": "",
                        "uid": "v-full-1667",
                        "file_name": "",
                        "time_stamp": "2022-10-19T15:00:00Z",
                        "time_start": "2022-10-19T15:00:00Z",
                        "time_end": "2022-10-19T15:10:00Z",
                        "paper_type": "full",
                        "keywords": "",
                        "has_image": "1",
                        "has_video": "586",
                        "paper_award": "",
                        "image_caption": "Visual Concept Programming, the first visual analytics approach of using visual concepts to program image data at scale for improving data quality and model performance. This approach is echoing the current ML research trend of Data-Centric AI by iterating data (e.g., quality or higher-level supervision), not models.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-full-1667-qa",
                        "session_id": "full30",
                        "type": "In Person Q+A",
                        "title": "Visual Concept Programming: A Visual Analytics Approach to Injecting Human Intelligence at Scale (Q+A)",
                        "contributors": [
                            "Wenbin He"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "v-full-1667",
                        "file_name": "",
                        "time_stamp": "2022-10-19T15:10:00Z",
                        "time_start": "2022-10-19T15:10:00Z",
                        "time_end": "2022-10-19T15:12:00Z",
                        "paper_type": "full",
                        "keywords": "",
                        "has_image": "1",
                        "has_video": "586",
                        "paper_award": "",
                        "image_caption": "Visual Concept Programming, the first visual analytics approach of using visual concepts to program image data at scale for improving data quality and model performance. This approach is echoing the current ML research trend of Data-Centric AI by iterating data (e.g., quality or higher-level supervision), not models.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    }
                ]
            },
            {
                "title": "Provenance and Guidance",
                "session_id": "full31",
                "event_prefix": "v-full",
                "track": "ok1",
                "livestream_id": "ok1-thu",
                "session_image": "full31.png",
                "chair": [
                    "Alvitta Ottley"
                ],
                "organizers": [],
                "time_start": "2022-10-20T20:45:00Z",
                "time_end": "2022-10-20T22:00:00Z",
                "discord_category": "",
                "discord_channel": "",
                "discord_channel_id": "",
                "discord_link": "",
                "slido_link": "",
                "youtube_url": "",
                "zoom_meeting": "",
                "zoom_password": "",
                "zoom_link": "",
                "ff_link": "",
                "ff_playlist": "",
                "time_slots": [
                    {
                        "slot_id": "full31-opening",
                        "session_id": "full31",
                        "type": "In Person Other",
                        "title": "Session Opening",
                        "contributors": [
                            "Alvitta Ottley"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "",
                        "file_name": "",
                        "time_stamp": "2022-10-20T20:45:00Z",
                        "time_start": "2022-10-20T20:45:00Z",
                        "time_end": "2022-10-20T20:45:00Z",
                        "paper_type": "",
                        "keywords": "",
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-full-1089-pres",
                        "session_id": "full31",
                        "type": "In Person Presentation",
                        "title": "Provenance Representations and Resulting Strategies in an Exploratory Data Analysis Scenario",
                        "contributors": [
                            "Jeremy E Block"
                        ],
                        "authors": [
                            "Jeremy E Block",
                            "Shaghayegh Esmaeili",
                            "Eric Ragan",
                            "John Goodall",
                            "Gregory David Richardson"
                        ],
                        "abstract": "",
                        "uid": "v-full-1089",
                        "file_name": "",
                        "time_stamp": "2022-10-20T20:45:00Z",
                        "time_start": "2022-10-20T20:45:00Z",
                        "time_end": "2022-10-20T20:55:00Z",
                        "paper_type": "full",
                        "keywords": "",
                        "has_image": "1",
                        "has_video": "539",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-full-1089-qa",
                        "session_id": "full31",
                        "type": "In Person Q+A",
                        "title": "Provenance Representations and Resulting Strategies in an Exploratory Data Analysis Scenario (Q+A)",
                        "contributors": [
                            "Jeremy E Block"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "v-full-1089",
                        "file_name": "",
                        "time_stamp": "2022-10-20T20:55:00Z",
                        "time_start": "2022-10-20T20:55:00Z",
                        "time_end": "2022-10-20T20:57:00Z",
                        "paper_type": "full",
                        "keywords": "",
                        "has_image": "1",
                        "has_video": "539",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-tvcg-9768153-pres",
                        "session_id": "full31",
                        "type": "Virtual Presentation (pre-recorded)",
                        "title": "Understanding How In-Visualization Provenance Can Support Trade-off Analysis",
                        "contributors": [
                            "Mehdi Chakhchoukh"
                        ],
                        "authors": [
                            "Mehdi Chakhchoukh",
                            "Nadia Boukhelifa",
                            "Anastasia Bezerianos"
                        ],
                        "abstract": "In domains such as agronomy or manufacturing, experts need to consider trade-offs when making decisions that involve several, often competing, objectives. Such analysis is complex and may be conducted over long periods of time, making it hard to revisit.In this paper, we consider the use of analytic provenance mechanisms to aid experts recall and keep track of trade-off analysis. Weimplemented VisProm, a web-based trade-off analysis system, that incorporates in-visualization provenance views, designed to helpexperts keep track of trade-offs and their objectives. We used VisProm as a technology probe to understand user needs and explore thepotential role of provenance in this context. Through observation sessions with three groups of experts analyzing their own data, we makethe following contributions. We first, identify eight high-level tasks that experts engaged in during trade-off analysis, such as locating andcharacterizing interest zones in the trade-off space, and show how these tasks can be supported by provenance visualization. Second,we refine findings from previous work on provenance purposes such as recall and reproduce, by identifying specific objects of thesepurposes related to trade-off analysis, such as interest zones, and exploration structure (e.g., exploration of alternatives and branches).Third, we discuss insights on how the identified provenance objects and our designs support these trade-off analysis tasks, both whenrevisiting past analysis and while actively exploring. And finally, we identify new opportunities for provenance-driven trade-off analysis, forexample related to monitoring the coverage of the trade-off space, and tracking alternative trade-off scenario.",
                        "uid": "v-tvcg-9768153",
                        "file_name": "",
                        "time_stamp": "2022-10-20T20:57:00Z",
                        "time_start": "2022-10-20T20:57:00Z",
                        "time_end": "2022-10-20T21:07:00Z",
                        "paper_type": "full",
                        "keywords": "Provenance, visualization, trade-offs, multi-criteria, decision making, qualitative study",
                        "has_image": "1",
                        "has_video": "587",
                        "paper_award": "",
                        "image_caption": "The VisProm technology probe includes several in-visualization provenance views to aid trade-off analysis, such as views of what objectives are maximized and minimized, what trade-offs have been considered, etc. We used it in an observational study with domain experts analyzing their own data, to gain insights into: when and how provenance visualization is used in trade-off analysis, differences in provenance use during a-posteri and active analysis, as well as to identify opportunities for future trade-off provenance visualizations.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-tvcg-9768153-qa",
                        "session_id": "full31",
                        "type": "Virtual Q+A",
                        "title": "Understanding How In-Visualization Provenance Can Support Trade-off Analysis (Q+A)",
                        "contributors": [
                            "Mehdi Chakhchoukh"
                        ],
                        "authors": [],
                        "abstract": "In domains such as agronomy or manufacturing, experts need to consider trade-offs when making decisions that involve several, often competing, objectives. Such analysis is complex and may be conducted over long periods of time, making it hard to revisit.In this paper, we consider the use of analytic provenance mechanisms to aid experts recall and keep track of trade-off analysis. Weimplemented VisProm, a web-based trade-off analysis system, that incorporates in-visualization provenance views, designed to helpexperts keep track of trade-offs and their objectives. We used VisProm as a technology probe to understand user needs and explore thepotential role of provenance in this context. Through observation sessions with three groups of experts analyzing their own data, we makethe following contributions. We first, identify eight high-level tasks that experts engaged in during trade-off analysis, such as locating andcharacterizing interest zones in the trade-off space, and show how these tasks can be supported by provenance visualization. Second,we refine findings from previous work on provenance purposes such as recall and reproduce, by identifying specific objects of thesepurposes related to trade-off analysis, such as interest zones, and exploration structure (e.g., exploration of alternatives and branches).Third, we discuss insights on how the identified provenance objects and our designs support these trade-off analysis tasks, both whenrevisiting past analysis and while actively exploring. And finally, we identify new opportunities for provenance-driven trade-off analysis, forexample related to monitoring the coverage of the trade-off space, and tracking alternative trade-off scenario.",
                        "uid": "v-tvcg-9768153",
                        "file_name": "",
                        "time_stamp": "2022-10-20T21:07:00Z",
                        "time_start": "2022-10-20T21:07:00Z",
                        "time_end": "2022-10-20T21:09:00Z",
                        "paper_type": "full",
                        "keywords": "Provenance, visualization, trade-offs, multi-criteria, decision making, qualitative study",
                        "has_image": "1",
                        "has_video": "587",
                        "paper_award": "",
                        "image_caption": "The VisProm technology probe includes several in-visualization provenance views to aid trade-off analysis, such as views of what objectives are maximized and minimized, what trade-offs have been considered, etc. We used it in an observational study with domain experts analyzing their own data, to gain insights into: when and how provenance visualization is used in trade-off analysis, differences in provenance use during a-posteri and active analysis, as well as to identify opportunities for future trade-off provenance visualizations.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-tvcg-9652041-pres",
                        "session_id": "full31",
                        "type": "Virtual Presentation (pre-recorded)",
                        "title": "Provectories: Embedding-based Analysis of Interaction Provenance Data",
                        "contributors": [
                            "Conny Walchshofer",
                            "Andreas Hinterreiter"
                        ],
                        "authors": [
                            "Conny Walchshofer",
                            "Andreas Hinterreiter",
                            "Kai Xu",
                            "Holger Stitz",
                            "Marc Streit"
                        ],
                        "abstract": "Understanding user behavior patterns and visual analysis strategies is a long-standing challenge. Existing approaches rely largely on time-consuming manual processes such as interviews and the analysis of observational data. While it is technically possible to capture a history of user interactions and application states, it remains difficult to extract and describe analysis strategies based on interaction provenance. In this paper, we propose a novel visual approach to the meta-analysis of interaction provenance. We capture single and multiple user sessions as graphs of high-dimensional application states. Our meta-analysis is based on two different types of two-dimensional embeddings of these high-dimensional states: layouts based on (i) topology and (ii) attribute similarity. We applied these visualization approaches to synthetic and real user provenance data captured in two user studies. From our visualizations, we were able to extract patterns for data types and analytical reasoning strategies.",
                        "uid": "v-tvcg-9652041",
                        "file_name": "",
                        "time_stamp": "2022-10-20T21:09:00Z",
                        "time_start": "2022-10-20T21:09:00Z",
                        "time_end": "2022-10-20T21:19:00Z",
                        "paper_type": "full",
                        "keywords": "Visualization techniques, Information visualization, Visual analytics, Interaction Provenance, Sensemaking",
                        "has_image": "1",
                        "has_video": "555",
                        "paper_award": "",
                        "image_caption": "Cluster-based analysis using t-SNE on the example of theoutlier cluster dataset, performing a multiple-user investigation. Distinct clusters (Cluster A--H) can be observed for outlier selections and superimposed trajectories, which indicates that data points were selected performed in the same a similar sequence by multiple users. The ground truth is indicated in orange",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-tvcg-9652041-qa",
                        "session_id": "full31",
                        "type": "Virtual Q+A",
                        "title": "Provectories: Embedding-based Analysis of Interaction Provenance Data (Q+A)",
                        "contributors": [
                            "Conny Walchshofer",
                            "Andreas Hinterreiter"
                        ],
                        "authors": [],
                        "abstract": "Understanding user behavior patterns and visual analysis strategies is a long-standing challenge. Existing approaches rely largely on time-consuming manual processes such as interviews and the analysis of observational data. While it is technically possible to capture a history of user interactions and application states, it remains difficult to extract and describe analysis strategies based on interaction provenance. In this paper, we propose a novel visual approach to the meta-analysis of interaction provenance. We capture single and multiple user sessions as graphs of high-dimensional application states. Our meta-analysis is based on two different types of two-dimensional embeddings of these high-dimensional states: layouts based on (i) topology and (ii) attribute similarity. We applied these visualization approaches to synthetic and real user provenance data captured in two user studies. From our visualizations, we were able to extract patterns for data types and analytical reasoning strategies.",
                        "uid": "v-tvcg-9652041",
                        "file_name": "",
                        "time_stamp": "2022-10-20T21:19:00Z",
                        "time_start": "2022-10-20T21:19:00Z",
                        "time_end": "2022-10-20T21:21:00Z",
                        "paper_type": "full",
                        "keywords": "Visualization techniques, Information visualization, Visual analytics, Interaction Provenance, Sensemaking",
                        "has_image": "1",
                        "has_video": "555",
                        "paper_award": "",
                        "image_caption": "Cluster-based analysis using t-SNE on the example of theoutlier cluster dataset, performing a multiple-user investigation. Distinct clusters (Cluster A--H) can be observed for outlier selections and superimposed trajectories, which indicates that data points were selected performed in the same a similar sequence by multiple users. The ground truth is indicated in orange",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-full-1003-pres",
                        "session_id": "full31",
                        "type": "Virtual Presentation (pre-recorded)",
                        "title": "Lotse: A Practical Framework for Guidance in Visual Analytics",
                        "contributors": [
                            "Fabian Sperrle"
                        ],
                        "authors": [
                            "Fabian Sperrle",
                            "Davide Ceneda",
                            "Mennatallah El-Assady"
                        ],
                        "abstract": "",
                        "uid": "v-full-1003",
                        "file_name": "",
                        "time_stamp": "2022-10-20T21:21:00Z",
                        "time_start": "2022-10-20T21:21:00Z",
                        "time_end": "2022-10-20T21:31:00Z",
                        "paper_type": "full",
                        "keywords": "",
                        "has_image": "1",
                        "has_video": "511",
                        "paper_award": "",
                        "image_caption": "We provide a library called Lotse that allows specifying guidance strategies in yaml definition files and generates running code from them. Lotse is the first guidance library to use such an approach. It supports the creation of reusable guidance strategies to retrofit existing applications with guidance and fosters the creation of general guidance strategy patterns. We demonstrated its effectiveness through first-use case studies with VA researchers of varying guidance design expertise and find that they are able to effectively and quickly implement guidance with Lotse.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-full-1003-qa",
                        "session_id": "full31",
                        "type": "Virtual Q+A",
                        "title": "Lotse: A Practical Framework for Guidance in Visual Analytics (Q+A)",
                        "contributors": [
                            "Fabian Sperrle"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "v-full-1003",
                        "file_name": "",
                        "time_stamp": "2022-10-20T21:31:00Z",
                        "time_start": "2022-10-20T21:31:00Z",
                        "time_end": "2022-10-20T21:33:00Z",
                        "paper_type": "full",
                        "keywords": "",
                        "has_image": "1",
                        "has_video": "511",
                        "paper_award": "",
                        "image_caption": "We provide a library called Lotse that allows specifying guidance strategies in yaml definition files and generates running code from them. Lotse is the first guidance library to use such an approach. It supports the creation of reusable guidance strategies to retrofit existing applications with guidance and fosters the creation of general guidance strategy patterns. We demonstrated its effectiveness through first-use case studies with VA researchers of varying guidance design expertise and find that they are able to effectively and quickly implement guidance with Lotse.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-full-1142-pres",
                        "session_id": "full31",
                        "type": "In Person Presentation",
                        "title": "Medley: Intent-based Recommendations to Support Dashboard Composition",
                        "contributors": [
                            "Arjun Srinivasan"
                        ],
                        "authors": [
                            "Aditeya Pandey",
                            "Arjun Srinivasan",
                            "Vidya Setlur"
                        ],
                        "abstract": "",
                        "uid": "v-full-1142",
                        "file_name": "",
                        "time_stamp": "2022-10-20T21:33:00Z",
                        "time_start": "2022-10-20T21:33:00Z",
                        "time_end": "2022-10-20T21:43:00Z",
                        "paper_type": "full",
                        "keywords": "",
                        "has_image": "1",
                        "has_video": "704",
                        "paper_award": "",
                        "image_caption": "MEDLEY\u2019s user interface. Users can select data attribute and intents from the input panel in the left. Collection recommendations are shown in the center of the screen. Views and widgets can be added from the recommendations to the dashboard canvas on the right.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-full-1142-qa",
                        "session_id": "full31",
                        "type": "In Person Q+A",
                        "title": "Medley: Intent-based Recommendations to Support Dashboard Composition (Q+A)",
                        "contributors": [
                            "Arjun Srinivasan"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "v-full-1142",
                        "file_name": "",
                        "time_stamp": "2022-10-20T21:43:00Z",
                        "time_start": "2022-10-20T21:43:00Z",
                        "time_end": "2022-10-20T21:45:00Z",
                        "paper_type": "full",
                        "keywords": "",
                        "has_image": "1",
                        "has_video": "704",
                        "paper_award": "",
                        "image_caption": "MEDLEY\u2019s user interface. Users can select data attribute and intents from the input panel in the left. Collection recommendations are shown in the center of the screen. Views and widgets can be added from the recommendations to the dashboard canvas on the right.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-tvcg-9524484-pres",
                        "session_id": "full31",
                        "type": "In Person Presentation",
                        "title": "GEViTRec: Data Reconnaissance Through Recommendation Using a Domain-Specific Visualization Prevalence Design Space",
                        "contributors": [
                            "Tamara Munzner",
                            "Anamaria Crisan"
                        ],
                        "authors": [
                            "Anamaria Crisan",
                            "Shannah Fisher",
                            "Jennifer L. Gardy",
                            "Tamara Munzner"
                        ],
                        "abstract": "Genomic Epidemiology (genEpi) is a branch of public health that uses many different data types including tabular, network, genomic, and geographic, to identify and contain outbreaks of deadly diseases. Due to the volume and variety of data, it is challenging for genEpi domain experts to conduct data reconnaissance; that is, have an overview of the data they have and make assessments toward its quality, completeness, and suitability. We present an algorithm for data reconnaissance through automatic visualization recommendation, GEViTRec. Our approach handles a broad variety of dataset types and automatically generates visually coherent combinations of charts, in contrast to existing systems that primarily focus on singleton visual encodings of tabular datasets. We automatically detect linkages across multiple input datasets by analyzing non-numeric attribute fields, creating a data source graph within which we analyze and rank paths. For each high-ranking path, we specify chart combinations with positional and color alignments between shared fields, using a gradual binding approach to transform initial partial specifications of singleton charts to complete specifications that are aligned and oriented consistently. A novel aspect of our approach is its combination of domain-agnostic elements with domain-specific information that is captured through a domain-specific visualization prevalence design space. Our implementation is applied to both synthetic data and real Ebola outbreak data. We compare GEViTRec\u2019s output to what previous visualization recommendation systems would generate, and to manually crafted visualizations used by practitioners. We conducted formative evaluations with ten genEpi experts to assess the relevance and interpretability of our results.",
                        "uid": "v-tvcg-9524484",
                        "file_name": "",
                        "time_stamp": "2022-10-20T21:45:00Z",
                        "time_start": "2022-10-20T21:45:00Z",
                        "time_end": "2022-10-20T21:55:00Z",
                        "paper_type": "full",
                        "keywords": "Heterogeneous Data, Multiple Coordinated Views, Data Reconnaissance, Bioinformatics.",
                        "has_image": "1",
                        "has_video": "556",
                        "paper_award": "",
                        "image_caption": "We present GEViTRec - an approach for automatically generating visually coherent chart combinations from multiple diverse data sources.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-tvcg-9524484-qa",
                        "session_id": "full31",
                        "type": "In Person Q+A",
                        "title": "GEViTRec: Data Reconnaissance Through Recommendation Using a Domain-Specific Visualization Prevalence Design Space (Q+A)",
                        "contributors": [
                            "Tamara Munzner",
                            "Anamaria Crisan"
                        ],
                        "authors": [],
                        "abstract": "Genomic Epidemiology (genEpi) is a branch of public health that uses many different data types including tabular, network, genomic, and geographic, to identify and contain outbreaks of deadly diseases. Due to the volume and variety of data, it is challenging for genEpi domain experts to conduct data reconnaissance; that is, have an overview of the data they have and make assessments toward its quality, completeness, and suitability. We present an algorithm for data reconnaissance through automatic visualization recommendation, GEViTRec. Our approach handles a broad variety of dataset types and automatically generates visually coherent combinations of charts, in contrast to existing systems that primarily focus on singleton visual encodings of tabular datasets. We automatically detect linkages across multiple input datasets by analyzing non-numeric attribute fields, creating a data source graph within which we analyze and rank paths. For each high-ranking path, we specify chart combinations with positional and color alignments between shared fields, using a gradual binding approach to transform initial partial specifications of singleton charts to complete specifications that are aligned and oriented consistently. A novel aspect of our approach is its combination of domain-agnostic elements with domain-specific information that is captured through a domain-specific visualization prevalence design space. Our implementation is applied to both synthetic data and real Ebola outbreak data. We compare GEViTRec\u2019s output to what previous visualization recommendation systems would generate, and to manually crafted visualizations used by practitioners. We conducted formative evaluations with ten genEpi experts to assess the relevance and interpretability of our results.",
                        "uid": "v-tvcg-9524484",
                        "file_name": "",
                        "time_stamp": "2022-10-20T21:55:00Z",
                        "time_start": "2022-10-20T21:55:00Z",
                        "time_end": "2022-10-20T21:57:00Z",
                        "paper_type": "full",
                        "keywords": "Heterogeneous Data, Multiple Coordinated Views, Data Reconnaissance, Bioinformatics.",
                        "has_image": "1",
                        "has_video": "556",
                        "paper_award": "",
                        "image_caption": "We present GEViTRec - an approach for automatically generating visually coherent chart combinations from multiple diverse data sources.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    }
                ]
            },
            {
                "title": "(Volume) Rendering",
                "session_id": "full32",
                "event_prefix": "v-full",
                "track": "ok6",
                "livestream_id": "ok6-wed",
                "session_image": "full32.png",
                "chair": [
                    "Christoph Garth"
                ],
                "organizers": [],
                "time_start": "2022-10-19T19:00:00Z",
                "time_end": "2022-10-19T20:15:00Z",
                "discord_category": "",
                "discord_channel": "",
                "discord_channel_id": "",
                "discord_link": "",
                "slido_link": "",
                "youtube_url": "",
                "zoom_meeting": "",
                "zoom_password": "",
                "zoom_link": "",
                "ff_link": "",
                "ff_playlist": "",
                "time_slots": [
                    {
                        "slot_id": "full32-opening",
                        "session_id": "full32",
                        "type": "In Person Other",
                        "title": "Session Opening",
                        "contributors": [
                            "Christoph Garth"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "",
                        "file_name": "",
                        "time_stamp": "2022-10-19T19:00:00Z",
                        "time_start": "2022-10-19T19:00:00Z",
                        "time_end": "2022-10-19T19:00:00Z",
                        "paper_type": "",
                        "keywords": "",
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-full-1010-pres",
                        "session_id": "full32",
                        "type": "In Person Presentation",
                        "title": "FoVolNet: Fast Volume Rendering using Foveated Deep Neural Networks",
                        "contributors": [
                            "David Bauer"
                        ],
                        "authors": [
                            "David Bauer",
                            "Qi Wu",
                            "Kwan-Liu Ma"
                        ],
                        "abstract": "",
                        "uid": "v-full-1010",
                        "file_name": "",
                        "time_stamp": "2022-10-19T19:00:00Z",
                        "time_start": "2022-10-19T19:00:00Z",
                        "time_end": "2022-10-19T19:10:00Z",
                        "paper_type": "full",
                        "keywords": "",
                        "has_image": "1",
                        "has_video": "615",
                        "paper_award": "",
                        "image_caption": "Our neural rendering pipeline improves rendering performance while preserving image quality.\nA foveated ray marcher sparsely samples a volume (top left). The full image is then reconstructed using a multi-stage hybrid neural network (lower row). The resulting image quality is very similar to the ground truth image (top right) while providing between two to three times speed-up.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-full-1010-qa",
                        "session_id": "full32",
                        "type": "In Person Q+A",
                        "title": "FoVolNet: Fast Volume Rendering using Foveated Deep Neural Networks (Q+A)",
                        "contributors": [
                            "David Bauer"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "v-full-1010",
                        "file_name": "",
                        "time_stamp": "2022-10-19T19:10:00Z",
                        "time_start": "2022-10-19T19:10:00Z",
                        "time_end": "2022-10-19T19:12:00Z",
                        "paper_type": "full",
                        "keywords": "",
                        "has_image": "1",
                        "has_video": "615",
                        "paper_award": "",
                        "image_caption": "Our neural rendering pipeline improves rendering performance while preserving image quality.\nA foveated ray marcher sparsely samples a volume (top left). The full image is then reconstructed using a multi-stage hybrid neural network (lower row). The resulting image quality is very similar to the ground truth image (top right) while providing between two to three times speed-up.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-full-1418-pres",
                        "session_id": "full32",
                        "type": "Virtual Presentation (pre-recorded)",
                        "title": "GRay: Ray Casting for Visualization and Interactive Data Exploration of Gaussian Mixture Models",
                        "contributors": [
                            "Kai Lawonn"
                        ],
                        "authors": [
                            "Kai Lawonn",
                            "Monique Meuschke",
                            "Pepe Eulzer",
                            "Matthias Mitterreiter",
                            "Joachim Giesen",
                            "Tobias G\u00fcnther"
                        ],
                        "abstract": "",
                        "uid": "v-full-1418",
                        "file_name": "",
                        "time_stamp": "2022-10-19T19:12:00Z",
                        "time_start": "2022-10-19T19:12:00Z",
                        "time_end": "2022-10-19T19:22:00Z",
                        "paper_type": "full",
                        "keywords": "",
                        "has_image": "1",
                        "has_video": "593",
                        "paper_award": "",
                        "image_caption": "We present an exploration framework for Gaussian Mixture Models that combines different visualization techniques. The top row shows raycasting-based visualizations that reveal cluster memberships (MIP view), spatial arrangement of Gaussians (hull view), and new modes (DVR view). To manage complexity, the bottom row contains overview visualizations that allow for the comparison of shapes (circle plot, line plot, PCA plot, and small multiples) and different choices of basis vectors (small multiples).",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-full-1418-qa",
                        "session_id": "full32",
                        "type": "Virtual Q+A",
                        "title": "GRay: Ray Casting for Visualization and Interactive Data Exploration of Gaussian Mixture Models (Q+A)",
                        "contributors": [
                            "Kai Lawonn"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "v-full-1418",
                        "file_name": "",
                        "time_stamp": "2022-10-19T19:22:00Z",
                        "time_start": "2022-10-19T19:22:00Z",
                        "time_end": "2022-10-19T19:24:00Z",
                        "paper_type": "full",
                        "keywords": "",
                        "has_image": "1",
                        "has_video": "593",
                        "paper_award": "",
                        "image_caption": "We present an exploration framework for Gaussian Mixture Models that combines different visualization techniques. The top row shows raycasting-based visualizations that reveal cluster memberships (MIP view), spatial arrangement of Gaussians (hull view), and new modes (DVR view). To manage complexity, the bottom row contains overview visualizations that allow for the comparison of shapes (circle plot, line plot, PCA plot, and small multiples) and different choices of basis vectors (small multiples).",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-full-1339-pres",
                        "session_id": "full32",
                        "type": "In Person Presentation",
                        "title": "Quick Clusters: A GPU-Parallel Partitioning for Efficient Path Tracing of Unstructured Volumetric Grids",
                        "contributors": [
                            "Nate Morrical"
                        ],
                        "authors": [
                            "Nate Morrical",
                            "Alper Sahistan",
                            "Ugur Gudukbay",
                            "Ingo Wald",
                            "Valerio Pascucci"
                        ],
                        "abstract": "",
                        "uid": "v-full-1339",
                        "file_name": "",
                        "time_stamp": "2022-10-19T19:24:00Z",
                        "time_start": "2022-10-19T19:24:00Z",
                        "time_end": "2022-10-19T19:34:00Z",
                        "paper_type": "full",
                        "keywords": "",
                        "has_image": "0",
                        "has_video": "664",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-full-1339-qa",
                        "session_id": "full32",
                        "type": "In Person Q+A",
                        "title": "Quick Clusters: A GPU-Parallel Partitioning for Efficient Path Tracing of Unstructured Volumetric Grids (Q+A)",
                        "contributors": [
                            "Nate Morrical"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "v-full-1339",
                        "file_name": "",
                        "time_stamp": "2022-10-19T19:34:00Z",
                        "time_start": "2022-10-19T19:34:00Z",
                        "time_end": "2022-10-19T19:36:00Z",
                        "paper_type": "full",
                        "keywords": "",
                        "has_image": "0",
                        "has_video": "664",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-tvcg-9806341-pres",
                        "session_id": "full32",
                        "type": "In Person Presentation",
                        "title": "Finding Nano-\u00d6tzi: Cryo-Electron Tomography Visualization Guided by Learned Segmentation",
                        "contributors": [
                            "Ngan Nguyen"
                        ],
                        "authors": [
                            "Ngan Nguyen",
                            "Ciril Bohak",
                            "Dominik Engel",
                            "Peter Mindek",
                            "Ond\u0159ej Strnad",
                            "Peter Wonka",
                            "Sai Li",
                            "Timo Ropinski",
                            "Ivan Viola"
                        ],
                        "abstract": "Cryo-electron tomography (cryo-ET) is a new 3D imaging technique with unprecedented potential for resolving submicron structural details. Existing volume visualization methods, however, are not able to reveal details of interest due to low signal-to-noise ratio. In order to design more powerful transfer functions, we propose leveraging soft segmentation as an explicit component of visualization for noisy volumes. Our technical realization is based on semi-supervised learning, where we combine the advantages of two segmentation algorithms. First, the weak segmentation algorithm provides good results for propagating sparse user-provided labels to other voxels in the same volume and is used to generate dense pseudo-labels. Second, the powerful deep-learning-based segmentation algorithm learns from these pseudo-labels to generalize the segmentation to other unseen volumes, a task that the weak segmentation algorithm fails at completely. The proposed volume visualization uses deep-learning-based segmentation as a component for segmentation-aware transfer function design. Appropriate ramp parameters can be suggested automatically through frequency distribution analysis. Furthermore, our visualization uses gradient-free ambient occlusion shading to further suppress the visual presence of noise, and to give structural detail the desired prominence. The cryo-ET data studied in our technical experiments are based on the highest-quality tilted series of intact SARS-CoV-2 virions. Our technique shows the high impact in target sciences for visual data analysis of very noisy volumes that cannot be visualized with existing techniques.",
                        "uid": "v-tvcg-9806341",
                        "file_name": "",
                        "time_stamp": "2022-10-19T19:36:00Z",
                        "time_start": "2022-10-19T19:36:00Z",
                        "time_end": "2022-10-19T19:46:00Z",
                        "paper_type": "full",
                        "keywords": "Volume Rendering; Computer Graphics Techniques; Machine Learning Techniques; Scalar Field Data; Life Sciences",
                        "has_image": "1",
                        "has_video": "491",
                        "paper_award": "",
                        "image_caption": "The image shows a volume containing several intact SARS-CoV-2 virions acquired using cryo-electron tomography 3D imaging. From left to right: slice of the original data; direct volume rendering of the original data; foreground-background segmentation; color-coded four-class segmented data (background, spikes, membrane, lumen).",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-tvcg-9806341-qa",
                        "session_id": "full32",
                        "type": "In Person Q+A",
                        "title": "Finding Nano-\u00d6tzi: Cryo-Electron Tomography Visualization Guided by Learned Segmentation (Q+A)",
                        "contributors": [
                            "Ngan Nguyen"
                        ],
                        "authors": [],
                        "abstract": "Cryo-electron tomography (cryo-ET) is a new 3D imaging technique with unprecedented potential for resolving submicron structural details. Existing volume visualization methods, however, are not able to reveal details of interest due to low signal-to-noise ratio. In order to design more powerful transfer functions, we propose leveraging soft segmentation as an explicit component of visualization for noisy volumes. Our technical realization is based on semi-supervised learning, where we combine the advantages of two segmentation algorithms. First, the weak segmentation algorithm provides good results for propagating sparse user-provided labels to other voxels in the same volume and is used to generate dense pseudo-labels. Second, the powerful deep-learning-based segmentation algorithm learns from these pseudo-labels to generalize the segmentation to other unseen volumes, a task that the weak segmentation algorithm fails at completely. The proposed volume visualization uses deep-learning-based segmentation as a component for segmentation-aware transfer function design. Appropriate ramp parameters can be suggested automatically through frequency distribution analysis. Furthermore, our visualization uses gradient-free ambient occlusion shading to further suppress the visual presence of noise, and to give structural detail the desired prominence. The cryo-ET data studied in our technical experiments are based on the highest-quality tilted series of intact SARS-CoV-2 virions. Our technique shows the high impact in target sciences for visual data analysis of very noisy volumes that cannot be visualized with existing techniques.",
                        "uid": "v-tvcg-9806341",
                        "file_name": "",
                        "time_stamp": "2022-10-19T19:46:00Z",
                        "time_start": "2022-10-19T19:46:00Z",
                        "time_end": "2022-10-19T19:48:00Z",
                        "paper_type": "full",
                        "keywords": "Volume Rendering; Computer Graphics Techniques; Machine Learning Techniques; Scalar Field Data; Life Sciences",
                        "has_image": "1",
                        "has_video": "491",
                        "paper_award": "",
                        "image_caption": "The image shows a volume containing several intact SARS-CoV-2 virions acquired using cryo-electron tomography 3D imaging. From left to right: slice of the original data; direct volume rendering of the original data; foreground-background segmentation; color-coded four-class segmented data (background, spikes, membrane, lumen).",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-full-1496-pres",
                        "session_id": "full32",
                        "type": "Virtual Presentation (pre-recorded)",
                        "title": "Level Set Restricted Voronoi Decomposition for Large Scale Spatial Statistical Analysis",
                        "contributors": [
                            "Tyson Neuroth"
                        ],
                        "authors": [
                            "Tyson Neuroth",
                            "Martin Rieth",
                            "Myoungkyu Lee",
                            "Konduri Aditya",
                            "Jacqueline Chen",
                            "Kwan-Liu Ma"
                        ],
                        "abstract": "",
                        "uid": "v-full-1496",
                        "file_name": "",
                        "time_stamp": "2022-10-19T19:48:00Z",
                        "time_start": "2022-10-19T19:48:00Z",
                        "time_end": "2022-10-19T19:58:00Z",
                        "paper_type": "full",
                        "keywords": "",
                        "has_image": "1",
                        "has_video": "620",
                        "paper_award": "",
                        "image_caption": "We decompose volume data hierachically based on isobands, connected components, and then restricted centroidal Voronoi tessellation of the connected components. These segments are then summarized with statistics and the data and the summaries are sorted so that each feature is contiguous on disk at each level. This provides an efficient method for out-of-core extraction to support efficient interactive visualization of the large multivariate data.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-full-1496-qa",
                        "session_id": "full32",
                        "type": "Virtual Q+A",
                        "title": "Level Set Restricted Voronoi Decomposition for Large Scale Spatial Statistical Analysis (Q+A)",
                        "contributors": [
                            "Tyson Neuroth"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "v-full-1496",
                        "file_name": "",
                        "time_stamp": "2022-10-19T19:58:00Z",
                        "time_start": "2022-10-19T19:58:00Z",
                        "time_end": "2022-10-19T20:00:00Z",
                        "paper_type": "full",
                        "keywords": "",
                        "has_image": "1",
                        "has_video": "620",
                        "paper_award": "",
                        "image_caption": "We decompose volume data hierachically based on isobands, connected components, and then restricted centroidal Voronoi tessellation of the connected components. These segments are then summarized with statistics and the data and the summaries are sorted so that each feature is contiguous on disk at each level. This provides an efficient method for out-of-core extraction to support efficient interactive visualization of the large multivariate data.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-tvcg-9770381-pres",
                        "session_id": "full32",
                        "type": "In Person Presentation",
                        "title": "Watertight Incremental Heightfield Tessellation",
                        "contributors": [
                            "Daniel Cornel"
                        ],
                        "authors": [
                            "Daniel Cornel; Silvana Zechmeister; Eduard Gr\u00f6ller; J\u00fcrgen Waser"
                        ],
                        "abstract": "In this paper, we propose a method for the interactive visualization of medium-scale dynamic heightfields without visual artifacts. Our data fall into a category too large to be rendered directly at full resolution, but small enough to fit into GPU memory without pre-filtering and data streaming. We present the real-world use case of unfiltered flood simulation data of such medium scale that need to be visualized in real time for scientific purposes. Our solution facilitates compute shaders to maintain a guaranteed watertight triangulation in GPU memory that approximates the interpolated heightfields with view-dependent, continuous levels of detail. In each frame, the triangulation is updated incrementally by iteratively refining the cached result of the previous frame to minimize the computational effort. In particular, we minimize the number of heightfield sampling operations to make adaptive and higher-order interpolations viable options. We impose no restriction on the number of subdivisions and the achievable level of detail to allow for extreme zoom ranges required in geospatial visualization. Our method provides a stable runtime performance and can be executed with a limited time budget. We present a comparison of our method to three state-of-the-art methods, in which our method is competitive to previous non-watertight methods in terms of runtime, while outperforming them in terms of accuracy.",
                        "uid": "v-tvcg-9770381",
                        "file_name": "",
                        "time_stamp": "2022-10-19T20:00:00Z",
                        "time_start": "2022-10-19T20:00:00Z",
                        "time_end": "2022-10-19T20:10:00Z",
                        "paper_type": "full",
                        "keywords": "Visualization techniques and methodologies, heightfield rendering, terrain rendering, level of detail, tessellation",
                        "has_image": "1",
                        "has_video": "796",
                        "paper_award": "",
                        "image_caption": "Illustration of watertight tessellation of terrain and flood simulation heightfields with view-dependent level of detail. The generated triangulation is visible on the left with alternating colors to indicate odd and even numbers of subdivisions of the triangles. The shaded result rendered in real time is visible on the right.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-tvcg-9770381-qa",
                        "session_id": "full32",
                        "type": "In Person Q+A",
                        "title": "Watertight Incremental Heightfield Tessellation (Q+A)",
                        "contributors": [
                            "Daniel Cornel"
                        ],
                        "authors": [],
                        "abstract": "In this paper, we propose a method for the interactive visualization of medium-scale dynamic heightfields without visual artifacts. Our data fall into a category too large to be rendered directly at full resolution, but small enough to fit into GPU memory without pre-filtering and data streaming. We present the real-world use case of unfiltered flood simulation data of such medium scale that need to be visualized in real time for scientific purposes. Our solution facilitates compute shaders to maintain a guaranteed watertight triangulation in GPU memory that approximates the interpolated heightfields with view-dependent, continuous levels of detail. In each frame, the triangulation is updated incrementally by iteratively refining the cached result of the previous frame to minimize the computational effort. In particular, we minimize the number of heightfield sampling operations to make adaptive and higher-order interpolations viable options. We impose no restriction on the number of subdivisions and the achievable level of detail to allow for extreme zoom ranges required in geospatial visualization. Our method provides a stable runtime performance and can be executed with a limited time budget. We present a comparison of our method to three state-of-the-art methods, in which our method is competitive to previous non-watertight methods in terms of runtime, while outperforming them in terms of accuracy.",
                        "uid": "v-tvcg-9770381",
                        "file_name": "",
                        "time_stamp": "2022-10-19T20:10:00Z",
                        "time_start": "2022-10-19T20:10:00Z",
                        "time_end": "2022-10-19T20:12:00Z",
                        "paper_type": "full",
                        "keywords": "Visualization techniques and methodologies, heightfield rendering, terrain rendering, level of detail, tessellation",
                        "has_image": "1",
                        "has_video": "796",
                        "paper_award": "",
                        "image_caption": "Illustration of watertight tessellation of terrain and flood simulation heightfields with view-dependent level of detail. The generated triangulation is visible on the left with alternating colors to indicate odd and even numbers of subdivisions of the triangles. The shaded result rendered in real time is visible on the right.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    }
                ]
            }
        ]
    },
    "v-short": {
        "event": "VIS Short Papers",
        "long_name": "VIS Short Papers",
        "event_type": "Paper Presentations",
        "event_prefix": "v-short",
        "event_description": "",
        "event_url": "",
        "organizers": [],
        "sessions": [
            {
                "title": "Visualization Systems and Graph Visualization",
                "session_id": "short1",
                "event_prefix": "v-short",
                "track": "ok1",
                "livestream_id": "ok1-wed",
                "session_image": "short1.png",
                "chair": [
                    "Katherine E. Isaacs"
                ],
                "organizers": [],
                "time_start": "2022-10-19T19:00:00Z",
                "time_end": "2022-10-19T20:15:00Z",
                "discord_category": "",
                "discord_channel": "",
                "discord_channel_id": "",
                "discord_link": "",
                "slido_link": "",
                "youtube_url": "",
                "zoom_meeting": "",
                "zoom_password": "",
                "zoom_link": "",
                "ff_link": "",
                "ff_playlist": "",
                "time_slots": [
                    {
                        "slot_id": "short1-opening",
                        "session_id": "short1",
                        "type": "In Person Other",
                        "title": "Session Opening",
                        "contributors": [
                            "Katherine E. Isaacs"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "",
                        "file_name": "",
                        "time_stamp": "2022-10-19T19:00:00Z",
                        "time_start": "2022-10-19T19:00:00Z",
                        "time_end": "2022-10-19T19:00:00Z",
                        "paper_type": "",
                        "keywords": "",
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-short-1012-pres",
                        "session_id": "short1",
                        "type": "In Person Presentation",
                        "title": "Facilitating Conversational Interaction in Natural Language Interfaces for Visualization",
                        "contributors": [
                            "Arpit Narechania"
                        ],
                        "authors": [
                            "Rishab Mitra",
                            "Arpit Narechania",
                            "Alex Endert",
                            "John Stasko"
                        ],
                        "abstract": "Natural language (NL) toolkits enable visualization developers, who may not have a background in natural language processing (NLP), to create natural language interfaces (NLIs) for end-users to flexibly specify and interact with visualizations. However, these toolkits currently only support one-off utterances, with minimal capability to facilitate a multi-turn dialog between the user and the system. Developing NLIs with such conversational interaction capabilities remains a challenging task, requiring implementations of low-level NLP techniques to process a new query as an intent to follow-up on an older query. We extend an existing Python-based toolkit, NL4DV, that processes an NL query about a tabular dataset and returns an analytic specification containing data attributes, analytic tasks, and relevant visualizations, modeled as a JSON object. Specifically, NL4DV now enables developers to facilitate multiple simultaneous conversations about a dataset and resolve associated ambiguities, augmenting new conversational information into the output JSON object. We demonstrate these capabilities through three examples: (1) an NLI to learn aspects of the Vega-Lite grammar, (2) a mind mapping application to create free-flowing conversations, and (3) a chatbot to answer questions and resolve ambiguities.",
                        "uid": "v-short-1012",
                        "file_name": "",
                        "time_stamp": "2022-10-19T19:00:00Z",
                        "time_start": "2022-10-19T19:00:00Z",
                        "time_end": "2022-10-19T19:07:00Z",
                        "paper_type": "short",
                        "keywords": "Natural Language Interfaces; Visualization Toolkits; Conversational Interaction",
                        "has_image": "1",
                        "has_video": "372",
                        "paper_award": "",
                        "image_caption": "Natural language (NL) toolkits enable visualization developers, who may not have a background in natural language processing (NLP), to create natural language interfaces (NLIs) for end-users to flexibly specify and interact with visualizations. NL4DV is one such Python-based toolkit that processes an NL query about a tabular dataset and returns an analytic specification containing data attributes, analytic tasks, and relevant visualizations, modeled as a JSON object. We extend NL4DV to enable developers to integrate powerful conversational interaction capabilities, e.g., facilitate a multi-turn dialog between the user and the system instead of one-off utterances. For example, given a dataset on houses, a user asks, \u201cShow mean prices for different home types over the years.\u201d In response, NL4DV recommends a multi-series line-chart visualization. Desiring a bar chart instead, the user asks a follow-up query, \u201cAs a bar chart\u201d and NL4DV outputs a bar chart visualization. Finally, desiring only certain home types, the user asks, \u201cJust show condos and duplexes.\u201d NL4DV outputs a filtered visualization.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-short-1012-qa",
                        "session_id": "short1",
                        "type": "In Person Q+A",
                        "title": "Facilitating Conversational Interaction in Natural Language Interfaces for Visualization (Q+A)",
                        "contributors": [
                            "Arpit Narechania"
                        ],
                        "authors": [],
                        "abstract": "Natural language (NL) toolkits enable visualization developers, who may not have a background in natural language processing (NLP), to create natural language interfaces (NLIs) for end-users to flexibly specify and interact with visualizations. However, these toolkits currently only support one-off utterances, with minimal capability to facilitate a multi-turn dialog between the user and the system. Developing NLIs with such conversational interaction capabilities remains a challenging task, requiring implementations of low-level NLP techniques to process a new query as an intent to follow-up on an older query. We extend an existing Python-based toolkit, NL4DV, that processes an NL query about a tabular dataset and returns an analytic specification containing data attributes, analytic tasks, and relevant visualizations, modeled as a JSON object. Specifically, NL4DV now enables developers to facilitate multiple simultaneous conversations about a dataset and resolve associated ambiguities, augmenting new conversational information into the output JSON object. We demonstrate these capabilities through three examples: (1) an NLI to learn aspects of the Vega-Lite grammar, (2) a mind mapping application to create free-flowing conversations, and (3) a chatbot to answer questions and resolve ambiguities.",
                        "uid": "v-short-1012",
                        "file_name": "",
                        "time_stamp": "2022-10-19T19:07:00Z",
                        "time_start": "2022-10-19T19:07:00Z",
                        "time_end": "2022-10-19T19:09:00Z",
                        "paper_type": "short",
                        "keywords": "Natural Language Interfaces; Visualization Toolkits; Conversational Interaction",
                        "has_image": "1",
                        "has_video": "372",
                        "paper_award": "",
                        "image_caption": "Natural language (NL) toolkits enable visualization developers, who may not have a background in natural language processing (NLP), to create natural language interfaces (NLIs) for end-users to flexibly specify and interact with visualizations. NL4DV is one such Python-based toolkit that processes an NL query about a tabular dataset and returns an analytic specification containing data attributes, analytic tasks, and relevant visualizations, modeled as a JSON object. We extend NL4DV to enable developers to integrate powerful conversational interaction capabilities, e.g., facilitate a multi-turn dialog between the user and the system instead of one-off utterances. For example, given a dataset on houses, a user asks, \u201cShow mean prices for different home types over the years.\u201d In response, NL4DV recommends a multi-series line-chart visualization. Desiring a bar chart instead, the user asks a follow-up query, \u201cAs a bar chart\u201d and NL4DV outputs a bar chart visualization. Finally, desiring only certain home types, the user asks, \u201cJust show condos and duplexes.\u201d NL4DV outputs a filtered visualization.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-short-1044-pres",
                        "session_id": "short1",
                        "type": "In Person Presentation",
                        "title": "VegaFusion: Automatic Server-Side Scaling for Interactive Vega Visualizations",
                        "contributors": [
                            "Nicolas Kruchten"
                        ],
                        "authors": [
                            "Nicolas Kruchten",
                            "Jon Mease",
                            "Dominik Moritz"
                        ],
                        "abstract": "The Vega grammar has been broadly adopted by a growing ecosystem of browser-based visualization tools. However, the reference Vega renderer does not scale well to large datasets (e.g., millions of rows or hundreds of megabytes) because it requires the entire dataset to be loaded into browser memory. We introduce VegaFusion, which brings automatic server-side scaling to the Vega ecosystem. VegaFusion accepts generic Vega specifications and partitions the required computation between the client and an out-of-browser, natively-compiled server-side process. Large datasets can be processed server-side to avoid loading them into the browser and to take advantage of multi-threading, more powerful server hardware and caching. We demonstrate how VegaFusion can be integrated into the existing Vega ecosystem, and show that VegaFusion greatly outperforms the reference implementation. We demonstrate these benefits with VegaFusion running on the same machine as the client as well as on a remote machine.",
                        "uid": "v-short-1044",
                        "file_name": "",
                        "time_stamp": "2022-10-19T19:09:00Z",
                        "time_start": "2022-10-19T19:09:00Z",
                        "time_end": "2022-10-19T19:16:00Z",
                        "paper_type": "short",
                        "keywords": "Human-centered computing\u2014Visualization\u2014Visualization systems and tools\u2014Visualization toolkits; Human-centered computing\u2014Visualization\u2014Visualization application domains\u2014Information visualization;",
                        "has_image": "1",
                        "has_video": "413",
                        "paper_award": "",
                        "image_caption": "A generic Vega specification is automatically partitioned by the VegaFusion Planner into a runtime specification for the VegaFusion Middleware (describing operations on large datasets) and a client specification for Vega (describing the visualization of the output of these operations as well as client-side interactions). The Middleware dynamically responds to interaction signals from Vega by querying an out-of-browser, natively-compiled VegaFusion Runtime instance and relaying the results back to Vega.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-short-1044-qa",
                        "session_id": "short1",
                        "type": "In Person Q+A",
                        "title": "VegaFusion: Automatic Server-Side Scaling for Interactive Vega Visualizations (Q+A)",
                        "contributors": [
                            "Nicolas Kruchten"
                        ],
                        "authors": [],
                        "abstract": "The Vega grammar has been broadly adopted by a growing ecosystem of browser-based visualization tools. However, the reference Vega renderer does not scale well to large datasets (e.g., millions of rows or hundreds of megabytes) because it requires the entire dataset to be loaded into browser memory. We introduce VegaFusion, which brings automatic server-side scaling to the Vega ecosystem. VegaFusion accepts generic Vega specifications and partitions the required computation between the client and an out-of-browser, natively-compiled server-side process. Large datasets can be processed server-side to avoid loading them into the browser and to take advantage of multi-threading, more powerful server hardware and caching. We demonstrate how VegaFusion can be integrated into the existing Vega ecosystem, and show that VegaFusion greatly outperforms the reference implementation. We demonstrate these benefits with VegaFusion running on the same machine as the client as well as on a remote machine.",
                        "uid": "v-short-1044",
                        "file_name": "",
                        "time_stamp": "2022-10-19T19:16:00Z",
                        "time_start": "2022-10-19T19:16:00Z",
                        "time_end": "2022-10-19T19:18:00Z",
                        "paper_type": "short",
                        "keywords": "Human-centered computing\u2014Visualization\u2014Visualization systems and tools\u2014Visualization toolkits; Human-centered computing\u2014Visualization\u2014Visualization application domains\u2014Information visualization;",
                        "has_image": "1",
                        "has_video": "413",
                        "paper_award": "",
                        "image_caption": "A generic Vega specification is automatically partitioned by the VegaFusion Planner into a runtime specification for the VegaFusion Middleware (describing operations on large datasets) and a client specification for Vega (describing the visualization of the output of these operations as well as client-side interactions). The Middleware dynamically responds to interaction signals from Vega by querying an out-of-browser, natively-compiled VegaFusion Runtime instance and relaying the results back to Vega.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-short-1098-pres",
                        "session_id": "short1",
                        "type": "In Person Presentation",
                        "title": "Streamlining Visualization Authoring in D3 Through User-Driven Templates",
                        "contributors": [
                            "Hannah K. Bako"
                        ],
                        "authors": [
                            "Hannah K. Bako",
                            "Alisha Varma",
                            "Anuoluwapo Faboro",
                            "Mahreen Haider",
                            "Favour Nerrise",
                            "Bissaka Kenah",
                            "Leilani Battle"
                        ],
                        "abstract": "D3 is arguably the most popular tool for implementing web-based visualizations. Yet D3 has a steep learning curve that may hinder its adoption and continued use. To simplify the process of programming D3 visualizations, we must first understand the space of implementation practices that D3 users engage in. We present a qualitative analysis of 2500 D3 visualizations and their corresponding implementations. We find that 5 visualization types (Bar Charts, Geomaps, Line Charts, Scatterplots, and Force Directed Graphs) account for 80% of D3 visualizations found in our corpus. While implementation styles vary slightly across designs, the underlying code structure for all visualization types remains the same; presenting an opportunity for code reuse. Using our corpus of D3 examples, we synthesize reusable code templates for eight popular D3 visualization types and share them in our open source repository. Based on our results, we discuss design considerations for leveraging users\u2019 implementation patterns to reduce visualization design effort through design templates and auto-generated code recommendations.",
                        "uid": "v-short-1098",
                        "file_name": "",
                        "time_stamp": "2022-10-19T19:18:00Z",
                        "time_start": "2022-10-19T19:18:00Z",
                        "time_end": "2022-10-19T19:25:00Z",
                        "paper_type": "short",
                        "keywords": "Human-centered computing\u2014Visualization\u2014Visualization systems and tools\u2014Visualization toolkits;",
                        "has_image": "1",
                        "has_video": "478",
                        "paper_award": "",
                        "image_caption": "The image begins with text which reads \"Streamlining visualization in D3 through user-driven templates\" on a red background across the top of the image. Below is a collage made up of examples of bespoke visualizations from our analysis. (A) renders the number of IMDB votes and corresponding ratings of movies in a movies dataset. (B) is a narrative chart of scenes from Star Wars: Episode IV. (C) visualizes a braille clock, (D) is a D3\nrendering of Sierpinski Charlet, and (E) is a rendering of bounding box collisions using D3\u2019s force simulation.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-short-1098-qa",
                        "session_id": "short1",
                        "type": "In Person Q+A",
                        "title": "Streamlining Visualization Authoring in D3 Through User-Driven Templates (Q+A)",
                        "contributors": [
                            "Hannah K. Bako"
                        ],
                        "authors": [],
                        "abstract": "D3 is arguably the most popular tool for implementing web-based visualizations. Yet D3 has a steep learning curve that may hinder its adoption and continued use. To simplify the process of programming D3 visualizations, we must first understand the space of implementation practices that D3 users engage in. We present a qualitative analysis of 2500 D3 visualizations and their corresponding implementations. We find that 5 visualization types (Bar Charts, Geomaps, Line Charts, Scatterplots, and Force Directed Graphs) account for 80% of D3 visualizations found in our corpus. While implementation styles vary slightly across designs, the underlying code structure for all visualization types remains the same; presenting an opportunity for code reuse. Using our corpus of D3 examples, we synthesize reusable code templates for eight popular D3 visualization types and share them in our open source repository. Based on our results, we discuss design considerations for leveraging users\u2019 implementation patterns to reduce visualization design effort through design templates and auto-generated code recommendations.",
                        "uid": "v-short-1098",
                        "file_name": "",
                        "time_stamp": "2022-10-19T19:25:00Z",
                        "time_start": "2022-10-19T19:25:00Z",
                        "time_end": "2022-10-19T19:27:00Z",
                        "paper_type": "short",
                        "keywords": "Human-centered computing\u2014Visualization\u2014Visualization systems and tools\u2014Visualization toolkits;",
                        "has_image": "1",
                        "has_video": "478",
                        "paper_award": "",
                        "image_caption": "The image begins with text which reads \"Streamlining visualization in D3 through user-driven templates\" on a red background across the top of the image. Below is a collage made up of examples of bespoke visualizations from our analysis. (A) renders the number of IMDB votes and corresponding ratings of movies in a movies dataset. (B) is a narrative chart of scenes from Star Wars: Episode IV. (C) visualizes a braille clock, (D) is a D3\nrendering of Sierpinski Charlet, and (E) is a rendering of bounding box collisions using D3\u2019s force simulation.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-short-1021-pres",
                        "session_id": "short1",
                        "type": "In Person Presentation",
                        "title": "Plotly-Resampler: Effective Visual Analytics for Large Time Series",
                        "contributors": [
                            "Jonas Van Der Donckt"
                        ],
                        "authors": [
                            "Jonas Van Der Donckt",
                            "Jeroen Van Der Donckt",
                            "Emiel Deprost",
                            "Sofie Van Hoecke"
                        ],
                        "abstract": "Visual analytics is arguably the most important step in getting acquainted with your data. This is especially the case for time series, as this data type is hard to describe and cannot be fully understood when using for example summary statistics. To realize effective time series visualization, four requirements have to be met; a tool should be (1) interactive, (2) scalable to millions of data points, (3) integrable in conventional data science environments, and (4) highly configurable. We observe that open source Python visualization toolkits empower data scientists in most visual analytics tasks, but lack the combination of scalability and interactivity to realize effective time series visualization. As a means to facilitate these requirements, we created Plotly-Resampler, an open source Python library. Plotly-Resampler is an add-on for Plotly's Python bindings, enhancing line chart scalability on top of an interactive toolkit by aggregating the underlying data depending on the current graph view. Plotly-Resampler is built to be snappy, as the reactivity of a tool qualitatively affects how analysts visually explore and analyze data. A benchmark task highlights how our toolkit scales better than alternatives in terms of number of samples and time series. Additionally, Plotly-Resampler's flexible data aggregation functionality paves the path towards researching novel aggregation techniques. Plotly-Resampler's integrability, together with its configurability, convenience, and high scalability, allows to effectively analyze high-frequency data in your day-to-day Python environment.",
                        "uid": "v-short-1021",
                        "file_name": "",
                        "time_stamp": "2022-10-19T19:27:00Z",
                        "time_start": "2022-10-19T19:27:00Z",
                        "time_end": "2022-10-19T19:34:00Z",
                        "paper_type": "short",
                        "keywords": "Time series, Visual analytics, Python, Dash, Plotly, Open source",
                        "has_image": "1",
                        "has_video": "533",
                        "paper_award": "",
                        "image_caption": "v-short 1021 highlights Plotly-Resampler; an open-source Python toolkit which aims to improve effective visual analytics for large time series. Plotly-Resampler adds scalability to an interactive visualization toolkit (Plotly), by separating the visualization into two components; a front- and a back-end. The front-end shows an aggregation of the raw time series data which is stored in the back-end. Optimized callbacks and aggregation methods enable Plotly-Resampler to interactively visualize large time series. Check out the code at GitHub: github.com/predict-idlab/plotly-resampler",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-short-1021-qa",
                        "session_id": "short1",
                        "type": "In Person Q+A",
                        "title": "Plotly-Resampler: Effective Visual Analytics for Large Time Series (Q+A)",
                        "contributors": [
                            "Jonas Van Der Donckt"
                        ],
                        "authors": [],
                        "abstract": "Visual analytics is arguably the most important step in getting acquainted with your data. This is especially the case for time series, as this data type is hard to describe and cannot be fully understood when using for example summary statistics. To realize effective time series visualization, four requirements have to be met; a tool should be (1) interactive, (2) scalable to millions of data points, (3) integrable in conventional data science environments, and (4) highly configurable. We observe that open source Python visualization toolkits empower data scientists in most visual analytics tasks, but lack the combination of scalability and interactivity to realize effective time series visualization. As a means to facilitate these requirements, we created Plotly-Resampler, an open source Python library. Plotly-Resampler is an add-on for Plotly's Python bindings, enhancing line chart scalability on top of an interactive toolkit by aggregating the underlying data depending on the current graph view. Plotly-Resampler is built to be snappy, as the reactivity of a tool qualitatively affects how analysts visually explore and analyze data. A benchmark task highlights how our toolkit scales better than alternatives in terms of number of samples and time series. Additionally, Plotly-Resampler's flexible data aggregation functionality paves the path towards researching novel aggregation techniques. Plotly-Resampler's integrability, together with its configurability, convenience, and high scalability, allows to effectively analyze high-frequency data in your day-to-day Python environment.",
                        "uid": "v-short-1021",
                        "file_name": "",
                        "time_stamp": "2022-10-19T19:34:00Z",
                        "time_start": "2022-10-19T19:34:00Z",
                        "time_end": "2022-10-19T19:36:00Z",
                        "paper_type": "short",
                        "keywords": "Time series, Visual analytics, Python, Dash, Plotly, Open source",
                        "has_image": "1",
                        "has_video": "533",
                        "paper_award": "",
                        "image_caption": "v-short 1021 highlights Plotly-Resampler; an open-source Python toolkit which aims to improve effective visual analytics for large time series. Plotly-Resampler adds scalability to an interactive visualization toolkit (Plotly), by separating the visualization into two components; a front- and a back-end. The front-end shows an aggregation of the raw time series data which is stored in the back-end. Optimized callbacks and aggregation methods enable Plotly-Resampler to interactively visualize large time series. Check out the code at GitHub: github.com/predict-idlab/plotly-resampler",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-short-1011-pres",
                        "session_id": "short1",
                        "type": "In Person Presentation",
                        "title": "Explaining Website Reliability by Visualizing Hyperlink Connectivity",
                        "contributors": [
                            "Seongmin Lee"
                        ],
                        "authors": [
                            "Seongmin Lee",
                            "Sadia Afroz",
                            "Haekyu Park",
                            "Zijie J. Wang",
                            "Omar Shaikh",
                            "Vibhor Sehgal",
                            "Ankit Peshin",
                            "Duen Horng Chau"
                        ],
                        "abstract": "As the information on the Internet continues growing exponentially, understanding and assessing the reliability of a website is becoming increasingly important. Misinformation has far-ranging repercussions, from sowing mistrust in media to undermining democratic elections. While some research investigates how to alert people to misinformation on the web, much less research has been conducted on explaining how websites engage in spreading false information. To fill the research gap, we present MisVis, a web-based interactive visualization tool that helps users assess a website\u2019s reliability by understanding how it engages in spreading false information on the World Wide Web. MisVis visualizes the hyperlink connectivity of the website and summarizes key characteristics of the Twitter accounts that mention the site. A large-scale user study with 139 participants demonstrates that MisVis facilitates users to assess and understand false information on the web and node-link diagrams can be used to communicate with non-experts. MisVis is available at the public demo link: https://poloclub.github.io/MisVis.",
                        "uid": "v-short-1011",
                        "file_name": "",
                        "time_stamp": "2022-10-19T19:36:00Z",
                        "time_start": "2022-10-19T19:36:00Z",
                        "time_end": "2022-10-19T19:43:00Z",
                        "paper_type": "short",
                        "keywords": "Human-centered computing\u2014Visualization\u2014Visualization systems and tools\u2014Visualization toolkits",
                        "has_image": "1",
                        "has_video": "419",
                        "paper_award": "",
                        "image_caption": "MisVis helps users assess a website\u2019s reliability and understand how the site may be involved in spreading false information by visualizing its hyperlink connectivity and summarizing how it is shared on Twitter. A large-scale user study demonstrates the effectiveness of MisVis.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-short-1011-qa",
                        "session_id": "short1",
                        "type": "In Person Q+A",
                        "title": "Explaining Website Reliability by Visualizing Hyperlink Connectivity (Q+A)",
                        "contributors": [
                            "Seongmin Lee"
                        ],
                        "authors": [],
                        "abstract": "As the information on the Internet continues growing exponentially, understanding and assessing the reliability of a website is becoming increasingly important. Misinformation has far-ranging repercussions, from sowing mistrust in media to undermining democratic elections. While some research investigates how to alert people to misinformation on the web, much less research has been conducted on explaining how websites engage in spreading false information. To fill the research gap, we present MisVis, a web-based interactive visualization tool that helps users assess a website\u2019s reliability by understanding how it engages in spreading false information on the World Wide Web. MisVis visualizes the hyperlink connectivity of the website and summarizes key characteristics of the Twitter accounts that mention the site. A large-scale user study with 139 participants demonstrates that MisVis facilitates users to assess and understand false information on the web and node-link diagrams can be used to communicate with non-experts. MisVis is available at the public demo link: https://poloclub.github.io/MisVis.",
                        "uid": "v-short-1011",
                        "file_name": "",
                        "time_stamp": "2022-10-19T19:43:00Z",
                        "time_start": "2022-10-19T19:43:00Z",
                        "time_end": "2022-10-19T19:45:00Z",
                        "paper_type": "short",
                        "keywords": "Human-centered computing\u2014Visualization\u2014Visualization systems and tools\u2014Visualization toolkits",
                        "has_image": "1",
                        "has_video": "419",
                        "paper_award": "",
                        "image_caption": "MisVis helps users assess a website\u2019s reliability and understand how the site may be involved in spreading false information by visualizing its hyperlink connectivity and summarizing how it is shared on Twitter. A large-scale user study demonstrates the effectiveness of MisVis.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-short-1137-pres",
                        "session_id": "short1",
                        "type": "In Person Presentation",
                        "title": "Paths through Spatial Networks",
                        "contributors": [
                            "Alex Godwin"
                        ],
                        "authors": [
                            "Alex Godwin"
                        ],
                        "abstract": "Spatial networks present unique challenges to understanding topological structure. Each node occupies a location in physical space (e.g., longitude and latitude) and each link may either indicate a fixed and well-described path (e.g., along streets) or a logical connection only. Placing these elements in a map maintains these physical relationships while making it more difficult to identify topological features of interest such as clusters, cliques, and paths. While some systems provide coordinated representations of a spatial network, it is almost universally assumed that the network itself is the sole mechanism for movement across space. In this paper, I present a novel approach for exploring spatial networks by orienting them along a point or path in physical space that provides the guide for parameters in a force-directed layout. By specifying a path across topography, networks can be spatially filtered independently of the topology of the network. Initial case studies indicate promising results for exploring spatial networks in transportation and energy distribution.",
                        "uid": "v-short-1137",
                        "file_name": "",
                        "time_stamp": "2022-10-19T19:45:00Z",
                        "time_start": "2022-10-19T19:45:00Z",
                        "time_end": "2022-10-19T19:52:00Z",
                        "paper_type": "short",
                        "keywords": "Human-centered computing\u2014Visualization\u2014Visualization techniques\u2014Graph Drawing; Human-centered computing\u2014Visualization\u2014Visualization application domains\u2014Geographic visualization",
                        "has_image": "1",
                        "has_video": "391",
                        "paper_award": "",
                        "image_caption": "The power network around Washington, D.C. Using a path to query the network and create parameters for a new force-direct layout, the resulting image reveals two major energy components around the DC area: one to the Northeast and one to the Southwest.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-short-1137-qa",
                        "session_id": "short1",
                        "type": "In Person Q+A",
                        "title": "Paths through Spatial Networks (Q+A)",
                        "contributors": [
                            "Alex Godwin"
                        ],
                        "authors": [],
                        "abstract": "Spatial networks present unique challenges to understanding topological structure. Each node occupies a location in physical space (e.g., longitude and latitude) and each link may either indicate a fixed and well-described path (e.g., along streets) or a logical connection only. Placing these elements in a map maintains these physical relationships while making it more difficult to identify topological features of interest such as clusters, cliques, and paths. While some systems provide coordinated representations of a spatial network, it is almost universally assumed that the network itself is the sole mechanism for movement across space. In this paper, I present a novel approach for exploring spatial networks by orienting them along a point or path in physical space that provides the guide for parameters in a force-directed layout. By specifying a path across topography, networks can be spatially filtered independently of the topology of the network. Initial case studies indicate promising results for exploring spatial networks in transportation and energy distribution.",
                        "uid": "v-short-1137",
                        "file_name": "",
                        "time_stamp": "2022-10-19T19:52:00Z",
                        "time_start": "2022-10-19T19:52:00Z",
                        "time_end": "2022-10-19T19:54:00Z",
                        "paper_type": "short",
                        "keywords": "Human-centered computing\u2014Visualization\u2014Visualization techniques\u2014Graph Drawing; Human-centered computing\u2014Visualization\u2014Visualization application domains\u2014Geographic visualization",
                        "has_image": "1",
                        "has_video": "391",
                        "paper_award": "",
                        "image_caption": "The power network around Washington, D.C. Using a path to query the network and create parameters for a new force-direct layout, the resulting image reveals two major energy components around the DC area: one to the Northeast and one to the Southwest.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-short-1070-pres",
                        "session_id": "short1",
                        "type": "Virtual Presentation (pre-recorded)",
                        "title": "LineCap: Line Charts for Data Visualization Captioning Models",
                        "contributors": [
                            "Anita Mahinpei"
                        ],
                        "authors": [
                            "Anita Mahinpei",
                            "Zona Kostic",
                            "Chris Tanner"
                        ],
                        "abstract": "Data visualization captions help readers understand the purpose of a visualization and are crucial for individuals with visual impairments. The prevalence of poor figure captions and the successful application of deep learning approaches to image captioning motivate the use of similar techniques for automated figure captioning. However, research in this field has been stunted by the lack of suitable datasets. We introduce LineCap, a novel figure captioning dataset of 3,528 figures, and we provide insights from curating this dataset and using end-to-end deep learning models for automated figure captioning.",
                        "uid": "v-short-1070",
                        "file_name": "",
                        "time_stamp": "2022-10-19T19:54:00Z",
                        "time_start": "2022-10-19T19:54:00Z",
                        "time_end": "2022-10-19T20:01:00Z",
                        "paper_type": "short",
                        "keywords": "figure captioning, line charts, deep learning dataset",
                        "has_image": "1",
                        "has_video": "475",
                        "paper_award": "",
                        "image_caption": "Diagram showing a figure captioning system. The system is made of two boxes titled Line Count Prediction Model and Caption Generation Model. Arrows indicate that both boxes are taking a single-lined chart as input. The output of the Line Count Prediction Model is a number. This output is passed to the Caption Generation Model as input. The output of the Caption Generation Model is \"Ylabel decreases at a decreasing rate\".",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-short-1070-qa",
                        "session_id": "short1",
                        "type": "Virtual Q+A",
                        "title": "LineCap: Line Charts for Data Visualization Captioning Models (Q+A)",
                        "contributors": [
                            "Anita Mahinpei"
                        ],
                        "authors": [],
                        "abstract": "Data visualization captions help readers understand the purpose of a visualization and are crucial for individuals with visual impairments. The prevalence of poor figure captions and the successful application of deep learning approaches to image captioning motivate the use of similar techniques for automated figure captioning. However, research in this field has been stunted by the lack of suitable datasets. We introduce LineCap, a novel figure captioning dataset of 3,528 figures, and we provide insights from curating this dataset and using end-to-end deep learning models for automated figure captioning.",
                        "uid": "v-short-1070",
                        "file_name": "",
                        "time_stamp": "2022-10-19T20:01:00Z",
                        "time_start": "2022-10-19T20:01:00Z",
                        "time_end": "2022-10-19T20:03:00Z",
                        "paper_type": "short",
                        "keywords": "figure captioning, line charts, deep learning dataset",
                        "has_image": "1",
                        "has_video": "475",
                        "paper_award": "",
                        "image_caption": "Diagram showing a figure captioning system. The system is made of two boxes titled Line Count Prediction Model and Caption Generation Model. Arrows indicate that both boxes are taking a single-lined chart as input. The output of the Line Count Prediction Model is a number. This output is passed to the Caption Generation Model as input. The output of the Caption Generation Model is \"Ylabel decreases at a decreasing rate\".",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-short-1048-pres",
                        "session_id": "short1",
                        "type": "In Person Presentation",
                        "title": "Intentable: A Mixed-Initiative System for Intent-Based Chart Captioning",
                        "contributors": [
                            "Jiwon Choi"
                        ],
                        "authors": [
                            "Jiwon Choi",
                            "Jaemin Jo"
                        ],
                        "abstract": "We present Intentable, a mixed-initiative caption authoring system that allows the author to steer an automatic caption generation process to reflect their intent, e.g., the finding that the author gained from visualization and thus wants to write a caption for. We first derive a grammar for specifying the intent, i.e., a caption recipe, and build a neural network that generates caption sentences given a recipe. Our quantitative evaluation revealed that our intent-based generation system not only allows the author to engage in the generation process but also produces more fluent captions than the previous end-to-end approaches without user intervention. Finally, we demonstrate the versatility of our system, such as context adaptation, unit conversion, and sentence reordering.",
                        "uid": "v-short-1048",
                        "file_name": "",
                        "time_stamp": "2022-10-19T20:03:00Z",
                        "time_start": "2022-10-19T20:03:00Z",
                        "time_end": "2022-10-19T20:10:00Z",
                        "paper_type": "short",
                        "keywords": "Human-centered computing\u2014Visualization\u2014Visualization systems and tools; Human-centered computing\u2014Human-computer interaction (HCI)\u2014Interactive systems and tools",
                        "has_image": "1",
                        "has_video": "457",
                        "paper_award": "",
                        "image_caption": "Overview of mixed-initative interaction with author and Intentable system.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-short-1048-qa",
                        "session_id": "short1",
                        "type": "In Person Q+A",
                        "title": "Intentable: A Mixed-Initiative System for Intent-Based Chart Captioning (Q+A)",
                        "contributors": [
                            "Jiwon Choi"
                        ],
                        "authors": [],
                        "abstract": "We present Intentable, a mixed-initiative caption authoring system that allows the author to steer an automatic caption generation process to reflect their intent, e.g., the finding that the author gained from visualization and thus wants to write a caption for. We first derive a grammar for specifying the intent, i.e., a caption recipe, and build a neural network that generates caption sentences given a recipe. Our quantitative evaluation revealed that our intent-based generation system not only allows the author to engage in the generation process but also produces more fluent captions than the previous end-to-end approaches without user intervention. Finally, we demonstrate the versatility of our system, such as context adaptation, unit conversion, and sentence reordering.",
                        "uid": "v-short-1048",
                        "file_name": "",
                        "time_stamp": "2022-10-19T20:10:00Z",
                        "time_start": "2022-10-19T20:10:00Z",
                        "time_end": "2022-10-19T20:12:00Z",
                        "paper_type": "short",
                        "keywords": "Human-centered computing\u2014Visualization\u2014Visualization systems and tools; Human-centered computing\u2014Human-computer interaction (HCI)\u2014Interactive systems and tools",
                        "has_image": "1",
                        "has_video": "457",
                        "paper_award": "",
                        "image_caption": "Overview of mixed-initative interaction with author and Intentable system.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    }
                ]
            },
            {
                "title": "Visual Analytics, Decision Support, and Machine Learning",
                "session_id": "short2",
                "event_prefix": "v-short",
                "track": "ok4",
                "livestream_id": "ok4-wed",
                "session_image": "short2.png",
                "chair": [
                    "Matthew Berger"
                ],
                "organizers": [],
                "time_start": "2022-10-19T20:45:00Z",
                "time_end": "2022-10-19T22:00:00Z",
                "discord_category": "",
                "discord_channel": "",
                "discord_channel_id": "",
                "discord_link": "",
                "slido_link": "",
                "youtube_url": "",
                "zoom_meeting": "",
                "zoom_password": "",
                "zoom_link": "",
                "ff_link": "",
                "ff_playlist": "",
                "time_slots": [
                    {
                        "slot_id": "short2-opening",
                        "session_id": "short2",
                        "type": "In Person Other",
                        "title": "Session Opening",
                        "contributors": [
                            "Matthew Berger"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "",
                        "file_name": "",
                        "time_stamp": "2022-10-19T20:45:00Z",
                        "time_start": "2022-10-19T20:45:00Z",
                        "time_end": "2022-10-19T20:45:00Z",
                        "paper_type": "",
                        "keywords": "",
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-short-1004-pres",
                        "session_id": "short2",
                        "type": "Virtual Presentation (pre-recorded)",
                        "title": "Visual Auditor: Interactive Visualization for Detection and Summarization of Model Biases",
                        "contributors": [
                            "David Munechika"
                        ],
                        "authors": [
                            "David Munechika",
                            "Zijie J. Wang",
                            "Jack Reidy",
                            "Josh Rubin",
                            "Krishna Gade",
                            "Krishnaram Kenthapadi",
                            "Duen Horng Chau"
                        ],
                        "abstract": "As machine learning (ML) systems become increasingly widespread, it is necessary to audit these systems for biases prior to their deployment. Recent research has developed algorithms for effectively identifying intersectional bias in the form of interpretable, underperforming subsets (or slices) of the data. However, these solutions and their insights are limited without a tool for visually understanding and interacting with the results of these algorithms. We propose Visual Auditor, an interactive visualization tool for auditing and summarizing model biases. Visual Auditor assists model validation by providing an interpretable overview of intersectional bias (bias that is present when examining populations defined by multiple features), details about relationships between problematic data slices, and a comparison between underperforming and overperforming data slices in a model. Our open-source tool runs directly in both computational notebooks and web browsers, making model auditing accessible and easily integrated into current ML development workflows. An observational user study in collaboration with domain experts at Fiddler AI highlights that our tool can help ML practitioners identify and understand model biases.",
                        "uid": "v-short-1004",
                        "file_name": "",
                        "time_stamp": "2022-10-19T20:45:00Z",
                        "time_start": "2022-10-19T20:45:00Z",
                        "time_end": "2022-10-19T20:52:00Z",
                        "paper_type": "short",
                        "keywords": "Machine Learning, Statistics, Modelling, and Simulation Applications",
                        "has_image": "1",
                        "has_video": "435",
                        "paper_award": "",
                        "image_caption": "Visual Auditor provides an overview of underperforming data slices to show where intersectional biases exist. Here currently displays the Force Layout which shows underperforming data slices as nodes on a grid. The location of each node is determined by the features that define the data slice. Users can view clusters of similar data slices to better understand where intersectional bias might exist in their model. The sidebar contains options for filtering the data and modifying the visualization. Visual Auditor is an open-source tool that easily integrates within existing data science workflows and can be accessed directly within computational notebooks.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-short-1004-qa",
                        "session_id": "short2",
                        "type": "Virtual Q+A",
                        "title": "Visual Auditor: Interactive Visualization for Detection and Summarization of Model Biases (Q+A)",
                        "contributors": [
                            "David Munechika"
                        ],
                        "authors": [],
                        "abstract": "As machine learning (ML) systems become increasingly widespread, it is necessary to audit these systems for biases prior to their deployment. Recent research has developed algorithms for effectively identifying intersectional bias in the form of interpretable, underperforming subsets (or slices) of the data. However, these solutions and their insights are limited without a tool for visually understanding and interacting with the results of these algorithms. We propose Visual Auditor, an interactive visualization tool for auditing and summarizing model biases. Visual Auditor assists model validation by providing an interpretable overview of intersectional bias (bias that is present when examining populations defined by multiple features), details about relationships between problematic data slices, and a comparison between underperforming and overperforming data slices in a model. Our open-source tool runs directly in both computational notebooks and web browsers, making model auditing accessible and easily integrated into current ML development workflows. An observational user study in collaboration with domain experts at Fiddler AI highlights that our tool can help ML practitioners identify and understand model biases.",
                        "uid": "v-short-1004",
                        "file_name": "",
                        "time_stamp": "2022-10-19T20:52:00Z",
                        "time_start": "2022-10-19T20:52:00Z",
                        "time_end": "2022-10-19T20:54:00Z",
                        "paper_type": "short",
                        "keywords": "Machine Learning, Statistics, Modelling, and Simulation Applications",
                        "has_image": "1",
                        "has_video": "435",
                        "paper_award": "",
                        "image_caption": "Visual Auditor provides an overview of underperforming data slices to show where intersectional biases exist. Here currently displays the Force Layout which shows underperforming data slices as nodes on a grid. The location of each node is determined by the features that define the data slice. Users can view clusters of similar data slices to better understand where intersectional bias might exist in their model. The sidebar contains options for filtering the data and modifying the visualization. Visual Auditor is an open-source tool that easily integrates within existing data science workflows and can be accessed directly within computational notebooks.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-short-1076-pres",
                        "session_id": "short2",
                        "type": "In Person Presentation",
                        "title": "RMExplorer: A Visual Analytics Approach to Explore the Performance and the Fairness of Disease Risk Models on Population Subgroups",
                        "contributors": [
                            "Bum Chul Kwon"
                        ],
                        "authors": [
                            "Bum Chul Kwon",
                            "Uri Kartoun",
                            "Shaan Khurshid",
                            "Mikhail Yurochkin",
                            "Subha Maity",
                            "Deanna G Brockman",
                            "Amit V Khera",
                            "Patrick T Ellinor",
                            "Steven A Lubitz",
                            "Kenney Ng"
                        ],
                        "abstract": "Disease risk models can identify high-risk patients and help clinicians provide more personalized care. However, risk models developed on one dataset may not generalize across diverse subpopulations of patients in different datasets and may have unexpected performance. It is challenging for clinical researchers to inspect risk models across different subgroups without any tools. Therefore, we developed an interactive visualization system called RMExplorer (Risk Model Explorer) to enable interactive risk model assessment. Specifically, the system allows users to define subgroups of patients by selecting clinical, demographic, or other characteristics, to explore the performance and fairness of risk models on the subgroups, and to understand the feature contributions to risk scores. To demonstrate the usefulness of the tool, we conduct a case study, where we use RMExplorer to explore three atrial fibrillation risk models by applying them to the UK Biobank dataset of 445,329 individuals. RMExplorer can help researchers to evaluate the performance and biases of risk models on subpopulations of interest in their data.",
                        "uid": "v-short-1076",
                        "file_name": "",
                        "time_stamp": "2022-10-19T20:54:00Z",
                        "time_start": "2022-10-19T20:54:00Z",
                        "time_end": "2022-10-19T21:01:00Z",
                        "paper_type": "short",
                        "keywords": "visual analytics, health informatics, fairness, subgroup analysis, explainability, interpretability, electronic health records",
                        "has_image": "1",
                        "has_video": "465",
                        "paper_award": "",
                        "image_caption": "RMExplorer (i.e., Risk Model Explorer) helps users to investigate the performance and the fairness of disease risk models through interactive visualizations.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-short-1076-qa",
                        "session_id": "short2",
                        "type": "In Person Q+A",
                        "title": "RMExplorer: A Visual Analytics Approach to Explore the Performance and the Fairness of Disease Risk Models on Population Subgroups (Q+A)",
                        "contributors": [
                            "Bum Chul Kwon"
                        ],
                        "authors": [],
                        "abstract": "Disease risk models can identify high-risk patients and help clinicians provide more personalized care. However, risk models developed on one dataset may not generalize across diverse subpopulations of patients in different datasets and may have unexpected performance. It is challenging for clinical researchers to inspect risk models across different subgroups without any tools. Therefore, we developed an interactive visualization system called RMExplorer (Risk Model Explorer) to enable interactive risk model assessment. Specifically, the system allows users to define subgroups of patients by selecting clinical, demographic, or other characteristics, to explore the performance and fairness of risk models on the subgroups, and to understand the feature contributions to risk scores. To demonstrate the usefulness of the tool, we conduct a case study, where we use RMExplorer to explore three atrial fibrillation risk models by applying them to the UK Biobank dataset of 445,329 individuals. RMExplorer can help researchers to evaluate the performance and biases of risk models on subpopulations of interest in their data.",
                        "uid": "v-short-1076",
                        "file_name": "",
                        "time_stamp": "2022-10-19T21:01:00Z",
                        "time_start": "2022-10-19T21:01:00Z",
                        "time_end": "2022-10-19T21:03:00Z",
                        "paper_type": "short",
                        "keywords": "visual analytics, health informatics, fairness, subgroup analysis, explainability, interpretability, electronic health records",
                        "has_image": "1",
                        "has_video": "465",
                        "paper_award": "",
                        "image_caption": "RMExplorer (i.e., Risk Model Explorer) helps users to investigate the performance and the fairness of disease risk models through interactive visualizations.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-short-1105-pres",
                        "session_id": "short2",
                        "type": "Virtual Presentation (pre-recorded)",
                        "title": "Visualizing Rule-based Classifiers for Clinical Risk Prognosis",
                        "contributors": [
                            "Dario Antweiler"
                        ],
                        "authors": [
                            "Dario Antweiler",
                            "Georg Fuchs"
                        ],
                        "abstract": "Deteriorating conditions in hospital patients are a major factor in clinical patient mortality. Currently, timely detection is based on clinical experience, expertise, and attention. However, healthcare trends towards larger patient cohorts, more data, and the desire for better and more personalized care are pushing the existing, simple scoring systems to their limits. Data-driven approaches can extract decision rules from available medical coding data, which offer good interpretability and thus are key for successful adoption in practice. Before deployment, models need to be scrutinized by domain experts to identify errors and check them against existing medical knowledge. We propose a visual analytics system to support healthcare professionals in inspecting and enhancing rule-based classifier through identification of similarities and contradictions, as well as modification of rules. This work was developed iteratively in close collaboration with medical professionals. We discuss how our tool supports the inspection and assessment of rule-based classifiers in the clinical coding domain and propose possible extensions.",
                        "uid": "v-short-1105",
                        "file_name": "",
                        "time_stamp": "2022-10-19T21:03:00Z",
                        "time_start": "2022-10-19T21:03:00Z",
                        "time_end": "2022-10-19T21:10:00Z",
                        "paper_type": "short",
                        "keywords": "Information systems applications, Decision support systems, Data analytics, Human computer interaction (HCI), HCI design and evaluation methods, User studies, Applied computing, Life and medical sciences, Health care information systems",
                        "has_image": "1",
                        "has_video": "275",
                        "paper_award": "",
                        "image_caption": "Overview of the our proposed Visual Analytics system with the goal of analyzing rule-based classifiers for clinical risk prognosis as a first prototype. It consists of the main rule list view containing rule attributes as well as quality metrics, a hierarchical tree view containing ICD and OPS codes and a feature interaction view showcasing how code combination are distributed across a user-selected subset of rules. The work was developed in close collaboration with hospital doctors and the dataset used contains patient records from hospitals in Germany.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-short-1105-qa",
                        "session_id": "short2",
                        "type": "Virtual Q+A",
                        "title": "Visualizing Rule-based Classifiers for Clinical Risk Prognosis (Q+A)",
                        "contributors": [
                            "Dario Antweiler"
                        ],
                        "authors": [],
                        "abstract": "Deteriorating conditions in hospital patients are a major factor in clinical patient mortality. Currently, timely detection is based on clinical experience, expertise, and attention. However, healthcare trends towards larger patient cohorts, more data, and the desire for better and more personalized care are pushing the existing, simple scoring systems to their limits. Data-driven approaches can extract decision rules from available medical coding data, which offer good interpretability and thus are key for successful adoption in practice. Before deployment, models need to be scrutinized by domain experts to identify errors and check them against existing medical knowledge. We propose a visual analytics system to support healthcare professionals in inspecting and enhancing rule-based classifier through identification of similarities and contradictions, as well as modification of rules. This work was developed iteratively in close collaboration with medical professionals. We discuss how our tool supports the inspection and assessment of rule-based classifiers in the clinical coding domain and propose possible extensions.",
                        "uid": "v-short-1105",
                        "file_name": "",
                        "time_stamp": "2022-10-19T21:10:00Z",
                        "time_start": "2022-10-19T21:10:00Z",
                        "time_end": "2022-10-19T21:12:00Z",
                        "paper_type": "short",
                        "keywords": "Information systems applications, Decision support systems, Data analytics, Human computer interaction (HCI), HCI design and evaluation methods, User studies, Applied computing, Life and medical sciences, Health care information systems",
                        "has_image": "1",
                        "has_video": "275",
                        "paper_award": "",
                        "image_caption": "Overview of the our proposed Visual Analytics system with the goal of analyzing rule-based classifiers for clinical risk prognosis as a first prototype. It consists of the main rule list view containing rule attributes as well as quality metrics, a hierarchical tree view containing ICD and OPS codes and a feature interaction view showcasing how code combination are distributed across a user-selected subset of rules. The work was developed in close collaboration with hospital doctors and the dataset used contains patient records from hospitals in Germany.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-short-1006-pres",
                        "session_id": "short2",
                        "type": "In Person Presentation",
                        "title": "TimberTrek: Exploring and Curating Sparse Decision Trees with Interactive Visualization",
                        "contributors": [
                            "Zijie J. Wang"
                        ],
                        "authors": [
                            "Zijie J. Wang",
                            "Chudi Zhong",
                            "Rui Xin",
                            "Takuya Takagi",
                            "Zhi Chen",
                            "Duen Horng Chau",
                            "Cynthia Rudin",
                            "Margo Seltzer"
                        ],
                        "abstract": "Given thousands of equally accurate machine learning (ML) models, how can users choose among them? A recent ML technique enables domain experts and data scientists to generate a complete Rashomon set for sparse decision trees\u2014a huge set of almost-optimal interpretable ML models. To help ML practitioners identify models with desirable properties from this Rashomon set, we develop TimberTrek, the first interactive visualization system that summarizes thousands of sparse decision trees at scale. Two usage scenarios highlight how TimberTrek can empower users to easily explore, compare, and curate models that align with their domain knowledge and values. Our open-source tool runs directly in users' computational notebooks and web browsers, lowering the barrier to creating more responsible ML models. TimberTrek is available at the following public demo link: https://poloclub.github.io/timbertrek.",
                        "uid": "v-short-1006",
                        "file_name": "",
                        "time_stamp": "2022-10-19T21:12:00Z",
                        "time_start": "2022-10-19T21:12:00Z",
                        "time_end": "2022-10-19T21:19:00Z",
                        "paper_type": "short",
                        "keywords": "Machine Learning, Interpretability, Rashomon Set, Decision Trees",
                        "has_image": "1",
                        "has_video": "480",
                        "paper_award": "",
                        "image_caption": "Given thousands of equally accurate machine learning (ML) models, how can users choose among them? A recent ML technique enables domain experts and data scientists to generate a complete Rashomon set for sparse decision trees\u2014a huge set of almost-optimal interpretable ML models. To help ML practitioners identify models with desirable properties from this Rashomon set, we develop TimberTrek, the first interactive visualization system that summarizes thousands of sparse decision trees at scale. Two usage scenarios highlight how TimberTrek can empower users to easily explore, com- pare, and curate models that align with their domain knowledge and values. Our open-source tool runs directly in users\u2019 computational notebooks and web browsers, lowering the barrier to creating more responsible ML models. TimberTrek is available at the following public demo link: https://poloclub.github.io/timbertrek.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-short-1006-qa",
                        "session_id": "short2",
                        "type": "In Person Q+A",
                        "title": "TimberTrek: Exploring and Curating Sparse Decision Trees with Interactive Visualization (Q+A)",
                        "contributors": [
                            "Zijie J. Wang"
                        ],
                        "authors": [],
                        "abstract": "Given thousands of equally accurate machine learning (ML) models, how can users choose among them? A recent ML technique enables domain experts and data scientists to generate a complete Rashomon set for sparse decision trees\u2014a huge set of almost-optimal interpretable ML models. To help ML practitioners identify models with desirable properties from this Rashomon set, we develop TimberTrek, the first interactive visualization system that summarizes thousands of sparse decision trees at scale. Two usage scenarios highlight how TimberTrek can empower users to easily explore, compare, and curate models that align with their domain knowledge and values. Our open-source tool runs directly in users' computational notebooks and web browsers, lowering the barrier to creating more responsible ML models. TimberTrek is available at the following public demo link: https://poloclub.github.io/timbertrek.",
                        "uid": "v-short-1006",
                        "file_name": "",
                        "time_stamp": "2022-10-19T21:19:00Z",
                        "time_start": "2022-10-19T21:19:00Z",
                        "time_end": "2022-10-19T21:21:00Z",
                        "paper_type": "short",
                        "keywords": "Machine Learning, Interpretability, Rashomon Set, Decision Trees",
                        "has_image": "1",
                        "has_video": "480",
                        "paper_award": "",
                        "image_caption": "Given thousands of equally accurate machine learning (ML) models, how can users choose among them? A recent ML technique enables domain experts and data scientists to generate a complete Rashomon set for sparse decision trees\u2014a huge set of almost-optimal interpretable ML models. To help ML practitioners identify models with desirable properties from this Rashomon set, we develop TimberTrek, the first interactive visualization system that summarizes thousands of sparse decision trees at scale. Two usage scenarios highlight how TimberTrek can empower users to easily explore, com- pare, and curate models that align with their domain knowledge and values. Our open-source tool runs directly in users\u2019 computational notebooks and web browsers, lowering the barrier to creating more responsible ML models. TimberTrek is available at the following public demo link: https://poloclub.github.io/timbertrek.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-short-1083-pres",
                        "session_id": "short2",
                        "type": "In Person Presentation",
                        "title": "FairFuse: Interactive Visual Support for Fair Consensus Ranking",
                        "contributors": [
                            "Hilson Shrestha"
                        ],
                        "authors": [
                            "Hilson Shrestha",
                            "Kathleen Cachel",
                            "Mallak Alkhathlan",
                            "Elke A Rundensteiner",
                            "Lane Harrison"
                        ],
                        "abstract": "Fair consensus building combines the preferences of multiple rankers into a single consensus ranking, while ensuring any ranked candidate group defined by a protected attribute (such as race or gender) is not disadvantaged compared to other groups. Manually generating a fair consensus ranking is time-consuming and impractical\u2014 even for a fairly small number of candidates. While algorithmic approaches for auditing and generating fair consensus rankings have been developed recently, these have not been operationalized in interactive systems. To bridge this gap, we introduce FairFuse, a visualization-enabled tool for generating, analyzing, and auditing fair consensus rankings. In developing FairFuse, we construct a data model which includes several base rankings entered by rankers, augmented with measures of group fairness, algorithms for generating consensus rankings with varying degrees of fairness, and other fairness and rank-related capabilities. We design novel visualizations that encode these measures in a parallel-coordinates style rank visualization, with interactive capabilities for generating and exploring fair consensus rankings. We provide case studies in which FairFuse supports a decision-maker in ranking scenarios in which fairness is important. Finally, we discuss emerging challenges for future efforts supporting fairness-oriented rank analysis; including handling intersectionality, defined by multiple protected attributes, and the need for user studies targeting peoples\u2019 perceptions and use of fairness oriented visualization systems. Code and demo videos available at https://osf.io/hd639/.",
                        "uid": "v-short-1083",
                        "file_name": "",
                        "time_stamp": "2022-10-19T21:21:00Z",
                        "time_start": "2022-10-19T21:21:00Z",
                        "time_end": "2022-10-19T21:28:00Z",
                        "paper_type": "short",
                        "keywords": "Fairness, consensus, rank aggregation, visualization",
                        "has_image": "1",
                        "has_video": "446",
                        "paper_award": "",
                        "image_caption": "FairFuse provides multiple views supporting fairness-oriented ranking workflows: A) Consensus Generation View, B) Rank Similarity View, C) Attribute / Protected Attribute Legends, D) Group Fairness View, E) Ranking Exploration View. (Right) Illustrating a fairness-oriented ranking workflow enabled by FairFuse.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-short-1083-qa",
                        "session_id": "short2",
                        "type": "In Person Q+A",
                        "title": "FairFuse: Interactive Visual Support for Fair Consensus Ranking (Q+A)",
                        "contributors": [
                            "Hilson Shrestha"
                        ],
                        "authors": [],
                        "abstract": "Fair consensus building combines the preferences of multiple rankers into a single consensus ranking, while ensuring any ranked candidate group defined by a protected attribute (such as race or gender) is not disadvantaged compared to other groups. Manually generating a fair consensus ranking is time-consuming and impractical\u2014 even for a fairly small number of candidates. While algorithmic approaches for auditing and generating fair consensus rankings have been developed recently, these have not been operationalized in interactive systems. To bridge this gap, we introduce FairFuse, a visualization-enabled tool for generating, analyzing, and auditing fair consensus rankings. In developing FairFuse, we construct a data model which includes several base rankings entered by rankers, augmented with measures of group fairness, algorithms for generating consensus rankings with varying degrees of fairness, and other fairness and rank-related capabilities. We design novel visualizations that encode these measures in a parallel-coordinates style rank visualization, with interactive capabilities for generating and exploring fair consensus rankings. We provide case studies in which FairFuse supports a decision-maker in ranking scenarios in which fairness is important. Finally, we discuss emerging challenges for future efforts supporting fairness-oriented rank analysis; including handling intersectionality, defined by multiple protected attributes, and the need for user studies targeting peoples\u2019 perceptions and use of fairness oriented visualization systems. Code and demo videos available at https://osf.io/hd639/.",
                        "uid": "v-short-1083",
                        "file_name": "",
                        "time_stamp": "2022-10-19T21:28:00Z",
                        "time_start": "2022-10-19T21:28:00Z",
                        "time_end": "2022-10-19T21:30:00Z",
                        "paper_type": "short",
                        "keywords": "Fairness, consensus, rank aggregation, visualization",
                        "has_image": "1",
                        "has_video": "446",
                        "paper_award": "",
                        "image_caption": "FairFuse provides multiple views supporting fairness-oriented ranking workflows: A) Consensus Generation View, B) Rank Similarity View, C) Attribute / Protected Attribute Legends, D) Group Fairness View, E) Ranking Exploration View. (Right) Illustrating a fairness-oriented ranking workflow enabled by FairFuse.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-short-1041-pres",
                        "session_id": "short2",
                        "type": "In Person Presentation",
                        "title": "Guided Data Discovery in Interactive Visualizations via Active Search",
                        "contributors": [
                            "Shayan Monadjemi"
                        ],
                        "authors": [
                            "Shayan Monadjemi",
                            "Sunwoo Ha",
                            "Quan Nguyen",
                            "Henry Chai",
                            "Roman Garnett",
                            "Alvitta Ottley"
                        ],
                        "abstract": "Recent advances in visual analytics have enabled us to learn from user interactions and uncover analytic goals. These innovations set the foundation for actively guiding users during data exploration. Providing such guidance will become more critical as datasets grow in size and complexity, precluding exhaustive investigation. Meanwhile, the machine learning community also struggles with datasets growing in size and complexity, precluding exhaustive labeling. Active learning is a broad family of algorithms developed for actively guiding models during training. We will consider the intersection of these analogous research thrusts. First, we discuss the nuances of matching the choice of an active learning algorithm to the task at hand. This is critical for performance, a fact we demonstrate in a simulation study. We then present results of a user study for the particular task of data discovery guided by an active learning algorithm specifically designed for this task.",
                        "uid": "v-short-1041",
                        "file_name": "",
                        "time_stamp": "2022-10-19T21:30:00Z",
                        "time_start": "2022-10-19T21:30:00Z",
                        "time_end": "2022-10-19T21:37:00Z",
                        "paper_type": "short",
                        "keywords": "visual analytics, empirical studies in visualization, active learning settings",
                        "has_image": "1",
                        "has_video": "503",
                        "paper_award": "",
                        "image_caption": "Recent advances in visual analytics have enabled us to learn from user interactions and uncover analytic goals. These innovations set the foundation for actively guiding users during data exploration which become more critical as datasets grow in size and complexity. We will consider how the active search algorithm can learn from user interactions and guide them during data exploration and discovery.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-short-1041-qa",
                        "session_id": "short2",
                        "type": "In Person Q+A",
                        "title": "Guided Data Discovery in Interactive Visualizations via Active Search (Q+A)",
                        "contributors": [
                            "Shayan Monadjemi"
                        ],
                        "authors": [],
                        "abstract": "Recent advances in visual analytics have enabled us to learn from user interactions and uncover analytic goals. These innovations set the foundation for actively guiding users during data exploration. Providing such guidance will become more critical as datasets grow in size and complexity, precluding exhaustive investigation. Meanwhile, the machine learning community also struggles with datasets growing in size and complexity, precluding exhaustive labeling. Active learning is a broad family of algorithms developed for actively guiding models during training. We will consider the intersection of these analogous research thrusts. First, we discuss the nuances of matching the choice of an active learning algorithm to the task at hand. This is critical for performance, a fact we demonstrate in a simulation study. We then present results of a user study for the particular task of data discovery guided by an active learning algorithm specifically designed for this task.",
                        "uid": "v-short-1041",
                        "file_name": "",
                        "time_stamp": "2022-10-19T21:37:00Z",
                        "time_start": "2022-10-19T21:37:00Z",
                        "time_end": "2022-10-19T21:39:00Z",
                        "paper_type": "short",
                        "keywords": "visual analytics, empirical studies in visualization, active learning settings",
                        "has_image": "1",
                        "has_video": "503",
                        "paper_award": "",
                        "image_caption": "Recent advances in visual analytics have enabled us to learn from user interactions and uncover analytic goals. These innovations set the foundation for actively guiding users during data exploration which become more critical as datasets grow in size and complexity. We will consider how the active search algorithm can learn from user interactions and guide them during data exploration and discovery.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-short-1028-pres",
                        "session_id": "short2",
                        "type": "In Person Presentation",
                        "title": "Parametric Dimension Reduction by Preserving Local Structure",
                        "contributors": [
                            "Yu-Shuen Wang",
                            "Yun Hsuan Lien"
                        ],
                        "authors": [
                            "Chien-Hsun Lai",
                            "Ming-Feng Kuo",
                            "Yun-Hsuan Lien",
                            "Kuan-An Su",
                            "Yu-Shuen Wang"
                        ],
                        "abstract": "We extend a well-known dimension reduction method, t-distributed stochastic neighbor embedding (t-SNE), from non-parametric to parametric by training neural networks. The main advantage of a parametric technique is the generalization of handling new data, which is beneficial for streaming data visualization. While previous parametric methods either require a network pre-training by the restricted Boltzmann machine or intermediate results obtained from the traditional non-parametric t-SNE, we found that recent network training skills can enable a direct optimization for the t-SNE objective function. Accordingly, our method achieves high embedding quality while enjoying generalization. Due to mini-batch network training, our parametric dimension reduction method is highly efficient. For evaluation, we compared our method to several baselines on a variety of datasets. Experiment results demonstrate the feasibility of our method. The source code is available at https://github.com/a07458666/parametric_dr.",
                        "uid": "v-short-1028",
                        "file_name": "",
                        "time_stamp": "2022-10-19T21:39:00Z",
                        "time_start": "2022-10-19T21:39:00Z",
                        "time_end": "2022-10-19T21:46:00Z",
                        "paper_type": "short",
                        "keywords": "Computing methodologies\u2014Dimensionality reduction and manifold learning\u2014; Human-centered computing\u2014Visualization toolkit",
                        "has_image": "1",
                        "has_video": "416",
                        "paper_award": "",
                        "image_caption": "We extend a well-known dimension reduction method, t-distributed stochastic neighbor embedding (t-SNE), from non-parametric to parametric by training neural networks. Our method achieves high embedding quality while enjoying generalization. In addition, our method is highly efficient, thanks to the mini-batch network training.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-short-1028-qa",
                        "session_id": "short2",
                        "type": "In Person Q+A",
                        "title": "Parametric Dimension Reduction by Preserving Local Structure (Q+A)",
                        "contributors": [
                            "Yu-Shuen Wang",
                            "Yun Hsuan Lien"
                        ],
                        "authors": [],
                        "abstract": "We extend a well-known dimension reduction method, t-distributed stochastic neighbor embedding (t-SNE), from non-parametric to parametric by training neural networks. The main advantage of a parametric technique is the generalization of handling new data, which is beneficial for streaming data visualization. While previous parametric methods either require a network pre-training by the restricted Boltzmann machine or intermediate results obtained from the traditional non-parametric t-SNE, we found that recent network training skills can enable a direct optimization for the t-SNE objective function. Accordingly, our method achieves high embedding quality while enjoying generalization. Due to mini-batch network training, our parametric dimension reduction method is highly efficient. For evaluation, we compared our method to several baselines on a variety of datasets. Experiment results demonstrate the feasibility of our method. The source code is available at https://github.com/a07458666/parametric_dr.",
                        "uid": "v-short-1028",
                        "file_name": "",
                        "time_stamp": "2022-10-19T21:46:00Z",
                        "time_start": "2022-10-19T21:46:00Z",
                        "time_end": "2022-10-19T21:48:00Z",
                        "paper_type": "short",
                        "keywords": "Computing methodologies\u2014Dimensionality reduction and manifold learning\u2014; Human-centered computing\u2014Visualization toolkit",
                        "has_image": "1",
                        "has_video": "416",
                        "paper_award": "",
                        "image_caption": "We extend a well-known dimension reduction method, t-distributed stochastic neighbor embedding (t-SNE), from non-parametric to parametric by training neural networks. Our method achieves high embedding quality while enjoying generalization. In addition, our method is highly efficient, thanks to the mini-batch network training.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-short-1047-pres",
                        "session_id": "short2",
                        "type": "In Person Presentation",
                        "title": "Uniform Manifold Approximation with Two-phase Optimization",
                        "contributors": [
                            "Hyeon Jeon"
                        ],
                        "authors": [
                            "Hyeon Jeon",
                            "Hyung-Kwon Ko",
                            "Soohyun Lee",
                            "Jaemin Jo",
                            "Jinwook Seo"
                        ],
                        "abstract": "We introduce Uniform Manifold Approximation with Two-phase Optimization (UMATO), a dimensionality reduction (DR) technique that improves UMAP to capture the global structure of high-dimensional data more accurately. In UMATO, optimization is divided into two phases so that the resulting embeddings can depict the global structure reliably while preserving the local structure with sufficient accuracy. In the first phase, hub points are identified and projected to construct a skeletal layout for the global structure. In the second phase, the remaining points are added to the embedding preserving the regional characteristics of local areas. Through quantitative experiments, we found that UMATO (1) outperformed widely used DR techniques in preserving the global structure while (2) producing competitive accuracy in representing the local structure. We also verified that UMATO is preferable in terms of robustness over diverse initialization methods, numbers of epochs, and subsampling techniques.",
                        "uid": "v-short-1047",
                        "file_name": "",
                        "time_stamp": "2022-10-19T21:48:00Z",
                        "time_start": "2022-10-19T21:48:00Z",
                        "time_end": "2022-10-19T21:55:00Z",
                        "paper_type": "short",
                        "keywords": "Human-centered computing\u2014Visualization\u2014Visualization techniques; Computing methodologies\u2014Machine learning\u2014Machine learning algorithms",
                        "has_image": "1",
                        "has_video": "394",
                        "paper_award": "",
                        "image_caption": "2D embeddings of UMATO and six competitors. Overall, UMATO surpassed other techniques in preserving global structure while showing comparable performance in capturing local structure.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-short-1047-qa",
                        "session_id": "short2",
                        "type": "In Person Q+A",
                        "title": "Uniform Manifold Approximation with Two-phase Optimization (Q+A)",
                        "contributors": [
                            "Hyeon Jeon"
                        ],
                        "authors": [],
                        "abstract": "We introduce Uniform Manifold Approximation with Two-phase Optimization (UMATO), a dimensionality reduction (DR) technique that improves UMAP to capture the global structure of high-dimensional data more accurately. In UMATO, optimization is divided into two phases so that the resulting embeddings can depict the global structure reliably while preserving the local structure with sufficient accuracy. In the first phase, hub points are identified and projected to construct a skeletal layout for the global structure. In the second phase, the remaining points are added to the embedding preserving the regional characteristics of local areas. Through quantitative experiments, we found that UMATO (1) outperformed widely used DR techniques in preserving the global structure while (2) producing competitive accuracy in representing the local structure. We also verified that UMATO is preferable in terms of robustness over diverse initialization methods, numbers of epochs, and subsampling techniques.",
                        "uid": "v-short-1047",
                        "file_name": "",
                        "time_stamp": "2022-10-19T21:55:00Z",
                        "time_start": "2022-10-19T21:55:00Z",
                        "time_end": "2022-10-19T21:57:00Z",
                        "paper_type": "short",
                        "keywords": "Human-centered computing\u2014Visualization\u2014Visualization techniques; Computing methodologies\u2014Machine learning\u2014Machine learning algorithms",
                        "has_image": "1",
                        "has_video": "394",
                        "paper_award": "",
                        "image_caption": "2D embeddings of UMATO and six competitors. Overall, UMATO surpassed other techniques in preserving global structure while showing comparable performance in capturing local structure.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    }
                ]
            },
            {
                "title": "Scientific Visualization, Ensembles, and Accessibility",
                "session_id": "short3",
                "event_prefix": "v-short",
                "track": "ok1",
                "livestream_id": "ok1-thu",
                "session_image": "short3.png",
                "chair": [
                    "Hanqi Guo"
                ],
                "organizers": [],
                "time_start": "2022-10-20T19:00:00Z",
                "time_end": "2022-10-20T20:15:00Z",
                "discord_category": "",
                "discord_channel": "",
                "discord_channel_id": "",
                "discord_link": "",
                "slido_link": "",
                "youtube_url": "",
                "zoom_meeting": "",
                "zoom_password": "",
                "zoom_link": "",
                "ff_link": "",
                "ff_playlist": "",
                "time_slots": [
                    {
                        "slot_id": "short3-opening",
                        "session_id": "short3",
                        "type": "In Person Other",
                        "title": "Session Opening",
                        "contributors": [
                            "Hanqi Guo"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "",
                        "file_name": "",
                        "time_stamp": "2022-10-20T19:00:00Z",
                        "time_start": "2022-10-20T19:00:00Z",
                        "time_end": "2022-10-20T19:00:00Z",
                        "paper_type": "",
                        "keywords": "",
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-short-1072-pres",
                        "session_id": "short3",
                        "type": "In Person Presentation",
                        "title": "Color Coding of Large Value Ranges Applied to Meteorological Data",
                        "contributors": [
                            "Daniel Braun"
                        ],
                        "authors": [
                            "Daniel Braun",
                            "Kerstin Ebell",
                            "Vera Schemann",
                            "Laura Pelchmann",
                            "Susanne Crewell",
                            "Rita Borgo",
                            "Tatiana von Landesberger"
                        ],
                        "abstract": "This paper presents a novel color scheme designed to address the challenge of visualizing data series with large value ranges, where scale transformation provides limited support. We focus on meteorological data, where the presence of large value ranges is common. We apply our approach to meteorological scatterplots, as one of the most common plots used in this domain area. Our approach leverages the numerical representation of mantissa and exponent of the values to guide the design of novel ``nested'' color schemes, able to emphasize differences between magnitudes. Our user study evaluates the new designs, the state of the art color scales and representative color schemes used in the analysis of meteorological data: ColorCrafter, Viridis, and Rainbow. We assess accuracy, time and confidence in the context of discrimination (comparison) and interpretation (reading) tasks. Our proposed color scheme significantly outperforms the others in interpretation tasks, while showing comparable performances in discrimination tasks.",
                        "uid": "v-short-1072",
                        "file_name": "",
                        "time_stamp": "2022-10-20T19:00:00Z",
                        "time_start": "2022-10-20T19:00:00Z",
                        "time_end": "2022-10-20T19:07:00Z",
                        "paper_type": "short",
                        "keywords": "Color Coding\u2014Perception\u2014Large Value Ranges\u2014User study",
                        "has_image": "1",
                        "has_video": "364",
                        "paper_award": "",
                        "image_caption": "Order of Magnitude Colors: A new color coding approach to encode data with large value ranges applied to meteorological cloud data.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-short-1072-qa",
                        "session_id": "short3",
                        "type": "In Person Q+A",
                        "title": "Color Coding of Large Value Ranges Applied to Meteorological Data (Q+A)",
                        "contributors": [
                            "Daniel Braun"
                        ],
                        "authors": [],
                        "abstract": "This paper presents a novel color scheme designed to address the challenge of visualizing data series with large value ranges, where scale transformation provides limited support. We focus on meteorological data, where the presence of large value ranges is common. We apply our approach to meteorological scatterplots, as one of the most common plots used in this domain area. Our approach leverages the numerical representation of mantissa and exponent of the values to guide the design of novel ``nested'' color schemes, able to emphasize differences between magnitudes. Our user study evaluates the new designs, the state of the art color scales and representative color schemes used in the analysis of meteorological data: ColorCrafter, Viridis, and Rainbow. We assess accuracy, time and confidence in the context of discrimination (comparison) and interpretation (reading) tasks. Our proposed color scheme significantly outperforms the others in interpretation tasks, while showing comparable performances in discrimination tasks.",
                        "uid": "v-short-1072",
                        "file_name": "",
                        "time_stamp": "2022-10-20T19:07:00Z",
                        "time_start": "2022-10-20T19:07:00Z",
                        "time_end": "2022-10-20T19:09:00Z",
                        "paper_type": "short",
                        "keywords": "Color Coding\u2014Perception\u2014Large Value Ranges\u2014User study",
                        "has_image": "1",
                        "has_video": "364",
                        "paper_award": "",
                        "image_caption": "Order of Magnitude Colors: A new color coding approach to encode data with large value ranges applied to meteorological cloud data.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-short-1085-pres",
                        "session_id": "short3",
                        "type": "Virtual Presentation (pre-recorded)",
                        "title": "Volume Puzzle: visual analysis of segmented volume data with multivariate attributes",
                        "contributors": [
                            "Marco Agus"
                        ],
                        "authors": [
                            "Marco Agus",
                            "Amal Aboulhassan",
                            "Khaled Ahmed Lutf Al-Thelaya",
                            "Giovanni Pintore",
                            "Enrico Gobbetti",
                            "Corrado Cali'",
                            "Jens Schneider"
                        ],
                        "abstract": "A variety of application domains, including material science, neuroscience, and connectomics, commonly use segmented volume data for explorative visual analysis. In many cases, segmented objects are characterized by multivariate attributes expressing specific geometric or physical features. Objects with similar characteristics, determined by selected attribute configurations, can create peculiar spatial patterns, whose detection and study is of fundamental importance. This task is notoriously difficult, especially when the number of attributes per segment is large. In this work, we propose an interactive framework that combines a state-of-the-art direct volume renderer for categorical volumes with techniques for the analysis of the attribute space and for the automatic creation of 2D transfer function. We show, in particular, how dimensionality reduction, kernel-density estimation, and topological techniques such as Morse analysis combined with scatter and density plots allow the efficient design of two-dimensional color maps that highlight spatial patterns. The capabilities of our framework are demonstrated on synthetic and real-world data from several domains.",
                        "uid": "v-short-1085",
                        "file_name": "",
                        "time_stamp": "2022-10-20T19:09:00Z",
                        "time_start": "2022-10-20T19:09:00Z",
                        "time_end": "2022-10-20T19:16:00Z",
                        "paper_type": "short",
                        "keywords": "Segmented Volumes, Multivariate data, Color mapping, Dimensionality reduction",
                        "has_image": "1",
                        "has_video": "379",
                        "paper_award": "",
                        "image_caption": "Inspired by word search puzzles,\nwe present Volume Puzzle, a framework that allows practitioners\nto interactively and/or automatically\nreveal spatial patterns from segmented volumes with associated multivariate attributes.\nFor speeding up spatial analysis,\nwe propose an algorithm that computes attribute projection\nthrough dimensionality reduction, kernel density estimation, and\ntopological analysis based on the Morse-Smale complex.\nThe framework can be used for explorative analysis in various domains,\nlike material science or neuroscience.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-short-1085-qa",
                        "session_id": "short3",
                        "type": "Virtual Q+A",
                        "title": "Volume Puzzle: visual analysis of segmented volume data with multivariate attributes (Q+A)",
                        "contributors": [
                            "Marco Agus"
                        ],
                        "authors": [],
                        "abstract": "A variety of application domains, including material science, neuroscience, and connectomics, commonly use segmented volume data for explorative visual analysis. In many cases, segmented objects are characterized by multivariate attributes expressing specific geometric or physical features. Objects with similar characteristics, determined by selected attribute configurations, can create peculiar spatial patterns, whose detection and study is of fundamental importance. This task is notoriously difficult, especially when the number of attributes per segment is large. In this work, we propose an interactive framework that combines a state-of-the-art direct volume renderer for categorical volumes with techniques for the analysis of the attribute space and for the automatic creation of 2D transfer function. We show, in particular, how dimensionality reduction, kernel-density estimation, and topological techniques such as Morse analysis combined with scatter and density plots allow the efficient design of two-dimensional color maps that highlight spatial patterns. The capabilities of our framework are demonstrated on synthetic and real-world data from several domains.",
                        "uid": "v-short-1085",
                        "file_name": "",
                        "time_stamp": "2022-10-20T19:16:00Z",
                        "time_start": "2022-10-20T19:16:00Z",
                        "time_end": "2022-10-20T19:18:00Z",
                        "paper_type": "short",
                        "keywords": "Segmented Volumes, Multivariate data, Color mapping, Dimensionality reduction",
                        "has_image": "1",
                        "has_video": "379",
                        "paper_award": "",
                        "image_caption": "Inspired by word search puzzles,\nwe present Volume Puzzle, a framework that allows practitioners\nto interactively and/or automatically\nreveal spatial patterns from segmented volumes with associated multivariate attributes.\nFor speeding up spatial analysis,\nwe propose an algorithm that computes attribute projection\nthrough dimensionality reduction, kernel density estimation, and\ntopological analysis based on the Morse-Smale complex.\nThe framework can be used for explorative analysis in various domains,\nlike material science or neuroscience.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-short-1075-pres",
                        "session_id": "short3",
                        "type": "In Person Presentation",
                        "title": "Droplet-Local Line Integration for Multiphase Flow",
                        "contributors": [
                            "Alexander Straub"
                        ],
                        "authors": [
                            "Alexander Straub",
                            "Sebastian Boblest",
                            "Grzegorz Karch",
                            "Filip Sadlo",
                            "Thomas Ertl"
                        ],
                        "abstract": "Line integration of stream-, streak-, and pathlines is widely used and popular for visualizing single-phase flow. In multiphase flow, i.e., where the fluid consists, e.g., of a liquid and a gaseous phase, these techniques could also provide valuable insights into the internal flow of droplets and ligaments, and thus into their dynamics. However, since such structures tend to act as entities, high translational and rotational velocities often obfuscate their detail. As a remedy, we present a method for deriving a droplet-local velocity field, using a decomposition of the original velocity field removing translational and rotational velocity parts, and adapt path- and streaklines. Generally, the resulting integral lines are thus shorter and less tangled, which simplifies their analysis. We demonstrate and discuss the utility of our approach on droplets in two-phase flow data, and visualize the removed velocity parts employing glyphs for context.",
                        "uid": "v-short-1075",
                        "file_name": "",
                        "time_stamp": "2022-10-20T19:18:00Z",
                        "time_start": "2022-10-20T19:18:00Z",
                        "time_end": "2022-10-20T19:25:00Z",
                        "paper_type": "short",
                        "keywords": "Human-centered computing\u2014Visualization\u2014Visualization application domains\u2014Scientific visualization;",
                        "has_image": "1",
                        "has_video": "431",
                        "paper_award": "",
                        "image_caption": "Comparison between pathlines in the original vector field and droplet-local pathlines.\n(Left:) The original vector field exhibits strong rotation, which obfuscates local details when visualizing pathlines, and hinders the analysis of the internal flow of the droplet.\n(Right:) The droplet-local pathlines show internal flow revealing two rotating regions to either side. Additional arrow glyphs and the rotation axis provide information about the removed translational and rotational velocity components.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-short-1075-qa",
                        "session_id": "short3",
                        "type": "In Person Q+A",
                        "title": "Droplet-Local Line Integration for Multiphase Flow (Q+A)",
                        "contributors": [
                            "Alexander Straub"
                        ],
                        "authors": [],
                        "abstract": "Line integration of stream-, streak-, and pathlines is widely used and popular for visualizing single-phase flow. In multiphase flow, i.e., where the fluid consists, e.g., of a liquid and a gaseous phase, these techniques could also provide valuable insights into the internal flow of droplets and ligaments, and thus into their dynamics. However, since such structures tend to act as entities, high translational and rotational velocities often obfuscate their detail. As a remedy, we present a method for deriving a droplet-local velocity field, using a decomposition of the original velocity field removing translational and rotational velocity parts, and adapt path- and streaklines. Generally, the resulting integral lines are thus shorter and less tangled, which simplifies their analysis. We demonstrate and discuss the utility of our approach on droplets in two-phase flow data, and visualize the removed velocity parts employing glyphs for context.",
                        "uid": "v-short-1075",
                        "file_name": "",
                        "time_stamp": "2022-10-20T19:25:00Z",
                        "time_start": "2022-10-20T19:25:00Z",
                        "time_end": "2022-10-20T19:27:00Z",
                        "paper_type": "short",
                        "keywords": "Human-centered computing\u2014Visualization\u2014Visualization application domains\u2014Scientific visualization;",
                        "has_image": "1",
                        "has_video": "431",
                        "paper_award": "",
                        "image_caption": "Comparison between pathlines in the original vector field and droplet-local pathlines.\n(Left:) The original vector field exhibits strong rotation, which obfuscates local details when visualizing pathlines, and hinders the analysis of the internal flow of the droplet.\n(Right:) The droplet-local pathlines show internal flow revealing two rotating regions to either side. Additional arrow glyphs and the rotation axis provide information about the removed translational and rotational velocity components.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-short-1037-pres",
                        "session_id": "short3",
                        "type": "In Person Presentation",
                        "title": "Efficient Interpolation-based Pathline Tracing with B-spline Curves in Particle Dataset",
                        "contributors": [
                            "Haoyu Li"
                        ],
                        "authors": [
                            "Haoyu Li",
                            "Tianyu Xiong",
                            "Han-Wei Shen"
                        ],
                        "abstract": "Particle tracing through numerical integration is a well-known approach to generating pathlines for visualization. However, for particle simulations, the computation of pathlines is expensive, since the interpolation method is complicated due to the lack of connectivity information. Previous studies utilize the $k$-d tree to reduce the time for neighborhood search. However, the efficiency is still limited by the number of tracing time steps. Therefore, we propose a novel interpolation-based particle tracing method that first represents particle data as B-spline curves and interpolates B-spline control points to reduce the number of interpolation time steps. We demonstrate our approach achieves good tracing accuracy with much less computation time.",
                        "uid": "v-short-1037",
                        "file_name": "",
                        "time_stamp": "2022-10-20T19:27:00Z",
                        "time_start": "2022-10-20T19:27:00Z",
                        "time_end": "2022-10-20T19:34:00Z",
                        "paper_type": "short",
                        "keywords": "Particle Tracing, Pathlines, flow visualization, B-spline",
                        "has_image": "1",
                        "has_video": "402",
                        "paper_award": "",
                        "image_caption": "The workflow for our B-spline curve control point interpolation method for pathline tracing is shown on the left. The particle dataset is represented as B-spline curves and their control points are interpolated to trace new pathlines. The left figures show a comparison of tracing time, accuracy, and quality between our method and the baseline method by Chandler et al. Our approach reduces the computation time while preserving the tracing quality.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-short-1037-qa",
                        "session_id": "short3",
                        "type": "In Person Q+A",
                        "title": "Efficient Interpolation-based Pathline Tracing with B-spline Curves in Particle Dataset (Q+A)",
                        "contributors": [
                            "Haoyu Li"
                        ],
                        "authors": [],
                        "abstract": "Particle tracing through numerical integration is a well-known approach to generating pathlines for visualization. However, for particle simulations, the computation of pathlines is expensive, since the interpolation method is complicated due to the lack of connectivity information. Previous studies utilize the $k$-d tree to reduce the time for neighborhood search. However, the efficiency is still limited by the number of tracing time steps. Therefore, we propose a novel interpolation-based particle tracing method that first represents particle data as B-spline curves and interpolates B-spline control points to reduce the number of interpolation time steps. We demonstrate our approach achieves good tracing accuracy with much less computation time.",
                        "uid": "v-short-1037",
                        "file_name": "",
                        "time_stamp": "2022-10-20T19:34:00Z",
                        "time_start": "2022-10-20T19:34:00Z",
                        "time_end": "2022-10-20T19:36:00Z",
                        "paper_type": "short",
                        "keywords": "Particle Tracing, Pathlines, flow visualization, B-spline",
                        "has_image": "1",
                        "has_video": "402",
                        "paper_award": "",
                        "image_caption": "The workflow for our B-spline curve control point interpolation method for pathline tracing is shown on the left. The particle dataset is represented as B-spline curves and their control points are interpolated to trace new pathlines. The left figures show a comparison of tracing time, accuracy, and quality between our method and the baseline method by Chandler et al. Our approach reduces the computation time while preserving the tracing quality.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-short-1081-pres",
                        "session_id": "short3",
                        "type": "In Person Presentation",
                        "title": "Visualizing Confidence Intervals for Critical Point Probabilities in 2D Scalar Field Ensembles",
                        "contributors": [
                            "Dominik Vietinghoff"
                        ],
                        "authors": [
                            "Dominik Vietinghoff",
                            "Michael B\u00f6ttinger",
                            "Gerik Scheuermann",
                            "Christian Heine"
                        ],
                        "abstract": "An important task in visualization is the extraction and highlighting of dominant features in data to support users in their analysis process. Topological methods are a well-known means of identifying such features in deterministic fields. However, many real-world phenomena studied today are the result of a chaotic system that cannot be fully described by a single simulation. Instead, the variability of such systems is usually captured with ensemble simulations that produce a variety of possible outcomes of the simulated process. The topological analysis of such ensemble data sets and uncertain data, in general, is less well studied. In this work, we present an approach for the computation and visual representation of confidence intervals for the occurrence probabilities of critical points in ensemble data sets. We demonstrate the added value of our approach over existing methods for critical point prediction in uncertain data on a synthetic data set and show its applicability to a data set from climate research.",
                        "uid": "v-short-1081",
                        "file_name": "",
                        "time_stamp": "2022-10-20T19:36:00Z",
                        "time_start": "2022-10-20T19:36:00Z",
                        "time_end": "2022-10-20T19:43:00Z",
                        "paper_type": "short",
                        "keywords": "Uncertainty visualization, scalar topology, critical points, ensemble data, climate data, inferential statistics, glyphs.",
                        "has_image": "1",
                        "has_video": "445",
                        "paper_award": "",
                        "image_caption": "Confidence intervals for critical point occurrence probabilities for two ensembles of synthetic fields with a different number of members. The red, blue, and green thirds of the glyphs show the confidence intervals for maxima, minima, and saddles at the respective grid points. The area of the darker, inner arcs is proportional to the lower bounds of the found confidence intervals, the area of the lighter, outer arcs to their upper bounds. The black line marks the point estimate for the probability of a critical point of each type computed from the input ensemble.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-short-1081-qa",
                        "session_id": "short3",
                        "type": "In Person Q+A",
                        "title": "Visualizing Confidence Intervals for Critical Point Probabilities in 2D Scalar Field Ensembles (Q+A)",
                        "contributors": [
                            "Dominik Vietinghoff"
                        ],
                        "authors": [],
                        "abstract": "An important task in visualization is the extraction and highlighting of dominant features in data to support users in their analysis process. Topological methods are a well-known means of identifying such features in deterministic fields. However, many real-world phenomena studied today are the result of a chaotic system that cannot be fully described by a single simulation. Instead, the variability of such systems is usually captured with ensemble simulations that produce a variety of possible outcomes of the simulated process. The topological analysis of such ensemble data sets and uncertain data, in general, is less well studied. In this work, we present an approach for the computation and visual representation of confidence intervals for the occurrence probabilities of critical points in ensemble data sets. We demonstrate the added value of our approach over existing methods for critical point prediction in uncertain data on a synthetic data set and show its applicability to a data set from climate research.",
                        "uid": "v-short-1081",
                        "file_name": "",
                        "time_stamp": "2022-10-20T19:43:00Z",
                        "time_start": "2022-10-20T19:43:00Z",
                        "time_end": "2022-10-20T19:45:00Z",
                        "paper_type": "short",
                        "keywords": "Uncertainty visualization, scalar topology, critical points, ensemble data, climate data, inferential statistics, glyphs.",
                        "has_image": "1",
                        "has_video": "445",
                        "paper_award": "",
                        "image_caption": "Confidence intervals for critical point occurrence probabilities for two ensembles of synthetic fields with a different number of members. The red, blue, and green thirds of the glyphs show the confidence intervals for maxima, minima, and saddles at the respective grid points. The area of the darker, inner arcs is proportional to the lower bounds of the found confidence intervals, the area of the lighter, outer arcs to their upper bounds. The black line marks the point estimate for the probability of a critical point of each type computed from the input ensemble.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-short-1103-pres",
                        "session_id": "short3",
                        "type": "In Person Presentation",
                        "title": "ASEVis: Visual Exploration of Active System Ensembles to Define Characteristic Measures",
                        "contributors": [
                            "Marina Evers"
                        ],
                        "authors": [
                            "Marina Evers",
                            "Raphael Wittkowski",
                            "Lars Linsen"
                        ],
                        "abstract": "Simulation ensembles are a common tool in physics for understanding how a model outcome depends on input parameters. We analyze an active particle system, where each particle can use energy from its surroundings to propel itself. A multi-dimensional feature vector containing all particles' motion information can describe the whole system at each time step. The system's behavior strongly depends on input parameters like the propulsion mechanism of the particles. To understand how the time-varying behavior depends on the input parameters, it is necessary to introduce new measures to quantify the difference of the dynamics of the ensemble members. We propose a tool that supports the interactive visual analysis of time-varying feature-vector ensembles. A core component of our tool allows for the interactive definition and refinement of new measures that can then be used to understand the system's behavior and compare the ensemble members. Different visualizations support the user in finding a characteristic measure for the system. By visualizing the user-defined measure, the user can then investigate the parameter dependencies and gain insights into the relationship between input parameters and simulation output.",
                        "uid": "v-short-1103",
                        "file_name": "",
                        "time_stamp": "2022-10-20T19:45:00Z",
                        "time_start": "2022-10-20T19:45:00Z",
                        "time_end": "2022-10-20T19:52:00Z",
                        "paper_type": "short",
                        "keywords": "Physical & Environmental Sciences, Engineering, Mathematics ; Comparison and Similarity ; Coordinated and Multiple Views ; Application Motivated Visualization ; Task Abstractions & Application Domains ; Temporal Data",
                        "has_image": "1",
                        "has_video": "402",
                        "paper_award": "",
                        "image_caption": "The interactive analysis tool ASEVis: A programming interface (a) allows for the definition of time-dependent measures as well as aggregations. Aggregated measures are shown in a heatmap (b) while the aggregation over time is visualized in the timeplot (d). Detail visualizations for single ensemble members include animations (c), a line plot, and a scatter plot matrix.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-short-1103-qa",
                        "session_id": "short3",
                        "type": "In Person Q+A",
                        "title": "ASEVis: Visual Exploration of Active System Ensembles to Define Characteristic Measures (Q+A)",
                        "contributors": [
                            "Marina Evers"
                        ],
                        "authors": [],
                        "abstract": "Simulation ensembles are a common tool in physics for understanding how a model outcome depends on input parameters. We analyze an active particle system, where each particle can use energy from its surroundings to propel itself. A multi-dimensional feature vector containing all particles' motion information can describe the whole system at each time step. The system's behavior strongly depends on input parameters like the propulsion mechanism of the particles. To understand how the time-varying behavior depends on the input parameters, it is necessary to introduce new measures to quantify the difference of the dynamics of the ensemble members. We propose a tool that supports the interactive visual analysis of time-varying feature-vector ensembles. A core component of our tool allows for the interactive definition and refinement of new measures that can then be used to understand the system's behavior and compare the ensemble members. Different visualizations support the user in finding a characteristic measure for the system. By visualizing the user-defined measure, the user can then investigate the parameter dependencies and gain insights into the relationship between input parameters and simulation output.",
                        "uid": "v-short-1103",
                        "file_name": "",
                        "time_stamp": "2022-10-20T19:52:00Z",
                        "time_start": "2022-10-20T19:52:00Z",
                        "time_end": "2022-10-20T19:54:00Z",
                        "paper_type": "short",
                        "keywords": "Physical & Environmental Sciences, Engineering, Mathematics ; Comparison and Similarity ; Coordinated and Multiple Views ; Application Motivated Visualization ; Task Abstractions & Application Domains ; Temporal Data",
                        "has_image": "1",
                        "has_video": "402",
                        "paper_award": "",
                        "image_caption": "The interactive analysis tool ASEVis: A programming interface (a) allows for the definition of time-dependent measures as well as aggregations. Aggregated measures are shown in a heatmap (b) while the aggregation over time is visualized in the timeplot (d). Detail visualizations for single ensemble members include animations (c), a line plot, and a scatter plot matrix.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-short-1110-pres",
                        "session_id": "short3",
                        "type": "In Person Presentation",
                        "title": "Accelerated Probabilistic Marching Cubes by Deep Learning for Time-Varying Scalar Ensembles",
                        "contributors": [
                            "Mengjiao Han"
                        ],
                        "authors": [
                            "Mengjiao Han",
                            "Tushar M. Athawale",
                            "David Pugmire",
                            "Chris R. Johnson"
                        ],
                        "abstract": "Visualizing the uncertainty of ensemble simulations is challenging due to the large size and multivariate and temporal features of ensemble data sets. One popular approach to studying the uncertainty of ensembles is analyzing the positional uncertainty of the level sets. Probabilistic marching cubes is a technique that performs Monte Carlo sampling of multivariate Gaussian noise distributions for positional uncertainty visualization of level sets. However, the technique suffers from high computational time, making interactive visualization and analysis impossible to achieve. This paper introduces a deep-learning-based approach to learning the level-set uncertainty for two-dimensional ensemble data with a multivariate Gaussian noise assumption. We train the model using the first few time steps from time-varying ensemble data in our workflow. We demonstrate that our trained model accurately infers uncertainty in level sets for new time steps and is up to 170X faster than that of the original probabilistic model with serial computation and 10X faster than that of the original parallel computation.",
                        "uid": "v-short-1110",
                        "file_name": "",
                        "time_stamp": "2022-10-20T19:54:00Z",
                        "time_start": "2022-10-20T19:54:00Z",
                        "time_end": "2022-10-20T20:01:00Z",
                        "paper_type": "short",
                        "keywords": "Human-centered computing\u2014Visualization\u2014Visualization application domains\u2014Scientific visualization; Computing methodologies\u2014Machine learning\u2014Machine learning approaches\u2014Neural networks",
                        "has_image": "1",
                        "has_video": "418",
                        "paper_award": "",
                        "image_caption": "Visualizations of the level-crossing probability for isovalue 0.1 in the Red Sea data set using our proposed neural\nnetwork. Image (left) shows the level-crossing probabilities calculated using the original probabilistic marching cubes algorithm.\nImage (right) shows the result computed by our trained model. The zoomed-in views are displayed in the top right. Our method can provide a visually identical result and is 10X faster than the original probabilistic marching cubes algorithm with parallel computing.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-short-1110-qa",
                        "session_id": "short3",
                        "type": "In Person Q+A",
                        "title": "Accelerated Probabilistic Marching Cubes by Deep Learning for Time-Varying Scalar Ensembles (Q+A)",
                        "contributors": [
                            "Mengjiao Han"
                        ],
                        "authors": [],
                        "abstract": "Visualizing the uncertainty of ensemble simulations is challenging due to the large size and multivariate and temporal features of ensemble data sets. One popular approach to studying the uncertainty of ensembles is analyzing the positional uncertainty of the level sets. Probabilistic marching cubes is a technique that performs Monte Carlo sampling of multivariate Gaussian noise distributions for positional uncertainty visualization of level sets. However, the technique suffers from high computational time, making interactive visualization and analysis impossible to achieve. This paper introduces a deep-learning-based approach to learning the level-set uncertainty for two-dimensional ensemble data with a multivariate Gaussian noise assumption. We train the model using the first few time steps from time-varying ensemble data in our workflow. We demonstrate that our trained model accurately infers uncertainty in level sets for new time steps and is up to 170X faster than that of the original probabilistic model with serial computation and 10X faster than that of the original parallel computation.",
                        "uid": "v-short-1110",
                        "file_name": "",
                        "time_stamp": "2022-10-20T20:01:00Z",
                        "time_start": "2022-10-20T20:01:00Z",
                        "time_end": "2022-10-20T20:03:00Z",
                        "paper_type": "short",
                        "keywords": "Human-centered computing\u2014Visualization\u2014Visualization application domains\u2014Scientific visualization; Computing methodologies\u2014Machine learning\u2014Machine learning approaches\u2014Neural networks",
                        "has_image": "1",
                        "has_video": "418",
                        "paper_award": "",
                        "image_caption": "Visualizations of the level-crossing probability for isovalue 0.1 in the Red Sea data set using our proposed neural\nnetwork. Image (left) shows the level-crossing probabilities calculated using the original probabilistic marching cubes algorithm.\nImage (right) shows the result computed by our trained model. The zoomed-in views are displayed in the top right. Our method can provide a visually identical result and is 10X faster than the original probabilistic marching cubes algorithm with parallel computing.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-short-1055-pres",
                        "session_id": "short3",
                        "type": "In Person Presentation",
                        "title": "Beyond Visuals : Examining the Experiences of Geoscience Professionals With Vision Disabilities in Accessing Data Visualizations",
                        "contributors": [
                            "Dr Nihanth W Cherukuru"
                        ],
                        "authors": [
                            "Nihanth W Cherukuru",
                            "David Bailey",
                            "Tiffany Fourment",
                            "Becca Hatheway",
                            "Marika Holland",
                            "Matt Rehme"
                        ],
                        "abstract": "Data visualizations are ubiquitous in all disciplines and have become the primary means of analysing data and communicating insights. However, the predominant reliance on visual encoding of data continues to create accessibility barriers for people who are blind/vision impaired resulting in their under representation in Science, Technology, Engineering and Mathematics (STEM) disciplines. This research study seeks to understand the experiences of professionals who are blind/vision impaired in one such STEM discipline (geosciences) in accessing data visualizations. In-depth, semi-structured interviews with seven professionals were conducted to examine the accessibility barriers and areas for improvement to inform accessibility research pertaining to data visualizations through a socio-technical lens. A reflexive thematic analysis revealed the negative impact of visualizations in influencing their career path, lack of data exploration tools for research, barriers in accessing works of peers and mismatched pace of visualization and accessibility research. The article also includes recommendations from the participants to address some of these accessibility barriers.",
                        "uid": "v-short-1055",
                        "file_name": "",
                        "time_stamp": "2022-10-20T20:03:00Z",
                        "time_start": "2022-10-20T20:03:00Z",
                        "time_end": "2022-10-20T20:10:00Z",
                        "paper_type": "short",
                        "keywords": "Human-centered computing\u2014Visualization\u2014Visualization Design and evaluation methods; Human-centered computing\u2014Accessibility\u2014Accessibility technologies",
                        "has_image": "1",
                        "has_video": "699",
                        "paper_award": "",
                        "image_caption": "A photograph showing a woman's hand over a tactile data visualization 3D model. The tactile representation is a white 3D printed surface of the earth with cardboard cutouts representing sea-ice data.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-short-1055-qa",
                        "session_id": "short3",
                        "type": "In Person Q+A",
                        "title": "Beyond Visuals : Examining the Experiences of Geoscience Professionals With Vision Disabilities in Accessing Data Visualizations (Q+A)",
                        "contributors": [
                            "Dr Nihanth W Cherukuru"
                        ],
                        "authors": [],
                        "abstract": "Data visualizations are ubiquitous in all disciplines and have become the primary means of analysing data and communicating insights. However, the predominant reliance on visual encoding of data continues to create accessibility barriers for people who are blind/vision impaired resulting in their under representation in Science, Technology, Engineering and Mathematics (STEM) disciplines. This research study seeks to understand the experiences of professionals who are blind/vision impaired in one such STEM discipline (geosciences) in accessing data visualizations. In-depth, semi-structured interviews with seven professionals were conducted to examine the accessibility barriers and areas for improvement to inform accessibility research pertaining to data visualizations through a socio-technical lens. A reflexive thematic analysis revealed the negative impact of visualizations in influencing their career path, lack of data exploration tools for research, barriers in accessing works of peers and mismatched pace of visualization and accessibility research. The article also includes recommendations from the participants to address some of these accessibility barriers.",
                        "uid": "v-short-1055",
                        "file_name": "",
                        "time_stamp": "2022-10-20T20:10:00Z",
                        "time_start": "2022-10-20T20:10:00Z",
                        "time_end": "2022-10-20T20:12:00Z",
                        "paper_type": "short",
                        "keywords": "Human-centered computing\u2014Visualization\u2014Visualization Design and evaluation methods; Human-centered computing\u2014Accessibility\u2014Accessibility technologies",
                        "has_image": "1",
                        "has_video": "699",
                        "paper_award": "",
                        "image_caption": "A photograph showing a woman's hand over a tactile data visualization 3D model. The tactile representation is a white 3D printed surface of the earth with cardboard cutouts representing sea-ice data.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    }
                ]
            },
            {
                "title": "Personal Visualization, Theory, Evaluation, and eXtended Reality",
                "session_id": "short4",
                "event_prefix": "v-short",
                "track": "ok1",
                "livestream_id": "ok1-thu",
                "session_image": "short4.png",
                "chair": [
                    "Cindy Xiong"
                ],
                "organizers": [],
                "time_start": "2022-10-20T14:00:00Z",
                "time_end": "2022-10-20T15:15:00Z",
                "discord_category": "",
                "discord_channel": "",
                "discord_channel_id": "",
                "discord_link": "",
                "slido_link": "",
                "youtube_url": "",
                "zoom_meeting": "",
                "zoom_password": "",
                "zoom_link": "",
                "ff_link": "",
                "ff_playlist": "",
                "time_slots": [
                    {
                        "slot_id": "short4-opening",
                        "session_id": "short4",
                        "type": "In Person Other",
                        "title": "Session Opening",
                        "contributors": [
                            "Cindy Xiong"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "",
                        "file_name": "",
                        "time_stamp": "2022-10-20T14:00:00Z",
                        "time_start": "2022-10-20T14:00:00Z",
                        "time_end": "2022-10-20T14:00:00Z",
                        "paper_type": "",
                        "keywords": "",
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-short-1064-pres",
                        "session_id": "short4",
                        "type": "In Person Presentation",
                        "title": "Let's Get Personal: Exploring the Design of Personalized Visualizations",
                        "contributors": [
                            "Beleicia Bullock"
                        ],
                        "authors": [
                            "Beleicia Bullock",
                            "Shunan Guo",
                            "Eunyee Koh",
                            "Ryan Rossi",
                            "Fan Du",
                            "Jane Hoffswell"
                        ],
                        "abstract": "Media outlets often publish visualizations that can be personalized based on users\u2019 demographics, such as location, race, and age. However, the design of such personalized visualizations remains underexplored. In this work, we contribute a design space analysis of 47 public-facing articles with personalized visualizations to understand how designers structure content, encourage exploration, and present insights. We find that articles often lack explicit exploration suggestions or instructions, data notices, and personalized visual insights. We then outline three trajectories for future research: (1) explore how users choose to personalize visualizations, (2) examine how exploration suggestions and examples impact user interaction, and (3) investigate how personalization influences user insights.",
                        "uid": "v-short-1064",
                        "file_name": "",
                        "time_stamp": "2022-10-20T14:00:00Z",
                        "time_start": "2022-10-20T14:00:00Z",
                        "time_end": "2022-10-20T14:07:00Z",
                        "paper_type": "short",
                        "keywords": "Human-centered computing\u2014Visualization\u2014Visualization application domains",
                        "has_image": "1",
                        "has_video": "462",
                        "paper_award": "",
                        "image_caption": "This heat map provides an overview of the articles in our personalized visualization corpus based on the (a) personalized attributes contained in the article, (b) granularity, and (c) resulting codes for different publications. We also break down the articles by publication. Attributes that only appeared in 1-3 articles are grouped together under \u201cother.\u201d",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-short-1064-qa",
                        "session_id": "short4",
                        "type": "In Person Q+A",
                        "title": "Let's Get Personal: Exploring the Design of Personalized Visualizations (Q+A)",
                        "contributors": [
                            "Beleicia Bullock"
                        ],
                        "authors": [],
                        "abstract": "Media outlets often publish visualizations that can be personalized based on users\u2019 demographics, such as location, race, and age. However, the design of such personalized visualizations remains underexplored. In this work, we contribute a design space analysis of 47 public-facing articles with personalized visualizations to understand how designers structure content, encourage exploration, and present insights. We find that articles often lack explicit exploration suggestions or instructions, data notices, and personalized visual insights. We then outline three trajectories for future research: (1) explore how users choose to personalize visualizations, (2) examine how exploration suggestions and examples impact user interaction, and (3) investigate how personalization influences user insights.",
                        "uid": "v-short-1064",
                        "file_name": "",
                        "time_stamp": "2022-10-20T14:07:00Z",
                        "time_start": "2022-10-20T14:07:00Z",
                        "time_end": "2022-10-20T14:09:00Z",
                        "paper_type": "short",
                        "keywords": "Human-centered computing\u2014Visualization\u2014Visualization application domains",
                        "has_image": "1",
                        "has_video": "462",
                        "paper_award": "",
                        "image_caption": "This heat map provides an overview of the articles in our personalized visualization corpus based on the (a) personalized attributes contained in the article, (b) granularity, and (c) resulting codes for different publications. We also break down the articles by publication. Attributes that only appeared in 1-3 articles are grouped together under \u201cother.\u201d",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-short-1016-pres",
                        "session_id": "short4",
                        "type": "Virtual Presentation (pre-recorded)",
                        "title": "Who benefits from Visualization Adaptations? Towards a better Understanding of the Influence of Visualization Literacy",
                        "contributors": [
                            "Marc Satkowski"
                        ],
                        "authors": [
                            "Marc Satkowski",
                            "Franziska Kessler",
                            "Susanne Narciss",
                            "Raimund Dachselt"
                        ],
                        "abstract": "The ability to read, understand, and comprehend visual information representations is subsumed under the term visualization literacy (VL). One possibility to improve the use of information visualizations is to introduce adaptations. However, it is yet unclear whether people with different VL benefit from adaptations to the same degree. We conducted an online experiment (n = 42) to investigate whether the effect of an adaptation (here: De-Emphasis) of visualizations (bar charts, scatter plots) on performance (accuracy, time) and user experiences depends on users\u2019 VL level. Using linear mixed models for the analyses, we found a positive impact of the De-Emphasis adaptation across all conditions, as well as an interaction effect of adaptation and VL on the task completion time for bar charts. This work contributes to a better understanding of the intertwined relationship of VL and visual adaptations and motivates future research.",
                        "uid": "v-short-1016",
                        "file_name": "",
                        "time_stamp": "2022-10-20T14:09:00Z",
                        "time_start": "2022-10-20T14:09:00Z",
                        "time_end": "2022-10-20T14:16:00Z",
                        "paper_type": "short",
                        "keywords": "User Study; Visualization Adaptation; Visualization Literacy; Visualization Competence; Information Visualization; Online Survey; User Experience",
                        "has_image": "1",
                        "has_video": "425",
                        "paper_award": "",
                        "image_caption": "An altered title slides of the talk.\nAt the top of the image 4 visualizations are shown.\nLeft are two grouped bar charts, right two scatterplots, whereof one is presented normally, and one makes use of the De-Emphasis technique.\nAll 4 visualizations demonstrate the 4 conditions used in the study of this paper.\nIn the middle, the title of the paper, the authors are shown, and their affiliations are shown.\nFurther, a QR code is visible, which encodes the project page: imld.de/VL-Adaptation-Study.\nOn the bottom of the slides, several logos are shown in accordance with the affiliations.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-short-1016-qa",
                        "session_id": "short4",
                        "type": "Virtual Q+A",
                        "title": "Who benefits from Visualization Adaptations? Towards a better Understanding of the Influence of Visualization Literacy (Q+A)",
                        "contributors": [
                            "Marc Satkowski"
                        ],
                        "authors": [],
                        "abstract": "The ability to read, understand, and comprehend visual information representations is subsumed under the term visualization literacy (VL). One possibility to improve the use of information visualizations is to introduce adaptations. However, it is yet unclear whether people with different VL benefit from adaptations to the same degree. We conducted an online experiment (n = 42) to investigate whether the effect of an adaptation (here: De-Emphasis) of visualizations (bar charts, scatter plots) on performance (accuracy, time) and user experiences depends on users\u2019 VL level. Using linear mixed models for the analyses, we found a positive impact of the De-Emphasis adaptation across all conditions, as well as an interaction effect of adaptation and VL on the task completion time for bar charts. This work contributes to a better understanding of the intertwined relationship of VL and visual adaptations and motivates future research.",
                        "uid": "v-short-1016",
                        "file_name": "",
                        "time_stamp": "2022-10-20T14:16:00Z",
                        "time_start": "2022-10-20T14:16:00Z",
                        "time_end": "2022-10-20T14:18:00Z",
                        "paper_type": "short",
                        "keywords": "User Study; Visualization Adaptation; Visualization Literacy; Visualization Competence; Information Visualization; Online Survey; User Experience",
                        "has_image": "1",
                        "has_video": "425",
                        "paper_award": "",
                        "image_caption": "An altered title slides of the talk.\nAt the top of the image 4 visualizations are shown.\nLeft are two grouped bar charts, right two scatterplots, whereof one is presented normally, and one makes use of the De-Emphasis technique.\nAll 4 visualizations demonstrate the 4 conditions used in the study of this paper.\nIn the middle, the title of the paper, the authors are shown, and their affiliations are shown.\nFurther, a QR code is visible, which encodes the project page: imld.de/VL-Adaptation-Study.\nOn the bottom of the slides, several logos are shown in accordance with the affiliations.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-short-1141-pres",
                        "session_id": "short4",
                        "type": "In Person Presentation",
                        "title": "VisQuiz: Exploring Feedback Mechanisms to Improve Graphical Perception",
                        "contributors": [
                            "No\u00eblle Rakotondravony",
                            "Yiren Ding"
                        ],
                        "authors": [
                            "Ryan Birchfield",
                            "Maddison Caten",
                            "Errica Cheng",
                            "Madyson Kelly",
                            "Truman Larson",
                            "Hoang Phan Pham",
                            "Yiren Ding",
                            "No\u00eblle Rakotondravony",
                            "Lane Harrison"
                        ],
                        "abstract": "Graphical perception studies are a key element of visualization research, forming the basis of design recommendations and contributing to our understanding of how people make sense of visualizations. However, graphical perception studies typically include only brief training sessions, and the impact of longer and more in-depth feedback remains unclear. In this paper, we explore the design and evaluation of feedback for graphical perception tasks, called VisQuiz. Using a quiz-like metaphor, we design feedback for a typical visualization comparison experiment, showing participants their answer alongside the correct answer in an animated sequence in each trial. We extend this quiz metaphor to include summary feedback after each stage of the experiment, providing additional moments for participants to reflect on their performance. To evaluate VisQuiz, we conduct a between-subjects experiment, including three stages of 40 trials each with a control condition that included only summary feedback. Results from n = 80 participants show that once participants started receiving trial feedback (Stage 2) they performed significantly better with bubble charts than those in the control condition. This effect carried over when feedback was removed (Stage 3). Results also suggest an overall trend of improved performance due to feedback. We discuss these findings in the context of other visualization literacy efforts, and possible future work at the intersection of visualization, feedback, and learning. Experiment data and analysis scripts are available at the following repository https://osf.io/jys5d/",
                        "uid": "v-short-1141",
                        "file_name": "",
                        "time_stamp": "2022-10-20T14:18:00Z",
                        "time_start": "2022-10-20T14:18:00Z",
                        "time_end": "2022-10-20T14:25:00Z",
                        "paper_type": "short",
                        "keywords": "Visualization, Graphical Perception, Feedback",
                        "has_image": "1",
                        "has_video": "415",
                        "paper_award": "",
                        "image_caption": "We explore the design and evaluation of feedback for graphical perception tasks. Using a quiz-like metaphor, we design animation-powered feedback for a typical visualization comparison experiment, showing participants their answer alongside the correct answer in an animated sequence in each trial, as well as summary feedback at the end of trial sections. We conduct a between-subjects experiment, including three stages with a control condition that included only summary feedback. Results show that once participants started receiving trial feedback they performed significantly better with bubble charts than those in the control condition. This effect carried over when feedback was removed.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-short-1141-qa",
                        "session_id": "short4",
                        "type": "In Person Q+A",
                        "title": "VisQuiz: Exploring Feedback Mechanisms to Improve Graphical Perception (Q+A)",
                        "contributors": [
                            "No\u00eblle Rakotondravony",
                            "Yiren Ding"
                        ],
                        "authors": [],
                        "abstract": "Graphical perception studies are a key element of visualization research, forming the basis of design recommendations and contributing to our understanding of how people make sense of visualizations. However, graphical perception studies typically include only brief training sessions, and the impact of longer and more in-depth feedback remains unclear. In this paper, we explore the design and evaluation of feedback for graphical perception tasks, called VisQuiz. Using a quiz-like metaphor, we design feedback for a typical visualization comparison experiment, showing participants their answer alongside the correct answer in an animated sequence in each trial. We extend this quiz metaphor to include summary feedback after each stage of the experiment, providing additional moments for participants to reflect on their performance. To evaluate VisQuiz, we conduct a between-subjects experiment, including three stages of 40 trials each with a control condition that included only summary feedback. Results from n = 80 participants show that once participants started receiving trial feedback (Stage 2) they performed significantly better with bubble charts than those in the control condition. This effect carried over when feedback was removed (Stage 3). Results also suggest an overall trend of improved performance due to feedback. We discuss these findings in the context of other visualization literacy efforts, and possible future work at the intersection of visualization, feedback, and learning. Experiment data and analysis scripts are available at the following repository https://osf.io/jys5d/",
                        "uid": "v-short-1141",
                        "file_name": "",
                        "time_stamp": "2022-10-20T14:25:00Z",
                        "time_start": "2022-10-20T14:25:00Z",
                        "time_end": "2022-10-20T14:27:00Z",
                        "paper_type": "short",
                        "keywords": "Visualization, Graphical Perception, Feedback",
                        "has_image": "1",
                        "has_video": "415",
                        "paper_award": "",
                        "image_caption": "We explore the design and evaluation of feedback for graphical perception tasks. Using a quiz-like metaphor, we design animation-powered feedback for a typical visualization comparison experiment, showing participants their answer alongside the correct answer in an animated sequence in each trial, as well as summary feedback at the end of trial sections. We conduct a between-subjects experiment, including three stages with a control condition that included only summary feedback. Results show that once participants started receiving trial feedback they performed significantly better with bubble charts than those in the control condition. This effect carried over when feedback was removed.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-short-1100-pres",
                        "session_id": "short4",
                        "type": "In Person Presentation",
                        "title": "OSCAR: A Semantic-based Data Binning Approach",
                        "contributors": [
                            "Vidya Setlur"
                        ],
                        "authors": [
                            "Vidya Setlur",
                            "Michael Correll",
                            "Sarah Battersby"
                        ],
                        "abstract": "Binning is applied to categorize data values or to see distributions of data. Existing binning algorithms often rely on statistical properties of data. However, there are semantic considerations for selecting appropriate binning schemes. Surveys, for instance, gather respondent data for demographic-related questions such as age, salary, number of employees, etc., that are bucketed into defined semantic categories. In this paper, we leverage common semantic categories from survey data and Tableau Public visualizations to identify a set of semantic binning categories. We employ these semantic binning categories in OSCAR: a method for automatically selecting bins based on the inferred semantic type of the field. We conducted a crowdsourced study with 120 participants to better understand user preferences for bins generated by OSCAR vs. binning provided in Tableau. We find that maps and histograms using binned values generated by OSCAR are preferred by users as compared to binning schemes based purely on the statistical properties of the data.",
                        "uid": "v-short-1100",
                        "file_name": "",
                        "time_stamp": "2022-10-20T14:27:00Z",
                        "time_start": "2022-10-20T14:27:00Z",
                        "time_end": "2022-10-20T14:34:00Z",
                        "paper_type": "short",
                        "keywords": "Data-driven semantics, binning, constraints, geospatial",
                        "has_image": "1",
                        "has_video": "382",
                        "paper_award": "",
                        "image_caption": "Visualizations showing comparisons of bins for data on per-country life expectancy (left) and per-U.S. county obesity rates (right). The top-row bins are computed based on statistical properties, while the bottom-row bins are computed by Oscar. Semantic bins have benefits for legibility, reducing the number of bins (i.e., the visual complexity of the map or histogram), and taking advantage of non-uniformity to either highlight areas of interest or compress long tails of the distribution into single bins.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-short-1100-qa",
                        "session_id": "short4",
                        "type": "In Person Q+A",
                        "title": "OSCAR: A Semantic-based Data Binning Approach (Q+A)",
                        "contributors": [
                            "Vidya Setlur"
                        ],
                        "authors": [],
                        "abstract": "Binning is applied to categorize data values or to see distributions of data. Existing binning algorithms often rely on statistical properties of data. However, there are semantic considerations for selecting appropriate binning schemes. Surveys, for instance, gather respondent data for demographic-related questions such as age, salary, number of employees, etc., that are bucketed into defined semantic categories. In this paper, we leverage common semantic categories from survey data and Tableau Public visualizations to identify a set of semantic binning categories. We employ these semantic binning categories in OSCAR: a method for automatically selecting bins based on the inferred semantic type of the field. We conducted a crowdsourced study with 120 participants to better understand user preferences for bins generated by OSCAR vs. binning provided in Tableau. We find that maps and histograms using binned values generated by OSCAR are preferred by users as compared to binning schemes based purely on the statistical properties of the data.",
                        "uid": "v-short-1100",
                        "file_name": "",
                        "time_stamp": "2022-10-20T14:34:00Z",
                        "time_start": "2022-10-20T14:34:00Z",
                        "time_end": "2022-10-20T14:36:00Z",
                        "paper_type": "short",
                        "keywords": "Data-driven semantics, binning, constraints, geospatial",
                        "has_image": "1",
                        "has_video": "382",
                        "paper_award": "",
                        "image_caption": "Visualizations showing comparisons of bins for data on per-country life expectancy (left) and per-U.S. county obesity rates (right). The top-row bins are computed based on statistical properties, while the bottom-row bins are computed by Oscar. Semantic bins have benefits for legibility, reducing the number of bins (i.e., the visual complexity of the map or histogram), and taking advantage of non-uniformity to either highlight areas of interest or compress long tails of the distribution into single bins.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-short-1143-pres",
                        "session_id": "short4",
                        "type": "Virtual Presentation (pre-recorded)",
                        "title": "Towards Systematic Design Considerations of Organizing Multiple Views",
                        "contributors": [
                            "Abdul Rahman Shaikh"
                        ],
                        "authors": [
                            "Abdul Rahman Shaikh",
                            "David Koop",
                            "Hamed Alhoori",
                            "Maoyuan Sun"
                        ],
                        "abstract": "Multiple-view visualization (MV) has been used for visual analytics in various fields (e.g., bioinformatics, cybersecurity, and intelligence analysis). Because each view encodes data from a particular perspective, analysts often use a set of views laid out in 2D space to link and synthesize information. The difficulty of this process is impacted by the spatial organization of these views. For instance, connecting information from views far from each other can be more challenging than neighboring ones. However, most visual analysis tools currently either fix the positions of the views or completely delegate this organization of views to users (who must manually drag and move views). This either limits user involvement in managing the layout of MV or is overly flexible without much guidance. Then, a key design challenge in MV layout is determining the factors in a spatial organization that impact understanding. To address this, we review a set of MV-based systems and identify considerations for MV layout rooted in two key concerns: perception, which considers how users perceive view relationships, and content, which considers the relationships in the data. We show how these allow us to study and analyze the design of MV layout systematically.",
                        "uid": "v-short-1143",
                        "file_name": "",
                        "time_stamp": "2022-10-20T14:36:00Z",
                        "time_start": "2022-10-20T14:36:00Z",
                        "time_end": "2022-10-20T14:43:00Z",
                        "paper_type": "short",
                        "keywords": "Multiple views, visual analytics, spatial layout",
                        "has_image": "1",
                        "has_video": "414",
                        "paper_award": "",
                        "image_caption": "Examples of MV layout designs based on factors of user perception (a-g) and view content (h-j), which highlights broadening the design considerations from purely perception-driven to intelligently content-driven.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-short-1143-qa",
                        "session_id": "short4",
                        "type": "Virtual Q+A",
                        "title": "Towards Systematic Design Considerations of Organizing Multiple Views (Q+A)",
                        "contributors": [
                            "Abdul Rahman Shaikh"
                        ],
                        "authors": [],
                        "abstract": "Multiple-view visualization (MV) has been used for visual analytics in various fields (e.g., bioinformatics, cybersecurity, and intelligence analysis). Because each view encodes data from a particular perspective, analysts often use a set of views laid out in 2D space to link and synthesize information. The difficulty of this process is impacted by the spatial organization of these views. For instance, connecting information from views far from each other can be more challenging than neighboring ones. However, most visual analysis tools currently either fix the positions of the views or completely delegate this organization of views to users (who must manually drag and move views). This either limits user involvement in managing the layout of MV or is overly flexible without much guidance. Then, a key design challenge in MV layout is determining the factors in a spatial organization that impact understanding. To address this, we review a set of MV-based systems and identify considerations for MV layout rooted in two key concerns: perception, which considers how users perceive view relationships, and content, which considers the relationships in the data. We show how these allow us to study and analyze the design of MV layout systematically.",
                        "uid": "v-short-1143",
                        "file_name": "",
                        "time_stamp": "2022-10-20T14:43:00Z",
                        "time_start": "2022-10-20T14:43:00Z",
                        "time_end": "2022-10-20T14:45:00Z",
                        "paper_type": "short",
                        "keywords": "Multiple views, visual analytics, spatial layout",
                        "has_image": "1",
                        "has_video": "414",
                        "paper_award": "",
                        "image_caption": "Examples of MV layout designs based on factors of user perception (a-g) and view content (h-j), which highlights broadening the design considerations from purely perception-driven to intelligently content-driven.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-short-1133-pres",
                        "session_id": "short4",
                        "type": "Virtual Presentation (pre-recorded)",
                        "title": "Toward Systematic Considerations of Missingness in Visual Analytics",
                        "contributors": [
                            "Maoyuan Sun"
                        ],
                        "authors": [
                            "Maoyuan Sun",
                            "Yue Ma",
                            "Yuanxin Wang",
                            "Tianyi Li",
                            "Jian Zhao",
                            "Yujun Liu",
                            "Ping-Shou Zhong"
                        ],
                        "abstract": "Data-driven decision making has been a common task in today\u2019s big data era, from simple choices such as finding a fast way to drive home, to complex decisions on medical treatment. It is often supported by visual analytics. For various reasons (e.g., system failure, interrupted network, intentional information hiding, or bias), visual analytics for sensemaking of data involves missingness (e.g., data loss and incomplete analysis), which impacts human decisions. For example, missing data can cost a business millions of dollars, and failing to recognize key evidence can put an innocent person in jail. Being aware of missingness is critical to avoid such catastrophes. To fulfill this, as an initial step, we consider missingness in visual analytics from two aspects: data-centric and human-centric. The former emphasizes missingness in three data-related categories: data composition, data relationship, and data usage. The latter focuses on the human-perceived missingness at three levels: observed-level, inferred-level, and ignored-level. Based on them, we discuss possible roles of visualizations for handling missingness, and conclude our discussion with future research opportunities.",
                        "uid": "v-short-1133",
                        "file_name": "",
                        "time_stamp": "2022-10-20T14:45:00Z",
                        "time_start": "2022-10-20T14:45:00Z",
                        "time_end": "2022-10-20T14:52:00Z",
                        "paper_type": "short",
                        "keywords": "Missingness, missing data visualization, sensemaking, visual analytics",
                        "has_image": "1",
                        "has_video": "421",
                        "paper_award": "",
                        "image_caption": "Examples corresponding to the data-centric view of missingness.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-short-1133-qa",
                        "session_id": "short4",
                        "type": "Virtual Q+A",
                        "title": "Toward Systematic Considerations of Missingness in Visual Analytics (Q+A)",
                        "contributors": [
                            "Maoyuan Sun"
                        ],
                        "authors": [],
                        "abstract": "Data-driven decision making has been a common task in today\u2019s big data era, from simple choices such as finding a fast way to drive home, to complex decisions on medical treatment. It is often supported by visual analytics. For various reasons (e.g., system failure, interrupted network, intentional information hiding, or bias), visual analytics for sensemaking of data involves missingness (e.g., data loss and incomplete analysis), which impacts human decisions. For example, missing data can cost a business millions of dollars, and failing to recognize key evidence can put an innocent person in jail. Being aware of missingness is critical to avoid such catastrophes. To fulfill this, as an initial step, we consider missingness in visual analytics from two aspects: data-centric and human-centric. The former emphasizes missingness in three data-related categories: data composition, data relationship, and data usage. The latter focuses on the human-perceived missingness at three levels: observed-level, inferred-level, and ignored-level. Based on them, we discuss possible roles of visualizations for handling missingness, and conclude our discussion with future research opportunities.",
                        "uid": "v-short-1133",
                        "file_name": "",
                        "time_stamp": "2022-10-20T14:52:00Z",
                        "time_start": "2022-10-20T14:52:00Z",
                        "time_end": "2022-10-20T14:54:00Z",
                        "paper_type": "short",
                        "keywords": "Missingness, missing data visualization, sensemaking, visual analytics",
                        "has_image": "1",
                        "has_video": "421",
                        "paper_award": "",
                        "image_caption": "Examples corresponding to the data-centric view of missingness.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-short-1061-pres",
                        "session_id": "short4",
                        "type": "Virtual Presentation (pre-recorded)",
                        "title": "The role of extended reality for planning coronary artery bypass graft surgery",
                        "contributors": [
                            "Prof. Amanda Randles",
                            "David Urick"
                        ],
                        "authors": [
                            "Madhurima Vardhan",
                            "Harvey Shi",
                            "David Urick",
                            "Manesh Patel",
                            "Jane A. Leopold",
                            "Amanda Randles"
                        ],
                        "abstract": "Immersive visual displays are becoming more common in the diagnostic imaging and pre-procedural planning of complex cardiology revascularization surgeries. One such procedure is coronary artery bypass grafting (CABG) surgery, which is a gold standard treatment for patients with advanced coronary heart disease. Treatment planning of the CABG surgery can be aided by extended reality (XR) displays as they are known for offering advantageous visualization of spatially heterogeneous and complex tasks. Despite the benefits of XR, it remains unknown whether clinicians will benefit from higher visual immersion offered by XR. In order to assess the impact of increased immersion as well as the latent factor of geometrical complexity, a quantitative user evaluation (n=14) was performed with clinicians of advanced cardiology training simulating CABG placement on sixteen 3D arterial tree models derived from 6 patients two levels of anatomic complexity. These arterial models were rendered on 3D/XR and 2D display modes with the same tactile interaction input device. The findings of this study reveal that compared to a monoscopic 2D display, the greater visual immersion of 3D/XR does not significantly alter clinician accuracy in the task of bypass graft placement. Latent factors such as arterial complexity and clinical experience both influence the accuracy of graft placement. In addition, an anatomically less complex model and greater clinical experience improve accuracy on both 3D/XR and 2D display modes. The findings of this study can help inform design guidelines of efficient and robust XR tools for pre-procedural planning of complex coronary revascularization surgeries such as CABG.",
                        "uid": "v-short-1061",
                        "file_name": "",
                        "time_stamp": "2022-10-20T14:54:00Z",
                        "time_start": "2022-10-20T14:54:00Z",
                        "time_end": "2022-10-20T15:01:00Z",
                        "paper_type": "short",
                        "keywords": "extended reality, coronary artery bypass graft surgery, anatomic complexity, treatment planning, stereoscopic and monoscopic displays",
                        "has_image": "1",
                        "has_video": "451",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-short-1061-qa",
                        "session_id": "short4",
                        "type": "Virtual Q+A",
                        "title": "The role of extended reality for planning coronary artery bypass graft surgery (Q+A)",
                        "contributors": [
                            "Prof. Amanda Randles",
                            "David Urick"
                        ],
                        "authors": [],
                        "abstract": "Immersive visual displays are becoming more common in the diagnostic imaging and pre-procedural planning of complex cardiology revascularization surgeries. One such procedure is coronary artery bypass grafting (CABG) surgery, which is a gold standard treatment for patients with advanced coronary heart disease. Treatment planning of the CABG surgery can be aided by extended reality (XR) displays as they are known for offering advantageous visualization of spatially heterogeneous and complex tasks. Despite the benefits of XR, it remains unknown whether clinicians will benefit from higher visual immersion offered by XR. In order to assess the impact of increased immersion as well as the latent factor of geometrical complexity, a quantitative user evaluation (n=14) was performed with clinicians of advanced cardiology training simulating CABG placement on sixteen 3D arterial tree models derived from 6 patients two levels of anatomic complexity. These arterial models were rendered on 3D/XR and 2D display modes with the same tactile interaction input device. The findings of this study reveal that compared to a monoscopic 2D display, the greater visual immersion of 3D/XR does not significantly alter clinician accuracy in the task of bypass graft placement. Latent factors such as arterial complexity and clinical experience both influence the accuracy of graft placement. In addition, an anatomically less complex model and greater clinical experience improve accuracy on both 3D/XR and 2D display modes. The findings of this study can help inform design guidelines of efficient and robust XR tools for pre-procedural planning of complex coronary revascularization surgeries such as CABG.",
                        "uid": "v-short-1061",
                        "file_name": "",
                        "time_stamp": "2022-10-20T15:01:00Z",
                        "time_start": "2022-10-20T15:01:00Z",
                        "time_end": "2022-10-20T15:03:00Z",
                        "paper_type": "short",
                        "keywords": "extended reality, coronary artery bypass graft surgery, anatomic complexity, treatment planning, stereoscopic and monoscopic displays",
                        "has_image": "1",
                        "has_video": "451",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-short-1092-pres",
                        "session_id": "short4",
                        "type": "In Person Presentation",
                        "title": "ARShopping: In-Store Shopping Decision Support Through Augmented Reality and Immersive Visualization",
                        "contributors": [
                            "Shunan Guo"
                        ],
                        "authors": [
                            "Bingjie Xu",
                            "Shunan Guo",
                            "Eunyee Koh",
                            "Jane Hoffswell",
                            "Ryan Rossi",
                            "Fan Du"
                        ],
                        "abstract": "Online shopping gives customers boundless options to choose from, backed by extensive product details and customer reviews, all from the comfort of home; yet, no amount of detailed, online information can outweigh the instant gratification and hands-on understanding of a product that is provided by physical stores. However, making purchasing decisions in physical stores can be challenging due to a large number of similar alternatives and limited accessibility of the relevant product information (e.g., features, ratings, and reviews). In this work, we present ARShopping: a web-based prototype to visually communicate detailed product information from an online setting on portable smart devices (e.g., phones, tablets, glasses), within the physical space at the point of purchase. This prototype uses augmented reality (AR) to identify products and display detailed information to help consumers make purchasing decisions that fulfill their needs while decreasing the decision-making time. In particular, we use a data fusion algorithm to improve the precision of the product detection; we then integrate AR visualizations into the scene to facilitate comparisons across multiple products and features. We designed our prototype based on interviews with 14 participants to better understand the utility and ease of use of the prototype.",
                        "uid": "v-short-1092",
                        "file_name": "",
                        "time_stamp": "2022-10-20T15:03:00Z",
                        "time_start": "2022-10-20T15:03:00Z",
                        "time_end": "2022-10-20T15:10:00Z",
                        "paper_type": "short",
                        "keywords": "Human-centered computing\u2014Visualization\u2014Visualization systems and tools; Human-centered computing\u2014Human computer interaction (HCI)\u2014Interaction paradigms\u2014Mixed / augmented reality",
                        "has_image": "1",
                        "has_video": "451",
                        "paper_award": "",
                        "image_caption": "ARShopping detects markers attached to the label of the product and also the product object to identify the type of products in the scene. Through the app, user can (a) filter products based on certain features, (b) compare product features intuitively through visualization glyphs, (c) bookmark products to collection for (d) further investigation on more product features and customer reviews.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-short-1092-qa",
                        "session_id": "short4",
                        "type": "In Person Q+A",
                        "title": "ARShopping: In-Store Shopping Decision Support Through Augmented Reality and Immersive Visualization (Q+A)",
                        "contributors": [
                            "Shunan Guo"
                        ],
                        "authors": [],
                        "abstract": "Online shopping gives customers boundless options to choose from, backed by extensive product details and customer reviews, all from the comfort of home; yet, no amount of detailed, online information can outweigh the instant gratification and hands-on understanding of a product that is provided by physical stores. However, making purchasing decisions in physical stores can be challenging due to a large number of similar alternatives and limited accessibility of the relevant product information (e.g., features, ratings, and reviews). In this work, we present ARShopping: a web-based prototype to visually communicate detailed product information from an online setting on portable smart devices (e.g., phones, tablets, glasses), within the physical space at the point of purchase. This prototype uses augmented reality (AR) to identify products and display detailed information to help consumers make purchasing decisions that fulfill their needs while decreasing the decision-making time. In particular, we use a data fusion algorithm to improve the precision of the product detection; we then integrate AR visualizations into the scene to facilitate comparisons across multiple products and features. We designed our prototype based on interviews with 14 participants to better understand the utility and ease of use of the prototype.",
                        "uid": "v-short-1092",
                        "file_name": "",
                        "time_stamp": "2022-10-20T15:10:00Z",
                        "time_start": "2022-10-20T15:10:00Z",
                        "time_end": "2022-10-20T15:12:00Z",
                        "paper_type": "short",
                        "keywords": "Human-centered computing\u2014Visualization\u2014Visualization systems and tools; Human-centered computing\u2014Human computer interaction (HCI)\u2014Interaction paradigms\u2014Mixed / augmented reality",
                        "has_image": "1",
                        "has_video": "451",
                        "paper_award": "",
                        "image_caption": "ARShopping detects markers attached to the label of the product and also the product object to identify the type of products in the scene. Through the app, user can (a) filter products based on certain features, (b) compare product features intuitively through visualization glyphs, (c) bookmark products to collection for (d) further investigation on more product features and customer reviews.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    }
                ]
            }
        ]
    },
    "v-cga": {
        "event": "CG&A Invited Partnership Presentations",
        "long_name": "CG&A Invited Partnership Presentations",
        "event_type": "Invited Partnership Presentations",
        "event_prefix": "v-cga",
        "event_description": "",
        "event_url": "",
        "organizers": [],
        "sessions": [
            {
                "title": "Visualization Teaching and Literacy (cont.) and Machine Learning for Vis.",
                "session_id": "cga1",
                "event_prefix": "v-cga",
                "track": "ok6",
                "livestream_id": "ok6-thu",
                "session_image": "cga1.png",
                "chair": [
                    "Jieqiong Zhao"
                ],
                "organizers": [],
                "time_start": "2022-10-20T20:45:00Z",
                "time_end": "2022-10-20T22:00:00Z",
                "discord_category": "",
                "discord_channel": "",
                "discord_channel_id": "",
                "discord_link": "",
                "slido_link": "",
                "youtube_url": "",
                "zoom_meeting": "",
                "zoom_password": "",
                "zoom_link": "",
                "ff_link": "",
                "ff_playlist": "",
                "time_slots": [
                    {
                        "slot_id": "cga1-opening",
                        "session_id": "cga1",
                        "type": "In Person Other",
                        "title": "Session Opening",
                        "contributors": [
                            "Jieqiong Zhao"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "",
                        "file_name": "",
                        "time_stamp": "2022-10-20T20:45:00Z",
                        "time_start": "2022-10-20T20:45:00Z",
                        "time_end": "2022-10-20T20:45:00Z",
                        "paper_type": "",
                        "keywords": "",
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-cga-9547792-pres",
                        "session_id": "cga1",
                        "type": "Virtual Presentation (pre-recorded)",
                        "title": "What Students Learn With Personal Data Physicalization",
                        "contributors": [
                            "Charles Perin"
                        ],
                        "authors": [
                            "Charles Perin"
                        ],
                        "abstract": "",
                        "uid": "v-cga-9547792",
                        "file_name": "",
                        "time_stamp": "2022-10-20T20:45:00Z",
                        "time_start": "2022-10-20T20:45:00Z",
                        "time_end": "2022-10-20T20:55:00Z",
                        "paper_type": "full",
                        "keywords": "",
                        "has_image": "1",
                        "has_video": "567",
                        "paper_award": "",
                        "image_caption": "Some of the Personal Data Physicalizations created by students for the assignment of the same name.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-cga-9547792-qa",
                        "session_id": "cga1",
                        "type": "Virtual Q+A",
                        "title": "What Students Learn With Personal Data Physicalization (Q+A)",
                        "contributors": [
                            "Charles Perin"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "v-cga-9547792",
                        "file_name": "",
                        "time_stamp": "2022-10-20T20:55:00Z",
                        "time_start": "2022-10-20T20:55:00Z",
                        "time_end": "2022-10-20T20:57:00Z",
                        "paper_type": "full",
                        "keywords": "",
                        "has_image": "1",
                        "has_video": "567",
                        "paper_award": "",
                        "image_caption": "Some of the Personal Data Physicalizations created by students for the assignment of the same name.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-cga-9476996-pres",
                        "session_id": "cga1",
                        "type": "In Person Presentation",
                        "title": "DeepGD: A Deep Learning Framework for Graph Drawing Using GNN",
                        "contributors": [
                            "Xiaoqi Wang"
                        ],
                        "authors": [
                            "Xiaoqi Wang",
                            "Kevin Yen",
                            "Yifan Hu",
                            "Han-Wei Shen"
                        ],
                        "abstract": "",
                        "uid": "v-cga-9476996",
                        "file_name": "",
                        "time_stamp": "2022-10-20T20:57:00Z",
                        "time_start": "2022-10-20T20:57:00Z",
                        "time_end": "2022-10-20T21:07:00Z",
                        "paper_type": "full",
                        "keywords": "",
                        "has_image": "1",
                        "has_video": "542",
                        "paper_award": "",
                        "image_caption": "We propose a novel Graph Neural Network-based deep learning framework for graph drawing, which can optimize the generated graph layouts toward any differentiable aesthetic metric. Given the fact that a visually pleasing graph layout usually complies with multiple aesthetic aspects, our method attempts to optimize multiple aesthetics simultaneously.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-cga-9476996-qa",
                        "session_id": "cga1",
                        "type": "In Person Q+A",
                        "title": "DeepGD: A Deep Learning Framework for Graph Drawing Using GNN (Q+A)",
                        "contributors": [
                            "Xiaoqi Wang"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "v-cga-9476996",
                        "file_name": "",
                        "time_stamp": "2022-10-20T21:07:00Z",
                        "time_start": "2022-10-20T21:07:00Z",
                        "time_end": "2022-10-20T21:09:00Z",
                        "paper_type": "full",
                        "keywords": "",
                        "has_image": "1",
                        "has_video": "542",
                        "paper_award": "",
                        "image_caption": "We propose a novel Graph Neural Network-based deep learning framework for graph drawing, which can optimize the generated graph layouts toward any differentiable aesthetic metric. Given the fact that a visually pleasing graph layout usually complies with multiple aesthetic aspects, our method attempts to optimize multiple aesthetics simultaneously.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-cga-9490338-pres",
                        "session_id": "cga1",
                        "type": "In Person Presentation",
                        "title": "Interactive Visualization of Hyperspectral Images based on Neural Networks",
                        "contributors": [
                            "Hongfeng Yu"
                        ],
                        "authors": [
                            "Feiyu Zhu",
                            "Yu Pan",
                            "Tian Gao",
                            "Harkamal Walia",
                            "Hongfeng Yu"
                        ],
                        "abstract": "",
                        "uid": "v-cga-9490338",
                        "file_name": "",
                        "time_stamp": "2022-10-20T21:09:00Z",
                        "time_start": "2022-10-20T21:09:00Z",
                        "time_end": "2022-10-20T21:19:00Z",
                        "paper_type": "full",
                        "keywords": "",
                        "has_image": "1",
                        "has_video": "509",
                        "paper_award": "",
                        "image_caption": "User interface of the visualization tool and its classification results for hyperspectal images based on neural networks.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-cga-9490338-qa",
                        "session_id": "cga1",
                        "type": "In Person Q+A",
                        "title": "Interactive Visualization of Hyperspectral Images based on Neural Networks (Q+A)",
                        "contributors": [
                            "Hongfeng Yu"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "v-cga-9490338",
                        "file_name": "",
                        "time_stamp": "2022-10-20T21:19:00Z",
                        "time_start": "2022-10-20T21:19:00Z",
                        "time_end": "2022-10-20T21:21:00Z",
                        "paper_type": "full",
                        "keywords": "",
                        "has_image": "1",
                        "has_video": "509",
                        "paper_award": "",
                        "image_caption": "User interface of the visualization tool and its classification results for hyperspectal images based on neural networks.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-cga-9488227-pres",
                        "session_id": "cga1",
                        "type": "Virtual Presentation (pre-recorded)",
                        "title": "STSRNet: Deep Joint Space\u2013Time Super-Resolution for Vector Field Visualization",
                        "contributors": [
                            "Guihua Shan"
                        ],
                        "authors": [
                            "Yifei An",
                            "Han-Wei Shen",
                            "Guihua Shan",
                            "Guan Li",
                            "Jun Liu"
                        ],
                        "abstract": "",
                        "uid": "v-cga-9488227",
                        "file_name": "",
                        "time_stamp": "2022-10-20T21:21:00Z",
                        "time_start": "2022-10-20T21:21:00Z",
                        "time_end": "2022-10-20T21:31:00Z",
                        "paper_type": "full",
                        "keywords": "",
                        "has_image": "1",
                        "has_video": "595",
                        "paper_award": "",
                        "image_caption": "In this paper, we proposed a joint space-time super-resolution deep learning-based model to reconstruct high temporal and spatial resolution vector field sequences. The model consists of a temporal super-resolution model and a spatial super-resolution, using a physically based loss function combined with temporal coherence for reconstructing vectors. We proved the effectiveness of our model on different datasets through quantitative and qualitative evaluations.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-cga-9488227-qa",
                        "session_id": "cga1",
                        "type": "Virtual Q+A",
                        "title": "STSRNet: Deep Joint Space\u2013Time Super-Resolution for Vector Field Visualization (Q+A)",
                        "contributors": [
                            "Guihua Shan"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "v-cga-9488227",
                        "file_name": "",
                        "time_stamp": "2022-10-20T21:31:00Z",
                        "time_start": "2022-10-20T21:31:00Z",
                        "time_end": "2022-10-20T21:33:00Z",
                        "paper_type": "full",
                        "keywords": "",
                        "has_image": "1",
                        "has_video": "595",
                        "paper_award": "",
                        "image_caption": "In this paper, we proposed a joint space-time super-resolution deep learning-based model to reconstruct high temporal and spatial resolution vector field sequences. The model consists of a temporal super-resolution model and a spatial super-resolution, using a physically based loss function combined with temporal coherence for reconstructing vectors. We proved the effectiveness of our model on different datasets through quantitative and qualitative evaluations.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-cga-9495208-pres",
                        "session_id": "cga1",
                        "type": "Virtual Presentation (pre-recorded)",
                        "title": "Visual Clustering Factors in Scatterplots",
                        "contributors": [
                            "Weixing Lin"
                        ],
                        "authors": [
                            "Jiazhi Xia",
                            "Weixing Lin",
                            "Guang Jiang",
                            "Yunhai Wang",
                            "Wei Chen",
                            "Tobias Schreck"
                        ],
                        "abstract": "",
                        "uid": "v-cga-9495208",
                        "file_name": "",
                        "time_stamp": "2022-10-20T21:33:00Z",
                        "time_start": "2022-10-20T21:33:00Z",
                        "time_end": "2022-10-20T21:43:00Z",
                        "paper_type": "full",
                        "keywords": "",
                        "has_image": "1",
                        "has_video": "578",
                        "paper_award": "",
                        "image_caption": "What are the influence factors of visual clustering in scatterplots? We conduct a data-driven study for this question.\nThe study shows that shape and area are not influencing factors. The factors texture, distance, position, density, angle, and noise are influencing factors.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-cga-9495208-qa",
                        "session_id": "cga1",
                        "type": "Virtual Q+A",
                        "title": "Visual Clustering Factors in Scatterplots (Q+A)",
                        "contributors": [
                            "Weixing Lin"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "v-cga-9495208",
                        "file_name": "",
                        "time_stamp": "2022-10-20T21:43:00Z",
                        "time_start": "2022-10-20T21:43:00Z",
                        "time_end": "2022-10-20T21:45:00Z",
                        "paper_type": "full",
                        "keywords": "",
                        "has_image": "1",
                        "has_video": "578",
                        "paper_award": "",
                        "image_caption": "What are the influence factors of visual clustering in scatterplots? We conduct a data-driven study for this question.\nThe study shows that shape and area are not influencing factors. The factors texture, distance, position, density, angle, and noise are influencing factors.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-cga-9238399-pres",
                        "session_id": "cga1",
                        "type": "In Person Presentation",
                        "title": "Cartolabe: A Web-Based Scalable Visualization of Large Document Collections",
                        "contributors": [
                            "Jean-Daniel Fekete"
                        ],
                        "authors": [
                            "Philippe Caillou",
                            "Jonas Renault",
                            "Jean-Daniel Fekete",
                            "Anne-Catherine Letournel",
                            "Mich\u00e8le Sebag"
                        ],
                        "abstract": "",
                        "uid": "v-cga-9238399",
                        "file_name": "",
                        "time_stamp": "2022-10-20T21:45:00Z",
                        "time_start": "2022-10-20T21:45:00Z",
                        "time_end": "2022-10-20T21:55:00Z",
                        "paper_type": "full",
                        "keywords": "",
                        "has_image": "1",
                        "has_video": "531",
                        "paper_award": "",
                        "image_caption": "Cartolabe visualization of the HAL scientific repository, containing all the French scientific articles (about 1 million) and authors (about 2 millions). Blue points represent articles and red points authors.\nThe layout is computed using the UMAP projection so that close points relate to similar entites whereas distant points are less similar.\nOne author is selected (Jean-Daniel Fekete) and  highlighted, along with his 10 nearest neighbors.\nThe visualization is available at https://cartolabe.fr and allow exploring large document collections, such as arXiv, HAL, and wikipedia.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-cga-9238399-qa",
                        "session_id": "cga1",
                        "type": "In Person Q+A",
                        "title": "Cartolabe: A Web-Based Scalable Visualization of Large Document Collections (Q+A)",
                        "contributors": [
                            "Jean-Daniel Fekete"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "v-cga-9238399",
                        "file_name": "",
                        "time_stamp": "2022-10-20T21:55:00Z",
                        "time_start": "2022-10-20T21:55:00Z",
                        "time_end": "2022-10-20T21:57:00Z",
                        "paper_type": "full",
                        "keywords": "",
                        "has_image": "1",
                        "has_video": "531",
                        "paper_award": "",
                        "image_caption": "Cartolabe visualization of the HAL scientific repository, containing all the French scientific articles (about 1 million) and authors (about 2 millions). Blue points represent articles and red points authors.\nThe layout is computed using the UMAP projection so that close points relate to similar entites whereas distant points are less similar.\nOne author is selected (Jean-Daniel Fekete) and  highlighted, along with his 10 nearest neighbors.\nThe visualization is available at https://cartolabe.fr and allow exploring large document collections, such as arXiv, HAL, and wikipedia.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    }
                ]
            },
            {
                "title": "Visualization Teaching and Literacy",
                "session_id": "cga2",
                "event_prefix": "v-cga",
                "track": "ok6",
                "livestream_id": "ok6-thu",
                "session_image": "cga2.png",
                "chair": [
                    "Tamara Munzner"
                ],
                "organizers": [],
                "time_start": "2022-10-20T19:00:00Z",
                "time_end": "2022-10-20T20:15:00Z",
                "discord_category": "",
                "discord_channel": "",
                "discord_channel_id": "",
                "discord_link": "",
                "slido_link": "",
                "youtube_url": "",
                "zoom_meeting": "",
                "zoom_password": "",
                "zoom_link": "",
                "ff_link": "",
                "ff_playlist": "",
                "time_slots": [
                    {
                        "slot_id": "cga2-opening",
                        "session_id": "cga2",
                        "type": "In Person Other",
                        "title": "Session Opening",
                        "contributors": [
                            "Tamara Munzner"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "",
                        "file_name": "",
                        "time_stamp": "2022-10-20T19:00:00Z",
                        "time_start": "2022-10-20T19:00:00Z",
                        "time_end": "2022-10-20T19:00:00Z",
                        "paper_type": "",
                        "keywords": "",
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-cga-9556564-pres",
                        "session_id": "cga2",
                        "type": "Virtual Presentation (pre-recorded)",
                        "title": "A Taxonomy-Driven Model for Designing Educational Games in  Visualization",
                        "contributors": [
                            "Renata Raidou"
                        ],
                        "authors": [
                            "Lorenzo Amabili",
                            "Kuhu Gupta",
                            "Renata Georgia Raidou"
                        ],
                        "abstract": "",
                        "uid": "v-cga-9556564",
                        "file_name": "",
                        "time_stamp": "2022-10-20T19:00:00Z",
                        "time_start": "2022-10-20T19:00:00Z",
                        "time_end": "2022-10-20T19:10:00Z",
                        "paper_type": "full",
                        "keywords": "",
                        "has_image": "1",
                        "has_video": "605",
                        "paper_award": "",
                        "image_caption": "Prototype of the cards used for Guess Viz? and From A to viZ. The upper set of cards shows the proposed sliding cards: In the top layer, a visualization is shown. In the bottom layer, the related label and the visualization characteristics are represented. The lower set of cards shows the proposed legend cards: We encoded the type of visualization characteristic with color [i.e., data-related (light green), users-related (dark green), tasks-related (light blue), visual-variables-related (tan), visualization-vocabulary-related (pink)]. In Guess Viz?, players use both layers of the sliding cards. In From A to viZ, players use primarily the legend cards.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-cga-9556564-qa",
                        "session_id": "cga2",
                        "type": "Virtual Q+A",
                        "title": "A Taxonomy-Driven Model for Designing Educational Games in  Visualization (Q+A)",
                        "contributors": [
                            "Renata Raidou"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "v-cga-9556564",
                        "file_name": "",
                        "time_stamp": "2022-10-20T19:10:00Z",
                        "time_start": "2022-10-20T19:10:00Z",
                        "time_end": "2022-10-20T19:12:00Z",
                        "paper_type": "full",
                        "keywords": "",
                        "has_image": "1",
                        "has_video": "605",
                        "paper_award": "",
                        "image_caption": "Prototype of the cards used for Guess Viz? and From A to viZ. The upper set of cards shows the proposed sliding cards: In the top layer, a visualization is shown. In the bottom layer, the related label and the visualization characteristics are represented. The lower set of cards shows the proposed legend cards: We encoded the type of visualization characteristic with color [i.e., data-related (light green), users-related (dark green), tasks-related (light blue), visual-variables-related (tan), visualization-vocabulary-related (pink)]. In Guess Viz?, players use both layers of the sliding cards. In From A to viZ, players use primarily the legend cards.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-cga-9551781-pres",
                        "session_id": "cga2",
                        "type": "In Person Presentation",
                        "title": "Remote Instruction for Data Visualization Design-A Report From the Trenches",
                        "contributors": [
                            "Jan Aerts"
                        ],
                        "authors": [
                            "Jan Aerts",
                            "Jannes Peeters",
                            "Jelmer Bot",
                            "Danai Kafetzaki",
                            "Houda Lamqaddam"
                        ],
                        "abstract": "",
                        "uid": "v-cga-9551781",
                        "file_name": "",
                        "time_stamp": "2022-10-20T19:12:00Z",
                        "time_start": "2022-10-20T19:12:00Z",
                        "time_end": "2022-10-20T19:22:00Z",
                        "paper_type": "full",
                        "keywords": "",
                        "has_image": "1",
                        "has_video": "516",
                        "paper_award": "",
                        "image_caption": "Teaching how to explore visual design space to a large group of students is challenging, especially if it needs to happen in an online setting. Using a combination of gather.town and miro, we were able to provide the students an environment and framework in which they could still be creative in creating, discussing and reworking sketches. Gather.town was set up as a communication channel so that students could interact in a relatively natural and self-organising way. Sketches that they created individually at home were uploaded in miro and could be re-organised and annotated to lead to new versions.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-cga-9551781-qa",
                        "session_id": "cga2",
                        "type": "In Person Q+A",
                        "title": "Remote Instruction for Data Visualization Design-A Report From the Trenches (Q+A)",
                        "contributors": [
                            "Jan Aerts"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "v-cga-9551781",
                        "file_name": "",
                        "time_stamp": "2022-10-20T19:22:00Z",
                        "time_start": "2022-10-20T19:22:00Z",
                        "time_end": "2022-10-20T19:24:00Z",
                        "paper_type": "full",
                        "keywords": "",
                        "has_image": "1",
                        "has_video": "516",
                        "paper_award": "",
                        "image_caption": "Teaching how to explore visual design space to a large group of students is challenging, especially if it needs to happen in an online setting. Using a combination of gather.town and miro, we were able to provide the students an environment and framework in which they could still be creative in creating, discussing and reworking sketches. Gather.town was set up as a communication channel so that students could interact in a relatively natural and self-organising way. Sketches that they created individually at home were uploaded in miro and could be re-organised and annotated to lead to new versions.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-cga-9556143-pres",
                        "session_id": "cga2",
                        "type": "Virtual Presentation (pre-recorded)",
                        "title": "A Didactic Framework for Analyzing Learning Activities to Design InfoVis Courses",
                        "contributors": [
                            "Mandy Keck or Dietrich Kammer",
                            "Elena Stoll"
                        ],
                        "authors": [
                            "Mandy Keck",
                            "Elena Stoll",
                            "Dietrich Kammer"
                        ],
                        "abstract": "",
                        "uid": "v-cga-9556143",
                        "file_name": "",
                        "time_stamp": "2022-10-20T19:24:00Z",
                        "time_start": "2022-10-20T19:24:00Z",
                        "time_end": "2022-10-20T19:34:00Z",
                        "paper_type": "full",
                        "keywords": "",
                        "has_image": "1",
                        "has_video": "513",
                        "paper_award": "",
                        "image_caption": "A Data Visualization Activity is a hands-on engagement with data visualization with the goal of learning, reflecting, discussing, or designing visualizations. Today, numerous data visualization activities are available to teach data visualization knowledge in a variety of contexts\u200b. Selecting one or more vis activity in comprehensive courses, however, remains a challenge. To support this process, we propose a didactic vis framework in which complex DataVis activities are broken down into multiple learning activities with different learning goals.\u200b A learning activity template, a learning activity matrix, and a didactic structure chart assist in planning, analyzing and adapting InfoVis courses.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-cga-9556143-qa",
                        "session_id": "cga2",
                        "type": "Virtual Q+A",
                        "title": "A Didactic Framework for Analyzing Learning Activities to Design InfoVis Courses (Q+A)",
                        "contributors": [
                            "Mandy Keck or Dietrich Kammer",
                            "Elena Stoll"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "v-cga-9556143",
                        "file_name": "",
                        "time_stamp": "2022-10-20T19:34:00Z",
                        "time_start": "2022-10-20T19:34:00Z",
                        "time_end": "2022-10-20T19:36:00Z",
                        "paper_type": "full",
                        "keywords": "",
                        "has_image": "1",
                        "has_video": "513",
                        "paper_award": "",
                        "image_caption": "A Data Visualization Activity is a hands-on engagement with data visualization with the goal of learning, reflecting, discussing, or designing visualizations. Today, numerous data visualization activities are available to teach data visualization knowledge in a variety of contexts\u200b. Selecting one or more vis activity in comprehensive courses, however, remains a challenge. To support this process, we propose a didactic vis framework in which complex DataVis activities are broken down into multiple learning activities with different learning goals.\u200b A learning activity template, a learning activity matrix, and a didactic structure chart assist in planning, analyzing and adapting InfoVis courses.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-cga-9547773-pres",
                        "session_id": "cga2",
                        "type": "In Person Presentation",
                        "title": "Through the Looking Glass: Insights Into Visualization Pedagogy Through Sentiment Analysis of Peer Review Text",
                        "contributors": [
                            "Paul Rosen"
                        ],
                        "authors": [
                            "Zachariah J. Beasley",
                            "Alon Friedman",
                            "Paul Rosen"
                        ],
                        "abstract": "",
                        "uid": "v-cga-9547773",
                        "file_name": "",
                        "time_stamp": "2022-10-20T19:36:00Z",
                        "time_start": "2022-10-20T19:36:00Z",
                        "time_end": "2022-10-20T19:46:00Z",
                        "paper_type": "full",
                        "keywords": "",
                        "has_image": "1",
                        "has_video": "510",
                        "paper_award": "",
                        "image_caption": "Illustrative example of a student\u2019s projects and the feedback they gave to their peers, reflecting applied concepts in Data Visualization.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-cga-9547773-qa",
                        "session_id": "cga2",
                        "type": "In Person Q+A",
                        "title": "Through the Looking Glass: Insights Into Visualization Pedagogy Through Sentiment Analysis of Peer Review Text (Q+A)",
                        "contributors": [
                            "Paul Rosen"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "v-cga-9547773",
                        "file_name": "",
                        "time_stamp": "2022-10-20T19:46:00Z",
                        "time_start": "2022-10-20T19:46:00Z",
                        "time_end": "2022-10-20T19:48:00Z",
                        "paper_type": "full",
                        "keywords": "",
                        "has_image": "1",
                        "has_video": "510",
                        "paper_award": "",
                        "image_caption": "Illustrative example of a student\u2019s projects and the feedback they gave to their peers, reflecting applied concepts in Data Visualization.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-cga-9547834-pres",
                        "session_id": "cga2",
                        "type": "In Person Presentation",
                        "title": "Visualization Design Sprints for Online and On-Campus Courses",
                        "contributors": [
                            "Johanna Beyer"
                        ],
                        "authors": [
                            "Johanna Beyer",
                            "Yalong Yang",
                            "Hanspeter Pfister"
                        ],
                        "abstract": "",
                        "uid": "v-cga-9547834",
                        "file_name": "",
                        "time_stamp": "2022-10-20T19:48:00Z",
                        "time_start": "2022-10-20T19:48:00Z",
                        "time_end": "2022-10-20T19:58:00Z",
                        "paper_type": "full",
                        "keywords": "",
                        "has_image": "1",
                        "has_video": "655",
                        "paper_award": "",
                        "image_caption": "Visualization design sprints are a learner-centered, project-based teaching approach for visualization courses. Students gain hands-on experience by following the five design sprint steps.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-cga-9547834-qa",
                        "session_id": "cga2",
                        "type": "In Person Q+A",
                        "title": "Visualization Design Sprints for Online and On-Campus Courses (Q+A)",
                        "contributors": [
                            "Johanna Beyer"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "v-cga-9547834",
                        "file_name": "",
                        "time_stamp": "2022-10-20T19:58:00Z",
                        "time_start": "2022-10-20T19:58:00Z",
                        "time_end": "2022-10-20T20:00:00Z",
                        "paper_type": "full",
                        "keywords": "",
                        "has_image": "1",
                        "has_video": "655",
                        "paper_award": "",
                        "image_caption": "Visualization design sprints are a learner-centered, project-based teaching approach for visualization courses. Students gain hands-on experience by following the five design sprint steps.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-cga-9547790-pres",
                        "session_id": "cga2",
                        "type": "In Person Presentation",
                        "title": "Activity Worksheets for Teaching and Learning Data Visualization",
                        "contributors": [
                            "Vetria Byrd"
                        ],
                        "authors": [
                            "Vetria L. Byrd",
                            "Nicole Dwenger"
                        ],
                        "abstract": "",
                        "uid": "v-cga-9547790",
                        "file_name": "",
                        "time_stamp": "2022-10-20T20:00:00Z",
                        "time_start": "2022-10-20T20:00:00Z",
                        "time_end": "2022-10-20T20:10:00Z",
                        "paper_type": "full",
                        "keywords": "",
                        "has_image": "1",
                        "has_video": "428",
                        "paper_award": "",
                        "image_caption": "This talk presents Activity Worksheets for Teaching and Learning Data Visualization.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-cga-9547790-qa",
                        "session_id": "cga2",
                        "type": "In Person Q+A",
                        "title": "Activity Worksheets for Teaching and Learning Data Visualization (Q+A)",
                        "contributors": [
                            "Vetria Byrd"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "v-cga-9547790",
                        "file_name": "",
                        "time_stamp": "2022-10-20T20:10:00Z",
                        "time_start": "2022-10-20T20:10:00Z",
                        "time_end": "2022-10-20T20:12:00Z",
                        "paper_type": "full",
                        "keywords": "",
                        "has_image": "1",
                        "has_video": "428",
                        "paper_award": "",
                        "image_caption": "This talk presents Activity Worksheets for Teaching and Learning Data Visualization.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    }
                ]
            },
            {
                "title": "Visualization in Industry",
                "session_id": "cga3",
                "event_prefix": "v-cga",
                "track": "mistletoe",
                "livestream_id": "mistletoe-wed",
                "session_image": "cga3.png",
                "chair": [
                    "Michael Wybrow"
                ],
                "organizers": [],
                "time_start": "2022-10-19T20:45:00Z",
                "time_end": "2022-10-19T22:00:00Z",
                "discord_category": "",
                "discord_channel": "",
                "discord_channel_id": "",
                "discord_link": "",
                "slido_link": "",
                "youtube_url": "",
                "zoom_meeting": "",
                "zoom_password": "",
                "zoom_link": "",
                "ff_link": "",
                "ff_playlist": "",
                "time_slots": [
                    {
                        "slot_id": "cga3-opening",
                        "session_id": "cga3",
                        "type": "In Person Other",
                        "title": "Session Opening",
                        "contributors": [
                            "Michael Wybrow"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "",
                        "file_name": "",
                        "time_stamp": "2022-10-19T20:45:00Z",
                        "time_start": "2022-10-19T20:45:00Z",
                        "time_end": "2022-10-19T20:45:00Z",
                        "paper_type": "",
                        "keywords": "",
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-cga-8948290-pres",
                        "session_id": "cga3",
                        "type": "Virtual Presentation (pre-recorded)",
                        "title": "Surgical Navigation System for Low-Dose-Rate Brachytherapy Based on Mixed Reality",
                        "contributors": [
                            "Zeyang Zhou"
                        ],
                        "authors": [
                            "Zeyang Zhou",
                            "Zhiyong Yang",
                            "Shan Jiang",
                            "Xiaodong Ma",
                            "Fujun Zhang",
                            "Huzheng Yan"
                        ],
                        "abstract": "",
                        "uid": "v-cga-8948290",
                        "file_name": "",
                        "time_stamp": "2022-10-19T20:45:00Z",
                        "time_start": "2022-10-19T20:45:00Z",
                        "time_end": "2022-10-19T20:55:00Z",
                        "paper_type": "full",
                        "keywords": "",
                        "has_image": "1",
                        "has_video": "539",
                        "paper_award": "",
                        "image_caption": "Surgical workflow. We prepared a mockup to simulate a real patient with a specific dose plan and fusion of all data\nfrom the mockup. After each mockup surgery was performed with our automated needle and seed detection system, we estimated the location error of the needle and seeds",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-cga-8948290-qa",
                        "session_id": "cga3",
                        "type": "Virtual Q+A",
                        "title": "Surgical Navigation System for Low-Dose-Rate Brachytherapy Based on Mixed Reality (Q+A)",
                        "contributors": [
                            "Zeyang Zhou"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "v-cga-8948290",
                        "file_name": "",
                        "time_stamp": "2022-10-19T20:55:00Z",
                        "time_start": "2022-10-19T20:55:00Z",
                        "time_end": "2022-10-19T20:57:00Z",
                        "paper_type": "full",
                        "keywords": "",
                        "has_image": "1",
                        "has_video": "539",
                        "paper_award": "",
                        "image_caption": "Surgical workflow. We prepared a mockup to simulate a real patient with a specific dose plan and fusion of all data\nfrom the mockup. After each mockup surgery was performed with our automated needle and seed detection system, we estimated the location error of the needle and seeds",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-cga-9709159-pres",
                        "session_id": "cga3",
                        "type": "In Person Presentation",
                        "title": "Visualization for Architecture, Engineering, and Construction: Shaping the Future of Our Built World",
                        "contributors": [
                            "Moataz Abdelaal"
                        ],
                        "authors": [
                            "Moataz Abdelaal",
                            "Felix Amtsberg",
                            "Michael Becher",
                            "Rebeca Duque Estrada",
                            "Fabian Kannenberg",
                            "Aimee Sousa Calepso",
                            "Hans Jakob Wagner",
                            "Guido Reina",
                            "Michael Sedlmair",
                            "Achim Menges",
                            "Daniel Weiskopf"
                        ],
                        "abstract": "",
                        "uid": "v-cga-9709159",
                        "file_name": "",
                        "time_stamp": "2022-10-19T20:57:00Z",
                        "time_start": "2022-10-19T20:57:00Z",
                        "time_end": "2022-10-19T21:07:00Z",
                        "paper_type": "full",
                        "keywords": "",
                        "has_image": "1",
                        "has_video": "771",
                        "paper_award": "",
                        "image_caption": "Interactive data visualization and immersive technology will be the vehicle to support advanced\ndigital and robotic fabrication and construction. Copyright \u00a9 2019 ICD/ITKE University of Stuttgart.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-cga-9709159-qa",
                        "session_id": "cga3",
                        "type": "In Person Q+A",
                        "title": "Visualization for Architecture, Engineering, and Construction: Shaping the Future of Our Built World (Q+A)",
                        "contributors": [
                            "Moataz Abdelaal"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "v-cga-9709159",
                        "file_name": "",
                        "time_stamp": "2022-10-19T21:07:00Z",
                        "time_start": "2022-10-19T21:07:00Z",
                        "time_end": "2022-10-19T21:09:00Z",
                        "paper_type": "full",
                        "keywords": "",
                        "has_image": "1",
                        "has_video": "771",
                        "paper_award": "",
                        "image_caption": "Interactive data visualization and immersive technology will be the vehicle to support advanced\ndigital and robotic fabrication and construction. Copyright \u00a9 2019 ICD/ITKE University of Stuttgart.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-cga-9726809-pres",
                        "session_id": "cga3",
                        "type": "In Person Presentation",
                        "title": "Visual Parameter Space Analysis for Optimizing the Quality of  Industrial Nonwovens",
                        "contributors": [
                            "Viny Saajan Victor"
                        ],
                        "authors": [
                            "Viny Saajan Victor",
                            "Andre Schmeiser",
                            "Heike Leitte",
                            "Simone Gramsch"
                        ],
                        "abstract": "",
                        "uid": "v-cga-9726809",
                        "file_name": "",
                        "time_stamp": "2022-10-19T21:09:00Z",
                        "time_start": "2022-10-19T21:09:00Z",
                        "time_end": "2022-10-19T21:19:00Z",
                        "paper_type": "full",
                        "keywords": "",
                        "has_image": "1",
                        "has_video": "547",
                        "paper_award": "",
                        "image_caption": "'VirtualNonwovenExplorer' is a visualization tool designed to support the textile industry in optimizing the quality of industrial nonwovens.\nThe figure shows the different parameter space analysis strategies that the tool offers in order to obtain optimal and robust process parameter settings.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-cga-9726809-qa",
                        "session_id": "cga3",
                        "type": "In Person Q+A",
                        "title": "Visual Parameter Space Analysis for Optimizing the Quality of  Industrial Nonwovens (Q+A)",
                        "contributors": [
                            "Viny Saajan Victor"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "v-cga-9726809",
                        "file_name": "",
                        "time_stamp": "2022-10-19T21:19:00Z",
                        "time_start": "2022-10-19T21:19:00Z",
                        "time_end": "2022-10-19T21:21:00Z",
                        "paper_type": "full",
                        "keywords": "",
                        "has_image": "1",
                        "has_video": "547",
                        "paper_award": "",
                        "image_caption": "'VirtualNonwovenExplorer' is a visualization tool designed to support the textile industry in optimizing the quality of industrial nonwovens.\nThe figure shows the different parameter space analysis strategies that the tool offers in order to obtain optimal and robust process parameter settings.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-cga-9729397-pres",
                        "session_id": "cga3",
                        "type": "In Person Presentation",
                        "title": "Reflections on Visualization Research Projects in the Manufacturing Industry",
                        "contributors": [
                            "Johanna Schmidt"
                        ],
                        "authors": [
                            "Lena Cibulski",
                            "Johanna Schmidt",
                            "Wolfgang Aigner"
                        ],
                        "abstract": "",
                        "uid": "v-cga-9729397",
                        "file_name": "",
                        "time_stamp": "2022-10-19T21:21:00Z",
                        "time_start": "2022-10-19T21:21:00Z",
                        "time_end": "2022-10-19T21:31:00Z",
                        "paper_type": "full",
                        "keywords": "",
                        "has_image": "1",
                        "has_video": "537",
                        "paper_award": "",
                        "image_caption": "As members of research institutions involved in several applied research projects dealing with visualization in manufacturing, we characterized and analyzed our experiences for a detailed qualitative view, to distill important lessons learned, and to identify research gaps.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-cga-9729397-qa",
                        "session_id": "cga3",
                        "type": "In Person Q+A",
                        "title": "Reflections on Visualization Research Projects in the Manufacturing Industry (Q+A)",
                        "contributors": [
                            "Johanna Schmidt"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "v-cga-9729397",
                        "file_name": "",
                        "time_stamp": "2022-10-19T21:31:00Z",
                        "time_start": "2022-10-19T21:31:00Z",
                        "time_end": "2022-10-19T21:33:00Z",
                        "paper_type": "full",
                        "keywords": "",
                        "has_image": "1",
                        "has_video": "537",
                        "paper_award": "",
                        "image_caption": "As members of research institutions involved in several applied research projects dealing with visualization in manufacturing, we characterized and analyzed our experiences for a detailed qualitative view, to distill important lessons learned, and to identify research gaps.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-cga-9732172-pres",
                        "session_id": "cga3",
                        "type": "In Person Presentation",
                        "title": "Situated Visual Analysis and Live Monitoring for Manufacturing",
                        "contributors": [
                            "Michael Becher"
                        ],
                        "authors": [
                            "Michael Becher",
                            "Dominik Herr",
                            "Christoph Muller",
                            "Kuno Kurzhals",
                            "Guido Reina",
                            "Lena Wagner",
                            "Thomas Ertl",
                            "Daniel Weiskopf"
                        ],
                        "abstract": "",
                        "uid": "v-cga-9732172",
                        "file_name": "",
                        "time_stamp": "2022-10-19T21:33:00Z",
                        "time_start": "2022-10-19T21:33:00Z",
                        "time_end": "2022-10-19T21:43:00Z",
                        "paper_type": "full",
                        "keywords": "",
                        "has_image": "1",
                        "has_video": "493",
                        "paper_award": "",
                        "image_caption": "Modern machines continuously produce a wealth of data in real time about important events, such as faults during the production process. Hence, the tasks of operators on the shop floor are shifting from manual work to monitoring machines and resolving faults reported by those. With our situated visualization approach, operators can use a touch-based interface to perform a visual analysis of event data directly on the shop floor. Visualization in augmented reality allows them to monitor live events during production processes, guides them to the respective machines and provides important contextual information.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-cga-9732172-qa",
                        "session_id": "cga3",
                        "type": "In Person Q+A",
                        "title": "Situated Visual Analysis and Live Monitoring for Manufacturing (Q+A)",
                        "contributors": [
                            "Michael Becher"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "v-cga-9732172",
                        "file_name": "",
                        "time_stamp": "2022-10-19T21:43:00Z",
                        "time_start": "2022-10-19T21:43:00Z",
                        "time_end": "2022-10-19T21:45:00Z",
                        "paper_type": "full",
                        "keywords": "",
                        "has_image": "1",
                        "has_video": "493",
                        "paper_award": "",
                        "image_caption": "Modern machines continuously produce a wealth of data in real time about important events, such as faults during the production process. Hence, the tasks of operators on the shop floor are shifting from manual work to monitoring machines and resolving faults reported by those. With our situated visualization approach, operators can use a touch-based interface to perform a visual analysis of event data directly on the shop floor. Visualization in augmented reality allows them to monitor live events during production processes, guides them to the respective machines and provides important contextual information.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-cga-9709109-pres",
                        "session_id": "cga3",
                        "type": "Virtual Presentation (pre-recorded)",
                        "title": "Stress Visualization for Interface Optimization of a Hybrid Component Using Surface Tensor Spines",
                        "contributors": [
                            "Vanessa Kretzschmar"
                        ],
                        "authors": [
                            "Vanessa Kretzschmar",
                            "Allan Rocha",
                            "Fabian Gunther",
                            "Markus Stommel",
                            "Gerik Scheuermann"
                        ],
                        "abstract": "",
                        "uid": "v-cga-9709109",
                        "file_name": "",
                        "time_stamp": "2022-10-19T21:45:00Z",
                        "time_start": "2022-10-19T21:45:00Z",
                        "time_end": "2022-10-19T21:55:00Z",
                        "paper_type": "full",
                        "keywords": "",
                        "has_image": "1",
                        "has_video": "588",
                        "paper_award": "",
                        "image_caption": "Surface Tensor Spines visualizing a stress tensor field on a component interface layer.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-cga-9709109-qa",
                        "session_id": "cga3",
                        "type": "Virtual Q+A",
                        "title": "Stress Visualization for Interface Optimization of a Hybrid Component Using Surface Tensor Spines (Q+A)",
                        "contributors": [
                            "Vanessa Kretzschmar"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "v-cga-9709109",
                        "file_name": "",
                        "time_stamp": "2022-10-19T21:55:00Z",
                        "time_start": "2022-10-19T21:55:00Z",
                        "time_end": "2022-10-19T21:57:00Z",
                        "paper_type": "full",
                        "keywords": "",
                        "has_image": "1",
                        "has_video": "588",
                        "paper_award": "",
                        "image_caption": "Surface Tensor Spines visualizing a stress tensor field on a component interface layer.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    }
                ]
            }
        ]
    },
    "v-siggraph": {
        "event": "SIGGRAPH Invited Partnership Presentations",
        "long_name": "SIGGRAPH Invited Partnership Presentations",
        "event_type": "Invited Partnership Presentations",
        "event_prefix": "v-siggraph",
        "event_description": "",
        "event_url": "",
        "organizers": [],
        "sessions": [
            {
                "title": "SIGGRAPH Invited Talks",
                "session_id": "sig1",
                "event_prefix": "v-siggraph",
                "track": "mistletoe",
                "livestream_id": "mistletoe-wed",
                "session_image": "sig1.png",
                "chair": [
                    "Markus Hadwiger"
                ],
                "organizers": [],
                "time_start": "2022-10-19T15:45:00Z",
                "time_end": "2022-10-19T17:00:00Z",
                "discord_category": "",
                "discord_channel": "",
                "discord_channel_id": "",
                "discord_link": "",
                "slido_link": "",
                "youtube_url": "",
                "zoom_meeting": "",
                "zoom_password": "",
                "zoom_link": "",
                "ff_link": "",
                "ff_playlist": "",
                "time_slots": [
                    {
                        "slot_id": "sig1-opening",
                        "session_id": "sig1",
                        "type": "In Person Other",
                        "title": "Session Opening",
                        "contributors": [
                            "Markus Hadwiger"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "",
                        "file_name": "",
                        "time_stamp": "2022-10-19T15:45:00Z",
                        "time_start": "2022-10-19T15:45:00Z",
                        "time_end": "2022-10-19T15:45:00Z",
                        "paper_type": "",
                        "keywords": "",
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-siggraph-1-pres",
                        "session_id": "sig1",
                        "type": "In Person Presentation",
                        "title": "Image Features Influence Reaction Time: A Learned Probabilistic Perceptual Model for Saccade Latency",
                        "contributors": [
                            "Budmonde Duinkharjav"
                        ],
                        "authors": [
                            "Budmonde Duinkharjav",
                            "Praneeth Chakravarthula",
                            "Rachel Brown",
                            "Anjul Patney",
                            "Qi Sun"
                        ],
                        "abstract": "We aim to ask and answer an essential question \"how quickly do we react after observing a displayed visual target?\" To this end, we present psychophysical studies that characterize the remarkable disconnect between human saccadic behaviors and spatial visual acuity. Building on the results of our studies, we develop a perceptual model to predict temporal gaze behavior, particularly saccadic latency, as a function of the statistics of a displayed image. Specifically, we implement a neurologically-inspired probabilistic model that mimics the accumulation of confidence that leads to a perceptual decision. We validate our model with a series of objective measurements and user studies using an eye-tracked VR display. The results demonstrate that our model prediction is in statistical alignment with real-world human behavior. Further, we establish that many sub-threshold image modifications commonly introduced in graphics pipelines may significantly alter human reaction timing, even if the differences are visually undetectable. Finally, we show that our model can serve as a metric to predict and alter reaction latency of users in interactive computer graphics applications, thus may improve gaze-contingent rendering, design of virtual experiences, and player performance in e-sports. We illustrate this with two examples: estimating competition fairness in a video game with two different team colors, and tuning display viewing distance to minimize player reaction time.",
                        "uid": "v-siggraph-1",
                        "file_name": "",
                        "time_stamp": "2022-10-19T15:45:00Z",
                        "time_start": "2022-10-19T15:45:00Z",
                        "time_end": "2022-10-19T15:55:00Z",
                        "paper_type": "full",
                        "keywords": "Virtual Reality, Augmented Reality, Visual Perception, Human Performance, Esports, Gaze-Contingent Rendering",
                        "has_image": "1",
                        "has_video": "930",
                        "paper_award": "",
                        "image_caption": "We propose a model which predicts the reaction latency for users to identify and saccade to a peripheral target.\nBased on our psychophysical data collected for stimuli with varying visual characteristics, we model the likelihood distribution of the time users take to process, react, and saccade to a target.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-siggraph-1-qa",
                        "session_id": "sig1",
                        "type": "In Person Q+A",
                        "title": "Image Features Influence Reaction Time: A Learned Probabilistic Perceptual Model for Saccade Latency (Q+A)",
                        "contributors": [
                            "Budmonde Duinkharjav"
                        ],
                        "authors": [],
                        "abstract": "We aim to ask and answer an essential question \"how quickly do we react after observing a displayed visual target?\" To this end, we present psychophysical studies that characterize the remarkable disconnect between human saccadic behaviors and spatial visual acuity. Building on the results of our studies, we develop a perceptual model to predict temporal gaze behavior, particularly saccadic latency, as a function of the statistics of a displayed image. Specifically, we implement a neurologically-inspired probabilistic model that mimics the accumulation of confidence that leads to a perceptual decision. We validate our model with a series of objective measurements and user studies using an eye-tracked VR display. The results demonstrate that our model prediction is in statistical alignment with real-world human behavior. Further, we establish that many sub-threshold image modifications commonly introduced in graphics pipelines may significantly alter human reaction timing, even if the differences are visually undetectable. Finally, we show that our model can serve as a metric to predict and alter reaction latency of users in interactive computer graphics applications, thus may improve gaze-contingent rendering, design of virtual experiences, and player performance in e-sports. We illustrate this with two examples: estimating competition fairness in a video game with two different team colors, and tuning display viewing distance to minimize player reaction time.",
                        "uid": "v-siggraph-1",
                        "file_name": "",
                        "time_stamp": "2022-10-19T15:55:00Z",
                        "time_start": "2022-10-19T15:55:00Z",
                        "time_end": "2022-10-19T15:57:00Z",
                        "paper_type": "full",
                        "keywords": "Virtual Reality, Augmented Reality, Visual Perception, Human Performance, Esports, Gaze-Contingent Rendering",
                        "has_image": "1",
                        "has_video": "930",
                        "paper_award": "",
                        "image_caption": "We propose a model which predicts the reaction latency for users to identify and saccade to a peripheral target.\nBased on our psychophysical data collected for stimuli with varying visual characteristics, we model the likelihood distribution of the time users take to process, react, and saccade to a target.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-siggraph-2-pres",
                        "session_id": "sig1",
                        "type": "In Person Presentation",
                        "title": "CLIPasso: Semantically Aware Object Sketching",
                        "contributors": [
                            "Arik Shamir"
                        ],
                        "authors": [
                            "Yael Vinker",
                            "Ehsan Pajouheshgar",
                            "Jessica Y. Bo",
                            "Roman Christian Bachmann",
                            "Amit Bermano",
                            "Daniel Cohen-Or",
                            "Amir Zamir",
                            "Ariel Shamir"
                        ],
                        "abstract": "Abstraction is at the heart of sketching due to the simple and minimal nature of line drawings. Abstraction entails identifying the essential visual properties of an object or scene, which requires semantic understanding and prior knowledge of high-level concepts. Abstract depictions are therefore challenging for artists, and even more so for machines. We present CLIPasso, an object sketching method that can achieve different levels of abstraction, guided by geometric and semantic simplifications. While sketch generation methods often rely on explicit sketch datasets for training, we utilize the remarkable ability of CLIP (Contrastive-Language-Image-Pretraining) to distill semantic concepts from sketches and images alike. We define a sketch as a set of B\u00e9zier curves and use a differentiable rasterizer to optimize the parameters of the curves directly with respect to a CLIP-based perceptual loss. The abstraction degree is controlled by varying the number of strokes. The generated sketches demonstrate multiple levels of abstraction while maintaining recognizability, underlying structure, and essential visual components of the subject drawn.",
                        "uid": "v-siggraph-2",
                        "file_name": "",
                        "time_stamp": "2022-10-19T15:57:00Z",
                        "time_start": "2022-10-19T15:57:00Z",
                        "time_end": "2022-10-19T16:07:00Z",
                        "paper_type": "full",
                        "keywords": "Sketch Synthesis, Image-based Rendering, Vector Line Art Generation",
                        "has_image": "1",
                        "has_video": "775",
                        "paper_award": "",
                        "image_caption": "{\\rtf1\\ansi\\ansicpg1252\\cocoartf2636\n\\cocoatextscaling0\\cocoaplatform0{\\fonttbl\\f0\\fnil\\fcharset0 HelveticaNeue;}\n{\\colortbl;\\red255\\green255\\blue255;}\n{\\*\\expandedcolortbl;;}\n\\paperw11900\\paperh16840\\margl1440\\margr1440\\vieww11520\\viewh8400\\viewkind0\n\\deftab560\n\\pard\\pardeftab560\\partightenfactor0\n\n\\f0\\fs24 \\cf0 Our work converts an image of an object to a sketch, allowing for varying levels of abstraction, while preserving its key visual features. Even with a very minimal representation (the rightmost flamingo and horse are drawn with only a few strokes), one can recognize both the semantics and the structure of the subject depicted.}",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-siggraph-2-qa",
                        "session_id": "sig1",
                        "type": "In Person Q+A",
                        "title": "CLIPasso: Semantically Aware Object Sketching (Q+A)",
                        "contributors": [
                            "Arik Shamir"
                        ],
                        "authors": [],
                        "abstract": "Abstraction is at the heart of sketching due to the simple and minimal nature of line drawings. Abstraction entails identifying the essential visual properties of an object or scene, which requires semantic understanding and prior knowledge of high-level concepts. Abstract depictions are therefore challenging for artists, and even more so for machines. We present CLIPasso, an object sketching method that can achieve different levels of abstraction, guided by geometric and semantic simplifications. While sketch generation methods often rely on explicit sketch datasets for training, we utilize the remarkable ability of CLIP (Contrastive-Language-Image-Pretraining) to distill semantic concepts from sketches and images alike. We define a sketch as a set of B\u00e9zier curves and use a differentiable rasterizer to optimize the parameters of the curves directly with respect to a CLIP-based perceptual loss. The abstraction degree is controlled by varying the number of strokes. The generated sketches demonstrate multiple levels of abstraction while maintaining recognizability, underlying structure, and essential visual components of the subject drawn.",
                        "uid": "v-siggraph-2",
                        "file_name": "",
                        "time_stamp": "2022-10-19T16:07:00Z",
                        "time_start": "2022-10-19T16:07:00Z",
                        "time_end": "2022-10-19T16:09:00Z",
                        "paper_type": "full",
                        "keywords": "Sketch Synthesis, Image-based Rendering, Vector Line Art Generation",
                        "has_image": "1",
                        "has_video": "775",
                        "paper_award": "",
                        "image_caption": "{\\rtf1\\ansi\\ansicpg1252\\cocoartf2636\n\\cocoatextscaling0\\cocoaplatform0{\\fonttbl\\f0\\fnil\\fcharset0 HelveticaNeue;}\n{\\colortbl;\\red255\\green255\\blue255;}\n{\\*\\expandedcolortbl;;}\n\\paperw11900\\paperh16840\\margl1440\\margr1440\\vieww11520\\viewh8400\\viewkind0\n\\deftab560\n\\pard\\pardeftab560\\partightenfactor0\n\n\\f0\\fs24 \\cf0 Our work converts an image of an object to a sketch, allowing for varying levels of abstraction, while preserving its key visual features. Even with a very minimal representation (the rightmost flamingo and horse are drawn with only a few strokes), one can recognize both the semantics and the structure of the subject depicted.}",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-siggraph-3-pres",
                        "session_id": "sig1",
                        "type": "Virtual Presentation (pre-recorded)",
                        "title": "Joint Neural Phase Retrieval and Compression for Energy- and Computation-efficient Holography on the Edge",
                        "contributors": [
                            "Yujie Wang"
                        ],
                        "authors": [
                            "Yujie Wang",
                            "Praneeth Chakravarthula",
                            "Qi Sun",
                            "Baoquan Chen"
                        ],
                        "abstract": "Recent deep learning approaches have shown remarkable promise to enable high fidelity holographic displays. However, lightweight wearable display devices cannot afford the computation demand and energy consumption for hologram generation due to the limited onboard compute capability and battery life. On the other hand, if the computation is conducted entirely remotely on a cloud server, transmitting lossless hologram data is not only challenging but also result in prohibitively high latency and storage. In this work, by distributing the computation and optimizing the transmission, we propose the first framework that jointly generates and compresses high-quality phase-only holograms. Specifically, our framework asymmetrically separates the hologram generation process into high-compute remote encoding (on the server), and low-compute decoding (on the edge) stages. Our encoding enables light weight latent space data, thus faster and efficient transmission to the edge device. With our framework, we observed a reduction of 76% computation and consequently 83% in energy cost on edge devices, compared to the existing hologram generation methods. Our framework is robust to transmission and decoding errors, and approach high image fidelity for as low as 2 bits-per-pixel, and further reduced average bit-rates and decoding time for holographic videos.",
                        "uid": "v-siggraph-3",
                        "file_name": "",
                        "time_stamp": "2022-10-19T16:09:00Z",
                        "time_start": "2022-10-19T16:09:00Z",
                        "time_end": "2022-10-19T16:19:00Z",
                        "paper_type": "full",
                        "keywords": "Computer generated holography, neural hologram generation, hologram compression",
                        "has_image": "1",
                        "has_video": "672",
                        "paper_award": "",
                        "image_caption": "We propose the first framework devised for cloud-based holographic displaying scenarios in the future.\n\nUsing an end-to-end framework with coupling hologram generation and compression, the transmission efficiency is highly improved by reducing the number of bits for coding holograms.\n\nBy asymmetrically distributing the computation between cloud servers (~80%) and edge devices (~20%), \nour framework considerably reduces the computational cost for edge devices.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-siggraph-3-qa",
                        "session_id": "sig1",
                        "type": "Virtual Q+A",
                        "title": "Joint Neural Phase Retrieval and Compression for Energy- and Computation-efficient Holography on the Edge (Q+A)",
                        "contributors": [
                            "Yujie Wang"
                        ],
                        "authors": [],
                        "abstract": "Recent deep learning approaches have shown remarkable promise to enable high fidelity holographic displays. However, lightweight wearable display devices cannot afford the computation demand and energy consumption for hologram generation due to the limited onboard compute capability and battery life. On the other hand, if the computation is conducted entirely remotely on a cloud server, transmitting lossless hologram data is not only challenging but also result in prohibitively high latency and storage. In this work, by distributing the computation and optimizing the transmission, we propose the first framework that jointly generates and compresses high-quality phase-only holograms. Specifically, our framework asymmetrically separates the hologram generation process into high-compute remote encoding (on the server), and low-compute decoding (on the edge) stages. Our encoding enables light weight latent space data, thus faster and efficient transmission to the edge device. With our framework, we observed a reduction of 76% computation and consequently 83% in energy cost on edge devices, compared to the existing hologram generation methods. Our framework is robust to transmission and decoding errors, and approach high image fidelity for as low as 2 bits-per-pixel, and further reduced average bit-rates and decoding time for holographic videos.",
                        "uid": "v-siggraph-3",
                        "file_name": "",
                        "time_stamp": "2022-10-19T16:19:00Z",
                        "time_start": "2022-10-19T16:19:00Z",
                        "time_end": "2022-10-19T16:21:00Z",
                        "paper_type": "full",
                        "keywords": "Computer generated holography, neural hologram generation, hologram compression",
                        "has_image": "1",
                        "has_video": "672",
                        "paper_award": "",
                        "image_caption": "We propose the first framework devised for cloud-based holographic displaying scenarios in the future.\n\nUsing an end-to-end framework with coupling hologram generation and compression, the transmission efficiency is highly improved by reducing the number of bits for coding holograms.\n\nBy asymmetrically distributing the computation between cloud servers (~80%) and edge devices (~20%), \nour framework considerably reduces the computational cost for edge devices.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-siggraph-4-pres",
                        "session_id": "sig1",
                        "type": "Virtual Presentation (pre-recorded)",
                        "title": "Umbrella Meshes: Volumetric Elastic Mechanisms for Freeform Shape Deployment",
                        "contributors": [
                            "Yingying Ren",
                            "Uday Kusupati, Yingying Ren"
                        ],
                        "authors": [
                            "Yingying Ren",
                            "Uday Kusupati",
                            "Julian Panetta",
                            "Florin Isvoranu",
                            "Davide Pellis",
                            "Tian Chen",
                            "Mark Pauly"
                        ],
                        "abstract": "We present a computational inverse design framework for a new class of volumetric deployable structures that have compact rest states and deploy into bending-active 3D target surfaces. Umbrella meshes consist of elastic beams, rigid plates, and hinge joints that can be directly printed or assembled in a zero-energy fabrication state. During deployment, as the elastic beams of varying heights rotate from vertical to horizontal configurations, the entire structure transforms from a compact block into a target curved surface. Umbrella Meshes encode both intrinsic and extrinsic curvature of the target surface and in principle are free from the area expansion ratio bounds of past auxetic material systems.  We build a reduced physics-based simulation framework to accurately and efficiently model the complex interaction between the elastically deforming components. To determine the mesh topology and optimal shape parameters for approximating a given target surface, we propose an inverse design optimization algorithm initialized with conformal flattening. Our algorithm minimizes the structure's strain energy in its deployed state and optimizes actuation forces so that the final deployed structure is in stable equilibrium close to the desired surface with few or no external constraints. We validate our approach by fabricating a series of physical models at various scales using different manufacturing techniques.",
                        "uid": "v-siggraph-4",
                        "file_name": "",
                        "time_stamp": "2022-10-19T16:21:00Z",
                        "time_start": "2022-10-19T16:21:00Z",
                        "time_end": "2022-10-19T16:31:00Z",
                        "paper_type": "full",
                        "keywords": "Deployable structure, physics-based simulation, numerical optimization, computational design, fabrication",
                        "has_image": "1",
                        "has_video": "524",
                        "paper_award": "",
                        "image_caption": "An umbrella mesh is a volumetric deployable structure with a compact, zero-energy rest state that deploys into a given 3D target surface. We show here the physical model of an umbrella mesh unit cell and the deployment sequence of an umbrella mesh prototype optimized to match the input design surface.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-siggraph-4-qa",
                        "session_id": "sig1",
                        "type": "Virtual Q+A",
                        "title": "Umbrella Meshes: Volumetric Elastic Mechanisms for Freeform Shape Deployment (Q+A)",
                        "contributors": [
                            "Yingying Ren",
                            "Uday Kusupati, Yingying Ren"
                        ],
                        "authors": [],
                        "abstract": "We present a computational inverse design framework for a new class of volumetric deployable structures that have compact rest states and deploy into bending-active 3D target surfaces. Umbrella meshes consist of elastic beams, rigid plates, and hinge joints that can be directly printed or assembled in a zero-energy fabrication state. During deployment, as the elastic beams of varying heights rotate from vertical to horizontal configurations, the entire structure transforms from a compact block into a target curved surface. Umbrella Meshes encode both intrinsic and extrinsic curvature of the target surface and in principle are free from the area expansion ratio bounds of past auxetic material systems.  We build a reduced physics-based simulation framework to accurately and efficiently model the complex interaction between the elastically deforming components. To determine the mesh topology and optimal shape parameters for approximating a given target surface, we propose an inverse design optimization algorithm initialized with conformal flattening. Our algorithm minimizes the structure's strain energy in its deployed state and optimizes actuation forces so that the final deployed structure is in stable equilibrium close to the desired surface with few or no external constraints. We validate our approach by fabricating a series of physical models at various scales using different manufacturing techniques.",
                        "uid": "v-siggraph-4",
                        "file_name": "",
                        "time_stamp": "2022-10-19T16:31:00Z",
                        "time_start": "2022-10-19T16:31:00Z",
                        "time_end": "2022-10-19T16:33:00Z",
                        "paper_type": "full",
                        "keywords": "Deployable structure, physics-based simulation, numerical optimization, computational design, fabrication",
                        "has_image": "1",
                        "has_video": "524",
                        "paper_award": "",
                        "image_caption": "An umbrella mesh is a volumetric deployable structure with a compact, zero-energy rest state that deploys into a given 3D target surface. We show here the physical model of an umbrella mesh unit cell and the deployment sequence of an umbrella mesh prototype optimized to match the input design surface.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-siggraph-5-pres",
                        "session_id": "sig1",
                        "type": "Virtual Presentation (pre-recorded)",
                        "title": "Sketch2Pose: estimating a 3D character pose from a bitmap sketch",
                        "contributors": [
                            "Kirill Brodt"
                        ],
                        "authors": [
                            "Kirill Brodt",
                            "Mikhail Bessmeltsev"
                        ],
                        "abstract": "Artists frequently capture character poses via raster sketches, then use these drawings as a reference while posing a 3D character in a specialized 3D software --- a time-consuming process, requiring specialized 3D training and mental effort. We tackle this challenge by proposing the first system for automatically inferring a 3D character pose from a single bitmap sketch, producing poses consistent with viewer expectations. Algorithmically interpreting bitmap sketches is challenging, as they contain significantly distorted proportions and foreshortening. We address this by predicting three key elements of a drawing, necessary to disambiguate the drawn poses: 2D bone tangents, self-contacts, and bone foreshortening. These elements are then leveraged in an optimization inferring the 3D character pose consistent with the artist's intent. Our optimization balances cues derived from artistic literature and perception research to compensate for distorted character proportions. We demonstrate a gallery of results on sketches of numerous styles. We validate our method via numerical evaluations, user studies, and comparisons to manually posed characters and previous work. Code and data for our paper are available at http://www-labs.iro.umontreal.ca/bmpix/sketch2pose/.",
                        "uid": "v-siggraph-5",
                        "file_name": "",
                        "time_stamp": "2022-10-19T16:33:00Z",
                        "time_start": "2022-10-19T16:33:00Z",
                        "time_end": "2022-10-19T16:43:00Z",
                        "paper_type": "full",
                        "keywords": "Character posing, rigged and skinned characters, sketch-based posing, character sketches",
                        "has_image": "1",
                        "has_video": "458",
                        "paper_award": "",
                        "image_caption": "Given a single natural bitmap sketch of a character, our learning-based\napproach allows to automatically, with no additional input, recover the 3D pose\nconsistent with the viewer expectation. This pose can be then automatically\ncopied a custom rigged and skinned 3D character using standard retargeting\ntools",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-siggraph-5-qa",
                        "session_id": "sig1",
                        "type": "Virtual Q+A",
                        "title": "Sketch2Pose: estimating a 3D character pose from a bitmap sketch (Q+A)",
                        "contributors": [
                            "Kirill Brodt"
                        ],
                        "authors": [],
                        "abstract": "Artists frequently capture character poses via raster sketches, then use these drawings as a reference while posing a 3D character in a specialized 3D software --- a time-consuming process, requiring specialized 3D training and mental effort. We tackle this challenge by proposing the first system for automatically inferring a 3D character pose from a single bitmap sketch, producing poses consistent with viewer expectations. Algorithmically interpreting bitmap sketches is challenging, as they contain significantly distorted proportions and foreshortening. We address this by predicting three key elements of a drawing, necessary to disambiguate the drawn poses: 2D bone tangents, self-contacts, and bone foreshortening. These elements are then leveraged in an optimization inferring the 3D character pose consistent with the artist's intent. Our optimization balances cues derived from artistic literature and perception research to compensate for distorted character proportions. We demonstrate a gallery of results on sketches of numerous styles. We validate our method via numerical evaluations, user studies, and comparisons to manually posed characters and previous work. Code and data for our paper are available at http://www-labs.iro.umontreal.ca/bmpix/sketch2pose/.",
                        "uid": "v-siggraph-5",
                        "file_name": "",
                        "time_stamp": "2022-10-19T16:43:00Z",
                        "time_start": "2022-10-19T16:43:00Z",
                        "time_end": "2022-10-19T16:45:00Z",
                        "paper_type": "full",
                        "keywords": "Character posing, rigged and skinned characters, sketch-based posing, character sketches",
                        "has_image": "1",
                        "has_video": "458",
                        "paper_award": "",
                        "image_caption": "Given a single natural bitmap sketch of a character, our learning-based\napproach allows to automatically, with no additional input, recover the 3D pose\nconsistent with the viewer expectation. This pose can be then automatically\ncopied a custom rigged and skinned 3D character using standard retargeting\ntools",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-siggraph-6-pres",
                        "session_id": "sig1",
                        "type": "Virtual Presentation (pre-recorded)",
                        "title": "Spelunking the Deep: Guaranteed Queries on General Neural Implicit Surfaces",
                        "contributors": [
                            "Nicholas Sharp"
                        ],
                        "authors": [
                            "Nicholas Sharp",
                            "Alec Jacobson"
                        ],
                        "abstract": "Neural implicit representations, which encode a surface as the level set of a neural network applied to spatial coordinates, have proven to be remarkably effective for optimizing, compressing, and generating 3D geometry. Although these representations are easy to fit, it is not clear how to best evaluate geometric queries on the shape, such as intersecting against a ray or finding a closest point. The predominant approach is to encourage the network to have a signed distance property. However, this property typically holds only approximately, leading to robustness issues, and holds only at the conclusion of training, inhibiting the use of queries in loss functions. Instead, this work presents a new approach to perform queries directly on general neural implicit functions for a wide range of existing architectures. Our key tool is the application of range analysis to neural networks, using automatic arithmetic rules to bound the output of a network over a region; we conduct a study of range analysis on neural networks, and identify variants of affine arithmetic which are highly effective. We use the resulting bounds to develop geometric queries including ray casting, intersection testing, constructing spatial hierarchies, fast mesh extraction, closest-point evaluation, evaluating bulk properties, and more. Our queries can be efficiently evaluated on GPUs, and offer concrete accuracy guarantees even on randomly-initialized networks, enabling their use in training objectives and beyond. We also show a preliminary application to inverse rendering.",
                        "uid": "v-siggraph-6",
                        "file_name": "",
                        "time_stamp": "2022-10-19T16:45:00Z",
                        "time_start": "2022-10-19T16:45:00Z",
                        "time_end": "2022-10-19T16:55:00Z",
                        "paper_type": "full",
                        "keywords": "implicit surfaces, neural networks, range analysis, geometry processing",
                        "has_image": "1",
                        "has_video": "718",
                        "paper_award": "",
                        "image_caption": "This method enables geometric queries on neural implicit surfaces, without relying on fitting a signed distance function. Several queries are shown here on a neural implicit occupancy function encoding a mine cart. These operations open up new explorations of neural implicit surfaces.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-siggraph-6-qa",
                        "session_id": "sig1",
                        "type": "Virtual Q+A",
                        "title": "Spelunking the Deep: Guaranteed Queries on General Neural Implicit Surfaces (Q+A)",
                        "contributors": [
                            "Nicholas Sharp"
                        ],
                        "authors": [],
                        "abstract": "Neural implicit representations, which encode a surface as the level set of a neural network applied to spatial coordinates, have proven to be remarkably effective for optimizing, compressing, and generating 3D geometry. Although these representations are easy to fit, it is not clear how to best evaluate geometric queries on the shape, such as intersecting against a ray or finding a closest point. The predominant approach is to encourage the network to have a signed distance property. However, this property typically holds only approximately, leading to robustness issues, and holds only at the conclusion of training, inhibiting the use of queries in loss functions. Instead, this work presents a new approach to perform queries directly on general neural implicit functions for a wide range of existing architectures. Our key tool is the application of range analysis to neural networks, using automatic arithmetic rules to bound the output of a network over a region; we conduct a study of range analysis on neural networks, and identify variants of affine arithmetic which are highly effective. We use the resulting bounds to develop geometric queries including ray casting, intersection testing, constructing spatial hierarchies, fast mesh extraction, closest-point evaluation, evaluating bulk properties, and more. Our queries can be efficiently evaluated on GPUs, and offer concrete accuracy guarantees even on randomly-initialized networks, enabling their use in training objectives and beyond. We also show a preliminary application to inverse rendering.",
                        "uid": "v-siggraph-6",
                        "file_name": "",
                        "time_stamp": "2022-10-19T16:55:00Z",
                        "time_start": "2022-10-19T16:55:00Z",
                        "time_end": "2022-10-19T16:57:00Z",
                        "paper_type": "full",
                        "keywords": "implicit surfaces, neural networks, range analysis, geometry processing",
                        "has_image": "1",
                        "has_video": "718",
                        "paper_award": "",
                        "image_caption": "This method enables geometric queries on neural implicit surfaces, without relying on fitting a signed distance function. Several queries are shown here on a neural implicit occupancy function encoding a mine cart. These operations open up new explorations of neural implicit surfaces.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    }
                ]
            }
        ]
    },
    "v-vr": {
        "event": "VR Invited Partnership Presentations",
        "long_name": "VR Invited Partnership Presentations",
        "event_type": "Invited Partnership Presentations",
        "event_prefix": "v-vr",
        "event_description": "",
        "event_url": "",
        "organizers": [],
        "sessions": [
            {
                "title": "VR Invited Talks",
                "session_id": "vr1",
                "event_prefix": "v-vr",
                "track": "mistletoe",
                "livestream_id": "mistletoe-thu",
                "session_image": "vr1.png",
                "chair": [
                    "David Laidlaw"
                ],
                "organizers": [],
                "time_start": "2022-10-20T19:00:00Z",
                "time_end": "2022-10-20T20:15:00Z",
                "discord_category": "",
                "discord_channel": "",
                "discord_channel_id": "",
                "discord_link": "",
                "slido_link": "",
                "youtube_url": "",
                "zoom_meeting": "",
                "zoom_password": "",
                "zoom_link": "",
                "ff_link": "",
                "ff_playlist": "",
                "time_slots": [
                    {
                        "slot_id": "vr1-opening",
                        "session_id": "vr1",
                        "type": "In Person Other",
                        "title": "Session Opening",
                        "contributors": [
                            "David Laidlaw"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "",
                        "file_name": "",
                        "time_stamp": "2022-10-20T19:00:00Z",
                        "time_start": "2022-10-20T19:00:00Z",
                        "time_end": "2022-10-20T19:00:00Z",
                        "paper_type": "",
                        "keywords": "",
                        "has_image": false,
                        "has_video": "",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-vr-9714117-pres",
                        "session_id": "vr1",
                        "type": "Virtual Presentation (pre-recorded)",
                        "title": "Breaking Plausibility Without Breaking Presence - Evidence For The Multi-Layer Nature Of Plausibility",
                        "contributors": [
                            "Franziska Westermeier"
                        ],
                        "authors": [
                            "Larissa Br\u00fcbach",
                            "Franziska Westermeier",
                            "Carolin Wienrich",
                            "Marc Erich Latoschik"
                        ],
                        "abstract": "A novel theoretical model recently introduced coherence and plausibility as the essential conditions of XR experiences, challenging contemporary presence-oriented concepts. This article reports on two experiments validating this model, which assumes coherence activation on three layers (cognition, perception, and sensation) as the potential sources leading to a condition of plausibility and from there to other XR qualia such as presence or body ownership. The experiments introduce and utilize breaks in plausibility (in analogy to breaks in presence): We induce incoherence on the perceptual and the cognitive layer simultaneously by a simulation of object behaviors that do not conform to the laws of physics, i.e., gravity. We show that this manipulation breaks plausibility and hence confirm that it results in the desired effects in the theorized condition space but that the breaks in plausibility did not affect presence. In addition, we show that a cognitive manipulation by a storyline framing is too weak to successfully counteract the strong bottom-up inconsistencies. Both results are in line with the predictions of the recently introduced three-layer model of coherence and plausibility, which incorporates well-known top-down and bottom-up rivalries and its theorized increased independence between plausibility and presence.",
                        "uid": "v-vr-9714117",
                        "file_name": "",
                        "time_stamp": "2022-10-20T19:00:00Z",
                        "time_start": "2022-10-20T19:00:00Z",
                        "time_end": "2022-10-20T19:10:00Z",
                        "paper_type": "full",
                        "keywords": "A novel theoretical model recently introduced coherence and plausibility as the essential conditions of XR experiences, challenging contemporary presence-oriented concepts. This article reports on two experiments validating this model, which assumes coherence activation on three layers (cognition, perception, and sensation) as the potential sources leading to a condition of plausibility and from there to other XR qualia such as presence or body ownership. The experiments introduce and utilize breaks in plausibility (in analogy to breaks in presence): We induce incoherence on the perceptual and the cognitive layer simultaneously by a simulation of object behaviors that do not conform to the laws of physics, i.e., gravity. We show that this manipulation breaks plausibility and hence confirm that it results in the desired effects in the theorized condition space but that the breaks in plausibility did not affect presence. In addition, we show that a cognitive manipulation by a storyline framing is too weak to successfully counteract the strong bottom-up inconsistencies. Both results are in line with the predictions of the recently introduced three-layer model of coherence and plausibility, which incorporates well-known top-down and bottom-up rivalries and its theorized increased independence between plausibility and presence.",
                        "has_image": "1",
                        "has_video": "356",
                        "paper_award": "",
                        "image_caption": "Environment of the experiment showing the participant\u2019s interaction with the circuit breakers in order to fix the ship after the crash.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-vr-9714117-qa",
                        "session_id": "vr1",
                        "type": "Virtual Q+A",
                        "title": "Breaking Plausibility Without Breaking Presence - Evidence For The Multi-Layer Nature Of Plausibility (Q+A)",
                        "contributors": [
                            "Franziska Westermeier"
                        ],
                        "authors": [],
                        "abstract": "A novel theoretical model recently introduced coherence and plausibility as the essential conditions of XR experiences, challenging contemporary presence-oriented concepts. This article reports on two experiments validating this model, which assumes coherence activation on three layers (cognition, perception, and sensation) as the potential sources leading to a condition of plausibility and from there to other XR qualia such as presence or body ownership. The experiments introduce and utilize breaks in plausibility (in analogy to breaks in presence): We induce incoherence on the perceptual and the cognitive layer simultaneously by a simulation of object behaviors that do not conform to the laws of physics, i.e., gravity. We show that this manipulation breaks plausibility and hence confirm that it results in the desired effects in the theorized condition space but that the breaks in plausibility did not affect presence. In addition, we show that a cognitive manipulation by a storyline framing is too weak to successfully counteract the strong bottom-up inconsistencies. Both results are in line with the predictions of the recently introduced three-layer model of coherence and plausibility, which incorporates well-known top-down and bottom-up rivalries and its theorized increased independence between plausibility and presence.",
                        "uid": "v-vr-9714117",
                        "file_name": "",
                        "time_stamp": "2022-10-20T19:10:00Z",
                        "time_start": "2022-10-20T19:10:00Z",
                        "time_end": "2022-10-20T19:12:00Z",
                        "paper_type": "full",
                        "keywords": "A novel theoretical model recently introduced coherence and plausibility as the essential conditions of XR experiences, challenging contemporary presence-oriented concepts. This article reports on two experiments validating this model, which assumes coherence activation on three layers (cognition, perception, and sensation) as the potential sources leading to a condition of plausibility and from there to other XR qualia such as presence or body ownership. The experiments introduce and utilize breaks in plausibility (in analogy to breaks in presence): We induce incoherence on the perceptual and the cognitive layer simultaneously by a simulation of object behaviors that do not conform to the laws of physics, i.e., gravity. We show that this manipulation breaks plausibility and hence confirm that it results in the desired effects in the theorized condition space but that the breaks in plausibility did not affect presence. In addition, we show that a cognitive manipulation by a storyline framing is too weak to successfully counteract the strong bottom-up inconsistencies. Both results are in line with the predictions of the recently introduced three-layer model of coherence and plausibility, which incorporates well-known top-down and bottom-up rivalries and its theorized increased independence between plausibility and presence.",
                        "has_image": "1",
                        "has_video": "356",
                        "paper_award": "",
                        "image_caption": "Environment of the experiment showing the participant\u2019s interaction with the circuit breakers in order to fix the ship after the crash.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-vr-9756791-pres",
                        "session_id": "vr1",
                        "type": "Virtual Presentation (pre-recorded)",
                        "title": "Combining Real-World Constraints on User Behavior with Deep Neural Networks for Virtual Reality (VR) Biometrics",
                        "contributors": [
                            "Sean Banerjee"
                        ],
                        "authors": [
                            "Robert Miller",
                            "Natasha Kholgade Banerjee",
                            "Sean Banerjee"
                        ],
                        "abstract": "Deep networks have demonstrated enormous potential for identification and authentication using behavioral biometrics in virtual reality (VR). However, existing VR behavioral biometrics datasets have small sample sizes which can make it challenging for deep networks to automatically learn features that characterize real-world user behavior and that may enable high success, e.g., high-level spatial relationships between headset and hand controller devices and underlying smoothness of trajectories despite noise. We provide an approach to perform behavioral biometrics using deep networks while incorporating spatial and smoothing constraints on input data to represent real-world behavior. We represent the input data to neural networks as a combination of scale- and translation-invariant device-centric position and orientation features, and displacement vectors representing spatial relationships between device pairs. We assess identification and authentication by including spatial relationships and by performing Gaussian smoothing of the position features. We evaluate our approach against baseline methods that use the raw data directly and that perform a global normalization of the data. By using displacement vectors, our work shows higher success over baseline methods in 36 out of 42 cases of analysis done by varying user sets and pairings of VR systems and sessions.",
                        "uid": "v-vr-9756791",
                        "file_name": "",
                        "time_stamp": "2022-10-20T19:12:00Z",
                        "time_start": "2022-10-20T19:12:00Z",
                        "time_end": "2022-10-20T19:22:00Z",
                        "paper_type": "full",
                        "keywords": "Virtual reality; Biometrics",
                        "has_image": "1",
                        "has_video": "416",
                        "paper_award": "",
                        "image_caption": "We provide user identification and authentication using behavioral biometrics in virtual reality by augmenting orientation and normalized position features, expressed within the local coordinate systems of the hand controllers and headset devices, with inter-device displacement vectors. We demonstrate that using inter-device displacement vectors provides maximum success rate more often than baseline methods.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-vr-9756791-qa",
                        "session_id": "vr1",
                        "type": "Virtual Q+A",
                        "title": "Combining Real-World Constraints on User Behavior with Deep Neural Networks for Virtual Reality (VR) Biometrics (Q+A)",
                        "contributors": [
                            "Sean Banerjee"
                        ],
                        "authors": [],
                        "abstract": "Deep networks have demonstrated enormous potential for identification and authentication using behavioral biometrics in virtual reality (VR). However, existing VR behavioral biometrics datasets have small sample sizes which can make it challenging for deep networks to automatically learn features that characterize real-world user behavior and that may enable high success, e.g., high-level spatial relationships between headset and hand controller devices and underlying smoothness of trajectories despite noise. We provide an approach to perform behavioral biometrics using deep networks while incorporating spatial and smoothing constraints on input data to represent real-world behavior. We represent the input data to neural networks as a combination of scale- and translation-invariant device-centric position and orientation features, and displacement vectors representing spatial relationships between device pairs. We assess identification and authentication by including spatial relationships and by performing Gaussian smoothing of the position features. We evaluate our approach against baseline methods that use the raw data directly and that perform a global normalization of the data. By using displacement vectors, our work shows higher success over baseline methods in 36 out of 42 cases of analysis done by varying user sets and pairings of VR systems and sessions.",
                        "uid": "v-vr-9756791",
                        "file_name": "",
                        "time_stamp": "2022-10-20T19:22:00Z",
                        "time_start": "2022-10-20T19:22:00Z",
                        "time_end": "2022-10-20T19:24:00Z",
                        "paper_type": "full",
                        "keywords": "Virtual reality; Biometrics",
                        "has_image": "1",
                        "has_video": "416",
                        "paper_award": "",
                        "image_caption": "We provide user identification and authentication using behavioral biometrics in virtual reality by augmenting orientation and normalized position features, expressed within the local coordinate systems of the hand controllers and headset devices, with inter-device displacement vectors. We demonstrate that using inter-device displacement vectors provides maximum success rate more often than baseline methods.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-vr-9756796-pres",
                        "session_id": "vr1",
                        "type": "Virtual Presentation (pre-recorded)",
                        "title": "Real-Time Gaze Tracking with Event-Driven Eye Segmentation",
                        "contributors": [
                            "Feng Yu"
                        ],
                        "authors": [
                            "Yu Feng",
                            "Nathan Goulding-Hotta",
                            "Asif Khan",
                            "Hans Reyserhove",
                            "Yuhao Zhu"
                        ],
                        "abstract": "Gaze tracking is increasingly becoming an essential component in Augmented and Virtual Reality. Modern gaze tracking algorithms are heavyweight; they operate at most 5 Hz on mobile processors despite that near-eye cameras comfortably operate at a real-time rate (> 30 Hz). This paper presents a real-time eye tracking algorithm that, on average, operates at 30 Hz on a mobile processor, achieves 0.1\u00b0\u20130.5\u00b0 gaze accuracies, all the while requiring only 30K parameters, one to two orders of magnitude smaller than state-of-the-art eye tracking algorithms. The crux of our algorithm is an Auto ROI mode, which continuously predicts the Regions of Interest (ROIs) of near-eye images and judiciously processes only the ROIs for gaze estimation. To that end, we introduce a novel, lightweight ROI prediction algorithm by emulating an event camera. We discuss how a software emulation of events enables accurate ROI prediction without requiring special hardware. The code of our paper is available at https://github.com/horizon-research/edgaze.",
                        "uid": "v-vr-9756796",
                        "file_name": "",
                        "time_stamp": "2022-10-20T19:24:00Z",
                        "time_start": "2022-10-20T19:24:00Z",
                        "time_end": "2022-10-20T19:34:00Z",
                        "paper_type": "full",
                        "keywords": "Gaze, eye tracking, event camera, segmentation",
                        "has_image": "1",
                        "has_video": "536",
                        "paper_award": "",
                        "image_caption": "Event-Driven ROI-based eye segmentation.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-vr-9756796-qa",
                        "session_id": "vr1",
                        "type": "Virtual Q+A",
                        "title": "Real-Time Gaze Tracking with Event-Driven Eye Segmentation (Q+A)",
                        "contributors": [
                            "Feng Yu"
                        ],
                        "authors": [],
                        "abstract": "Gaze tracking is increasingly becoming an essential component in Augmented and Virtual Reality. Modern gaze tracking algorithms are heavyweight; they operate at most 5 Hz on mobile processors despite that near-eye cameras comfortably operate at a real-time rate (> 30 Hz). This paper presents a real-time eye tracking algorithm that, on average, operates at 30 Hz on a mobile processor, achieves 0.1\u00b0\u20130.5\u00b0 gaze accuracies, all the while requiring only 30K parameters, one to two orders of magnitude smaller than state-of-the-art eye tracking algorithms. The crux of our algorithm is an Auto ROI mode, which continuously predicts the Regions of Interest (ROIs) of near-eye images and judiciously processes only the ROIs for gaze estimation. To that end, we introduce a novel, lightweight ROI prediction algorithm by emulating an event camera. We discuss how a software emulation of events enables accurate ROI prediction without requiring special hardware. The code of our paper is available at https://github.com/horizon-research/edgaze.",
                        "uid": "v-vr-9756796",
                        "file_name": "",
                        "time_stamp": "2022-10-20T19:34:00Z",
                        "time_start": "2022-10-20T19:34:00Z",
                        "time_end": "2022-10-20T19:36:00Z",
                        "paper_type": "full",
                        "keywords": "Gaze, eye tracking, event camera, segmentation",
                        "has_image": "1",
                        "has_video": "536",
                        "paper_award": "",
                        "image_caption": "Event-Driven ROI-based eye segmentation.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-vr-9714118-pres",
                        "session_id": "vr1",
                        "type": "Virtual Presentation (pre-recorded)",
                        "title": "Mood-Driven Colorization of Virtual Indoor Scenes",
                        "contributors": [
                            "Michael Solah"
                        ],
                        "authors": [
                            "Michael S Solah",
                            "Haikun Huang",
                            "Jiachuan Sheng",
                            "Tian Feng",
                            "Marc Pomplun",
                            "Lap-Fai Yu"
                        ],
                        "abstract": "One of the challenging tasks in virtual scene design for Virtual Reality (VR) is causing it to invoke a particular mood in viewers. The subjective nature of moods brings uncertainty to the purpose. We propose a novel approach to automatic adjustment of the colors of textures for objects in a virtual indoor scene, enabling it to match a target mood. A dataset of 25,000 images, including building/home interiors, was used to train a classifier with the features extracted via deep learning. It contributes to an optimization process that colorizes virtual scenes automatically according to the target mood. Our approach was tested on four different indoor scenes, and we conducted a user study demonstrating its efficacy through statistical analysis with the focus on the impact of the scenes experienced with a VR headset.",
                        "uid": "v-vr-9714118",
                        "file_name": "",
                        "time_stamp": "2022-10-20T19:36:00Z",
                        "time_start": "2022-10-20T19:36:00Z",
                        "time_end": "2022-10-20T19:46:00Z",
                        "paper_type": "full",
                        "keywords": "Virtual reality; Perception; Visualization design and evaluation methods",
                        "has_image": "1",
                        "has_video": "634",
                        "paper_award": "",
                        "image_caption": "An image showing how our approach works. We take a virtual indoor environment and run our optimization process to colorize the textures of objects. The goal is for the colors in the environment to match a target mood. Our approach uses an optimization process with a classifier trained on a dataset of indoor images with features obtained through deep learning. This image shows a bedroom environment, with the input on the left side and the result on the right side. The input mood was cheerful. A person on the lower left is viewing the scene with a VR headset.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-vr-9714118-qa",
                        "session_id": "vr1",
                        "type": "Virtual Q+A",
                        "title": "Mood-Driven Colorization of Virtual Indoor Scenes (Q+A)",
                        "contributors": [
                            "Michael Solah"
                        ],
                        "authors": [],
                        "abstract": "One of the challenging tasks in virtual scene design for Virtual Reality (VR) is causing it to invoke a particular mood in viewers. The subjective nature of moods brings uncertainty to the purpose. We propose a novel approach to automatic adjustment of the colors of textures for objects in a virtual indoor scene, enabling it to match a target mood. A dataset of 25,000 images, including building/home interiors, was used to train a classifier with the features extracted via deep learning. It contributes to an optimization process that colorizes virtual scenes automatically according to the target mood. Our approach was tested on four different indoor scenes, and we conducted a user study demonstrating its efficacy through statistical analysis with the focus on the impact of the scenes experienced with a VR headset.",
                        "uid": "v-vr-9714118",
                        "file_name": "",
                        "time_stamp": "2022-10-20T19:46:00Z",
                        "time_start": "2022-10-20T19:46:00Z",
                        "time_end": "2022-10-20T19:48:00Z",
                        "paper_type": "full",
                        "keywords": "Virtual reality; Perception; Visualization design and evaluation methods",
                        "has_image": "1",
                        "has_video": "634",
                        "paper_award": "",
                        "image_caption": "An image showing how our approach works. We take a virtual indoor environment and run our optimization process to colorize the textures of objects. The goal is for the colors in the environment to match a target mood. Our approach uses an optimization process with a classifier trained on a dataset of indoor images with features obtained through deep learning. This image shows a bedroom environment, with the input on the left side and the result on the right side. The input mood was cheerful. A person on the lower left is viewing the scene with a VR headset.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-vr-9714040-pres",
                        "session_id": "vr1",
                        "type": "Virtual Presentation (pre-recorded)",
                        "title": "Omnidirectional Galvanic Vestibular Stimulation in Virtual Reality",
                        "contributors": [
                            "Colin Groth"
                        ],
                        "authors": [
                            "Colin Groth",
                            "Jan-Philipp Tauscher",
                            "Nikkel Heesen",
                            "Max Hattenbach",
                            "Susana Castillo",
                            "Marcus Magnor"
                        ],
                        "abstract": "In this paper we propose omnidirectional galvanic vestibular stimulation (GVS) to mitigate cybersickness in virtual reality applications. One of the most accepted theories indicates that Cybersickness is caused by the visually induced impression of ego motion while physically remaining at rest. As a result of this sensory mismatch, people associate negative symptoms with VR and sometimes avoid the technology altogether. To reconcile the two contradicting sensory perceptions, we investigate GVS to stimulate the vestibular canals behind our ears with low-current electrical signals that are specifically attuned to the visually displayed camera motion. We describe how to calibrate and generate the appropriate GVS signals in real-time for pre-recorded omnidirectional videos exhibiting ego-motion in all three spatial directions. For validation, we conduct an experiment presenting real-world 360\u00b0 videos shot from a moving first-person perspective in a VR head-mounted display. Our findings indicate that GVS is able to significantly reduce discomfort for cybersickness-susceptible VR users, creating a deeper and more enjoyable immersive experience for many people.",
                        "uid": "v-vr-9714040",
                        "file_name": "",
                        "time_stamp": "2022-10-20T19:48:00Z",
                        "time_start": "2022-10-20T19:48:00Z",
                        "time_end": "2022-10-20T19:58:00Z",
                        "paper_type": "full",
                        "keywords": "Galvanic Vestibular Stimulation, GVS, Virtual Reality, VR, 360 Videos, Cybersickness, Presence",
                        "has_image": "1",
                        "has_video": "633",
                        "paper_award": "",
                        "image_caption": "To reduce cybersickness for moving-camera sequences in VR, we evaluate the effectiveness of galvanic vestibular stimulation. We stimulate the VR user's vestibular sense in all three spatial directions taking the motion of the camera in the 360\u00b0 video as well as the user\u2019s current viewing direction into account. This way, we aim to reconcile visually induced and felt self-motion.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-vr-9714040-qa",
                        "session_id": "vr1",
                        "type": "Virtual Q+A",
                        "title": "Omnidirectional Galvanic Vestibular Stimulation in Virtual Reality (Q+A)",
                        "contributors": [
                            "Colin Groth"
                        ],
                        "authors": [],
                        "abstract": "In this paper we propose omnidirectional galvanic vestibular stimulation (GVS) to mitigate cybersickness in virtual reality applications. One of the most accepted theories indicates that Cybersickness is caused by the visually induced impression of ego motion while physically remaining at rest. As a result of this sensory mismatch, people associate negative symptoms with VR and sometimes avoid the technology altogether. To reconcile the two contradicting sensory perceptions, we investigate GVS to stimulate the vestibular canals behind our ears with low-current electrical signals that are specifically attuned to the visually displayed camera motion. We describe how to calibrate and generate the appropriate GVS signals in real-time for pre-recorded omnidirectional videos exhibiting ego-motion in all three spatial directions. For validation, we conduct an experiment presenting real-world 360\u00b0 videos shot from a moving first-person perspective in a VR head-mounted display. Our findings indicate that GVS is able to significantly reduce discomfort for cybersickness-susceptible VR users, creating a deeper and more enjoyable immersive experience for many people.",
                        "uid": "v-vr-9714040",
                        "file_name": "",
                        "time_stamp": "2022-10-20T19:58:00Z",
                        "time_start": "2022-10-20T19:58:00Z",
                        "time_end": "2022-10-20T20:00:00Z",
                        "paper_type": "full",
                        "keywords": "Galvanic Vestibular Stimulation, GVS, Virtual Reality, VR, 360 Videos, Cybersickness, Presence",
                        "has_image": "1",
                        "has_video": "633",
                        "paper_award": "",
                        "image_caption": "To reduce cybersickness for moving-camera sequences in VR, we evaluate the effectiveness of galvanic vestibular stimulation. We stimulate the VR user's vestibular sense in all three spatial directions taking the motion of the camera in the 360\u00b0 video as well as the user\u2019s current viewing direction into account. This way, we aim to reconcile visually induced and felt self-motion.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-vr-9714044-pres",
                        "session_id": "vr1",
                        "type": "Virtual Presentation (pre-recorded)",
                        "title": "Comparing Direct and Indirect Methods of Audio Quality Evaluation in Virtual Reality Scenes of Varying Complexity",
                        "contributors": [
                            "Thomas Robotham"
                        ],
                        "authors": [
                            "Thomas Robotham",
                            "Olli S. Rummukainen",
                            "Miriam Kurz",
                            "Marie Eckert",
                            "Emanu\u00ebl A. P. Habets"
                        ],
                        "abstract": "Many quality evaluation methods are used to assess uni-modal audio or video content without considering perceptual, cognitive, and interactive aspects present in virtual reality (VR) settings. Consequently, little is known regarding the repercussions of the employed evaluation method, content, and subject behavior on the quality ratings in VR. This mixed between- and within-subjects study uses four subjective audio quality evaluation methods (viz. multiple-stimulus with and without reference for direct scaling, and rank-order elimination and pairwise comparison for indirect scaling) to investigate the contributing factors present in multi-modal 6-DoF VR on quality ratings of real-time audio rendering. For each between-subjects employed method, two sets of conditions in five VR scenes were evaluated within-subjects. The conditions targeted relevant attributes for binaural audio reproduction using scenes with various amounts of user interactivity. Our results show all referenceless methods produce similar results using both condition sets. However, rank-order elimination proved to be the fastest method, required the least amount of repetitive motion, and yielded the highest discrimination between spatial conditions. Scene complexity was found to be a main effect within results, with behavioral and task load index results implying more complex scenes and interactive aspects of 6-DoF VR can impede quality judgments.",
                        "uid": "v-vr-9714044",
                        "file_name": "",
                        "time_stamp": "2022-10-20T20:00:00Z",
                        "time_start": "2022-10-20T20:00:00Z",
                        "time_end": "2022-10-20T20:10:00Z",
                        "paper_type": "full",
                        "keywords": "Multi-modal, virtual reality, 6-Degrees-of-freedom, audio quality, direct scaling, indirect scaling, evaluation methods",
                        "has_image": "1",
                        "has_video": "418",
                        "paper_award": "",
                        "image_caption": "Perceptual study focusing on evaluation methods in multi-modal interactive virtual environment settings with different levels of complexity.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    },
                    {
                        "slot_id": "v-vr-9714044-qa",
                        "session_id": "vr1",
                        "type": "Virtual Q+A",
                        "title": "Comparing Direct and Indirect Methods of Audio Quality Evaluation in Virtual Reality Scenes of Varying Complexity (Q+A)",
                        "contributors": [
                            "Thomas Robotham"
                        ],
                        "authors": [],
                        "abstract": "Many quality evaluation methods are used to assess uni-modal audio or video content without considering perceptual, cognitive, and interactive aspects present in virtual reality (VR) settings. Consequently, little is known regarding the repercussions of the employed evaluation method, content, and subject behavior on the quality ratings in VR. This mixed between- and within-subjects study uses four subjective audio quality evaluation methods (viz. multiple-stimulus with and without reference for direct scaling, and rank-order elimination and pairwise comparison for indirect scaling) to investigate the contributing factors present in multi-modal 6-DoF VR on quality ratings of real-time audio rendering. For each between-subjects employed method, two sets of conditions in five VR scenes were evaluated within-subjects. The conditions targeted relevant attributes for binaural audio reproduction using scenes with various amounts of user interactivity. Our results show all referenceless methods produce similar results using both condition sets. However, rank-order elimination proved to be the fastest method, required the least amount of repetitive motion, and yielded the highest discrimination between spatial conditions. Scene complexity was found to be a main effect within results, with behavioral and task load index results implying more complex scenes and interactive aspects of 6-DoF VR can impede quality judgments.",
                        "uid": "v-vr-9714044",
                        "file_name": "",
                        "time_stamp": "2022-10-20T20:10:00Z",
                        "time_start": "2022-10-20T20:10:00Z",
                        "time_end": "2022-10-20T20:12:00Z",
                        "paper_type": "full",
                        "keywords": "Multi-modal, virtual reality, 6-Degrees-of-freedom, audio quality, direct scaling, indirect scaling, evaluation methods",
                        "has_image": "1",
                        "has_video": "418",
                        "paper_award": "",
                        "image_caption": "Perceptual study focusing on evaluation methods in multi-modal interactive virtual environment settings with different levels of complexity.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": ""
                    }
                ]
            }
        ]
    },
    "v-panels1": {
        "event": "Grand Challenges in Visual Analytic Systems",
        "long_name": "Grand Challenges in Visual Analytic Systems",
        "event_type": "Panel",
        "event_prefix": "v-panels1",
        "event_description": "",
        "event_url": "",
        "organizers": [
            "Aoyu Wu",
            "Dazhen Deng",
            "Min Chen",
            "Shixia Liu",
            "Daniel Keim",
            "Ross Maciejewski",
            "Silvia Miksch",
            "Hendrik Strobelt"
        ],
        "sessions": []
    },
    "v-panels2": {
        "event": "Merits and Limits of User Study Preregistration",
        "long_name": "Merits and Limits of User Study Preregistration",
        "event_type": "Panel",
        "event_prefix": "v-panels2",
        "event_description": "",
        "event_url": "",
        "organizers": [
            "Brian Nosek",
            "Tamarinde Haven",
            "Miriah Meyer",
            "Lonni Besan\u00e7on",
            "Cody Dunne",
            "Mohammad Ghoniem"
        ],
        "sessions": []
    },
    "v-panels3": {
        "event": "Is This (Panel) Good Enough for IEEE VIS?",
        "long_name": "Is This (Panel) Good Enough for IEEE VIS?",
        "event_type": "Panel",
        "event_prefix": "v-panels3",
        "event_description": "",
        "event_url": "",
        "organizers": [
            "Cody Dunne",
            "Alexander Lex",
            "Torsten M\u00f6ller",
            "Alvitta Ottley",
            "Melanie Tory",
            "Robert Laramee",
            "Petra Isenberg",
            "Tobias Isenberg"
        ],
        "sessions": []
    },
    "a-vizsec": {
        "event": "VizSec",
        "long_name": "VizSec",
        "event_type": "Associated Event (Symposium)",
        "event_prefix": "a-vizsec",
        "event_description": "",
        "event_url": "",
        "organizers": [
            "Chris Bryan",
            "Steven Gomez",
            "Sanchari Das",
            "Lyndsey R. Franklin",
            "Fabian B\u00f6hm",
            "No\u00eblle Rakotondravony",
            "Alex Ulmer"
        ],
        "sessions": [
            {
                "title": "VizSec Symposium",
                "session_id": "vizsec",
                "event_prefix": "a-vizsec",
                "track": "pinon",
                "livestream_id": "pinon-wed",
                "session_image": "vizsec.png",
                "chair": [],
                "organizers": [],
                "time_start": "2022-10-19T14:00:00Z",
                "time_end": "2022-10-19T15:15:00Z",
                "discord_category": "",
                "discord_channel": "",
                "discord_channel_id": "",
                "discord_link": "",
                "slido_link": "",
                "youtube_url": "",
                "zoom_meeting": "",
                "zoom_password": "",
                "zoom_link": "",
                "ff_link": "",
                "ff_playlist": "",
                "time_slots": []
            }
        ]
    },
    "a-visap": {
        "event": "VIS Arts Program",
        "long_name": "VIS Arts Program",
        "event_type": "Associated Event (Arts)",
        "event_prefix": "a-visap",
        "event_description": "",
        "event_url": "",
        "organizers": [
            "Charles Perin",
            "Uta Hinrichs",
            "Rebecca Ruige Xu",
            "Peter Froslie",
            "Bon Adriel Aseniero",
            "Tommaso Elli",
            "Till Nagel",
            "Maria Lantin",
            "Yoon Chung Han"
        ],
        "sessions": [
            {
                "title": "VISAP Session 1",
                "session_id": "visap1",
                "event_prefix": "a-visap",
                "track": "mistletoe",
                "livestream_id": "mistletoe-wed",
                "session_image": "visap1.png",
                "chair": [],
                "organizers": [],
                "time_start": "2022-10-19T14:00:00Z",
                "time_end": "2022-10-19T15:15:00Z",
                "discord_category": "",
                "discord_channel": "",
                "discord_channel_id": "",
                "discord_link": "",
                "slido_link": "",
                "youtube_url": "",
                "zoom_meeting": "",
                "zoom_password": "",
                "zoom_link": "",
                "ff_link": "",
                "ff_playlist": "",
                "time_slots": []
            },
            {
                "title": "VISAP Session 2",
                "session_id": "visap2",
                "event_prefix": "a-visap",
                "track": "mistletoe",
                "livestream_id": "mistletoe-thu",
                "session_image": "visap2.png",
                "chair": [],
                "organizers": [],
                "time_start": "2022-10-20T14:00:00Z",
                "time_end": "2022-10-20T15:15:00Z",
                "discord_category": "",
                "discord_channel": "",
                "discord_channel_id": "",
                "discord_link": "",
                "slido_link": "",
                "youtube_url": "",
                "zoom_meeting": "",
                "zoom_password": "",
                "zoom_link": "",
                "ff_link": "",
                "ff_playlist": "",
                "time_slots": []
            }
        ]
    },
    "v-spotlights1": {
        "event": "Data Analysis Methods for Climate Modeling of Extreme Weather Events",
        "long_name": "Data Analysis Methods for Climate Modeling of Extreme Weather Events",
        "event_type": "Application Spotlight",
        "event_prefix": "v-spotlights1",
        "event_description": "",
        "event_url": "",
        "organizers": [
            "Divya Banesh",
            "Ayan Biswas",
            "James Benedict",
            "Rupsa Bhowmick",
            "Soumya Dutta",
            "Michael Grosskopf"
        ],
        "sessions": []
    },
    "v-spotlights2": {
        "event": "Audio-Visual Analytics: Potential Applications of Combined Sonifications and Visualizations",
        "long_name": "Audio-Visual Analytics: Potential Applications of Combined Sonifications and Visualizations",
        "event_type": "Application Spotlight",
        "event_prefix": "v-spotlights2",
        "event_description": "",
        "event_url": "",
        "organizers": [
            "Wolfgang Aigner",
            "Michael Iber",
            "Kajetan Enge",
            "Alexander Rind",
            "Niklas Elmqvist",
            "Robert H\u00f6ldrich",
            "Niklas R\u00f6nnberg",
            "Bruce Walker"
        ],
        "sessions": []
    },
    "v-spotlights3": {
        "event": "Application Papers: How should we deal with them?",
        "long_name": "Application Papers: How should we deal with them?",
        "event_type": "Application Spotlight",
        "event_prefix": "v-spotlights3",
        "event_description": "",
        "event_url": "",
        "organizers": [
            "Johanna Schmidt",
            "Daniel Wiegreffe",
            "Christina Gillmann"
        ],
        "sessions": []
    },
    "a-sciviscontest": {
        "event": "SciVis Contest",
        "long_name": "SciVis Contest",
        "event_type": "Associated Event (Competition)",
        "event_prefix": "a-sciviscontest",
        "event_description": "",
        "event_url": "",
        "organizers": [
            "Alex Razoumov",
            "Divya Banesh"
        ],
        "sessions": [
            {
                "title": "SciVis Contest",
                "session_id": "contest",
                "event_prefix": "a-sciviscontest",
                "track": "pinon",
                "livestream_id": "pinon-thu",
                "session_image": "contest.png",
                "chair": [],
                "organizers": [],
                "time_start": "2022-10-20T14:00:00Z",
                "time_end": "2022-10-20T15:15:00Z",
                "discord_category": "",
                "discord_channel": "",
                "discord_channel_id": "",
                "discord_link": "",
                "slido_link": "",
                "youtube_url": "",
                "zoom_meeting": "",
                "zoom_password": "",
                "zoom_link": "",
                "ff_link": "",
                "ff_playlist": "",
                "time_slots": []
            }
        ]
    },
    "a-ldav": {
        "event": "LDAV: 12th IEEE Symposium on Large Data Analysis and Visualization",
        "long_name": "LDAV: 12th IEEE Symposium on Large Data Analysis and Visualization",
        "event_type": "Associated Event (Symposium)",
        "event_prefix": "a-ldav",
        "event_description": "",
        "event_url": "",
        "organizers": [
            "Chaoli Wang",
            "Peer-Timo Bremer",
            "Kristi Potter"
        ],
        "sessions": [
            {
                "title": "LDAV: 12th IEEE Symposium on Large Data Analysis and Visualization",
                "session_id": "ae1",
                "event_prefix": "a-ldav",
                "track": "ok4",
                "livestream_id": "ok4-sun",
                "session_image": "ae1.png",
                "chair": [
                    "Chaoli Wang",
                    "Peer-Timo Bremer",
                    "Kristi Potter"
                ],
                "organizers": [],
                "time_start": "2022-10-16T14:00:00Z",
                "time_end": "2022-10-16T22:00:00Z",
                "discord_category": "",
                "discord_channel": "",
                "discord_channel_id": "",
                "discord_link": "",
                "slido_link": "",
                "youtube_url": "",
                "zoom_meeting": "",
                "zoom_password": "",
                "zoom_link": "",
                "ff_link": "",
                "ff_playlist": "",
                "time_slots": []
            }
        ]
    },
    "w-beliv": {
        "event": "BELIV: 9th Workshop on evaluation and BEyond - methodoLogIcal approaches for Visualization",
        "long_name": "BELIV: 9th Workshop on evaluation and BEyond - methodoLogIcal approaches for Visualization",
        "event_type": "Associated Event (Workshop)",
        "event_prefix": "w-beliv",
        "event_description": "",
        "event_url": "",
        "organizers": [
            "Anastasia Bezerianos",
            "Kyle Hall",
            "Samuel Huron",
            "Matthew Kay",
            "Miriah Meyer"
        ],
        "sessions": [
            {
                "title": "BELIV: 9th Workshop on evaluation and BEyond - methodoLogIcal approaches for Visualization",
                "session_id": "ae3",
                "event_prefix": "w-beliv",
                "track": "ok4",
                "livestream_id": "ok4-mon",
                "session_image": "ae3.png",
                "chair": [
                    "Anastasia Bezerianos",
                    "Kyle Hall",
                    "Samuel Huron",
                    "Matthew Kay",
                    "Miriah Meyer"
                ],
                "organizers": [],
                "time_start": "2022-10-17T14:00:00Z",
                "time_end": "2022-10-17T22:00:00Z",
                "discord_category": "",
                "discord_channel": "",
                "discord_channel_id": "",
                "discord_link": "",
                "slido_link": "",
                "youtube_url": "",
                "zoom_meeting": "",
                "zoom_password": "",
                "zoom_link": "",
                "ff_link": "",
                "ff_playlist": "",
                "time_slots": []
            }
        ]
    },
    "a-vds": {
        "event": "VDS: Visualization in Data Science Symposium",
        "long_name": "VDS: Visualization in Data Science Symposium",
        "event_type": "Associated Event (Symposium)",
        "event_prefix": "a-vds",
        "event_description": "",
        "event_url": "",
        "organizers": [
            "Liang Gou",
            "Alvitta Ottley",
            "Anamaria Crisan"
        ],
        "sessions": [
            {
                "title": "VDS: Visualization in Data Science Symposium",
                "session_id": "ae2",
                "event_prefix": "a-vds",
                "track": "ok5",
                "livestream_id": "ok5-sun",
                "session_image": "ae2.png",
                "chair": [
                    "Liang Gou",
                    "Alvitta Ottley",
                    "Anamaria Crisan"
                ],
                "organizers": [],
                "time_start": "2022-10-16T14:00:00Z",
                "time_end": "2022-10-16T17:00:00Z",
                "discord_category": "",
                "discord_channel": "",
                "discord_channel_id": "",
                "discord_link": "",
                "slido_link": "",
                "youtube_url": "",
                "zoom_meeting": "",
                "zoom_password": "",
                "zoom_link": "",
                "ff_link": "",
                "ff_playlist": "",
                "time_slots": []
            }
        ]
    },
    "a-biomedvischallenge": {
        "event": "Bio+MedVis Challenges",
        "long_name": "Bio+MedVis Challenges",
        "event_type": "Associated Event (Competition)",
        "event_prefix": "a-biomedvischallenge",
        "event_description": "",
        "event_url": "",
        "organizers": [
            "Thomas H\u00f6llt",
            "Zeynep Gumus",
            "Daniel J\u00f6nsson",
            "Renata Raidou"
        ],
        "sessions": [
            {
                "title": "Bio+MedVis Challenges",
                "session_id": "ae6",
                "event_prefix": "a-biomedvischallenge",
                "track": "ok5",
                "livestream_id": "ok5-sun",
                "session_image": "ae6.png",
                "chair": [
                    "Thomas H\u00f6llt",
                    "Zeynep Gumus",
                    "Daniel J\u00f6nsson",
                    "Renata Raidou"
                ],
                "organizers": [],
                "time_start": "2022-10-16T19:00:00Z",
                "time_end": "2022-10-16T22:00:00Z",
                "discord_category": "",
                "discord_channel": "",
                "discord_channel_id": "",
                "discord_link": "",
                "slido_link": "",
                "youtube_url": "",
                "zoom_meeting": "",
                "zoom_password": "",
                "zoom_link": "",
                "ff_link": "",
                "ff_playlist": "",
                "time_slots": []
            }
        ]
    },
    "w-biomedicalai": {
        "event": "Workshop on Visualization in BioMedical AI",
        "long_name": "Workshop on Visualization in BioMedical AI",
        "event_type": "Workshop",
        "event_prefix": "w-biomedicalai",
        "event_description": "",
        "event_url": "",
        "organizers": [
            "Qianwen Wang",
            "Vicky Yao",
            "Bum Chul Kwon",
            "Nils Gehlenborg"
        ],
        "sessions": [
            {
                "title": "Workshop on Visualization in BioMedical AI",
                "session_id": "w3",
                "event_prefix": "w-biomedicalai",
                "track": "ok5",
                "livestream_id": "ok5-mon",
                "session_image": "w3.png",
                "chair": [
                    "Qianwen Wang",
                    "Vicky Yao",
                    "Bum Chul Kwon",
                    "Nils Gehlenborg"
                ],
                "organizers": [],
                "time_start": "2022-10-17T14:00:00Z",
                "time_end": "2022-10-17T17:00:00Z",
                "discord_category": "",
                "discord_channel": "",
                "discord_channel_id": "",
                "discord_link": "",
                "slido_link": "",
                "youtube_url": "",
                "zoom_meeting": "",
                "zoom_password": "",
                "zoom_link": "",
                "ff_link": "",
                "ff_playlist": "",
                "time_slots": []
            }
        ]
    },
    "w-visxai": {
        "event": "VISxAI: 5th Workshop on Visualization for AI Explainability",
        "long_name": "VISxAI: 5th Workshop on Visualization for AI Explainability",
        "event_type": "Associated Event (Workshop)",
        "event_prefix": "w-visxai",
        "event_description": "",
        "event_url": "",
        "organizers": [
            "Adam Perer",
            "Angie Boggust",
            "Fred Hohman",
            "Hendrik Strobelt",
            "Mennatallah El-Assady",
            "Zijie Jay Wang"
        ],
        "sessions": [
            {
                "title": "VISxAI: 5th Workshop on Visualization for AI Explainability",
                "session_id": "ae5",
                "event_prefix": "w-visxai",
                "track": "ok5",
                "livestream_id": "ok5-mon",
                "session_image": "ae5.png",
                "chair": [
                    "Adam Perer",
                    "Angie Boggust",
                    "Fred Hohman",
                    "Hendrik Strobelt",
                    "Mennatallah El-Assady",
                    "Zijie Jay Wang"
                ],
                "organizers": [],
                "time_start": "2022-10-17T19:00:00Z",
                "time_end": "2022-10-17T22:00:00Z",
                "discord_category": "",
                "discord_channel": "",
                "discord_channel_id": "",
                "discord_link": "",
                "slido_link": "",
                "youtube_url": "",
                "zoom_meeting": "",
                "zoom_password": "",
                "zoom_link": "",
                "ff_link": "",
                "ff_playlist": "",
                "time_slots": []
            }
        ]
    },
    "w-vis4dh": {
        "event": "VIS4DH: 7th Workshop on Visualization for the Digital Humanities",
        "long_name": "VIS4DH: 7th Workshop on Visualization for the Digital Humanities",
        "event_type": "Associated Event (Workshop)",
        "event_prefix": "w-vis4dh",
        "event_description": "",
        "event_url": "",
        "organizers": [
            "Alfie Abdul-Rahman",
            "Houda Lamqaddam",
            "Chris Weaver"
        ],
        "sessions": [
            {
                "title": "VIS4DH: 7th Workshop on Visualization for the Digital Humanities",
                "session_id": "ae4",
                "event_prefix": "w-vis4dh",
                "track": "ok1",
                "livestream_id": "ok1-sun",
                "session_image": "ae4.png",
                "chair": [
                    "Alfie Abdul-Rahman",
                    "Houda Lamqaddam",
                    "Chris Weaver"
                ],
                "organizers": [],
                "time_start": "2022-10-16T14:00:00Z",
                "time_end": "2022-10-16T22:00:00Z",
                "discord_category": "",
                "discord_channel": "",
                "discord_channel_id": "",
                "discord_link": "",
                "slido_link": "",
                "youtube_url": "",
                "zoom_meeting": "",
                "zoom_password": "",
                "zoom_link": "",
                "ff_link": "",
                "ff_playlist": "",
                "time_slots": []
            }
        ]
    },
    "a-visinpractice": {
        "event": "VisInPractice",
        "long_name": "VisInPractice",
        "event_type": "Associated Event",
        "event_prefix": "a-visinpractice",
        "event_description": "",
        "event_url": "",
        "organizers": [
            "Anamaria Crisan",
            "Zhicheng (Leo) Liu",
            "Sudhanshu Kumar Semwal",
            "Chris Weaver"
        ],
        "sessions": [
            {
                "title": "VisInPractice",
                "session_id": "ae0",
                "event_prefix": "a-visinpractice",
                "track": "ok1",
                "livestream_id": "ok1-mon",
                "session_image": "ae0.png",
                "chair": [
                    "Anamaria Crisan",
                    "Zhicheng (Leo) Liu",
                    "Sudhanshu Kumar Semwal",
                    "Chris Weaver"
                ],
                "organizers": [],
                "time_start": "2022-10-17T14:00:00Z",
                "time_end": "2022-10-17T22:00:00Z",
                "discord_category": "",
                "discord_channel": "",
                "discord_channel_id": "",
                "discord_link": "",
                "slido_link": "",
                "youtube_url": "",
                "zoom_meeting": "",
                "zoom_password": "",
                "zoom_link": "",
                "ff_link": "",
                "ff_playlist": "",
                "time_slots": []
            }
        ]
    },
    "a-vast": {
        "event": "VAST Challenge",
        "long_name": "VAST Challenge",
        "event_type": "Associated Event (Competition)",
        "event_prefix": "a-vast",
        "event_description": "",
        "event_url": "",
        "organizers": [
            "Kristin Cook",
            "R. Jordan Crouser"
        ],
        "sessions": [
            {
                "title": "VAST Challenge",
                "session_id": "ae7",
                "event_prefix": "a-vast",
                "track": "ok6",
                "livestream_id": "ok6-sun",
                "session_image": "ae7.png",
                "chair": [
                    "Kristin Cook",
                    "R. Jordan Crouser"
                ],
                "organizers": [],
                "time_start": "2022-10-16T14:00:00Z",
                "time_end": "2022-10-16T17:00:00Z",
                "discord_category": "",
                "discord_channel": "",
                "discord_channel_id": "",
                "discord_link": "",
                "slido_link": "",
                "youtube_url": "",
                "zoom_meeting": "",
                "zoom_password": "",
                "zoom_link": "",
                "ff_link": "",
                "ff_playlist": "",
                "time_slots": []
            }
        ]
    },
    "t-netvis": {
        "event": "Multilayer Network Visualization: Theory and applications",
        "long_name": "Multilayer Network Visualization: Theory and applications",
        "event_type": "Tutorial",
        "event_prefix": "t-netvis",
        "event_description": "",
        "event_url": "",
        "organizers": [
            "Sonali Agarwal",
            "Sanjay Kumar Sonbhadra",
            "Narinder Singh Punn"
        ],
        "sessions": [
            {
                "title": "Multilayer Network Visualization: Theory and applications",
                "session_id": "t9",
                "event_prefix": "t-netvis",
                "track": "ok6",
                "livestream_id": "ok6-sun",
                "session_image": "t9.png",
                "chair": [
                    "Sonali Agarwal",
                    "Sanjay Kumar Sonbhadra",
                    "Narinder Singh Punn"
                ],
                "organizers": [],
                "time_start": "2022-10-16T19:00:00Z",
                "time_end": "2022-10-16T22:00:00Z",
                "discord_category": "",
                "discord_channel": "",
                "discord_channel_id": "",
                "discord_link": "",
                "slido_link": "",
                "youtube_url": "",
                "zoom_meeting": "",
                "zoom_password": "",
                "zoom_link": "",
                "ff_link": "",
                "ff_playlist": "",
                "time_slots": []
            }
        ]
    },
    "t-ttk": {
        "event": "Topological Analysis of Ensemble Scalar Data with TTK, A Sequel",
        "long_name": "Topological Analysis of Ensemble Scalar Data with TTK, A Sequel",
        "event_type": "Tutorial",
        "event_prefix": "t-ttk",
        "event_description": "",
        "event_url": "",
        "organizers": [
            "Christoph Garth",
            "Charles Gueunet",
            "Pierre Guillou",
            "Federico Iuricich",
            "Joshua Levine",
            "Jonas Lukasczyk",
            "Mathieu Pont",
            "Julien Tierny",
            "Jules Vidal",
            "Bei Wang",
            "Florian Wetzels"
        ],
        "sessions": [
            {
                "title": "Topological Analysis of Ensemble Scalar Data with TTK, A Sequel",
                "session_id": "t2",
                "event_prefix": "t-ttk",
                "track": "ok6",
                "livestream_id": "ok6-mon",
                "session_image": "t2.png",
                "chair": [
                    "Christoph Garth",
                    "Charles Gueunet",
                    "Pierre Guillou",
                    "Federico Iuricich",
                    "Joshua Levine",
                    "Jonas Lukasczyk",
                    "Mathieu Pont",
                    "Julien Tierny",
                    "Jules Vidal",
                    "Bei Wang",
                    "Florian Wetzels"
                ],
                "organizers": [],
                "time_start": "2022-10-17T14:00:00Z",
                "time_end": "2022-10-17T17:00:00Z",
                "discord_category": "",
                "discord_channel": "",
                "discord_channel_id": "",
                "discord_link": "",
                "slido_link": "",
                "youtube_url": "",
                "zoom_meeting": "",
                "zoom_password": "",
                "zoom_link": "",
                "ff_link": "",
                "ff_playlist": "",
                "time_slots": []
            }
        ]
    },
    "w-topoinvis": {
        "event": "TopoInVis: Topological Data Analysis and Visualization",
        "long_name": "TopoInVis: Topological Data Analysis and Visualization",
        "event_type": "Workshop",
        "event_prefix": "w-topoinvis",
        "event_description": "",
        "event_url": "",
        "organizers": [
            "Talha Bin Masood",
            "Vijay Natarajan",
            "Paul Rosen",
            "Julien Tierny"
        ],
        "sessions": [
            {
                "title": "TopoInVis: Topological Data Analysis and Visualization",
                "session_id": "w8",
                "event_prefix": "w-topoinvis",
                "track": "ok6",
                "livestream_id": "ok6-mon",
                "session_image": "w8.png",
                "chair": [
                    "Talha Bin Masood",
                    "Vijay Natarajan",
                    "Paul Rosen",
                    "Julien Tierny"
                ],
                "organizers": [],
                "time_start": "2022-10-17T19:00:00Z",
                "time_end": "2022-10-17T22:00:00Z",
                "discord_category": "",
                "discord_channel": "",
                "discord_channel_id": "",
                "discord_link": "",
                "slido_link": "",
                "youtube_url": "",
                "zoom_meeting": "",
                "zoom_password": "",
                "zoom_link": "",
                "ff_link": "",
                "ff_playlist": "",
                "time_slots": []
            }
        ]
    },
    "w-altvis": {
        "event": "alt.VIS 2022",
        "long_name": "alt.VIS 2022",
        "event_type": "Workshop",
        "event_prefix": "w-altvis",
        "event_description": "",
        "event_url": "",
        "organizers": [
            "Lonni Besan\u00e7on",
            "Andrew McNutt",
            "Arnaud Prouzeau",
            "Jane Adams",
            "Derya Akbaba",
            "Charles Perin"
        ],
        "sessions": [
            {
                "title": "alt.VIS 2022",
                "session_id": "w2",
                "event_prefix": "w-altvis",
                "track": "ok7",
                "livestream_id": "ok7-sun",
                "session_image": "w2.png",
                "chair": [
                    "Lonni Besan\u00e7on",
                    "Andrew McNutt",
                    "Arnaud Prouzeau",
                    "Jane Adams",
                    "Derya Akbaba",
                    "Charles Perin"
                ],
                "organizers": [],
                "time_start": "2022-10-16T14:00:00Z",
                "time_end": "2022-10-16T17:00:00Z",
                "discord_category": "",
                "discord_channel": "",
                "discord_channel_id": "",
                "discord_link": "",
                "slido_link": "",
                "youtube_url": "",
                "zoom_meeting": "",
                "zoom_password": "",
                "zoom_link": "",
                "ff_link": "",
                "ff_playlist": "",
                "time_slots": []
            }
        ]
    },
    "t-design": {
        "event": "Visualization Analysis and Design",
        "long_name": "Visualization Analysis and Design",
        "event_type": "Tutorial",
        "event_prefix": "t-design",
        "event_description": "",
        "event_url": "",
        "organizers": [
            "Tamara Munzner"
        ],
        "sessions": [
            {
                "title": "Visualization Analysis and Design",
                "session_id": "t6",
                "event_prefix": "t-design",
                "track": "ok7",
                "livestream_id": "ok7-sun",
                "session_image": "t6.png",
                "chair": [
                    "Tamara Munzner"
                ],
                "organizers": [],
                "time_start": "2022-10-16T19:00:00Z",
                "time_end": "2022-10-16T22:00:00Z",
                "discord_category": "",
                "discord_channel": "",
                "discord_channel_id": "",
                "discord_link": "",
                "slido_link": "",
                "youtube_url": "",
                "zoom_meeting": "",
                "zoom_password": "",
                "zoom_link": "",
                "ff_link": "",
                "ff_playlist": "",
                "time_slots": []
            }
        ]
    },
    "w-testvis": {
        "event": "TestVis: Workshop on Visualization in Testing of Hardware, Software, and Manufacturing",
        "long_name": "TestVis: Workshop on Visualization in Testing of Hardware, Software, and Manufacturing",
        "event_type": "Workshop",
        "event_prefix": "w-testvis",
        "event_description": "",
        "event_url": "",
        "organizers": [
            "Katherine Isaacs",
            "Steffen Koch",
            "Timo Ropinski",
            "Stefan Wagner",
            "Daniel Weiskopf"
        ],
        "sessions": [
            {
                "title": "TestVis: Workshop on Visualization in Testing of Hardware, Software, and Manufacturing",
                "session_id": "w10",
                "event_prefix": "w-testvis",
                "track": "ok7",
                "livestream_id": "ok7-mon",
                "session_image": "w10.png",
                "chair": [
                    "Katherine Isaacs",
                    "Steffen Koch",
                    "Timo Ropinski",
                    "Stefan Wagner",
                    "Daniel Weiskopf"
                ],
                "organizers": [],
                "time_start": "2022-10-17T14:00:00Z",
                "time_end": "2022-10-17T17:00:00Z",
                "discord_category": "",
                "discord_channel": "",
                "discord_channel_id": "",
                "discord_link": "",
                "slido_link": "",
                "youtube_url": "",
                "zoom_meeting": "",
                "zoom_password": "",
                "zoom_link": "",
                "ff_link": "",
                "ff_playlist": "",
                "time_slots": []
            }
        ]
    },
    "w-visguides": {
        "event": "VisGuides: 4th IEEE Workshop on Visualization Guidelines Visualization Guidelines in Research, Design, and Education",
        "long_name": "VisGuides: 4th IEEE Workshop on Visualization Guidelines Visualization Guidelines in Research, Design, and Education",
        "event_type": "Workshop",
        "event_prefix": "w-visguides",
        "event_description": "",
        "event_url": "",
        "organizers": [
            "Benjamin Bach",
            "Alfie Abdul-Rahman",
            "Alexandra Diehl"
        ],
        "sessions": [
            {
                "title": "VisGuides: 4th IEEE Workshop on Visualization Guidelines Visualization Guidelines in Research, Design, and Education",
                "session_id": "w1",
                "event_prefix": "w-visguides",
                "track": "ok7",
                "livestream_id": "ok7-mon",
                "session_image": "w1.png",
                "chair": [
                    "Benjamin Bach",
                    "Alfie Abdul-Rahman",
                    "Alexandra Diehl"
                ],
                "organizers": [],
                "time_start": "2022-10-17T19:00:00Z",
                "time_end": "2022-10-17T22:00:00Z",
                "discord_category": "",
                "discord_channel": "",
                "discord_channel_id": "",
                "discord_link": "",
                "slido_link": "",
                "youtube_url": "",
                "zoom_meeting": "",
                "zoom_password": "",
                "zoom_link": "",
                "ff_link": "",
                "ff_playlist": "",
                "time_slots": []
            }
        ]
    },
    "w-trex": {
        "event": "TREX: Workshop on TRust and EXpertise in Visualization",
        "long_name": "TREX: Workshop on TRust and EXpertise in Visualization",
        "event_type": "Workshop",
        "event_prefix": "w-trex",
        "event_description": "",
        "event_url": "",
        "organizers": [
            "Mahsan Nourani",
            "Eric Ragan",
            "Alireza Karduni",
            "Cindy Xiong",
            "Brittany Davis Pierson"
        ],
        "sessions": [
            {
                "title": "TREX: Workshop on TRust and EXpertise in Visualization",
                "session_id": "w7",
                "event_prefix": "w-trex",
                "track": "ok8",
                "livestream_id": "ok8-sun",
                "session_image": "w7.png",
                "chair": [
                    "Mahsan Nourani",
                    "Eric Ragan",
                    "Alireza Karduni",
                    "Cindy Xiong",
                    "Brittany Davis Pierson"
                ],
                "organizers": [],
                "time_start": "2022-10-16T14:00:00Z",
                "time_end": "2022-10-16T17:00:00Z",
                "discord_category": "",
                "discord_channel": "",
                "discord_channel_id": "",
                "discord_link": "",
                "slido_link": "",
                "youtube_url": "",
                "zoom_meeting": "",
                "zoom_password": "",
                "zoom_link": "",
                "ff_link": "",
                "ff_playlist": "",
                "time_slots": []
            }
        ]
    },
    "w-vis4good": {
        "event": "Visualization for Social Good 2022",
        "long_name": "Visualization for Social Good 2022",
        "event_type": "Workshop",
        "event_prefix": "w-vis4good",
        "event_description": "",
        "event_url": "",
        "organizers": [
            "Leilani Battle",
            "Michelle Borkin",
            "Lane Harrison",
            "Narges Mahyar",
            "Emily Wall"
        ],
        "sessions": [
            {
                "title": "Visualization for Social Good 2022",
                "session_id": "w9",
                "event_prefix": "w-vis4good",
                "track": "ok8",
                "livestream_id": "ok8-sun",
                "session_image": "w9.png",
                "chair": [
                    "Leilani Battle",
                    "Michelle Borkin",
                    "Lane Harrison",
                    "Narges Mahyar",
                    "Emily Wall"
                ],
                "organizers": [],
                "time_start": "2022-10-16T19:00:00Z",
                "time_end": "2022-10-16T22:00:00Z",
                "discord_category": "",
                "discord_channel": "",
                "discord_channel_id": "",
                "discord_link": "",
                "slido_link": "",
                "youtube_url": "",
                "zoom_meeting": "",
                "zoom_password": "",
                "zoom_link": "",
                "ff_link": "",
                "ff_playlist": "",
                "time_slots": []
            }
        ]
    },
    "w-viscomm": {
        "event": "VisComm: Fifth Workshop on Visualization for Communication",
        "long_name": "VisComm: Fifth Workshop on Visualization for Communication",
        "event_type": "Workshop",
        "event_prefix": "w-viscomm",
        "event_description": "",
        "event_url": "",
        "organizers": [
            "Barbara Millet",
            "Jonathan Schwabish",
            "Alvitta Ottley",
            "Alice Feng"
        ],
        "sessions": [
            {
                "title": "VisComm: Fifth Workshop on Visualization for Communication",
                "session_id": "w5",
                "event_prefix": "w-viscomm",
                "track": "ok8",
                "livestream_id": "ok8-mon",
                "session_image": "w5.png",
                "chair": [
                    "Barbara Millet",
                    "Jonathan Schwabish",
                    "Alvitta Ottley",
                    "Alice Feng"
                ],
                "organizers": [],
                "time_start": "2022-10-17T14:00:00Z",
                "time_end": "2022-10-17T17:00:00Z",
                "discord_category": "",
                "discord_channel": "",
                "discord_channel_id": "",
                "discord_link": "",
                "slido_link": "",
                "youtube_url": "",
                "zoom_meeting": "",
                "zoom_password": "",
                "zoom_link": "",
                "ff_link": "",
                "ff_playlist": "",
                "time_slots": []
            }
        ]
    },
    "w-vis4climate": {
        "event": "Viz4Climate: Workshop on High Impact Techniques for Visual Climate Science Communication",
        "long_name": "Viz4Climate: Workshop on High Impact Techniques for Visual Climate Science Communication",
        "event_type": "Workshop",
        "event_prefix": "w-vis4climate",
        "event_description": "",
        "event_url": "",
        "organizers": [
            "Helen-Nicole Kostis",
            "Mark SubbaRao",
            "Marlen Promann"
        ],
        "sessions": [
            {
                "title": "Viz4Climate: Workshop on High Impact Techniques for Visual Climate Science Communication",
                "session_id": "w6",
                "event_prefix": "w-vis4climate",
                "track": "ok8",
                "livestream_id": "ok8-mon",
                "session_image": "w6.png",
                "chair": [
                    "Helen-Nicole Kostis",
                    "Mark SubbaRao",
                    "Marlen Promann"
                ],
                "organizers": [],
                "time_start": "2022-10-17T19:00:00Z",
                "time_end": "2022-10-17T22:00:00Z",
                "discord_category": "",
                "discord_channel": "",
                "discord_channel_id": "",
                "discord_link": "",
                "slido_link": "",
                "youtube_url": "",
                "zoom_meeting": "",
                "zoom_password": "",
                "zoom_link": "",
                "ff_link": "",
                "ff_playlist": "",
                "time_slots": []
            }
        ]
    },
    "t-nlp4vis": {
        "event": "NLP4Vis: Natural Language Processing for Information Visualization",
        "long_name": "NLP4Vis: Natural Language Processing for Information Visualization",
        "event_type": "Tutorial",
        "event_prefix": "t-nlp4vis",
        "event_description": "",
        "event_url": "",
        "organizers": [
            "Enamul Hoque",
            "Shafiq Joty"
        ],
        "sessions": [
            {
                "title": "NLP4Vis: Natural Language Processing for Information Visualization",
                "session_id": "t10",
                "event_prefix": "t-nlp4vis",
                "track": "pinon",
                "livestream_id": "pinon-sun",
                "session_image": "t10.png",
                "chair": [
                    "Enamul Hoque",
                    "Shafiq Joty"
                ],
                "organizers": [],
                "time_start": "2022-10-16T14:00:00Z",
                "time_end": "2022-10-16T17:00:00Z",
                "discord_category": "",
                "discord_channel": "",
                "discord_channel_id": "",
                "discord_link": "",
                "slido_link": "",
                "youtube_url": "",
                "zoom_meeting": "",
                "zoom_password": "",
                "zoom_link": "",
                "ff_link": "",
                "ff_playlist": "",
                "time_slots": []
            }
        ]
    },
    "w-nlvis": {
        "event": "NLVIZ Workshop: Exploring Research Opportunities for Natural Language, Text, and Data Visualization",
        "long_name": "NLVIZ Workshop: Exploring Research Opportunities for Natural Language, Text, and Data Visualization",
        "event_type": "Workshop",
        "event_prefix": "w-nlvis",
        "event_description": "",
        "event_url": "",
        "organizers": [
            "Vidya Setlur",
            "Arjun Srinivasan"
        ],
        "sessions": [
            {
                "title": "NLVIZ Workshop: Exploring Research Opportunities for Natural Language, Text, and Data Visualization",
                "session_id": "w4",
                "event_prefix": "w-nlvis",
                "track": "pinon",
                "livestream_id": "pinon-sun",
                "session_image": "w4.png",
                "chair": [
                    "Vidya Setlur",
                    "Arjun Srinivasan"
                ],
                "organizers": [],
                "time_start": "2022-10-16T19:00:00Z",
                "time_end": "2022-10-16T22:00:00Z",
                "discord_category": "",
                "discord_channel": "",
                "discord_channel_id": "",
                "discord_link": "",
                "slido_link": "",
                "youtube_url": "",
                "zoom_meeting": "",
                "zoom_password": "",
                "zoom_link": "",
                "ff_link": "",
                "ff_playlist": "",
                "time_slots": []
            }
        ]
    },
    "t-bayesian": {
        "event": "Visualization in Bayesian workflow",
        "long_name": "Visualization in Bayesian workflow",
        "event_type": "Tutorial",
        "event_prefix": "t-bayesian",
        "event_description": "",
        "event_url": "",
        "organizers": [
            "Clinton Brownley"
        ],
        "sessions": [
            {
                "title": "Visualization in Bayesian workflow",
                "session_id": "t4",
                "event_prefix": "t-bayesian",
                "track": "pinon",
                "livestream_id": "pinon-mon",
                "session_image": "t4.png",
                "chair": [
                    "Clinton Brownley"
                ],
                "organizers": [],
                "time_start": "2022-10-17T14:00:00Z",
                "time_end": "2022-10-17T17:00:00Z",
                "discord_category": "",
                "discord_channel": "",
                "discord_channel_id": "",
                "discord_link": "",
                "slido_link": "",
                "youtube_url": "",
                "zoom_meeting": "",
                "zoom_password": "",
                "zoom_link": "",
                "ff_link": "",
                "ff_playlist": "",
                "time_slots": []
            }
        ]
    },
    "t-vtk": {
        "event": "VTK-m \u2013 A ToolKit for Scientific Visualization on Many-Core Processors",
        "long_name": "VTK-m \u2013 A ToolKit for Scientific Visualization on Many-Core Processors",
        "event_type": "Tutorial",
        "event_prefix": "t-vtk",
        "event_description": "",
        "event_url": "",
        "organizers": [
            "Tushar Athawale",
            "Kenneth Moreland",
            "David Pugmire",
            "Silvio Rizzi",
            "Mark Bolstad"
        ],
        "sessions": [
            {
                "title": "VTK-m \u2013 A ToolKit for Scientific Visualization on Many-Core Processors",
                "session_id": "t5",
                "event_prefix": "t-vtk",
                "track": "pinon",
                "livestream_id": "pinon-mon",
                "session_image": "t5.png",
                "chair": [
                    "Tushar Athawale",
                    "Kenneth Moreland",
                    "David Pugmire",
                    "Silvio Rizzi",
                    "Mark Bolstad"
                ],
                "organizers": [],
                "time_start": "2022-10-17T19:00:00Z",
                "time_end": "2022-10-17T22:00:00Z",
                "discord_category": "",
                "discord_channel": "",
                "discord_channel_id": "",
                "discord_link": "",
                "slido_link": "",
                "youtube_url": "",
                "zoom_meeting": "",
                "zoom_password": "",
                "zoom_link": "",
                "ff_link": "",
                "ff_playlist": "",
                "time_slots": []
            }
        ]
    },
    "t-color": {
        "event": "Color Scheming in Visualization",
        "long_name": "Color Scheming in Visualization",
        "event_type": "Tutorial",
        "event_prefix": "t-color",
        "event_description": "",
        "event_url": "",
        "organizers": [
            "Theresa-Marie Rhyne"
        ],
        "sessions": [
            {
                "title": "Color Scheming in Visualization",
                "session_id": "t1",
                "event_prefix": "t-color",
                "track": "mistletoe",
                "livestream_id": "mistletoe-sun",
                "session_image": "t1.png",
                "chair": [
                    "Theresa-Marie Rhyne"
                ],
                "organizers": [],
                "time_start": "2022-10-16T14:00:00Z",
                "time_end": "2022-10-16T17:00:00Z",
                "discord_category": "",
                "discord_channel": "",
                "discord_channel_id": "",
                "discord_link": "",
                "slido_link": "",
                "youtube_url": "",
                "zoom_meeting": "",
                "zoom_password": "",
                "zoom_link": "",
                "ff_link": "",
                "ff_playlist": "",
                "time_slots": []
            }
        ]
    },
    "t-sports": {
        "event": "Sports Data Analysis and Visualization",
        "long_name": "Sports Data Analysis and Visualization",
        "event_type": "Tutorial",
        "event_prefix": "t-sports",
        "event_description": "",
        "event_url": "",
        "organizers": [
            "Romain Vuillemot"
        ],
        "sessions": [
            {
                "title": "Sports Data Analysis and Visualization",
                "session_id": "t7",
                "event_prefix": "t-sports",
                "track": "mistletoe",
                "livestream_id": "mistletoe-sun",
                "session_image": "t7.png",
                "chair": [
                    "Romain Vuillemot"
                ],
                "organizers": [],
                "time_start": "2022-10-16T19:00:00Z",
                "time_end": "2022-10-16T22:00:00Z",
                "discord_category": "",
                "discord_channel": "",
                "discord_channel_id": "",
                "discord_link": "",
                "slido_link": "",
                "youtube_url": "",
                "zoom_meeting": "",
                "zoom_password": "",
                "zoom_link": "",
                "ff_link": "",
                "ff_playlist": "",
                "time_slots": []
            }
        ]
    },
    "t-riemannian": {
        "event": "Riemannian Geometry for Scientific Visualization",
        "long_name": "Riemannian Geometry for Scientific Visualization",
        "event_type": "Tutorial",
        "event_prefix": "t-riemannian",
        "event_description": "",
        "event_url": "",
        "organizers": [
            "Markus Hadwiger",
            "Thomas Theussl",
            "Peter Rautek"
        ],
        "sessions": [
            {
                "title": "Riemannian Geometry for Scientific Visualization",
                "session_id": "t3",
                "event_prefix": "t-riemannian",
                "track": "mistletoe",
                "livestream_id": "mistletoe-mon",
                "session_image": "t3.png",
                "chair": [
                    "Markus Hadwiger",
                    "Thomas Theussl",
                    "Peter Rautek"
                ],
                "organizers": [],
                "time_start": "2022-10-17T14:00:00Z",
                "time_end": "2022-10-17T17:00:00Z",
                "discord_category": "",
                "discord_channel": "",
                "discord_channel_id": "",
                "discord_link": "",
                "slido_link": "",
                "youtube_url": "",
                "zoom_meeting": "",
                "zoom_password": "",
                "zoom_link": "",
                "ff_link": "",
                "ff_playlist": "",
                "time_slots": []
            }
        ]
    },
    "t-geo": {
        "event": "Analyze and Visualize Large Scale Geospatial Data with H3 and HexTile",
        "long_name": "Analyze and Visualize Large Scale Geospatial Data with H3 and HexTile",
        "event_type": "Tutorial",
        "event_prefix": "t-geo",
        "event_description": "",
        "event_url": "",
        "organizers": [
            "Shan He"
        ],
        "sessions": [
            {
                "title": "Analyze and Visualize Large Scale Geospatial Data with H3 and HexTile",
                "session_id": "t8",
                "event_prefix": "t-geo",
                "track": "mistletoe",
                "livestream_id": "mistletoe-mon",
                "session_image": "t8.png",
                "chair": [
                    "Shan He"
                ],
                "organizers": [],
                "time_start": "2022-10-17T19:00:00Z",
                "time_end": "2022-10-17T22:00:00Z",
                "discord_category": "",
                "discord_channel": "",
                "discord_channel_id": "",
                "discord_link": "",
                "slido_link": "",
                "youtube_url": "",
                "zoom_meeting": "",
                "zoom_password": "",
                "zoom_link": "",
                "ff_link": "",
                "ff_playlist": "",
                "time_slots": []
            }
        ]
    }
}