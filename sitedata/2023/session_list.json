{
    "conf": {
        "event": "Conference Events",
        "long_name": "Conference Events",
        "event_type": "VIS",
        "event_prefix": "conf",
        "event_description": "",
        "event_url": "",
        "organizers": [],
        "sessions": [
            {
                "title": "VIS Welcome (8:45am-9:00am)| VGTC Awards (9:00am-9:45am)| Test of Time Awards (9:45am-10:30am)",
                "session_id": "conf1",
                "event_prefix": "conf",
                "track": "Plenary-1",
                "session_image": "conf1.png",
                "chair": [],
                "organizers": [],
                "time_start": "2023-10-22T00:00:00Z",
                "time_end": "2023-10-23T23:30:00Z",
                "discord_category": "",
                "discord_channel": "",
                "discord_channel_id": "",
                "discord_link": "",
                "zoom_meeting": "",
                "zoom_password": "",
                "zoom_link": "",
                "time_slots": []
            },
            {
                "title": "VIS Keynote (11:00am-12:00pm)",
                "session_id": "conf2",
                "event_prefix": "conf",
                "track": "Plenary-1",
                "session_image": "conf2.png",
                "chair": [],
                "organizers": [],
                "time_start": "2023-10-22T00:00:00Z",
                "time_end": "2023-10-24T01:00:00Z",
                "discord_category": "",
                "discord_channel": "",
                "discord_channel_id": "",
                "discord_link": "",
                "zoom_meeting": "",
                "zoom_password": "",
                "zoom_link": "",
                "time_slots": []
            },
            {
                "title": "VIS Capstone (10:45am-11:45am)| VIS Closing (11:45am-12:00pm)",
                "session_id": "conf3",
                "event_prefix": "conf",
                "track": "105(234)",
                "session_image": "conf3.png",
                "chair": [],
                "organizers": [],
                "time_start": "2023-10-22T00:00:00Z",
                "time_end": "2023-10-27T01:00:00Z",
                "discord_category": "",
                "discord_channel": "",
                "discord_channel_id": "",
                "discord_link": "",
                "zoom_meeting": "",
                "zoom_password": "",
                "zoom_link": "",
                "time_slots": []
            },
            {
                "title": "VIS Poster Session (4:45pm-6:15pm)| -- includes all Associated Event Posters| -- Exhibit Hall Location: Foyer",
                "session_id": "conf4",
                "event_prefix": "conf",
                "track": "Plenary-1",
                "session_image": "conf4.png",
                "chair": [],
                "organizers": [],
                "time_start": "2023-10-22T00:00:00Z",
                "time_end": "2023-10-24T07:15:00Z",
                "discord_category": "",
                "discord_channel": "",
                "discord_channel_id": "",
                "discord_link": "",
                "zoom_meeting": "",
                "zoom_password": "",
                "zoom_link": "",
                "time_slots": []
            },
            {
                "title": "VISAP Opening Reception (6:00pm-8:00pm)| -- Art Exhibit Location: Library at the Docks",
                "session_id": "conf5",
                "event_prefix": "conf",
                "track": "Plenary-1",
                "session_image": "conf5.png",
                "chair": [],
                "organizers": [],
                "time_start": "2023-10-22T00:00:00Z",
                "time_end": "2023-10-24T09:00:00Z",
                "discord_category": "",
                "discord_channel": "",
                "discord_channel_id": "",
                "discord_link": "",
                "zoom_meeting": "",
                "zoom_password": "",
                "zoom_link": "",
                "time_slots": []
            },
            {
                "title": "VIS Banquet (6:00pm-10:00pm)| -- Location: Melbourne Planetarium| -- Shows at 6, 6:45, and 7:30pm| -- Transport on your own / via myki",
                "session_id": "conf6",
                "event_prefix": "conf",
                "track": "Plenary-1",
                "session_image": "conf6.png",
                "chair": [],
                "organizers": [],
                "time_start": "2023-10-22T00:00:00Z",
                "time_end": "2023-10-25T11:00:00Z",
                "discord_category": "",
                "discord_channel": "",
                "discord_channel_id": "",
                "discord_link": "",
                "zoom_meeting": "",
                "zoom_password": "",
                "zoom_link": "",
                "time_slots": []
            }
        ]
    },
    "v-full": {
        "event": "VIS Full Papers",
        "long_name": "VIS Full Papers",
        "event_type": "Paper Presentations",
        "event_prefix": "v-full",
        "event_description": "",
        "event_url": "",
        "organizers": [],
        "sessions": [
            {
                "title": "VIS Opening (2:00pm-2:20pm)| ACC Summary (2:20pm-2:30pm)| Best Papers 1 (2:30pm-3:10pm)",
                "session_id": "full0",
                "event_prefix": "v-full",
                "track": "Plenary-1",
                "session_image": "full0.png",
                "chair": [],
                "organizers": [],
                "time_start": "2023-10-22T00:00:00Z",
                "time_end": "2023-10-24T04:10:00Z",
                "discord_category": "",
                "discord_channel": "",
                "discord_channel_id": "",
                "discord_link": "",
                "zoom_meeting": "",
                "zoom_password": "",
                "zoom_link": "",
                "time_slots": [
                    {
                        "slot_id": "v-full-1009",
                        "session_id": "full0",
                        "title": "Affective Visualization Design: Leveraging the Emotional Impact of Data",
                        "contributors": [
                            "Xingyu Lan"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "v-full-1009",
                        "time_stamp": "2023-10-24T03:00:00Z",
                        "time_start": "2023-10-24T03:00:00Z",
                        "time_end": "2023-10-24T03:12:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "1",
                        "paper_award": "",
                        "image_caption": "The image has 3 panels: (i)examples of affective visualization design projects; (ii)six identified genres of affective visualization design projects (interactive interface, video, static image/painting, installation, artifact, event), and (iii) the design space proposed by this work, showing WHERE to apply affective visualization design, WHAT design tasks it undertakes, and HOW to perform the design.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    },
                    {
                        "slot_id": "v-full-1392",
                        "session_id": "full0",
                        "title": "Fast Compressed Segmentation Volumes for Scientific Visualization",
                        "contributors": [
                            "Max Piochowiak"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "v-full-1392",
                        "time_stamp": "2023-10-24T03:12:00Z",
                        "time_start": "2023-10-24T03:12:00Z",
                        "time_end": "2023-10-24T03:24:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "1",
                        "paper_award": "",
                        "image_caption": "Our novel segmentation volume compression can be used to interactively render large data sets. The compression builds on a brick-wise hierarchical multi-grid of segmentation labels which are compactly stored as a stream of operations. Compression rates are between 1% and 3% on complex segmentation volumes.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    },
                    {
                        "slot_id": "v-full-1256",
                        "session_id": "full0",
                        "title": "Swaying the Public? Impacts of Election Forecast Visualizations on Emotion, Trust, and Intention in the 2022 U.S. Midterms",
                        "contributors": [
                            "Fumeng Yang"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "v-full-1256",
                        "time_stamp": "2023-10-24T03:24:00Z",
                        "time_start": "2023-10-24T03:24:00Z",
                        "time_end": "2023-10-24T03:36:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "1",
                        "paper_award": "",
                        "image_caption": "We conducted a longitudinal study during the 2022 U.S. midterm elections, investigating the real-world impacts of uncertainty visualizations. Using our forecast model of the governor elections in 33 states, we created a website and deployed four uncertainty visualizations for the election forecasts. Our online experiment ran from Oct. 18, 2022 to Nov. 23, 2022, involving 1,327 participants from 15 states. We find that election forecast visualizations can heighten emotions, increase trust, and slightly affect people\u2019s intentions to participate in elections. Our qualitative analysis uncovers the complex political and social contexts of election forecast visualizations, showcasing that visualizations may provoke polarization.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    }
                ]
            },
            {
                "title": "Best Papers 2 (3:40pm-4:30pm)| -- Includes Best Short Paper",
                "session_id": "full1",
                "event_prefix": "v-full",
                "track": "Plenary-1",
                "session_image": "full1.png",
                "chair": [
                    "G. Elisabeta Marai"
                ],
                "organizers": [],
                "time_start": "2023-10-22T00:00:00Z",
                "time_end": "2023-10-24T05:30:00Z",
                "discord_category": "",
                "discord_channel": "",
                "discord_channel_id": "",
                "discord_link": "",
                "zoom_meeting": "",
                "zoom_password": "",
                "zoom_link": "",
                "time_slots": [
                    {
                        "slot_id": "v-full-1439",
                        "session_id": "full1",
                        "title": "TimeSplines: Sketch-based Authoring of Flexible and Idiosyncratic Timelines",
                        "contributors": [
                            "Anna Offenwanger"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "v-full-1439",
                        "time_stamp": "2023-10-24T04:40:00Z",
                        "time_start": "2023-10-24T04:40:00Z",
                        "time_end": "2023-10-24T04:55:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "1",
                        "paper_award": "",
                        "image_caption": "The TimeSplines interface. Left: The canvas on which the idiosyncratic timelines are created and manipulated. Right: The data drawer which contains multiple heterogeneous datasets.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    },
                    {
                        "slot_id": "v-full-1716",
                        "session_id": "full1",
                        "title": "Visualization of Discontinuous Vector Field Topology",
                        "contributors": [
                            "Egzon Miftari"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "v-full-1716",
                        "time_stamp": "2023-10-24T04:55:00Z",
                        "time_start": "2023-10-24T04:55:00Z",
                        "time_end": "2023-10-24T05:10:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "1",
                        "paper_award": "",
                        "image_caption": "Topological analysis of vector field with discontinuity (transparent gray) exhibiting repelling sliding flow (red LIC). Qualitatively different equivalence streamsets (white) are separated by stable (blue) and unstable (red) manifolds. Equitrices (violet) separate streamsets of different dimensionality.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    },
                    {
                        "slot_id": "v-full-1226",
                        "session_id": "full1",
                        "title": "Vortex Lens: Interactive Vortex Core Line Extraction using Observed Line Integral Convolution",
                        "contributors": [
                            "Peter Rautek"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "v-full-1226",
                        "time_stamp": "2023-10-24T05:10:00Z",
                        "time_start": "2023-10-24T05:10:00Z",
                        "time_end": "2023-10-24T05:25:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "1",
                        "paper_award": "",
                        "image_caption": "Fluid flow simulation visualized using line integral convolution and color coding of vorticity.   Inside the vortex lens observed line integral convolution is used to show a vortex at its actual position in space and time.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    },
                    {
                        "slot_id": "v-short-1170",
                        "session_id": "full1",
                        "title": "Gridded Glyphmaps for Supporting Spatial COVID-19 Modelling",
                        "contributors": [
                            "Aidan Slingsby"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "v-short-1170",
                        "time_stamp": "2023-10-24T05:25:00Z",
                        "time_start": "2023-10-24T05:25:00Z",
                        "time_end": "2023-10-24T05:40:00Z",
                        "paper_type": "short",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "1",
                        "paper_award": "",
                        "image_caption": "This gridded glyphmap shows the proportions of population (x-axis) in each grid square, by age group (y-axis; youngest at top) in different COVID-19 infection states (hue).",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    }
                ]
            },
            {
                "title": "Clustering & Scatterplots",
                "session_id": "full2",
                "event_prefix": "v-full",
                "track": "104(132)",
                "session_image": "full2.png",
                "chair": [
                    "John Wenskovitch"
                ],
                "organizers": [],
                "time_start": "2023-10-22T00:00:00Z",
                "time_end": "2023-10-25T23:15:00Z",
                "discord_category": "",
                "discord_channel": "",
                "discord_channel_id": "",
                "discord_link": "",
                "zoom_meeting": "",
                "zoom_password": "",
                "zoom_link": "",
                "time_slots": [
                    {
                        "slot_id": "v-tvcg-9826389",
                        "session_id": "full2",
                        "title": "Automatic Scatterplot Design Optimization for Clustering Identification",
                        "contributors": [
                            "Ghulam Jilani Quadri"
                        ],
                        "authors": [],
                        "abstract": "Scatterplots are among the most widely used visualization techniques. Compelling scatterplot visualizations improve understanding of data by leveraging visual perception to boost awareness when performing specific visual analytic tasks. Design choices in scatterplots, such as graphical encodings or data aspects, can directly impact decision-making quality for low-level tasks like clustering. Hence, constructing frameworks that consider both the perceptions of the visual encodings and the task being performed enables optimizing visualizations to maximize efficacy. In this paper, we propose an automatic tool to optimize the design factors of scatterplots to reveal the most salient cluster structure. Our approach leverages the merge tree data structure to identify the clusters and optimize the choice of subsampling algorithm, sampling rate, marker size, and marker opacity used to generate a scatterplot image. We validate our approach with user and case studies that show it efficiently provides high-quality scatterplot designs from a large parameter space.",
                        "uid": "v-tvcg-9826389",
                        "time_stamp": "2023-10-25T22:00:00Z",
                        "time_start": "2023-10-25T22:00:00Z",
                        "time_end": "2023-10-25T22:12:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "1",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    },
                    {
                        "slot_id": "v-tvcg-10068257",
                        "session_id": "full2",
                        "title": "Interactive Subspace Cluster Analysis Guided by Semantic Attribute Associations",
                        "contributors": [
                            "Klaus Mueller"
                        ],
                        "authors": [],
                        "abstract": "Multivariate datasets with many variables are increasingly common in many application areas. Most methods approach multivariate data from a singular perspective. Subspace analysis techniques, on the other hand. provide the user a set of subspaces which can be used to view the data from multiple perspectives. However, many subspace analysis methods produce a huge amount of subspaces, a number of which are usually redundant. The enormity of the number of subspaces can be overwhelming to analysts, making it difficult for them to find informative patterns in the data. In this paper, we propose a new paradigm that constructs semantically consistent subspaces. These subspaces can then be expanded into more general subspaces by ways of conventional techniques. Our framework uses the labels/meta-data of a dataset to learn the semantic meanings and associations of the attributes. We employ a neural network to learn a semantic word embedding of the attributes and then divide this attribute space into semantically consistent subspaces. The user is provided with a visual analytics interface that guides the analysis process. We show via various examples that these semantic subspaces can help organize the data and guide the user in finding interesting patterns in the dataset.",
                        "uid": "v-tvcg-10068257",
                        "time_stamp": "2023-10-25T22:12:00Z",
                        "time_start": "2023-10-25T22:12:00Z",
                        "time_end": "2023-10-25T22:24:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "1",
                        "paper_award": "",
                        "image_caption": "The Semantic Subspace Explorer learns the semantic associations among data attributes, exposing interesting data patterns. The interface consists of:  the Control Panel, where users can determine the number of subspaces; the Semantic Space View, which visualizes the semantic space of all attributes; the Subspace Organizer, which shows an overview of the generated subspaces; the Subspace View which, shows a user-selected subspace in more detail as a biplot.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    },
                    {
                        "slot_id": "v-tvcg-10173631",
                        "session_id": "full2",
                        "title": "Investigating the Visual Utility of Differentially Private Scatterplots",
                        "contributors": [
                            "Liudas Panavas"
                        ],
                        "authors": [],
                        "abstract": "Increasingly, visualization practitioners are working with, using, and studying private and sensitive data. There can be many stakeholders interested in the resulting analyses\u2014but widespread sharing of the data can cause harm to individuals, companies, and organizations. Practitioners are increasingly turning to differential privacy to enable public sharing of data with a guaranteed amount of privacy. Differential privacy algorithms do this by aggregating data statistics with noise, and this now-private data can be released visually with differentially private scatterplots. While the private visual output is affected by the algorithm choice, privacy level, bin number, data distribution, and user task, there is little guidance on how to choose and balance the effect of these parameters. To address this gap, we had experts examine 1,200 differentially private scatterplots created with a variety of parameter choices and tested their ability to see aggregate patterns in the private output (i.e. the visual utility of the chart). We synthesized these results to provide easy-to-use guidance for visualization practitioners releasing private data through scatterplots. Our findings also provide a ground truth for visual utility, which we use to benchmark automated utility metrics from a variety of fields. We demonstrate how multi-scale structural similarity (MS-SSIM), the metric most strongly correlated with our study\u2019s utility results, can be used to optimize parameter selection. A free copy of this paper along with all supplemental materials is available at https://osf.io/wej4s/.",
                        "uid": "v-tvcg-10173631",
                        "time_stamp": "2023-10-25T22:24:00Z",
                        "time_start": "2023-10-25T22:24:00Z",
                        "time_end": "2023-10-25T22:36:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "1",
                        "paper_award": "",
                        "image_caption": "Illustration of how a differentially private algorithm generates private data from the original data. The data is binned through count queries, denoted F(x). Noise is added from Laplace distributions dictated by epsilon. The output is a differentially private scatterplot composed of F(x) + noise  = M(x).",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    },
                    {
                        "slot_id": "v-full-1026",
                        "session_id": "full2",
                        "title": "CLAMS: Cluster Ambiguity Measure for Estimating Perceptual Variability in Visual Clustering",
                        "contributors": [
                            "Hyeon Jeon"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "v-full-1026",
                        "time_stamp": "2023-10-25T22:36:00Z",
                        "time_start": "2023-10-25T22:36:00Z",
                        "time_end": "2023-10-25T22:48:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "1",
                        "paper_award": "",
                        "image_caption": "The comparison between the way of estimating the perceptual variability in conducting visual clustering, i.e., cluster ambiguity, of monochrome scatterplots.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    },
                    {
                        "slot_id": "v-full-1025",
                        "session_id": "full2",
                        "title": "Classes are not Clusters: Improving Label-based Evaluation of Dimensionality Reduction",
                        "contributors": [
                            "Hyeon Jeon"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "v-full-1025",
                        "time_stamp": "2023-10-25T22:48:00Z",
                        "time_start": "2023-10-25T22:48:00Z",
                        "time_end": "2023-10-25T23:00:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "1",
                        "paper_award": "",
                        "image_caption": "Guidelines to infer the Cluster-Label Matching (CLM) of the high-dimensional data based on the CLM of the embedded data (left column) and the scores given by Label-T (Trustworthiness) and Label-C (Continuity) (first row)",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    },
                    {
                        "slot_id": "v-full-1473",
                        "session_id": "full2",
                        "title": "Guaranteed Visibility in Scatterplots with Tolerance",
                        "contributors": [
                            "Loann Giovannangeli"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "v-full-1473",
                        "time_stamp": "2023-10-25T23:00:00Z",
                        "time_start": "2023-10-25T23:00:00Z",
                        "time_end": "2023-10-25T23:12:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "1",
                        "paper_award": "",
                        "image_caption": "GIST is a Layout Adjustment method to improve data visibility in scatter plots where their visual representations overlap.  It searches for the optimal node diameters that enable to remove overlaps with a limited quantity of movements. To speed up the process and minimize movements, it tolerates some overlap that do not hinder the visualization.  On the top left side, the initial layout represents 10000 points of the Fashion-MNIST dataset projected by t-SNE. On the bottom right side, the layout is post-processed by GIST. Data visual representations remain large enough, the overlaping issues are solved, and the cluster shapes are preserved.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    }
                ]
            },
            {
                "title": "Color and Accessibility",
                "session_id": "full3",
                "event_prefix": "v-full",
                "track": "105(234)",
                "session_image": "full3.png",
                "chair": [
                    "Karen Schloss"
                ],
                "organizers": [],
                "time_start": "2023-10-22T00:00:00Z",
                "time_end": "2023-10-25T23:15:00Z",
                "discord_category": "",
                "discord_channel": "",
                "discord_channel_id": "",
                "discord_link": "",
                "zoom_meeting": "",
                "zoom_password": "",
                "zoom_link": "",
                "time_slots": [
                    {
                        "slot_id": "v-tvcg-9919390",
                        "session_id": "full3",
                        "title": "Rainbow Colormaps: What are they good and bad for?",
                        "contributors": [
                            "Khairi Reda"
                        ],
                        "authors": [],
                        "abstract": "Guidelines for color use in quantitative visualizations have strongly discouraged the use of rainbow colormaps, arguing instead for smooth designs that do not induce visual discontinuities or implicit color categories. However, the empirical evidence behind this argument has been mixed and, at times, even contradictory. In practice, rainbow colormaps are widely used, raising questions about the true utility or dangers of such designs. We study how color categorization impacts the interpretation of scalar fields. We first introduce an approach to detect latent categories in colormaps. We hypothesize that the appearance of color categories in scalar visualizations can be beneficial in that they enhance the perception of certain features, although at the cost of rendering other features less noticeable. In three crowdsourced experiments, we show that observers are more likely to discriminate global, distributional features when viewing colorful scales that induce categorization (e.g., rainbow or diverging schemes). Conversely, when seeing the same data through a less colorful representation, observers are more likely to report localized features defined by small variations in the data. Participants showed awareness of these different affordances, and exhibited bias for exploiting the more discriminating colormap, given a particular feature type. Our results demonstrate costs and benefits for rainbows (and similarly colorful schemes), suggesting that their complementary utility for analyzing scalar data should not be dismissed. In addition to explaining potentially valid uses of rainbow, our study provides actionable guidelines, including on when such designs can be more harmful than useful. Data and materials are available at https://osf.io/xjhtf",
                        "uid": "v-tvcg-9919390",
                        "time_stamp": "2023-10-25T22:00:00Z",
                        "time_start": "2023-10-25T22:00:00Z",
                        "time_end": "2023-10-25T22:12:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "1",
                        "paper_award": "",
                        "image_caption": "Analysis of color nameability in rainbow vs. other perceptual colormaps. This metric predicts the type of data features people attend to.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    },
                    {
                        "slot_id": "v-tvcg-9904859",
                        "session_id": "full3",
                        "title": "Sensemaking Sans Power: Interactive Data Visualization Using Color-Changing Ink",
                        "contributors": [
                            "Biswaksen Patnaik"
                        ],
                        "authors": [],
                        "abstract": "We present an approach for interactively visualizing data using color-changing inks without the need for electronic displays or computers. Color-changing inks are a family of physical inks that change their color characteristics in response to an external stimulus such as heat, UV light, water, and pressure. Visualizations created using color-changing inks can embed interactivity in printed material without external computational media. In this paper, we survey current color-changing ink technology and then use these findings to derive a framework for how it can be used to construct interactive data representations. We also enumerate the interaction techniques possible using this technology. We then show some examples of how to use color-changing ink to create interactive visualizations on paper. While obviously limited in scope to situations where no power or computing is present, or as a complement to digital displays, our findings can be employed for paper, data physicalization, and embedded visualizations.",
                        "uid": "v-tvcg-9904859",
                        "time_stamp": "2023-10-25T22:12:00Z",
                        "time_start": "2023-10-25T22:12:00Z",
                        "time_end": "2023-10-25T22:24:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "1",
                        "paper_award": "",
                        "image_caption": "Changing the distribution on printed media though touch without electronic computation or power. Color changing ink is used that changes it's appearance from colored (right-top) to colorless (right-bottom) upon human touch. This changes the distribution (right-top to right-bottom) upon touch.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    },
                    {
                        "slot_id": "v-full-1580",
                        "session_id": "full3",
                        "title": "Data Navigator: An accessibility-centered data navigation toolkit",
                        "contributors": [
                            "Frank Elavsky"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "v-full-1580",
                        "time_stamp": "2023-10-25T22:24:00Z",
                        "time_start": "2023-10-25T22:24:00Z",
                        "time_end": "2023-10-25T22:36:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "1",
                        "paper_award": "",
                        "image_caption": "Data Navigator provides visualization toolkits with rich, accessible navigation structures, robust input handling, and flexible, semantic rendering.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    },
                    {
                        "slot_id": "v-full-1830",
                        "session_id": "full3",
                        "title": "NL2Color: Refining Color Palettes for Charts with Natural Language",
                        "contributors": [
                            "Chuhan Shi"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "v-full-1830",
                        "time_stamp": "2023-10-25T22:36:00Z",
                        "time_start": "2023-10-25T22:36:00Z",
                        "time_end": "2023-10-25T22:48:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "1",
                        "paper_award": "",
                        "image_caption": "Examples of color palette refinement by NL2Color. (a)-(d) show four pairs of an original chart (left) and a new chart (right) refined by NL2Color according to the request. (e) shows an original chart (left) and three new charts (right) NL2Color generated according to three refinement requests. The color palette of each chart is displayed above the chart. The original charts are collected from Vega-Lite.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    },
                    {
                        "slot_id": "v-full-1152",
                        "session_id": "full3",
                        "title": "Reducing Ambiguity in Line-based Density Plots by Image-space Colorization",
                        "contributors": [
                            "Yumeng Xue"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "v-full-1152",
                        "time_stamp": "2023-10-25T22:48:00Z",
                        "time_start": "2023-10-25T22:48:00Z",
                        "time_end": "2023-10-25T23:00:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "1",
                        "paper_award": "",
                        "image_caption": "Trajectories of taxi rides in Beijing: (a) A line-based visualization of the trajectories is cluttered and convoluted; (b) Using a density plot eliminates clutter, but the continuation of the trends is ambiguous, e.g., it looks like taxis follow the prominent, circular route around the city center; (c) Pixel-based colorization reveals clusters, we see that taxis mostly stay in one part of the city.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    },
                    {
                        "slot_id": "v-full-1245",
                        "session_id": "full3",
                        "title": "TactualPlot: Spatializing Data as Sound using Sensory Substitution for Touchscreen Accessibility",
                        "contributors": [
                            "Pramod Chundury"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "v-full-1245",
                        "time_stamp": "2023-10-25T23:00:00Z",
                        "time_start": "2023-10-25T23:00:00Z",
                        "time_end": "2023-10-25T23:12:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "1",
                        "paper_award": "",
                        "image_caption": "Teaser with 3 composite images. Image A shows a mockup of the tactual plot technique. Image B shows a screenshot of the TactualPlot system implemented on an iPad. A square-shaped scatterplot is shown at the bottom of the screen, and contains the data points, and the axes. The top part of the screenshot displays buttons that control the prototype. Image C shows the same scatter plot that has been printed on thermoform paper, and is overlaid on the iPad.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    }
                ]
            },
            {
                "title": "Dashboards & Multiple Views",
                "session_id": "full4",
                "event_prefix": "v-full",
                "track": "103(132)",
                "session_image": "full4.png",
                "chair": [
                    "Jonathan C Roberts"
                ],
                "organizers": [],
                "time_start": "2023-10-22T00:00:00Z",
                "time_end": "2023-10-25T04:15:00Z",
                "discord_category": "",
                "discord_channel": "",
                "discord_channel_id": "",
                "discord_link": "",
                "zoom_meeting": "",
                "zoom_password": "",
                "zoom_link": "",
                "time_slots": [
                    {
                        "slot_id": "v-tvcg-10057994",
                        "session_id": "full4",
                        "title": "DMiner: Dashboard Design Mining and Recommendation",
                        "contributors": [
                            "Yanna Lin"
                        ],
                        "authors": [],
                        "abstract": "Dashboards, which comprise multiple views on a single display, help analyze and communicate multiple perspectives of data simultaneously. However, creating effective and elegant dashboards is challenging since it requires careful and logical arrangement and coordination of multiple visualizations. To solve the problem, we propose a data-driven approach for mining design rules from dashboards and automating dashboard organization. Specifically, we focus on two prominent aspects of the organization: arrangement, which describes the position, size, and layout of each view in the display space; and coordination, which indicates the interaction between pairwise views. We build a new dataset containing 854 dashboards crawled online, and develop feature engineering methods for describing the single views and view-wise relationships in terms of data, encoding, layout, and interactions. Further, we identify design rules among those features and develop a recommender for dashboard design. We demonstrate the usefulness of DMiner through an expert study and a user study. The expert study shows that our extracted design rules are reasonable and conform to the design practice of experts. Moreover, a comparative user study shows that our recommender could help automate dashboard organization and reach human-level performance. In summary, our work offers a promising starting point for design mining visualizations to build recommenders.",
                        "uid": "v-tvcg-10057994",
                        "time_stamp": "2023-10-25T03:00:00Z",
                        "time_start": "2023-10-25T03:00:00Z",
                        "time_end": "2023-10-25T03:12:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "1",
                        "paper_award": "",
                        "image_caption": "The workflow of DMiner. This paper proposes DMiner as a framework for dashboard design mining and automatic recommendation. With the dashboard dataset as the input, DMiner: (A) first surveys a set of features important for dashboard design, and then extracts those features to delineate dashboard designs comprehensively. These are categorized into two types, i.e., single-view features such as data and encoding and pairwise-view features such as coordination and relative position; (B) then mines design rules using decision rule approach, and further filters them; and (C) finally leverages these rules for recommending dashboard arrangement and coordination.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    },
                    {
                        "slot_id": "v-tvcg-9916137",
                        "session_id": "full4",
                        "title": "Revisiting the Design Patterns of Composite Visualizations",
                        "contributors": [
                            "Dazhen Deng"
                        ],
                        "authors": [],
                        "abstract": "Composite visualization is a popular design strategy that represents complex datasets by integrating multiple visualizations in a meaningful and aesthetic layout, such as juxtaposition, overlay, and nesting. With this strategy, numerous novel designs have been proposed in visualization publications to accomplish various visual analytic tasks. However, there is a lack of understanding of design patterns of composite visualization, thus failing to provide holistic design space and concrete examples for practical use. In this paper, we opted to revisit the composite visualizations in IEEE VIS publications and answered what and how visualizations of different types are composed together. To achieve this, we first constructed a corpus of composite visualizations from the publications and analyzed common practices, such as the pattern distributions and co-occurrence of visualization types. From the analysis, we obtained insights into different design patterns on the utilities and their potential pros and cons. Furthermore, we discussed usage scenarios of our taxonomy and corpus and how future research on visualization composition can be conducted on the basis of this study.",
                        "uid": "v-tvcg-9916137",
                        "time_stamp": "2023-10-25T03:12:00Z",
                        "time_start": "2023-10-25T03:12:00Z",
                        "time_end": "2023-10-25T03:24:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "1",
                        "paper_award": "",
                        "image_caption": "Design Patterns of Composite Visualizations",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    },
                    {
                        "slot_id": "v-tvcg-10029921",
                        "session_id": "full4",
                        "title": "Semi-Automatic Layout Adaptation for Responsive Multiple-View Visualization Design",
                        "contributors": [
                            "Wei Zeng"
                        ],
                        "authors": [],
                        "abstract": "Multiple-view (MV) visualizations have become ubiquitous for visual communication and exploratory data visualization. However, most existing MV visualizations are designed for the desktop, which can be unsuitable for the continuously evolving displays of varying screen sizes. In this paper, we present a two-stage adaptation framework that supports the automated retargeting and semi-automated tailoring of a desktop MV visualization for rendering on devices with displays of varying sizes. First, we cast layout retargeting as an optimization problem and propose a simulated annealing technique that can automatically preserve the layout of multiple views. Second, we enable fine-tuning for the visual appearance of each view, using a rule-based auto configuration method complemented with an interactive interface for chart-oriented encoding adjustment. To demonstrate the feasibility and expressivity of our proposed approach, we present a gallery of MV visualizations that have been adapted from the desktop to small displays. We also report the result of a user study comparing visualizations generated using our approach with those by existing methods. The outcome indicates that the participants generally prefer visualizations generated using our approach and find them to be easier to use.",
                        "uid": "v-tvcg-10029921",
                        "time_stamp": "2023-10-25T03:24:00Z",
                        "time_start": "2023-10-25T03:24:00Z",
                        "time_end": "2023-10-25T03:36:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "1",
                        "paper_award": "",
                        "image_caption": "This work considers the design space of a MV visualization with regards to visualization space, display property, and interaction.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    },
                    {
                        "slot_id": "v-full-1015",
                        "session_id": "full4",
                        "title": "From Information to Choice: A Critical Inquiry Into Visualization Tools for Decision Making",
                        "contributors": [
                            "Emre Oral"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "v-full-1015",
                        "time_stamp": "2023-10-25T03:36:00Z",
                        "time_start": "2023-10-25T03:36:00Z",
                        "time_end": "2023-10-25T03:48:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "1",
                        "paper_award": "",
                        "image_caption": "At the top, our research goal is depicted. To the left, a mosaic of 27 decision-focused visualization tools identified in our research. On the right, an image of a woman holding a magnifying glass, showcasing our evaluation metrics, including flexibility and visibility. DR, AI, domain, and MCDM are attached to the magnifier to highlight the additional tags regarding our investigation.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    },
                    {
                        "slot_id": "v-full-1122",
                        "session_id": "full4",
                        "title": "Heuristics for Supporting Cooperative Dashboard Design",
                        "contributors": [
                            "Vidya Setlur"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "v-full-1122",
                        "time_stamp": "2023-10-25T03:48:00Z",
                        "time_start": "2023-10-25T03:48:00Z",
                        "time_end": "2023-10-25T04:00:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "1",
                        "paper_award": "",
                        "image_caption": "Successes and failures of cooperative dashboard design throughout the five stages of an analytical conversation: initiation, grounding, turn taking, repair & refinement, and close.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    },
                    {
                        "slot_id": "v-full-1642",
                        "session_id": "full4",
                        "title": "Transitioning to a Commercial Dashboarding System: Socio-technical Observations and Opportunities",
                        "contributors": [
                            "Conny Walchshofer"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "v-full-1642",
                        "time_stamp": "2023-10-25T04:00:00Z",
                        "time_start": "2023-10-25T04:00:00Z",
                        "time_end": "2023-10-25T04:12:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "1",
                        "paper_award": "",
                        "image_caption": "A summary of our 14 socio-technical observations and three opportunities from an interview study conducted at a large, conventional company as they transitioned to use Power BI.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    }
                ]
            },
            {
                "title": "Education and Assessment",
                "session_id": "full5",
                "event_prefix": "v-full",
                "track": "106(234)",
                "session_image": "full5.png",
                "chair": [
                    "Niklas Elmqvist"
                ],
                "organizers": [],
                "time_start": "2023-10-22T00:00:00Z",
                "time_end": "2023-10-25T06:00:00Z",
                "discord_category": "",
                "discord_channel": "",
                "discord_channel_id": "",
                "discord_link": "",
                "zoom_meeting": "",
                "zoom_password": "",
                "zoom_link": "",
                "time_slots": [
                    {
                        "slot_id": "v-full-1368",
                        "session_id": "full5",
                        "title": "SpeechMirror: A Multimodal Visual Analytics System for Personalized Reflection of Online Public Speaking Effectiveness",
                        "contributors": [
                            "Zeyuan Huang"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "v-full-1368",
                        "time_stamp": "2023-10-25T04:45:00Z",
                        "time_start": "2023-10-25T04:45:00Z",
                        "time_end": "2023-10-25T04:57:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "1",
                        "paper_award": "",
                        "image_caption": "Our SpeechMirror system facilitates reflection on a speech based on insights from a collection of online speeches.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    },
                    {
                        "slot_id": "v-full-1217",
                        "session_id": "full5",
                        "title": "VisGrader: Automatic Grading of D3 Visualizations",
                        "contributors": [
                            "Matthew Hull"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "v-full-1217",
                        "time_stamp": "2023-10-25T04:57:00Z",
                        "time_start": "2023-10-25T04:57:00Z",
                        "time_end": "2023-10-25T05:09:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "1",
                        "paper_award": "",
                        "image_caption": "VisGrader: Automatic Grading of D3 Visualizations",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    },
                    {
                        "slot_id": "v-tvcg-10045801",
                        "session_id": "full5",
                        "title": "Anchorage: Visual Analysis of Satisfaction in Customer Service Videos via Anchor Events",
                        "contributors": [
                            "Jason Wong"
                        ],
                        "authors": [],
                        "abstract": "Delivering customer services through video communications has brought new opportunities to analyze customer satisfaction for quality management. However, due to the lack of reliable self-reported responses, service providers are troubled by the inadequate estimation of customer services and the tedious investigation into multimodal video recordings. We introduce Anchorage , a visual analytics system to evaluate customer satisfaction by summarizing multimodal behavioral features in customer service videos and revealing abnormal operations in the service process. We leverage the semantically meaningful operations to introduce structured event understanding into videos which help service providers quickly navigate to events of their interest. Anchorage supports a comprehensive evaluation of customer satisfaction from the service and operation levels and efficient analysis of customer behavioral dynamics via multifaceted visualization views. We extensively evaluate Anchorage through a case study and a carefully-designed user study. The results demonstrate its effectiveness and usability in assessing customer satisfaction using customer service videos. We found that introducing event contexts in assessing customer satisfaction can enhance its performance without compromising annotation precision. Our approach can be adapted in situations where unlabelled and unstructured videos are collected along with sequential records.",
                        "uid": "v-tvcg-10045801",
                        "time_stamp": "2023-10-25T05:09:00Z",
                        "time_start": "2023-10-25T05:09:00Z",
                        "time_end": "2023-10-25T05:21:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "1",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    },
                    {
                        "slot_id": "v-full-1210",
                        "session_id": "full5",
                        "title": "Adaptive Assessment of Visualization Literacy",
                        "contributors": [
                            "Yuan Cui"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "v-full-1210",
                        "time_stamp": "2023-10-25T05:21:00Z",
                        "time_start": "2023-10-25T05:21:00Z",
                        "time_end": "2023-10-25T05:33:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "1",
                        "paper_award": "",
                        "image_caption": "The process of developing adaptive visualization literacy assessments: A-VLAT and A-CALVI. We start with the item banks of VLAT and CALVI and use the parameters from item analysis to construct the CAT algorithms for the adaptive assessments. We then evaluate their validity and reliability via four online studies. The annotations in blue are the components\u2019 corresponding sections.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    },
                    {
                        "slot_id": "v-full-1519",
                        "session_id": "full5",
                        "title": "Causality-Based Visual Analysis of Questionnaire Responses",
                        "contributors": [
                            "Renzhong Li"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "v-full-1519",
                        "time_stamp": "2023-10-25T05:33:00Z",
                        "time_start": "2023-10-25T05:33:00Z",
                        "time_end": "2023-10-25T05:45:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "1",
                        "paper_award": "",
                        "image_caption": "The interface of QE: (A) The question list view displays all questions. (B) The question combination view provides an overview of the whole dataset. (C) The causal view presents the causality in a relevant question combination. (D) The respondent view visualizes the clusters of respondents divided by a set of relevant questions for users to deep dive.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    },
                    {
                        "slot_id": "v-full-1428",
                        "session_id": "full5",
                        "title": "Challenges and Opportunities in Data Visualization Education: A Call to Action",
                        "contributors": [
                            "Benjamin Bach"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "v-full-1428",
                        "time_stamp": "2023-10-25T05:45:00Z",
                        "time_start": "2023-10-25T05:45:00Z",
                        "time_end": "2023-10-25T05:57:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "1",
                        "paper_award": "",
                        "image_caption": "In this paper, we identify 19 challenges informed by our collective practical experience in data visualization education. We organize these challenges around seven themes and formulate 43 research questions to address these challenges. In a call to action, we conclude with 5 cross-cutting opportunities. We aim to inspire researchers and educators to drive visualization education forward and discuss why, how, who and where we educate.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    }
                ]
            },
            {
                "title": "Evaluation",
                "session_id": "full6",
                "event_prefix": "v-full",
                "track": "109(234)",
                "session_image": "full6.png",
                "chair": [
                    "Matthew Kay"
                ],
                "organizers": [],
                "time_start": "2023-10-22T00:00:00Z",
                "time_end": "2023-10-26T01:00:00Z",
                "discord_category": "",
                "discord_channel": "",
                "discord_channel_id": "",
                "discord_link": "",
                "zoom_meeting": "",
                "zoom_password": "",
                "zoom_link": "",
                "time_slots": [
                    {
                        "slot_id": "v-tvcg-10034850",
                        "session_id": "full6",
                        "title": "A Qualitative Interview Study of Distributed Tracing Visualisation: A Characterisation of Challenges and Opportunities in Visualisation Research",
                        "contributors": [
                            "Thomas Davidson"
                        ],
                        "authors": [],
                        "abstract": "Distributed tracing tools have emerged in recent years to enable the operators of modern internet applications to troubleshoot cross-component problems in deployed applications. Due to the rich, detailed diagnostic data captured by distributed tracing tools, effectively presenting this data is important. However, use of visualisation to enable perceptual sensemaking of this complex data in today\u2019s distributed tracing tools has received relatively little attention. Consequently, operators struggle to make effective use of existing tools. In this paper we present the first characterisation of distributed tracing visualisation through a qualitative interview study with six practitioners from two large internet companies. Across two rounds of 1-on-1 interviews we use grounded theory coding to establish users, extract concrete use cases and identify shortcomings of existing distributed tracing tools. We derive guidelines for development of future distributed tracing tools and expose several open research problems that have wide reaching implications for visualisation research and other domain applications.",
                        "uid": "v-tvcg-10034850",
                        "time_stamp": "2023-10-25T23:45:00Z",
                        "time_start": "2023-10-25T23:45:00Z",
                        "time_end": "2023-10-25T23:57:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "1",
                        "paper_award": "",
                        "image_caption": "A diagram depicting distributed systems and existing analytical solutions,  the work we carried out in this project - a qualitative interview study  and grounded theory coding, and the outputs of the project: a  characterisation of distributed tracing, 5 key challenges, and 8 design  guidelines",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    },
                    {
                        "slot_id": "v-tvcg-10002316",
                        "session_id": "full6",
                        "title": "Evaluating Glyph Design for Showing Large-Magnitude-Range Quantum Spins",
                        "contributors": [
                            "Jian Chen"
                        ],
                        "authors": [],
                        "abstract": "We present experimental results to explore a form of bivariate glyphs for representing large-magnitude-range vectors. The glyphs meet two conditions: (1) two visual dimensions are separable; and (2) one of the two visual dimensions uses a categorical representation (e.g., a categorical colormap). We evaluate how much these two conditions determine the bivariate glyphs\u2019 effectiveness. The first experiment asks participants to perform three local tasks requiring reading no more than two glyphs. The second experiment scales up the search space in global tasks when participants must look at the entire scene of hundreds of vector glyphs to get an answer. Our results support that the first condition is necessary for local tasks when a few items are compared. But it is not enough for understanding a large amount of data. The second condition is necessary for perceiving global structures of examining very complex datasets. Participants\u2019 comments reveal that the categorical features in the bivariate glyphs trigger emergent optimal viewers\u2019 behaviors. This work contributes to perceptually accurate glyph representations for revealing patterns from large scientific results. We release source code, quantum physics data, training documents, participants\u2019 answers, and statistical analyses for reproducible science at https : //osf.io/4xcf5/?viewonly = 94123139df9c4ac984a1e0df811cd580.",
                        "uid": "v-tvcg-10002316",
                        "time_stamp": "2023-10-25T23:57:00Z",
                        "time_start": "2023-10-25T23:57:00Z",
                        "time_end": "2023-10-26T00:09:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "0",
                        "paper_award": "",
                        "image_caption": "We introduce a novel glyph design capable of conveying both overall scene structures and local numerical values with precision akin to a text display. The glyph scene has been employed to analyze highly complex three-dimensional quantum spins at the National Institute of Standards and Technology.  In this design, colors and glyph sizes are employed to cluster similar data, facilitating the visual comprehension of the distribution of spin charge densities, with a very large dynamic range. Our use of size visual attributes simplifies the search process of locating specific items, enabling quantum physicists to quickly extract quantitative data from the glyphs, thereby enhancing their research efficiency.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    },
                    {
                        "slot_id": "v-tvcg-10058545",
                        "session_id": "full6",
                        "title": "Evaluating the Impact of Uncertainty Visualization on Model Reliance",
                        "contributors": [
                            "Jieqiong Zhao"
                        ],
                        "authors": [],
                        "abstract": "Machine learning models have gained traction as decision support tools for tasks that require processing copious amounts of data. However, to achieve the primary benefits of automating this part of decision-making, people must be able to trust the machine learning model's outputs. In order to enhance people's trust and promote appropriate reliance on the model, visualization techniques such as interactive model steering, performance analysis, model comparison, and uncertainty visualization have been proposed. In this study, we tested the effects of two uncertainty visualization techniques in a college admissions forecasting task, under two task difficulty levels, using Amazon's Mechanical Turk platform. Results show that (1) people's reliance on the model depends on the task difficulty and level of machine uncertainty and (2) ordinal forms of expressing model uncertainty are more likely to calibrate model usage behavior. These outcomes emphasize that reliance on decision support tools can depend on the cognitive accessibility of the visualization technique and perceptions of model performance and task difficulty.",
                        "uid": "v-tvcg-10058545",
                        "time_stamp": "2023-10-26T00:09:00Z",
                        "time_start": "2023-10-26T00:09:00Z",
                        "time_end": "2023-10-26T00:21:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "1",
                        "paper_award": "",
                        "image_caption": "In a human-AI teaming collaborative environment, humans have to appropriately incorporate AI/ML model predictions to fully leverage the benefits of automation. We conducted an experiment using a crowdsourcing platform to assess the impact of the presence and absence of uncertainty information on a college admission forecasting task. Furthermore, we examined how uncertainty information influences model adoption behavior using two uncertainty visualization techniques and two levels of task difficulty. Our findings indicate that ordinal forms of expressing model uncertainty are more likely to calibrate humans' adoption of model predictions.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    },
                    {
                        "slot_id": "v-tvcg-9905944",
                        "session_id": "full6",
                        "title": "Fitting Bell Curves to Data Distributions using Visualization",
                        "contributors": [
                            "Niklas Elmqvist"
                        ],
                        "authors": [],
                        "abstract": "Idealized probability distributions, such as normal or other curves, lie at the root of confirmatory statistical tests. But how well do people understand these idealized curves? In practical terms, does the human visual system allow us to match sample data distributions with hypothesized population distributions from which those samples might have been drawn? And how do different visualization techniques impact this capability? This paper shares the results of a crowdsourced experiment that tested the ability of respondents to fit normal curves to four different data distribution visualizations: bar histograms, dotplot histograms, strip plots, and boxplots. We find that the crowd can estimate the center (mean) of a distribution with some success and little bias. We also find that people generally overestimate the standard deviation---which we dub the \"umbrella effect\" because people tend to want to cover the whole distribution using the curve, as if sheltering it from the heavens above---and that strip plots yield the best accuracy.",
                        "uid": "v-tvcg-9905944",
                        "time_stamp": "2023-10-26T00:21:00Z",
                        "time_start": "2023-10-26T00:21:00Z",
                        "time_end": "2023-10-26T00:33:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "1",
                        "paper_award": "",
                        "image_caption": "Can people fit a normal curve to a data distribution through visual interactions alone?   In other words, do we have the visual intuitions to see the connection between an image of a sample distribution and the population distribution from which those samples might have been drawn?    And does graphic design matter?  Our experiment sought answers to these questions.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    },
                    {
                        "slot_id": "v-full-1048",
                        "session_id": "full6",
                        "title": "A Heuristic Approach for Dual Expert/End-User Evaluation of Guidance in Visual Analytics",
                        "contributors": [
                            "Davide Ceneda"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "v-full-1048",
                        "time_stamp": "2023-10-26T00:33:00Z",
                        "time_start": "2023-10-26T00:33:00Z",
                        "time_end": "2023-10-26T00:45:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "1",
                        "paper_award": "",
                        "image_caption": "We introduce and validate a practical evaluation methodology for guidance in visual analytics. We identify eight quality criteria to be fulfilled and to facilitate actual evaluation studies, we derive two sets of heuristics, to be used by VA Users and VA experts.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    },
                    {
                        "slot_id": "v-full-1542",
                        "session_id": "full6",
                        "title": "The Arrangement of Marks Impacts Afforded Messages: Ordering, Partitioning, Spacing, and Coloring in Bar Charts",
                        "contributors": [
                            "Racquel Fygenson"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "v-full-1542",
                        "time_stamp": "2023-10-26T00:45:00Z",
                        "time_start": "2023-10-26T00:45:00Z",
                        "time_end": "2023-10-26T00:57:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "1",
                        "paper_award": "",
                        "image_caption": "A 2x2 grid of bar chart arrangements. Top left: ORDER shows two bar charts. One sorted in ascending order, the other in descending order. Top right: PARTITIONS shows two bar charts. One has bars stacked into two columns, each consisting of three smaller bars. The other shows the bars side-by-side. Bottom left: COLOR V SPACING shows two bar charts. One is uniformly spaced, using color to group the bars, while the other is uniformly colored, using spacing to group the bars. Bottom right: SPACING shows two bar charts. One uniformly spaced, the other spaced to make two groups of bars.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    }
                ]
            },
            {
                "title": "GeoVis| (Rooms 105/106 combined)",
                "session_id": "full7",
                "event_prefix": "v-full",
                "track": "105(234)",
                "session_image": "full7.png",
                "chair": [
                    "Di Weng"
                ],
                "organizers": [],
                "time_start": "2023-10-22T00:00:00Z",
                "time_end": "2023-10-26T23:15:00Z",
                "discord_category": "",
                "discord_channel": "",
                "discord_channel_id": "",
                "discord_link": "",
                "zoom_meeting": "",
                "zoom_password": "",
                "zoom_link": "",
                "time_slots": [
                    {
                        "slot_id": "v-tvcg-10026242",
                        "session_id": "full7",
                        "title": "IF-City: Intelligible Fair City Planning to Measure, Explain and Mitigate Inequality",
                        "contributors": [
                            "Yan Lyu"
                        ],
                        "authors": [],
                        "abstract": "With the increasing pervasiveness of Artificial Intelligence (AI), many visual analytics tools have been proposed to examine fairness, but they mostly focus on data scientist users. Instead, tackling fairness must be inclusive and involve domain experts with specialized tools and workflows. Thus, domain-specific visualizations are needed for algorithmic fairness. Furthermore, while much work on AI fairness has focused on predictive decisions, less has been done for fair allocation and planning, which require human expertise and iterative design to integrate myriad constraints. We propose the Intelligible Fair Allocation (IF-Alloc) Framework that leverages explanations of causal attribution (Why), contrastive (Why Not) and counterfactual reasoning (What If, How To) to aid domain experts to assess and alleviate unfairness in allocation problems. We apply the framework to fair urban planning for designing cities that provide equal access to amenities and benefits for diverse resident types. Specifically, we propose an interactive visual tool, Intelligible Fair City Planner (IF-City), to help urban planners to perceive inequality across groups, identify and attribute sources of inequality, and mitigate inequality with automatic allocation simulations and constraint-satisfying recommendations (IF-Plan). We demonstrate and evaluate the usage and usefulness of IF-City on a real neighborhood in New York City, US, with practicing urban planners from multiple countries, and discuss generalizing our findings, application, and framework to other use cases and applications of fair allocation.",
                        "uid": "v-tvcg-10026242",
                        "time_stamp": "2023-10-26T22:00:00Z",
                        "time_start": "2023-10-26T22:00:00Z",
                        "time_end": "2023-10-26T22:12:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "1",
                        "paper_award": "",
                        "image_caption": "Intelligible Fair City Planner (IF-City) is an interactive visualization tool to help urban planners to perceive inequality across groups, identify and attribute sources of inequality, and mitigate inequality with automatic allocation simulations and constraint-satisfying recommendations.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    },
                    {
                        "slot_id": "v-tvcg-10057127",
                        "session_id": "full7",
                        "title": "MoReVis: A Visual Summary for Spatiotemporal Moving Regions",
                        "contributors": [
                            "Giovani Valdrighi"
                        ],
                        "authors": [],
                        "abstract": "Spatial and temporal interactions are central and fundamental in many activities in our world. A common problem faced when visualizing this type of data is how to provide an overview that helps users navigate efficiently. Traditional approaches use coordinated views or 3D metaphors like the Space-time cube to tackle this problem. However, they suffer from overplotting and often lack spatial context, hindering data exploration. More recent techniques, such as MotionRugs, propose compact temporal summaries based on 1D projection. While powerful, these techniques do not support the situation for which the spatial extent of the objects and their intersections is relevant, such as the analysis of surveillance videos or tracking weather storms. In this paper, we propose MoReVis, a visual overview of spatiotemporal data that considers the objects' spatial extent and strives to show spatial interactions among these objects by displaying spatial intersections. Like previous techniques, our method involves projecting the spatial coordinates to 1D to produce compact summaries. However, our solution's core consists of performing a layout optimization step that sets the size and positions of the visual marks on the summary to resemble the actual values on the original space. We also provide multiple interactive mechanisms to make interpreting the results more straightforward for the user. We perform an extensive experimental evaluation and usage scenarios. Moreover, we evaluated the usefulness of MoReVis in a study with 9 participants. The results point out the effectiveness and suitability of our method in representing different datasets compared to traditional techniques.",
                        "uid": "v-tvcg-10057127",
                        "time_stamp": "2023-10-26T22:12:00Z",
                        "time_start": "2023-10-26T22:12:00Z",
                        "time_end": "2023-10-26T22:24:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "1",
                        "paper_award": "",
                        "image_caption": "MoReVis is a visualization technique designed for spatiotemporal datasets of Moving Regions, entities with a spatial extension that evolves through time.  MoReVis uses a metaphor in which time is represented on one axis and space on another, and a curved ribbon of variable height represents each entity.  The resulting visualization is able to represent:  The movement of entities by the vertical movement of the ribbons,  The variable area of entities by the ribbon heights,  and the intersection between spatial regions through the intersection of ribbons.  A visual interface with linked panels and intersections was developed to support MoReVis.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    },
                    {
                        "slot_id": "v-tvcg-9991899",
                        "session_id": "full7",
                        "title": "Multilevel Visual Analysis of Aggregate Geo-Networks",
                        "contributors": [
                            "Zikun Deng"
                        ],
                        "authors": [],
                        "abstract": "Numerous patterns found in urban phenomena, such as air pollution and human mobility, can be characterized as many directed geospatial networks (geo-networks) that represent spreading processes in urban space. These geo-networks can be analyzed from multiple levels, ranging from the macro-level of summarizing all geo-networks, meso-level of comparing or summarizing parts of geo-networks, and micro-level of inspecting individual geo-networks. Most of the existing visualizations cannot support multilevel analysis well. These techniques work by: 1) showing geo-networks separately with multiple maps leads to heavy context switching costs between different maps; 2) summarizing all geo-networks into a single network can lead to the loss of individual information; 3) drawing all geo-networks onto one map might suffer from the visual scalability issue in distinguishing individual geo-networks. In this study, we propose GeoNetverse, a novel visualization technique for analyzing aggregate geo-networks from multiple levels. Inspired by metro maps, GeoNetverse balances the overview and details of the geo-networks by placing the edges shared between geo-networks in a stacked manner. To enhance the visual scalability, GeoNetverse incorporates a level-of-detail rendering, a progressive crossing minimization, and a coloring technique. A set of evaluations was conducted to evaluate GeoNetverse from multiple perspectives.",
                        "uid": "v-tvcg-9991899",
                        "time_stamp": "2023-10-26T22:24:00Z",
                        "time_start": "2023-10-26T22:24:00Z",
                        "time_end": "2023-10-26T22:36:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "1",
                        "paper_award": "",
                        "image_caption": "A novel visualization is proposed for depicting mutliple geo-network on the map. First, the geo-networks are aggregated as one and each geo-network is embbed into the aggregation by stacking the edges. The visualization is equipped with level-of-detail rendering, crossing minimization, and coloring for improved visual scalablity.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    },
                    {
                        "slot_id": "v-tvcg-9870679",
                        "session_id": "full7",
                        "title": "When, Where and How does it fail? A Spatial-temporal Visual Analytics Approach for Interpretable Object Detection in Autonomous Driving",
                        "contributors": [
                            "Siming Chen"
                        ],
                        "authors": [],
                        "abstract": "Arguably the most representative application of artificial intelligence, autonomous driving systems usually rely on computer vision techniques to detect the situations of the external environment. Object detection underpins the ability of scene understanding in such systems. However, existing object detection algorithms often behave as a black box, so when a model fails, no information is available on When, Where and How the failure happened. In this paper, we propose a visual analytics approach to help model developers interpret the model failures. The system includes the micro- and macro-interpreting modules to address the interpretability problem of object detection in autonomous driving. The micro-interpreting module extracts and visualizes the features of a convolutional neural network (CNN) algorithm with density maps, while the macro-interpreting module provides spatial-temporal information of an autonomous driving vehicle and its environment. With the situation awareness of the spatial, temporal and neural network information, our system facilitates the understanding of the results of object detection algorithms, and helps the model developers better understand, tune and develop the models. We use real-world autonomous driving data to perform case studies by involving domain experts in computer vision and autonomous driving to evaluate our system. The results from our interviews with them show the effectiveness of our approach.",
                        "uid": "v-tvcg-9870679",
                        "time_stamp": "2023-10-26T22:36:00Z",
                        "time_start": "2023-10-26T22:36:00Z",
                        "time_end": "2023-10-26T22:48:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "1",
                        "paper_award": "",
                        "image_caption": "System User Interface: (a) Micro-interpreting module: feature visualization including Density Map and Object Projection; (b) Macro-interpreting module: temporal visualization including Autonomous Driving Vehicle States and Object-level Density Maps; (c) Macro-interpreting module: spatial visualization including Scene and Trajectory; (d) Control for selecting and filtering object classes, results and locations. (e) A legend referring to all views introduces color encoding. (f) A guidance example to use our analysis workflow.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    },
                    {
                        "slot_id": "v-full-1343",
                        "session_id": "full7",
                        "title": "GeoExplainer: A Visual Analytics Framework for Spatial Modeling Contextualization and Report Generation",
                        "contributors": [
                            "Fan Lei"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "v-full-1343",
                        "time_stamp": "2023-10-26T22:48:00Z",
                        "time_start": "2023-10-26T22:48:00Z",
                        "time_end": "2023-10-26T23:00:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "1",
                        "paper_award": "",
                        "image_caption": "The model analysis and report authoring interface of GeoExplainer.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    },
                    {
                        "slot_id": "v-full-1797",
                        "session_id": "full7",
                        "title": "The Urban Toolkit: A Grammar-based Framework for Urban Visual Analytics",
                        "contributors": [
                            "Fabio Miranda"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "v-full-1797",
                        "time_stamp": "2023-10-26T23:00:00Z",
                        "time_start": "2023-10-26T23:00:00Z",
                        "time_end": "2023-10-26T23:12:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "1",
                        "paper_award": "",
                        "image_caption": "While cities around the world are looking for smart ways to use new advances in data collection, management, and analysis to address their day-to-day problems, the complex nature of urban issues and the overwhelming amount of available structured and unstructured data have posed significant challenges in translating these efforts into actionable insights. Recently, urban visual analytics tools have significantly helped tackle these challenges. With this in mind, we present the Urban Toolkit, a flexible and extensible visualization framework that enables the easy authoring of web-based visualizations through a new high-level grammar specifically built with common urban cases in mind.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    }
                ]
            },
            {
                "title": "Grammars",
                "session_id": "full8",
                "event_prefix": "v-full",
                "track": "105(234)",
                "session_image": "full8.png",
                "chair": [
                    "Dominik Moritz"
                ],
                "organizers": [],
                "time_start": "2023-10-22T00:00:00Z",
                "time_end": "2023-10-25T04:15:00Z",
                "discord_category": "",
                "discord_channel": "",
                "discord_channel_id": "",
                "discord_link": "",
                "zoom_meeting": "",
                "zoom_password": "",
                "zoom_link": "",
                "time_slots": [
                    {
                        "slot_id": "v-full-1307",
                        "session_id": "full8",
                        "title": "Visual Analytics for Understanding Draco's Knowledge Base",
                        "contributors": [
                            "Johanna Schmidt"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "v-full-1307",
                        "time_stamp": "2023-10-25T03:00:00Z",
                        "time_start": "2023-10-25T03:00:00Z",
                        "time_end": "2023-10-25T03:12:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "1",
                        "paper_award": "",
                        "image_caption": "Visually inspecting Draco's recommendations. Our interactive Visual Analytics solution allows users to explore the set of rules the visualization recommendation system Draco is built on. On the left side, four recommended visualizations are shown. Every recommendation has costs assigned, which relates to how many rules have been violated by this recommendation. On the right, side we present our hypergraph-based visualization of the set of rules and constraints that are used by Draco. By selecting recommendations on the left (A, blue, and B, red), the rules violated by these visualizations are highlighted in the graph (red and blue dashed lines).",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    },
                    {
                        "slot_id": "v-full-1116",
                        "session_id": "full8",
                        "title": "DIVI: Dynamically Interactive Visualization",
                        "contributors": [
                            "Luke S Snyder"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "v-full-1116",
                        "time_stamp": "2023-10-25T03:12:00Z",
                        "time_start": "2023-10-25T03:12:00Z",
                        "time_end": "2023-10-25T03:24:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "0",
                        "has_video": "0",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    },
                    {
                        "slot_id": "v-full-1212",
                        "session_id": "full8",
                        "title": "ggdist: Visualizations of Distributions and Uncertainty in the Grammar of Graphics",
                        "contributors": [
                            "Matthew Kay"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "v-full-1212",
                        "time_stamp": "2023-10-25T03:24:00Z",
                        "time_start": "2023-10-25T03:24:00Z",
                        "time_end": "2023-10-25T03:36:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "1",
                        "paper_award": "",
                        "image_caption": "Examples from the three major classes of geometries in ggdist: (A) slabintervals, such as density plots, CDFs, intervals, and gradient plots; (B) lineribbons, such as uncertainty bands and fan charts; and (C) dots, such as dotplots and beeswarm charts. Myriad combinations of these are possible, leading to charts like (D) logit dotplots and (E) raincloud plots.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    },
                    {
                        "slot_id": "v-full-1328",
                        "session_id": "full8",
                        "title": "Metrics-Based Evaluation and Comparison of Visualization Notations",
                        "contributors": [
                            "Nicolas Kruchten"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "v-full-1328",
                        "time_stamp": "2023-10-25T03:36:00Z",
                        "time_start": "2023-10-25T03:36:00Z",
                        "time_end": "2023-10-25T03:48:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "1",
                        "paper_award": "",
                        "image_caption": "Evaluating or comparing a visualization notation (such as ggplot2 or Matplotlib) with others is typically a qualitative and ad hoc process. We introduce a metrics-based approach to help readers structure that analysis via quantitative measurements based on Cognitive Dimensions of Notation-inspired properties. This approach allows analysts a measure of distance in their considerations of new notations, and provides some distance to previously close reading of textual systems for expressing visualizations.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    },
                    {
                        "slot_id": "v-full-1150",
                        "session_id": "full8",
                        "title": "Mosaic: An Architecture for Scalable & Interoperable Data Views",
                        "contributors": [
                            "Jeffrey Heer"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "v-full-1150",
                        "time_stamp": "2023-10-25T03:48:00Z",
                        "time_start": "2023-10-25T03:48:00Z",
                        "time_end": "2023-10-25T04:00:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "1",
                        "paper_award": "",
                        "image_caption": "A Mosaic-based interface for interactive visual exploration of all 1.8 billion stars in the Gaia star catalog. A high-resolution density map of the sky reveals our Milky Way and satellite galaxies.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    },
                    {
                        "slot_id": "v-full-1199",
                        "session_id": "full8",
                        "title": "Mystique: Deconstructing SVG Charts for Layout Reuse",
                        "contributors": [
                            "Chen Chen"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "v-full-1199",
                        "time_stamp": "2023-10-25T04:00:00Z",
                        "time_start": "2023-10-25T04:00:00Z",
                        "time_end": "2023-10-25T04:12:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "1",
                        "paper_award": "",
                        "image_caption": "Each pair shows an existing chart (left) and a new chart created using Mystique (right). The existing charts are produced using a variety of tools, such as D3, Vega-lite, Mascot, PlotDB, Highcharts, and Data Illustrator.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    }
                ]
            },
            {
                "title": "Graph Visualization",
                "session_id": "full9",
                "event_prefix": "v-full",
                "track": "105(234)",
                "session_image": "full9.png",
                "chair": [
                    "Daniel Archambault"
                ],
                "organizers": [],
                "time_start": "2023-10-22T00:00:00Z",
                "time_end": "2023-10-25T06:00:00Z",
                "discord_category": "",
                "discord_channel": "",
                "discord_channel_id": "",
                "discord_link": "",
                "zoom_meeting": "",
                "zoom_password": "",
                "zoom_link": "",
                "time_slots": [
                    {
                        "slot_id": "v-full-1289",
                        "session_id": "full9",
                        "title": "Calliope-Net: Automatic Generation of Graph Data Facts via Annotated Node-link Diagrams",
                        "contributors": [
                            "Qing Chen"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "v-full-1289",
                        "time_stamp": "2023-10-25T04:45:00Z",
                        "time_start": "2023-10-25T04:45:00Z",
                        "time_end": "2023-10-25T04:57:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "0",
                        "has_video": "0",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    },
                    {
                        "slot_id": "v-full-1061",
                        "session_id": "full9",
                        "title": "Quantivine: A Visualization Approach for Large-scale Quantum Circuit Representation and Analysis",
                        "contributors": [
                            "Zhen Wen"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "v-full-1061",
                        "time_stamp": "2023-10-25T04:57:00Z",
                        "time_start": "2023-10-25T04:57:00Z",
                        "time_end": "2023-10-25T05:09:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "1",
                        "paper_award": "",
                        "image_caption": "The system interface of Quantivine. The Structure View presents the construction of a quantum circuit in a tree diagram including primitive gates, component gates, and repetitive patterns. The Component View provides a flexibly-organized circuit diagram with grouped component gates. The Abstraction View shows a further simplified circuit diagram according to visual abstractions of repetitive patterns. Three Context Views reveal contextual information in the circuit, including qubit provenance, gate placement, and connectivity.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    },
                    {
                        "slot_id": "v-tvcg-9966829",
                        "session_id": "full9",
                        "title": "GraphDecoder: Recovering Diverse Network Graphs from Visualization Images via Attention-Aware Learning",
                        "contributors": [
                            "Sicheng Song"
                        ],
                        "authors": [],
                        "abstract": "DNGs are diverse network graphs with texts and different styles of nodes and edges, including mind maps, modeling graphs, and flowcharts. They are high-level visualizations that are easy for humans to understand but difficult for machines. Inspired by the process of human perception of graphs, we propose a method called GraphDecoder to extract data from raster images. Given a raster image, we extract the content based on a neural network. We built a semantic segmentation network based on U-Net. We increase the attention mechanism module, simplify the network model, and design a specific loss function to improve the model's ability to extract graph data. After this semantic segmentation network, we can extract the data of all nodes and edges. We then combine these data to obtain the topological relationship of the entire DNG. We also provide an interactive interface for users to redesign the DNGs. We verify the effectiveness of our method by evaluations and user studies on datasets collected on the Internet and generated datasets.",
                        "uid": "v-tvcg-9966829",
                        "time_stamp": "2023-10-25T05:09:00Z",
                        "time_start": "2023-10-25T05:09:00Z",
                        "time_end": "2023-10-25T05:21:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "1",
                        "paper_award": "",
                        "image_caption": "ThegraphdecodercanextractDNGdatafromrasterimagesandautomaticallyretargetthem.Ourmethodcan be applied to many DNGs, including flowcharts (A), hierarchical diagrams (B), model graphs, hand-drawn sketches, and mind maps (C).",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    },
                    {
                        "slot_id": "v-tvcg-9829321",
                        "session_id": "full9",
                        "title": "Influence Maximization with Visual Analytics",
                        "contributors": [
                            "Alessio Arleo"
                        ],
                        "authors": [],
                        "abstract": "In social networks, individuals\u2019 decisions are strongly influenced by recommendations from their friends, acquaintances, and favorite renowned personalities. The popularity of online social networking platforms makes them the prime venues to advertise products and promote opinions. The Influence Maximization (IM) problem entails selecting a seed set of users that maximizes the influence spread, i.e., the expected number of users positively influenced by a stochastic diffusion process triggered by the seeds. Engineering and analyzing IM algorithms remains a difficult and demanding task due to the NP-hardness of the problem and the stochastic nature of the diffusion processes. Despite several heuristics being introduced, they often fail in providing enough information on how the network topology affects the diffusion process, precious insights that could help researchers improve their seed set selection. In this paper, we present VAIM, a visual analytics system that supports users in analyzing, evaluating, and comparing information diffusion processes determined by different IM algorithms. Furthermore, VAIM provides useful insights that the analyst can use to modify the seed set of an IM algorithm, so to improve its influence spread. We assess our system by: (i) a qualitative evaluation based on a guided experiment with two domain experts on two different data sets; (ii) a quantitative estimation of the value of the proposed visualization through the ICE-T methodology by Wall (IEEE TVCG - 2018). The twofold assessment indicates that VAIM effectively supports our target users in the visual analysis of the performance of IM algorithms.",
                        "uid": "v-tvcg-9829321",
                        "time_stamp": "2023-10-25T05:21:00Z",
                        "time_start": "2023-10-25T05:21:00Z",
                        "time_end": "2023-10-25T05:33:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "1",
                        "paper_award": "",
                        "image_caption": "A Snapshot of VAIM, a Visual Analytics System for Influence Maximization.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    },
                    {
                        "slot_id": "v-full-1239",
                        "session_id": "full9",
                        "title": "Knowledge Graphs in Practice: Characterizing their Users, Challenges, and Visualization Opportunities",
                        "contributors": [
                            "Harry Li"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "v-full-1239",
                        "time_stamp": "2023-10-25T05:33:00Z",
                        "time_start": "2023-10-25T05:33:00Z",
                        "time_end": "2023-10-25T05:45:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "1",
                        "paper_award": "",
                        "image_caption": "We interviewed nineteen Knowledge Graph (KG) practitioners to identify critical challenges that could be alleviated through visualization design. We found three KG personas \u2013 Builders, Analysts, and Consumers \u2013 with distinct expertise, tasks, and needs. We identify limitations with node-link diagrams, and the need for domain-specific visualizations. Finally, we distill several visualization research directions to improve organic discovery and explainable AI with KGs",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    },
                    {
                        "slot_id": "v-full-1564",
                        "session_id": "full9",
                        "title": "Scalable Hypergraph Visualization",
                        "contributors": [
                            "Professor Eugene Zhang"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "v-full-1564",
                        "time_stamp": "2023-10-25T05:45:00Z",
                        "time_start": "2023-10-25T05:45:00Z",
                        "time_end": "2023-10-25T05:57:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "1",
                        "paper_award": "",
                        "image_caption": "We present a scalable layout optimization framework for polygon visualizations of hypergraphs. Our framework achieves near-optimal polygon layouts for large hypergraphs (left) by first iteratively applying vertex and hyperedge-based simplification operations to scale down the input hypergraph. The coarsest simplified scale is determined by some user-specified criteria (right). After the layout of this simplified scale is optimized, the applied operations are iteratively inverted, and the layout is refined at each intermediate scale until the original scale is reached. An example of an intermediate scale is also shown (middle).",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    }
                ]
            },
            {
                "title": "High Dimensional Data",
                "session_id": "full10",
                "event_prefix": "v-full",
                "track": "106(234)",
                "session_image": "full10.png",
                "chair": [
                    "Helwig Hauser"
                ],
                "organizers": [],
                "time_start": "2023-10-22T00:00:00Z",
                "time_end": "2023-10-24T23:15:00Z",
                "discord_category": "",
                "discord_channel": "",
                "discord_channel_id": "",
                "discord_link": "",
                "zoom_meeting": "",
                "zoom_password": "",
                "zoom_link": "",
                "time_slots": [
                    {
                        "slot_id": "v-full-1712",
                        "session_id": "full10",
                        "title": "A Parallel Framework for Streaming Dimensionality Reduction",
                        "contributors": [
                            "Jiazhi Xia"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "v-full-1712",
                        "time_stamp": "2023-10-24T22:00:00Z",
                        "time_start": "2023-10-24T22:00:00Z",
                        "time_end": "2023-10-24T22:12:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "1",
                        "paper_award": "",
                        "image_caption": "We show the parallel framework proposed in this paper in the red dashed box, and the three core modules of the framework are shown in the blue dotted boxes: (a) an incremental learning method for online embedding function updating, (b) a parametric non-linear embedding method for new data embedding, and (c) a hybrid strategy for local and global embedding updating. With this parallel framework, we proposed SDR to achieve realtime, trustworthy and stable embedding results.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    },
                    {
                        "slot_id": "v-full-1314",
                        "session_id": "full10",
                        "title": "QEVIS: Multi-grained Visualizing of Distributed Query Execution",
                        "contributors": [
                            "Qiaomu Shen"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "v-full-1314",
                        "time_stamp": "2023-10-24T22:12:00Z",
                        "time_start": "2023-10-24T22:12:00Z",
                        "time_end": "2023-10-24T22:24:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "1",
                        "paper_award": "",
                        "image_caption": "QEVIS is a visual analytics system designed for a comprehensive understanding of distributed query execution. It offers a suite of coordinated views, showcasing execution at various granularities. The job view (a) uses a new TDAG layout to depict Map/Reducer job processes and dependencies. Unique anomaly scores in the performance matrix (b) pinpoint suspicious jobs and machines. The task view (c), with a scatter-plot visualization, reveals patterns and highlights significant tasks. Entity list (d) provide additional insights, including detailed statistics on job, task, and machine components, and a profiling view (e) aligns machine status with task execution.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    },
                    {
                        "slot_id": "v-tvcg-9956753",
                        "session_id": "full10",
                        "title": "EVNet: An Explainable Deep Network for Dimension Reduction",
                        "contributors": [
                            "Zang Zelin"
                        ],
                        "authors": [],
                        "abstract": "Dimension reduction (DR) is commonly utilized to capture the intrinsic structure and transform high-dimensional data into low-dimensional space while retaining meaningful properties of the original data. It is used in various applications, such as image recognition, single-cell sequencing analysis, and biomarker discovery. However, contemporary parametric-free and parametric DR techniques suffer from several significant shortcomings, such as the inability to preserve global and local features and the pool generalization performance. On the other hand, regarding explainability, it is crucial to comprehend the embedding process, especially the contribution of each part to the embedding process, while understanding how each feature affects the embedding results that identify critical components and help diagnose the embedding process. To address these problems, we have developed a deep neural network method called EVNet, which provides not only excellent performance in structural maintainability but also explainability to the DR therein. EVNet starts with data augmentation and a manifold-based loss function to improve embedding performance. The explanation is based on saliency maps and aims to examine the trained EVNet parameters and contributions of components during the embedding process. The proposed techniques are integrated with a visual interface to help the user to adjust EVNet to achieve better DR performance and explainability. The interactive visual interface makes it easier to illustrate the data features, compare different DR techniques, and investigate DR. An in-depth experimental comparison shows that EVNet consistently outperforms the state-of-the-art methods in both performance measures and explainability.",
                        "uid": "v-tvcg-9956753",
                        "time_stamp": "2023-10-24T22:24:00Z",
                        "time_start": "2023-10-24T22:24:00Z",
                        "time_end": "2023-10-24T22:36:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "0",
                        "has_video": "0",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    },
                    {
                        "slot_id": "v-tvcg-10158903",
                        "session_id": "full10",
                        "title": "FS/DS: A Theoretical Framework for the Dual Analysis of Feature Space and Data Space",
                        "contributors": [
                            "Frederik Dennig"
                        ],
                        "authors": [],
                        "abstract": "With the surge of data-driven analysis techniques, there is a rising demand for enhancing the exploration of large high-dimensional data by enabling interactions for the joint analysis of features (i.e., dimensions). Such a dual analysis of the feature space and data space is characterized by three components, (1) a view visualizing feature summaries, (2) a view that visualizes the data records, and (3) a bidirectional linking of both plots triggered by human interaction in one of both visualizations, e.g., Linking & Brushing. Dual analysis approaches span many domains, e.g., medicine, crime analysis, and biology. The proposed solutions encapsulate various techniques, such as feature selection or statistical analysis. However, each approach establishes a new definition of dual analysis. To address this gap, we systematically reviewed published dual analysis methods to investigate and formalize the key elements, such as the techniques used to visualize the feature space and data space, as well as the interaction between both spaces. From the information elicited during our review, we propose a unified theoretical framework for dual analysis, encompassing all existing approaches extending the field. We apply our proposed formalization describing the interactions between each component and relate them to the addressed tasks. Additionally, we categorize the existing approaches using our framework and derive future research directions to advance dual analysis by including state-of-the-art visual analysis techniques to improve data exploration.",
                        "uid": "v-tvcg-10158903",
                        "time_stamp": "2023-10-24T22:36:00Z",
                        "time_start": "2023-10-24T22:36:00Z",
                        "time_end": "2023-10-24T22:48:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "1",
                        "paper_award": "",
                        "image_caption": "Due to the rising demand for enhancing the exploration of large high-dimensional data, enabling interactions for the joint analysis of features or dimensions is crucial. Such a dual analysis of the feature space and data space is characterized by three components: A view visualizing feature summaries, a view that visualizes the data items, and a bidirectional linking of both plots. Existing solutions encapsulate techniques, such as feature selection or statistical analysis, but establish idiosyncratic definitions of dual analysis. Thus, we propose a unified theoretical framework for dual analysis, encompassing all existing approaches extending the field.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    },
                    {
                        "slot_id": "v-full-1507",
                        "session_id": "full10",
                        "title": "Class-constrained t-SNE: Combining Data Features and Class Probabilities",
                        "contributors": [
                            "Linhao Meng"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "v-full-1507",
                        "time_stamp": "2023-10-24T22:48:00Z",
                        "time_start": "2023-10-24T22:48:00Z",
                        "time_end": "2023-10-24T23:00:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "1",
                        "paper_award": "",
                        "image_caption": "Class-constrained t-SNE is a dimensionality reduction-based visualization method which integrates both data feature and class probability structures into a single projection view. Users can control the balance between the two structures with an interactive parameter.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    },
                    {
                        "slot_id": "v-full-1469",
                        "session_id": "full10",
                        "title": "ManiVault: A Flexible and Extensible Visual Analytics Framework for High-Dimensional Data",
                        "contributors": [
                            "Alexander Vieth"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "v-full-1469",
                        "time_stamp": "2023-10-24T23:00:00Z",
                        "time_start": "2023-10-24T23:00:00Z",
                        "time_end": "2023-10-24T23:12:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "1",
                        "paper_award": "",
                        "image_caption": "Example screenshot of ManiVault used for the exploration of a hyperspectral imaging data. The high-dimensional data set is explored using an image viewer, two scatterplot views that show different levels a hierarchical embedding, and a line chart that depicts average spectra of three data clusters.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    }
                ]
            },
            {
                "title": "Image and Video",
                "session_id": "full11",
                "event_prefix": "v-full",
                "track": "103(132)",
                "session_image": "full11.png",
                "chair": [
                    "Quan Li"
                ],
                "organizers": [],
                "time_start": "2023-10-22T00:00:00Z",
                "time_end": "2023-10-24T23:15:00Z",
                "discord_category": "",
                "discord_channel": "",
                "discord_channel_id": "",
                "discord_link": "",
                "zoom_meeting": "",
                "zoom_password": "",
                "zoom_link": "",
                "time_slots": [
                    {
                        "slot_id": "v-full-1515",
                        "session_id": "full11",
                        "title": "Guided Visual Analytics for Image Selection in Time and Space",
                        "contributors": [
                            "Ignacio P\u00e9rez-Messina"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "v-full-1515",
                        "time_stamp": "2023-10-24T22:00:00Z",
                        "time_start": "2023-10-24T22:00:00Z",
                        "time_end": "2023-10-24T22:12:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "1",
                        "paper_award": "",
                        "image_caption": "To enhance the UXO detection expert's decision-making process, our guided VA system for image selection in UXO detection visualizes image metadata in time while providing orientation (blue) and recommendations (red), employing an optimisation model that adapts to the user's selection (green).",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    },
                    {
                        "slot_id": "v-tvcg-9984953",
                        "session_id": "full11",
                        "title": "An Image-based Exploration and Query System for Large Visualization Collections via Neural Image Embedding",
                        "contributors": [
                            "Yilin Ye"
                        ],
                        "authors": [],
                        "abstract": "High-quality visualization collections are beneficial for a variety of applications including visualization reference and data-driven visualization design. The visualization community has created many visualization collections, and developed interactive exploration systems for the collections. However, the systems are mainly based on extrinsic attributes like authors and publication years, whilst neglect intrinsic property ( i.e ., visual appearance) of visualizations, hindering visual comparison and query of visualization designs. This paper presents VISAtlas , an image-based approach empowered by neural image embedding, to facilitate exploration and query for visualization collections. To improve embedding accuracy, we create a comprehensive collection of synthetic and real-world visualizations, and use it to train a convolutional neural network (CNN) model with a triplet loss for taxonomical classification of visualizations. Next, we design a coordinated multiple view (CMV) system that enables multi-perspective exploration and design retrieval based on visualization embeddings. Specifically, we design a novel embedding overview that leverages contextual layout framework to preserve the context of the embedding vectors with the associated visualization taxonomies, and density plot and sampling techniques to address the overdrawing problem. We demonstrate in three case studies and one user study the effectiveness of VISAtlas in supporting comparative analysis of visualization collections, exploration of composite visualizations, and image-based retrieval of visualization designs. The studies reveal that real-world visualization collections ( e.g ., Beagle and VIS30K) better accord with the richness and diversity of visualization designs than synthetic collections ( e.g ., Data2Vis), inspiring composite visualizations are identified in real-world collections, and distinct design patterns exist in visualizations from different sources.",
                        "uid": "v-tvcg-9984953",
                        "time_stamp": "2023-10-24T22:12:00Z",
                        "time_start": "2023-10-24T22:12:00Z",
                        "time_end": "2023-10-24T22:24:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "1",
                        "paper_award": "",
                        "image_caption": "Visualization image collections are valuable for learning visualization, training AI, and inspiring design.  Many existing exploration system of such collections rely on attribute metadata, limiting application to various collections with divergent metadata.  We propose image embedding method to allow for interpretable image-based exploration.   The method combines neural image embeddings based on classification model and contextual projection with density plot and point sampling.  A system is developed based on the method to assist visual comparative analysis and exploration of different collections.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    },
                    {
                        "slot_id": "v-tvcg-10081386",
                        "session_id": "full11",
                        "title": "Image Collage on Arbitrary Shape via Shape-Aware Slicing and Optimization",
                        "contributors": [
                            "Prof. Tong-Yee Lee"
                        ],
                        "authors": [],
                        "abstract": "Image collage is a very useful tool for visualizing an image collection. Most of the existing methods and commercial applications for generating image collages are designed on simple shapes, such as rectangular and circular layouts. This greatly limits the use of image collages in some artistic and creative settings. Although there are some methods that can generate irregularly-shaped image collages, they often suffer from severe image overlapping and excessive blank space. This prevents such methods from being effective information communication tools. In this paper, we present a shape slicing algorithm and an optimization scheme that can create image collages of arbitrary shapes in an informative and visually pleasing manner given an input shape and an image collection. To overcome the challenge of irregular shapes, we propose a novel algorithm, called Shape-Aware Slicing, which partitions the input shape into cells based on medial axis and binary slicing tree. Shape-Aware Slicing, which is designed specifically for irregular shapes, takes human perception and shape structure into account to generate visually pleasing partitions. Then, the layout is optimized by analyzing input images with the goal of maximizing the total salient regions of the images. To evaluate our method, we conduct extensive experiments and compare our results against previous work. The evaluations show that our proposed algorithm can efficiently arrange image collections on irregular shapes and create visually superior results than prior work and existing commercial tools.",
                        "uid": "v-tvcg-10081386",
                        "time_stamp": "2023-10-24T22:24:00Z",
                        "time_start": "2023-10-24T22:24:00Z",
                        "time_end": "2023-10-24T22:36:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "1",
                        "paper_award": "",
                        "image_caption": "Image Collage on Arbitrary Shape via Shape-Aware Slicing and Optimization",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    },
                    {
                        "slot_id": "v-tvcg-9729541",
                        "session_id": "full11",
                        "title": "VisImages: A Fine-Grained Expert-Annotated Visualization Dataset",
                        "contributors": [
                            "Dazhen Deng"
                        ],
                        "authors": [],
                        "abstract": "Images in visualization publications contain rich information, e.g., novel visualization designs and implicit design patterns of visualizations. A systematic collection of these images can contribute to the community in many aspects, such as literature analysis and automated tasks for visualization. In this paper, we build and make public a dataset, VisImages, which collects 12,267 images with captions from 1,397 papers in IEEE InfoVis and VAST. Built upon a comprehensive visualization taxonomy, the dataset includes 35,096 visualizations and their bounding boxes in the images. We demonstrate the usefulness of VisImages through three use cases: 1) investigating the use of visualizations in the publications with VisImages Explorer, 2) training and benchmarking models for visualization classification, and 3) localizing visualizations in the visual analytics systems automatically.",
                        "uid": "v-tvcg-9729541",
                        "time_stamp": "2023-10-24T22:36:00Z",
                        "time_start": "2023-10-24T22:36:00Z",
                        "time_end": "2023-10-24T22:48:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "1",
                        "paper_award": "",
                        "image_caption": "VisImages: A Fine-Grained Expert-Annotated Visualization Dataset",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    },
                    {
                        "slot_id": "v-full-1405",
                        "session_id": "full11",
                        "title": "A Unified Interactive Model Evaluation for Classification, Object Detection, and Instance Segmentation in Computer Vision",
                        "contributors": [
                            "Changjian Chen"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "v-full-1405",
                        "time_stamp": "2023-10-24T22:48:00Z",
                        "time_start": "2023-10-24T22:48:00Z",
                        "time_end": "2023-10-24T23:00:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "1",
                        "paper_award": "",
                        "image_caption": "Uni-Evaluator is an open-source visual analysis tool to support a unified interactive model evaluation for classification, object detection, and instance segmentation in computer vision. The tool consists of (a) the filtering panel; (b) the matrix-based visualization that provides an overview of model performance; (c) the table visualization that helps identify problematic data subsets; and (d) the grid visualization that displays the samples of interest. These modules work together to facilitate the model evaluation from a global overview to individual samples.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    },
                    {
                        "slot_id": "v-full-1408",
                        "session_id": "full11",
                        "title": "VideoPro: A Visual Analytics Approach for Interactive Video Programming",
                        "contributors": [
                            "Jianben He"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "v-full-1408",
                        "time_stamp": "2023-10-24T23:00:00Z",
                        "time_start": "2023-10-24T23:00:00Z",
                        "time_end": "2023-10-24T23:12:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "1",
                        "paper_award": "",
                        "image_caption": "The VideoPro interface consists of three major views. The Template View (A) offers descriptive statistics and rich interactions to facilitate multi-faceted exploration and comprehension of labeling templates. The Labeling View (B) provides a summary of the nuanced event compositions within the selected template to allow effective template validation and refinement. It also displays retrieved matching videos for efficient examination and at-scale programming. The Info View (C) presents comprehensive information regarding data embedding distribution in latent space and the model iteration process.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    }
                ]
            },
            {
                "title": "Immersive Analytics and Virtual Reality",
                "session_id": "full12",
                "event_prefix": "v-full",
                "track": "106(234)",
                "session_image": "full12.png",
                "chair": [
                    "Dieter Schmalstieg"
                ],
                "organizers": [],
                "time_start": "2023-10-22T00:00:00Z",
                "time_end": "2023-10-25T04:15:00Z",
                "discord_category": "",
                "discord_channel": "",
                "discord_channel_id": "",
                "discord_link": "",
                "zoom_meeting": "",
                "zoom_password": "",
                "zoom_link": "",
                "time_slots": [
                    {
                        "slot_id": "v-full-1103",
                        "session_id": "full12",
                        "title": "VIRD: Immersive Match Video Analysis for High-Performance Badminton Coaching",
                        "contributors": [
                            "Tica Lin"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "v-full-1103",
                        "time_stamp": "2023-10-25T03:00:00Z",
                        "time_start": "2023-10-25T03:00:00Z",
                        "time_end": "2023-10-25T03:12:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "1",
                        "paper_award": "",
                        "image_caption": "{\\rtf1\\ansi\\ansicpg1252\\cocoartf2709 \\cocoatextscaling0\\cocoaplatform0{\\fonttbl\\f0\\fswiss\\fcharset0 ArialMT;\\f1\\froman\\fcharset0 Times-Roman;} {\\colortbl;\\red255\\green255\\blue255;\\red0\\green0\\blue0;} {\\*\\expandedcolortbl;;\\cssrgb\\c0\\c0\\c0;} \\margl1440\\margr1440\\vieww24000\\viewh16320\\viewkind0 \\deftab720 \\pard\\pardeftab720\\partightenfactor0  \\f0\\fs29\\fsmilli14667 \\cf0 \\expnd0\\expndtw0\\kerning0 \\outl0\\strokewidth0 \\strokec2 VIRD, VR Bird, is a VR video analysis tool for badminton coaches and players to effectively analyze badminton match with multi-modal data. \\f1\\fs24 \\  \\f0\\fs29\\fsmilli14667 Users can experience the dynamic 3D poses and ball trajectories in VR, and analyze game data with situated visualizations.\\'a0 \\f1\\fs24 \\  \\f0\\fs29\\fsmilli14667 We contributed to a semi-automatic pipeline, where the coaches only need to provide a game video, and can later analyze the game in VR with all the supporting 3D views and data. \\f1\\fs24 \\ }",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    },
                    {
                        "slot_id": "v-tvcg-9834145",
                        "session_id": "full12",
                        "title": "Visual Cue Effects on a Classification Accuracy Estimation Task in Immersive Scatterplots",
                        "contributors": [
                            "Fumeng Yang"
                        ],
                        "authors": [],
                        "abstract": "Immersive visualization in virtual reality (VR) allows us to exploit visual cues for perception in 3D space, yet few existing studies have measured the effects of visual cues. Across a desktop monitor and a head-mounted display (HMD), we assessed scatterplot designs which vary their use of visual cues\u2014motion, shading, perspective (graphical projection), and dimensionality\u2014on two sets of data. We conducted a user study with a summary task in which 32 participants estimated the classification accuracy of an artificial neural network from the scatterplots. With Bayesian multilevel modeling, we capture the intricate visual effects and find that no cue alone explains all the variance in estimation error. Visual motion cues generally reduce participants\u2019 estimation error; besides this motion, using other cues may increase participants\u2019 estimation error. Using an HMD, adding visual motion cues, providing a third data dimension, or showing a more complicated dataset leads to longer response times. We speculate that most visual cues may not strongly affect perception in immersive analytics unless they change people\u2019s mental model about data. In summary, by studying participants as they interpret the output from a complicated machine learning model, we advance our understanding of how to use the visual cues in immersive analytics.",
                        "uid": "v-tvcg-9834145",
                        "time_stamp": "2023-10-25T03:12:00Z",
                        "time_start": "2023-10-25T03:12:00Z",
                        "time_end": "2023-10-25T03:24:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "1",
                        "paper_award": "",
                        "image_caption": "Immersive visualization in virtual reality (VR) allows us to exploit visual cues for perception in 3D space, yet few existing studies have measured the effects of visual cues. Across a desktop monitor (Left) and a head-mounted display (HMD; Right), we assessed scatterplot designs which vary their use of visual cues\u2014motion, shading, perspective (graphical projection), and dimensionality\u2014on two sets of data. We advance our understanding of how to use the visual cues in immersive analytics.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    },
                    {
                        "slot_id": "v-full-1161",
                        "session_id": "full12",
                        "title": "2D, 2.5D, or 3D? An Exploratory Study on Multilayer Network Visualisations in Virtual Reality",
                        "contributors": [
                            "Stefan Paul Feyer"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "v-full-1161",
                        "time_stamp": "2023-10-25T03:24:00Z",
                        "time_start": "2023-10-25T03:24:00Z",
                        "time_end": "2023-10-25T03:36:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "1",
                        "paper_award": "",
                        "image_caption": "The layers of a multilayer network (MLN) can be arranged in different ways in a visual representation, however, the impact of the arrangement on the readability of the network is an open question. Therefore, we studied this impact for several commonly occurring tasks related to MLN analysis. We ran a human subject study utilising a Virtual Reality headset to evaluate 2D, 2.5D, and 3D layer arrangements. We found no clear overall winner. However, we explore the task-to-arrangement space and derive empirical-based recommendations on the effective use of 2D, 2.5D, and 3D layer arrangements for MLNs.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    },
                    {
                        "slot_id": "v-full-1431",
                        "session_id": "full12",
                        "title": "MeTACAST: Target- and Context-aware Spatial Selection in VR",
                        "contributors": [
                            "Lingyun Yu"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "v-full-1431",
                        "time_stamp": "2023-10-25T03:36:00Z",
                        "time_start": "2023-10-25T03:36:00Z",
                        "time_end": "2023-10-25T03:48:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "1",
                        "paper_award": "",
                        "image_caption": "MeTACAST is a family of three target- and context-aware spatial selection techniques for 3D point cloud data in VR environments. Each technique is designed to be adjusted to particular selection intents: the selection of consecutive dense regions, the selection of filament-like structures, and the selection of clusters. These techniques allow users to precisely select those regions of space for further exploration---with simple and approximate 3D pointing, brushing, or drawing input---using flexible point- or path-based input and without being limited by 3D occlusions, non-homogeneous feature density, or complex data shapes.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    },
                    {
                        "slot_id": "v-full-1606",
                        "session_id": "full12",
                        "title": "Unraveling the Design Space of Immersive Analytics: A Systematic Review",
                        "contributors": [
                            "David Saffo"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "v-full-1606",
                        "time_stamp": "2023-10-25T03:48:00Z",
                        "time_start": "2023-10-25T03:48:00Z",
                        "time_end": "2023-10-25T04:00:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "1",
                        "paper_award": "",
                        "image_caption": "Five represenative images inspired by works surveyed in our systematic review. Each image is accompanied by several of the design aspects that sum up their unique final design.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    },
                    {
                        "slot_id": "v-full-1492",
                        "session_id": "full12",
                        "title": "Wizualization: A \"Hard Magic\" Visualization System for Immersive and Ubiquitous Analytics",
                        "contributors": [
                            "Andrea Batch"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "v-full-1492",
                        "time_stamp": "2023-10-25T04:00:00Z",
                        "time_start": "2023-10-25T04:00:00Z",
                        "time_end": "2023-10-25T04:12:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "1",
                        "paper_award": "",
                        "image_caption": "Data scientists at the CDC use the Wizualization system in augmented reality for faster, more intuitive data visualization.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    }
                ]
            },
            {
                "title": "Journalism & the Public",
                "session_id": "full13",
                "event_prefix": "v-full",
                "track": "109(234)",
                "session_image": "full13.png",
                "chair": [
                    "Narges Mahyar"
                ],
                "organizers": [],
                "time_start": "2023-10-22T00:00:00Z",
                "time_end": "2023-10-26T23:15:00Z",
                "discord_category": "",
                "discord_channel": "",
                "discord_channel_id": "",
                "discord_link": "",
                "zoom_meeting": "",
                "zoom_password": "",
                "zoom_link": "",
                "time_slots": [
                    {
                        "slot_id": "v-full-1144",
                        "session_id": "full13",
                        "title": "From shock to shift: Data visualization for constructive climate journalism",
                        "contributors": [
                            "Francesca Morini"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "v-full-1144",
                        "time_stamp": "2023-10-26T22:00:00Z",
                        "time_start": "2023-10-26T22:00:00Z",
                        "time_end": "2023-10-26T22:12:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "1",
                        "paper_award": "",
                        "image_caption": "Four postcards are laid on top of a newspaper page. The postcards show visualizations about climate protection.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    },
                    {
                        "slot_id": "v-tvcg-10130316",
                        "session_id": "full13",
                        "title": "Towards Visualization Thumbnail Designs that Entice Reading Data-driven Articles",
                        "contributors": [
                            "Sungahn Ko"
                        ],
                        "authors": [],
                        "abstract": "As online news increasingly include data journalism, there is a corresponding increase in the incorporation of visualization in article thumbnail images. However, little research exists on the design rationale for visualization thumbnails, such as resizing, cropping, simplifying, and embellishing charts that appear within the body of the associated article. Therefore, in this paper we aim to understand these design choices and determine what makes a visualization thumbnail inviting and interpretable. To this end, we first survey visualization thumbnails collected online and discuss visualization thumbnail practices with data journalists and news graphics designers. Based on the survey and discussion results, we then define a design space for visualization thumbnails and conduct a user study with four types of visualization thumbnails derived from the design space. The study results indicate that different chart components play different roles in attracting reader attention and enhancing reader understandability of the visualization thumbnails. We also find various thumbnail design strategies for effectively combining the charts' components, such as a data summary with highlights and data labels, and a visual legend with text labels and Human Recognizable Objects (HROs), into thumbnails. Ultimately, we distill our findings into design implications that allow effective visualization thumbnail designs for data-rich news articles. Our work can thus be seen as a first step toward providing structured guidance on how to design compelling thumbnails for data stories.",
                        "uid": "v-tvcg-10130316",
                        "time_stamp": "2023-10-26T22:12:00Z",
                        "time_start": "2023-10-26T22:12:00Z",
                        "time_end": "2023-10-26T22:24:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "1",
                        "paper_award": "",
                        "image_caption": "This study is about design choices of visualization thumbnail for news articles.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    },
                    {
                        "slot_id": "v-full-1383",
                        "session_id": "full13",
                        "title": "Embellishments Revisited: Perceptions of Embellished Visualisations Through the Viewer's Lens",
                        "contributors": [
                            "Muna Alebri"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "v-full-1383",
                        "time_stamp": "2023-10-26T22:24:00Z",
                        "time_start": "2023-10-26T22:24:00Z",
                        "time_end": "2023-10-26T22:36:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "1",
                        "paper_award": "",
                        "image_caption": "Revisiting Embellishments: bar charts with salient icons and arrows instead of bars. Small icon labels in charts that have a semantic meaning to the labels. Icon arrays and background images in charts.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    },
                    {
                        "slot_id": "v-full-1254",
                        "session_id": "full13",
                        "title": "Enthusiastic and Grounded, Avoidant and Cautious: Understanding Public Receptivity to Data and Visualizations",
                        "contributors": [
                            "Wesley Willett"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "v-full-1254",
                        "time_stamp": "2023-10-26T22:36:00Z",
                        "time_start": "2023-10-26T22:36:00Z",
                        "time_end": "2023-10-26T22:48:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "1",
                        "paper_award": "",
                        "image_caption": "A 2D Information Receptivity space, which characterizes interview participants based on their receptivity to open energy information when presented as data and as interpretation. The space shows four clusters: Data-Cautious (receptive to interpretation but not data), Data-Enthusiastic (receptive to interpretation and data), Domain-Grounded (receptive to data but not interpretation), and Information-Avoidant (not receptive to data or interpretation).",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    },
                    {
                        "slot_id": "v-full-1513",
                        "session_id": "full13",
                        "title": "Polarizing Political Polls: Visualization Design Choices Can Shape Public Opinion and Increase Political Polarization",
                        "contributors": [
                            "Eli Holder"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "v-full-1513",
                        "time_stamp": "2023-10-26T22:48:00Z",
                        "time_start": "2023-10-26T22:48:00Z",
                        "time_end": "2023-10-26T23:00:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "1",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    }
                ]
            },
            {
                "title": "Layout Algorithms (Full+Short)",
                "session_id": "full14",
                "event_prefix": "v-full",
                "track": "105(234)",
                "session_image": "full14.png",
                "chair": [
                    "Helen Purchase"
                ],
                "organizers": [],
                "time_start": "2023-10-22T00:00:00Z",
                "time_end": "2023-10-25T01:00:00Z",
                "discord_category": "",
                "discord_channel": "",
                "discord_channel_id": "",
                "discord_link": "",
                "zoom_meeting": "",
                "zoom_password": "",
                "zoom_link": "",
                "time_slots": [
                    {
                        "slot_id": "v-tvcg-10122175",
                        "session_id": "full14",
                        "title": "A Scalable Method for Readable Tree Layouts",
                        "contributors": [
                            "Reyan Ahmed"
                        ],
                        "authors": [],
                        "abstract": "Large tree structures are ubiquitous and real-world relational datasets often have information associated with nodes (e.g., labels or other attributes) and edges (e.g., weights or distances) that need to be communicated to the viewers. Yet, scalable, easy to read tree layouts are difficult to achieve. We consider tree layouts to be readable if they meet some basic requirements: node labels should not overlap, edges should not cross, edge lengths should be preserved, and the output should be compact. There are many algorithms for drawing trees, although very few take node labels or edge lengths into account, and none optimizes all requirements above. With this in mind, we propose a new scalable method for readable tree layouts. The algorithm guarantees that the layout has no edge crossings and no label overlaps, and optimizing one of the remaining aspects: desired edge lengths and compactness. We evaluate the performance of the new algorithm by comparison with related earlier approaches using several real-world datasets, ranging from a few thousand nodes to hundreds of thousands of nodes. Tree layout algorithms can be used to visualize large general graphs, by extracting a hierarchy of progressively larger trees. We illustrate this functionality by presenting several map-like visualizations generated by the new tree layout algorithm.",
                        "uid": "v-tvcg-10122175",
                        "time_stamp": "2023-10-24T23:45:00Z",
                        "time_start": "2023-10-24T23:45:00Z",
                        "time_end": "2023-10-24T23:57:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "1",
                        "paper_award": "",
                        "image_caption": "Overview of the readable tree (RT) method. The input is a node-labeled tree with pre-specified edge lengths from which a multi-level Steiner tree (MLST) is computed. (1) RT initializes with a crossing-free layout, with options targeting compactness or edge length preservation.  (2) A force-directed improvement removes label overlaps, preserves desired edge lengths, and minimizes the area. (3) Remaining label overlaps are removed through resizing and position fine tuning. Note that we have two options for prioritizing either compactness or edge length preservation, matching the corresponding initialization. The tree layout together with the MLST drive a Map-like visualizations with semantic zooming.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    },
                    {
                        "slot_id": "v-tvcg-10024360",
                        "session_id": "full14",
                        "title": "Force-directed graph layouts revisited: a new force based on the t-Distribution",
                        "contributors": [
                            "Yunhai Wang"
                        ],
                        "authors": [],
                        "abstract": "In this paper, we propose the t-FDP model, a force-directed placement method based on a novel bounded short-range force (t-force) defined by Student\u2019s t-distribution. Our formulation is flexible, exerts limited repulsive forces for nearby nodes and can be adapted separately in its short- and long-range effects. Using such forces in force-directed graph layouts yields better neighborhood preservation than current methods, while maintaining low stress errors. Our efficient implementation using a Fast Fourier Transform is one order of magnitude faster than state-of-the-art methods and two orders faster on the GPU, enabling us to perform parameter tuning by globally and locally adjusting the t-force in real-time even for complex graphs of up to 10.000 nodes. We demonstrate the quality of our approach by numerical evaluation against state-of-the-art approaches and extensions for interactive exploration.",
                        "uid": "v-tvcg-10024360",
                        "time_stamp": "2023-10-24T23:57:00Z",
                        "time_start": "2023-10-24T23:57:00Z",
                        "time_end": "2023-10-25T00:09:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "1",
                        "paper_award": "",
                        "image_caption": "In this paper, we propose the t-FDP model, a force-directed placement method based on a bounded short-range force (t-force) defined by Student's t-distribution. Our formulation exerts limited repulsive forces for nearby nodes and can be adapted separately in its short- and long-range effects. (A) Using such forces yields better neighborhood preservation than current methods. (B) Our efficient implementation using a Fast Fourier Transform is one order of magnitude faster than state-of-the-art methods and two orders faster on the GPU (C), enabling us to perform parameter tuning by globally and locally adjusting the t-force in real-time for complex graphs. (D)",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    },
                    {
                        "slot_id": "v-tvcg-9814874",
                        "session_id": "full14",
                        "title": "Target Netgrams: An Annulus-constrained Stress Model for Radial Graph Visualization",
                        "contributors": [
                            "Mingliang Xue"
                        ],
                        "authors": [],
                        "abstract": "We present Target Netgrams as a visualization technique for radial layouts of graphs. Inspired by manually created target sociograms, we propose an annulus-constrained stress model that aims to position nodes onto the annuli between adjacent circles for indicating their radial hierarchy, while maintaining the network structure (clusters and neighborhoods) and improving readability as much as possible. This is achieved by having more space on the annuli than traditional layout techniques. By adapting stress majorization to this model, the layout is computed as a constrained least square optimization problem. Additional constraints (e.g., parent-child preservation, attribute-based clusters and structure-aware radii) are provided for exploring nodes, edges, and levels of interest. We demonstrate the effectiveness of our method through a comprehensive evaluation, a user study, and a case study.",
                        "uid": "v-tvcg-9814874",
                        "time_stamp": "2023-10-25T00:09:00Z",
                        "time_start": "2023-10-25T00:09:00Z",
                        "time_end": "2023-10-25T00:21:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "1",
                        "paper_award": "",
                        "image_caption": "In this paper, we present Target Netgrams, a visualization technique for radial graph layouts. We propose an annulus-constrained stress model to position nodes on the annuli between adjacent circles, indicating their radial hierarchy while maintaining network structure and improving readability. (A) This is achieved by providing more space on the annuli compared to traditional layouts, computed through a constrained least square optimization problem using stress majorization. (B) Additional constraints facilitate exploration of nodes, edges, and levels of interest. (C) We demonstrate the effectiveness of our method through comprehensive evaluation, a user study, and a case study. (D)",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    },
                    {
                        "slot_id": "v-full-1377",
                        "session_id": "full14",
                        "title": "Cluster-Aware Grid Layout",
                        "contributors": [
                            "Weikai Yang"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "v-full-1377",
                        "time_stamp": "2023-10-25T00:21:00Z",
                        "time_start": "2023-10-25T00:21:00Z",
                        "time_end": "2023-10-25T00:33:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "1",
                        "paper_award": "",
                        "image_caption": "Grid visualizations are widely used in many applications to visually explain a set of data and their proximity relationships. However, existing layout methods face difficulties when dealing with the inherent cluster structures within the data. To address this issue, we propose a cluster-aware grid layout method that aims to better preserve cluster structures by simultaneously considering proximity, compactness, and convexity in the optimization process. Our method utilizes a hybrid optimization strategy that consists of two phases. The global phase aims to balance proximity and compactness within each cluster, while the local phase ensures the convexity of cluster shapes. We evaluate the proposed grid layout method through a series of quantitative experiments and two use cases, demonstrating its effectiveness in preserving cluster structures and facilitating analysis tasks.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    },
                    {
                        "slot_id": "v-full-1128",
                        "session_id": "full14",
                        "title": "Radial Icicle Tree (RIT): Node Separation and Area Constancy",
                        "contributors": [
                            "Yuanzhe Jin"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "v-full-1128",
                        "time_stamp": "2023-10-25T00:33:00Z",
                        "time_start": "2023-10-25T00:33:00Z",
                        "time_end": "2023-10-25T00:45:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "1",
                        "paper_award": "",
                        "image_caption": "The Radial Icicle Tree (RIT) elucidates the connections and distinctions between various categories within the dataset. Through the implementation of RIT, we visualized data from the CBS dataset, which was aggregated from wearable devices adorned by volunteers in a laboratory setting. RIT plots were utilized for eight categories of human physical activity, unveiling inter-category relationships. This underscores one of the key benefits of RIT - the ability to swiftly and lucidly reveal intrinsic data relationships when structural connections are present. It also facilitates simple comparisons between diverse plot categories concurrently, enabling users to uncover intriguing relations among them.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    },
                    {
                        "slot_id": "v-short-1051",
                        "session_id": "full14",
                        "title": "Projection Ensemble: Visualizing the Robust Structures of Multidimensional Projections",
                        "contributors": [
                            "Myeongwon Jung"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "v-short-1051",
                        "time_stamp": "2023-10-25T00:45:00Z",
                        "time_start": "2023-10-25T00:45:00Z",
                        "time_end": "2023-10-25T00:57:00Z",
                        "paper_type": "short",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "1",
                        "paper_award": "",
                        "image_caption": "Projection Ensemble recognizes robust structures in multidimensional projections. A) A randomly initialized t-SNE projection for the MNIST dataset. The viewer may interpret groups of points (a) and (b) as individual clusters, which, in fact, have intricate structures. B) Projection Ensemble visualizes two robust structures identified by extracting common subgraphs between ten randomly initialized projections. This reveals that (a) actually consists of two entangled structures (group (c) and the other points), and a subgroup of the cluster (b) is found to be closer to a distant cluster (see (d)). C) The ground-truth class labels are shown as the color of points.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    }
                ]
            },
            {
                "title": "LLMs and Generative Models",
                "session_id": "full15",
                "event_prefix": "v-full",
                "track": "106(234)",
                "session_image": "full15.png",
                "chair": [
                    "Vidya Setlyr"
                ],
                "organizers": [],
                "time_start": "2023-10-22T00:00:00Z",
                "time_end": "2023-10-25T01:00:00Z",
                "discord_category": "",
                "discord_channel": "",
                "discord_channel_id": "",
                "discord_link": "",
                "zoom_meeting": "",
                "zoom_password": "",
                "zoom_link": "",
                "time_slots": [
                    {
                        "slot_id": "v-full-1114",
                        "session_id": "full15",
                        "title": "AttentionViz: A Global View of Transformer Attention",
                        "contributors": [
                            "Catherine Yeh"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "v-full-1114",
                        "time_stamp": "2023-10-24T23:45:00Z",
                        "time_start": "2023-10-24T23:45:00Z",
                        "time_end": "2023-10-24T23:57:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "1",
                        "paper_award": "",
                        "image_caption": "AttentionViz, our interactive visualization tool, allows users to explore transformer self-attention at scale by creating a joint embedding space for queries and keys. (a) In language transformers, these visualizations reveal striking visual traces that can be linked to attention patterns. Each point represents the query (green) or key (pink) version of a word. Users can explore individual attention heads (left) or zoom out for a \u201cglobal\u201d view of attention (right). (b) Our visualizations also divulge interesting insights in vision transformers, such as attention heads that group image patches by hue and brightness. (c) Sample input sentences and (d) images.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    },
                    {
                        "slot_id": "v-tvcg-9964397",
                        "session_id": "full15",
                        "title": "PhraseMap: Attention-based Keyphrases Recommendation for Information Seeking",
                        "contributors": [
                            "Yamei Tu"
                        ],
                        "authors": [],
                        "abstract": "Many Information Retrieval (IR) approaches have been proposed to extract relevant information from a large corpus. Among these methods, phrase-based retrieval methods have been proven to capture more concrete and concise information than word-based and paragraph-based methods. However, due to the complex relationship among phrases and a lack of proper visual guidance, achieving user-driven interactive information-seeking and retrieval remains challenging. In this study, we present a visual analytic approach for users to seek information from an extensive collection of documents efficiently. The main component of our approach is a PhraseMap, where nodes and edges represent the extracted keyphrases and their relationships, respectively, from a large corpus. To build the PhraseMap, we extract keyphrases from each document and link the phrases according to word attention determined using modern language models, i.e., BERT. As can be imagined, the graph is complex due to the extensive volume of information and the massive amount of relationships. Therefore, we develop a navigation algorithm to facilitate information seeking. It includes (1) a question-answering (QA) model to identify phrases related to users\u2019 queries and (2) updating relevant phrases based on users\u2019 feedback. To better present the PhraseMap, we introduce a resource-controlled self-organizing map (RC-SOM) to evenly and regularly display phrases on grid cells while expecting phrases with similar semantics to stay close in the visualization. To evaluate our approach, we conducted case studies with three domain experts in diverse literature. The results and feedback demonstrate its effectiveness, usability, and intelligence.",
                        "uid": "v-tvcg-9964397",
                        "time_stamp": "2023-10-24T23:57:00Z",
                        "time_start": "2023-10-24T23:57:00Z",
                        "time_end": "2023-10-25T00:09:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "1",
                        "paper_award": "",
                        "image_caption": "In this study, we introduce a visual analytics approach for efficient information retrieval from extensive document collections. We transform large corpora into an attention-based phrase graph, known as PhraseMap, where nodes represent keyphrases and edges illustrate their relationships. Our method employs the PhraseMap as a backend knowledge base and offers multiple user interactions. To enhance the visualization of the PhraseMap, we employ a resource-controlled self-organizing map (RC-SOM) to evenly display phrases on a grid while ensuring semantically similar phrases are located close together (A). We also devise a navigation algorithm to support information retrieval, consisting of (1) a question-answering (QA) model that identifies phrases relevant to user queries (A') and (2) the option to accept/decline top-recommended phrases (B), followed by further exploration through documents (C).",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    },
                    {
                        "slot_id": "v-tvcg-10056593",
                        "session_id": "full15",
                        "title": "Visual Explanation for Open-domain Question Answering with BERT",
                        "contributors": [
                            "Zekai Shao"
                        ],
                        "authors": [],
                        "abstract": "Open-domain question answering (OpenQA) is an essential but challenging task in natural language processing that aims to answer questions in natural language formats on the basis of large-scale unstructured passages. Recent research has taken the performance of benchmark datasets to new heights, especially when these datasets are combined with techniques for machine reading comprehension based on Transformer models. However, as identified through our ongoing collaboration with domain experts and our review of literature, three key challenges limit their further improvement: (i) complex data with multiple long texts, (ii) complex model architecture with multiple modules, and (iii) semantically complex decision process. In this paper, we present VEQA, a visual analytics system that helps experts understand the decision reasons of OpenQA and provides insights into model improvement. The system summarizes the data flow within and between modules in the OpenQA model as the decision process takes place at the summary, instance and candidate levels. Specifically, it guides users through a summary visualization of dataset and module response to explore individual instances with a ranking visualization that incorporates context. Furthermore, VEQA supports fine-grained exploration of the decision flow within a single module through a comparative tree visualization. We demonstrate the effectiveness of VEQA in promoting interpretability and providing insights into model enhancement through a case study and expert evaluation.",
                        "uid": "v-tvcg-10056593",
                        "time_stamp": "2023-10-25T00:09:00Z",
                        "time_start": "2023-10-25T00:09:00Z",
                        "time_end": "2023-10-25T00:21:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "1",
                        "paper_award": "",
                        "image_caption": "Understanding the decision process of a neural model for OpenQA. The User Panel (A) displays the statistical information about the model and the dataset, as well as the color legends. The Summary View (B) provides a global summary of performance and module behavior for subsets. The Context View (C) presents questions from the selected subset and retrieved passage for a selected question. The Instance View (D) summarizes the keywords of each candidate passage in different modules with ranking visualization incorporating text to analyze the selected instance. The Tree View (E) explains the local data flow within a single module or multiple modules in the model with comparable Sankey-tree layout.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    },
                    {
                        "slot_id": "v-full-1197",
                        "session_id": "full15",
                        "title": "CommonsenseVIS: Visualizing and Understanding Commonsense Reasoning Capabilities of Natural Language Models",
                        "contributors": [
                            "Xingbo Wang"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "v-full-1197",
                        "time_stamp": "2023-10-25T00:21:00Z",
                        "time_start": "2023-10-25T00:21:00Z",
                        "time_end": "2023-10-25T00:33:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "1",
                        "paper_award": "",
                        "image_caption": "Since commonsense knowledge is not explicitly stated, it is challenging to conduct a scalable analysis of what commonsense knowledge NLP models do (not) learn. We employ a knowledge graph to derive implicit commonsense in the model input as context information. Then, we use it to align model behavior with human reasoning through multi-level interactive visualizations. Thereafter, users can understand, diagnose, and edit specific knowledge areas where models do not perform well.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    },
                    {
                        "slot_id": "v-full-1292",
                        "session_id": "full15",
                        "title": "Let the Chart Spark: Embedding Semantic Context into Chart with Generative Model",
                        "contributors": [
                            "Shishi Xiao"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "v-full-1292",
                        "time_stamp": "2023-10-25T00:33:00Z",
                        "time_start": "2023-10-25T00:33:00Z",
                        "time_end": "2023-10-25T00:45:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "1",
                        "paper_award": "",
                        "image_caption": "ChartSpark\uff1a An authoring tool encompassing generation and evaluation to create faithful pictorial visualization.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    },
                    {
                        "slot_id": "v-full-1066",
                        "session_id": "full15",
                        "title": "PromptMagician: Interactive Prompt Engineering for Text-to-Image Creation",
                        "contributors": [
                            "Yingchaojie Feng"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "v-full-1066",
                        "time_stamp": "2023-10-25T00:45:00Z",
                        "time_start": "2023-10-25T00:45:00Z",
                        "time_end": "2023-10-25T00:57:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "0",
                        "has_video": "1",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    }
                ]
            },
            {
                "title": "Machine Learning for Volume Visualization",
                "session_id": "full16",
                "event_prefix": "v-full",
                "track": "106(234)",
                "session_image": "full16.png",
                "chair": [
                    "Joshua A. Levine"
                ],
                "organizers": [],
                "time_start": "2023-10-22T00:00:00Z",
                "time_end": "2023-10-26T01:00:00Z",
                "discord_category": "",
                "discord_channel": "",
                "discord_channel_id": "",
                "discord_link": "",
                "zoom_meeting": "",
                "zoom_password": "",
                "zoom_link": "",
                "time_slots": [
                    {
                        "slot_id": "v-tvcg-9852325",
                        "session_id": "full16",
                        "title": "CoordNet: Data Generation and Visualization Generation for Time-Varying Volumes via a Coordinate-Based Neural Network",
                        "contributors": [
                            "Jun Han"
                        ],
                        "authors": [],
                        "abstract": "Although deep learning has demonstrated its capability in solving diverse scientific visualization problems, it still lacks generalization power across different tasks. To address this challenge, we propose CoordNet, a single coordinate-based framework that tackles various tasks relevant to time-varying volumetric data visualization without modifying the network architecture. The core idea of our approach is to decompose diverse task inputs and outputs into a unified representation (i.e., coordinates and values) and learn a function from coordinates to their corresponding values. We achieve this goal using a residual block-based implicit neural representation architecture with periodic activation functions. We evaluate CoordNet on data generation (i.e., temporal super-resolution and spatial super-resolution) and visualization generation (i.e., view synthesis and ambient occlusion prediction) tasks using time-varying volumetric data sets of various characteristics. The experimental results indicate that CoordNet achieves better quantitative and qualitative results than the state-of-the-art approaches across all the evaluated tasks.",
                        "uid": "v-tvcg-9852325",
                        "time_stamp": "2023-10-25T23:45:00Z",
                        "time_start": "2023-10-25T23:45:00Z",
                        "time_end": "2023-10-25T23:57:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "1",
                        "paper_award": "",
                        "image_caption": "We propose an implicit neural representation for processing diverse scientific data generation and visualization tasks without changing network architecture.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    },
                    {
                        "slot_id": "v-tvcg-9920542",
                        "session_id": "full16",
                        "title": "Deep Hierarchical Super Resolution for Scientific Data",
                        "contributors": [
                            "Skylar W. Wurster"
                        ],
                        "authors": [],
                        "abstract": "We present a novel technique for hierarchical super resolution (SR) with neural networks (NNs), which upscales volumetric data represented with an octree data structure to a high-resolution uniform grid with minimal seam artifacts on octree node boundaries. Our method uses existing state-of-the-art SR models and adds flexibility to upscale input data with varying levels of detail across the domain, instead of only uniform grid data that are supported in previous approaches. The key is to use a hierarchy of SR NNs, each trained to perform 2x SR between two levels of detail, with a hierarchical SR algorithm that minimizes seam artifacts by starting from the coarsest level of detail and working up. We show that our hierarchical approach outperforms baseline interpolation and hierarchical upscaling methods, and demonstrate the usefulness of our proposed approach across three use cases including data reduction using hierarchical downsampling+SR instead of uniform downsampling+SR, computation savings for hierarchical finite-time Lyapunov exponent field calculation, and super-resolving low-resolution simulation results for a high-resolution approximation visualization.",
                        "uid": "v-tvcg-9920542",
                        "time_stamp": "2023-10-25T23:57:00Z",
                        "time_start": "2023-10-25T23:57:00Z",
                        "time_end": "2023-10-26T00:09:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "1",
                        "paper_award": "",
                        "image_caption": "Deep Hierarchical Super Resolution combines the data efficiency of hierarchical data formats with the inference power of state-of-the-art super resolution models. We build a pyramid of super resolution networks and develop a hierarchical super resolution algorithm to upscale octree-like data.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    },
                    {
                        "slot_id": "v-tvcg-10175377",
                        "session_id": "full16",
                        "title": "Interactive Volume Visualization via Multi-Resolution Hash Encoding based Neural Representation",
                        "contributors": [
                            "Qi Wu"
                        ],
                        "authors": [],
                        "abstract": "Neural networks have shown great potential in compressing volume data for visualization. However, due to the high cost of training and inference, such volumetric neural representations have thus far only been applied to offline data processing and non-interactive rendering. In this paper, we demonstrate that by simultaneously leveraging modern GPU tensor cores, a native CUDA neural network framework, and a well-designed rendering algorithm with macro-cell acceleration, we can interactively ray trace volumetric neural representations (10-60fps). Our neural representations are also high-fidelity (PSNR > 30dB) and compact (10-1000x smaller). Additionally, we show that it is possible to fit the entire training step inside a rendering loop and skip the pre-training process completely. To support extreme-scale volume data, we also develop an efficient out-of-core training strategy, which allows our volumetric neural representation training to potentially scale up to terascale using only an NVIDIA RTX 3090 workstation.",
                        "uid": "v-tvcg-10175377",
                        "time_stamp": "2023-10-26T00:09:00Z",
                        "time_start": "2023-10-26T00:09:00Z",
                        "time_end": "2023-10-26T00:21:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "1",
                        "paper_award": "",
                        "image_caption": "This paper showcases the use of modern GPU tensor cores, a CUDA neural network framework, and an optimized rendering algorithm to interactively ray trace volumetric neural representations at 10-60fps. These neural representations are of high quality (PSNR > 30dB) and are significantly compact (10-1000x smaller). The study also reveals that the entire training phase can be integrated into a rendering loop, eliminating the need for pre-training. Moreover, this method can be scaled to terascale using just an NVIDIA RTX 3090 workstation.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    },
                    {
                        "slot_id": "v-full-1036",
                        "session_id": "full16",
                        "title": "Adaptively Placed Multi-Grid Scene Representation Networks for Large-Scale Data Visualization",
                        "contributors": [
                            "Skylar Wolfgang Wurster"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "v-full-1036",
                        "time_stamp": "2023-10-26T00:21:00Z",
                        "time_start": "2023-10-26T00:21:00Z",
                        "time_end": "2023-10-26T00:33:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "1",
                        "paper_award": "",
                        "image_caption": "APMGSRN is a novel scene representation network that introduces flexible feature grids that learn their position within the scene to maximize the use of the network parameters regardless of the features within the data.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    },
                    {
                        "slot_id": "v-full-1039",
                        "session_id": "full16",
                        "title": "Photon Field Networks for Dynamic Real-Time Volumetric Global Illumination",
                        "contributors": [
                            "David Bauer"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "v-full-1039",
                        "time_stamp": "2023-10-26T00:33:00Z",
                        "time_start": "2023-10-26T00:33:00Z",
                        "time_end": "2023-10-26T00:45:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "1",
                        "paper_award": "",
                        "image_caption": "Photon Field Networks are implicit neural represenations trained on photon caches traced in volume datasets. They can represent indirect radiance parameterized for sample position and view direction. Moreover, they can be trained on multiple caches simultaneously to learn non-isotropic scattering effects. In this paper, we introduce the concept of Photon Fields, show how to efficiently train them, and evaluate them in a proof-of-concept path tracing application. Our results show that Photon Fields can faithfully represent the photon caches and create approximate global illumination effects several times faster than a comparable path tracer.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    },
                    {
                        "slot_id": "v-full-1215",
                        "session_id": "full16",
                        "title": "PSRFlow: Probabilistic Super Resolution with Flow-Based Models for Scientific Data",
                        "contributors": [
                            "JINGYI SHEN"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "v-full-1215",
                        "time_stamp": "2023-10-26T00:45:00Z",
                        "time_start": "2023-10-26T00:45:00Z",
                        "time_end": "2023-10-26T00:57:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "1",
                        "paper_award": "",
                        "image_caption": "We propose PSRFlow, a novel deep learning based super-resolution algorithm with uncertainty quantification. Our work is based on normalizing flows to capture the intricate relationships between low and high-resolution data. The missing high-frequency details and the low-resolution information are modeled separately in the latent space of a conditional normalizing flow. The high-frequency latent follows a Gaussian distribution conditioned on the low-resolution information. During testing, given a low-resolution input, one can sample from the conditional Gaussian distribution and utilize the inverse of the normalizing flow to obtain high-resolution outputs. The generated high-resolution outputs are then used for uncertainty estimation.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    }
                ]
            },
            {
                "title": "Medical and Biomedical Applications",
                "session_id": "full17",
                "event_prefix": "v-full",
                "track": "105(234)",
                "session_image": "full17.png",
                "chair": [
                    "Kai Lawonn"
                ],
                "organizers": [],
                "time_start": "2023-10-22T00:00:00Z",
                "time_end": "2023-10-26T06:00:00Z",
                "discord_category": "",
                "discord_channel": "",
                "discord_channel_id": "",
                "discord_link": "",
                "zoom_meeting": "",
                "zoom_password": "",
                "zoom_link": "",
                "time_slots": [
                    {
                        "slot_id": "v-full-1290",
                        "session_id": "full17",
                        "title": "HealthPrism: A Visual Analytics System for Exploring Children's Physical and Mental Health Profiles with Multimodal Data",
                        "contributors": [
                            "Zhihan Jiang"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "v-full-1290",
                        "time_stamp": "2023-10-26T04:45:00Z",
                        "time_start": "2023-10-26T04:45:00Z",
                        "time_end": "2023-10-26T04:57:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "1",
                        "paper_award": "",
                        "image_caption": "The overview of HealthPrism. The Summary View (A) showcases overall context and motion features, including categorical context feature flows (A1), numerical context feature correlation (A2), motion features distribution (A4), and context and motion feature importance and influence (A3). The Group View (B) presents a network graph (B1) showing health profile clusters based on health indicators, genders, and age. It also presents the feature importance and influence (B2) and feature overview (B3) by groups. The Individual View (C) presents health profile (C1), motion and context feature (C3, C4), and feature importance and influence (C2) for up to two individuals for comparison.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    },
                    {
                        "slot_id": "v-full-1279",
                        "session_id": "full17",
                        "title": "Marjorie: Visualizing Type 1 Diabetes Data to Support Pattern Exploration",
                        "contributors": [
                            "Klaus Eckelt"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "v-full-1279",
                        "time_stamp": "2023-10-26T04:57:00Z",
                        "time_start": "2023-10-26T04:57:00Z",
                        "time_end": "2023-10-26T05:09:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "1",
                        "paper_award": "",
                        "image_caption": "Marjorie is a visual analytics solution for finding and analyzing patterns in type 1 diabetes data. Designed in consultation with diabetologists, Marjorie features a unique representation of glucose data based on modified horizon graphs and performs automated clustering of interesting glucose patterns.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    },
                    {
                        "slot_id": "v-full-1389",
                        "session_id": "full17",
                        "title": "Roses Have Thorns: Understanding the Downside of Oncological Care Delivery Through Visual Analytics and Sequential Rule Mining",
                        "contributors": [
                            "Carla Gabriela Floricel"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "v-full-1389",
                        "time_stamp": "2023-10-26T05:09:00Z",
                        "time_start": "2023-10-26T05:09:00Z",
                        "time_end": "2023-10-26T05:21:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "1",
                        "paper_award": "",
                        "image_caption": "We present an interactive visual analytics system that supports sequential rule mining for model builders who work in cancer symptom research. The system facilitates mechanistic knowledge discovery of treatment-related toxicities (i.e., \"roses have thorns\") in large scale cohort data.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    },
                    {
                        "slot_id": "v-tvcg-10005035",
                        "session_id": "full17",
                        "title": "MitoVis: A Unified Visual Analytics System for End-to-End Neuronal Mitochondria Analysis",
                        "contributors": [
                            "JunYoung Choi"
                        ],
                        "authors": [],
                        "abstract": "Neurons have a polarized structure, with dendrites and axons, and compartment-specific functions can be affected by the dwelling mitochondria. Recent studies have shown that the morphology of mitochondria is closely related to the functions of neurons and neurodegenerative diseases. However, the conventional mitochondria analysis workflow mainly relies on manual annotations and generic image-processing software. Moreover, even though there have been recent developments in automatic mitochondria analysis using deep learning, the application of existing methods in a daily analysis remains challenging because the performance of a pretrained deep learning model can vary depending on the target data, and there are always errors in inference time, requiring human proofreading. To address these issues, we introduce MitoVis, a novel visualization system for end-to-end data processing and an interactive analysis of the morphology of neuronal mitochondria. MitoVis introduces a novel active learning framework based on recent contrastive learning, which allows accurate fine-tuning of the neural network model. MitoVis also provides novel visual guides for interactive proofreading so that users can quickly identify and correct errors in the result with minimal effort. We demonstrate the usefulness and efficacy of the system via case studies conducted by neuroscientists. The results show that MitoVis achieved up to 13.3\u00d7 faster total analysis time in the case study compared to the conventional manual analysis workflow.",
                        "uid": "v-tvcg-10005035",
                        "time_stamp": "2023-10-26T05:21:00Z",
                        "time_start": "2023-10-26T05:21:00Z",
                        "time_end": "2023-10-26T05:33:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "1",
                        "paper_award": "",
                        "image_caption": "MitoVis, a unified visual analytics system for end-to-end neuronal mitochondria analysis.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    },
                    {
                        "slot_id": "v-tvcg-10143227",
                        "session_id": "full17",
                        "title": "PanVA: Pangenomic Variant Analysis",
                        "contributors": [
                            "Astrid van den Brandt"
                        ],
                        "authors": [],
                        "abstract": "Genomics researchers increasingly use multiple reference genomes to comprehensively explore genetic variants underlying differences in detectable characteristics between organisms. Pangenomes allow for an efficient data representation of multiple related genomes and their associated metadata. However, current visual analysis approaches for exploring these complex genotype-phenotype relationships are often based on single reference approaches or lack adequate support for interpreting the variants in the genomic context with heterogeneous (meta)data. This design study introduces PanVA, a visual analytics design for pangenomic variant analysis developed with the active participation of genomics researchers. The design uniquely combines tailored visual representations with interactions such as sorting, grouping, and aggregation, allowing users to navigate and explore different perspectives on complex genotype-phenotype relations. Through evaluation in the context of plants and pathogen research, we show that PanVA helps researchers explore variants in genes and generate hypotheses about their role in phenotypic variation.",
                        "uid": "v-tvcg-10143227",
                        "time_stamp": "2023-10-26T05:33:00Z",
                        "time_start": "2023-10-26T05:33:00Z",
                        "time_end": "2023-10-26T05:45:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "1",
                        "paper_award": "",
                        "image_caption": "Overview of PanVA, annotated to show the main components: the Control Panel (A) to select a gene, review peripheral information, and select groups; the Gene Overview (B) to slice an interesting region within the gene for further analysis; and the Locus View (C), the core view to analyze genetic variation in the sliced region (1), and explore association with metadata (2) and hierarchical relations (3).",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    },
                    {
                        "slot_id": "v-full-1346",
                        "session_id": "full17",
                        "title": "Leveraging Historical Medical Records as a Proxy via Multimodal Modeling and Visualization to Enrich Medical Diagnostic Learning",
                        "contributors": [
                            "Yang Ouyang"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "v-full-1346",
                        "time_stamp": "2023-10-26T05:45:00Z",
                        "time_start": "2023-10-26T05:45:00Z",
                        "time_end": "2023-10-26T05:57:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "1",
                        "paper_award": "",
                        "image_caption": "The system interface of DiagnosisAssistant contains (A) the User panel, (B) the Embedding Transition View, (C) the Modality Exploration View, and (D) the Comparison View.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    }
                ]
            },
            {
                "title": "ML for VIS",
                "session_id": "full18",
                "event_prefix": "v-full",
                "track": "109(234)",
                "session_image": "full18.png",
                "chair": [
                    "Nan Cao"
                ],
                "organizers": [],
                "time_start": "2023-10-22T00:00:00Z",
                "time_end": "2023-10-26T04:15:00Z",
                "discord_category": "",
                "discord_channel": "",
                "discord_channel_id": "",
                "discord_link": "",
                "zoom_meeting": "",
                "zoom_password": "",
                "zoom_link": "",
                "time_slots": [
                    {
                        "slot_id": "v-full-1194",
                        "session_id": "full18",
                        "title": "Data Type Agnostic Visual Sensitivity Analysis",
                        "contributors": [
                            "Nikolaus Piccolotto"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "v-full-1194",
                        "time_stamp": "2023-10-26T03:00:00Z",
                        "time_start": "2023-10-26T03:00:00Z",
                        "time_end": "2023-10-26T03:12:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "1",
                        "paper_award": "",
                        "image_caption": "The schematized illustration of our proposed visualization shows a dendrogram with clusters colored in a blue-red diverging scale. The red cluster points to sensitive parameter settings, the blue cluster to stable settings.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    },
                    {
                        "slot_id": "v-full-1223",
                        "session_id": "full18",
                        "title": "LiveRetro: Visual Analytics for Strategic Retrospect in Livestream E-Commerce",
                        "contributors": [
                            "Yuchen Wu"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "v-full-1223",
                        "time_stamp": "2023-10-26T03:12:00Z",
                        "time_start": "2023-10-26T03:12:00Z",
                        "time_end": "2023-10-26T03:24:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "1",
                        "paper_award": "",
                        "image_caption": "LiveRetro system, consisting of Session View, Segment View, Exploration View, and Record View.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    },
                    {
                        "slot_id": "v-tvcg-10149378",
                        "session_id": "full18",
                        "title": "Towards Better Modeling with Missing Data: A Contrastive Learning-based Visual Analytics Perspective",
                        "contributors": [
                            "Laixin Xie"
                        ],
                        "authors": [],
                        "abstract": "Missing data can pose a challenge for machine learning (ML) modeling. To address this, current approaches are categorized into feature imputation and label prediction and are primarily focused on handling missing data to enhance ML performance. These approaches rely on the observed data to estimate the missing values and therefore encounter three main shortcomings in imputation, including the need for different imputation methods for various missing data mechanisms, heavy dependence on the assumption of data distribution, and potential introduction of bias. This study proposes a Contrastive Learning (CL) framework to model observed data with missing values, where the ML model learns the similarity between an incomplete sample and its complete counterpart and the dissimilarity between other samples. Our proposed approach demonstrates the advantages of CL without requiring any imputation. To enhance interpretability, we introduce CIVis, a visual analytics system that incorporates interpretable techniques to visualize the learning process and diagnose the model status. Users can leverage their domain knowledge through interactive sampling to identify negative and positive pairs in CL. The output of CIVis is an optimized model that takes specified features and predicts downstream tasks. We provide two usage scenarios in regression and classification tasks and conduct quantitative experiments, expert interviews, and a qualitative user study to demonstrate the effectiveness of our approach. In short, this study offers a valuable contribution to addressing the challenges associated with ML modeling in the presence of missing data by providing a practical solution that achieves high predictive accuracy and model interpretability.",
                        "uid": "v-tvcg-10149378",
                        "time_stamp": "2023-10-26T03:24:00Z",
                        "time_start": "2023-10-26T03:24:00Z",
                        "time_end": "2023-10-26T03:36:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "1",
                        "paper_award": "",
                        "image_caption": "Interaction and observation in CIVis. The notation 1 - 3 demonstrate how to configure positive and negative sampling based on visual cues; notation 4 - 7 highlight and explain the benefits that CIVis brings. The loss curves in (4) look like an area because the loss sharply and frequently jumps up and down.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    },
                    {
                        "slot_id": "v-tvcg-10034833",
                        "session_id": "full18",
                        "title": "uxSense: Supporting User Experience Analysis with Visualization and Computer Vision",
                        "contributors": [
                            "Andrea Batch"
                        ],
                        "authors": [],
                        "abstract": "Analyzing user behavior from usability evaluation can be a challenging and time-consuming task, especially as the number of participants and the scale and complexity of the evaluation grows. We propose uxSense, a visual analytics system using machine learning methods to extract user behavior from audio and video recordings as parallel time-stamped data streams. Our implementation draws on pattern recognition, computer vision, natural language processing, and machine learning to extract user sentiment, actions, posture, spoken words, and other features from such recordings. These streams are visualized as parallel timelines in a web-based front-end, enabling the researcher to search, filter, and annotate data across time and space. We present the results of a user study involving professional UX researchers evaluating user data using uxSense. In fact, we used uxSense itself to evaluate their sessions.",
                        "uid": "v-tvcg-10034833",
                        "time_stamp": "2023-10-26T03:36:00Z",
                        "time_start": "2023-10-26T03:36:00Z",
                        "time_end": "2023-10-26T03:48:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "1",
                        "paper_award": "",
                        "image_caption": "An interface with a video playback component at the top left, a transcript in the top middle, and an annotations table at the top right. The bottom half of the screen is a collection of timeline visualizations.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    },
                    {
                        "slot_id": "v-full-1567",
                        "session_id": "full18",
                        "title": "Data Formulator: AI-powered Concept-driven Visualization Authoring",
                        "contributors": [
                            "Chenglong Wang"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "v-full-1567",
                        "time_stamp": "2023-10-26T03:48:00Z",
                        "time_start": "2023-10-26T03:48:00Z",
                        "time_end": "2023-10-26T04:00:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "1",
                        "paper_award": "",
                        "image_caption": "Data Formulator User Interface. After loading the input data, the authors interact with Data Formulator in four steps: (1) in the Concept Shelf, create new data concepts they plan to visualize (e.g., Seattle and Atlanta) or derive (e.g., Difference, Warmer), (2) encode data concepts to visual channels of a chart using Chart Builder and formulate the chart, (3) inspect the derived data automatically generated by Data Formulator, and (4) examine and save generated visualizations. Throughout the process, Data Formulator provides feedback to help authors understand generated data and visualizations.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    },
                    {
                        "slot_id": "v-full-1420",
                        "session_id": "full18",
                        "title": "InvVis: Large-Scale Data Embedding for Invertible Visualization",
                        "contributors": [
                            "Huayuan Ye"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "v-full-1420",
                        "time_stamp": "2023-10-26T04:00:00Z",
                        "time_start": "2023-10-26T04:00:00Z",
                        "time_end": "2023-10-26T04:12:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "1",
                        "paper_award": "",
                        "image_caption": "InvVis can embed a large amount of data into visualization images, users can decode the embedded data and perform rich exploration, such as redesigning (a) or reconstructing (b) visualizations, rebuilding a visualization dashboard based on the decoded source code and chart data (c), or visualizing volume data based on the decoded raw data and rendering parameters (d).",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    }
                ]
            },
            {
                "title": "Natural Language",
                "session_id": "full19",
                "event_prefix": "v-full",
                "track": "109(234)",
                "session_image": "full19.png",
                "chair": [
                    "Andreas Kerren"
                ],
                "organizers": [],
                "time_start": "2023-10-22T00:00:00Z",
                "time_end": "2023-10-25T23:15:00Z",
                "discord_category": "",
                "discord_channel": "",
                "discord_channel_id": "",
                "discord_link": "",
                "zoom_meeting": "",
                "zoom_password": "",
                "zoom_link": "",
                "time_slots": [
                    {
                        "slot_id": "v-full-1284",
                        "session_id": "full19",
                        "title": "TransforLearn: Interactive Visual Tutorial for the Transformer Model",
                        "contributors": [
                            "Lin Gao"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "v-full-1284",
                        "time_stamp": "2023-10-25T22:00:00Z",
                        "time_start": "2023-10-25T22:00:00Z",
                        "time_end": "2023-10-25T22:12:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "1",
                        "paper_award": "",
                        "image_caption": "With TransforLearn, learners can gain an understanding of the Transformer structure and the process of machine translation. Input view (A) provides an interface for the text to be translated. Translation view (B) displays the translation results and current translation progress, helping users in task-driven exploration. Architecture view (C) provides an overview of model structure and data flow, with sub-views (C1-C4) that support computational processes. Once enabled, the Detailed view (C3) displays the Attention mechanism view (D), Layer normalization view (E), and Feed-forward network view (F). These views not only show the operational details but also support multiple interactions.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    },
                    {
                        "slot_id": "v-tvcg-10153659",
                        "session_id": "full19",
                        "title": "Creating Emordle: Animating Word Cloud for Emotion Expression",
                        "contributors": [
                            "Liwenhan Xie"
                        ],
                        "authors": [],
                        "abstract": "We propose emordle, a conceptual design that animates wordles (compact word clouds) to deliver their emotional context to the audiences. To inform the design, we first reviewed online examples of animated texts and animated wordles, and summarized strategies for injecting emotion into the animations. We introduced a composite approach that extends an existing animation scheme for one word to multiple words in a wordle with two global factors: the randomness of text animation (entropy) and the animation speed (speed). To create an emordle, general users can choose one predefined animated scheme that matches the intended emotion class and fine-tune the emotion intensity with the two parameters. We designed proof-of-concept emordle examples for four basic emotion classes, namely happiness, sadness, anger, and fear. We conducted two controlled crowdsourcing studies to evaluate our approach. The first study confirmed that people generally agreed on the conveyed emotions from well-crafted animations, and the second one demonstrated that our identified factors helped fine-tune the delivered emotion extent. We also invited general users to create emordles on their own based on our proposed framework. Through this user study, we confirmed the effectiveness of the approach. We concluded with implications for future research opportunities of supporting emotion expression in visualizations.",
                        "uid": "v-tvcg-10153659",
                        "time_stamp": "2023-10-25T22:12:00Z",
                        "time_start": "2023-10-25T22:12:00Z",
                        "time_end": "2023-10-25T22:24:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "1",
                        "paper_award": "",
                        "image_caption": "This paper presents an approach to create animated word cloud that matches intended emotion based on a simple kinetic typography.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    },
                    {
                        "slot_id": "v-tvcg-9939115",
                        "session_id": "full19",
                        "title": "DocFlow: A Visual Analytics System for Question-based Document Retrieval and Categorization",
                        "contributors": [
                            "Rui Qiu"
                        ],
                        "authors": [],
                        "abstract": "A systematic review (SR) is essential with up-to-date research evidence to support clinical decisions and practices. However, the growing literature volume makes it challenging for SR reviewers and clinicians to discover useful information efficiently. Many human-in-the-loop information retrieval approaches (HIR) have been proposed to rank documents semantically similar to users' queries and provide interactive visualizations to facilitate document retrieval. Given that the queries are mainly composed of keywords and keyphrases retrieving documents that are semantically similar to a query does not necessarily respond to the clinician's need. Clinicians still have to review many documents to find the solution. The problem motivates us to develop a visual analytics system, DocFlow, to facilitate information-seeking. One of the features of our DocFlow is accepting natural language questions. The detailed description enables retrieving documents that can answer users' questions. Additionally, clinicians often categorize documents based on their backgrounds and with different purposes (e.g., populations, treatments). Since the criteria are unknown and cannot be pre-defined in advance, existing methods can only achieve categorization by considering the entire information in documents. In contrast, by locating answers in each document, our DocFlow can intelligently categorize documents based on users' questions. The second feature of our DocFlow is a flexible interface where users can arrange a sequence of questions to customize their rules for document retrieval and categorization. The two features of this visual analytics system support a flexible information-seeking process. The case studies and the feedback from domain experts demonstrate the usefulness and effectiveness of our DocFlow.",
                        "uid": "v-tvcg-9939115",
                        "time_stamp": "2023-10-25T22:24:00Z",
                        "time_start": "2023-10-25T22:24:00Z",
                        "time_end": "2023-10-25T22:36:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "1",
                        "paper_award": "",
                        "image_caption": "We introduce DocFlow, a component-based visual analytics system tailored for nuanced information seeking. Designed with modularity in mind, DocFlow empowers users to effortlessly assemble information-seeking pipelines by interlinking functional components, facilitating retrieval, categorization, and in-depth visual analysis tasks. To enhance the efficiency of retrieval, DocFlow incorporates a query-based retrieval model, ensuring documents with a high likelihood of answering the user's query are prioritized. Furthermore, with its integrated query-based categorization model, DocFlow provides users with the flexibility to dynamically group documents based on specific perspectives of interest.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    },
                    {
                        "slot_id": "v-tvcg-10015807",
                        "session_id": "full19",
                        "title": "ShortcutLens: A Visual Analytics Approach for Exploring Shortcuts in Natural Language Understanding Dataset",
                        "contributors": [
                            "Zhihua Jin"
                        ],
                        "authors": [],
                        "abstract": "Benchmark datasets play an important role in evaluating Natural Language Understanding (NLU) models. However, shortcuts\u2014unwanted biases in the benchmark datasets\u2014can damage the effectiveness of benchmark datasets in revealing models\u2019 real capabilities. Since shortcuts vary in coverage, productivity, and semantic meaning, it is challenging for NLU experts to systematically understand and avoid them when creating benchmark datasets. In this paper, we develop a visual analytics system, ShortcutLens, to help NLU experts explore shortcuts in NLU benchmark datasets. The system allows users to conduct multi-level exploration of shortcuts. Specifically, Statistics View helps users grasp the statistics such as coverage and productivity of shortcuts in the benchmark dataset. Template View employs hierarchical and interpretable templates to summarize different types of shortcuts. Instance View allows users to check the corresponding instances covered by the shortcuts. We conduct case studies and expert interviews to evaluate the effectiveness and usability of the system. The results demonstrate that ShortcutLens supports users in gaining a better understanding of benchmark dataset issues through shortcuts, inspiring them to create challenging and pertinent benchmark datasets.",
                        "uid": "v-tvcg-10015807",
                        "time_stamp": "2023-10-25T22:36:00Z",
                        "time_start": "2023-10-25T22:36:00Z",
                        "time_end": "2023-10-25T22:48:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "1",
                        "paper_award": "",
                        "image_caption": "ShortcutLens is a visual analytics tool that assists NLU experts in conducting the multi-level exploration of shortcuts in NLU benchmark datasets. ShortcutLens consists of three visualization components. The Statistics View (b) helps users inspect the statistics about the benchmark dataset and shortcuts. It also allows users to conduct what-if analysis on shortcuts of interest. The Template View (c) enables users to check the relationship of shortcuts and inspect the statistics about individual shortcuts.  The Instance View (d) displays the instances covered by selected shortcuts from the Template View. They enable users to gain a better understanding of benchmark dataset issues and inspire the creation of more challenging and pertinent benchmark datasets.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    },
                    {
                        "slot_id": "v-tvcg-10026499",
                        "session_id": "full19",
                        "title": "XNLI: Explaining and Diagnosing NLI-based Visual Data Analysis",
                        "contributors": [
                            "Xingbo Wang"
                        ],
                        "authors": [],
                        "abstract": "Natural language interfaces (NLIs) enable users to flexibly specify analytical intentions in data visualization. However, diagnosing the visualization results without understanding the underlying generation process is challenging. Our research explores how to provide explanations for NLIs to help users locate the problems and further revise the queries. We present XNLI, an explainable NLI system for visual data analysis. The system introduces a Provenance Generator to reveal the detailed process of visual transformations, a suite of interactive widgets to support error adjustments, and a Hint Generator to provide query revision hints based on the analysis of user queries and interactions. Two usage scenarios of XNLI and a user study verify the effectiveness and usability of the system. Results suggest that XNLI can significantly enhance task accuracy without interrupting the NLI-based analysis process.",
                        "uid": "v-tvcg-10026499",
                        "time_stamp": "2023-10-25T22:48:00Z",
                        "time_start": "2023-10-25T22:48:00Z",
                        "time_end": "2023-10-25T23:00:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "1",
                        "paper_award": "",
                        "image_caption": "The user interface of XNLI consists of two views, including the Data View and the Query View. Users can select or upload the dataset and explore the data attributes in the Data View. Then, they can use Query View to enter natural language queries, analyze data via charts, understand and diagnose the NLI process through interactive widgets, and gain hint feedback for query revision.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    },
                    {
                        "slot_id": "v-full-1510",
                        "session_id": "full19",
                        "title": "Large-Scale Evaluation of Topic Models and Dimensionality Reductions for 2D Text Spatialization",
                        "contributors": [
                            "Daniel Atzberger"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "v-full-1510",
                        "time_stamp": "2023-10-25T23:00:00Z",
                        "time_start": "2023-10-25T23:00:00Z",
                        "time_end": "2023-10-25T23:12:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "1",
                        "paper_award": "",
                        "image_caption": "Two-dimensional layout for the 20 Newsgroups dataset. Each point represents a document within the corpus and  the color its class. The layout originates from applying Latent Semantic Indexing to the term-document matrix and  a subsequent application of t-SNE on the topics, which are then aggregated to the document positions.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    }
                ]
            },
            {
                "title": "Perception",
                "session_id": "full20",
                "event_prefix": "v-full",
                "track": "105(234)",
                "session_image": "full20.png",
                "chair": [
                    "Cindy Xiong Bearfield"
                ],
                "organizers": [],
                "time_start": "2023-10-22T00:00:00Z",
                "time_end": "2023-10-26T04:15:00Z",
                "discord_category": "",
                "discord_channel": "",
                "discord_channel_id": "",
                "discord_link": "",
                "zoom_meeting": "",
                "zoom_password": "",
                "zoom_link": "",
                "time_slots": [
                    {
                        "slot_id": "v-tvcg-9978718",
                        "session_id": "full20",
                        "title": "The Risks of Ranking: Revisiting Graphical Perception to Model Individual Differences in Visualization Performance",
                        "contributors": [
                            "Yiren Ding"
                        ],
                        "authors": [],
                        "abstract": "Graphical perception studies typically measure visualization encoding effectiveness using the error of an \u201caverage observer\u201d, leading to canonical rankings of encodings for numerical attributes: e.g., position > area > angle > volume. Yet different people may vary in their ability to read different visualization types, leading to variance in this ranking across individuals not captured by population-level metrics using \u201caverage observer\u201d models. One way we can bridge this gap is by recasting classic visual perception tasks as tools for assessing individual performance, in addition to overall visualization performance. In this paper we replicate and extend Cleveland and McGill\u2019s graphical comparison experiment using Bayesian multilevel regression, using these models to explore individual differences in visualization skill from multiple perspectives. The results from experiments and modeling indicate that some people show patterns of accuracy that credibly deviate from the canonical rankings of visualization effectiveness. We discuss implications of these findings, such as a need for new ways to communicate visualization effectiveness to designers, how patterns in individuals\u2019 responses may show systematic biases and strategies in visualization judgment, and how recasting classic visual perception tasks as tools for assessing individual performance may offer new ways to quantify aspects of visualization literacy. Experiment data, source code, and analysis scripts are available at the following repository: https://osf.io/8ub7t/?view_only=9be4798797404a4397be3c6fc2a68cc0.",
                        "uid": "v-tvcg-9978718",
                        "time_stamp": "2023-10-26T03:00:00Z",
                        "time_start": "2023-10-26T03:00:00Z",
                        "time_end": "2023-10-26T03:12:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "1",
                        "paper_award": "",
                        "image_caption": "Graphical perception studies traditionally assess visualization encoding effectiveness using an \u201caverage observer,\u201d establishing classic rankings like position > area > angle > volume for numerical attributes. However, people\u2019s individual abilities to interpret different visualizations vary, challenging this one-size-fits-all approach.  To address this, this study replicates a classic graphical perception tasks and uses Bayesian modeling to evaluate individual differences in chart performance. The results reveal diverse visualization skills among individuals and suggests a need for better ways to communicate visualization effectiveness to designers.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    },
                    {
                        "slot_id": "v-full-1229",
                        "session_id": "full20",
                        "title": "Design Characterization for Black-and-White Textures in Visualization",
                        "contributors": [
                            "Tingying He"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "v-full-1229",
                        "time_stamp": "2023-10-26T03:12:00Z",
                        "time_start": "2023-10-26T03:12:00Z",
                        "time_end": "2023-10-26T03:24:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "1",
                        "paper_award": "",
                        "image_caption": "The bar chart, pie chart designs with geometric and iconic textures with the highest ratings in Experiment 2.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    },
                    {
                        "slot_id": "v-full-1250",
                        "session_id": "full20",
                        "title": "Image or Information? Examining the Nature and Impact of Visualization Perceptual Classification",
                        "contributors": [
                            "Anjana Arunkumar"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "v-full-1250",
                        "time_stamp": "2023-10-26T03:24:00Z",
                        "time_start": "2023-10-26T03:24:00Z",
                        "time_end": "2023-10-26T03:36:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "1",
                        "paper_award": "",
                        "image_caption": "Visualization design faces challenges in reconciling good empirical results and rules of thumb with good exemplars of real-world visualizations. To disentangle these confounding factors, we set out to answer a fundamental question: how do people actually internalize visualizations: as image or information? We present visualization to be rated on a bilinear scale as 'image' or 'information', and investigate the agreement between externalized ratings and internalized memory through free recall.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    },
                    {
                        "slot_id": "v-full-1655",
                        "session_id": "full20",
                        "title": "Perception of Line Attributes for Visualization",
                        "contributors": [
                            "Anna Sterzik"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "v-full-1655",
                        "time_stamp": "2023-10-26T03:36:00Z",
                        "time_start": "2023-10-26T03:36:00Z",
                        "time_end": "2023-10-26T03:48:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "1",
                        "paper_award": "",
                        "image_caption": "We investigated the perception of line attributes for visualization.  The line attributes in the figure were the most popular in our drawing study.  We investigated their discriminability in two further studies.  From top to bottom: two types of dashing, luminance, waves, and width.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    },
                    {
                        "slot_id": "v-full-1509",
                        "session_id": "full20",
                        "title": "Perceptually Uniform Construction of Illustrative Textures",
                        "contributors": [
                            "Anna Sterzik"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "v-full-1509",
                        "time_stamp": "2023-10-26T03:48:00Z",
                        "time_start": "2023-10-26T03:48:00Z",
                        "time_end": "2023-10-26T04:00:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "1",
                        "paper_award": "",
                        "image_caption": "We studied the perceptual spaces of illustrative textures (stippling, hatching, triangles) and extracted perceptually uniform texture levels by reparameterizing them.  The image displays five texture levels generated for each texture type using our reparameterization.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    },
                    {
                        "slot_id": "v-full-1016",
                        "session_id": "full20",
                        "title": "Too Many Cooks: Exploring How Graphical Perception Studies Influence Visualization Recommendations in Draco",
                        "contributors": [
                            "Zehua Zeng"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "v-full-1016",
                        "time_stamp": "2023-10-26T04:00:00Z",
                        "time_start": "2023-10-26T04:00:00Z",
                        "time_end": "2023-10-26T04:12:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "1",
                        "paper_award": "",
                        "image_caption": "In this paper, we present a pipeline of applying a large body of graphical perception results to develop new visualization recommendation algorithms and conduct an exploratory study to investigate how results from graphical perception can alter the behaviors and outputs of downstream algorithms. Given our findings, we discuss the potential for mutually reinforcing advancements in graphical perception and visualization recommendation research.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    }
                ]
            },
            {
                "title": "Scientific Visualization",
                "session_id": "full21",
                "event_prefix": "v-full",
                "track": "103(132)",
                "session_image": "full21.png",
                "chair": [
                    "Ingrid Hotz"
                ],
                "organizers": [],
                "time_start": "2023-10-22T00:00:00Z",
                "time_end": "2023-10-26T23:15:00Z",
                "discord_category": "",
                "discord_channel": "",
                "discord_channel_id": "",
                "discord_link": "",
                "zoom_meeting": "",
                "zoom_password": "",
                "zoom_link": "",
                "time_slots": [
                    {
                        "slot_id": "v-tvcg-9905473",
                        "session_id": "full21",
                        "title": "Electromechanical Coupling in Electroactive Polymers - a Visual Analysis of a Third-Order Tensor Field",
                        "contributors": [
                            "Chiara Hergl"
                        ],
                        "authors": [],
                        "abstract": "Electroactive polymers are frequently used in engineering applications due to their ability to change their shape and properties under the influence of an electric field. This process also works vice versa such that a mechanical deformation of the material induces an electric field in the EAP device. This specific behaviour makes such materials highly attractive for the construction of actuators and sensors in various application areas. The electromechanical behaviour of electroactive polymers can be described by a third-order coupling tensor which represents the sensitivity of mechanical stresses with respect to the electric field, i.e. it establishes a relation between a second-order and a first-order tensor field. Due to the complexity of this coupling tensor and to the lack of meaningful visualization methods for third-order tensors in general, an interpretation of the tensor is rather difficult. Thus, the central engineering research question that this contribution deals with, is a deeper understanding of the electromechanical coupling by analyzing the third-order coupling tensor with the help of specific visualization methods. Starting with a deviatoric decomposition of the tensor, the multipoles of each deviator are visualized, which allows a first insight into this highly complex third-order tensor. In the present contribution, four examples including electromechanical coupling are simulated within a finite element framework and subsequently analyzed by using the tensor visualization method.",
                        "uid": "v-tvcg-9905473",
                        "time_stamp": "2023-10-26T22:00:00Z",
                        "time_start": "2023-10-26T22:00:00Z",
                        "time_end": "2023-10-26T22:12:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "1",
                        "paper_award": "",
                        "image_caption": "Multipole Glyph visualization of a bioinspired tunable lens out of electroactive polymers. Electroactive polymers are materials that change their shape under the influence of an electric field. This material can be described by the third-order coupling tensor. Three-dimensional tensors of arbitrary order or symmetry can be decomposed using the deviatoric decomposition. Deviators can then be represented by a set of so-called multipoles and a scalar. These multipoles are, except for an even number of sign changes, unique vectors that are used to design a glyph in a split view.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    },
                    {
                        "slot_id": "v-tvcg-9992117",
                        "session_id": "full21",
                        "title": "GPU Accelerated 3D Tomographic Reconstruction and Visualization from Noisy Electron Microscopy Tilt-Series",
                        "contributors": [
                            "Julio A Rey Ramirez"
                        ],
                        "authors": [],
                        "abstract": "We present a novel framework for 3D tomographic reconstruction and visualization of tomograms from noisy electron microscopy tilt-series. Our technique takes as an input aligned tilt-series from cryogenic electron microscopy and creates denoised 3D tomograms using a proximal jointly-optimized approach that iteratively performs reconstruction and denoising, relieving the users of the need to select appropriate denoising algorithms in the pre-reconstruction or post-reconstruction steps. The whole process is accelerated by exploiting parallelism on modern GPUs, and the results can be visualized immediately after the reconstruction using volume rendering tools incorporated in the framework. We show that our technique can be used with multiple combinations of reconstruction algorithms and regularizers, thanks to the flexibility provided by proximal algorithms. Additionally, the reconstruction framework is open-source and can be easily extended with additional reconstruction and denoising methods. Furthermore, our approach enables visualization of reconstruction error throughout the iterative process within the reconstructed tomogram and on projection planes of the input tilt-series. We evaluate our approach in comparison with state-of-the-art approaches and additionally show how our error visualization can be used for reconstruction evaluation.",
                        "uid": "v-tvcg-9992117",
                        "time_stamp": "2023-10-26T22:12:00Z",
                        "time_start": "2023-10-26T22:12:00Z",
                        "time_end": "2023-10-26T22:24:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "1",
                        "paper_award": "",
                        "image_caption": "Left: 3D rendering of a volume obtained with standard cryo-ET reconstruction techniques. Right: 3D rendering of a reconstruction of the same tilt-series using our framework.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    },
                    {
                        "slot_id": "v-tvcg-10091196",
                        "session_id": "full21",
                        "title": "RadVolViz: An Information Display-Inspired Transfer Function Editor for Multivariate Volume Visualization",
                        "contributors": [
                            "Ayush Kumar"
                        ],
                        "authors": [],
                        "abstract": "In volume visualization transfer functions are widely used for mapping voxel properties to color and opacity. Typically, volume density data are scalars which require simple 1D transfer functions to achieve this mapping. If the volume densities are vectors of three channels, one can straightforwardly map each channel to either red, green or blue, which requires a trivial extension of the 1D transfer function editor. We devise a new method that applies to volume data with more than three channels. These types of data often arise in scientific scanning applications, where the data are separated into spectral bands or chemical elements. Our method expands on prior work in which a multivariate information display, RadViz, was fused with a radial color map, in order to visualize multi-band 2D images. In this work, we extend this joint interface to blended volume rendering. The information display allows users to recognize the presence and value distribution of the multivariate voxels and the joint volume rendering display visualizes their spatial distribution. We design a set of operators and lenses that allow users to interactively control the mapping of the multivariate voxels to opacity and color. This enables users to isolate or emphasize volumetric structures with desired multivariate properties. Furthermore, it turns out that our method also enables more insightful displays even for RGB data. We demonstrate our method with three datasets obtained from spectral electron microscopy, high energy X-ray scanning, and atmospheric science.",
                        "uid": "v-tvcg-10091196",
                        "time_stamp": "2023-10-26T22:24:00Z",
                        "time_start": "2023-10-26T22:24:00Z",
                        "time_end": "2023-10-26T22:36:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "1",
                        "paper_award": "",
                        "image_caption": "An Information Display-Inspired Transfer Function Editor for Multivariate Volume Visualization",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    },
                    {
                        "slot_id": "v-full-1209",
                        "session_id": "full21",
                        "title": "A General Framework for Progressive Data Compression and Retrieval",
                        "contributors": [
                            "Victor A. P. Magri"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "v-full-1209",
                        "time_stamp": "2023-10-26T22:36:00Z",
                        "time_start": "2023-10-26T22:36:00Z",
                        "time_end": "2023-10-26T22:48:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "1",
                        "paper_award": "",
                        "image_caption": "This work presents a framework that enables progressive-precision data queries for any data compressor. Our strategy hinges on a multi-component representation that successively reduces the error between the original and compressed fields, allowing each field in the progressive sequence to be expressed as a partial sum of components. We have implemented this approach with four established scientific data compressors and assessed its effectiveness using real-world data sets from the SDRBench collection. The results show that our framework competes in accuracy and performance with other methods that natively support progressive data compression.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    },
                    {
                        "slot_id": "v-full-1288",
                        "session_id": "full21",
                        "title": "Differentiable Design Galleries: A Differentiable Approach to Explore the Design Space of Transfer Functions",
                        "contributors": [
                            "Bo Pan"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "v-full-1288",
                        "time_stamp": "2023-10-26T22:48:00Z",
                        "time_start": "2023-10-26T22:48:00Z",
                        "time_end": "2023-10-26T23:00:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "1",
                        "paper_award": "",
                        "image_caption": "We propose Differentiable Design Galleries, a transfer function design approach based on deep learning and differentiable rendering to assist users in exploring the design space of transfer functions.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    },
                    {
                        "slot_id": "v-full-1202",
                        "session_id": "full21",
                        "title": "Residency Octree: A Hybrid Approach for Scalable Web-Based Multi-Volume Rendering",
                        "contributors": [
                            "Lukas Herzberger"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "v-full-1202",
                        "time_stamp": "2023-10-26T23:00:00Z",
                        "time_start": "2023-10-26T23:00:00Z",
                        "time_end": "2023-10-26T23:12:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "1",
                        "paper_award": "",
                        "image_caption": "In contrast to octree-based out-of-core approaches (left) which employ a one-to-one mapping between octree nodes and bricks, the residency octree nodes in our approach (right) represent geometric spatial regions, with each node mapping to multiple bricks and vice versa. This decoupling of resolution levels in data set from the spatial subdivision of the tree allows for more fine-grained empty space skipping than previous approaches and makes it possible to directly access any resolution from any node in the residency octree.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    }
                ]
            },
            {
                "title": "Situated Analytics and Augmented Reality (Full+Short)",
                "session_id": "full22",
                "event_prefix": "v-full",
                "track": "109(234)",
                "session_image": "full22.png",
                "chair": [
                    "Maxime Cordeil"
                ],
                "organizers": [],
                "time_start": "2023-10-22T00:00:00Z",
                "time_end": "2023-10-26T06:00:00Z",
                "discord_category": "",
                "discord_channel": "",
                "discord_channel_id": "",
                "discord_link": "",
                "zoom_meeting": "",
                "zoom_password": "",
                "zoom_link": "",
                "time_slots": [
                    {
                        "slot_id": "v-full-1795",
                        "session_id": "full22",
                        "title": "ARGUS: Visualization of AI-assisted Task Guidance in AR",
                        "contributors": [
                            "Claudio Silva"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "v-full-1795",
                        "time_stamp": "2023-10-26T04:45:00Z",
                        "time_start": "2023-10-26T04:45:00Z",
                        "time_end": "2023-10-26T04:57:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "1",
                        "paper_award": "",
                        "image_caption": "ARGUS is a visual analytics tool for real-time and historical evaluation of sensor and model outputs of AR task assistants. Our system allows for online visualization of object, action, and step detection as well as offline analysis of previously recorded AR sessions. It visualizes not only the multimodal sensor data streams but also the output of the ML models. This allows developers to gain insights into the performer activities as well as the ML models, helping them troubleshoot, improve, and fine-tune the components of the AR assistant.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    },
                    {
                        "slot_id": "v-tvcg-10149486",
                        "session_id": "full22",
                        "title": "The Reality of the Situation: A Survey of Situated Analytics",
                        "contributors": [
                            "Sungbok Shin"
                        ],
                        "authors": [],
                        "abstract": "The advent of low-cost, accessible, and high-performance augmented reality (AR) has shed light on a situated form of analytics where in-situ visualizations embedded in the real world can facilitate sensemaking based on the user\u2019s physical location. In this work, we identify prior literature in this emerging field with a focus on the technologies enabling such situated analytics. After collecting 47 relevant situated analytics systems, we classify them using a taxonomy of three dimensions: situating triggers, view situatedness, and data depiction. We then identify four archetypical patterns in our classification using an ensemble cluster analysis. Finally, we discuss several insights and design guidelines that we learned from our analysis.",
                        "uid": "v-tvcg-10149486",
                        "time_stamp": "2023-10-26T04:57:00Z",
                        "time_start": "2023-10-26T04:57:00Z",
                        "time_end": "2023-10-26T05:09:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "1",
                        "paper_award": "",
                        "image_caption": "This figure shows the design space of the taxonomy to describe Situated Analytics Systems. Below we show the four most commonly-appearing patterns extracted from a list of 47 situated analytics systems.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    },
                    {
                        "slot_id": "v-full-1297",
                        "session_id": "full22",
                        "title": "Design Patterns for Situated Visualization in Augmented Reality",
                        "contributors": [
                            "Benjamin Lee"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "v-full-1297",
                        "time_stamp": "2023-10-26T05:09:00Z",
                        "time_start": "2023-10-26T05:09:00Z",
                        "time_end": "2023-10-26T05:21:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "1",
                        "paper_award": "",
                        "image_caption": "We present 10 design patterns for visualising data in the context of physical referents. Our design patterns include embedded views that encode data directly on the physical referent, such as glyphs, trajectories, and decals. They also include situated views such as panels and proxies. We describe common uses of each pattern, and characterise them through design dimensions and constraints.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    },
                    {
                        "slot_id": "v-full-1299",
                        "session_id": "full22",
                        "title": "Handling Non-Visible Referents in Situated Visualizations",
                        "contributors": [
                            "Ambre Assor"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "v-full-1299",
                        "time_stamp": "2023-10-26T05:21:00Z",
                        "time_start": "2023-10-26T05:21:00Z",
                        "time_end": "2023-10-26T05:33:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "1",
                        "paper_award": "",
                        "image_caption": "The user, a firefighter, has to save a victim. He uses a situated visualization: near the fire victim (the referent), stands a visualization of his vitals. However, a wall occludes the firefighter\u2019s view from the referent. To perform his task, he uses a XR system that handles the non-visibility of the physical referent. This system shows an overlay on the wall showing a representation of the victim as well as the visualization.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    },
                    {
                        "slot_id": "v-full-1532",
                        "session_id": "full22",
                        "title": "RL-LABEL: A Deep Reinforcement Learning Approach Intended for AR Label Placement in Dynamic Scenarios",
                        "contributors": [
                            "Zhutian Chen"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "v-full-1532",
                        "time_stamp": "2023-10-26T05:33:00Z",
                        "time_start": "2023-10-26T05:33:00Z",
                        "time_end": "2023-10-26T05:45:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "0",
                        "has_video": "0",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    },
                    {
                        "slot_id": "v-short-1191",
                        "session_id": "full22",
                        "title": "Quantifying the Impact of XR Visual Guidance on User Performance Using a Large-Scale Virtual Assembly Experiment",
                        "contributors": [
                            "Leon Pietschmann"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "v-short-1191",
                        "time_stamp": "2023-10-26T05:45:00Z",
                        "time_start": "2023-10-26T05:45:00Z",
                        "time_end": "2023-10-26T05:57:00Z",
                        "paper_type": "short",
                        "keywords": [],
                        "has_image": "0",
                        "has_video": "1",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    }
                ]
            },
            {
                "title": "Sports and Spatial Management",
                "session_id": "full23",
                "event_prefix": "v-full",
                "track": "106(234)",
                "session_image": "full23.png",
                "chair": [
                    "Charles Perin"
                ],
                "organizers": [],
                "time_start": "2023-10-22T00:00:00Z",
                "time_end": "2023-10-25T23:15:00Z",
                "discord_category": "",
                "discord_channel": "",
                "discord_channel_id": "",
                "discord_link": "",
                "zoom_meeting": "",
                "zoom_password": "",
                "zoom_link": "",
                "time_slots": [
                    {
                        "slot_id": "v-full-1329",
                        "session_id": "full23",
                        "title": "FSLens: A Visual Analytics Approach to Evaluating and Optimizing the Spatial Layout of Fire Stations",
                        "contributors": [
                            "Longfei Chen"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "v-full-1329",
                        "time_stamp": "2023-10-25T22:00:00Z",
                        "time_start": "2023-10-25T22:00:00Z",
                        "time_end": "2023-10-25T22:12:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "1",
                        "paper_award": "",
                        "image_caption": "System Overview of FSLens: (A) The Statistics Overview displays the statistical information of historical fires and fire stations. (B) The Fire Service S&D View serves as a tool for experts to comprehend the fluctuations in the supply and demand of firefighting resources over time. (C) The Spatiotemporal View employs a map-based exploration method to exhibit the spatial distribution of fire incidents and the spatial layout of fire stations. (D) The Optimization View offers a set of interactions to support the user in generating multiple optimization scenarios for consideration. (E) The Simulation and Comparison View aids in the assessment of the effects of the optimization solutions on the original layout and offers a comparative evaluation of the efficacy among solutions.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    },
                    {
                        "slot_id": "v-full-1344",
                        "session_id": "full23",
                        "title": "HoopInSight: Analyzing and Comparing Basketball Shooting Performance Through Visualization",
                        "contributors": [
                            "Yu Fu"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "v-full-1344",
                        "time_stamp": "2023-10-25T22:12:00Z",
                        "time_start": "2023-10-25T22:12:00Z",
                        "time_end": "2023-10-25T22:24:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "1",
                        "paper_award": "",
                        "image_caption": "The interface of HoopInSight. It consists of three large columns \u2014 two selection views and the comparison view in the middle. Each selection view has a shot chart and multiple supplementary views. The Comparison view is divided into two sub-views. The top subview shows where the frequency increases and the bottom subview shows where the frequency decreases.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    },
                    {
                        "slot_id": "v-full-1240",
                        "session_id": "full23",
                        "title": "SkiVis: Visual Exploration and Route Planning in Ski Resorts",
                        "contributors": [
                            "Julius Rauscher"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "v-full-1240",
                        "time_stamp": "2023-10-25T22:24:00Z",
                        "time_start": "2023-10-25T22:24:00Z",
                        "time_end": "2023-10-25T22:36:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "1",
                        "paper_award": "",
                        "image_caption": "SkiVis: Visual Exploration and Route Planning  in Ski Resorts",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    },
                    {
                        "slot_id": "v-tvcg-10076255",
                        "session_id": "full23",
                        "title": "Analysis of Wildfire Visualization Systems for Research and Training: Are They up for the Challenge of the Current State of Wildfires?",
                        "contributors": [
                            "Carlos Tirado Cortes"
                        ],
                        "authors": [],
                        "abstract": "Wildfires affect many regions across the world. The accelerated progression of global warming has amplified their frequency and scale, deepening their impact on human life, the economy, and the environment. The temperature rise has been driving wildfires to behave unpredictably compared to those previously observed, challenging researchers and fire management agencies to understand the factors behind this behavioral change. Furthermore, this change has rendered fire personnel training outdated and lost its ability to adequately prepare personnel to respond to these new fires. Immersive visualization can play a key role in tackling the growing issue of wildfires. Therefore, this survey reviews various studies that use immersive and non-immersive data visualization techniques to depict wildfire behavior and train first responders and planners. This paper identifies the most useful characteristics of these systems. While these studies support knowledge creation for certain situations, there is still scope to comprehensively improve immersive systems to address the unforeseen dynamics of wildfires.",
                        "uid": "v-tvcg-10076255",
                        "time_stamp": "2023-10-25T22:36:00Z",
                        "time_start": "2023-10-25T22:36:00Z",
                        "time_end": "2023-10-25T22:48:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "1",
                        "paper_award": "",
                        "image_caption": "Analysis of Wildfire Visualization Systems for Research and Training  Are They Up for the Challenge of the Current State of Wildfires?",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    },
                    {
                        "slot_id": "v-tvcg-9894103",
                        "session_id": "full23",
                        "title": "Team-Builder: Toward More Effective Lineup Selection in Soccer",
                        "contributors": [
                            "Anqi Cao"
                        ],
                        "authors": [],
                        "abstract": "Lineup selection is an essential and important task in soccer matches. To win a match, coaches must consider various factors and select appropriate players for a planned formation. Computation-based tools have been proposed to help coaches on this complex task, but they are usually based on over-simplified models on player performances, do not support interactive analysis, and overlook the inputs by coaches. In this paper, we propose a method for visual analytics of soccer lineup selection by tackling two challenges: characterizing essential factors involved in generating optimal lineup, and supporting coach-driven visual analytics of lineup selection. We develop a lineup selection model that integrates such important factors, such as spatial regions of player actions and defensive interactions with opponent players. A visualization system, Team-Builder, is developed to help coaches control the process of lineup generation, explanation, and comparison through multiple coordinated views. The usefulness and effectiveness of our system are demonstrated by two case studies on a real-world soccer event dataset.",
                        "uid": "v-tvcg-9894103",
                        "time_stamp": "2023-10-25T22:48:00Z",
                        "time_start": "2023-10-25T22:48:00Z",
                        "time_end": "2023-10-25T23:00:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "1",
                        "paper_award": "",
                        "image_caption": "System user interface. The interface consists of three views: a tactic view (A), a player view (B), and a lineup view (C). The tactic view provides confrontation tactic lists (A1, A2, A3, A4) to navigate tactics used in the lineup. The player view contains a lineup edit board (B1) for lineup generation, a candidate player list (B2) for player constraint identification, and an explanation component for comprehension of the reason of the selection of a player. The lineup view includes a candidate lineup list (C1) and lineup thumbnails (C2) for comparing multiple lineups.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    },
                    {
                        "slot_id": "v-full-1002",
                        "session_id": "full23",
                        "title": "Action-Evaluator: A Visualization Approach for Player Action Evaluation in Soccer",
                        "contributors": [
                            "Anqi Cao"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "v-full-1002",
                        "time_stamp": "2023-10-25T23:00:00Z",
                        "time_start": "2023-10-25T23:00:00Z",
                        "time_end": "2023-10-25T23:12:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "1",
                        "paper_award": "",
                        "image_caption": "System user interface. The interface contains three views: a player view (A), an action view (B), and an explanation view (C). The player view consists of a player ranking list (A1) to navigate players by importance and a player projection component (A2) to navigate players by similarity. The action view includes a match situation list (B1) to investigate action scores by match situations and an action score list (B2) to present those of different action choices. The adjustment view is composed of a record list (C1) and a ghost pitch (C2) to explain action scores to players.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    }
                ]
            },
            {
                "title": "Storytelling",
                "session_id": "full24",
                "event_prefix": "v-full",
                "track": "105(234)",
                "session_image": "full24.png",
                "chair": [
                    "Bongshin Lee"
                ],
                "organizers": [],
                "time_start": "2023-10-22T00:00:00Z",
                "time_end": "2023-10-24T23:15:00Z",
                "discord_category": "",
                "discord_channel": "",
                "discord_channel_id": "",
                "discord_link": "",
                "zoom_meeting": "",
                "zoom_password": "",
                "zoom_link": "",
                "time_slots": [
                    {
                        "slot_id": "v-tvcg-9998319",
                        "session_id": "full24",
                        "title": "From Invisible to Visible: Impacts of Metadata in Communicative Data Visualization",
                        "contributors": [
                            "Alyxander Burns"
                        ],
                        "authors": [],
                        "abstract": "Leaving the context of visualizations invisible can have negative impacts on understanding and transparency. While common wisdom suggests that recontextualizing visualizations with metadata (e.g., disclosing the data source or instructions for decoding the visualizations' encoding) may counter these effects, the impact remains largely unknown. To fill this gap, we conducted two experiments. In Experiment 1, we explored how chart type, topic, and user goal impacted which categories of metadata participants deemed most relevant. We presented 64 participants with four real-world visualizations. For each visualization, participants were given four goals and selected the type of metadata they most wanted from a set of 18 types. Our results indicated that participants were most interested in metadata which explained the visualization's encoding for goals related to understanding and metadata about the source of the data for assessing trustworthiness. In Experiment 2, we explored how these two types of metadata impact transparency, trustworthiness and persuasiveness, information relevance, and understanding. We asked 144 participants to explain the main message of two pairs of visualizations (one with metadata and one without); rate them on scales of transparency and relevance; and then predict the likelihood that they were selected for a presentation to policymakers. Our results suggested that visualizations with metadata were perceived as more thorough than those without metadata, but similarly relevant, accurate, clear, and complete. Additionally, we found that metadata did not impact the accuracy of the information extracted from visualizations, but may have influenced which information participants remembered as important or interesting.",
                        "uid": "v-tvcg-9998319",
                        "time_stamp": "2023-10-24T22:00:00Z",
                        "time_start": "2023-10-24T22:00:00Z",
                        "time_end": "2023-10-24T22:12:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "1",
                        "paper_award": "",
                        "image_caption": "In this paper, we explore what kinds of metadata people want and the impacts of having access to metadata on experience and understanding. Our results indicate that participants wanted different kinds of metadata depending on their reason for needing the data, but not the topic or chart. Further, We found that the presence of metadata did not affect the correctness of the statements made by participants, but may have re-directed their attention and impacted how transparent they thought the visualization was.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    },
                    {
                        "slot_id": "v-tvcg-9887905",
                        "session_id": "full24",
                        "title": "ScrollyVis: Interactive Visual Authoring of Guided Dynamic Narratives for Scientific Scrollytelling",
                        "contributors": [
                            "Eric M\u00f6rth"
                        ],
                        "authors": [],
                        "abstract": "Visual stories are an effective and powerful tool to convey specific information to a diverse public. Scrollytelling is a recent visual storytelling technique extensively used on the web, where content appears or changes as users scroll up or down a page. By employing the familiar gesture of scrolling as its primary interaction mechanism, it provides users with a sense of control, exploration and discoverability while still offering a simple and intuitive interface. In this paper, we present a novel approach for authoring, editing, and presenting data-driven scientific narratives using scrollytelling. Our method flexibly integrates common sources such as images, text, and video, but also supports more specialized visualization techniques such as interactive maps as well as scalar field and mesh data visualizations. We show that scrolling navigation can be used to traverse dynamic narratives and demonstrate how it can be combined with interactive parameter exploration. The resulting system consists of an extensible web-based authoring tool capable of exporting stand-alone stories that can be hosted on any web server. We demonstrate the power and utility of our approach with case studies from several diverse scientific fields and with a user study including 12 participants of diverse professional backgrounds. Furthermore, an expert in creating interactive articles assessed the usefulness of our approach and the quality of the created stories.",
                        "uid": "v-tvcg-9887905",
                        "time_stamp": "2023-10-24T22:12:00Z",
                        "time_start": "2023-10-24T22:12:00Z",
                        "time_end": "2023-10-24T22:24:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "1",
                        "paper_award": "",
                        "image_caption": "Visual stories are an effective and powerful tool to convey specific information to a diverse public. Scrollytelling is a recent visual storytelling technique extensively used on the web, where content appears or changes as users scroll up or down a page. By employing the familiar gesture of scrolling as its primary interaction mechanism, it provides users with a sense of control, exploration and discoverability while still offering a simple and intuitive interface. In this paper, we present a novel approach for authoring, editing, and presenting data-driven scientific narratives using scrollytelling. Our method flexibly integrates common sources such as images, text, and video, but also supports more specialized visualization techniques such as interactive maps as well as scalar field and mesh data visualizations. We show that scrolling navigation can be used to traverse dynamic narratives and demonstrate how it can be combined with interactive parameter exploration. The resulting system consists of an extensible web-based authoring tool capable of exporting stand-alone stories that can be hosted on any web server. We demonstrate the power and utility of our approach with case studies from several diverse scientific fields and with a user study including 12 participants of diverse professional backgrounds. Furthermore, an expert in creating interactive articles assessed the usefulness of our approach and the quality of the created stories.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    },
                    {
                        "slot_id": "v-full-1758",
                        "session_id": "full24",
                        "title": "Character-Oriented Design for Visual Data Storytelling",
                        "contributors": [
                            "Keshav Dasu"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "v-full-1758",
                        "time_stamp": "2023-10-24T22:24:00Z",
                        "time_start": "2023-10-24T22:24:00Z",
                        "time_end": "2023-10-24T22:36:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "1",
                        "paper_award": "",
                        "image_caption": "In other media, characters are often used as a bridge for the audience to cross into an unfamiliar and perhaps complex new worlds.  Through the lens of characters, the audience can gain an understanding of a world without prior knowledge.   We are inspired to investigate the possibility of applying characters to convey scientific insights in data stories.  A deeper understanding of data characters could address open data storytelling opportunities.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    },
                    {
                        "slot_id": "v-full-1049",
                        "session_id": "full24",
                        "title": "Data Player: Automatic Generation of Data Videos with Narration-Animation Interplay",
                        "contributors": [
                            "Leixian Shen"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "v-full-1049",
                        "time_stamp": "2023-10-24T22:36:00Z",
                        "time_start": "2023-10-24T22:36:00Z",
                        "time_end": "2023-10-24T22:48:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "1",
                        "paper_award": "",
                        "image_caption": "The pipeline of automatic generation of data videos with narration-animation interplay.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    },
                    {
                        "slot_id": "v-full-1158",
                        "session_id": "full24",
                        "title": "EmphasisChecker: A Tool for Guiding Chart and Caption Emphasis",
                        "contributors": [
                            "Dae Hyun Kim"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "v-full-1158",
                        "time_stamp": "2023-10-24T22:48:00Z",
                        "time_start": "2023-10-24T22:48:00Z",
                        "time_end": "2023-10-24T23:00:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "1",
                        "paper_award": "",
                        "image_caption": "The results of running EmphasisChecker at each stage of authoring a chart-caption pair about the real home price index between 1890 and 2006. (a) Prominent features are shown on top with a basic caption not describing any feature. (b) Caption text matches the most prominent visual feature (sharp rise at the end; blue). (c) Typo in the caption indicated by a red squiggly underline on \u2018declined since 1984\u2019. (d) Caption matching a less prominent feature, indicated by a blue squiggly underline on \u2018declined since 1894\u2019.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    },
                    {
                        "slot_id": "v-full-1311",
                        "session_id": "full24",
                        "title": "Socrates: Data Story Generation via Adaptive Machine-Guided Elicitation of User Feedback",
                        "contributors": [
                            "Guande Wu"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "v-full-1311",
                        "time_stamp": "2023-10-24T23:00:00Z",
                        "time_start": "2023-10-24T23:00:00Z",
                        "time_end": "2023-10-24T23:12:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "1",
                        "paper_award": "",
                        "image_caption": "Socrates: Data Story Generation via Adaptive Machine-Guided Elicitation of User Feedback. The figure includes the paper title, author information and an overview of the method. This paper presents a novel data story generation workflow called Socrates, that leverages adaptive machine-guided elicitation of user feedback to customize the story. The machine (Socrates) adaptively proposes questions to collect the user\u2019s feedback, which is incorporated into story generation.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    }
                ]
            },
            {
                "title": "Time Series Data",
                "session_id": "full25",
                "event_prefix": "v-full",
                "track": "104(132)",
                "session_image": "full25.png",
                "chair": [
                    "Silvia Miksch"
                ],
                "organizers": [],
                "time_start": "2023-10-22T00:00:00Z",
                "time_end": "2023-10-26T06:00:00Z",
                "discord_category": "",
                "discord_channel": "",
                "discord_channel_id": "",
                "discord_link": "",
                "zoom_meeting": "",
                "zoom_password": "",
                "zoom_link": "",
                "time_slots": [
                    {
                        "slot_id": "v-tvcg-9895311",
                        "session_id": "full25",
                        "title": "DOMINO: Visual Causal Reasoning with Time-Dependent Phenomena",
                        "contributors": [
                            "Klaus Mueller"
                        ],
                        "authors": [],
                        "abstract": "Current work on using visual analytics to determine causal relations among variables has mostly been based on the concept of counterfactuals. As such the derived static causal networks do not take into account the effect of time as an indicator. However, knowing the time delay of a causal relation can be crucial as it instructs how and when actions should be taken. Yet, similar to static causality, deriving causal relations from observational time-series data, as opposed to designed experiments, is not a straightforward process. It can greatly benefit from human insight to break ties and resolve errors.  We hence propose a set of visual analytics methods that allow humans to participate in the discovery of causal relations associated with windows of time delay. Specifically, we leverage a well-established method, logic-based causality, to enable analysts to test the significance of potential causes and measure their influences toward a certain effect. Furthermore, since an effect can be a cause of other effects, we allow users to  aggregate different temporal cause-effect relations found with our method into a visual flow diagram to enable the discovery of temporal causal networks. To demonstrate the effectiveness of our methods we constructed a prototype system named DOMINO and showcase it via a number of case studies using real-world datasets. Finally, we also used DOMINO to conduct several evaluations with human analysts from different science domains in order to gain feedback on the utility of our system in practical scenarios.",
                        "uid": "v-tvcg-9895311",
                        "time_stamp": "2023-10-26T04:45:00Z",
                        "time_start": "2023-10-26T04:45:00Z",
                        "time_end": "2023-10-26T04:57:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "1",
                        "paper_award": "",
                        "image_caption": "The visual interface of our DOMINO system. DOMINO allows humans to discover causal relations associated with windows of time delay. It consists of the conditional distribution view for manually exploring potential causes of a specified effect, the causal inference panel for the interactive analysis of causal relations under different time delays, the time sequence view for examining the synchronized time series, and the causal flow chart that aggregates the identified relations into a causal network.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    },
                    {
                        "slot_id": "v-full-1079",
                        "session_id": "full25",
                        "title": "Attribute-Aware RBFs: Interactive Visualization of Time Series Particle Volumes Using RT Core Range Queries",
                        "contributors": [
                            "Nate Morrical"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "v-full-1079",
                        "time_stamp": "2023-10-26T04:57:00Z",
                        "time_start": "2023-10-26T04:57:00Z",
                        "time_end": "2023-10-26T05:09:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "1",
                        "paper_award": "",
                        "image_caption": "The \"Cabana Dam Break\" data set, rendered interactively with our method at 46 FPS, 4-samples-per-pixel per-frame with volumetric shadows (left is 1 frame, right is 1024 averaged frames, bottom row are progressing time steps). GPU-accelerated tree construction and our blue noise approach enable interactive animation and improved perception over time.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    },
                    {
                        "slot_id": "v-full-1449",
                        "session_id": "full25",
                        "title": "Reclaiming the Horizon: Novel Visualization Designs for Time-Series Data with Large Value Ranges",
                        "contributors": [
                            "Daniel Braun"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "v-full-1449",
                        "time_stamp": "2023-10-26T05:09:00Z",
                        "time_start": "2023-10-26T05:09:00Z",
                        "time_end": "2023-10-26T05:21:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "1",
                        "paper_award": "",
                        "image_caption": "Our two new visualization designs for large value ranges in time-series data: The order of magnitude line chart (top) and the order of magnitude horizon graph (bottom).",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    },
                    {
                        "slot_id": "v-full-1082",
                        "session_id": "full25",
                        "title": "Supporting Guided Exploratory Visual Analysis on Time Series Data with Reinforcement Learning",
                        "contributors": [
                            "Yang Shi"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "v-full-1082",
                        "time_stamp": "2023-10-26T05:21:00Z",
                        "time_start": "2023-10-26T05:21:00Z",
                        "time_end": "2023-10-26T05:33:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "1",
                        "paper_award": "",
                        "image_caption": "The exploratory visual analysis (EVA) of time series data uses visualization as the main output medium and input interface for exploring new data. In this work, we present a reinforcement learning (RL)-based system, Visail, which generates EVA sequences to guide the exploration of time series data. As a user uploads a time series dataset, Visail can generate step-by-step EVA suggestions, while each step is visualized as an annotated chart combined with textual descriptions. interface of Visail consists of four components, including (1) Timeline view, (2) Sequence view, (3) Insight panel, (4) Insight tooltip, and (5) Suggestion Panel.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    },
                    {
                        "slot_id": "v-full-1321",
                        "session_id": "full25",
                        "title": "TimeTuner: Diagnosing Time Representations for Time-Series Forecasting with Counterfactual Explanations",
                        "contributors": [
                            "Jianing Hao"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "v-full-1321",
                        "time_stamp": "2023-10-26T05:33:00Z",
                        "time_start": "2023-10-26T05:33:00Z",
                        "time_end": "2023-10-26T05:45:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "1",
                        "paper_award": "",
                        "image_caption": "TimeTuner, a novel visual analytics framework, combines counterfactual explanations with interactive visualizations to enhance user engagement in exploring the feature space, selecting appropriate transformation methods, and gaining intuitive insights. It offers juxtaposed bivariate stripes and partition-based correlation matrices, enabling users to navigate the transformation selection process and feature space interactively. TimeTuner is instantiated with smoothing and sampling transformations, and evaluated on real-world time-series forecasting of univariate sunspots and multivariate air pollutants. Feedback highlights its effectiveness in analyzing the impact of time-series representations and guiding data representation learning.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    },
                    {
                        "slot_id": "v-full-1179",
                        "session_id": "full25",
                        "title": "Visualizing Large-Scale Spatial Time Series with GeoChron",
                        "contributors": [
                            "Zikun Deng"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "v-full-1179",
                        "time_stamp": "2023-10-26T05:45:00Z",
                        "time_start": "2023-10-26T05:45:00Z",
                        "time_end": "2023-10-26T05:57:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "1",
                        "paper_award": "",
                        "image_caption": "A novel Storyline-based visualization is proposed to visualizing large-scale spatial time series. Each curve in the Storyline represents a spatial time series, and each bundle of curves represents an evolution pattern where the spatial time series are close in space and have correlated trends. The Storyline and geographic map is visually linked using colors.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    }
                ]
            },
            {
                "title": "Topology and Morse Theory",
                "session_id": "full26",
                "event_prefix": "v-full",
                "track": "106(234)",
                "session_image": "full26.png",
                "chair": [
                    "Bei Wang Phillips"
                ],
                "organizers": [],
                "time_start": "2023-10-22T00:00:00Z",
                "time_end": "2023-10-26T04:15:00Z",
                "discord_category": "",
                "discord_channel": "",
                "discord_channel_id": "",
                "discord_link": "",
                "zoom_meeting": "",
                "zoom_password": "",
                "zoom_link": "",
                "time_slots": [
                    {
                        "slot_id": "v-tvcg-10021892",
                        "session_id": "full26",
                        "title": "Discrete Morse Sandwich: Fast Computation of Persistence Diagrams for Scalar Data -- An Algorithm and A Benchmark",
                        "contributors": [
                            "Julien Tierny"
                        ],
                        "authors": [],
                        "abstract": "This paper introduces an efficient algorithm for persistence diagram computation, given an input piecewise linear scalar field f defined on a d-dimensional simplicial complex K, with $d \\leq 3$. Our work revisits the seminal algorithm \"PairSimplices\" [31], [103] with discrete Morse theory (DMT) [34], [80], which greatly reduces the number of input simplices to consider. Further, we also extend to DMT and accelerate the stratification strategy described in \"PairSimplices\" for the fast computation of the 0th and (d - 1)th diagrams, noted $D_0(f)$ and $D_{d-1}(f)$. Minima-saddle persistence pairs ($D_0(f)$) and saddle-maximum persistence pairs ($D_{d-1}(f)$) are efficiently computed by processing, with a Union-Find, the unstable sets of 1-saddles and the stable sets of (d - 1)-saddles. This fast pre-computation for the dimensions 0 and (d - 1) enables an aggressive specialization of [4] to the 3D case, which results in a drastic reduction of the number of input simplices for the computation of $D_1(f)$, the intermediate layer of the sandwich. Finally, we document several performance improvements via shared-memory parallelism. We provide an open-source implementation of our algorithm for reproducibility purposes. We also contribute a reproducible benchmark package, which exploits three-dimensional data from a public repository and compares our algorithm to a variety of publicly available implementations. Extensive experiments indicate that our algorithm improves by two orders of magnitude the time performance of the seminal \"PairSimplices\" algorithm it extends. Moreover, it also improves memory footprint and time performance over a selection of 14 competing approaches, with a substantial gain over the fastest available approaches, while producing a strictly identical output.",
                        "uid": "v-tvcg-10021892",
                        "time_stamp": "2023-10-26T03:00:00Z",
                        "time_start": "2023-10-26T03:00:00Z",
                        "time_end": "2023-10-26T03:12:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "1",
                        "paper_award": "",
                        "image_caption": "This work presents a fast algorithm for the computation of persistence diagrams. Our algorithm can be viewed as a modern interpretation of the standard persistence algorithm, from the perspective of Discrete Morse Theory.  We provide an open-source implementation as well as a benchmark package, which shows that our method leads to faster computations than competing approaches.  Our work enables the interactive inspection of circular patterns in scalar data.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    },
                    {
                        "slot_id": "v-tvcg-10081444",
                        "session_id": "full26",
                        "title": "Parallel Computation of Piecewise Linear Morse-Smale Segmentations",
                        "contributors": [
                            "Robin G. C. Maack"
                        ],
                        "authors": [],
                        "abstract": "This paper presents a well-scaling parallel algorithm for the computation of Morse-Smale (MS) segmentations, including the region separators and region boundaries. The segmentation of the domain into ascending and descending manifolds, solely defined on the vertices, improves the computational time using path compression and fully segments the border region. Region boundaries and region separators are generated using a multi-label marching tetrahedra algorithm. This enables a fast and simple solution to find optimal parameter settings in preliminary exploration steps by generating an MS complex preview. It also poses a rapid option to generate a fast visual representation of the region geometries for immediate utilization. Two experiments demonstrate the performance of our approach with speedups of over an order of magnitude in comparison to two publicly available implementations. The example section shows the similarity to the MS complex, the useability of the approach, and the benefits of this method with respect to the presented datasets. We provide our implementation with the paper.",
                        "uid": "v-tvcg-10081444",
                        "time_stamp": "2023-10-26T03:12:00Z",
                        "time_start": "2023-10-26T03:12:00Z",
                        "time_end": "2023-10-26T03:24:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "1",
                        "paper_award": "",
                        "image_caption": "The image shows the region boundaries of the Morse-Smale Segmentation computed on the Viscous Fingering dataset, simplified with an absolute persistence threshold of 0.1. The boundary interface of viscous fingers is shown as contours of the salt concentration density scalar field, colored by the density from yellow (high concentration) to purple (low concentration). The Morse-Smale segmentation region boundaries can extract the region-separating geometries that separate single viscous fingers without cluttering the visualization. Many Morse-Smale complex implementations would clutter the visualization with additional geometry from the saddle-saddle separatices.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    },
                    {
                        "slot_id": "v-tvcg-9920234",
                        "session_id": "full26",
                        "title": "Principal Geodesic Analysis of Merge Trees (and Persistence Diagrams)",
                        "contributors": [
                            "Mathieu Pont"
                        ],
                        "authors": [],
                        "abstract": "This paper presents a computational framework for the Principal Geodesic Analysis of merge trees (MT-PGA), a novel adaptation of the celebrated Principal Component Analysis (PCA) framework [87] to the Wasserstein metric space of merge trees [92]. We formulate MT-PGA computation as a constrained optimization problem, aiming at adjusting a basis of orthogonal geodesic axes, while minimizing a fitting energy. We introduce an efficient, iterative algorithm which exploits shared-memory parallelism, as well as an analytic expression of the fitting energy gradient, to ensure fast iterations. Our approach also trivially extends to extremum persistence diagrams. Extensive experiments on public ensembles demonstrate the efficiency of our approach - with MT-PGA computations in the orders of minutes for the largest examples. We show the utility of our contributions by extending to merge trees two typical PCA applications. First, we apply MT-PGA to data reduction and reliably compress merge trees by concisely representing them by their first coordinates in the MT-PGA basis. Second, we present a dimensionality reduction framework exploiting the first two directions of the MT-PGA basis to generate two-dimensional layouts of the ensemble. We augment these layouts with persistence correlation views, enabling global and local visual inspections of the feature variability in the ensemble. In both applications, quantitative experiments assess the relevance of our framework. Finally, we provide a C++ implementation that can be used to reproduce our results.",
                        "uid": "v-tvcg-9920234",
                        "time_stamp": "2023-10-26T03:24:00Z",
                        "time_start": "2023-10-26T03:24:00Z",
                        "time_end": "2023-10-26T03:36:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "1",
                        "paper_award": "",
                        "image_caption": "Merge trees are mathematical objects that summarize the features of interest in the data. This work presents a new method for the variability analysis of ensembles of merge trees (or persistence diagrams) by adapting the celebrated Principal Component Analysis framework to these specific objects. We show the utility of our approach with visualization applications such as data reduction to reliably compress the input merge trees and dimensionality reduction to generate two-dimensional layouts of the ensemble. We augment these layouts with persistence correlation view, enabling visual inspections of the feature variability. And with the reconstruction of user-defined locations for interactive exploration.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    },
                    {
                        "slot_id": "v-full-1554",
                        "session_id": "full26",
                        "title": "A Comparative Study of the Perceptual Sensitivity of Topological Visualizations to Feature Variations",
                        "contributors": [
                            "Dr. Tushar M. Athawale"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "v-full-1554",
                        "time_stamp": "2023-10-26T03:36:00Z",
                        "time_start": "2023-10-26T03:36:00Z",
                        "time_end": "2023-10-26T03:48:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "1",
                        "paper_award": "",
                        "image_caption": "Sensitivity analysis results for a color map [column (a)] and topological visualizations [columns (b)-(d)]. Bar charts depict the percentage of correctly answered trials for the positional data variation in the center row and amplitude data variation in the bottom row. Reeb graphs showed sensitivity to positional variation. Persistence diagrams and color maps showed sensitivity to amplitude variation. However, no single visualization type effectively conveyed both position and amplitude variation in the data.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    },
                    {
                        "slot_id": "v-full-1663",
                        "session_id": "full26",
                        "title": "ExTreeM: Scalable Augmented Merge Tree Computation via Extremum Graphs",
                        "contributors": [
                            "Jonas Lukasczyk"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "v-full-1663",
                        "time_stamp": "2023-10-26T03:48:00Z",
                        "time_start": "2023-10-26T03:48:00Z",
                        "time_end": "2023-10-26T04:00:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "1",
                        "paper_award": "",
                        "image_caption": "Merge trees are fundamental data abstractions of scalar field topology that record at which scalar values superlevel set components appear and merge. They can be used for a plethora of visualization and analysis task such as data segmentation (top).  We present ExTreeM, a generic schema using the ascending / descending manifold to generate a smaller extremum graph of the dataset (bottom, middle) and a specialized merge tree algorithm (bottom, right) showing speedups of up to one order of magnitude over the current state of the art.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    },
                    {
                        "slot_id": "v-full-1463",
                        "session_id": "full26",
                        "title": "Merge Tree Geodesics and Barycenters with Path Mappings",
                        "contributors": [
                            "Florian Wetzels"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "v-full-1463",
                        "time_stamp": "2023-10-26T04:00:00Z",
                        "time_start": "2023-10-26T04:00:00Z",
                        "time_end": "2023-10-26T04:12:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "1",
                        "paper_award": "",
                        "image_caption": "A comparison of the Wasserstein interpolation of merge trees with the novel path mapping interpolation, together with the corresponding mappings embedded in the scalar field. The path mapping distance clearly yields a more meaningful interpolated merge tree.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    }
                ]
            },
            {
                "title": "Topology Applications",
                "session_id": "full27",
                "event_prefix": "v-full",
                "track": "106(234)",
                "session_image": "full27.png",
                "chair": [
                    "Filip Sadlo"
                ],
                "organizers": [],
                "time_start": "2023-10-22T00:00:00Z",
                "time_end": "2023-10-26T06:00:00Z",
                "discord_category": "",
                "discord_channel": "",
                "discord_channel_id": "",
                "discord_link": "",
                "zoom_meeting": "",
                "zoom_password": "",
                "zoom_link": "",
                "time_slots": [
                    {
                        "slot_id": "v-full-1609",
                        "session_id": "full27",
                        "title": "TROPHY: A Topologically Robust Physics-Informed Tracking Framework for Tropical Cyclone",
                        "contributors": [
                            "Lin Yan"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "v-full-1609",
                        "time_stamp": "2023-10-26T04:45:00Z",
                        "time_start": "2023-10-26T04:45:00Z",
                        "time_end": "2023-10-26T04:57:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "1",
                        "paper_award": "",
                        "image_caption": "Tropical cyclones are among the most destructive weather systems.  This paper introduces a physics-informed tropical cyclone tracking framework, TROPHY, that utilizes tools from vector field topology. We demonstrate that TROPHY can capture tropical cyclones' characteristics that are comparable to and sometimes even better than a well-validated tropical cyclone tracking algorithm while requiring far less input data.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    },
                    {
                        "slot_id": "v-full-1295",
                        "session_id": "full27",
                        "title": "A Local Iterative Approach for the Extraction of 2D Manifolds from Strongly Curved and Folded Thin-Layer Structures",
                        "contributors": [
                            "Nicolas Klenert"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "v-full-1295",
                        "time_stamp": "2023-10-26T04:57:00Z",
                        "time_start": "2023-10-26T04:57:00Z",
                        "time_end": "2023-10-26T05:09:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "1",
                        "paper_award": "",
                        "image_caption": "Volume Rendering of a silver sheet package, blended together with surfaces created by the proposed algorithm. The unfolded textured surfaces are also shown. The colored border help associate the folded and unfolded surfaces with each other.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    },
                    {
                        "slot_id": "v-full-1222",
                        "session_id": "full27",
                        "title": "A Task-Parallel Approach for Localized Topological Data Structures",
                        "contributors": [
                            "Guoxi Liu"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "v-full-1222",
                        "time_stamp": "2023-10-26T05:09:00Z",
                        "time_start": "2023-10-26T05:09:00Z",
                        "time_end": "2023-10-26T05:21:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "1",
                        "paper_award": "",
                        "image_caption": "The image illustrates our proposed task-parallel approach, showcasing the pipeline with the dragon dataset. It highlights three distinct thread roles and outlines the precomputation methods implemented within the ACTOPO data structure.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    },
                    {
                        "slot_id": "v-full-1360",
                        "session_id": "full27",
                        "title": "Global Topology of 3D Symmetric Tensor Fields",
                        "contributors": [
                            "Professor Eugene Zhang"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "v-full-1360",
                        "time_stamp": "2023-10-26T05:21:00Z",
                        "time_start": "2023-10-26T05:21:00Z",
                        "time_end": "2023-10-26T05:33:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "1",
                        "paper_award": "",
                        "image_caption": "Existing topology-driven tensor field visualization focuses on the robust extraction of individual features such as degenerate curves and neutral surfaces (left: colored curves and surfaces).     In this paper, we introduce the notion of topological graphs for 3D symmetric tensor fields (right), whose nodes represent individual degenerate curves (colored circles) and volumes bounded by neutral surfaces (colored squares). The edges of the graph encode interactions among the nodes, such as linked degenerate curves, adjacent regions, and a degenerate curve and its container region.     The topological graph provides a holistic and global view of 3D tensor field topology.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    },
                    {
                        "slot_id": "v-full-1319",
                        "session_id": "full27",
                        "title": "Interactive Design and Optics-Based Visualization of Arbitrary Non-Euclidean Kaleidoscopic Orbifolds",
                        "contributors": [
                            "Professor Eugene Zhang"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "v-full-1319",
                        "time_stamp": "2023-10-26T05:33:00Z",
                        "time_start": "2023-10-26T05:33:00Z",
                        "time_end": "2023-10-26T05:45:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "1",
                        "paper_award": "",
                        "image_caption": "In this paper, we provide an algorithm to generate arbitrary two-dimensional non-Euclidean kaleidoscopic orbifolds. The example shown in this figure is a hyperbolic orbifold with six sides. The orders of symmetries at the six corners are respectively 2, 3, 4, 5, 6, and 7. Note that at a corner of order N, there are 2N sectors. Notice the reflections of the words \"Non-Euclidean Orbifold\" in the universal cover of the orbifold, the disk. The polygonal layout generated from our algorithm can be used as the ceiling and floor of a 3D room, thus enabling our mirror-based visualization for kaleidoscopic orbifolds.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    },
                    {
                        "slot_id": "v-full-1616",
                        "session_id": "full27",
                        "title": "TopoSZ: Preserving Topology in Error-Bounded Lossy Compression",
                        "contributors": [
                            "Lin Yan"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "v-full-1616",
                        "time_stamp": "2023-10-26T05:45:00Z",
                        "time_start": "2023-10-26T05:45:00Z",
                        "time_end": "2023-10-26T05:57:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "1",
                        "paper_award": "",
                        "image_caption": "This paper introduces TopoSZ, an error-bounded lossy compression method that preserves the topological features in 2D and 3D scalar fields. The main idea is to derive topological constraints from contour-tree-induced segmentation from the data domain and incorporate such constraints iteratively with a customized error-controlled quantization strategy. TopoSZ allows users to control the pointwise error and the loss of topological features during the compression process with a global error bound and a persistence threshold.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    }
                ]
            },
            {
                "title": "Trust and Bias",
                "session_id": "full28",
                "event_prefix": "v-full",
                "track": "109(234)",
                "session_image": "full28.png",
                "chair": [
                    "Evanthia Dimara"
                ],
                "organizers": [],
                "time_start": "2023-10-22T00:00:00Z",
                "time_end": "2023-10-25T01:00:00Z",
                "discord_category": "",
                "discord_channel": "",
                "discord_channel_id": "",
                "discord_link": "",
                "zoom_meeting": "",
                "zoom_password": "",
                "zoom_link": "",
                "time_slots": [
                    {
                        "slot_id": "v-tvcg-10002893",
                        "session_id": "full28",
                        "title": "Reasoning Affordances with Tables and Bar Charts",
                        "contributors": [
                            "Cindy Xiong"
                        ],
                        "authors": [],
                        "abstract": "A viewer\u2019s existing beliefs can prevent accurate reasoning with data visualizations. In particular, confirmation bias can cause people to overweigh information that confirms their beliefs, and dismiss information that disconfirms them. We tested whether confirmation bias exists when people reason with visualized data and whether certain visualization designs can elicit less biased reasoning strategies. We asked crowd workers to solve reasoning problems that had the potential to evoke both poor reasoning strategies and confirmation bias. We created two scenarios, one in which we primed people with a belief before asking them to make a decision, and another in which people held pre-existing beliefs. The data was presented as either a table, a bar table, or a bar chart. To correctly solve the problem, participants should use a complex reasoning strategy to compare two ratios, each between two pairs of values. But participants could also be tempted to use simpler, superficial heuristics, shortcuts, or biased strategies to reason about the problem. Presenting the data in a table format helped participants reason with the correct ratio strategy while showing the data as a bar table or a bar chart led participants towards incorrect heuristics. Confirmation bias was not significantly present when beliefs were primed, but it was present when beliefs were pre-existing. Additionally, the table presentation format was more likely to afford the ratio reasoning strategy, and the use of the ratio strategy was more likely to lead to the correct answer. These findings suggest that data presentation formats can affect affordances for reasoning.",
                        "uid": "v-tvcg-10002893",
                        "time_stamp": "2023-10-24T23:45:00Z",
                        "time_start": "2023-10-24T23:45:00Z",
                        "time_end": "2023-10-24T23:57:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "1",
                        "paper_award": "",
                        "image_caption": "We studied confirmation bias in visualizations, asking participants to solve a hard reasoning problem. We tested if using a bar chart, a bar table, or a table would lead to less biased reasoning strategies. Participants were often tempted to use simple heuristics, only comparing a few data points. With a table, participants were more likely to solve it correctly, using a complex reasoning strategy of comparing two ratios.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    },
                    {
                        "slot_id": "v-full-1531",
                        "session_id": "full28",
                        "title": "Average Estimates in Line Graphs are Biased Towards Areas of Higher Variability",
                        "contributors": [
                            "Dominik Moritz"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "v-full-1531",
                        "time_stamp": "2023-10-24T23:57:00Z",
                        "time_start": "2023-10-24T23:57:00Z",
                        "time_end": "2023-10-25T00:09:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "1",
                        "paper_award": "",
                        "image_caption": "Demonstration of the bias toward variability. The red line shows the mean estimated averages across all participants in one of two experiments. The line chart shows a bias of the estimated average toward higher variability in the higher y-values. The bias is smallest when the data is shown as points equally spaced along the x-axis and can be simulated using points sampled at equal intervals along the arc of the line.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    },
                    {
                        "slot_id": "v-full-1196",
                        "session_id": "full28",
                        "title": "Eleven Years of Gender Data Visualization: Towards more Inclusive Gender Representation",
                        "contributors": [
                            "Florent Cabric"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "v-full-1196",
                        "time_stamp": "2023-10-25T00:09:00Z",
                        "time_start": "2023-10-25T00:09:00Z",
                        "time_end": "2023-10-25T00:21:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "1",
                        "paper_award": "",
                        "image_caption": "The colors used by scientists to represent women and men. Links show associations between colors. For example, when women are represented in red, men are almost exclusively represented in blue.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    },
                    {
                        "slot_id": "v-full-1073",
                        "session_id": "full28",
                        "title": "My Model is Unfair, Do People Even Care? Visual Design Affects Trust and Perceived Bias in Machine Learning",
                        "contributors": [
                            "Aimen Gaba"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "v-full-1073",
                        "time_stamp": "2023-10-25T00:21:00Z",
                        "time_start": "2023-10-25T00:21:00Z",
                        "time_end": "2023-10-25T00:33:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "1",
                        "paper_award": "",
                        "image_caption": "We conducted a user study, asking participants to select between two automated investors, one historically producing fair returns for men and women (top left) and the other producing higher returns but exhibiting sexist behavior, favoring either men or women (top right).  We found that women valued fairness more than men, regardless of whether bias hurt men or women (bottom left), and that using text to describe the historical returns, as opposed to bar charts, resulted in more participants selecting fair investors (bottom right).",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    },
                    {
                        "slot_id": "v-full-1778",
                        "session_id": "full28",
                        "title": "The Rational Agent Benchmark for Data Visualization",
                        "contributors": [
                            "Yifan Wu"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "v-full-1778",
                        "time_stamp": "2023-10-25T00:33:00Z",
                        "time_start": "2023-10-25T00:33:00Z",
                        "time_end": "2023-10-25T00:45:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "1",
                        "paper_award": "",
                        "image_caption": "Understanding how helpful a visualization is from experimental results is difficult because the observed performance is confounded with aspects of the study design, such as how useful the information that is visualized is for the task. We develop a rational agent framework for designing and interpreting visualization experiments. Our framework conceives two experiments with the same setup: one with behavioral agents (human subjects), and the other one with a hypothetical rational agent. Our framework can be used to pre-experimentally and post-experimentally evaluate the experiment design.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    },
                    {
                        "slot_id": "v-full-1522",
                        "session_id": "full28",
                        "title": "Vistrust: a Multidimensional Framework and Empirical Study of Trust in Data Visualizations",
                        "contributors": [
                            "Carolina Nobre"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "v-full-1522",
                        "time_stamp": "2023-10-25T00:45:00Z",
                        "time_start": "2023-10-25T00:45:00Z",
                        "time_end": "2023-10-25T00:57:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "1",
                        "paper_award": "",
                        "image_caption": "An integrated framework, which outlines the development of trust in visualizations. The framework defines the different trust antecedents of the two basic components of trust (cognitive and affective trust). Both cognitive and affective trust can relate to the visualization and the underlying data. Individual characteristics can play a role in shaping one's level of trust in visualizations, and behavioral outcomes can emerge as a results of trust judgements.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    }
                ]
            },
            {
                "title": "VIS for Data Scientists",
                "session_id": "full29",
                "event_prefix": "v-full",
                "track": "109(234)",
                "session_image": "full29.png",
                "chair": [
                    "Kate Isaacs"
                ],
                "organizers": [],
                "time_start": "2023-10-22T00:00:00Z",
                "time_end": "2023-10-24T23:15:00Z",
                "discord_category": "",
                "discord_channel": "",
                "discord_channel_id": "",
                "discord_link": "",
                "zoom_meeting": "",
                "zoom_password": "",
                "zoom_link": "",
                "time_slots": [
                    {
                        "slot_id": "v-tvcg-10077087",
                        "session_id": "full29",
                        "title": "Are Metrics Enough? Guidelines for Communicating and Visualizing Predictive Models to Subject Matter Experts",
                        "contributors": [
                            "Ashley Suh"
                        ],
                        "authors": [],
                        "abstract": "Presenting a predictive model's performance is a communication bottleneck that threatens collaborations between data scientists and subject matter experts.  Accuracy and error metrics alone fail to tell the whole story of a model \u2013 its risks, strengths, and limitations \u2013 making it difficult for subject matter experts to feel confident in their decision to use a model.  As a result, models may fail in unexpected ways or go entirely unused, as subject matter experts disregard poorly presented models in favor of familiar, yet arguably substandard methods.  In this paper, we describe an iterative study conducted with both subject matter experts and data scientists to understand the gaps in communication between these two groups.  We find that, while the two groups share common goals of understanding the data and predictions of the model, friction can stem from unfamiliar terms, metrics, and visualizations \u2013 limiting the transfer of knowledge to SMEs and discouraging clarifying questions being asked during presentations.  Based on our findings, we derive a set of communication guidelines that use visualization as a common medium for communicating the strengths and weaknesses of a model.  We provide a demonstration of our guidelines in a regression modeling scenario and elicit feedback on their use from subject matter experts.  From our demonstration, subject matter experts were more comfortable discussing a model's performance, more aware of the trade-offs for the presented model, and better equipped to assess the model's risks \u2013 ultimately informing and contextualizing the model's use beyond text and numbers.",
                        "uid": "v-tvcg-10077087",
                        "time_stamp": "2023-10-24T22:00:00Z",
                        "time_start": "2023-10-24T22:00:00Z",
                        "time_end": "2023-10-24T22:12:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "1",
                        "paper_award": "",
                        "image_caption": "An image showing a set of model performance visualizations that can be used by data scientists to communicate predictive models to subject matter experts. Underneath, a ScatterText analysis depicting the differences and commonalities in words spoken during interviews with data scientists and subject matter experts. On the right, a table with communication and visualization guidelines with subject matter experts' feedback on each of them.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    },
                    {
                        "slot_id": "v-full-1559",
                        "session_id": "full29",
                        "title": "Dataopsy: Scalable and Fluid Visual Exploration using Aggregate Query Sculpting",
                        "contributors": [
                            "Md Naimul Hoque"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "v-full-1559",
                        "time_stamp": "2023-10-24T22:12:00Z",
                        "time_start": "2023-10-24T22:12:00Z",
                        "time_end": "2023-10-24T22:24:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "1",
                        "paper_award": "",
                        "image_caption": "A screenshot of the Dataopsy system, showing different subsets of a dataset.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    },
                    {
                        "slot_id": "v-full-1517",
                        "session_id": "full29",
                        "title": "Dead or Alive: Continuous Data Profiling for Interactive Data Science",
                        "contributors": [
                            "Will Epperson"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "v-full-1517",
                        "time_stamp": "2023-10-24T22:24:00Z",
                        "time_start": "2023-10-24T22:24:00Z",
                        "time_end": "2023-10-24T22:36:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "1",
                        "paper_award": "",
                        "image_caption": "We present our system AutoProfiler for continuous data profiling in Jupyter. AutoProfiler helps users understand their data and quality issues through automatic EDA information, live updates, and writing analysis code for users. Learn more at https://github.com/cmudig/AutoProfiler",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    },
                    {
                        "slot_id": "v-full-1787",
                        "session_id": "full29",
                        "title": "EVM: Incorporating Model Checking into Exploratory Visual Analysis",
                        "contributors": [
                            "Alex Kale"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "v-full-1787",
                        "time_stamp": "2023-10-24T22:36:00Z",
                        "time_start": "2023-10-24T22:36:00Z",
                        "time_end": "2023-10-24T22:48:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "1",
                        "paper_award": "",
                        "image_caption": "EVM (Exploratory Visual Modeling) integrates the ability to express and visually check regression models into a drag-and-drop visual analytics interface. In a typical visual analytics workflow, (A and B) analysts use visualizations to discover possible patterns of interest. With EVM's visual model checks, (C) analysts can rule out and compare of provisional data interpretations by scrutinizing the compatibility of observed data with model predictions.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    },
                    {
                        "slot_id": "v-full-1594",
                        "session_id": "full29",
                        "title": "VISPUR: Visual Aids for Identifying and Interpreting Spurious Associations in Data-Driven Decisions",
                        "contributors": [
                            "Xian Teng"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "v-full-1594",
                        "time_stamp": "2023-10-24T22:48:00Z",
                        "time_start": "2023-10-24T22:48:00Z",
                        "time_end": "2023-10-24T23:00:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "1",
                        "paper_award": "",
                        "image_caption": "This work provides a de-paradox workflow to help analyze observational data and overcome spurious and paradoxical associations. Spurious associations, including Simpson's paradox, are prevalent in observational studies. E.g., in a study that investigates the effect of a job training program, the cause (training program) and outcome (earnings) can be distorted by a third variable (ethnicity), leading to a misleading interpretation of the causal effect. We identify two major sources for spuriousness: (1) confounding bias and (2) subgroup heterogeneity, based on causal literature. We develop VISPUR, visualizing spurious associations, a visual analytic system to enable causal analysis of spurious associations. The system incorporates a suite of statistical techniques, algorithms, and visual components to help identify causal roots of spurious associations, as well as modules to reason about association paradox and to make informed decisions.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    },
                    {
                        "slot_id": "v-full-1508",
                        "session_id": "full29",
                        "title": "Visualization According to Statisticians: An Interview Study on the Role of Visualization for Inferential Statistics",
                        "contributors": [
                            "Eric Newburger"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "v-full-1508",
                        "time_stamp": "2023-10-24T23:00:00Z",
                        "time_start": "2023-10-24T23:00:00Z",
                        "time_end": "2023-10-24T23:12:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "1",
                        "paper_award": "",
                        "image_caption": "How do professional statisticians use data visualization?  How do they consider visualization within the suite of analytic methods at their disposal? Do they trust visualization methods?  We conducted semi-structured interviews with 18 statisticians from government, academia, and private industry, with more than 350 years collective professional experience, to find out.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    }
                ]
            },
            {
                "title": "VIS for ML",
                "session_id": "full30",
                "event_prefix": "v-full",
                "track": "109(234)",
                "session_image": "full30.png",
                "chair": [
                    "Shixia Liu"
                ],
                "organizers": [],
                "time_start": "2023-10-22T00:00:00Z",
                "time_end": "2023-10-25T06:00:00Z",
                "discord_category": "",
                "discord_channel": "",
                "discord_channel_id": "",
                "discord_link": "",
                "zoom_meeting": "",
                "zoom_password": "",
                "zoom_link": "",
                "time_slots": [
                    {
                        "slot_id": "v-full-1227",
                        "session_id": "full30",
                        "title": "A Comparative Visual Analytics Framework for Evaluating Evolutionary Processes in Multi-objective Optimization",
                        "contributors": [
                            "Yuxin Ma"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "v-full-1227",
                        "time_stamp": "2023-10-25T04:45:00Z",
                        "time_start": "2023-10-25T04:45:00Z",
                        "time_end": "2023-10-25T04:57:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "1",
                        "paper_award": "",
                        "image_caption": "We propose a visual analytics framework that enables the exploration and comparison of evolutionary processes in EMO algorithms. Our visual analytics framework comprises three primary modules, namely the Algorithm-level Comparison module (V1), Evolution-level Exploration module (V2-4), and Solution-level Inspection module (V5).",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    },
                    {
                        "slot_id": "v-tvcg-9937145",
                        "session_id": "full30",
                        "title": "The Transform-and-Perform framework: Explainable deep learning beyond classification",
                        "contributors": [
                            "Vidya Prasad"
                        ],
                        "authors": [],
                        "abstract": "In recent years, visual analytics (VA) has shown promise in alleviating the challenges of interpreting black-box deep learning (DL) models. While the focus of VA for explainable DL has been mainly on classification problems, DL is gaining popularity in high-dimensional-to-high-dimensional (H-H) problems such as image-to-image translation. In contrast to classification, H-H problems have no explicit instance groups or classes to study. Each output is continuous, high-dimensional, and changes in an unknown non-linear manner with changes in the input. These unknown relations between the input, model and output necessitate the user to analyze them in conjunction, leveraging symmetries between them. Since classification tasks do not exhibit some of these challenges, most existing VA systems and frameworks allow limited control of the components required to analyze models beyond classification. Hence, we identify the need for and present a unified conceptual framework, the Transform-and-Perform framework (T&P), to facilitate the design of VA systems for DL model analysis focusing on H-H problems. T&P provides a checklist to structure and identify workflows and analysis strategies to design new VA systems, and understand existing ones to uncover potential gaps for improvements. The goal is to aid the creation of effective VA systems that support the structuring of model understanding and identifying actionable insights for model improvements. We highlight the growing need for new frameworks like T&P with a real-world image-to image translation application. We illustrate how T&P effectively supports the understanding and identification of potential gaps in existing VA systems.",
                        "uid": "v-tvcg-9937145",
                        "time_stamp": "2023-10-25T04:57:00Z",
                        "time_start": "2023-10-25T04:57:00Z",
                        "time_end": "2023-10-25T05:09:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "1",
                        "paper_award": "",
                        "image_caption": "We introduce the Transform-and-Perform (T&P) framework, designed to assist visual analytics (VA) designers in creating VA systems with general applicability to high-dimensional-to-high-dimensional (H-H) problems. T&P helps identify workflows and analysis strategies for designing new VA systems. It also helps reveal potential gaps in existing systems. T&P enables analysis across the \"3Ws\" of model behavior: 1) when a behavior occurs (input analysis), 2) how & why it occurs (model analysis), and 3) what the behavior is (output analysis). By utilizing input-output symmetries, T&P offers a formal approach to understanding a model's inductive biases, crucial for analyzing a range of DL models.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    },
                    {
                        "slot_id": "v-tvcg-9937064",
                        "session_id": "full30",
                        "title": "Visual Exploration of Machine Learning Model Behavior with Hierarchical Surrogate Rule Sets",
                        "contributors": [
                            "Jun Yuan"
                        ],
                        "authors": [],
                        "abstract": "One of the potential solutions for model interpretation is to train a surrogate model: a more transparent model that approximates the behavior of the model to be explained. Typically, classification rules or decision trees are used due to their logic-based expressions. However, decision trees can grow too deep, and rule sets can become too large to approximate a complex model. Unlike paths on a decision tree that must share ancestor nodes (conditions), rules are more flexible. However, the unstructured visual representation of rules makes it hard to make inferences across rules. In this paper, we focus on tabular data and present novel algorithmic and interactive solutions to address these issues. First, we present Hierarchical Surrogate Rules (HSR), an algorithm that generates hierarchical rules based on user-defined parameters. We also contribute SURE, a visual analytics (VA) system that integrates HSR and an interactive surrogate rule visualization, the Feature-Aligned Tree, which depicts rules as trees while aligning features for easier comparison. We evaluate the algorithm in terms of parameter sensitivity, time performance, and comparison with surrogate decision trees and find that it scales reasonably well and overcomes the shortcomings of surrogate decision trees. We evaluate the visualization and the system through a usability study and an observational study with domain experts. Our investigation shows that the participants can use feature-aligned trees to perform non-trivial tasks with very high accuracy. We also discuss many interesting findings, including a rule analysis task characterization, that can be used for visualization design and future research.",
                        "uid": "v-tvcg-9937064",
                        "time_stamp": "2023-10-25T05:09:00Z",
                        "time_start": "2023-10-25T05:09:00Z",
                        "time_end": "2023-10-25T05:21:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "1",
                        "paper_award": "",
                        "image_caption": "SuRE is a visual analytics (VA) system that integrates hierarchical surrogate rule generation and an interactive surrogate rule visualization, the Feature-Aligned Tree, which depicts rules as trees while aligning features for easier comparison.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    },
                    {
                        "slot_id": "v-full-1753",
                        "session_id": "full30",
                        "title": "Are We Closing the Loop Yet? Gaps in the Generalizability of VIS4ML Research",
                        "contributors": [
                            "Hariharan Subramonyam"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "v-full-1753",
                        "time_stamp": "2023-10-25T05:21:00Z",
                        "time_start": "2023-10-25T05:21:00Z",
                        "time_end": "2023-10-25T05:33:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "1",
                        "paper_award": "",
                        "image_caption": "Two concentric circles divided into four segments with the four main finding groups including human expertise, hitl tasks, ml components, and vis4ml tools. The segments are distintly colored, and the outer circle segments are slightly desaturated. The outer circle represents the scope of VIS4ML in real world applications, and the inner circle the scope of VIS4ML research.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    },
                    {
                        "slot_id": "v-full-1694",
                        "session_id": "full30",
                        "title": "Explore Your Network in Minutes: A Rapid Prototyping Toolkit for Understanding Neural Networks with Visual Analytics",
                        "contributors": [
                            "Jun Tao"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "v-full-1694",
                        "time_stamp": "2023-10-25T05:33:00Z",
                        "time_start": "2023-10-25T05:33:00Z",
                        "time_end": "2023-10-25T05:45:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "1",
                        "paper_award": "",
                        "image_caption": "Under the guidance of the NNVisBuilder conceptual model, you can easily use the NNVisBuilder programming toolkit to build visual analytics interfaces for neural networks.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    },
                    {
                        "slot_id": "v-full-1550",
                        "session_id": "full30",
                        "title": "OW-Adapter: Human-Assisted Open-World Object Detection with a Few Examples",
                        "contributors": [
                            "Suphanut Jamonnak"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "v-full-1550",
                        "time_stamp": "2023-10-25T05:45:00Z",
                        "time_start": "2023-10-25T05:45:00Z",
                        "time_end": "2023-10-25T05:57:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "1",
                        "paper_award": "",
                        "image_caption": "Open-world object detection (OWOD) is an emerging computer vision problem that involves not only the identification of predefined object classes, like what general object detectors do, but also detects new unknown objects simultaneously. Recently, several end-to-end deep learning models have been proposed to address the OWOD problem. However, these approaches face several challenges: a) significant changes in both network architecture and training procedure are required; b) they are trained from scratch, which can not leverage existing pre-trained general detectors; c) costly annotations for all unknown classes are needed. To overcome these challenges, we present a visual analytic framework called OW-Adapter. It acts as an adaptor to enable pre-trained general object detectors to handle the OWOD problem. Specifically, OW-Adapter is designed to identify, summarize, and annotate unknown examples with minimal human effort. Moreover, we introduce a lightweight classifier to learn newly annotated unknown classes and plug the classifier into pre-trained general detectors to detect unknown objects. We demonstrate the effectiveness of our framework through two case studies of different domains, including common object recognition and autonomous driving. The studies show that a simple yet powerful adaptor can extend the capability of pre-trained general detectors to detect unknown objects and improve the performance on known classes simultaneously.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    }
                ]
            },
            {
                "title": "Visualization Design and User Experience",
                "session_id": "full31",
                "event_prefix": "v-full",
                "track": "105(234)",
                "session_image": "full31.png",
                "chair": [
                    "Melanie Tory"
                ],
                "organizers": [],
                "time_start": "2023-10-22T00:00:00Z",
                "time_end": "2023-10-26T01:00:00Z",
                "discord_category": "",
                "discord_channel": "",
                "discord_channel_id": "",
                "discord_link": "",
                "zoom_meeting": "",
                "zoom_password": "",
                "zoom_link": "",
                "time_slots": [
                    {
                        "slot_id": "v-tvcg-9855227",
                        "session_id": "full31",
                        "title": "VisRecall: Quantifying Information Visualisation Recallability via Question Answering",
                        "contributors": [
                            "Yao Wang"
                        ],
                        "authors": [],
                        "abstract": "Despite its importance for assessing the effectiveness of communicating information visually, fine-grained recallability of information visualisations has not been studied quantitatively so far. In this work, we propose a question-answering paradigm to study visualisation recallability and present VisRecall - a novel dataset consisting of 200 visualisations that are annotated with crowd-sourced human (N = 305) recallability scores obtained from 1,000 questions of five question types. Furthermore, we present the first computational method to predict recallability of different visualisation elements, such as the title or specific data values. We report detailed analyses of our method on VisRecall and demonstrate that it outperforms several baselines in overall recallability and FE-, F-, RV-, and U-question recallability. Our work makes fundamental contributions towards a new generation of methods to assist designers in optimising visualisations.",
                        "uid": "v-tvcg-9855227",
                        "time_stamp": "2023-10-25T23:45:00Z",
                        "time_start": "2023-10-25T23:45:00Z",
                        "time_end": "2023-10-25T23:57:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "1",
                        "paper_award": "",
                        "image_caption": "\u200bThis work makes three contributions: (1) We adapt a question-answering paradigm to study fine-grained recallability of information visualisations. (2) We present VisRecall \u2013\u2013 a novel dataset consisting of 200 visualisations that are annotated with crowd-sourced human recallability scores obtained from 1,000 questions of five types. (3) We present the first computational method to predict recallability of visualisations.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    },
                    {
                        "slot_id": "v-full-1021",
                        "session_id": "full31",
                        "title": "A Computational Design Process for Sensing Network Physicalizations",
                        "contributors": [
                            "S. Sandra Bae"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "v-full-1021",
                        "time_stamp": "2023-10-25T23:57:00Z",
                        "time_start": "2023-10-25T23:57:00Z",
                        "time_end": "2023-10-26T00:09:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "1",
                        "paper_award": "",
                        "image_caption": "A sensing network physicalization (N = 20, L = 40). (a) A multi-material 3D printed network physicalization produced by Bae et al\u2019s computational design pipeline. Conductive traces are embedded in the network\u2019s links which enables node selection via capacitive sensing. (b) A computational rendering of the network physicalization showcasing how the conductive traces are distributed throughout the network\u2019s links. The conductive traces use a serpentine pattern.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    },
                    {
                        "slot_id": "v-full-1573",
                        "session_id": "full31",
                        "title": "Designing for Ambiguity in Visual Analytics: Lessons from Risk Assessment and Prediction",
                        "contributors": [
                            "Stan Nowak"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "v-full-1573",
                        "time_stamp": "2023-10-26T00:09:00Z",
                        "time_start": "2023-10-26T00:09:00Z",
                        "time_end": "2023-10-26T00:21:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "1",
                        "paper_award": "",
                        "image_caption": "An interactive visualization prototype showing reports of observed avalanches developed for snow avalanche forecasters. The system is designed to address issues of ambiguity that forecasters face when making sense of avalanche observations and assessing avalanche hazards.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    },
                    {
                        "slot_id": "v-full-1020",
                        "session_id": "full31",
                        "title": "Dupo: A Mixed-Initiative Authoring Tool for Responsive Visualization",
                        "contributors": [
                            "Hyeok Kim"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "v-full-1020",
                        "time_stamp": "2023-10-26T00:21:00Z",
                        "time_start": "2023-10-26T00:21:00Z",
                        "time_end": "2023-10-26T00:33:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "1",
                        "paper_award": "",
                        "image_caption": "Dupo is a mixed-initiative authoring tool for responsive visualization. Using Dupo, a visualization author can make manual edits, such as directly repositioning annotations in the chart. They can explore design alternatives that are automatically generated for different screen types. Design suggestions for mobile screens include simple rescaling, axes transpose, encoding changes, and numbering annotations, for example.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    },
                    {
                        "slot_id": "v-full-1160",
                        "session_id": "full31",
                        "title": "InkSight: Leveraging Sketch Interaction for Documenting Chart Findings in Computational Notebooks",
                        "contributors": [
                            "Yanna Lin"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "v-full-1160",
                        "time_stamp": "2023-10-26T00:33:00Z",
                        "time_start": "2023-10-26T00:33:00Z",
                        "time_end": "2023-10-26T00:45:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "1",
                        "paper_award": "",
                        "image_caption": "This figure shows the interface of InkSight, including the sketch panel and documentation panel. When the user sketches atop the chart to identify areas of interest, InkSight automatically generates corresponding documentation on the right.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    },
                    {
                        "slot_id": "v-full-1159",
                        "session_id": "full31",
                        "title": "Why Change My Design: Explaining Poorly Constructed Visualization Designs with Explorable Explanations",
                        "contributors": [
                            "Leo Yu-Ho Lo"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "v-full-1159",
                        "time_stamp": "2023-10-26T00:45:00Z",
                        "time_start": "2023-10-26T00:45:00Z",
                        "time_end": "2023-10-26T00:57:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "1",
                        "paper_award": "",
                        "image_caption": "Explorable explanation designs for five different common chart issues studied in the paper \"Why Change My Design: Explaining Poorly Constructed Visualization Designs with Explorable Explanations\"",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    }
                ]
            },
            {
                "title": "Visualization for Humanities and Social Sciences (Full+Short)",
                "session_id": "full32",
                "event_prefix": "v-full",
                "track": "109(234)",
                "session_image": "full32.png",
                "chair": [],
                "organizers": [],
                "time_start": "2023-10-22T00:00:00Z",
                "time_end": "2023-10-25T04:15:00Z",
                "discord_category": "",
                "discord_channel": "",
                "discord_channel_id": "",
                "discord_link": "",
                "zoom_meeting": "",
                "zoom_password": "",
                "zoom_link": "",
                "time_slots": [
                    {
                        "slot_id": "v-full-1104",
                        "session_id": "full32",
                        "title": "InnovationInsights: A Visual Analytics Approach for Understanding the Dual Frontiers between Science and Technology",
                        "contributors": [
                            "Dr. Yifang Wang"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "v-full-1104",
                        "time_stamp": "2023-10-25T03:00:00Z",
                        "time_start": "2023-10-25T03:00:00Z",
                        "time_end": "2023-10-25T03:12:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "1",
                        "paper_award": "",
                        "image_caption": "Science is central to improving the human condition. Not only has science long been recognized as the engine for long-run economic growth and prosperity, but also it has been essential to creating critical solutions to confront emergent threats to humanity, from climate change to the COVID-19 pandemic. While scientific research propels both fundamental understanding and practical applications, there has been a lack of visual analytics approaches to explore the complex linkages (i.e., the dual frontiers) between scientific advances and technical inventions. Here we introduce InnovationInsights, which represents an initial step toward filling this crucial gap.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    },
                    {
                        "slot_id": "v-full-1274",
                        "session_id": "full32",
                        "title": "LiberRoad: Probing into the Journey of Chinese Classics through Visual Analytics",
                        "contributors": [
                            "Yuhan Guo"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "v-full-1274",
                        "time_stamp": "2023-10-25T03:12:00Z",
                        "time_start": "2023-10-25T03:12:00Z",
                        "time_end": "2023-10-25T03:24:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "1",
                        "paper_award": "",
                        "image_caption": "LiberRoad is a visual analytics system for humanities scholars to explore and analyze the book circulation data. Consisting the Location Graph, the Event Timeline, and the Geo Map, LiberRoad provides a clear presentation of the book circulation patterns across different historical periods.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    },
                    {
                        "slot_id": "v-full-1387",
                        "session_id": "full32",
                        "title": "Visualizing Historical Book Trade Data: An Iterative Design Study with Close Collaboration with Domain Experts",
                        "contributors": [
                            "Yiwen Xing"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "v-full-1387",
                        "time_stamp": "2023-10-25T03:24:00Z",
                        "time_start": "2023-10-25T03:24:00Z",
                        "time_end": "2023-10-25T03:36:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "1",
                        "paper_award": "",
                        "image_caption": "In our work, we introduce DanteExploreVis, a Visual Analytic (VA) tool designed to assist historians in exploring, explaining, and presenting book trade data. This timeline provides a thorough overview of the design process behind DanteExploreVis. It traces the stages of interface evolution, labeled I1 through I8, and highlights key sketches, indicated as S1 to S7, from each design iteration. Beyond a visual aid, this figure offers an in-depth narrative that emphasizes the crucial moments, decisions, and methodological adaptations we incorporated throughout our design study.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    },
                    {
                        "slot_id": "v-full-1333",
                        "session_id": "full32",
                        "title": "OldVisOnline: Curating a Dataset of Historical Visualizations",
                        "contributors": [
                            "Yu Zhang"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "v-full-1333",
                        "time_stamp": "2023-10-25T03:36:00Z",
                        "time_start": "2023-10-25T03:36:00Z",
                        "time_end": "2023-10-25T03:48:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "1",
                        "paper_award": "",
                        "image_caption": "The image gallery interface of OldVisOnline contains images and corresponding metadata of 13K historical visualizations published before 1950 that we curate from various data sources. The user can search and filter metadata fields of historical visualizations.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    },
                    {
                        "slot_id": "v-short-1027",
                        "session_id": "full32",
                        "title": "What Exactly is an Insight? A Literature Review",
                        "contributors": [
                            "Leilani Battle"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "v-short-1027",
                        "time_stamp": "2023-10-25T03:48:00Z",
                        "time_start": "2023-10-25T03:48:00Z",
                        "time_end": "2023-10-25T04:00:00Z",
                        "paper_type": "short",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "1",
                        "paper_award": "",
                        "image_caption": "Based on a review of the literature, insights seem to capture knowledge that can be inferred directly from a dataset such as data facts, generalizations of these facts, and hypotheses to be tested (internal knowledge). Insights also link internal knowledge with user domain knowledge, personal experiences, and tool expertise (external knowledge).",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    },
                    {
                        "slot_id": "v-short-1083",
                        "session_id": "full32",
                        "title": "WhaleVis: Visualizing the History of Commercial Whaling",
                        "contributors": [
                            "Ameya B Patil"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "v-short-1083",
                        "time_stamp": "2023-10-25T04:00:00Z",
                        "time_start": "2023-10-25T04:00:00Z",
                        "time_end": "2023-10-25T04:12:00Z",
                        "paper_type": "short",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "1",
                        "paper_award": "",
                        "image_caption": "Selective visualizations from WhaleVis: The map visualization shows pelagic (offshore) whale catches and the routes traversed by whaling expeditions between 1880 and 1986. The bar chart shows the breakdown for pelagic vs land catches and also facilitates setting a filter for pelagic whale catches. The route density in the map visualization enables visual estimation of where whales were searched for. The North Atlantic and South Pacific Oceans have  fewer catches than other regions. Since fewer expeditions traversed those waters, we are aware of a relative reduction in search effort when inferring the whale populations from reported catches in those regions.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    }
                ]
            },
            {
                "title": "Visualization for the Physical Sciences",
                "session_id": "full33",
                "event_prefix": "v-full",
                "track": "103(132)",
                "session_image": "full33.png",
                "chair": [
                    "Markus Hadwiger"
                ],
                "organizers": [],
                "time_start": "2023-10-22T00:00:00Z",
                "time_end": "2023-10-25T23:15:00Z",
                "discord_category": "",
                "discord_channel": "",
                "discord_channel_id": "",
                "discord_link": "",
                "zoom_meeting": "",
                "zoom_password": "",
                "zoom_link": "",
                "time_slots": [
                    {
                        "slot_id": "v-full-1673",
                        "session_id": "full33",
                        "title": "Dr. KID: Direct Remeshing and K-set Isometric Decomposition for Scalable Physicalization of Organic Shapes",
                        "contributors": [
                            "Dawar Khan"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "v-full-1673",
                        "time_stamp": "2023-10-25T22:00:00Z",
                        "time_start": "2023-10-25T22:00:00Z",
                        "time_end": "2023-10-25T22:12:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "1",
                        "paper_award": "",
                        "image_caption": "The physicalization of potato-shaped biological structures with k types of triangles. Back row: SARS-CoV-2 virion membrane (left) with k = 2, SARS-CoV-2 virion membrane with smooth triangle patches (right), using k = 6, and front row: cell nuclei membrane (left), using k = 5, SARS-CoV-2 virion membrane (center), using k = 2, mitochondria outer membrane (right), using k = 6.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    },
                    {
                        "slot_id": "v-full-1669",
                        "session_id": "full33",
                        "title": "Extract and Characterize Hairpin Vortices in Turbulent Flows",
                        "contributors": [
                            "Adeel Zafar"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "v-full-1669",
                        "time_stamp": "2023-10-25T22:12:00Z",
                        "time_start": "2023-10-25T22:12:00Z",
                        "time_end": "2023-10-25T22:24:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "1",
                        "paper_award": "",
                        "image_caption": "Extract, separate, identify and characterize hairpin vortices in turbulent flows",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    },
                    {
                        "slot_id": "v-full-1530",
                        "session_id": "full33",
                        "title": "MolSieve: A Progressive Visual Analytics System for Molecular Dynamics Simulations",
                        "contributors": [
                            "Rostyslav Hnatyshyn"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "v-full-1530",
                        "time_stamp": "2023-10-25T22:24:00Z",
                        "time_start": "2023-10-25T22:24:00Z",
                        "time_end": "2023-10-25T22:36:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "1",
                        "paper_award": "",
                        "image_caption": "MolSieve is a visual analytics system that reduces large molecular dynamics simulations into their essential components. To accomplish this, GPCCA (Generalized Perron-Cluster-Cluster-Analysis) is applied to a trajectory's transition matrix. The results are then mapped to an easy-to-use and efficient visual representation. MolSieve is scalable and adaptable; multiple large trajectories can be analyzed at once, and many interactions exist to make it easy to compare multiple simulations. MolSieve can be customized through a simple Python programming interface to adapt to any kind of simulation.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    },
                    {
                        "slot_id": "v-full-1773",
                        "session_id": "full33",
                        "title": "ProWis: A Visual Approach for Building, Managing, and Analyzing Weather Simulation Ensembles at Runtime",
                        "contributors": [
                            "Marcos Lage"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "v-full-1773",
                        "time_stamp": "2023-10-25T22:36:00Z",
                        "time_start": "2023-10-25T22:36:00Z",
                        "time_end": "2023-10-25T22:48:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "1",
                        "paper_award": "",
                        "image_caption": "We present the Provenance-enabled Weather Visualization (ProWis) system, a visual analytics system to assist weather professionals to work with the Weather Research and Forecasting model (WRF). The interactive and provenance-oriented system was designed to help weather experts build, manage, and analyze simulation ensembles at runtime. Our system follows a human-in-the-loop approach to enable the exploration of multiple atmospheric variables and weather scenarios. In collaboration with weather experts, we demonstrate its effectiveness by presenting two case studies of rainfall events in Brazil.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    },
                    {
                        "slot_id": "v-full-1253",
                        "session_id": "full33",
                        "title": "Vimo: Visual Analysis of Neuronal Connectivity Motifs",
                        "contributors": [
                            "Jakob Troidl"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "v-full-1253",
                        "time_stamp": "2023-10-25T22:48:00Z",
                        "time_start": "2023-10-25T22:48:00Z",
                        "time_end": "2023-10-25T23:00:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "1",
                        "paper_award": "",
                        "image_caption": "Vimo is a visual analysis tool to search and analyze arbitrary connectivity motifs in large brain networks.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    },
                    {
                        "slot_id": "v-full-1298",
                        "session_id": "full33",
                        "title": "Visual Analysis of Displacement Processes in Porous Media using Spatio-Temporal Flow Graphs",
                        "contributors": [
                            "Alexander Straub"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "v-full-1298",
                        "time_stamp": "2023-10-25T23:00:00Z",
                        "time_start": "2023-10-25T23:00:00Z",
                        "time_end": "2023-10-25T23:12:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "1",
                        "paper_award": "",
                        "image_caption": "Different abstractions of the displacement process of fluids in a porous material.  Time map showing the invading fluid entering the porous domain on the right side and propagating towards the outlet on the left. The periodic color map provides time information directly (violet to yellow), and velocity from its frequency.  The extracted graphs are either shown in their physical embedding with time information (displacement graph; white to red), or employing a different layout to show the main channel and highlight the number of outgoing edges (breakthrough graph; categorical color map).",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    }
                ]
            }
        ]
    },
    "v-short": {
        "event": "VIS Short Papers",
        "long_name": "VIS Short Papers",
        "event_type": "Paper Presentations",
        "event_prefix": "v-short",
        "event_description": "",
        "event_url": "",
        "organizers": [],
        "sessions": [
            {
                "title": "Information Visualization / Interaction",
                "session_id": "short1",
                "event_prefix": "v-short",
                "track": "104(132)",
                "session_image": "short1.png",
                "chair": [
                    "Andreas Kerren"
                ],
                "organizers": [],
                "time_start": "2023-10-22T00:00:00Z",
                "time_end": "2023-10-26T04:15:00Z",
                "discord_category": "",
                "discord_channel": "",
                "discord_channel_id": "",
                "discord_link": "",
                "zoom_meeting": "",
                "zoom_password": "",
                "zoom_link": "",
                "time_slots": [
                    {
                        "slot_id": "v-short-1160",
                        "session_id": "short1",
                        "title": "A Simple yet Useful Spiral Visualization of Large Graphs",
                        "contributors": [
                            "Graima Jindal"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "v-short-1160",
                        "time_stamp": "2023-10-26T03:00:00Z",
                        "time_start": "2023-10-26T03:00:00Z",
                        "time_end": "2023-10-26T03:09:00Z",
                        "paper_type": "short",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "1",
                        "paper_award": "",
                        "image_caption": "This figure presents a Spiral Visualization on the Facebook dataset having 4,039 nodes and 88,234 edges. The Spiral Visualization uses continuous diverging color coding. Each spiral in a spiral visualization represents a community within the dataset. The radius of the spiral represents the size of the community. Each edge represents the existence of inter-community edges between two communities, and the edge width represents the number of interconnections between them. Nodes are ordered and colored based on degree centrality to visualize and compare degree distribution in spirals.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    },
                    {
                        "slot_id": "v-short-1189",
                        "session_id": "short1",
                        "title": "ProtoGraph: A Non-Expert Toolkit for Creating Animated Graphs",
                        "contributors": [
                            "Carolina Nobre"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "v-short-1189",
                        "time_stamp": "2023-10-26T03:09:00Z",
                        "time_start": "2023-10-26T03:09:00Z",
                        "time_end": "2023-10-26T03:18:00Z",
                        "paper_type": "short",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "1",
                        "paper_award": "",
                        "image_caption": "ProtoGraph web tool interface. The left column shows the code editor where the user can write ProtoGraph language statements, the right column displays the graph which dynamically renders as the user types in the editor. The bottom right panel  shows the animation timeline previewing each step of the animation. The user can share the current graph using an auto-generated  URL or export the visualization as an image or video.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    },
                    {
                        "slot_id": "v-short-1103",
                        "session_id": "short1",
                        "title": "Visual Validation versus Visual Estimation: A Study on the Average Value in Scatterplots",
                        "contributors": [
                            "Daniel Braun"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "v-short-1103",
                        "time_stamp": "2023-10-26T03:18:00Z",
                        "time_start": "2023-10-26T03:18:00Z",
                        "time_end": "2023-10-26T03:27:00Z",
                        "paper_type": "short",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "1",
                        "paper_award": "",
                        "image_caption": "Example scatterplot shown to participants in our user study to investigate the differences between (a) visually estimating  and (b) visually validating the average value of the shown data. In (c), the red lines indicate the upper border of the statistical 95%  confidence interval (CI) of the average value and the blue line shows the data\u2019s true average value.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    },
                    {
                        "slot_id": "v-short-1014",
                        "session_id": "short1",
                        "title": "Line Harp: Importance-Driven Sonification for Dense Line Charts",
                        "contributors": [
                            "Egil Bru"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "v-short-1014",
                        "time_stamp": "2023-10-26T03:27:00Z",
                        "time_start": "2023-10-26T03:27:00Z",
                        "time_end": "2023-10-26T03:36:00Z",
                        "paper_type": "short",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "1",
                        "paper_award": "",
                        "image_caption": "Line Harp is an interactive sonification approach for dense line charts. We utilize the metaphor of a string instrument, where individual line segments can be \u201dplucked\u201d. We propose an importance-driven approach which encodes the directionality of line segments using frequency and dynamically scales amplitude for improved density perception, and also provide additional sonified lenses for enhanced data exploration.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    },
                    {
                        "slot_id": "v-short-1030",
                        "session_id": "short1",
                        "title": "Compact Phase Histograms for Guided Exploration of Periodicity",
                        "contributors": [
                            "Max Franke"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "v-short-1030",
                        "time_stamp": "2023-10-26T03:36:00Z",
                        "time_start": "2023-10-26T03:36:00Z",
                        "time_end": "2023-10-26T03:45:00Z",
                        "paper_type": "short",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "1",
                        "paper_award": "",
                        "image_caption": "Our compact widget visualizes a histogram of the phases of event data for a specific period length, which can be interactively adjusted. If the period length matches a periodicity contained in the data, this shows up as a non-uniform distribution in the histogram. In a matrix visualization, the phase histograms for slightly longer and shorted period lengths are also shown row-wise.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    },
                    {
                        "slot_id": "v-short-1036",
                        "session_id": "short1",
                        "title": "ZADU: A Python Library for Evaluating the Reliability of Dimensionality Reduction Embeddings",
                        "contributors": [
                            "Hyeon Jeon"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "v-short-1036",
                        "time_stamp": "2023-10-26T03:45:00Z",
                        "time_start": "2023-10-26T03:45:00Z",
                        "time_end": "2023-10-26T03:54:00Z",
                        "paper_type": "short",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "1",
                        "paper_award": "",
                        "image_caption": "The UMAP embedding of the MNIST dataset (leftmost column), and two distortion visualizations generated by ZADUVis: CheckViz and the Reliability Map.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    },
                    {
                        "slot_id": "v-short-1149",
                        "session_id": "short1",
                        "title": "TimePool: Visually Answer \"Which and When\" Questions On Univariate Time Series",
                        "contributors": [
                            "Dr. Tinghao Feng"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "v-short-1149",
                        "time_stamp": "2023-10-26T03:54:00Z",
                        "time_start": "2023-10-26T03:54:00Z",
                        "time_end": "2023-10-26T04:03:00Z",
                        "paper_type": "short",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "1",
                        "paper_award": "",
                        "image_caption": "When exploring time series datasets, analysts often pose \u201cwhich and when\u201d questions. For example, with world life expectancy data over one hundred years, they may inquire about the top 10 countries in life expectancy and the time period when they achieved this status, or which countries have had longer life expectancy than Ireland and when. This paper proposes TimePool, a new visualization prototype, to address this need for univariate time series analysis. It allows users to construct interactive \u201cwhich and when\u201d queries and visually explore the results for insights.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    },
                    {
                        "slot_id": "v-short-1159",
                        "session_id": "short1",
                        "title": "Two Heads are Better than One: Pair-Interviews for Visualization",
                        "contributors": [
                            "Derya Akbaba"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "v-short-1159",
                        "time_stamp": "2023-10-26T04:03:00Z",
                        "time_start": "2023-10-26T04:03:00Z",
                        "time_end": "2023-10-26T04:12:00Z",
                        "paper_type": "short",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "1",
                        "paper_award": "",
                        "image_caption": "This technique is not just for dinosaurs! We introduce the pair-interview technique for visualization researchers. During the talk we review the technique, the roles of the two interviewers, and the benefits we experienced across four separate research teams.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    }
                ]
            },
            {
                "title": "Applications / Design",
                "session_id": "short2",
                "event_prefix": "v-short",
                "track": "104(132)",
                "session_image": "short2.png",
                "chair": [],
                "organizers": [],
                "time_start": "2023-10-22T00:00:00Z",
                "time_end": "2023-10-26T01:00:00Z",
                "discord_category": "",
                "discord_channel": "",
                "discord_channel_id": "",
                "discord_link": "",
                "zoom_meeting": "",
                "zoom_password": "",
                "zoom_link": "",
                "time_slots": [
                    {
                        "slot_id": "v-short-1140",
                        "session_id": "short2",
                        "title": "Taken By Surprise? Evaluating how Bayesian Surprise & Suppression Influences Peoples' Takeaways in Map Visualizations",
                        "contributors": [
                            "Akim Ndlovu"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "v-short-1140",
                        "time_stamp": "2023-10-25T23:45:00Z",
                        "time_start": "2023-10-25T23:45:00Z",
                        "time_end": "2023-10-25T23:54:00Z",
                        "paper_type": "short",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "1",
                        "paper_award": "",
                        "image_caption": "How do Bayesian surprise metrics and suppression encodings influence peoples\u2019 takeaways in map visualizations? We conduct two experiments with Covid-19 and Poverty datasets, randomly assigning 300 participants to three map conditions. We collect data across three map takeaway tasks T1-Best, T1-Worst: Identify and T2: Explore. To mitigate biases for particular dataset  contexts discovered in pilot studies (e.g. vaccine skepticism) we reframe both datasets as a sales and marketing task. Metrics include participants\u2019 exploration metadata (quantitative) and takeaway comments (qualitative)",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    },
                    {
                        "slot_id": "v-short-1071",
                        "session_id": "short2",
                        "title": "Towards Autocomplete Strategies for Visualization Construction",
                        "contributors": [
                            "Wei Wei"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "v-short-1071",
                        "time_stamp": "2023-10-25T23:54:00Z",
                        "time_start": "2023-10-25T23:54:00Z",
                        "time_end": "2023-10-26T00:03:00Z",
                        "paper_type": "short",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "1",
                        "paper_award": "",
                        "image_caption": "The three visualization autocomplete strategies identified in our study. The dashed boxes in different colors represent autocomplete suggestions. Based on the token(s) that has been placed, NEXT-STEP provides suggestions for a single next visual mapping operation. GHOST provides situated partial or completed visualization recommendations. GALLERY provides a gallery of completed visual mapping options.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    },
                    {
                        "slot_id": "v-short-1116",
                        "session_id": "short2",
                        "title": "Indy Survey Tool: A Framework to Unearth Correlations in Survey Data",
                        "contributors": [
                            "Tarik Crnovrsanin"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "v-short-1116",
                        "time_stamp": "2023-10-26T00:03:00Z",
                        "time_start": "2023-10-26T00:03:00Z",
                        "time_end": "2023-10-26T00:12:00Z",
                        "paper_type": "short",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "1",
                        "paper_award": "",
                        "image_caption": "An example of how the Indy Survey tool we present was used in recent survey on Immersive Analytics. The left panel lets users filter using a search bar, timeline, and topic selector. The top bar  provides information about the survey and how to add new entries. The center  shows a short summary of each included paper. The collapsible visualization panel on the right shows a correlation matrix for two selected dimensions. Interacting with the left and right panels filters the papers displayed in the center.  Upon selection of a paper, a detail view pops up with all of its information (not shown).",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    },
                    {
                        "slot_id": "v-short-1195",
                        "session_id": "short2",
                        "title": "Data in the Wind: Evaluating Multiple-Encoding Design for Particle Motion Visualizations",
                        "contributors": [
                            "Yiren Ding"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "v-short-1195",
                        "time_stamp": "2023-10-26T00:12:00Z",
                        "time_start": "2023-10-26T00:12:00Z",
                        "time_end": "2023-10-26T00:21:00Z",
                        "paper_type": "short",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "1",
                        "paper_award": "",
                        "image_caption": "This study explores the use of motion in data visualizations, particularly in particle flow maps for representing wind speed and direction. This research employs a staircase methodology to model just-noticeable differences in perception in both motion-only and motion + static conditions. The results indicate that using motion encoding, as seen in popular wind flow maps, enhance not only visual appeal but also improves speed discrimination for the average viewer. These findings contribute empirical guidance for particle motion encoding design, and lay groundwork for future investigations as such motion encodings continue to be used in visualization practice.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    },
                    {
                        "slot_id": "v-short-1002",
                        "session_id": "short2",
                        "title": "Show me my Users: A Dashboard Design Visualizing User Interaction Logs with Interactive Visualization",
                        "contributors": [
                            "Mashael AlKadi"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "v-short-1002",
                        "time_stamp": "2023-10-26T00:21:00Z",
                        "time_start": "2023-10-26T00:21:00Z",
                        "time_end": "2023-10-26T00:30:00Z",
                        "paper_type": "short",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "1",
                        "paper_award": "",
                        "image_caption": "(a) Dashboard Design Pipeline of Visualization Tool Logs: this diagram represents the process needed to create analytical dashboard to understand users\ufffd needs in visual exploration of networks. (b) Overview Page: this figure shows the first page of the dashboard\ufffds 3 pages (overview, visualizations, user) that provides analytics about the usage of the tool, the visualizations and the user exploration respectively.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    },
                    {
                        "slot_id": "v-short-1010",
                        "session_id": "short2",
                        "title": "What Is the Difference Between a Mountain and a Molehill? Quantifying Semantic Labeling of Visual Features in Line Charts",
                        "contributors": [
                            "Vidya Setlur"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "v-short-1010",
                        "time_stamp": "2023-10-26T00:30:00Z",
                        "time_start": "2023-10-26T00:30:00Z",
                        "time_end": "2023-10-26T00:39:00Z",
                        "paper_type": "short",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "1",
                        "paper_award": "",
                        "image_caption": "The figure shows the average segment slope of the trend annotations. The maximum possible slope range available for the charts is -3 to +3 allowing us to empirically derive various inter-word relationships.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    },
                    {
                        "slot_id": "v-short-1018",
                        "session_id": "short2",
                        "title": "Draco 2: An Extensible Platform to Model Visualization Design",
                        "contributors": [
                            "Junran Yang"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "v-short-1018",
                        "time_stamp": "2023-10-26T00:39:00Z",
                        "time_start": "2023-10-26T00:39:00Z",
                        "time_end": "2023-10-26T00:48:00Z",
                        "paper_type": "short",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "1",
                        "paper_award": "",
                        "image_caption": "Draco introduces a constraint-based framework to model visualization design guidelines in automated design tools. We present Draco 2 with an improved specification format, a comprehensive test suite, thorough documentation, and convenient APIs. Designed to be more extensible and easier to integrate into visualization systems, we demonstrate its distinct advantages. We believe these enhancements position Draco 2 as a platform for future research.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    }
                ]
            },
            {
                "title": "Perception / Evaluation",
                "session_id": "short3",
                "event_prefix": "v-short",
                "track": "104(132)",
                "session_image": "short3.png",
                "chair": [
                    "Lace M. Padilla"
                ],
                "organizers": [],
                "time_start": "2023-10-22T00:00:00Z",
                "time_end": "2023-10-24T23:15:00Z",
                "discord_category": "",
                "discord_channel": "",
                "discord_channel_id": "",
                "discord_link": "",
                "zoom_meeting": "",
                "zoom_password": "",
                "zoom_link": "",
                "time_slots": [
                    {
                        "slot_id": "v-short-1142",
                        "session_id": "short3",
                        "title": "Topological Analysis and Approximate Identification of Leading Lines in Artworks Based on Discrete Morse Theory",
                        "contributors": [
                            "Fuminori Shibasaki"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "v-short-1142",
                        "time_stamp": "2023-10-24T22:00:00Z",
                        "time_start": "2023-10-24T22:00:00Z",
                        "time_end": "2023-10-24T22:09:00Z",
                        "paper_type": "short",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "1",
                        "paper_award": "",
                        "image_caption": "Still Life With Quince, Cabbage, Melon, and Cucumber (Juan S\u00e1nchez Cot\u00e1n, c. 1602) and its compositional analyses in terms of leading lines.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    },
                    {
                        "slot_id": "v-short-1162",
                        "session_id": "short3",
                        "title": "Effects of data distribution and granularity on color semantics for colormap data visualizations",
                        "contributors": [
                            "Clementine Zimnicki"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "v-short-1162",
                        "time_stamp": "2023-10-24T22:09:00Z",
                        "time_start": "2023-10-24T22:09:00Z",
                        "time_end": "2023-10-24T22:18:00Z",
                        "paper_type": "short",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "1",
                        "paper_award": "",
                        "image_caption": "An image of 80 square colormap visualizations in 20 columns and 4 rows. They are grouped by the 10 color scales used to generate them; from left to right, ColorBrewer Red and Blue, Gray, Hot, Magma+, Mako+, Viridis, Plasma, Autumn, and Winter. Maps are also grouped by granularity (maps appear either coarse or smooth), background (maps are presented on a black or white background), and shift condition (the colors in the maps are either shifted to create large dark regions, or colors are uniformly distributed throughout the maps).",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    },
                    {
                        "slot_id": "v-short-1190",
                        "session_id": "short3",
                        "title": "Let's Get Vis-ical: Perceptual Accuracy in Visual & Tactile Encodings",
                        "contributors": [
                            "Zhongzheng Xu"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "v-short-1190",
                        "time_stamp": "2023-10-24T22:18:00Z",
                        "time_start": "2023-10-24T22:18:00Z",
                        "time_end": "2023-10-24T22:27:00Z",
                        "paper_type": "short",
                        "keywords": [],
                        "has_image": "0",
                        "has_video": "1",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    },
                    {
                        "slot_id": "v-short-1092",
                        "session_id": "short3",
                        "title": "MinMaxLTTB: Leveraging MinMax-Preselection to Scale LTTB",
                        "contributors": [
                            "Jeroen Van Der Donckt"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "v-short-1092",
                        "time_stamp": "2023-10-24T22:27:00Z",
                        "time_start": "2023-10-24T22:27:00Z",
                        "time_end": "2023-10-24T22:36:00Z",
                        "paper_type": "short",
                        "keywords": [],
                        "has_image": "0",
                        "has_video": "1",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    },
                    {
                        "slot_id": "v-short-1066",
                        "session_id": "short3",
                        "title": "Do You Trust What You See? Toward A Multidimensional Measure of Trust in Visualization",
                        "contributors": [
                            "Saugat Pandey"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "v-short-1066",
                        "time_stamp": "2023-10-24T22:36:00Z",
                        "time_start": "2023-10-24T22:36:00Z",
                        "time_end": "2023-10-24T22:45:00Z",
                        "paper_type": "short",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "1",
                        "paper_award": "",
                        "image_caption": "An illustrative of our experiments. Participants rated various visualizations on FAMILIARITY, CLARITY, CREDIBILITY, RELIABILITY, and CONFIDENCE, exploring their alignment with visual features and trust ratings.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    },
                    {
                        "slot_id": "v-short-1138",
                        "session_id": "short3",
                        "title": "reVISit: Supporting Scalable Evaluation of Interactive Visualizations",
                        "contributors": [
                            "Yiren Ding"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "v-short-1138",
                        "time_stamp": "2023-10-24T22:45:00Z",
                        "time_start": "2023-10-24T22:45:00Z",
                        "time_end": "2023-10-24T22:54:00Z",
                        "paper_type": "short",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "1",
                        "paper_award": "",
                        "image_caption": "reVISit is an open-source toolkit designed to simplify the process of conducting empirical visualization studies, which are often resource-intensive and technically challenging. It offers a domain-specific language for study setup and includes various software components like UI elements, behavior tracking, and experiment management tools. These components, along with interactive or static stimuli provided by researchers, are combined to create a deployable web-based experiment. reVISit streamlines the setup and monitoring of studies, making it easier to conduct both simple graphical perception tasks and more complex interactive studies, addressing the growing demands of modern visualization research.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    },
                    {
                        "slot_id": "v-short-1133",
                        "session_id": "short3",
                        "title": "Augmented Reality for Scholarly Publication of 3D Visualizations in Astronomy: An Empirical Evaluation",
                        "contributors": [
                            "Jane L. Adams"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "v-short-1133",
                        "time_stamp": "2023-10-24T22:54:00Z",
                        "time_start": "2023-10-24T22:54:00Z",
                        "time_end": "2023-10-24T23:03:00Z",
                        "paper_type": "short",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "1",
                        "paper_award": "",
                        "image_caption": "Experimental design: Part 1, A survey on participants' expertise, experience with AR, and demographic information, Part 2, Two sets of three tasks in each the non-AR and one of the two AR conditions, along with NASA-TLX workload questionnaires, and Part 3 Open feedback from participants.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    },
                    {
                        "slot_id": "v-short-1137",
                        "session_id": "short3",
                        "title": "Comparing Morse Complexes Using Optimal Transport: An Experimental Study",
                        "contributors": [
                            "Mingzhe Li"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "v-short-1137",
                        "time_stamp": "2023-10-24T23:03:00Z",
                        "time_start": "2023-10-24T23:03:00Z",
                        "time_end": "2023-10-24T23:12:00Z",
                        "paper_type": "short",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "1",
                        "paper_award": "",
                        "image_caption": "The source and the target networks are generated from Morse graphs. The color correspondence indicates structural alignments between the source and the target using the partial Fused Gromov-Wasserstein distance. Noisy features that are ignored in the alignment are shown as hollow circles.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    }
                ]
            },
            {
                "title": "Scientific Visualization (Short)",
                "session_id": "short4",
                "event_prefix": "v-short",
                "track": "104(132)",
                "session_image": "short4.png",
                "chair": [
                    "Paul Rosen"
                ],
                "organizers": [],
                "time_start": "2023-10-22T00:00:00Z",
                "time_end": "2023-10-25T04:15:00Z",
                "discord_category": "",
                "discord_channel": "",
                "discord_channel_id": "",
                "discord_link": "",
                "zoom_meeting": "",
                "zoom_password": "",
                "zoom_link": "",
                "time_slots": [
                    {
                        "slot_id": "v-short-1057",
                        "session_id": "short4",
                        "title": "Visualizing Query Traversals Over Bounding Volume Hierarchies Using Treemaps",
                        "contributors": [
                            "Abhishek Madan"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "v-short-1057",
                        "time_stamp": "2023-10-25T03:00:00Z",
                        "time_start": "2023-10-25T03:00:00Z",
                        "time_end": "2023-10-25T03:09:00Z",
                        "paper_type": "short",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "1",
                        "paper_award": "",
                        "image_caption": "An annotated screenshot of our visualization, containing a 3D viewport, a zoomable treemap, and a pixel grid. All three views work together to show the structure of a bounding volume hierarchy (BVH) with coordination between the viewport and treemap, view statistics of ray intersection queries with the BVH, and view individual query traces and results.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    },
                    {
                        "slot_id": "v-short-1089",
                        "session_id": "short4",
                        "title": "Visual Analysis of Large Multi-Field AMR Data on GPUs Using Interactive Volume Lines",
                        "contributors": [
                            "Stefan Zellmann"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "v-short-1089",
                        "time_stamp": "2023-10-25T03:09:00Z",
                        "time_start": "2023-10-25T03:09:00Z",
                        "time_end": "2023-10-25T03:18:00Z",
                        "paper_type": "short",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "1",
                        "paper_award": "",
                        "image_caption": "Coupled interactive volume lines (IVL) and large volume visualization. At the top we show volume renderings of four different fields of an astrophysical data set; at the bottom, we show two different representations of IVLs as a way to visually explore large AMR volumes using techniques from visual analytics.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    },
                    {
                        "slot_id": "v-short-1154",
                        "session_id": "short4",
                        "title": "Fast Fiber Line Extraction for 2D Bivariate Scalar Fields",
                        "contributors": [
                            "Felix Raith"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "v-short-1154",
                        "time_stamp": "2023-10-25T03:18:00Z",
                        "time_start": "2023-10-25T03:18:00Z",
                        "time_end": "2023-10-25T03:27:00Z",
                        "paper_type": "short",
                        "keywords": [],
                        "has_image": "0",
                        "has_video": "1",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    },
                    {
                        "slot_id": "v-short-1157",
                        "session_id": "short4",
                        "title": "GeneticFlow: Exploring Scholar Impact with Interactive Visualization",
                        "contributors": [
                            "Fengli Xiao"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "v-short-1157",
                        "time_stamp": "2023-10-25T03:27:00Z",
                        "time_start": "2023-10-25T03:27:00Z",
                        "time_end": "2023-10-25T03:36:00Z",
                        "paper_type": "short",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "1",
                        "paper_award": "",
                        "image_caption": "GeneticFlow visualization interface (Prof. Keim's graph): (a) system control panel; (b) scholar demographics; (c) graph statistics; (d) GF graph visualization; (e) topic distribution map; (f) author/paper/citation detail panel.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    },
                    {
                        "slot_id": "v-short-1171",
                        "session_id": "short4",
                        "title": "Visualizing Similarity of Pathline Dynamics in 2D Flow Fields",
                        "contributors": [
                            "Baldwin Nsonga"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "v-short-1171",
                        "time_stamp": "2023-10-25T03:36:00Z",
                        "time_start": "2023-10-25T03:36:00Z",
                        "time_end": "2023-10-25T03:45:00Z",
                        "paper_type": "short",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "1",
                        "paper_award": "",
                        "image_caption": "Even though the analysis of unsteady 2D flow fields is challenging, fluid mechanics experts generally have an intuition on where in the simulation domain specific features are expected. Using this intuition, showing similar regions enables the user to discover flow patterns within the simulation data.  We utilize infinitesimal strain theory and the Jensen-Shannon divergence to visualize similar and dissimilar regions with respect to a region selected by the user. We validate our method by applying it to two simulation datasets of two-dimensional unsteady flows.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    },
                    {
                        "slot_id": "v-short-1114",
                        "session_id": "short4",
                        "title": "Evaluation of cinematic volume rendering open-source and commercial solutions for the exploration of congenital heart data",
                        "contributors": [
                            "Irum Baseer"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "v-short-1114",
                        "time_stamp": "2023-10-25T03:45:00Z",
                        "time_start": "2023-10-25T03:45:00Z",
                        "time_end": "2023-10-25T03:54:00Z",
                        "paper_type": "short",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "1",
                        "paper_award": "",
                        "image_caption": "Exploring Innovative Medical Visualization: We've ventured into the realm of medical imaging, investigating advanced techniques to improve decision-making in congenital heart disease cases. Traditional methods, which depend on basic lighting models, often lead to distortions in complex cardiac data. Enter cinematic rendering (CR), an impressive three-dimensional photo-realistic visualization method. Our study thoroughly examined the effects of CR on heart anatomy visualization, utilizing the open-source MeVisLab framework, and compared its performance to a commercial clinical tool.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    },
                    {
                        "slot_id": "v-short-1182",
                        "session_id": "short4",
                        "title": "ExoplanetExplorer: Contextual Visualization of Exoplanet Systems",
                        "contributors": [
                            "Emma Broman"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "v-short-1182",
                        "time_stamp": "2023-10-25T03:54:00Z",
                        "time_start": "2023-10-25T03:54:00Z",
                        "time_end": "2023-10-25T04:03:00Z",
                        "paper_type": "short",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "1",
                        "paper_award": "",
                        "image_caption": "The overview mode of the ExoplanetExplorer, showing glyps for exoplanets positioned in their 3D spatial context, together with some menus.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    },
                    {
                        "slot_id": "v-short-1053",
                        "session_id": "short4",
                        "title": "A Visualization System for Hexahedral Mesh Quality Study",
                        "contributors": [
                            "Lei Si"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "v-short-1053",
                        "time_stamp": "2023-10-25T04:03:00Z",
                        "time_start": "2023-10-25T04:03:00Z",
                        "time_end": "2023-10-25T04:12:00Z",
                        "paper_type": "short",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "1",
                        "paper_award": "",
                        "image_caption": "Element quality visualization : element quality is visualized using glyphs, If glyphs overlap in the main view, they are aggregated. Each vertex glyph is displayed in the region view. Boundary error visualization : the distance between two boundaries is embedded into the UV domain. Users can access individual vertex quality details through an interactive bar chart.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    }
                ]
            },
            {
                "title": "Machine Learning / Language Models / Theory",
                "session_id": "short5",
                "event_prefix": "v-short",
                "track": "104(132)",
                "session_image": "short5.png",
                "chair": [
                    "Chaoli Wang"
                ],
                "organizers": [],
                "time_start": "2023-10-22T00:00:00Z",
                "time_end": "2023-10-26T23:15:00Z",
                "discord_category": "",
                "discord_channel": "",
                "discord_channel_id": "",
                "discord_link": "",
                "zoom_meeting": "",
                "zoom_password": "",
                "zoom_link": "",
                "time_slots": [
                    {
                        "slot_id": "v-short-1097",
                        "session_id": "short5",
                        "title": "Explain-and-Test: An Interactive Machine Learning Framework for Exploring Text Embeddings",
                        "contributors": [
                            "Shivam Raval"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "v-short-1097",
                        "time_stamp": "2023-10-26T22:00:00Z",
                        "time_start": "2023-10-26T22:00:00Z",
                        "time_end": "2023-10-26T22:09:00Z",
                        "paper_type": "short",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "1",
                        "paper_award": "",
                        "image_caption": "The Explain-and-test framework visualizes text embeddings, allowing users to lasso clusters of interest to get automated explanations of these clusters. The system provides two kinds of explanations: Contrastive PhraseClouds generated by an interpretable SVM model and Natural Language Explanations generated by a large language model (GPT-4). Users can test these automated explanations by entering manual labels to be dynamically embedded into the visualization, a feature we call Assessment by Re-projection. If the new text is mapped to the cluster, it validates the automated explanation. The figure shows a t-SNE projection where each point corresponds to the paper title from the Visualization Publications dataset along along with explanations and test inputs for two different clusters. The color encoding corresponds to labels predicted by clustering the projections",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    },
                    {
                        "slot_id": "v-short-1186",
                        "session_id": "short5",
                        "title": "Concept Lens: Visually Analyzing the Consistency of Semantic Manipulation in GANs",
                        "contributors": [
                            "Sangwon Jeong"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "v-short-1186",
                        "time_stamp": "2023-10-26T22:09:00Z",
                        "time_start": "2023-10-26T22:09:00Z",
                        "time_end": "2023-10-26T22:18:00Z",
                        "paper_type": "short",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "1",
                        "paper_award": "",
                        "image_caption": "Concept Lens is an interactive diagnostic tool to let users understand the latent space of any generative models. Individual icicle plot reveals hierarchy of code and direction, respectively. Code and direction hierarchy is jointly represented in the bi-hierarchy view in the middle which allows users to study the latent space from both global and local perspective.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    },
                    {
                        "slot_id": "v-short-1065",
                        "session_id": "short5",
                        "title": "HAiVA: Hybrid AI-assisted Visual Analysis Framework to Study the Effects of Cloud Properties on Climate Patterns",
                        "contributors": [
                            "Subhashis Hazarika"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "v-short-1065",
                        "time_stamp": "2023-10-26T22:18:00Z",
                        "time_start": "2023-10-26T22:18:00Z",
                        "time_end": "2023-10-26T22:27:00Z",
                        "paper_type": "short",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "1",
                        "paper_award": "",
                        "image_caption": "HAiVA is a Hybrid AI-assisted Visual Analysis Framework to Study the Effects of Cloud Properties on Climate Patterns. It offers an interactive visualization system for climate scientists to perform rapid prototyping and exploration of climate intervention technique called Marine Cloud Brightening (MCB).  HAiVA facilitate testing different MCB intervention scenarios and evaluating their possible intended and unintended climate consequences.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    },
                    {
                        "slot_id": "v-short-1108",
                        "session_id": "short5",
                        "title": "DataTales: Investigating the Use of Large Language Models for Authoring Data-Driven Articles",
                        "contributors": [
                            "Nicole Sultanum"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "v-short-1108",
                        "time_stamp": "2023-10-26T22:27:00Z",
                        "time_start": "2023-10-26T22:27:00Z",
                        "time_end": "2023-10-26T22:36:00Z",
                        "paper_type": "short",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "1",
                        "paper_award": "",
                        "image_caption": "User Interface of DataTales on the background, and DataTales logo on the foreground. User interface features a stacked bar chart titled \"America's favorite & least favorite months of the year\", the contents of a data story generated for this chart, and a list of other generated stories. A mouse cursor hovers over a text passage, and the corresponding data elements mentioned in the passage are highlighted in the chart.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    },
                    {
                        "slot_id": "v-short-1123",
                        "session_id": "short5",
                        "title": "Visualizing Linguistic Diversity of Text Datasets Synthesized by Large Language Models",
                        "contributors": [
                            "Emily Reif"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "v-short-1123",
                        "time_stamp": "2023-10-26T22:36:00Z",
                        "time_start": "2023-10-26T22:36:00Z",
                        "time_end": "2023-10-26T22:45:00Z",
                        "paper_type": "short",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "1",
                        "paper_award": "",
                        "image_caption": "LinguisticLens, a new visualization tool for making sense of text datasets synthesized by large language models (LLMs) and analyzing the diversity of examples.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    },
                    {
                        "slot_id": "v-short-1001",
                        "session_id": "short5",
                        "title": "WUDA: Visualizing and Transforming Rotations in Real-Time with Quaternions and Smart Devices",
                        "contributors": [
                            "Slobodan Milanko"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "v-short-1001",
                        "time_stamp": "2023-10-26T22:45:00Z",
                        "time_start": "2023-10-26T22:45:00Z",
                        "time_end": "2023-10-26T22:54:00Z",
                        "paper_type": "short",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "1",
                        "paper_award": "",
                        "image_caption": "Wuda draws an interactive sphere visualization that embeds a Geodesic Icosahedron with labeled faces. It allows you to study smart device rotations, angles, and transformations in real-time.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    },
                    {
                        "slot_id": "v-short-1006",
                        "session_id": "short5",
                        "title": "ScatterUQ: Interactive Uncertainty Visualizations for Multiclass Deep Learning Problems",
                        "contributors": [
                            "Harry Li"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "v-short-1006",
                        "time_stamp": "2023-10-26T22:54:00Z",
                        "time_start": "2023-10-26T22:54:00Z",
                        "time_end": "2023-10-26T23:03:00Z",
                        "paper_type": "short",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "1",
                        "paper_award": "",
                        "image_caption": "ScatterUQ plot of an out of distribution MNIST test sample (left sidebar and gray dot), ten Fashion-MNIST training examplesfrom the closest class Sandal (blue dots), and the nearest Sandal training example (right sidebar). ScatterUQ uses dimensionality reduction to visualize neural network uncertainty to help end users make more informed decisions and to help machine learning engineers improve their models.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    },
                    {
                        "slot_id": "v-short-1060",
                        "session_id": "short5",
                        "title": "Combining Degree of Interest Functions and Progressive Visualization",
                        "contributors": [
                            "Marius Hogr\u00e4fer"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "v-short-1060",
                        "time_stamp": "2023-10-26T23:03:00Z",
                        "time_start": "2023-10-26T23:03:00Z",
                        "time_end": "2023-10-26T23:12:00Z",
                        "paper_type": "short",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "1",
                        "paper_award": "",
                        "image_caption": "Our model that enables DOI functions on progressive visualization: Based on the current user interest, data that is deemed interesting gets retrieved from the database and integrated with the data already in the visualization, using the interest model to identify and update outdated interest values.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    }
                ]
            },
            {
                "title": "CoVID-19 / Bioinformatics / Visual Analytics",
                "session_id": "short6",
                "event_prefix": "v-short",
                "track": "104(132)",
                "session_image": "short6.png",
                "chair": [
                    "Alfie Abdul-Rahman"
                ],
                "organizers": [],
                "time_start": "2023-10-22T00:00:00Z",
                "time_end": "2023-10-25T06:00:00Z",
                "discord_category": "",
                "discord_channel": "",
                "discord_channel_id": "",
                "discord_link": "",
                "zoom_meeting": "",
                "zoom_password": "",
                "zoom_link": "",
                "time_slots": [
                    {
                        "slot_id": "v-short-1102",
                        "session_id": "short6",
                        "title": "The Role of Visualization in Genomics Data Analysis Workflows: The Interviews",
                        "contributors": [
                            "Sehi L'Yi"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "v-short-1102",
                        "time_stamp": "2023-10-25T04:45:00Z",
                        "time_start": "2023-10-25T04:45:00Z",
                        "time_end": "2023-10-25T04:54:00Z",
                        "paper_type": "short",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "1",
                        "paper_award": "",
                        "image_caption": "The summary of workflows of all interview participants. These flow charts are drawn collaboratively with participants during the interviews, reflecting their everyday workflows. The names of the tools used in individual stages are labeled right below circular nodes.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    },
                    {
                        "slot_id": "v-short-1147",
                        "session_id": "short6",
                        "title": "Vis-SPLIT: Interactive Hierarchical Modeling for mRNA Expression Classification",
                        "contributors": [
                            "Braden Roper"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "v-short-1147",
                        "time_stamp": "2023-10-25T04:54:00Z",
                        "time_start": "2023-10-25T04:54:00Z",
                        "time_end": "2023-10-25T05:03:00Z",
                        "paper_type": "short",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "1",
                        "paper_award": "",
                        "image_caption": "An overview of the Vis-SPLIT tool. (A) The Hierarchical Overview is an abstract view of the current clusters. (B) The Heatmap Overview displays the patterns for the expression values of genes in each cluster. (C) The Survival Analysis View visualizes survival curves for each cluster. (D) The PCA View allows users to view or split the selected node in the Hierarchical Overview, and includes (D.1) the Projection depicting individuals in 2D, placed based on genetic expression, (D.2-D.3) axis-aligned heatmaps displaying the expression values of genes, and (D.4) the Feature Loadings Plot showing current gene contributions.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    },
                    {
                        "slot_id": "v-short-1107",
                        "session_id": "short6",
                        "title": "Enabling Multimodal User Interactions for Genomics Visualization Creation",
                        "contributors": [
                            "Qianwen Wang"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "v-short-1107",
                        "time_stamp": "2023-10-25T05:03:00Z",
                        "time_start": "2023-10-25T05:03:00Z",
                        "time_end": "2023-10-25T05:12:00Z",
                        "paper_type": "short",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "0",
                        "paper_award": "",
                        "image_caption": "AutoGosling facilitates the creation of genomics visualizations by enabling multimodal interactions. Instead of directly constructing the grammar-based visualization specifications users express design intentions through sketches/examples images, a template GUI, and natural language commands, which are interpreted by AutoGosling and converted into interactive genomics visualizations. Interactions are progressively introduced to minimize information overload and reduce mode-switching.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    },
                    {
                        "slot_id": "v-short-1067",
                        "session_id": "short6",
                        "title": "Simulating the Geometric Growth of the Marine Sponge Crella Incrustans",
                        "contributors": [
                            "Andrew Chalmers"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "v-short-1067",
                        "time_stamp": "2023-10-25T05:12:00Z",
                        "time_start": "2023-10-25T05:12:00Z",
                        "time_end": "2023-10-25T05:21:00Z",
                        "paper_type": "short",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "1",
                        "paper_award": "",
                        "image_caption": "Overview of simulating the marine sponge Crella incrustans. Given the (a) skeletal architecture that resembles Crella  incrustans and the (b) simulation box for fluid and nutrients, we are able to (c) simulate sponge growth using the skeletal  architecture to guide the growth pattern. This results in a (d) 3D mesh of Crella incrustans.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    },
                    {
                        "slot_id": "v-short-1163",
                        "session_id": "short6",
                        "title": "How \"Applied\" is Fifteen Years of VAST conference?",
                        "contributors": [
                            "Lei Shi"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "v-short-1163",
                        "time_stamp": "2023-10-25T05:21:00Z",
                        "time_start": "2023-10-25T05:21:00Z",
                        "time_end": "2023-10-25T05:30:00Z",
                        "paper_type": "short",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "1",
                        "paper_award": "",
                        "image_caption": "The dynamics of the number of VAST (non-)application papers and their penetration rates.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    },
                    {
                        "slot_id": "v-short-1165",
                        "session_id": "short6",
                        "title": "CLEVER: A Framework for Connecting Lived Experiences with Visualisation of Electronic Records",
                        "contributors": [
                            "Mai Elshehaly"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "v-short-1165",
                        "time_stamp": "2023-10-25T05:30:00Z",
                        "time_start": "2023-10-25T05:30:00Z",
                        "time_end": "2023-10-25T05:39:00Z",
                        "paper_type": "short",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "1",
                        "paper_award": "",
                        "image_caption": "Initial framework for Connecting Lived Experiences with Visualisation of Electronic Records (CLEVER), in Population Health Management (PHM). This initial framework is based on a subjectivist case study approach. The rows represent contexts of soft intelligence that support decision making, with increasing levels of agency and knowledge centralisation (bottom to top). The columns capture examples of soft intelligence that can emerge in these contexts at different stages of formulating decisions (left to right). Deep dives are required in each of the individual contexts to iterate the framework and inform design guidelines.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    },
                    {
                        "slot_id": "v-short-1158",
                        "session_id": "short6",
                        "title": "Design of an Ecological Visual Analytics Interface for Operators of Time-Constant Processes",
                        "contributors": [
                            "Elmira Zohrevandi"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "v-short-1158",
                        "time_stamp": "2023-10-25T05:39:00Z",
                        "time_start": "2023-10-25T05:39:00Z",
                        "time_end": "2023-10-25T05:48:00Z",
                        "paper_type": "short",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "1",
                        "paper_award": "",
                        "image_caption": "On the proposed focus+context linked-view interface, the topological view enables interaction with an ML classifier which visualizes the historical data that are most similar to the current situation. The circle chart provides insight on model accuracy (mapped by color intensity) and performance (mapped on the radial axis). The situation view (i.e. the focus view) shows the key performance parameter profile through process cycle and its deviations from optimality.  The strategy view shows variation limits for relevant parameters obtained from the historical data. The control view shows the consequences of an adjustment predicted by the ML model for the selected parameter.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    }
                ]
            }
        ]
    },
    "v-cga": {
        "event": "CG&A Invited Partnership Presentations",
        "long_name": "CG&A Invited Partnership Presentations",
        "event_type": "Invited Partnership Presentations",
        "event_prefix": "v-cga",
        "event_description": "",
        "event_url": "",
        "organizers": [],
        "sessions": [
            {
                "title": "Systems, Techniques, and Applications",
                "session_id": "cga1",
                "event_prefix": "v-cga",
                "track": "104(132)",
                "session_image": "cga1.png",
                "chair": [
                    "Alexander Lex"
                ],
                "organizers": [],
                "time_start": "2023-10-22T00:00:00Z",
                "time_end": "2023-10-25T01:00:00Z",
                "discord_category": "",
                "discord_channel": "",
                "discord_channel_id": "",
                "discord_link": "",
                "zoom_meeting": "",
                "zoom_password": "",
                "zoom_link": "",
                "time_slots": [
                    {
                        "slot_id": "v-cga-9769955",
                        "session_id": "cga1",
                        "title": "Giga Graph Cities: Their Buckets, Buildings, Waves, and Fragments",
                        "contributors": [
                            "James Abello"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "v-cga-9769955",
                        "time_stamp": "2023-10-24T23:45:00Z",
                        "time_start": "2023-10-24T23:45:00Z",
                        "time_end": "2023-10-24T23:57:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "1",
                        "paper_award": "",
                        "image_caption": "Graph Cities allow visual exploration of billion-edge graphs. Challenges are the IO and the screen bottlenecks. Graphs get decomposed into edge layers called fixed points. This allows the disentangling of hairballs. Each fixed point is represented as a building, and all buildings are layout as a Graph City. The current implementation is scaled up to 1.8 billion edges. The rendering time is about 12 seconds. Please check out our paper Giga Graph Cities.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    },
                    {
                        "slot_id": "v-cga-9787979",
                        "session_id": "cga1",
                        "title": "Narrative In Situ Visual Analysis for Large-Scale Ocean Eddy Evolution",
                        "contributors": [
                            "Guihua Shan"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "v-cga-9787979",
                        "time_stamp": "2023-10-24T23:57:00Z",
                        "time_start": "2023-10-24T23:57:00Z",
                        "time_end": "2023-10-25T00:09:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "1",
                        "paper_award": "",
                        "image_caption": "The global view of our two views, which is used for high-precision streamline interaction and regional eddy characteristics analysis of large ocean areas.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    },
                    {
                        "slot_id": "v-cga-9830790",
                        "session_id": "cga1",
                        "title": "Technology Trends and Challenges for Large-Scale Scientific Visualization",
                        "contributors": [
                            "James Ahrens"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "v-cga-9830790",
                        "time_stamp": "2023-10-25T00:09:00Z",
                        "time_start": "2023-10-25T00:09:00Z",
                        "time_end": "2023-10-25T00:21:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "1",
                        "paper_award": "",
                        "image_caption": "A representation of the levels in the modern scientific process. A challenge for the visualization community is to explore the best visualization approaches for each of these levels as well as to be able to track, connect, and visualize relationships between levels. The levels increase in abstraction and complexity from the lowest level (data element) to the highest level (validation level).",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    },
                    {
                        "slot_id": "v-cga-9861728",
                        "session_id": "cga1",
                        "title": "SUBPLEX: A Visual Analytics Approach to Understand Local Model Explanations at the Subpopulation Level",
                        "contributors": [
                            "Jun Yuan"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "v-cga-9861728",
                        "time_stamp": "2023-10-25T00:21:00Z",
                        "time_start": "2023-10-25T00:21:00Z",
                        "time_end": "2023-10-25T00:33:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "1",
                        "paper_award": "",
                        "image_caption": "SUBPLEX is a visual analytics tool in Jupyter notebook to assist data scientist to understand local model explanations at the subpopulation level. SUBPLEX contains five linked views: (a) code block, (b) cluster refinement view, (c) projection view, (d) subpopulation creation panel, (e) local explanation detail view.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    },
                    {
                        "slot_id": "v-cga-9992069",
                        "session_id": "cga1",
                        "title": "A Multiscale Geospatial Dataset and an Interactive Visualization Dashboard for Computational Epidemiology and Open Scientific Research",
                        "contributors": [
                            "Muhammad Usman"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "v-cga-9992069",
                        "time_stamp": "2023-10-25T00:33:00Z",
                        "time_start": "2023-10-25T00:33:00Z",
                        "time_end": "2023-10-25T00:45:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "0",
                        "paper_award": "",
                        "image_caption": "A Multi-Scale Geospatial Dataset and An Interactive Visualization Dashboard for Computational Epidemiology and Open Scientific Research",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    }
                ]
            },
            {
                "title": "Theory and Evaluation plus ISMAR/VR",
                "session_id": "cga2",
                "event_prefix": "v-cga",
                "track": "103(132)",
                "session_image": "cga2.png",
                "chair": [
                    "Jian Chen"
                ],
                "organizers": [],
                "time_start": "2023-10-22T00:00:00Z",
                "time_end": "2023-10-26T04:15:00Z",
                "discord_category": "",
                "discord_channel": "",
                "discord_channel_id": "",
                "discord_link": "",
                "zoom_meeting": "",
                "zoom_password": "",
                "zoom_link": "",
                "time_slots": [
                    {
                        "slot_id": "v-cga-9830795",
                        "session_id": "cga2",
                        "title": "VisVisual: A Toolkit for Teaching and Learning Data Visualization",
                        "contributors": [
                            "Chaoli Wang"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "v-cga-9830795",
                        "time_stamp": "2023-10-26T03:00:00Z",
                        "time_start": "2023-10-26T03:00:00Z",
                        "time_end": "2023-10-26T03:12:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "1",
                        "paper_award": "",
                        "image_caption": "The VisVisual pedagogical toolkit (https://sites.nd.edu/chaoli-wang/visvisual) for teaching and learning essential visualization concepts, algorithms, and techniques. The toolkit was developed by Dr. Chaoli Wang's research team over ten years. It consists of four components: VolumeVisual, FlowVisual (including desktop and app versions), GraphVisual, and TreeVisual.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    },
                    {
                        "slot_id": "v-cga-9984051",
                        "session_id": "cga2",
                        "title": "Embracing Disciplinary Diversity in Visualization",
                        "contributors": [
                            "Sheelagh Carpendale"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "v-cga-9984051",
                        "time_stamp": "2023-10-26T03:12:00Z",
                        "time_start": "2023-10-26T03:12:00Z",
                        "time_end": "2023-10-26T03:24:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "1",
                        "paper_award": "",
                        "image_caption": "We outline six unique approaches to visualization research that represent a small sample of many that are possible. Different approaches produce different contribution types that can aid in enriching and strengthening the foundation of visualization research.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    },
                    {
                        "slot_id": "v-cga-9708430",
                        "session_id": "cga2",
                        "title": "Lessons Learned From Quantitatively Exploring Visualization Rubric Utilization for Peer Feedback",
                        "contributors": [
                            "Sophie J. Engle"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "v-cga-9708430",
                        "time_stamp": "2023-10-26T03:24:00Z",
                        "time_start": "2023-10-26T03:24:00Z",
                        "time_end": "2023-10-26T03:36:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "1",
                        "paper_award": "",
                        "image_caption": "Sentiment (-1 to 1) of written feedback by numeric rating (1 to 5) for rubric responses",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    },
                    {
                        "slot_id": "v-cga-9656613",
                        "session_id": "cga2",
                        "title": "Finding Their Data Voice: Practices and Challenges of Dashboard Users",
                        "contributors": [
                            "Melanie Tory"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "v-cga-9656613",
                        "time_stamp": "2023-10-26T03:36:00Z",
                        "time_start": "2023-10-26T03:36:00Z",
                        "time_end": "2023-10-26T03:48:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "1",
                        "paper_award": "",
                        "image_caption": "Dashboards are many people's portal to data, but only the starting point of their data work. To share with others, dashboard users frequently dump data and screenshots out of dashboards and curate them into new artifacts.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    },
                    {
                        "slot_id": "v-ismar-9995479",
                        "session_id": "cga2",
                        "title": "Augmented Scale Models: Presenting Multivariate Data Around Physical Scale Models in Augmented Reality",
                        "contributors": [
                            "Kadek Ananta Satriadi"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "v-ismar-9995479",
                        "time_stamp": "2023-10-26T03:48:00Z",
                        "time_start": "2023-10-26T03:48:00Z",
                        "time_end": "2023-10-26T04:00:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "1",
                        "paper_award": "",
                        "image_caption": "Illustration of one of the six augmented scale model techniques we evaluated in our study. This example depicts the technique with Dashboard Layout combined with \"On Scale Model\" View Arrangement.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    },
                    {
                        "slot_id": "v-vr-10108427",
                        "session_id": "cga2",
                        "title": "Towards an Understanding of Distributed Asymmetric Collaborative Visualization on Problem-solving",
                        "contributors": [
                            "Wai Tong"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "v-vr-10108427",
                        "time_stamp": "2023-10-26T04:00:00Z",
                        "time_start": "2023-10-26T04:00:00Z",
                        "time_end": "2023-10-26T04:12:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "1",
                        "paper_award": "",
                        "image_caption": "We studied the trade-offs of collaborative visualization for problem-solving in an asymmetric environment. This figure shows how two collaborators perceive and interact with visualizations using two different devices: VR (left) and PC (right). Visualizations are in different dimensions to adapt to different devices (i.e., 3D in VR and 2D on PC) and can be blended together (as envisaged in the center) with tailored techniques to support collaboration awareness.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    }
                ]
            }
        ]
    },
    "v-siggraph": {
        "event": "SIGGRAPH Invited Partnership Presentations",
        "long_name": "SIGGRAPH Invited Partnership Presentations",
        "event_type": "Invited Partnership Presentations",
        "event_prefix": "v-siggraph",
        "event_description": "",
        "event_url": "",
        "organizers": [],
        "sessions": []
    },
    "v-vr": {
        "event": "VR Invited Partnership Presentations",
        "long_name": "VR Invited Partnership Presentations",
        "event_type": "Invited Partnership Presentations",
        "event_prefix": "v-vr",
        "event_description": "",
        "event_url": "",
        "organizers": [],
        "sessions": []
    },
    "v-panels": {
        "event": "VIS Panels",
        "long_name": "VIS Panels",
        "event_type": "Panel",
        "event_prefix": "v-panels",
        "event_description": "",
        "event_url": "",
        "organizers": [],
        "sessions": [
            {
                "title": "Establishing and Thriving in an Academic Career",
                "session_id": "panel1",
                "event_prefix": "v-panels",
                "track": "101-102(140)",
                "session_image": "panel1.png",
                "chair": [],
                "organizers": [],
                "time_start": "2023-10-22T00:00:00Z",
                "time_end": "2023-10-25T06:00:00Z",
                "discord_category": "",
                "discord_channel": "",
                "discord_channel_id": "",
                "discord_link": "",
                "zoom_meeting": "",
                "zoom_password": "",
                "zoom_link": "",
                "time_slots": []
            },
            {
                "title": "What is a Visual Analytics contribution, and how is it changing?",
                "session_id": "panel2",
                "event_prefix": "v-panels",
                "track": "101-102(140)",
                "session_image": "panel2.png",
                "chair": [],
                "organizers": [],
                "time_start": "2023-10-22T00:00:00Z",
                "time_end": "2023-10-25T01:00:00Z",
                "discord_category": "",
                "discord_channel": "",
                "discord_channel_id": "",
                "discord_link": "",
                "zoom_meeting": "",
                "zoom_password": "",
                "zoom_link": "",
                "time_slots": []
            },
            {
                "title": "How should VIS4ML Redefine Itself in the Rapid Evolution of AI?",
                "session_id": "panel3",
                "event_prefix": "v-panels",
                "track": "101-102(140)",
                "session_image": "panel3.png",
                "chair": [],
                "organizers": [],
                "time_start": "2023-10-22T00:00:00Z",
                "time_end": "2023-10-26T06:00:00Z",
                "discord_category": "",
                "discord_channel": "",
                "discord_channel_id": "",
                "discord_link": "",
                "zoom_meeting": "",
                "zoom_password": "",
                "zoom_link": "",
                "time_slots": []
            }
        ]
    },
    "a-visap": {
        "event": "VIS Arts Program",
        "long_name": "VIS Arts Program",
        "event_type": "",
        "event_prefix": "a-visap",
        "event_description": "",
        "event_url": "",
        "organizers": [],
        "sessions": [
            {
                "title": "VISAP Session 1",
                "session_id": "visap1",
                "event_prefix": "a-visap",
                "track": "103(132)",
                "session_image": "visap1.png",
                "chair": [],
                "organizers": [],
                "time_start": "2023-10-22T00:00:00Z",
                "time_end": "2023-10-25T01:00:00Z",
                "discord_category": "",
                "discord_channel": "",
                "discord_channel_id": "",
                "discord_link": "",
                "zoom_meeting": "",
                "zoom_password": "",
                "zoom_link": "",
                "time_slots": [
                    {
                        "slot_id": "a-visap-1062",
                        "session_id": "visap1",
                        "title": "The Heart",
                        "contributors": [
                            "Robert Walton"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "a-visap-1062",
                        "time_stamp": "2023-10-24T23:45:00Z",
                        "time_start": "2023-10-24T23:45:00Z",
                        "time_end": "2023-10-25T01:00:00Z",
                        "paper_type": "associated",
                        "keywords": [],
                        "has_image": "0",
                        "has_video": "0",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    },
                    {
                        "slot_id": "a-visap-1070",
                        "session_id": "visap1",
                        "title": "Parasitic Signals: Coexistence with the SARS-CoV-2 virus",
                        "contributors": [
                            "Myungin Lee"
                        ],
                        "authors": [],
                        "abstract": "This project aims to transform the nano-scale of a striking biological phenomenon, the relationship between the SARS-CoV-2 virus and human molecules, into an interactive audiovisual simulation. In this work, Atomic Force Microscopy (AFM) touching and imaging a single molecule measures the interaction between the spike protein of SARS-CoV-2 and human cellular proteins and measures the dynamic of the spike protein. We create a comprehensive scientific model based on diverse datasets and theories presenting a real-time interactive complex system with efficient rendering and sonification using a single C++ platform. This project invites the audience into an immersive space where they can control the behavior of biomolecules, allowing them to intuitively perceive biological properties. This project is not only a demonstration of scientific data but also attempts to look at the interspecies relationship in parasitism which particularly deals with our current and post-pandemic life with coronavirus and how we might control our coexistence in a virtual space. By envisioning coexistence strategies in a virtual space, the project challenges dominant narratives, explores alternative ways to navigate the pandemic's complex dimensions, and serves as a catalyst for dialogue among a diverse audience.",
                        "uid": "a-visap-1070",
                        "time_stamp": "2023-10-24T23:45:00Z",
                        "time_start": "2023-10-24T23:45:00Z",
                        "time_end": "2023-10-25T01:00:00Z",
                        "paper_type": "associated",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "1",
                        "paper_award": "",
                        "image_caption": "Installation of Parasitic Signals: Multimodal Sonata for Real-time Interactive Simulation of the SARS-CoV-2 Virus",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    },
                    {
                        "slot_id": "a-visap-1053",
                        "session_id": "visap1",
                        "title": "Body Cosmos",
                        "contributors": [
                            "Rem RunGu Lin"
                        ],
                        "authors": [],
                        "abstract": "Body Cosmos is an immersive and interactive artwork that connects the human body with the cosmic environment through real-time bio-data. Using volumetric rendering and particle system, we create a surreal virtual reality that reflects the intricate structures of human anatomy and celestial nebulae. We integrate heart rate monitors and EEG devices to capture and process bio-data into emotion indicators, which influence the visualization of particle movements in the artwork. This creates an intimate, personal connection with the cosmos, transcending immediate presence and nurturing a perpetual presence within the cosmic expanse. We also provide two engagement modes: Roller Coaster Mode and Free Explore Mode, allowing different levels of exploration. Body Cosmos is more than a visual spectacle; it sparks curiosity, expands the imagination, and enhances awareness of our embodied and cosmic identity.",
                        "uid": "a-visap-1053",
                        "time_stamp": "2023-10-24T23:45:00Z",
                        "time_start": "2023-10-24T23:45:00Z",
                        "time_end": "2023-10-25T01:00:00Z",
                        "paper_type": "associated",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "1",
                        "paper_award": "",
                        "image_caption": "Screenshot of Unreal Engine\u2019s real-time demo (\u00a9 Rungu Lin 2023)",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    },
                    {
                        "slot_id": "a-visap-1026",
                        "session_id": "visap1",
                        "title": "Spotlight",
                        "contributors": [
                            "Charles Perin"
                        ],
                        "authors": [],
                        "abstract": "Spotlight is an interactive book on the topic of government-imposed internet shutdowns. This data visualization project explores the effects of internet shutdowns and some of the common reasons they occur, as well as their relation to sensitive events such as protests and elections. The book is made interactive through the use of glow-in-the-dark paint in multiple interactive activities and visualizations, which requires the viewer to use a UV flashlight provided to them on the first page of the book to reveal additional information. For the purposes of this exhibition, the 10 pages of the book are individually placed on a wall in a linear fashion, each with their own UV flashlight. The viewers are able to interact with each page by going through a tunnel-shaped space created by placing a dark curtain from the ceiling in front of the wall. The book itself is placed outside, at the entrance of this tunnel, for display. The overall experience aims to replicate living through an internet shutdown by placing readers of the interactive book in a low-light environment and providing them with a flashlight in order to shine light on information and data that does not easily reach many people due to a lack of internet access.",
                        "uid": "a-visap-1026",
                        "time_stamp": "2023-10-24T23:45:00Z",
                        "time_start": "2023-10-24T23:45:00Z",
                        "time_end": "2023-10-25T01:00:00Z",
                        "paper_type": "associated",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "1",
                        "paper_award": "",
                        "image_caption": "Spotlight in the book format",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    },
                    {
                        "slot_id": "a-visap-1010",
                        "session_id": "visap1",
                        "title": "Waiting for the Wave in Metaverse",
                        "contributors": [
                            "Midori Yamazaki"
                        ],
                        "authors": [],
                        "abstract": "The artwork visualises the momentary shape of a wave in perfect shape, ideal for surfing, which retains its aesthetics forever, by mixing reality and virtual reality. By presenting it, the flexible cognitive abilities of the audience who sees it continue to work to actively select beautiful experiences and create a better future. The work reaffirms the supple strength of human cognitive abilities and expresses a sense of human existence that will remain unchanged forever, even in a future where reality is in chaos.",
                        "uid": "a-visap-1010",
                        "time_stamp": "2023-10-24T23:45:00Z",
                        "time_start": "2023-10-24T23:45:00Z",
                        "time_end": "2023-10-25T01:00:00Z",
                        "paper_type": "associated",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "1",
                        "paper_award": "",
                        "image_caption": "The artwork visualises the momentary shape of a wave in perfect shape, ideal for surfing, which retains its aesthetics forever, by mixing reality and virtual reality. By presenting it, the flexible cognitive abilities of the audience who sees it continue to work to actively select beautiful experiences and create a better future. The work reaffirms the supple strength of human cognitive abilities and expresses a sense of human existence that will remain unchanged forever, even in a future where reality is in chaos.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    },
                    {
                        "slot_id": "a-visap-1034",
                        "session_id": "visap1",
                        "title": "Infinite Colours",
                        "contributors": [
                            "Dr Xavier Ho"
                        ],
                        "authors": [],
                        "abstract": "This generative work draws data from 2,499 queer independent games for about 12 seconds each. Each game adds a unique shape and colour onto the canvas, and plays a unique string of notes. Over 8 hours, the canvas will be filled with infinite colours to celebrate LGBTQIA+ independent videogames. History has always been queer. Through this generative visual and sound work, we aim to demonstrate the collective activism, movement, and creative expressions that queer folks are making to be visible, heard, and to say that we are here.  But queer movement does not happen over night; queer resistance is accumulative and built over generations of self-sacrifice and self-acceptance. The multitude intersectionality of the unruly times slowly bleeds colour into the world, blends motion into the landscape, and accumulatively becomes a canvas of ever-moving colourful light.",
                        "uid": "a-visap-1034",
                        "time_stamp": "2023-10-24T23:45:00Z",
                        "time_start": "2023-10-24T23:45:00Z",
                        "time_end": "2023-10-25T01:00:00Z",
                        "paper_type": "associated",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "1",
                        "paper_award": "",
                        "image_caption": "Infnite Colours This generative work draws data from 2,499 queer independent  games for about 12 seconds each. Each game adds a unique shape and colour onto  the canvas, and plays a unique string of notes. Over 8 hours, the canvas will be  filled with infinite colours to celebrate LGBTQIA+ independent videogames.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    },
                    {
                        "slot_id": "a-visap-1074",
                        "session_id": "visap1",
                        "title": "Posts with No Response: The Island of Loneliness",
                        "contributors": [
                            "Junxiu Tang"
                        ],
                        "authors": [],
                        "abstract": "Loneliness and isolation are eternal emotions in human beings. Technological advancements create ample avenues, like social medias, for individuals to articulate themselves and record emotions. However, the sense of loneliness has never vanished, as their expressions are easily buried in the digital stream. We analyze tweets that express loneliness during holiday seasons but receive few responses. By superimposing digital charts on physical models, we visualize these lonely posts and generate the island of loneliness. We aim to reveal the complexities of human emotions in the digital age and reflect on the interconnections between technology, solitude, and social communication.",
                        "uid": "a-visap-1074",
                        "time_stamp": "2023-10-24T23:45:00Z",
                        "time_start": "2023-10-24T23:45:00Z",
                        "time_end": "2023-10-25T01:00:00Z",
                        "paper_type": "associated",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "1",
                        "paper_award": "",
                        "image_caption": "In today's interconnected world, everything is connected by the internet, but people still feel lonely. Through a data-driven approach, we physicalize people's perpetual loneliness into an island metaphor and try to reveal the overlooked voices within the internet data flow.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    },
                    {
                        "slot_id": "a-visap-1018",
                        "session_id": "visap1",
                        "title": "Solar System",
                        "contributors": [
                            "Hyemi Song"
                        ],
                        "authors": [],
                        "abstract": "Solar System is an audio-visual live performance that marries data visualization and sonification. The installation system employs sidereal period data from the eight planets in our Solar System to generate a live soundtrack and visualization. The mission of this endeavor is to use audible and visual media to allow humans to uncover and cognize with the always surrounding, yet invisible, Solar System. Each planet's unique data patterns (sidereal periods) contribute to this exploration by being translated with audible and visible media. The media stimulate audiences' cognitive senses, enabling audiences to tangibly experience the aesthetic wonder of the cosmic world, which is deeply interconnected with the entirety of the Universe and humanity on Earth. The inspiration and message of the Solar System project resonate with this year's theme, Perpetual Presence. Throughout history, data from the Universe has been employed to uncover the existence of unseen worlds, the Cosmos. The discoveries gleaned from the process have influenced numerous sectors of our humanity, communicated through a multitude of languages and approaches. Scientists use numerical data and textual explanations to communicate information about the Cosmos. On the other hand, artists use sensory media, such as visuals and sounds, to narrate the story of the Cosmos. This art project aims to manifest the continuous and timeless presence of the Universe where it surrounds humanity, employing artistic transformation of universe data to illuminate its enduring existence.",
                        "uid": "a-visap-1018",
                        "time_stamp": "2023-10-24T23:45:00Z",
                        "time_start": "2023-10-24T23:45:00Z",
                        "time_end": "2023-10-25T01:00:00Z",
                        "paper_type": "associated",
                        "keywords": [],
                        "has_image": "0",
                        "has_video": "0",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    },
                    {
                        "slot_id": "a-visap-1081",
                        "session_id": "visap1",
                        "title": "Eco-Mending",
                        "contributors": [
                            "Racquel Fygenson"
                        ],
                        "authors": [],
                        "abstract": "Eco-Mending juxtaposes old and new to tell a story of ecological regeneration. We find stories of human-mediated restoration in longitudinal data, and stitch modern data into imagery from the past, creating decorative wall and ceiling hangings. With perpetuality of data, comes the possibility of changing it. In this collection of art pieces, we pair past ecological destruction with present and future projections of data that highlight successful ecological reconstructions",
                        "uid": "a-visap-1081",
                        "time_stamp": "2023-10-24T23:45:00Z",
                        "time_start": "2023-10-24T23:45:00Z",
                        "time_end": "2023-10-25T01:00:00Z",
                        "paper_type": "associated",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "1",
                        "paper_award": "",
                        "image_caption": "Left: A close-up from data about soil remediation in Nigeria. Right: A photograph of the hanging sculpture recording data about the ozone layer.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    },
                    {
                        "slot_id": "a-visap-1083",
                        "session_id": "visap1",
                        "title": "Latent Prism",
                        "contributors": [
                            "Jane L. Adams"
                        ],
                        "authors": [],
                        "abstract": "Latent Prism is a visually captivating and thought-provoking piece that incorporates artificial intelligence (AI) and data visualization. It presents a projection of an imagined environment, created through a generative adversarial network (GAN) trained on thousands of aerial photographs from royalty-free stock photo websites. The sculpture takes the form of a polished transparent lucite prism, within which layers of translucent mylar film are suspended. These films display frames extracted from an AI-generated video known as a \u201clatent walk,\u201d showcasing undulating ocean and forest landscapes captured from an aerial perspective. Frames are selected at regular intervals along the linear interpolation of images from the generative model, such that light from below is still able to permeate up to the viewer. The aggregated effect of these layered video frames results in an eerie visual sensation of peering down at a forested landscape that is being submerged in water. Surrounding the prism is a haphazardly piled 120ft. (36.5m)-long roll of credits, listing the names and photographers for every image used to train the model on a 2.5in (6.35cm) wide strip of drafting paper.",
                        "uid": "a-visap-1083",
                        "time_stamp": "2023-10-24T23:45:00Z",
                        "time_start": "2023-10-24T23:45:00Z",
                        "time_end": "2023-10-25T01:00:00Z",
                        "paper_type": "associated",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "1",
                        "paper_award": "",
                        "image_caption": "The latent prism is a lucite and mylar sculpture layering samples from an aerial photograph generative machine learning model. The sculpture is a rectangular stack of transparent blocks containing translucent layers of ocean and forest textures in undulating, indeterminate patterns. Coiled beneath the sculpture are 17,000 credits for each image used to train the model, printed onto a long scroll of architectural drafting paper.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    },
                    {
                        "slot_id": "a-visap-1040",
                        "session_id": "visap1",
                        "title": "The Vast Territory",
                        "contributors": [
                            "Ignacio P\u00e9rez-Messina"
                        ],
                        "authors": [],
                        "abstract": "In the novel The Vast Territory (El vasto territorio. Alfaguara, 2021; Caja Negra, 2023), by Chilean author Sim\u00f3n L\u00f3pez Trujillo, a mycologist analyzes the way a certain fungus infects the mind of a forest worker called Pedro. In one moment, an abstract image, of countless white dots against a black background, in the form of waves or a mountain range, appears to explain the infection: the genetic origin of the language of Pedro, when the fungus starts speaking through him.  That image, included in the revised edition of the book, is also a depiction of the novel\u2019s genetic origin, as it was generated by a visualization of different drafts of the novel. In every literary reading, two texts are involved: the written or actual text, that we can smoothly read with our eyes, and a second text, imperceptible and invisible, made of all the deletions, editions, and additions of words involved in the process of writing the text. This project conceives the visualization of The Vast Territory as a visual novel on its own that explores the unconscious of the book: that black, secret space, where the words involved during the writing process emerge as ghostly presences. There, the data of the previous draft are manifested in the following one, as a latent presence in the actuality of the text we read. The work, where every white dot represents a word in the juxtaposed draft sequence, is a two-dimensional pixel-based text visualization. Its construction follows only two straightforward rules: (1) words are sequentially arranged in a horizontal line; (2) the vertical position of each word-dot is determined by its initial appearance within the entire sequence of drafts. The result is a \u201cdata-palimpsest\u201d where each draft leaves its imprint on the next through their cumulative determination of the spatial order. By using the order of first appearance as the guiding principle, the visualization emphasizes the inherited structure of each draft from its predecessors, akin to looking at the fossil record or geological strata, with the most ancient elements appearing at the greatest depth. The layer on top, where the words are shown, allows the viewer to inspect the content at each point in time, which is indicated by a thin vertical line or by the \u201cfolding\u201d of the canvas. It acts as a zoom-in, where the overlaid words are placed at their precise height in the visualization, or first-appearance depth.",
                        "uid": "a-visap-1040",
                        "time_stamp": "2023-10-24T23:45:00Z",
                        "time_start": "2023-10-24T23:45:00Z",
                        "time_end": "2023-10-25T01:00:00Z",
                        "paper_type": "associated",
                        "keywords": [],
                        "has_image": "0",
                        "has_video": "0",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    },
                    {
                        "slot_id": "a-visap-1086",
                        "session_id": "visap1",
                        "title": "Mixtures of Human Experience, Intellectual Analysis, Data Representation and Our Natural Environment",
                        "contributors": [
                            "Francesca Samsel"
                        ],
                        "authors": [],
                        "abstract": "The work presented here are artistic endeavors tightly integrating topics of current climate research, contextual imagery referring to that research, data visualizations of a coupled climate model, and drawings weaving the art and the science into one, all in the service of the accelerating levels of carbon and methane altering and amplifying weather patterns, slowing oceanic currents, and fundamentally altering the habitability of our planet.",
                        "uid": "a-visap-1086",
                        "time_stamp": "2023-10-24T23:45:00Z",
                        "time_start": "2023-10-24T23:45:00Z",
                        "time_end": "2023-10-25T01:00:00Z",
                        "paper_type": "associated",
                        "keywords": [],
                        "has_image": "0",
                        "has_video": "0",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    },
                    {
                        "slot_id": "a-visap-1060",
                        "session_id": "visap1",
                        "title": "Monster in a Snow Globe. Biographies as Data Physicalizations",
                        "contributors": [
                            "Florian Windhager"
                        ],
                        "authors": [],
                        "abstract": "The data sculpture \"Monster in a Snowglobe\" offers a distant reading perspective on the life and work of the Austrian painter Herwig Zens (1943-2019). The 3D-printed artifact assembles major steps and movements of the artist's biography as an annotated trajectory in space and time. Its data selection draws both from biographical knowledge and from autobiographical entries in a unique diary, which the artist etched into a sequence of copper plates during his lifetime. A complete print of the diary\u2014also referred to as \u201cmonster\u201d due to its procedural and perceptual complexity\u2014is considered the longest etching in the world at 40 meters and was partially presented at the Museum of Art History in Vienna in Spring 2023. In this context, the data sculpture reframed the ephemeral complexity of a modern path of life as a material mimicry and instance of perpetual presence in an object-oriented exhibition environment.",
                        "uid": "a-visap-1060",
                        "time_stamp": "2023-10-24T23:45:00Z",
                        "time_start": "2023-10-24T23:45:00Z",
                        "time_end": "2023-10-25T01:00:00Z",
                        "paper_type": "associated",
                        "keywords": [],
                        "has_image": "0",
                        "has_video": "0",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    },
                    {
                        "slot_id": "a-visap-1056",
                        "session_id": "visap1",
                        "title": "Plastic Landscape - The Reversible World",
                        "contributors": [
                            "Yoon Chung Han"
                        ],
                        "authors": [],
                        "abstract": "\u201cPlastic Landscape - The Reversible World\u201d is an AI-generated 3D animated video design that shows the apocalyptic and surreal world surrounded by artificial plastic mixtures and objects in the ocean, urban city, Antarctica, and forest. Four different scenes are animated, with the camera panning slowly from left to right. Viewers can observe how the plastics are decomposed at a slower speed by looking at particle animations. Sound is created by the data of the decomposition of plastics. Different types of plastics and speed of decomposition determine the frequency, amplitude, and parameters of audio synthesis. This scene animation is inspired by Ilwalobongbyeong (a folding screen) behind the king\u2019s throne of the Joseon Dynasty. This animation depicts the twist of the landscape. Surreal objects/buildings in this animation made out of plastic look beautiful and mesmerizing at first glance. However, the viewers can notice that they are the decayed objects and destroyed nature impacted by human beings. This new multi-sensory artwork addresses the awareness of plastic pollution through the apocalyptic lens.",
                        "uid": "a-visap-1056",
                        "time_stamp": "2023-10-24T23:45:00Z",
                        "time_start": "2023-10-24T23:45:00Z",
                        "time_end": "2023-10-25T01:00:00Z",
                        "paper_type": "associated",
                        "keywords": [],
                        "has_image": "0",
                        "has_video": "0",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    },
                    {
                        "slot_id": "a-visap-1069",
                        "session_id": "visap1",
                        "title": "Parasitic signals: Multimodal Sonata for Real-time Interactive Simulation of the SARS-CoV-2 Virus",
                        "contributors": [
                            "Myungin Lee"
                        ],
                        "authors": [],
                        "abstract": "This project aims to transform the nano-scale of a striking biological phenomenon, the relationship between the SARS-CoV-2 virus and human molecules, into an interactive audiovisual simulation. In this work, Atomic Force Microscopy (AFM) touching and imaging a single molecule measures the interaction between the spike protein of SARS-CoV-2 and human cellular proteins and measures the dynamic of the spike protein. We create a comprehensive scientific model based on diverse datasets and theories presenting a real-time interactive complex system with efficient rendering and sonification using a single C++ platform. This project invites the audience into an immersive space where they can control the behavior of biomolecules, allowing them to intuitively perceive biological properties. This project is not only a demonstration of scientific data but also attempts to look at the interspecies relationship in parasitism which particularly deals with our current and post-pandemic life with coronavirus and how we might control our coexistence in a virtual space.",
                        "uid": "a-visap-1069",
                        "time_stamp": "2023-10-24T23:45:00Z",
                        "time_start": "2023-10-24T23:45:00Z",
                        "time_end": "2023-10-25T01:00:00Z",
                        "paper_type": "associated",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "1",
                        "paper_award": "",
                        "image_caption": "Installation of Parasitic Signals: Coexistence with the SARS-CoV-2 virus",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    },
                    {
                        "slot_id": "a-visap-1066",
                        "session_id": "visap1",
                        "title": "Bitter Data: An Exploration into Data Edibilization of Negative Emotion",
                        "contributors": [
                            "Yufan Li"
                        ],
                        "authors": [],
                        "abstract": "\u201cBitter Data\u201d transforms 100,000 distress postings from Chinese social media into a multi-sensory experience using data edibilization. We\u2019ve mapped distress data quantity to the bitterness and color of tea through data analysis and experimentation. Participants taste, smell, and observe 11 cups of tea, each embodying a year\u2019s distress data, in our workshop. Their facial expressions, recorded upon tasting, visually indicate emotional states. This project explores benefits and pragmatic solutions to challenges of data edibilization.",
                        "uid": "a-visap-1066",
                        "time_stamp": "2023-10-24T23:45:00Z",
                        "time_start": "2023-10-24T23:45:00Z",
                        "time_end": "2023-10-25T01:00:00Z",
                        "paper_type": "associated",
                        "keywords": [],
                        "has_image": "0",
                        "has_video": "0",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    },
                    {
                        "slot_id": "a-visap-1029",
                        "session_id": "visap1",
                        "title": "Body Cosmos: An Immersive Experience Driven by Real-Time Bio-Data",
                        "contributors": [
                            "Rem RunGu Lin"
                        ],
                        "authors": [],
                        "abstract": "This paper presents \"Body Cosmos\", an artwork that creates a symbiotic relationship between the human body and a simulated cosmic environment through volumetric rendering and particle system. Drawing from DICOM data to simulate the human body and nebulae, we create an interactive and dynamic virtual environment. The real-time bio-data of users, collected via heart rate sensors and EEG devices, is integrated into the visualization, fostering a personal engagement and unity within this 'cosmos.' Body Cosmos provokes curiosity and expands users' imagination, and deepens their understanding of life's macrocosm and microcosm. This exploratory project redefines traditional perceptions of the human body in relation to the universe, creating a unique lens to view selfhood, embodiment, and identity. As we look to the future, the system's evolution will include incorporation of more bio-data sensors, an investigation into its potential psychological and physiological benefits, and the development of social interactive features through multi-user capabilities.",
                        "uid": "a-visap-1029",
                        "time_stamp": "2023-10-24T23:45:00Z",
                        "time_start": "2023-10-24T23:45:00Z",
                        "time_end": "2023-10-25T01:00:00Z",
                        "paper_type": "associated",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "1",
                        "paper_award": "",
                        "image_caption": "Screenshot of Unreal Engine\u2019s real-time demo (\u00a9 Rungu Lin 2023)",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    },
                    {
                        "slot_id": "a-visap-1032",
                        "session_id": "visap1",
                        "title": "Posts with No Response: The Island of Loneliness",
                        "contributors": [
                            "Junxiu Tang"
                        ],
                        "authors": [],
                        "abstract": "Loneliness and isolation are eternal emotions in human beings. Technological advancements create ample avenues, like social medias, for individuals to articulate themselves and record emotions. However, the sense of loneliness has never vanished, as their expressions are easily buried in the digital stream. We analyze tweets that express loneliness during holiday seasons but receive few responses. By superimposing digital charts on physical models, we visualize these lonely posts and generate the island of loneliness. We aim to reveal the complexities of human emotions in the digital age and reflect on the interconnections between technology, solitude, and social communication.",
                        "uid": "a-visap-1032",
                        "time_stamp": "2023-10-24T23:45:00Z",
                        "time_start": "2023-10-24T23:45:00Z",
                        "time_end": "2023-10-25T01:00:00Z",
                        "paper_type": "associated",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "1",
                        "paper_award": "",
                        "image_caption": "In today's interconnected world, everything is connected by the internet, but people still feel lonely. Through a data-driven approach, we physicalize people's perpetual loneliness into an island metaphor and try to reveal the overlooked voices within the internet data flow.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    },
                    {
                        "slot_id": "a-visap-1051",
                        "session_id": "visap1",
                        "title": "Reinterpreted Spaces, an AI Printmaking Collaboration",
                        "contributors": [
                            "Dr. Hannen E. Wolfe"
                        ],
                        "authors": [],
                        "abstract": "We present an examination of organic spaces through print, book making, data, and machine learning. Artists created a book that explored the idea of organic and machine-made interpretations of a place that were generate using 3 different processes: a generative adversarial network, traditional printmaking and a camera. The artists found the results unexpected, discussing how the AI-generated image changed and complicated their understanding and constructed narrative about the original image and space. This caused them to think outside the box with over half of the students changing their print matrix and/or ink choices after seeing their AI-generated image. This supported the learning objective for students to collaborate with technology that is uninhibited by perspective or expectation, adapting and responding productively and creatively within a new framework. With the newfound accessibility to AI-generated images we encourage art teachers to explore how it fits into their curriculum.",
                        "uid": "a-visap-1051",
                        "time_stamp": "2023-10-24T23:45:00Z",
                        "time_start": "2023-10-24T23:45:00Z",
                        "time_end": "2023-10-25T01:00:00Z",
                        "paper_type": "associated",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "0",
                        "paper_award": "",
                        "image_caption": "The flag book is a sculptural object that can be read page by page or viewed in a full spread. It explores the idea of organic and machine-made interpretations of a place that were generated using 3 different processes: a generative adversarial network, traditional printmaking and a camera. This image shows a flag book from 4 different perspectives, opened like a book with 2 pages, spread open to reveal all nine pages, the back of a flag book showing the accordion folded spine, and the flag book from above.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    },
                    {
                        "slot_id": "a-visap-1084",
                        "session_id": "visap1",
                        "title": "Associative Forms for Encoding Multivariate Climate Data",
                        "contributors": [
                            "Francesca Samsel"
                        ],
                        "authors": [],
                        "abstract": "We are perpetually present in our environment, experiencing it with our senses. Scientific data describes the same environment quantitatively. Our goal is to use scientific and artistic methods to combine these environmental expressions and personal experience through the creation of glyphs visually abstracted from and associated with forms in nature in the representation of climate data. The use of these glyphs removes the distinctions between scientific data and sensory experience, to allow a fuller intuitive association between the two, creating an embodied experience and increasing awareness of the climate effects and changes all around us.",
                        "uid": "a-visap-1084",
                        "time_stamp": "2023-10-24T23:45:00Z",
                        "time_start": "2023-10-24T23:45:00Z",
                        "time_end": "2023-10-25T01:00:00Z",
                        "paper_type": "associated",
                        "keywords": [],
                        "has_image": "0",
                        "has_video": "0",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    }
                ]
            },
            {
                "title": "VISAP Session 2",
                "session_id": "visap2",
                "event_prefix": "a-visap",
                "track": "103(132)",
                "session_image": "visap2.png",
                "chair": [],
                "organizers": [],
                "time_start": "2023-10-22T00:00:00Z",
                "time_end": "2023-10-26T01:00:00Z",
                "discord_category": "",
                "discord_channel": "",
                "discord_channel_id": "",
                "discord_link": "",
                "zoom_meeting": "",
                "zoom_password": "",
                "zoom_link": "",
                "time_slots": []
            }
        ]
    },
    "v-spotlights": {
        "event": "Application Spotlights",
        "long_name": "Application Spotlights",
        "event_type": "",
        "event_prefix": "v-spotlights",
        "event_description": "",
        "event_url": "",
        "organizers": [],
        "sessions": [
            {
                "title": "Visualization for spatial single-cell atlases",
                "session_id": "app1",
                "event_prefix": "v-spotlights",
                "track": "101-102(140)",
                "session_image": "app1.png",
                "chair": [],
                "organizers": [],
                "time_start": "2023-10-22T00:00:00Z",
                "time_end": "2023-10-26T01:00:00Z",
                "discord_category": "",
                "discord_channel": "",
                "discord_channel_id": "",
                "discord_link": "",
                "zoom_meeting": "",
                "zoom_password": "",
                "zoom_link": "",
                "time_slots": []
            }
        ]
    },
    "a-sciviscontest": {
        "event": "SciVis Contest",
        "long_name": "SciVis Contest",
        "event_type": "",
        "event_prefix": "a-sciviscontest",
        "event_description": "",
        "event_url": "",
        "organizers": [],
        "sessions": []
    },
    "s-vds": {
        "event": "VDS: Visualization in Data Science Symposium",
        "long_name": "VDS: Visualization in Data Science Symposium",
        "event_type": "",
        "event_prefix": "s-vds",
        "event_description": "",
        "event_url": "",
        "organizers": [
            "Alvitta Ottley",
            "Anamaria Crisan",
            "Michael Behrisch",
            "Jorge Ono",
            "Shayan Monadjemi"
        ],
        "sessions": [
            {
                "title": "VDS: Visualization in Data Science Symposium",
                "session_id": "ae3",
                "event_prefix": "s-vds",
                "track": "106(234)",
                "session_image": "ae3.png",
                "chair": [
                    "Alvitta Ottley",
                    "Anamaria Crisan",
                    "Michael Behrisch",
                    "Jorge Ono",
                    "Shayan Monadjemi"
                ],
                "organizers": [],
                "time_start": "2023-10-22T00:00:00Z",
                "time_end": "2023-10-23T05:15:00Z",
                "discord_category": "",
                "discord_channel": "",
                "discord_channel_id": "",
                "discord_link": "",
                "zoom_meeting": "",
                "zoom_password": "",
                "zoom_link": "",
                "time_slots": [
                    {
                        "slot_id": "s-vds-1002",
                        "session_id": "ae3",
                        "title": "HPCClusterScape: Increasing Transparency and Efficiency of Shared High-Performance Computing Clusters for Large-scale AI Models",
                        "contributors": [
                            "Heungseok Park"
                        ],
                        "authors": [],
                        "abstract": "The emergence of large-scale AI models, like GPT-4, has significantly impacted academia and industry, driving the demand for high-performance computing (HPC) to accelerate workloads. To address this, we present HPCClusterScape, a visualization system that enhances the efficiency and transparency of shared HPC clusters for large-scale AI models. HPCClusterScape provides a comprehensive overview of system-level (e.g., partitions, hosts, and workload status) and application-level (e.g., identification of experiments and researchers) information, allowing HPC operators and machine learning researchers to monitor resource utilization and identify issues through customizable violation rules. The system includes diagnostic tools to investigate workload imbalances and synchronization bottlenecks in large-scale distributed deep learning experiments. Deployed in industrial-scale HPC clusters, HPCClusterScape incorporates user feedback and meets specific requirements. This paper outlines the challenges and prerequisites for efficient HPC operation, introduces the interactive visualization system, and highlights its contributions in addressing pain points and optimizing resource utilization in shared HPC clusters.",
                        "uid": "s-vds-1002",
                        "time_stamp": "2023-10-23T04:00:00Z",
                        "time_start": "2023-10-23T04:00:00Z",
                        "time_end": "2023-10-23T05:15:00Z",
                        "paper_type": "",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "1",
                        "paper_award": "",
                        "image_caption": "HPCClusterScape offers real-time resource monitoring for HPC clusters. The (A) Cluster Overview displays GPU resources, allowing users to observe both the overall cluster and individual resources. Users can set (B) Violation Rules to track system metric statistics and detect anomalies over time. Clicking on specific workloads leads to the (C) Diagnostics View, enabling detailed node, GPU, and metric analysis for large-scale distributed training.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    },
                    {
                        "slot_id": "s-vds-1014",
                        "session_id": "ae3",
                        "title": "NeighViz: Towards Better Understanding of Neighborhood Effects on Social Groups with Spatial Data",
                        "contributors": [
                            "Yue Yu"
                        ],
                        "authors": [],
                        "abstract": "Understanding how local environments influence individual behaviors, such as voting patterns or suicidal tendencies, is crucial in social science to reveal and reduce spatial disparities and promote social well-being. With the increasing availability of large-scale individual-level census data, new analytical opportunities arise for social scientists to explore human behaviors (e.g., political engagement) among social groups at a fine-grained level. However, traditional statistical methods mostly focus on global, aggregated spatial correlations, which are limited to understanding and comparing the impact of local environments (e.g., neighborhoods) on human behaviors among social groups.  In this study, we introduce a new analytical framework for analyzing multi-variate neighborhood effects between social groups. We then propose NeighViz, an interactive visual analytics system that helps social scientists explore, understand, and verify the influence of neighborhood effects on human behaviors. Finally, we use a case study to illustrate the effectiveness and usability of our system.",
                        "uid": "s-vds-1014",
                        "time_stamp": "2023-10-23T04:00:00Z",
                        "time_start": "2023-10-23T04:00:00Z",
                        "time_end": "2023-10-23T05:15:00Z",
                        "paper_type": "",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "0",
                        "paper_award": "",
                        "image_caption": "The impact of local environments on individual behaviors, such as voting patterns and suicidal tendencies, is important in social science to address spatial disparities and promote social well-being. Traditional statistical methods offer limited understanding of this impact with only global, aggregated spatial correlations. To address this, we introduce NeighViz, an interactive visual analytics system designed to help social scientists explore, understand, and verify the influence of neighborhood effects on human behaviors.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    },
                    {
                        "slot_id": "s-vds-1016",
                        "session_id": "ae3",
                        "title": "Aardvark: Comparative Visualization of Data Analysis Scripts",
                        "contributors": [
                            "Dr. Rebecca Faust"
                        ],
                        "authors": [],
                        "abstract": "Debugging programs is famously one of the most challenging aspects of programming. Data analysis scripts present additional challenges as debugging tasks are often more exploratory, such as comparing results under different parameter settings. In fact, a common exploratory debugging process is to run, modify, and re-run a script to observe the effects of the change. Analyst\u2019s perform this process repeatedly as they explore different settings in their script. However, traditional debugging methods do not support direct comparison across script executions. To address this, we present Aardvark, a comparative trace-based debugging method for identifying and visualizing the differences between consecutive executions of analysis scripts. Aardvark traces two consecutive instances of a script, identifies the differences between them, and presents them through comparative visualizations. We present a prototype implementation in Python along with an extension to Jupyter notebooks and  demonstrate Aardvark through two usage scenarios on real world analysis scripts.",
                        "uid": "s-vds-1016",
                        "time_stamp": "2023-10-23T04:00:00Z",
                        "time_start": "2023-10-23T04:00:00Z",
                        "time_end": "2023-10-23T05:15:00Z",
                        "paper_type": "",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "1",
                        "paper_award": "",
                        "image_caption": "\u200b\u200bAn overview of Aardvark, a comparative visual debugging method for data analysis programs. Aardvark visualizes the differences between consecutive executions of an analysis script. The source view, (A), highlights the differences in the analysis source code. (B) shows the comparative generalized context tree to illustrate the differences in the execution structure. The original execution tree builds down from the center block, while the modified version builds up.\u00a0 (C) shows the comparative scatter plot that illustrates how the values differ across the consecutive executions.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    },
                    {
                        "slot_id": "s-vds-1017",
                        "session_id": "ae3",
                        "title": "A Declarative Specification for Authoring Metrics Dashboards",
                        "contributors": [
                            "Will Epperson"
                        ],
                        "authors": [],
                        "abstract": "Despite their ubiquity, authoring dashboards for metrics reporting in modern data analysis tools remains a manual, time-consuming process. Rather than focusing on interesting combinations of their data, users have to spend time creating each chart in a dashboard one by one. This makes dashboard creation slow and tedious. We conducted a review of production metrics dashboards and found that many dashboards contain a common structure: breaking down one or more metrics by different dimensions. In response, we developed a high-level specification for describing dashboards as sections of metrics repeated across the same dimensions and a graphical interface, Quick Dashboard, for authoring dashboards based on this specification. We present several usage examples that demonstrate the flexibility of this specification to create various kinds of dashboards and support a data-first approach to dashboard authoring",
                        "uid": "s-vds-1017",
                        "time_stamp": "2023-10-23T04:00:00Z",
                        "time_start": "2023-10-23T04:00:00Z",
                        "time_end": "2023-10-23T05:15:00Z",
                        "paper_type": "",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "1",
                        "paper_award": "",
                        "image_caption": "Quick dashboarding presents a novel specification for dashboard authoring, comprised of sections of metrics combined with dimensions.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    },
                    {
                        "slot_id": "s-vds-1018",
                        "session_id": "ae3",
                        "title": "Visual Comparison of Text Sequences Generated by Large Language Models",
                        "contributors": [
                            "Rita Sevastjanova"
                        ],
                        "authors": [],
                        "abstract": "Causal language models have emerged as the leading technology for automating text generation tasks. Although these models tend to produce outputs that resemble human writing, they still suffer from quality issues (e.g., social biases). Researchers typically use automatic analysis methods to evaluate the model limitations, such as statistics on stereotypical words. Since different types of issues are embedded in the model parameters, the development of automated methods that capture all relevant aspects remains a challenge. To tackle this challenge, we propose a visual analytics approach that supports the exploratory analysis of text sequences generated by causal language models. Our approach enables users to specify starting prompts and effectively groups the resulting text sequences. To this end, we leverage a unified, ontology-driven embedding space, serving as a shared foundation for the thematic concepts present in the generated text sequences. Visual summaries provide insights into various levels of granularity within the generated data. Among others, we propose a novel comparison visualization that slices the embedding space and represents the differences between two prompt outputs in a radial layout. We demonstrate the effectiveness of our approach through case studies, showcasing its potential to reveal model biases and other quality issues.",
                        "uid": "s-vds-1018",
                        "time_stamp": "2023-10-23T04:00:00Z",
                        "time_start": "2023-10-23T04:00:00Z",
                        "time_end": "2023-10-23T05:15:00Z",
                        "paper_type": "",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "1",
                        "paper_award": "",
                        "image_caption": "We introduce a novel visual analytics approach supporting exploratory analysis of automatically generated text sequences and their comparison.  Our visualizations help to investigate stereotypes associated with different prompts, inspect model differences, and detect unexpected associations encoded in open source language models.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    }
                ]
            }
        ]
    },
    "w-vis4dh": {
        "event": "VIS4DH: 8th Workshop on Visualization for the Digital Humanities",
        "long_name": "VIS4DH: 8th Workshop on Visualization for the Digital Humanities",
        "event_type": "",
        "event_prefix": "w-vis4dh",
        "event_description": "",
        "event_url": "",
        "organizers": [
            "Alfie Abdul-Rahman",
            "Eric Alexander"
        ],
        "sessions": [
            {
                "title": "VIS4DH: 8th Workshop on Visualization for the Digital Humanities",
                "session_id": "w1",
                "event_prefix": "w-vis4dh",
                "track": "105(234)",
                "session_image": "w1.png",
                "chair": [
                    "Alfie Abdul-Rahman",
                    "Eric Alexander"
                ],
                "organizers": [],
                "time_start": "2023-10-22T00:00:00Z",
                "time_end": "2023-10-23T05:15:00Z",
                "discord_category": "",
                "discord_channel": "",
                "discord_channel_id": "",
                "discord_link": "",
                "zoom_meeting": "",
                "zoom_password": "",
                "zoom_link": "",
                "time_slots": [
                    {
                        "slot_id": "w-vis4dh-1004",
                        "session_id": "w1",
                        "title": "A Stitch in Time: Using Data Embroidery to Tell Australian Convict Stories",
                        "contributors": [
                            "Dr. Monika M. Schwarz"
                        ],
                        "authors": [],
                        "abstract": "The Stitch in Time project extends a traditional browser-based visualisation, the LifeLines, by creating physical data embroideries from a subset of the data. The LifeLines visualise the individual life courses of over 13.600 nineteenth century Australian convict women, according to their paper trail left behind in the Colonial Archives. In this project we created 18 embroideries of 21 convict women, based on a sketch inspired by a specific bit of information in the woman's life, and then integrating the data of her lifeline into the embroidery. This way we use a novel approach to show convict women, of whom little imagery has survived. By exhibiting the embroideries in the Penitentiary Chapel in Hobart, Tasmania, we hope to arouse interest in the largely forgotten lives of this first coerced generation of European settlers. We use the unusual medium of embroidery on fabric because they both are tightly connected to the daily experiences of convict women. This way, we hope to create new access points to historical data by extending traditional data visualisation using this specific form of data physicalisation.",
                        "uid": "w-vis4dh-1004",
                        "time_stamp": "2023-10-23T04:00:00Z",
                        "time_start": "2023-10-23T04:00:00Z",
                        "time_end": "2023-10-23T05:15:00Z",
                        "paper_type": "workshop",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "0",
                        "paper_award": "",
                        "image_caption": "The Stitch in Time project extends a traditional timeline visualisation of Australian convict women\ufffds lives by creating physical data embroideries drawing from the available historical data of individual women. In this project we created 18 embroideries of 21 convict women, based on sketches inspired by specific pieces of information in each woman\ufffds life. We use the unusual medium of embroidery on fabric because it is tightly connected to the daily experiences of convict women. By extending traditional data visualisation using this specific form of data physicalisation we aim to create compelling access points to engage with individual Australian convict stories.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    },
                    {
                        "slot_id": "w-vis4dh-1006",
                        "session_id": "w1",
                        "title": "Do Words Matter: Visualising Historical Policy and Media Narratives around Opportunity and Disadvantage in Australia",
                        "contributors": [
                            "Sarah Goodwin"
                        ],
                        "authors": [],
                        "abstract": "Despite the widely held belief that public discourse shapes and informs public policy, tracking and analysing the dynamics of public discourse over long time-frames remains a significant challenge. Myriad factors such as editorial policies, news sensationalism, election cycles, societal priorities, and political agendas can all impact the attention given, and treatment of, a range of important societal issues such as systematic disadvantage. Here, we introduce and describe `Discourse of the Past', an interactive visualisation created for both public touch-screen exhibition and online. The visualisation presents an AI-assisted analysis of hundreds of thousands of op-ed news articles and speeches from the major Australian mastheads and federal parliament respectively. By focusing on 23 population groups and 33 issues, we provide a rich, dynamic picture of how disadvantage is experienced in Australia and by whom. Users can discover a series of findings, such as: how News and Parliament have their own agenda and how each changes its focus over time; how some issues are more recurrent than others; how coverage and discourse intensity change relative to cycles and events; and how both discourses contribute to a better understanding of how disadvantage is lived in Australia.",
                        "uid": "w-vis4dh-1006",
                        "time_stamp": "2023-10-23T04:00:00Z",
                        "time_start": "2023-10-23T04:00:00Z",
                        "time_end": "2023-10-23T05:15:00Z",
                        "paper_type": "workshop",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "0",
                        "paper_award": "",
                        "image_caption": "Discourse of the Past: An exploratory visualisation tool with story narrative to highlight patterns of how Australian News coverage varies from that of the Parliament (Senate and House) when discussing disadvantaged groups or issues.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    },
                    {
                        "slot_id": "w-vis4dh-1008",
                        "session_id": "w1",
                        "title": "A Workflow Approach to Visualization-Based Storytelling with Cultural Heritage Data",
                        "contributors": [
                            "Jakob Kusnick"
                        ],
                        "authors": [],
                        "abstract": "Stories are as old as human history\u2013-and a powerful means for the engaging communication of information, especially in combination with visualizations. The InTaVia project is built on this intersection and has developed a platform which supports the workflow of cultural heritage experts to create compelling visualization-based stories: From the search for relevant cultural objects and actors in a cultural knowledge graph, to the curation and visual analysis of the selected information, and to the creation of stories based on these data and visualizations, which can be shared with the interested public.",
                        "uid": "w-vis4dh-1008",
                        "time_stamp": "2023-10-23T04:00:00Z",
                        "time_start": "2023-10-23T04:00:00Z",
                        "time_end": "2023-10-23T05:15:00Z",
                        "paper_type": "workshop",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "0",
                        "paper_award": "",
                        "image_caption": "The InTaVia platform is designed to support diverse cultural heritage data practices through visualization-based interfaces. This includes activities like searching, creating, curating, analyzing, and communicating for a wide range of users. The platform follows an iterative workflow model, represented by blue arrows, with main modules such as the Data Curation Lab, Visual Analytics Studio, and Storytelling Suite covering each stage of development.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    }
                ]
            }
        ]
    },
    "w-topoinvis": {
        "event": "TopoInVis: Topological Data Analysis and Visualization",
        "long_name": "TopoInVis: Topological Data Analysis and Visualization",
        "event_type": "",
        "event_prefix": "w-topoinvis",
        "event_description": "",
        "event_url": "",
        "organizers": [
            "Paul Rosen"
        ],
        "sessions": [
            {
                "title": "TopoInVis: Topological Data Analysis and Visualization",
                "session_id": "w3",
                "event_prefix": "w-topoinvis",
                "track": "106(234)",
                "session_image": "w3.png",
                "chair": [
                    "Paul Rosen"
                ],
                "organizers": [],
                "time_start": "2023-10-22T00:00:00Z",
                "time_end": "2023-10-22T05:15:00Z",
                "discord_category": "",
                "discord_channel": "",
                "discord_channel_id": "",
                "discord_link": "",
                "zoom_meeting": "",
                "zoom_password": "",
                "zoom_link": "",
                "time_slots": [
                    {
                        "slot_id": "w-topoinvis-1006",
                        "session_id": "w3",
                        "title": "Homology-Preserving Multi-Scale Graph Skeletonization Using Mapper on Graphs",
                        "contributors": [
                            "Paul Rosen"
                        ],
                        "authors": [],
                        "abstract": "Node-link diagrams are a popular method for representing graphs that capture relationships between individuals, businesses, proteins, and telecommunication endpoints. However, node-link diagrams may fail to convey insights regarding graph structures, even for moderately sized data of a few hundred nodes, due to visual clutter. We propose to apply the mapper construction---a popular tool in topological data analysis---to graph visualization, which provides a strong theoretical basis for summarizing the data while preserving their core structures. We develop a variation of the mapper construction targeting weighted, undirected graphs, called mapper on graphs, which generates homology-preserving skeletons of graphs. We further show how the adjustment of a single parameter enables multi-scale skeletonization of the input graph. We provide a software tool that enables interactive explorations of such skeletons and demonstrate the effectiveness of our method for synthetic and real-world data.",
                        "uid": "w-topoinvis-1006",
                        "time_stamp": "2023-10-22T04:00:00Z",
                        "time_start": "2023-10-22T04:00:00Z",
                        "time_end": "2023-10-22T05:15:00Z",
                        "paper_type": "workshop",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "0",
                        "paper_award": "",
                        "image_caption": "We present our technique, called Mapper on Graph, which adapts the mapper construction\u2014a popular tool from topological data analysis\u2014to the visualization of graphs. It provides multi-scale skeletonizations of the graph from diverse perspectives. By modifying a single input parameter, namely the number of cover elements, our approach provides a multi-scale skeletonization of the input graph that emphasizes the property of interest. In the examples presented here, the visible level-of-detail is reduced as the number of elements decreases while the homology of the graph\u2014in the form of components and tunnels\u2014remains well persevered.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    },
                    {
                        "slot_id": "w-topoinvis-1007",
                        "session_id": "w3",
                        "title": "Planar Symmetry Detection and Quantification using the Extended Persistent Homology Transform",
                        "contributors": [
                            "Mr Nicholas A Bermingham"
                        ],
                        "authors": [],
                        "abstract": "Symmetry is ubiquitous throughout nature and can often give great insights into the formation, structure and stability of objects studied by mathematicians, physicists, chemists and biologists. However, perfect symmetry occurs rarely so quantitative techniques must be developed to identify approximate symmetries. To facilitate the analysis of an independent variable on the symmetry of some object, we would like this quantity to be a smoothly varying real parameter rather than a boolean one. The extended persistent homology transform is a recently developed tool which can be used to define a distance between certain kinds of objects. Here, we describe  how the extended persistent homology transform can be used to visualise, detect and quantify certain kinds of symmetry and discuss the effectiveness and limitations of this method.",
                        "uid": "w-topoinvis-1007",
                        "time_stamp": "2023-10-22T04:00:00Z",
                        "time_start": "2023-10-22T04:00:00Z",
                        "time_end": "2023-10-22T05:15:00Z",
                        "paper_type": "workshop",
                        "keywords": [],
                        "has_image": "0",
                        "has_video": "0",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    },
                    {
                        "slot_id": "w-topoinvis-1008",
                        "session_id": "w3",
                        "title": "Visualizing Topological Importance: A Class-Driven Approach",
                        "contributors": [
                            "Yu Qin"
                        ],
                        "authors": [],
                        "abstract": "This paper presents the first approach to visualize the importance of topological features that define classes of data. Topological features, with their ability to abstract the fundamental structure of complex data, are an integral component of visualization and analysis pipelines. Although not all topological features present in data are of equal importance. To date, the default definition of feature importance is often assumed and fixed. This work shows how proven explainable deep learning approaches can be adapted for use in topological classification. In doing so, it provides the first technique that illuminates what topological structures are important in each dataset in regards to their class label. In particular, the approach uses a learned metric classifier with a density estimator of the points of a persistence diagram as input. This metric learns how to reweigh this density such that classification accuracy is high. By extracting this weight, an importance field on persistent point density can be created. This provides an intuitive representation of persistence point importance that can be used to drive new visualizations. This work provides two examples: Visualization on each diagram directly and, in the case of sublevel set filtrations on images, directly on the images themselves. This work highlights real-world examples of this approach visualizing the important topological features in graph, 3D shape, and medical image data.",
                        "uid": "w-topoinvis-1008",
                        "time_stamp": "2023-10-22T04:00:00Z",
                        "time_start": "2023-10-22T04:00:00Z",
                        "time_end": "2023-10-22T05:15:00Z",
                        "paper_type": "workshop",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "0",
                        "paper_award": "",
                        "image_caption": "(a) A common task in topological data analysis: extracting a persistence diagram of topological features. In this case, features are based on the sublevel set filtration of pathology images with class labels (Gleason grade) that define the progression of prostate cancer~\\cite{lawson2019persistent}. Knowing which features are important for each class is commonly an educated guess with the lifetime of a feature (persistence) often assumed to define importance. (b) Our approach, based on a learned metric classifier, takes as input the unweighted density of persistence points and reweighs this density based on what best defines a class. This allows us to build a field of importance for regions of a diagram. (c) This importance field can be used to create visualizations to illuminate which features drive a classification. For example, it can highlight what points are important directly in a diagram or, in the case of sublevel set filtrations, visualize the important structure directly in an image. Consider that a hallmark of prostate cancer is gland degeneration as the disease progresses.  Calcifications (red arrow) are only present in well-structured glands and are highlighted as important structures for Gleason 3, an earlier stage of the disease.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    },
                    {
                        "slot_id": "w-topoinvis-1009",
                        "session_id": "w3",
                        "title": "Taming Horizontal Instability in Merge Trees: On the Computation of a Comprehensive Deformation-based Edit Distance",
                        "contributors": [
                            "Florian Wetzels"
                        ],
                        "authors": [],
                        "abstract": "Comparative analysis of scalar fields in scientific visualization often involves distance functions on topological abstractions. This paper focuses on the merge tree abstraction (representing the nesting of sub- or superlevel sets) and proposes the application of the unconstrained deformation-based edit distance. Previous approaches on merge trees often suffer from instability: small perturbations in the data can lead to large distances of the abstractions. While some existing methods can handle so-called vertical instability, the unconstrained deformation-based edit distance addresses both vertical and horizontal instabilities, also called saddle swaps. We establish the computational complexity as NP-complete, and provide an integer linear program formulation for computation. Experimental results on the TOSCA shape matching ensemble provide evidence for the stability of the proposed distance. We thereby showcase the potential of handling saddle swaps for comparison of scalar fields through merge trees.",
                        "uid": "w-topoinvis-1009",
                        "time_stamp": "2023-10-22T04:00:00Z",
                        "time_start": "2023-10-22T04:00:00Z",
                        "time_end": "2023-10-22T05:15:00Z",
                        "paper_type": "workshop",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "1",
                        "paper_award": "",
                        "image_caption": "Illustration of the improvements of unconstrained edit distances over constrained edit distances on the TOSCA ensemble: two embedded mappings of critical points are shown on the left, the same mappings in a typical merge tree layout on the right. In between, distance matrices for multiple members of the ensemble can be found.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    },
                    {
                        "slot_id": "w-topoinvis-1010",
                        "session_id": "w3",
                        "title": "A Mathematical Foundation for the Spatial Uncertainty of Critical Points in Probabilistic Scalar Fields",
                        "contributors": [
                            "Dominik Vietinghoff"
                        ],
                        "authors": [],
                        "abstract": "Critical points mark locations in the domain where the level-set topology of a scalar function undergoes fundamental changes and thus indicate potentially interesting features in the data. Established methods exist to locate and relate such points in a deterministic setting, but it is less well understood how the concept of critical points can be extended to the analysis of uncertain data. Most methods for this task aim at finding likely locations of critical points or estimate the probability of their occurrence locally but do not indicate if critical points at potentially different locations in different realizations of a stochastic process are manifestations of the same feature, which is required to characterize the spatial uncertainty of critical points. Previous work on relating critical points across different realizations reported challenges for interpreting the resulting spatial distribution of critical points but did not investigate the causes. In this work, we provide a mathematical formulation of the problem of finding critical points with spatial uncertainty and computing their spatial distribution, which leads us to the notion of uncertain critical points. We analyze the theoretical properties of these structures and highlight connections to existing works for special classes of uncertain fields. We derive conditions under which well-interpretable results can be obtained and discuss the implications of those restrictions for the field of visualization. We demonstrate that the discussed limitations are not purely academic but also arise in real-world data.",
                        "uid": "w-topoinvis-1010",
                        "time_stamp": "2023-10-22T04:00:00Z",
                        "time_start": "2023-10-22T04:00:00Z",
                        "time_end": "2023-10-22T05:15:00Z",
                        "paper_type": "workshop",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "0",
                        "paper_award": "",
                        "image_caption": "Set of critical points of the function x^2/10 + a*sin(x) is decomposed into connected components by poles (black lines) and degenerate critical points (gray lines) where the critical type changes. We call these components uncertain critical points. If the randomness of the field is described by the random parameter 'a' following a certain distribution (left), we can compute the density at each point along the graph (color map on the right). Projection on the domain (bottom) yields the spatial distribution of critical points with spatial uncertainty. We analyze this in a more general setting in our paper.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    },
                    {
                        "slot_id": "w-topoinvis-1012",
                        "session_id": "w3",
                        "title": "Probabilistic Gradient-Based Extrema Tracking",
                        "contributors": [
                            "Emma Nilsson"
                        ],
                        "authors": [],
                        "abstract": "Feature tracking is a common task in visualization applications, where methods based on topological data analysis (TDA) have successfully been applied in the past for feature definition as well as tracking. In this work, we focus on tracking extrema of temporal scalar fields. A family of TDA approaches address this task by establishing one-to-one correspondences between extrema based on discrete gradient vector fields. More specifically, two extrema of subsequent time steps are matched if they fall into their respective ascending and descending manifolds. However, due to this one-to-one assignment, these approaches are prone to fail where, e.g., extrema are located in regions with low gradient magnitude, or are located close to boundaries of the manifolds. Therefore, we propose a probabilistic matching that captures a larger set of possible correspondences via neighborhood sampling, or by computing the overlap of the manifolds. We illustrate the usefulness of the approach with two application cases.",
                        "uid": "w-topoinvis-1012",
                        "time_stamp": "2023-10-22T04:00:00Z",
                        "time_start": "2023-10-22T04:00:00Z",
                        "time_end": "2023-10-22T05:15:00Z",
                        "paper_type": "workshop",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "1",
                        "paper_award": "",
                        "image_caption": "The image shows our result using gradient-based tracking on maxima of a valence electronic density distribution in a molecular dynamics simulation. The graph in (e) shows feature changes between time steps, while the top row (a-d) shows volume renderings of the density and the maxima. Each interesting feature is colored and highlighted based on the index of the maximum. In (f), we show closeups of a split event using the graph. On top, the line thickness encodes a binary matching and below, the line thickness encodes four probability categories based on our proposed maximum descending manifold overlap.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    },
                    {
                        "slot_id": "w-topoinvis-1014",
                        "session_id": "w3",
                        "title": "Sketching Merge Trees for Scientific Visualization",
                        "contributors": [
                            "Mingzhe Li"
                        ],
                        "authors": [],
                        "abstract": "Merge trees are a type of topological descriptors that record the connectivity among the sublevel sets of scalar fields. They are among the most widely used topological tools in visualization. In this paper, we are interested in sketching a set of merge trees using techniques from matrix sketching. That is, given a large set T of merge trees, we would like to find a much smaller set of basis trees S such that each tree in T can be approximately reconstructed from a linear combination of merge trees in S. A set of high-dimensional vectors can be approximated via matrix sketching techniques such as principal component analysis and column subset selection. However, until now, there has not been any work on sketching a set of merge trees. We develop a framework for sketching a set of merge trees that combines matrix sketching with tools from optimal transport. In particular, we vectorize a set of merge trees into high-dimensional vectors while preserving their structures and structural relations. We demonstrate the applications of our framework in sketching merge trees that arise from time-varying scientific simulations. Specifically, our framework obtains a set of basis trees as representatives that capture the \u201cmodes\u201d of physical phenomena for downstream analysis and visualization.",
                        "uid": "w-topoinvis-1014",
                        "time_stamp": "2023-10-22T04:00:00Z",
                        "time_start": "2023-10-22T04:00:00Z",
                        "time_end": "2023-10-22T05:15:00Z",
                        "paper_type": "workshop",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "1",
                        "paper_award": "",
                        "image_caption": "Left: the pipeline of our framework for apply matrix sketching techniques to a set of merge trees.   Right: results for the Rotating Gaussian dataset. Basis merge trees selected by our framework can represent the structure of all merge trees.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    },
                    {
                        "slot_id": "w-topoinvis-1015",
                        "session_id": "w3",
                        "title": "Comparing Mapper Graphs of Artificial Neuron Activations",
                        "contributors": [
                            "Bei Wang"
                        ],
                        "authors": [],
                        "abstract": "The mapper graph is a popular tool from topological data analysis that provides a graphical summary of point cloud data. It has been used to study data from cancer research, sports analytics, neurosciences, and machine learning. In particular, mapper graphs have been used recently to visualize the topology of high-dimensional artificial neural activations from convolutional neural networks and large language models. However, a key question that arises from using mapper graphs across applications is how to compare mapper graphs to study their structural differences. In this paper, we introduce a distance between mapper graphs using tools from optimal transport. We demonstrate the utility of such a distance by studying the topological changes of neural activations across convolutional layers in deep learning, as well as by capturing the loss of structural information for a multiscale mapper.",
                        "uid": "w-topoinvis-1015",
                        "time_stamp": "2023-10-22T04:00:00Z",
                        "time_start": "2023-10-22T04:00:00Z",
                        "time_end": "2023-10-22T05:15:00Z",
                        "paper_type": "workshop",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "0",
                        "paper_award": "",
                        "image_caption": "Mapper graphs have been used to visualize the topology of artificial neural activations from convolutional neural networks and large language models. A key question is how to compare mapper graphs to study their structural differences. We introduce a distance between mapper graphs using tools from optimal transport. We demonstrate the utility of such a distance by studying the topological changes of neural activations across convolutional layers in deep learning. Here we show a color transfer from the mapper graph nodes in two adjacent layers that highlights structural correspondences between the neuron activations.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    },
                    {
                        "slot_id": "w-topoinvis-1016",
                        "session_id": "w3",
                        "title": "Multi-field Visualisation via Trait-induced Merge Trees",
                        "contributors": [
                            "Ingrid Hotz"
                        ],
                        "authors": [],
                        "abstract": "In this work, we propose trait-based merge trees a generalization of merge trees to feature level sets, targeting the analysis of tensor field or general multi-variate data. For this, we employ the notion of traits defined in attribute space as introduced in the feature level sets framework. The resulting distance field in attribute space induces a scalar field in the spatial domain that serves as input for topological data analysis. The leaves in the merge tree represent those areas in the input data that are closest to the defined trait and thus most closely resemble the defined feature. Hence, the merge tree yields a hierarchy of features that allows for querying the most relevant and persistent features. The presented method includes different query methods for the tree which enable the highlighting of different aspects. We demonstrate the cross-application capabilities of this approach with three case studies from different domains.",
                        "uid": "w-topoinvis-1016",
                        "time_stamp": "2023-10-22T04:00:00Z",
                        "time_start": "2023-10-22T04:00:00Z",
                        "time_end": "2023-10-22T05:15:00Z",
                        "paper_type": "workshop",
                        "keywords": [],
                        "has_image": "0",
                        "has_video": "0",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    },
                    {
                        "slot_id": "w-topoinvis-1018",
                        "session_id": "w3",
                        "title": "Combinatorial Exploration of Morse\u2013Smale Functions on the Sphere via Interactive Visualization",
                        "contributors": [
                            "Bei Wang"
                        ],
                        "authors": [],
                        "abstract": "In this paper, we are interested in the characterization and classification of Morse\u2013Smale functions. To that end, we present MSF Designer, an interactive visualization tool that supports the combinatorial exploration of Morse\u2013Smale functions on the sphere. Our tool supports the design and visualization of a Morse\u2013Smale function in a simple way using fundamental moves, which are combinatorial operations introduced by Catanzaro et al. that modify the Morse\u2013Smale graph of the function. It also provides fine-grained control over the geometry and topology of its gradient vector field. The tool is designed to help mathematicians explore the complex configuration spaces of Morse\u2013Smale functions, as well as their associated gradient vector fields and Morse\u2013Smale complexes. Understanding these spaces will help mathematicians expand their applicability in topological data analysis and visualization. In particular, our tool helps topologists, geometers, and combinatorialists explore invariants in the classification of vector fields and characterize Morse functions in the persistent homology setting.",
                        "uid": "w-topoinvis-1018",
                        "time_stamp": "2023-10-22T04:00:00Z",
                        "time_start": "2023-10-22T04:00:00Z",
                        "time_end": "2023-10-22T05:15:00Z",
                        "paper_type": "workshop",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "0",
                        "paper_award": "",
                        "image_caption": "We present MSF Designer, a visualization tool that supports the combinatorial exploration of Morse-Smale functions on the sphere. The interface consists of (A) Function and flow visualization panel supports modifying the topology and geometry of the Morse-Smale graph of the function and visualizes the dynamics of its underlying gradient vector field; (B) Elementary moves panel provides a set of elementary moves as fundamental building blocks of a Morse-Smale function; (C) Function adjustment panel allows modifying the function values at singularities; (D) History panel provides undo and redo features; and (E) Barcode panel computes and displays barcodes to guide persistence simplification.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    }
                ]
            }
        ]
    },
    "w-vis4good": {
        "event": "Visualization for Social Good",
        "long_name": "Visualization for Social Good",
        "event_type": "",
        "event_prefix": "w-vis4good",
        "event_description": "",
        "event_url": "",
        "organizers": [
            "Evanthia Dimara",
            "Uzma Haque Syeda",
            "Narges Mahyar",
            "Delvin Varghese",
            "Emily Wall"
        ],
        "sessions": [
            {
                "title": "Visualization for Social Good",
                "session_id": "w6",
                "event_prefix": "w-vis4good",
                "track": "105(234)",
                "session_image": "w6.png",
                "chair": [
                    "Evanthia Dimara",
                    "Uzma Haque Syeda",
                    "Narges Mahyar",
                    "Delvin Varghese",
                    "Emily Wall"
                ],
                "organizers": [],
                "time_start": "2023-10-22T00:00:00Z",
                "time_end": "2023-10-23T00:15:00Z",
                "discord_category": "",
                "discord_channel": "",
                "discord_channel_id": "",
                "discord_link": "",
                "zoom_meeting": "",
                "zoom_password": "",
                "zoom_link": "",
                "time_slots": [
                    {
                        "slot_id": "w-vis4good-1892",
                        "session_id": "w6",
                        "title": "Supporting Mathematical Education with Interactive Visual Proofs",
                        "contributors": [
                            "Moritz Weckbecker"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "w-vis4good-1892",
                        "time_stamp": "2023-10-22T23:00:00Z",
                        "time_start": "2023-10-22T23:00:00Z",
                        "time_end": "2023-10-23T00:15:00Z",
                        "paper_type": "workshop",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "1",
                        "paper_award": "",
                        "image_caption": "Visual proofs are illustrations, which demonstrate the validity of a mathematical statement in a self-evident manner. Their potential for explaining and engaging can be enhanced by the addition of interactive elements. We present an open platform for interactive visual proofs that is freely available for students and teachers alike. We aim to support equity in mathematical education by creating the first platform of this kind which is free, participatory and multilingual. Check it out at visualproofs.github.io and find the paper and the rest of our work at visualization.group!",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    },
                    {
                        "slot_id": "w-vis4good-3038",
                        "session_id": "w6",
                        "title": "From Flowchart to Questionnaire: Increasing Access to Justice via Visualization",
                        "contributors": [
                            "Bei Wang"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "w-vis4good-3038",
                        "time_stamp": "2023-10-22T23:00:00Z",
                        "time_start": "2023-10-22T23:00:00Z",
                        "time_end": "2023-10-23T00:15:00Z",
                        "paper_type": "workshop",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "0",
                        "paper_award": "",
                        "image_caption": "One of the main barriers of access to justice is the lack of awareness of legal rights, procedures, and available resources. In this project, we develop F2Q (Flowchart to Questionnaire), an open-source toolbox for legal experts or staff members at legal clinics or help centers (oftentimes with no programming expertise) to easily design and automatically generate web-based interactive questionnaires.  Such questionnaires help guide the clients of these help centers through a series of questions that help them correctly categorize their legal problems and identify appropriate remedies or solutions. We show in this figure the visual interface of the tool.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    },
                    {
                        "slot_id": "w-vis4good-3347",
                        "session_id": "w6",
                        "title": "Bridging the Divide: Promoting Serendipitous Discovery of Opposing Viewpoints with Visual Analytics in Social Media",
                        "contributors": [
                            "Mahmood Jasim"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "w-vis4good-3347",
                        "time_stamp": "2023-10-22T23:00:00Z",
                        "time_start": "2023-10-22T23:00:00Z",
                        "time_end": "2023-10-23T00:15:00Z",
                        "paper_type": "workshop",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "0",
                        "paper_award": "",
                        "image_caption": "Serendyze enables several features for social media post exploration including a search option to look for a specific word, a set of filters corresponding to representative pairs of keywords, filters for social media posts with For, Against, and Neutral alignments, three exploration metrics - Visit, Coverage, and Distribution, all social media posts, and suggested social media posts generated by the bias mitigation model that the readers may find interesting. This image was taken during P14\u2019s exploration of social media posts.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    },
                    {
                        "slot_id": "w-vis4good-3888",
                        "session_id": "w6",
                        "title": "The Good Life: visualizing the complexity of supported living for people with disability",
                        "contributors": [
                            "Georgina Hibberd"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "w-vis4good-3888",
                        "time_stamp": "2023-10-22T23:00:00Z",
                        "time_start": "2023-10-22T23:00:00Z",
                        "time_end": "2023-10-23T00:15:00Z",
                        "paper_type": "workshop",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "0",
                        "paper_award": "",
                        "image_caption": "The Good Life interactive visualisation tool",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    },
                    {
                        "slot_id": "w-vis4good-6432",
                        "session_id": "w6",
                        "title": "Empowering People with Intellectual and Developmental Disabilities through Cognitively Accessible Visualizations",
                        "contributors": [
                            "Keke Wu"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "w-vis4good-6432",
                        "time_stamp": "2023-10-22T23:00:00Z",
                        "time_start": "2023-10-22T23:00:00Z",
                        "time_end": "2023-10-23T00:15:00Z",
                        "paper_type": "workshop",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "0",
                        "paper_award": "",
                        "image_caption": "The teaser figure illustrates two immediate objectives of cognitively accessible visualizations. Namely, designing for empathetic and therapeutic storytelling, and for multi-sensory and culturally relevant data experiences. The figure illustrates the idea in a creative manner: the four key words are laid out in two separate word clouds. In the middle, the three letters, V, I, S are artistically illustrated with light bulbs and a brain icon to represent cognitive diversity and creativity.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    },
                    {
                        "slot_id": "w-vis4good-6918",
                        "session_id": "w6",
                        "title": "Beyond English: Centering Multilingualism in Data Visualization",
                        "contributors": [
                            "No\u00eblle Rakotondravony"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "w-vis4good-6918",
                        "time_stamp": "2023-10-22T23:00:00Z",
                        "time_start": "2023-10-22T23:00:00Z",
                        "time_end": "2023-10-23T00:15:00Z",
                        "paper_type": "workshop",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "1",
                        "paper_award": "",
                        "image_caption": "A picture with the paper title: \u201cBeyond English: Centering Multilingualism in data Visualization\u201d, and the illustration of diverse people dialoging.  The research questions are listed: How do experiences with data and visualization vary in non-English speaking contexts? Can we do data visualization in languages other than English? How to overcome challenges that come with that? Long-term collective thinking: What can be done? Where can we start?",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    },
                    {
                        "slot_id": "w-vis4good-7520",
                        "session_id": "w6",
                        "title": "Open Questions about the Visualization of Sociodemographic Data",
                        "contributors": [
                            "Florent Cabric"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "w-vis4good-7520",
                        "time_stamp": "2023-10-22T23:00:00Z",
                        "time_start": "2023-10-22T23:00:00Z",
                        "time_end": "2023-10-23T00:15:00Z",
                        "paper_type": "workshop",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "1",
                        "paper_award": "",
                        "image_caption": "Visualizing sociodemographic data without causing harm presents many challenges. Among them, designers must balance the efficiency, inclusiveness, and simplicity of their visualization.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    },
                    {
                        "slot_id": "w-vis4good-9098",
                        "session_id": "w6",
                        "title": "Mapping Minority Women Bicycle Riding",
                        "contributors": [
                            "Ms Mirela Reljan-Delaney Reljan-Delaney"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "w-vis4good-9098",
                        "time_stamp": "2023-10-22T23:00:00Z",
                        "time_start": "2023-10-22T23:00:00Z",
                        "time_end": "2023-10-23T00:15:00Z",
                        "paper_type": "workshop",
                        "keywords": [],
                        "has_image": "0",
                        "has_video": "0",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    },
                    {
                        "slot_id": "w-vis4good-9528",
                        "session_id": "w6",
                        "title": "Visual Salience to Mitigate Gender Bias in Recommendation Letters",
                        "contributors": [
                            "Yanan Da"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "w-vis4good-9528",
                        "time_stamp": "2023-10-22T23:00:00Z",
                        "time_start": "2023-10-22T23:00:00Z",
                        "time_end": "2023-10-23T00:15:00Z",
                        "paper_type": "workshop",
                        "keywords": [],
                        "has_image": "0",
                        "has_video": "0",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    }
                ]
            }
        ]
    },
    "w-altvis": {
        "event": "alt.VIS 2023",
        "long_name": "alt.VIS 2023",
        "event_type": "",
        "event_prefix": "w-altvis",
        "event_description": "",
        "event_url": "",
        "organizers": [
            "Andrew M McNutt",
            "Lonni Besan\u00e7on",
            "Derya Akbaba",
            "Sara Di Bartolomeo",
            "Victor Schetinger"
        ],
        "sessions": [
            {
                "title": "alt.VIS 2023",
                "session_id": "w8",
                "event_prefix": "w-altvis",
                "track": "110(234)",
                "session_image": "w8.png",
                "chair": [
                    "Andrew M McNutt",
                    "Lonni Besan\u00e7on",
                    "Derya Akbaba",
                    "Sara Di Bartolomeo",
                    "Victor Schetinger"
                ],
                "organizers": [],
                "time_start": "2023-10-22T00:00:00Z",
                "time_end": "2023-10-23T05:15:00Z",
                "discord_category": "",
                "discord_channel": "",
                "discord_channel_id": "",
                "discord_link": "",
                "zoom_meeting": "",
                "zoom_password": "",
                "zoom_link": "",
                "time_slots": [
                    {
                        "slot_id": "w-altvis-4225",
                        "session_id": "w8",
                        "title": "On nonstandard visualization",
                        "contributors": [
                            "Dn. Alex Ravsky"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "w-altvis-4225",
                        "time_stamp": "2023-10-23T04:00:00Z",
                        "time_start": "2023-10-23T04:00:00Z",
                        "time_end": "2023-10-23T05:15:00Z",
                        "paper_type": "workshop",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "0",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    },
                    {
                        "slot_id": "w-altvis-4650",
                        "session_id": "w8",
                        "title": "DevOps for DataVis: A Survey and Provocation for Teaching Deployment of Data Visualizations",
                        "contributors": [
                            "Jane L. Adams"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "w-altvis-4650",
                        "time_stamp": "2023-10-23T04:00:00Z",
                        "time_start": "2023-10-23T04:00:00Z",
                        "time_end": "2023-10-23T05:15:00Z",
                        "paper_type": "workshop",
                        "keywords": [],
                        "has_image": "0",
                        "has_video": "0",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    },
                    {
                        "slot_id": "w-altvis-5076",
                        "session_id": "w8",
                        "title": "VisFutures",
                        "contributors": [
                            "Charles Perin"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "w-altvis-5076",
                        "time_stamp": "2023-10-23T04:00:00Z",
                        "time_start": "2023-10-23T04:00:00Z",
                        "time_end": "2023-10-23T05:15:00Z",
                        "paper_type": "workshop",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "0",
                        "paper_award": "",
                        "image_caption": "Vis Futures is a card-based sketching game where players think critically (and playfully) about the future of data and visualization.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    },
                    {
                        "slot_id": "w-altvis-2463",
                        "session_id": "w8",
                        "title": "Data Embroidery with Black-and-White Textures",
                        "contributors": [
                            "Tingying He"
                        ],
                        "authors": [],
                        "abstract": "We investigated data embroidery with black-and-white textures, identifying challenges in the use of textures for machine embroidery based on our own experience. Data embroidery, as a method of physically representing data, offers a unique way to integrate personal data into one's everyday objects. Owing to their monochromatic characteristics, black-and-white textures promise to be easy to use in machine embroidery. We experimented with different textured visualizations designed by experts, and in this article, we detail our workflow and evaluate the performance and suitability of different textures. We then conducted a survey on vegetable preferences within a family and made a canvas bag featuring the embroidered family data to show how embroidered data can be used in practice.",
                        "uid": "w-altvis-2463",
                        "time_stamp": "2023-10-23T04:00:00Z",
                        "time_start": "2023-10-23T04:00:00Z",
                        "time_end": "2023-10-23T05:15:00Z",
                        "paper_type": "workshop",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "1",
                        "paper_award": "",
                        "image_caption": "Two embroidered charts depict the performance of different textures in machine embroidery: the left chart for geometric textures, and the right chart for iconic textures.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    },
                    {
                        "slot_id": "w-altvis-4956",
                        "session_id": "w8",
                        "title": "n Walks in the Fictional Woods",
                        "contributors": [
                            "Victor Schetinger"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "w-altvis-4956",
                        "time_stamp": "2023-10-23T04:00:00Z",
                        "time_start": "2023-10-23T04:00:00Z",
                        "time_end": "2023-10-23T05:15:00Z",
                        "paper_type": "workshop",
                        "keywords": [],
                        "has_image": "0",
                        "has_video": "0",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    },
                    {
                        "slot_id": "w-altvis-9647",
                        "session_id": "w8",
                        "title": "LSDvis: Hallucinatory data visualisations in real world environments",
                        "contributors": [
                            "Benjamin Lee"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "w-altvis-9647",
                        "time_stamp": "2023-10-23T04:00:00Z",
                        "time_start": "2023-10-23T04:00:00Z",
                        "time_end": "2023-10-23T05:15:00Z",
                        "paper_type": "workshop",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "0",
                        "paper_award": "",
                        "image_caption": "Examples of LSDvis based on Australian landmarks. Left: A bar chart of popular visiting times blended onto the facade of the Federation Square building. Middle: An area chart of visiting vehicle counts added as a rock in The Twelve Apostles. Right: A pie chart of revenue percentages blended into the shells of the Sydney Opera House.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    },
                    {
                        "slot_id": "w-altvis-1171",
                        "session_id": "w8",
                        "title": "Only YOU Can Make IEEE VIS Environmentally Sustainable",
                        "contributors": [
                            "Elsie Lee-Robbins"
                        ],
                        "authors": [],
                        "abstract": "The IEEE VIS Conference (or VIS) hosts more than 1000 people annually. It brings together visualization researchers and practitioners from across the world to share new research and knowledge. Behind the scenes, a team of volunteers puts together the entire conference and makes sure it runs smoothly. Organizing involves logistics of the conference, ensuring that the attendees have an enjoyable time, allocating rooms to multiple concurrent tracks, and keeping the conference within budget. In recent years, the COVID-19 pandemic has abruptly disrupted plans, forcing organizers to switch to virtual, hybrid, and satellite formats. These alternatives offer many benefits: fewer costs (e.g., travel, venue, institutional), greater accessibility (who can physically travel, who can get visas, who can get child care), and a lower carbon footprint (as people do not need to fly to attend). As many conferences begin to revert to the pre-pandemic status quo of primarily in-person conferences, we suggest that it is an opportune moment to reflect on the benefits and drawbacks of lower-carbon conference formats. To learn more about the logistics of conference organizing, we talked to 6 senior executive-level VIS organizers. We review some of the many considerations that go into planning, particularly with regard to how they influence decisions about alternative formats. We aim to start a discussion about the sustainability of VIS -- including sustainability for finance, volunteers, and, central to this work, the environment -- for the next three years and the next three hundred years.",
                        "uid": "w-altvis-1171",
                        "time_stamp": "2023-10-23T04:00:00Z",
                        "time_start": "2023-10-23T04:00:00Z",
                        "time_end": "2023-10-23T05:15:00Z",
                        "paper_type": "workshop",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "1",
                        "paper_award": "",
                        "image_caption": "The IEEE VIS conference hosts more than a thousand participants, altogether flying thousands of miles, emitting thousands of kilograms of CO2 to attend. This figure represents that our tree(map)s are burning in wildfires, to show the direct connection that our IEEE VIS conference has on climate change.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    },
                    {
                        "slot_id": "w-altvis-5620",
                        "session_id": "w8",
                        "title": "Visualizing the Weird and the Eerie",
                        "contributors": [
                            "Matthew Brehmer"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "w-altvis-5620",
                        "time_stamp": "2023-10-23T04:00:00Z",
                        "time_start": "2023-10-23T04:00:00Z",
                        "time_end": "2023-10-23T05:15:00Z",
                        "paper_type": "workshop",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "0",
                        "paper_award": "",
                        "image_caption": "A montage of imagery emblematic of the weird and the eerie; top row: a strange creature of the sea, something where there should be nothing, a tunnel; middle row: the grotesque, a flock of birds, the mysterious Nazca Lines; bottom row: crop circles, a beach, wildfire haze and the Seattle skyline; all images from Unsplash or Wikimedia - credits in paper.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    },
                    {
                        "slot_id": "w-altvis-8303",
                        "session_id": "w8",
                        "title": "Humanity Influenced Visualization Design for Aerial Sensor-based Visualization of Environmental Factors",
                        "contributors": [
                            "Dr. and Prof. Brian J. d'Auriol"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "w-altvis-8303",
                        "time_stamp": "2023-10-23T04:00:00Z",
                        "time_start": "2023-10-23T04:00:00Z",
                        "time_end": "2023-10-23T05:15:00Z",
                        "paper_type": "workshop",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "0",
                        "paper_award": "",
                        "image_caption": "The motivating perspective of this work is that visualization is human-centric.   This paper reports on the visualization approach, system and deployment, guided   by the Engineering Insightful Serviceable Visualization (EISV) model, and is thus   in the context of this human-centric perspective. An aerial drone-based sensor   platform is proposed to sample environmental data. One of the included sensors   on this platform is a LiDAR. The visual output of the LiDAR is primarily studied   in this paper using the notions of iconicity and indexicality in the Peircean   sense. Several work-in-progress experiments that illustrate how the proposed   system may respond are described.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    }
                ]
            }
        ]
    },
    "a-vast-challenge": {
        "event": "VAST Challenge",
        "long_name": "VAST Challenge",
        "event_type": "",
        "event_prefix": "a-vast-challenge",
        "event_description": "",
        "event_url": "",
        "organizers": [
            "R. Jordan Crouser",
            "Jereme Haack"
        ],
        "sessions": [
            {
                "title": "VAST Challenge",
                "session_id": "c2",
                "event_prefix": "a-vast-challenge",
                "track": "111-112(140)",
                "session_image": "c2.png",
                "chair": [
                    "R. Jordan Crouser",
                    "Jereme Haack"
                ],
                "organizers": [],
                "time_start": "2023-10-22T00:00:00Z",
                "time_end": "2023-10-22T00:15:00Z",
                "discord_category": "",
                "discord_channel": "",
                "discord_channel_id": "",
                "discord_link": "",
                "zoom_meeting": "",
                "zoom_password": "",
                "zoom_link": "",
                "time_slots": [
                    {
                        "slot_id": "a-vast-challenge-1003",
                        "session_id": "c2",
                        "title": "UKON-Frings-MC3",
                        "contributors": [
                            "Udo Schlegel"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "a-vast-challenge-1003",
                        "time_stamp": "2023-10-21T23:00:00Z",
                        "time_start": "2023-10-21T23:00:00Z",
                        "time_end": "2023-10-22T00:15:00Z",
                        "paper_type": "associated",
                        "keywords": [],
                        "has_image": "0",
                        "has_video": "0",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    },
                    {
                        "slot_id": "a-vast-challenge-1004",
                        "session_id": "c2",
                        "title": "UKON-Grotzeck-MC3",
                        "contributors": [
                            "Udo Schlegel"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "a-vast-challenge-1004",
                        "time_stamp": "2023-10-21T23:00:00Z",
                        "time_start": "2023-10-21T23:00:00Z",
                        "time_end": "2023-10-22T00:15:00Z",
                        "paper_type": "associated",
                        "keywords": [],
                        "has_image": "0",
                        "has_video": "0",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    },
                    {
                        "slot_id": "a-vast-challenge-1006",
                        "session_id": "c2",
                        "title": "FishLense",
                        "contributors": [
                            "Mr Robert H\u00f6nig"
                        ],
                        "authors": [],
                        "abstract": "We present FishLense, an interactive tool to visually explore large  knowledge graphs and identify interesting nodes and patterns. FishLense offers smooth transitions from small- to large-scale graph exploration and a ``click-it-all'' interface that lowers the learning curve. FishLense demonstrates these design patterns at the example of a large-scale fishing knowledge graph.",
                        "uid": "a-vast-challenge-1006",
                        "time_stamp": "2023-10-21T23:00:00Z",
                        "time_start": "2023-10-21T23:00:00Z",
                        "time_end": "2023-10-22T00:15:00Z",
                        "paper_type": "associated",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "0",
                        "paper_award": "",
                        "image_caption": "FishLense interactively visualizes large knowledge graphs. Its main innovation is a seamless integration of large-scale and small-scale graph exploration.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    },
                    {
                        "slot_id": "a-vast-challenge-1015",
                        "session_id": "c2",
                        "title": "FishHook: A Visual Analytics System for Tracing Suspicious Entities in the Fisheries Domain using Knowledge Graphs",
                        "contributors": [
                            "Jingfu Wu"
                        ],
                        "authors": [],
                        "abstract": "Undertaking the visual exploration of a large knowledge graph in the domain of illegal fishery activities, FishHook offers an interactive visual analytics solution for scrutinizing individual entities via four distinct views: the Ego Net view, the Hierarchical Tree view, the Unity Net view, and the Parallel Coordinates view. The system incorporates an anomalous pattern detection mechanism, which is designed to address the challenges detailed in the 2023 IEEE VAST Challenge MC1. This integrated solution not only demonstrates robustness and scalability but also provides a powerful tool for comprehending large-scale knowledge graph datasets.",
                        "uid": "a-vast-challenge-1015",
                        "time_stamp": "2023-10-21T23:00:00Z",
                        "time_start": "2023-10-21T23:00:00Z",
                        "time_end": "2023-10-22T00:15:00Z",
                        "paper_type": "associated",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "0",
                        "paper_award": "",
                        "image_caption": "FishHook: A Visual Analytics System for Tracing Suspicious Entities in the Fisheries Domain Using Knowledge Graphs",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    },
                    {
                        "slot_id": "a-vast-challenge-1017",
                        "session_id": "c2",
                        "title": "Purdue-Chen-MC3",
                        "contributors": [
                            "Zuotian Li"
                        ],
                        "authors": [],
                        "abstract": "To solve the VAST Challenge 2023 MC3, our team employed a large language model, ChatGPT, to explore the potential of AI-guided visual analytics for the detection of anomalies within a knowledge graph in the context of illegal fishing and marine trade. We employed a systematic and iterative approach, guided by GPT augmentation, that enabled problem understanding, data processing, solution explo- ration, code writing, and results analysis. By generating and analyz- ing various graphs, we identified anomalies related to revenue and product services. Further analyses unveiled potential illegal fishing activities and identified instances warranting additional investigation. Overall, our work highlights both the strengths and limitations of ChatGPT in aiding the visual analytics process and emphasizes the importance of human judgment in refining AI-generated outputs.",
                        "uid": "a-vast-challenge-1017",
                        "time_stamp": "2023-10-21T23:00:00Z",
                        "time_start": "2023-10-21T23:00:00Z",
                        "time_end": "2023-10-22T00:15:00Z",
                        "paper_type": "associated",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "1",
                        "paper_award": "",
                        "image_caption": "Self-structured mind map with data visualizations that generated via ChatGPT-assisted learning.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    },
                    {
                        "slot_id": "a-vast-challenge-1023",
                        "session_id": "c2",
                        "title": "TTU-Phornsawan-C2",
                        "contributors": [
                            "Phornsawan Roemsri"
                        ],
                        "authors": [],
                        "abstract": "Given an incomplete dataset, FishEye employs various tools, including artificial intelligence, to propose links that enhance the dataset. This paper addresses the challenge of identifying the most reliable tools for completing a knowledge graph. Additionally, the paper tackles the issue of identifying companies potentially involved in illegal, unreported, and unregulated (IUU) fishing. To aid in this endeavor, we introduce a web application designed to detect temporal patterns associated with companies that halt operations and reemerge with new identities. By aggregating data monthly and strategically displaying AI-generated bundle nodes using dropdown controls, our analysis uncovers suspicious patterns, particularly within the Chub Mackerel context. Notably, the Shark and Lichen bundles emerge as dependable graph-completion tools. The overlap between generated data and the knowledge graph in these two bundles underscores the reliability of predictions.",
                        "uid": "a-vast-challenge-1023",
                        "time_stamp": "2023-10-21T23:00:00Z",
                        "time_start": "2023-10-21T23:00:00Z",
                        "time_end": "2023-10-22T00:15:00Z",
                        "paper_type": "associated",
                        "keywords": [],
                        "has_image": "0",
                        "has_video": "0",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    }
                ]
            }
        ]
    },
    "t-colorvis": {
        "event": "Colorizing your Data Visualizations",
        "long_name": "Colorizing your Data Visualizations",
        "event_type": "",
        "event_prefix": "t-colorvis",
        "event_description": "",
        "event_url": "",
        "organizers": [
            "Theresa-Marie Rhyne"
        ],
        "sessions": [
            {
                "title": "Colorizing your Data Visualizations",
                "session_id": "t1",
                "event_prefix": "t-colorvis",
                "track": "111-112(140)",
                "session_image": "t1.png",
                "chair": [
                    "Theresa-Marie Rhyne"
                ],
                "organizers": [],
                "time_start": "2023-10-22T00:00:00Z",
                "time_end": "2023-10-22T05:15:00Z",
                "discord_category": "",
                "discord_channel": "",
                "discord_channel_id": "",
                "discord_link": "",
                "zoom_meeting": "",
                "zoom_password": "",
                "zoom_link": "",
                "time_slots": []
            }
        ]
    },
    "w-vahc": {
        "event": "VAHC: 14th Workshop on Visual Analytics in Healthcare",
        "long_name": "VAHC: 14th Workshop on Visual Analytics in Healthcare",
        "event_type": "",
        "event_prefix": "w-vahc",
        "event_description": "",
        "event_url": "",
        "organizers": [
            "J\u00fcrgen Bernard",
            "Annie T. Chen",
            "Danny T.Y. Wu"
        ],
        "sessions": [
            {
                "title": "VAHC: 14th Workshop on Visual Analytics in Healthcare",
                "session_id": "w4",
                "event_prefix": "w-vahc",
                "track": "104(132)",
                "session_image": "w4.png",
                "chair": [
                    "J\u00fcrgen Bernard",
                    "Annie T. Chen",
                    "Danny T.Y. Wu"
                ],
                "organizers": [],
                "time_start": "2023-10-22T00:00:00Z",
                "time_end": "2023-10-22T05:15:00Z",
                "discord_category": "",
                "discord_channel": "",
                "discord_channel_id": "",
                "discord_link": "",
                "zoom_meeting": "",
                "zoom_password": "",
                "zoom_link": "",
                "time_slots": [
                    {
                        "slot_id": "w-vahc-9996",
                        "session_id": "w4",
                        "title": "Demo: Cohort Visualization and Analysis of Patients with Inflammatory Bowel Disease",
                        "contributors": [
                            "J\u00f6rn Kohlhammer"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "w-vahc-9996",
                        "time_stamp": "2023-10-22T04:00:00Z",
                        "time_start": "2023-10-22T04:00:00Z",
                        "time_end": "2023-10-22T05:15:00Z",
                        "paper_type": "workshop",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "1",
                        "paper_award": "",
                        "image_caption": "This demo paper introduces the final version of a cohort analysis module for the support of treating patients with inflammatory bowel disease (IBD). It is not trivial to correctly diagnose the specific IBD in patients, and wrongly treated patients have to endure the disease effects for a long time, with large costs for the individuals and the healthcare systems. The goal of this work is complementing the examination of individual patients with interactive analyses of cohorts and populations with similar disease patterns to support learning from such similarities for future treatments.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    },
                    {
                        "slot_id": "w-vahc-8545",
                        "session_id": "w4",
                        "title": "Multi-Task Transformer Visualization to build Trust for Clinical Outcome Prediction",
                        "contributors": [
                            "Dario Antweiler"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "w-vahc-8545",
                        "time_stamp": "2023-10-22T04:00:00Z",
                        "time_start": "2023-10-22T04:00:00Z",
                        "time_end": "2023-10-22T05:15:00Z",
                        "paper_type": "workshop",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "1",
                        "paper_award": "",
                        "image_caption": "Proposed visual analytics system that fosters trust into clinical transformer models, consisting of multiple interactive views: (1) Trust in dataset with feature distribution plots and coordinated hierarchical medical code visualization and co-occurance diagrams, (2) trust in model architecture & training with architecture diagram and training loss graph, (3) trust in validation with precision-recall/ROC curves & baseline benchmarks, and (4) trust in prediction with Shapley-values to display feature importance for individual predictions.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    },
                    {
                        "slot_id": "w-vahc-2982",
                        "session_id": "w4",
                        "title": "Towards medhub: A Self-Service Platform for Analysts and Physicians",
                        "contributors": [
                            "Markus H\u00f6hn"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "w-vahc-2982",
                        "time_stamp": "2023-10-22T04:00:00Z",
                        "time_start": "2023-10-22T04:00:00Z",
                        "time_end": "2023-10-22T05:15:00Z",
                        "paper_type": "workshop",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "0",
                        "paper_award": "",
                        "image_caption": "We are presenting the results of our first phase in a multi-year collaboration with analysts and physicians to improve biomarker identification by combining clinical and omics data.  We describe our user-centered visualization and the technical artifacts used as basis for evaluation in the second project phase.  The concept of our approach is based on the data-user-task triangle by Miksch et al.  Since there are two user groups with different usage scenarios, the analysts and physicians are obligated to communicate on a common basis.  This demand is also reflected in the task description.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    },
                    {
                        "slot_id": "w-vahc-4703",
                        "session_id": "w4",
                        "title": "A Visual Analytics Approach to Exploring the Feature and Label Space Based on Semi-structured Electronic Medical Records",
                        "contributors": [
                            "He Wang"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "w-vahc-4703",
                        "time_stamp": "2023-10-22T04:00:00Z",
                        "time_start": "2023-10-22T04:00:00Z",
                        "time_end": "2023-10-22T05:15:00Z",
                        "paper_type": "workshop",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "0",
                        "paper_award": "",
                        "image_caption": "System Interface: (A) The interface panel showcases statistical information concerning the dataset and model. (B) Label Identification View aids clinicians in recognizing potential labels from unstructured diagnostic texts. (C) Feature Exploration View empowers clinicians to choose features by evaluating feature distribution and significance. (D) ML Modeling and Interpretation View reveals the distribution of features among diverse groups and the importance of features contributing to distinct categories.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    },
                    {
                        "slot_id": "w-vahc-6912",
                        "session_id": "w4",
                        "title": "Data Visualization for Mental Health Monitoring in Smart Home Environment: A Case Study",
                        "contributors": [
                            "Youngji Koh"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "w-vahc-6912",
                        "time_stamp": "2023-10-22T04:00:00Z",
                        "time_start": "2023-10-22T04:00:00Z",
                        "time_end": "2023-10-22T05:15:00Z",
                        "paper_type": "workshop",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "0",
                        "paper_award": "",
                        "image_caption": "Correlation View of Data Visualization System",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    },
                    {
                        "slot_id": "w-vahc-3479",
                        "session_id": "w4",
                        "title": "Clinical Issues and Suggestions: Dashboard Visualization of the Trajectory of Patients with Malignant Hormone-Producing Tumors for Precision Medicine",
                        "contributors": [
                            "Masaki Uchihara"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "w-vahc-3479",
                        "time_stamp": "2023-10-22T04:00:00Z",
                        "time_start": "2023-10-22T04:00:00Z",
                        "time_end": "2023-10-22T05:15:00Z",
                        "paper_type": "workshop",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "0",
                        "paper_award": "",
                        "image_caption": "Metastatic hormone-producing tumors have characteristics of both tumors and endocrine disorders with many time-series parameters. Therefore, making treatment decisions is often challenging. An important point in determining the therapeutic strategy for patients with these tumors is the cause of the symptoms and complications of tumor mass and hormone excess, which contribute to a decreased QOL. In addition, the speed of tumor progression is highly variable among patients and is considered the treatment choice. Therefore, it is necessary to visualize tumor size, hormone excess, QOL-related information, and treatments as time-series data.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    },
                    {
                        "slot_id": "w-vahc-8888",
                        "session_id": "w4",
                        "title": "ExpLIMEable: A Visual Analytics Approach for Exploring LIME",
                        "contributors": [
                            "Sonia Laguna"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "w-vahc-8888",
                        "time_stamp": "2023-10-22T04:00:00Z",
                        "time_start": "2023-10-22T04:00:00Z",
                        "time_end": "2023-10-22T05:15:00Z",
                        "paper_type": "workshop",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "0",
                        "paper_award": "",
                        "image_caption": "This figure includes the ExpLIMEable interactive workflow and a depiction of the four steps of the proposed pipeline in purple: 1. Image Selection, 2. Explanation, 3. Segmentation, 4. Reduction, and a, potentially endless, loop back to step 2. The algorithms used and methodology are highlighted in red, the user input in green, and the pipeline outputs in yellow. The machine learning expert is portrayed as the user of this pipeline. This explainability tool allows users to tailor and explore the explanation space generated post hoc by different LIME parameters to gain deeper insights into the model's decision-making process, its sensitivity, and limitations.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    },
                    {
                        "slot_id": "w-vahc-6909",
                        "session_id": "w4",
                        "title": "The Iterative Design Process of an Explainable AI Application for Non-Invasive Diagnosis of CNS Tumors: A User-Centered Approach",
                        "contributors": [
                            "Eric Prince"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "w-vahc-6909",
                        "time_stamp": "2023-10-22T04:00:00Z",
                        "time_start": "2023-10-22T04:00:00Z",
                        "time_end": "2023-10-22T05:15:00Z",
                        "paper_type": "workshop",
                        "keywords": [],
                        "has_image": "0",
                        "has_video": "0",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    },
                    {
                        "slot_id": "w-vahc-7983",
                        "session_id": "w4",
                        "title": "Scalable, interactive and hierarchical visualization of virus taxonomic data",
                        "contributors": [
                            "Kashyap Balakavi"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "w-vahc-7983",
                        "time_stamp": "2023-10-22T04:00:00Z",
                        "time_start": "2023-10-22T04:00:00Z",
                        "time_end": "2023-10-22T05:15:00Z",
                        "paper_type": "workshop",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "0",
                        "paper_award": "",
                        "image_caption": "The image shows a linked view visualization of virus taxonomy that maintains topology with interactive features such as a font size slider, a dropdown for switching years, zoom, and drag.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    },
                    {
                        "slot_id": "w-vahc-6655",
                        "session_id": "w4",
                        "title": "MS Pattern Explorer: Interactive Visual Exploration of Temporal Activity Patterns",
                        "contributors": [
                            "Gabriela Morgenshtern"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "w-vahc-6655",
                        "time_stamp": "2023-10-22T04:00:00Z",
                        "time_start": "2023-10-22T04:00:00Z",
                        "time_end": "2023-10-22T05:15:00Z",
                        "paper_type": "workshop",
                        "keywords": [],
                        "has_image": "0",
                        "has_video": "0",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    },
                    {
                        "slot_id": "w-vahc-7145",
                        "session_id": "w4",
                        "title": "Designing the Australian Cancer Atlas: Visualising Geostatistical Model Uncertainty for Multiple Audiences",
                        "contributors": [
                            "Sarah Goodwin"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "w-vahc-7145",
                        "time_stamp": "2023-10-22T04:00:00Z",
                        "time_start": "2023-10-22T04:00:00Z",
                        "time_end": "2023-10-22T05:15:00Z",
                        "paper_type": "workshop",
                        "keywords": [],
                        "has_image": "0",
                        "has_video": "0",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    }
                ]
            }
        ]
    },
    "w-Vis4PandEmRes": {
        "event": "Visualization for Pandemic and Emergency Responses Workshop (Vis4PandEmRes)",
        "long_name": "Visualization for Pandemic and Emergency Responses Workshop (Vis4PandEmRes)",
        "event_type": "",
        "event_prefix": "w-Vis4PandEmRes",
        "event_description": "",
        "event_url": "",
        "organizers": [
            "Alfie Abdul-Rahman",
            "Kelly Gaither",
            "Wolfgang Jentner",
            "Tobias Schreck",
            "Min Chen",
            "David Ebert"
        ],
        "sessions": [
            {
                "title": "Visualization for Pandemic and Emergency Responses Workshop (Vis4PandEmRes)",
                "session_id": "w7",
                "event_prefix": "w-Vis4PandEmRes",
                "track": "104(132)",
                "session_image": "w7.png",
                "chair": [
                    "Alfie Abdul-Rahman",
                    "Kelly Gaither",
                    "Wolfgang Jentner",
                    "Tobias Schreck",
                    "Min Chen",
                    "David Ebert"
                ],
                "organizers": [],
                "time_start": "2023-10-22T00:00:00Z",
                "time_end": "2023-10-22T00:15:00Z",
                "discord_category": "",
                "discord_channel": "",
                "discord_channel_id": "",
                "discord_link": "",
                "zoom_meeting": "",
                "zoom_password": "",
                "zoom_link": "",
                "time_slots": [
                    {
                        "slot_id": "w-Vis4PandEmRes-1013",
                        "session_id": "w7",
                        "title": "A Lens to Pandemic Stay at Home Attitudes",
                        "contributors": [
                            "Andrew Wentzel"
                        ],
                        "authors": [],
                        "abstract": "We describe the design process and the challenges we met during a rapid multi-disciplinary pandemic project related to stay-at-home orders and social media moral frames. Unlike our typical design experience, we had to handle a steeper learning curve, emerging and continually changing datasets, as well as under-specified design requirements, persistent low visual literacy, and an extremely fast turnaround for new data ingestion, prototyping, testing and deployment. We describe the lessons learned through this experience.",
                        "uid": "w-Vis4PandEmRes-1013",
                        "time_stamp": "2023-10-21T23:00:00Z",
                        "time_start": "2023-10-21T23:00:00Z",
                        "time_end": "2023-10-22T00:15:00Z",
                        "paper_type": "workshop",
                        "keywords": [],
                        "has_image": "0",
                        "has_video": "0",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    },
                    {
                        "slot_id": "w-Vis4PandEmRes-1017",
                        "session_id": "w7",
                        "title": "ESID: Exploring the Design and Development of a Visual Analytics Tool for Epidemiological Emergencies",
                        "contributors": [
                            "Dr Pawandeep Kaur Betz"
                        ],
                        "authors": [],
                        "abstract": "Visual analytics tools can help illustrate the spread of infectious diseases and enable informed decisions on epidemiological and public health issues. To create visualisation tools that are intuitive, easy to use, and effective in communicating information, continued research and development focusing on user-centric and methodological design models is extremely important. As a contribution to this topic, this paper presents the design and development process of the visual analytics application ESID (Epidemiological Scenarios for Infectious Diseases ). ESID is a visual analytics tool aimed at projecting the future developments of infectious disease spread using reported and simulated data based on sound mathematical-epidemiological models. The development process involved a collaborative and participatory design approach with project partners from diverse scientific fields. The findings from these studies, along with the guidelines derived from them, played a pivotal role in shaping the visualisation tool.",
                        "uid": "w-Vis4PandEmRes-1017",
                        "time_stamp": "2023-10-21T23:00:00Z",
                        "time_start": "2023-10-21T23:00:00Z",
                        "time_end": "2023-10-22T00:15:00Z",
                        "paper_type": "workshop",
                        "keywords": [],
                        "has_image": "0",
                        "has_video": "0",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    },
                    {
                        "slot_id": "w-Vis4PandEmRes-1015",
                        "session_id": "w7",
                        "title": "EnsembleDashVis Views and Volunteers: A Retrospective and Early History",
                        "contributors": [
                            "Qiru Wang"
                        ],
                        "authors": [],
                        "abstract": "This paper offers a retrospective history of the early development stages of EnsembleDashVis, a visualization dashboard specifically crafted to support modelers in interpreting a simulation model utilized to forecast COVID-19 trends. The volunteer effort behind this dashboard was collaboratively contributed with the Scottish COVID-19 Response Consortium (SCRC), with the objective of enabling an enhanced comprehension of the complex dynamics of the pandemic through modeling of COVID-19 data collected by NHS Scotland during the first wave of the outbreak. This retrospective chronicles the design and development journey of the system, guided by feedback from domain experts, all taking place amidst the exceptional circumstances of an unprecedented pandemic. The outcome of this volunteer work is a streamlined relationship discovery process between sets of simulation input parameters and their respective outcomes, which leverages the power of information visualization and visual analytics (VIS). We hope that this retrospective will serve as an insightful resource for future effort, in VIS for pandemic and emergency responses and promote mutually beneficial engagement between scientific communities.",
                        "uid": "w-Vis4PandEmRes-1015",
                        "time_stamp": "2023-10-21T23:00:00Z",
                        "time_start": "2023-10-21T23:00:00Z",
                        "time_end": "2023-10-22T00:15:00Z",
                        "paper_type": "workshop",
                        "keywords": [],
                        "has_image": "0",
                        "has_video": "0",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    },
                    {
                        "slot_id": "w-Vis4PandEmRes-1018",
                        "session_id": "w7",
                        "title": "Visual Analytics based Search-Analyze-Forecast Framework for Epidemiological Time-series Data",
                        "contributors": [
                            "Cagatay Turkay"
                        ],
                        "authors": [],
                        "abstract": "The COVID-19 pandemic has been a period where time-series of disease statistics, such as the number of cases or vaccinations, have been intensively used by public health professionals to estimate how their region compares to others and estimate what future could look like at home. Conventional visualizations are often limited in terms of advanced comparative features and in supporting forecasting systematically. This paper presents a visual analytics approach to support data-driven prediction based on a search-analyze-predict process comprising a multi-metric, multi-criteria time-series search method and a data-driven prediction technique. These are supported by a visualization framework for the comprehensive comparison of multiple time-series. We inform the design of our approach by getting iterative feedback from public health experts globally, and evaluate it both quantitatively and qualitatively.",
                        "uid": "w-Vis4PandEmRes-1018",
                        "time_stamp": "2023-10-21T23:00:00Z",
                        "time_start": "2023-10-21T23:00:00Z",
                        "time_end": "2023-10-22T00:15:00Z",
                        "paper_type": "workshop",
                        "keywords": [],
                        "has_image": "0",
                        "has_video": "0",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    }
                ]
            }
        ]
    },
    "w-viscomm": {
        "event": "Sixth Workshop on Visualization for Communication (VisComm)",
        "long_name": "Sixth Workshop on Visualization for Communication (VisComm)",
        "event_type": "",
        "event_prefix": "w-viscomm",
        "event_description": "",
        "event_url": "",
        "organizers": [
            "Paul Parsons",
            "Jon Schwabish",
            "Alvitta Ottley",
            "Alice Feng"
        ],
        "sessions": [
            {
                "title": "Sixth Workshop on Visualization for Communication (VisComm)",
                "session_id": "w9",
                "event_prefix": "w-viscomm",
                "track": "105(234)",
                "session_image": "w9.png",
                "chair": [
                    "Paul Parsons",
                    "Jon Schwabish",
                    "Alvitta Ottley",
                    "Alice Feng"
                ],
                "organizers": [],
                "time_start": "2023-10-22T00:00:00Z",
                "time_end": "2023-10-22T05:15:00Z",
                "discord_category": "",
                "discord_channel": "",
                "discord_channel_id": "",
                "discord_link": "",
                "zoom_meeting": "",
                "zoom_password": "",
                "zoom_link": "",
                "time_slots": [
                    {
                        "slot_id": "w-viscomm-1002",
                        "session_id": "w9",
                        "title": "Visual Communication of Aftershock Forecasts Based on User Needs: A Case Study of the US, Mexico and El Salvador",
                        "contributors": [
                            "Max Schneider"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "w-viscomm-1002",
                        "time_stamp": "2023-10-22T04:00:00Z",
                        "time_start": "2023-10-22T04:00:00Z",
                        "time_end": "2023-10-22T05:15:00Z",
                        "paper_type": "workshop",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "1",
                        "paper_award": "",
                        "image_caption": "Example forecast map showing the probability of strong shaking due to aftershocks in the week following the 2010 M7.2 El Mayor-Cucapah earthquake in northern Mexico. Strong shaking is defined as a Modified Mercalli Intensity level of VI or higher.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    },
                    {
                        "slot_id": "w-viscomm-1007",
                        "session_id": "w9",
                        "title": "Towards an Online System to Generate Tailored Infographics: Supporting the Health Information Sharing Needs of Community-Based Organizations",
                        "contributors": [
                            "Dr. Adriana Arcia"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "w-viscomm-1007",
                        "time_stamp": "2023-10-22T04:00:00Z",
                        "time_start": "2023-10-22T04:00:00Z",
                        "time_end": "2023-10-22T05:15:00Z",
                        "paper_type": "workshop",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "1",
                        "paper_award": "",
                        "image_caption": "Excerpts from the COVID-19 testing infographic showing the tailoring features of TailorVis Toolbox.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    },
                    {
                        "slot_id": "w-viscomm-1009",
                        "session_id": "w9",
                        "title": "Explicating Implicit Frames: A Key to Communicating Visualization Successfully",
                        "contributors": [
                            "Prakash Chandra Shukla"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "w-viscomm-1009",
                        "time_stamp": "2023-10-22T04:00:00Z",
                        "time_start": "2023-10-22T04:00:00Z",
                        "time_end": "2023-10-22T05:15:00Z",
                        "paper_type": "workshop",
                        "keywords": [],
                        "has_image": "0",
                        "has_video": "0",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    },
                    {
                        "slot_id": "w-viscomm-1010",
                        "session_id": "w9",
                        "title": "Animating history: An energy Sankey movie, 1800\u20132019",
                        "contributors": [
                            "Nathan Matteson"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "w-viscomm-1010",
                        "time_stamp": "2023-10-22T04:00:00Z",
                        "time_start": "2023-10-22T04:00:00Z",
                        "time_end": "2023-10-22T05:15:00Z",
                        "paper_type": "workshop",
                        "keywords": [],
                        "has_image": "0",
                        "has_video": "0",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    },
                    {
                        "slot_id": "w-viscomm-1006",
                        "session_id": "w9",
                        "title": "Design of Visualization Onboarding Concepts for a 2D Scatterplot in a Biomedical Visual Analytics Tool",
                        "contributors": [
                            "Christina Stoiber"
                        ],
                        "authors": [],
                        "abstract": "Biomedical research is highly data-driven. Domain experts need to learn how to interpret complex data visualizations to gain insights. They often need help interpreting data visualizations as they are not part of their training. Integrating visualization onboarding concepts into visual analytics (VA) tools can support users in interpreting, reading, and extracting information from visual presentations. In this paper, we present the design of the onboarding concept for an interactive VA tool to analyze large scaled biological data, particularly high-throughput screening (HTS) data. We evaluated our onboarding design by conducting a cognitive walkthrough and interviews with thinking aloud. We also collected data on domain experts\u2019 visualization literacy. The results of the cognitive walkthrough showed that domain experts positively commented on the onboarding design and proposed adjusting smaller aspects. The interviews showed that domain experts are well-trained in interpreting basic visualizations (e.g., scatterplot, bar chart, line chart). However, they need support correctly interpreting the data visualized in the scatterplot, as they are new to them. Another important insight was fitting the onboarding messages into the domain\u2019s language.",
                        "uid": "w-viscomm-1006",
                        "time_stamp": "2023-10-22T04:00:00Z",
                        "time_start": "2023-10-22T04:00:00Z",
                        "time_end": "2023-10-22T05:15:00Z",
                        "paper_type": "workshop",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "0",
                        "paper_award": "",
                        "image_caption": "{\\rtf1\\ansi\\ansicpg1252\\cocoartf2709 \\cocoatextscaling0\\cocoaplatform0{\\fonttbl\\f0\\fswiss\\fcharset0 ArialMT;\\f1\\froman\\fcharset0 Times-Roman;} {\\colortbl;\\red255\\green255\\blue255;\\red0\\green0\\blue0;} {\\*\\expandedcolortbl;;\\cssrgb\\c0\\c0\\c0;} \\paperw11900\\paperh16840\\margl1440\\margr1440\\vieww11520\\viewh8400\\viewkind0 \\deftab720 \\pard\\pardeftab720\\sa320\\partightenfactor0  \\f0\\fs29\\fsmilli14667 \\cf0 \\expnd0\\expndtw0\\kerning0 \\outl0\\strokewidth0 \\strokec2 Onboarding navigation concept. The navigation comprises two elements: the floating action button (1) revealing three onboarding stages: Reading, Interacting, and Analyzing, and (2) a step-by-step navigation widget that represents onboarding messages and arrow icons to navigate through them. \\f1\\fs24 \\ }",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    },
                    {
                        "slot_id": "w-viscomm-1005",
                        "session_id": "w9",
                        "title": "Exploring Annotation Strategies in Professional Visualizations: Insights from Prominent US News Portals",
                        "contributors": [
                            "Md Dilshadur Rahman"
                        ],
                        "authors": [],
                        "abstract": "Annotations play a vital role in visualizations, providing valuable insights and focusing attention on critical visual elements.This study analyzes a curated corpus of 72 professionally designed static charts with annotations from prominent US news portals includingThe New York Times, The Economists, The Wall Street Journal, and The Washington Post. The analysis employed a qualitative approach involving identifying annotation types, assessing their frequency, exploring annotation combinations, categorizing text quantity, and examining the relationship between chart captions and annotations. The analysis reveals common patterns in annotation strategies used by professionals, including extensive use of annotations aligned with chart captions, targeted highlighting and descriptive text within charts, strategic utilization of multiple annotations as ensembles, and emphasis on article-related numerical values. These findings provide valuable guidance for improving annotation practices, tools, and methodologies, enhancing data comprehension and communication in visualizations.",
                        "uid": "w-viscomm-1005",
                        "time_stamp": "2023-10-22T04:00:00Z",
                        "time_start": "2023-10-22T04:00:00Z",
                        "time_end": "2023-10-22T05:15:00Z",
                        "paper_type": "workshop",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "0",
                        "paper_award": "",
                        "image_caption": "Examples of annotated professional charts used in prominent US news portals, including (a) a waterfall chart (i.e., a variant of a bar chart) utilizing gray highlights, connectors, and text descriptions; (b) a bar chart with directional marks, value text, descriptions, and context-specific color highlighting; (c) a scatterplot featuring a trend line and text; (d) a line chart with context-specific highlighting and value text; (e) another line chart using text descriptions, connectors, enclosures, and context-specific highlighting; and (f) a scatterplot with context-specific highlighting, data point labels, and a text description.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    }
                ]
            }
        ]
    },
    "w-eduvis": {
        "event": "EduVis: Workshop on Visualization Education, Literacy, and Activities",
        "long_name": "EduVis: Workshop on Visualization Education, Literacy, and Activities",
        "event_type": "",
        "event_prefix": "w-eduvis",
        "event_description": "",
        "event_url": "",
        "organizers": [
            "Mandy Keck",
            "Samuel Huron",
            "Georgia Panagiotidou",
            "Christina Stoiber",
            "Fateme Rajabiyazdi",
            "Charles Perin",
            "Jonathan C Roberts",
            "Benjamin Bach"
        ],
        "sessions": [
            {
                "title": "EduVis: Workshop on Visualization Education, Literacy, and Activities",
                "session_id": "w11",
                "event_prefix": "w-eduvis",
                "track": "104(132)",
                "session_image": "w11.png",
                "chair": [
                    "Mandy Keck",
                    "Samuel Huron",
                    "Georgia Panagiotidou",
                    "Christina Stoiber",
                    "Fateme Rajabiyazdi",
                    "Charles Perin",
                    "Jonathan C Roberts",
                    "Benjamin Bach"
                ],
                "organizers": [],
                "time_start": "2023-10-22T00:00:00Z",
                "time_end": "2023-10-23T00:15:00Z",
                "discord_category": "",
                "discord_channel": "",
                "discord_channel_id": "",
                "discord_link": "",
                "zoom_meeting": "",
                "zoom_password": "",
                "zoom_link": "",
                "time_slots": [
                    {
                        "slot_id": "w-eduvis-1001",
                        "session_id": "w11",
                        "title": "Reflections on Designing and Running Visualization Design and Programming Activities in Courses with Many Students",
                        "contributors": [
                            "S\u00f8ren Knudsen"
                        ],
                        "authors": [],
                        "abstract": "In this paper, we reflect on the educational challenges and research opportunities in running data visualization design activities in the context of large courses. With the increasing number and sizes of data visualization course, we need to better understand approaches to scaling our teaching efforts. We draw on experiences organizing and facilitating activities primarily based on one instance of a master's course given to about 130 students. We provide a detailed account of the course with particular focus on the purpose, structure, and outcome of six two-hour design activities. Based on this, we reflect on three aspects of the course: First, how the course scale led us to thoroughly plan, evaluate, and revise communication between students, teaching assistants, and lecturers. Second, how we designed learning scaffolds through the design activities, and the reflections we received from students on this matter. Finally, we reflect on the diversity of the students that followed the course, the visualization exercises we used, the projects they worked on, and when to key in on simple boring problems and data sets. Thus, our paper contributes with discussions about balancing topical diversity, scaling courses to many students, and problem-based learning.",
                        "uid": "w-eduvis-1001",
                        "time_stamp": "2023-10-22T23:00:00Z",
                        "time_start": "2023-10-22T23:00:00Z",
                        "time_end": "2023-10-23T00:15:00Z",
                        "paper_type": "workshop",
                        "keywords": [],
                        "has_image": "0",
                        "has_video": "0",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    },
                    {
                        "slot_id": "w-eduvis-1002",
                        "session_id": "w11",
                        "title": "Developing Technical Skillsets in Diverse Audiences",
                        "contributors": [
                            "Jonathan P. Leidig"
                        ],
                        "authors": [],
                        "abstract": "Designing courses that are taken by diverse student audiences is complicated by differing technical backgrounds and desired future roles. One proposed approach to course design bundles course assignments into parallel tracks for students to self-select and complete. An initial set of three tracks of computing laboratory-based assignments were developed and tested in the classroom. Each track option utilized different visualization software and libraries with a range of expected pre-requisite knowledge and technical abilities. Student and instructor reflections on the approach reinforced the suitability of a track-based course design to fulfill course objectives towards the education of diverse audiences.",
                        "uid": "w-eduvis-1002",
                        "time_stamp": "2023-10-22T23:00:00Z",
                        "time_start": "2023-10-22T23:00:00Z",
                        "time_end": "2023-10-23T00:15:00Z",
                        "paper_type": "workshop",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "0",
                        "paper_award": "",
                        "image_caption": "The paper discusses three tracks of laboratory assignments for visualization courses. Each track bundles multiple laboratory assignment covering one topical area. Students select and complete one track in the author's courses based on their own diverse backgrounds and personal interests.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    },
                    {
                        "slot_id": "w-eduvis-1003",
                        "session_id": "w11",
                        "title": "Choose-your-own D3 labs for learning to adapt online code",
                        "contributors": [
                            "Maryam Hedayati"
                        ],
                        "authors": [],
                        "abstract": "D3 is a popular library for implementing data visualizations, and is often taught in data visualization classes. However, D3 can be difficult to learn, and it can be especially challenging to make use of online examples, which often require changes to work in standalone JavaScript. We have previously taught D3 using guided tutorials, but found that students struggled to apply what they had learned to other visualization types or contexts. To address this, we introduced a new assignment type: choose-your-own labs. In each lab, students implemented a visualization technique from that week\u2019s lecture. We provided a code sample and asked students to get the code sample working in the latest version of D3 as a standalone webpage, sometimes with a new dataset. This paper reflects on our experiences using this new assignment. Although students seemed to find the process of debugging real-world example D3 code to be tedious, they generally responded well to the assignment. We also observed that the quality and creativity of the final group projects in the class were improved from previous iterations of the course. We provide suggestions for educators who want to use a similar format in their courses, and provide our materials at https://osf.io/47vfy/?view_only=7e478a23e3e5414086569694279d38fe.",
                        "uid": "w-eduvis-1003",
                        "time_stamp": "2023-10-22T23:00:00Z",
                        "time_start": "2023-10-22T23:00:00Z",
                        "time_end": "2023-10-23T00:15:00Z",
                        "paper_type": "workshop",
                        "keywords": [],
                        "has_image": "0",
                        "has_video": "0",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    },
                    {
                        "slot_id": "w-eduvis-1006",
                        "session_id": "w11",
                        "title": "Educational Data Comics: What can Comics do for Education in Visualization?",
                        "contributors": [
                            "Magdalena Boucher"
                        ],
                        "authors": [],
                        "abstract": "This paper discusses the potential of comics for explaining concepts with and around data visualization. With the increasing spread of visualizations and the democratization of access to visualization tools, we see a growing need for easily approachable resources for learning visualization techniques, applications, design processes, etc. Comics are a promising medium for such explanation as they concisely combine graphical and textual content in a sequential manner and they provide fast visual access to specific parts of the explanations. Based on a first literature review and our extensive experience with the subject, we survey works at the respective intersections of comics, visualization and education: data comics, educational comics, and visualization education. We report on five potentials of comics to create and share educational material, to engage wide and potentially diverse audiences, and to support educational activities. For each potential we list, we describe open questions for future research. Our discussion aims to inform both the application of comics by educators and their extension and study by researchers.",
                        "uid": "w-eduvis-1006",
                        "time_stamp": "2023-10-22T23:00:00Z",
                        "time_start": "2023-10-22T23:00:00Z",
                        "time_end": "2023-10-23T00:15:00Z",
                        "paper_type": "workshop",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "0",
                        "paper_award": "",
                        "image_caption": "What can comics do for Education in Visualization? The image shows two simple comic figures sitting at a table, studying a comic and talking about it. In the second panel, both express happiness and seem to have understood something.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    },
                    {
                        "slot_id": "w-eduvis-1007",
                        "session_id": "w11",
                        "title": "Design Actions for the Design of Visualization Onboarding Methods",
                        "contributors": [
                            "Christina Stoiber"
                        ],
                        "authors": [],
                        "abstract": "Integrating visualization onboarding methods into visual analytics tools presents challenges for designers and developers. These challenges include the varying complexity of visualization techniques, data types, and users\u2019 expertise levels. Selecting and integrating educational theories, ensuring the completeness and clarity of onboarding instructions, choosing the appropriate medium, and determining interaction techniques for exploration during onboarding are also problematic. However, there needs to be established design guidance specifically focused on visualization onboarding. Existing resources like VisGuides and design patterns offer some qualitative advice but lack scientific foundations. To address this gap, we propose nine design actions based on critical reflections of empirical studies, the development of a design space, and user studies. These design actions emphasize customizing onboarding experiences to address users\u2019 knowledge gaps, finding a balance between flexibility and structure, and incorporating concrete examples while considering potential limitations in knowledge transfer. Furthermore, we explore the connections, issues, and contradictions within the design actions. In conclusion, it is crucial to tailor onboarding experiences, balance flexibility and structure, and provide concrete examples while acknowledging knowledge transfer challenges.",
                        "uid": "w-eduvis-1007",
                        "time_stamp": "2023-10-22T23:00:00Z",
                        "time_start": "2023-10-22T23:00:00Z",
                        "time_end": "2023-10-23T00:15:00Z",
                        "paper_type": "workshop",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "0",
                        "paper_award": "",
                        "image_caption": "{\\rtf1\\ansi\\ansicpg1252\\cocoartf2709 \\cocoatextscaling0\\cocoaplatform0{\\fonttbl\\f0\\fswiss\\fcharset0 ArialMT;\\f1\\froman\\fcharset0 Times-Roman;} {\\colortbl;\\red255\\green255\\blue255;\\red0\\green0\\blue0;} {\\*\\expandedcolortbl;;\\cssrgb\\c0\\c0\\c0;} \\paperw11900\\paperh16840\\margl1440\\margr1440\\vieww11520\\viewh8400\\viewkind0 \\deftab720 \\pard\\pardeftab720\\sa320\\partightenfactor0  \\f0\\fs29\\fsmilli14667 \\cf0 \\expnd0\\expndtw0\\kerning0 \\outl0\\strokewidth0 \\strokec2 Overview of our proposed design actions categorized according to the guiding questions of our design space \\uc0\\u8232 The methodology we employ in our development of the design actions: \\u8232 First, we derive design guidelines and implications from empirical studies in visualization onboarding. Additionally, we revisit our study results and collect the lessons learned. As a final step, we develop design actions along the framework by De Bruijn and Spence [19], including: (1) description and title, the (2) effect of the design action in the context of onboarding, advantages, and trade-offs ((3) upside and (4) downside), (5) issues describing the application of the design action, and (6) references to cognitive theory are provided. Furthermore, we structure the design actions along the guiding ques- tions from our design space [71]: WHO is the user, and which knowl- edge gap does the user have? Which parts of a visualization need to be explained? How to phrase onboarding instructions? HOW, WHERE, and WHEN is visualization onboarding provided? \\f1\\fs24 \\ \\pard\\pardeftab720\\partightenfactor0 \\cf0 \\ }",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    },
                    {
                        "slot_id": "w-eduvis-1008",
                        "session_id": "w11",
                        "title": "dAn-oNo Learning Environment for Data Journalists Teaching Data Analytics Principles",
                        "contributors": [
                            "Christina Stoiber"
                        ],
                        "authors": [],
                        "abstract": "To derive narratives from data, several journalistic abilities are required, including the skill to discover and construct compelling stories (data storytelling), employ data-driven techniques to research and analyze information (data literacy), utilize visualization methods effectively (visualization literacy), and approaching data with a combination of creativity and critical thinking. Despite their expertise in journalism, journalists often encounter challenges in comprehending and utilizing novel visual representations or understanding data analysis methods. The main objective of the dAn-oNo learning environment is to guide journalists through the data analytics process by removing coding hurdles and allowing them to experiment with and understand the code. The learning environment utilizes a Jupyter notebook with Markdown sections and incorporates a step-by-step approach, covering various stages of the data analytics workflow, such as data importing, inspection, statistical analysis, and in-depth analysis. The learning environment also includes an automated profiler that translates warnings and information into human-understandable insights. The design and implementation of the dAn-oNo learning environment were informed by a literature review, user research, interviews with Austrian data journalists, a phase of exploring different technical possibilities for the learning environment, and rapid prototyping. The prototype is accessible here: https://github.com/stemrich/SEVA\\_DA-Onboarding-Tool",
                        "uid": "w-eduvis-1008",
                        "time_stamp": "2023-10-22T23:00:00Z",
                        "time_start": "2023-10-22T23:00:00Z",
                        "time_end": "2023-10-23T00:15:00Z",
                        "paper_type": "workshop",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "0",
                        "paper_award": "",
                        "image_caption": "{\\rtf1\\ansi\\ansicpg1252\\cocoartf2709 \\cocoatextscaling0\\cocoaplatform0{\\fonttbl\\f0\\fswiss\\fcharset0 ArialMT;\\f1\\froman\\fcharset0 Times-Roman;} {\\colortbl;\\red255\\green255\\blue255;\\red0\\green0\\blue0;} {\\*\\expandedcolortbl;;\\cssrgb\\c0\\c0\\c0;} \\paperw11900\\paperh16840\\margl1440\\margr1440\\vieww11520\\viewh8400\\viewkind0 \\deftab720 \\pard\\pardeftab720\\sa320\\partightenfactor0  \\f0\\fs29\\fsmilli14667 \\cf0 \\expnd0\\expndtw0\\kerning0 \\outl0\\strokewidth0 \\strokec2 Automated data set analysis using the Python library pandas_profiling delivering extensive insight, often overwhelming for DA novices \\f1\\fs24 \\  \\f0\\fs29\\fsmilli14667 Design Process of the dAn-oNo learning environment: (1) selection of an easy-to-understand data set & exploration and validation of technical possibilities for the implementation to overcome the hurdle of coding, support understanding of pitfalls and challenges of data analytics, easy access, and step-wise instructions and feedback. We meet the needs of the journalists by utilizing Jupyter and Markdown, binder, and GitHub, the learning environment. (2) The second step was the development of the structure and content, including the following steps: importing data to the notebook, inspecting the data set, statistical analysis, and in-depth analysis. (3) The final step was implementing the dAn-oNo learning environment, including developing the core component of translating the warnings and information delivered by the automated profiler into human-understandable information. \\f1\\fs24 \\ }",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    },
                    {
                        "slot_id": "w-eduvis-1011",
                        "session_id": "w11",
                        "title": "Beyond Generating Code: Evaluating GPT on a Data Visualization Course",
                        "contributors": [
                            "Zhutian Chen"
                        ],
                        "authors": [],
                        "abstract": "This paper presents an empirical evaluation of the performance of the Generative Pre-trained Transformer (GPT) model in Harvard\u2019s CS171 data visualization course. While previous studies have focused on GPT\u2019s ability to generate code for visualizations, this study goes beyond code generation to evaluate GPT\u2019s abilities in various visualization tasks, such as data interpretation, visualization design, visual data exploration, and insight communication. The evaluation utilized GPT-3.5 and GPT-4 through the APIs of OpenAI to complete assignments of CS171, and included a quantitative assessment based on the established course rubrics, a qualitative analysis informed by the feedback of three experienced graders, and an exploratory study of GPT\u2019s capabilities in completing border visualization tasks. Findings show that GPT-4 scored 80% on quizzes and homework, and Teaching Fellows could distinguish between GPT- and human-generated homework with 70% accuracy. The study also demonstrates GPT\u2019s potential in completing various visualization tasks, such as data cleanup, interaction with visualizations, and insight communication. The paper concludes by discussing the strengths and limitations of GPT in data visualization, potential avenues for incorporating GPT in broader visualization tasks, and the need to redesign visualization education.",
                        "uid": "w-eduvis-1011",
                        "time_stamp": "2023-10-22T23:00:00Z",
                        "time_start": "2023-10-22T23:00:00Z",
                        "time_end": "2023-10-23T00:15:00Z",
                        "paper_type": "workshop",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "0",
                        "paper_award": "",
                        "image_caption": "Our experiments show that GPT can a) clean and explore CSV datasets, b) read visualizations in SVG format, interact with visualizations through dispatching Javascript events, and c) create explanatory visualizations to present data insights.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    },
                    {
                        "slot_id": "w-eduvis-1014",
                        "session_id": "w11",
                        "title": "Preparing Future Data Visualization Designers for Professional Practice",
                        "contributors": [
                            "Dr Paul Parsons"
                        ],
                        "authors": [],
                        "abstract": "As the professional field of data visualization grows, so does the importance of preparing students effectively for the demands of real-world practice. Computing education has historically sought to teach and evaluate abstract knowledge (e.g., theories, principles, guidelines, design patterns) and the application of such knowledge to given problems. However, situations faced in professional practice are often messy, dynamic, and uncertain, and do not lend themselves well to the clear and direct application of such knowledge. This leaves a gap between the knowledge learned in the classroom and what is required for skillful practice in professional settings. In this paper, I discuss some historical reasons for this dominant pedagogical perspective, some of the core features of professional practice that are not typically taught in classrooms, and ways in which data visualization design can be taught to be more resonant with the experience of professional practice.",
                        "uid": "w-eduvis-1014",
                        "time_stamp": "2023-10-22T23:00:00Z",
                        "time_start": "2023-10-22T23:00:00Z",
                        "time_end": "2023-10-23T00:15:00Z",
                        "paper_type": "workshop",
                        "keywords": [],
                        "has_image": "0",
                        "has_video": "0",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    },
                    {
                        "slot_id": "w-eduvis-1018",
                        "session_id": "w11",
                        "title": "Reflections on Teaching \u2018Data Exploration and Visualisation\u2019 in Multiple Modes",
                        "contributors": [
                            "Sarah Goodwin"
                        ],
                        "authors": [],
                        "abstract": "Whilst the visual arts is a very practical and tangible field, data visualisation is a combination of programming skills, data science and design theory. Each of these can be taught in a variety of ways, from traditional methods of lectures to technical programming classes or even asynchronous worksheets. The challenge is how to cover all of the required content within the constraints of the teaching period, environment, delivery method and ensuring the appropriate activities and assessment tasks. In this paper we reflect on the past 8 years of delivering the postgraduate unit \u2018Data Exploration and Visualisation\u2019 at Monash University. We present the four different ways that the same content has been taught to different cohorts, through a combination of on-campus lectures, large-format workshops, small group tutorials and online or hybrid tutorials and workshops, as well as various readings, videos and asynchronous activities. We explain the different external and internal requirements set for the teaching and describe the adaption of the teaching methods and assessments in order to meet these conditions. We reflect on our experiences of teaching multiple methods across different formats from very small to very large student cohorts.",
                        "uid": "w-eduvis-1018",
                        "time_stamp": "2023-10-22T23:00:00Z",
                        "time_start": "2023-10-22T23:00:00Z",
                        "time_end": "2023-10-23T00:15:00Z",
                        "paper_type": "workshop",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "0",
                        "paper_award": "",
                        "image_caption": "Data visualisation teaching at Monash University in multiple modes, represented by an image of points plotted on a vertical map, overlaid with a capital M on the left and the right.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    }
                ]
            }
        ]
    },
    "a-ldav": {
        "event": "LDAV: 13th IEEE Symposium on Large Data Analysis and Visualization",
        "long_name": "LDAV: 13th IEEE Symposium on Large Data Analysis and Visualization",
        "event_type": "",
        "event_prefix": "a-ldav",
        "event_description": "",
        "event_url": "",
        "organizers": [
            "Peer-Timo Bremer",
            "Kristi Potter",
            "Steffen Frey",
            "Silvio Rizzi",
            "Gunther Weber"
        ],
        "sessions": [
            {
                "title": "LDAV: 13th IEEE Symposium on Large Data Analysis and Visualization",
                "session_id": "ae2",
                "event_prefix": "a-ldav",
                "track": "106(234)",
                "session_image": "ae2.png",
                "chair": [
                    "Peer-Timo Bremer",
                    "Kristi Potter",
                    "Steffen Frey",
                    "Silvio Rizzi",
                    "Gunther Weber"
                ],
                "organizers": [],
                "time_start": "2023-10-22T00:00:00Z",
                "time_end": "2023-10-23T00:15:00Z",
                "discord_category": "",
                "discord_channel": "",
                "discord_channel_id": "",
                "discord_link": "",
                "zoom_meeting": "",
                "zoom_password": "",
                "zoom_link": "",
                "time_slots": [
                    {
                        "slot_id": "a-ldav-1002",
                        "session_id": "ae2",
                        "title": "Speculative Progressive Raycasting for Memory Constrained Isosurface Visualization of Massive Volumes",
                        "contributors": [
                            "Will Usher"
                        ],
                        "authors": [],
                        "abstract": "New web technologies have enabled the deployment of powerful GPU-based computational pipelines that run entirely in the web browser, opening a new frontier for accessible scientific visualization applications. However, these new capabilities do not address the memory constraints of lightweight end-user devices encountered when attempting to visualize the massive data sets produced by today\u2019s simulations and data acquisition systems. In this paper, we propose a novel implicit isosurface rendering algorithm for interactive visualization of massive volumes within a small memory footprint. We achieve this by progressively traversing a wavefront of rays through the volume and decompressing blocks of the data on-demand to perform implicit ray-isosurface intersections. The progressively rendered surface is displayed after each pass to improve interactivity. Furthermore, to accelerate rendering and increase GPU utilization, we introduce speculative ray-block intersection into our algorithm, where additional blocks are traversed and intersected speculatively along rays as other rays terminate to exploit additional parallelism in the workload. Our entire pipeline is run in parallel on the GPU to leverage the parallel computing power that is available even on lightweight end-user devices. We compare our algorithm to the state of the art in low-overhead isosurface extraction and demonstrate that it achieves 1.7\u00d7\u20135.7\u00d7 reductions in memory overhead and up to 8.4\u00d7 reductions in data decompressed.",
                        "uid": "a-ldav-1002",
                        "time_stamp": "2023-10-22T23:00:00Z",
                        "time_start": "2023-10-22T23:00:00Z",
                        "time_end": "2023-10-23T00:15:00Z",
                        "paper_type": "associated",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "0",
                        "paper_award": "",
                        "image_caption": "Interactive full-resolution isosurface visualization of the 2048x2048x1920 Richtmyer-Meshkov (R-M) data set in the browser. We propose a new GPU algorithm for implicit isosurface rendering that progressively traverses rays through the volume and decompresses data on-demand to minimize its memory footprint. We achieve up to 5.7x reductions in overall memory use and 8.4\u00d7 reductions in data decompressed compared to the state of the art in memory constrained isosurface extraction, without sacrificing interactivity. At 1280x720, the Richtmyer-Meshkov averages 264ms per-pass and 1.2s total on an RTX 3080.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    },
                    {
                        "slot_id": "a-ldav-1003",
                        "session_id": "ae2",
                        "title": "Low-Cost Post Hoc Reconstruction of HPC Simulations at Full Resolution",
                        "contributors": [
                            "Ayman Yousef"
                        ],
                        "authors": [],
                        "abstract": "High performance computing has played a pivotal and ongoing role in the field of computational fluid dynamics, enabling the simulation of increasingly larger-scale models. However, this rapid growth in model and simulation size has outpaced the capabilities of input/output (I/O) operations. Consequently, the conventional approach of saving and outputting data to persistent storage for analysis has become increasingly challenging, limiting the benefits of these advanced models. To address this challenge, we present a method for effectively handling massive-scale simulation data, ensuring its persistence at full spatial and temporal resolution for flexible post hoc analysis. We employ an in situ approach that captures interprocess communicated data, compressing cached data to a fraction of the overall simulation domain. We successfully reconstruct subdomains at full spatial and temporal resolution during post-processing through communication-free rerun using the cached halos. We detail the storage requirements of the new approach and demonstrate the substantial reductions in computational resources required to precisely recapitulate data within a local region of interest.",
                        "uid": "a-ldav-1003",
                        "time_stamp": "2023-10-22T23:00:00Z",
                        "time_start": "2023-10-22T23:00:00Z",
                        "time_end": "2023-10-23T00:15:00Z",
                        "paper_type": "associated",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "0",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    },
                    {
                        "slot_id": "a-ldav-1013",
                        "session_id": "ae2",
                        "title": "Sub-Linear Time Sampling Approach for Large-Scale Data Visualization Using Reinforcement Learning",
                        "contributors": [
                            "Ayan Biswas"
                        ],
                        "authors": [],
                        "abstract": "As the compute capabilities of the modern supercomputers continue to rise, domain scientists are able to run their simulations at very fine spatial and temporal resolutions. Compared to the compute speeds, the I/O bandwidth continues to lag by orders of magnitude. This necessitates that the analysis and data reduction are performed in situ while the simulation generated data is still at the supercomputer memory. Recently, intelligent data-driven sampling schemes have been proposed by visualization researchers. These sampling methods are scalable, in situ capable and able to identify the feature regions of the data. Although powerful, these sampling schemes need to traverse through the data sets at least twice, which can become problematic as the simulations start to touch the exascale capabilities. In this paper, we propose to use a reinforcement learning-based approach to devise a sub-linear time sampling algorithm. Using multi-arm bandits, we show that we can identify samples that are of similar or better quality compared to the existing methods while only touching a small fraction of the original data. We use multiple simulation data sets to show the efficacy of our proposed method.",
                        "uid": "a-ldav-1013",
                        "time_stamp": "2023-10-22T23:00:00Z",
                        "time_start": "2023-10-22T23:00:00Z",
                        "time_end": "2023-10-23T00:15:00Z",
                        "paper_type": "associated",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "0",
                        "paper_award": "",
                        "image_caption": "We propose a reinforcement learning-based intelligent sampling scheme using multi-arm bandits. Using this sublinear sampling strategy, we touch less than 30% of the original data and produce samples of similar quality as the existing sampling schemes that need to iterate the original data two times.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    },
                    {
                        "slot_id": "a-ldav-1014",
                        "session_id": "ae2",
                        "title": "A Distributed-Memory Parallel Approach for Volume Rendering with Shadows",
                        "contributors": [
                            "Manish Mathai"
                        ],
                        "authors": [],
                        "abstract": "We present a parallel, distributed-memory technique that enhances traditional ray-casting volume rendering of large data sets to highlight the depth and perception of interesting volumetric features. The technique introduces a lighting system that accounts for global shadows across distributed MPI nodes while using shared-memory parallelism within each node to compute shading information efficiently. The first stage of the approach involves estimating energy attenuation from a point light source through the global volume, using a reduced spatial resolution representation of the volume, with minimal global communication between nodes. It is then used in the second stage during volume rendering to shade sample points captured during ray-casting, generating a high-quality image. In this work, we study the technique's performance across varying spatial resolutions of the estimated light attenuation using synthetic and real-world volumetric data sets on distributed systems.",
                        "uid": "a-ldav-1014",
                        "time_stamp": "2023-10-22T23:00:00Z",
                        "time_start": "2023-10-22T23:00:00Z",
                        "time_end": "2023-10-23T00:15:00Z",
                        "paper_type": "associated",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "0",
                        "paper_award": "",
                        "image_caption": "Volume renderings of four data sets using shadows. From left to right, the data sets are Perlin Noise, Rayleigh-Taylor Instability, Richtmyer-Meshhkov Instability, and Rotating Stratified Turbulence.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    },
                    {
                        "slot_id": "a-ldav-1015",
                        "session_id": "ae2",
                        "title": "Towards Adaptive Refinement for Multivariate Functional Approximation of Scientific Data",
                        "contributors": [
                            "Tom Peterka"
                        ],
                        "authors": [],
                        "abstract": "We investigate a data model for adaptive refinement in multivariate functional approximation of scientific data, based on a mesh of varying-resolution tensor products, and offering reduced size compared with a single tensor product representation. The mesh of tensor products adjoins in irregular fashion to tessellate the entire domain, with high-degree continuity across tensor product boundaries. The result is that regions of refinement, with additional knots and control points, are localized to individual tensor products rather than extending throughout the entire domain. The model attains similar accuracy with fewer total control points than a single tensor product model, at the cost of added computational complexity to manage the continuity and accuracy across tensor products. We describe our high-dimensional data organization and demonstrate how to approximate scientific datasets using our data model. Size and speed are compared between a single tensor product and our representation. Initial results demonstrate correct functionality and modest reduction in the number of control points required to attain comparable accuracy as a single tensor, with increased computation time as expected. Our initial findings indicate two avenues for future research: additional tuning of the adaptive refinement algorithm to reduce size further, and accelerating computation through parallelism.",
                        "uid": "a-ldav-1015",
                        "time_stamp": "2023-10-22T23:00:00Z",
                        "time_start": "2023-10-22T23:00:00Z",
                        "time_end": "2023-10-23T00:15:00Z",
                        "paper_type": "associated",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "0",
                        "paper_award": "",
                        "image_caption": "Left: adding one control point to a single tensor product generates additional redundant control points in order to maintain the definition of a tensor product. Right: in this paper we propose a mesh of different resolution tensor products that limits the number of extra control points required.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    }
                ]
            }
        ]
    },
    "a-biomedchallenge": {
        "event": "Bio+MedVis Challenges",
        "long_name": "Bio+MedVis Challenges",
        "event_type": "",
        "event_prefix": "a-biomedchallenge",
        "event_description": "",
        "event_url": "",
        "organizers": [
            "Barbora Kozlikova",
            "Daniel J\u00f6nsson",
            "Renata Raidou",
            "Sean O\u2019Donoghue"
        ],
        "sessions": [
            {
                "title": "Bio+MedVis Challenges",
                "session_id": "c1",
                "event_prefix": "a-biomedchallenge",
                "track": "111-112(140)",
                "session_image": "c1.png",
                "chair": [
                    "Barbora Kozlikova",
                    "Daniel J\u00f6nsson",
                    "Renata Raidou",
                    "Sean O\u2019Donoghue"
                ],
                "organizers": [],
                "time_start": "2023-10-22T00:00:00Z",
                "time_end": "2023-10-23T00:15:00Z",
                "discord_category": "",
                "discord_channel": "",
                "discord_channel_id": "",
                "discord_link": "",
                "zoom_meeting": "",
                "zoom_password": "",
                "zoom_link": "",
                "time_slots": [
                    {
                        "slot_id": "a-biomedchallenge-7741",
                        "session_id": "c1",
                        "title": "CytoCave: An Interactive Visualization Tool for Exploring Protein and Drug Interaction Networks",
                        "contributors": [
                            "Morris Chukhman"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "a-biomedchallenge-7741",
                        "time_stamp": "2023-10-22T23:00:00Z",
                        "time_start": "2023-10-22T23:00:00Z",
                        "time_end": "2023-10-23T00:15:00Z",
                        "paper_type": "associated",
                        "keywords": [],
                        "has_image": "0",
                        "has_video": "0",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    },
                    {
                        "slot_id": "a-biomedchallenge-5794",
                        "session_id": "c1",
                        "title": "PepProEx - Peptide and Protein Exploration Framework",
                        "contributors": [
                            "Vikash Prasad"
                        ],
                        "authors": [],
                        "abstract": "We present a visual analytics framework, PepProEx, for unraveling peptide and protein interactions. In the framework, users can understand holistic trends, discover patterns, and get insights from the peptide/protein intensity data. Our framework provides multiple linked views to navigate intricate tissue/cancer-related dynamics. We enable researchers to shift effortlessly from an overview of the input data to granular statistical insights.",
                        "uid": "a-biomedchallenge-5794",
                        "time_stamp": "2023-10-22T23:00:00Z",
                        "time_start": "2023-10-22T23:00:00Z",
                        "time_end": "2023-10-23T00:15:00Z",
                        "paper_type": "associated",
                        "keywords": [],
                        "has_image": "0",
                        "has_video": "0",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    },
                    {
                        "slot_id": "a-biomedchallenge-3267",
                        "session_id": "c1",
                        "title": "ProtEGOnist \u2013 Exploration of protein-protein interactions using ego-graph networks",
                        "contributors": [
                            "Mathias Witte Paz"
                        ],
                        "authors": [],
                        "abstract": "The complexity of protein-protein interaction (PPI) networks often leads to visual clutter and limited interpretability. To overcome these problems, we present ProtEGOnist, a novel, interactive visualization approach designed to explore PPI networks with a focus on drug-protein associations. ProtEGOnist addresses the challenges by introducing the concept of ego-graphs to represent local PPI neighborhoods around proteins of interest. These ego-graphs are aggregated into an ego-graph network, where edges between ego-graphs encoded their similarity using the Jaccard index. Our proposed visualization design offers an overview of drug-associated proteins, radar charts to compare protein functions, and detailed ego-graph subnetworks for interactive exploration. Our aim was to reduce visual complexity while enabling detailed exploration, facilitating the discovery of meaningful patterns in PPI networks. A web-based prototype of ProtEGOnist is available for interactive use.",
                        "uid": "a-biomedchallenge-3267",
                        "time_stamp": "2023-10-22T23:00:00Z",
                        "time_start": "2023-10-22T23:00:00Z",
                        "time_end": "2023-10-23T00:15:00Z",
                        "paper_type": "associated",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "0",
                        "paper_award": "",
                        "image_caption": "ProtEGOnist focuses on comparing the local protein-protein interaction neighborhoods - called ego-graphs - of proteins of interest. It consists of three views: (1) a network of ego-graphs of the proteins of interest, (2) an ego-graph subnetwork of ego-graphs selected from the overview that provides more details on the ego-graphs and their overlap when decollapsing them, and (3) a radar chart showing the similarity between a selected ego-graph and its neighbor ego-graphs, which are grouped by the function of the central protein using the BRITE hierarchy. All views are linked and provide details on demand.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    }
                ]
            }
        ]
    },
    "a-scivis-contest": {
        "event": "SciVis Contest",
        "long_name": "SciVis Contest",
        "event_type": "",
        "event_prefix": "a-scivis-contest",
        "event_description": "",
        "event_url": "",
        "organizers": [
            "Tim Gerrits",
            "Divya Banesh"
        ],
        "sessions": [
            {
                "title": "SciVis Contest",
                "session_id": "c3",
                "event_prefix": "a-scivis-contest",
                "track": "111-112(140)",
                "session_image": "c3.png",
                "chair": [
                    "Tim Gerrits",
                    "Divya Banesh"
                ],
                "organizers": [],
                "time_start": "2023-10-22T00:00:00Z",
                "time_end": "2023-10-23T05:15:00Z",
                "discord_category": "",
                "discord_channel": "",
                "discord_channel_id": "",
                "discord_link": "",
                "zoom_meeting": "",
                "zoom_password": "",
                "zoom_link": "",
                "time_slots": [
                    {
                        "slot_id": "a-scivis-contest-1001",
                        "session_id": "c3",
                        "title": "Immersive Exploration of Brain Simulation Data",
                        "contributors": [
                            "Hogr\u00e4fer, Marius"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "a-scivis-contest-1001",
                        "time_stamp": "2023-10-23T04:00:00Z",
                        "time_start": "2023-10-23T04:00:00Z",
                        "time_end": "2023-10-23T05:15:00Z",
                        "paper_type": "associated",
                        "keywords": [],
                        "has_image": "0",
                        "has_video": "0",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    },
                    {
                        "slot_id": "a-scivis-contest-1005",
                        "session_id": "c3",
                        "title": "PlastiVis: An Interactive Visualization Tool for Synaptic Networks",
                        "contributors": [
                            "Dang, Tommy"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "a-scivis-contest-1005",
                        "time_stamp": "2023-10-23T04:00:00Z",
                        "time_start": "2023-10-23T04:00:00Z",
                        "time_end": "2023-10-23T05:15:00Z",
                        "paper_type": "associated",
                        "keywords": [],
                        "has_image": "0",
                        "has_video": "0",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    },
                    {
                        "slot_id": "a-scivis-contest-1007",
                        "session_id": "c3",
                        "title": "NeuroViz: Visual Analytics of Neural Behavior in Temporal Plasticity Changes",
                        "contributors": [
                            "Han, Xiaoyang"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "a-scivis-contest-1007",
                        "time_stamp": "2023-10-23T04:00:00Z",
                        "time_start": "2023-10-23T04:00:00Z",
                        "time_end": "2023-10-23T05:15:00Z",
                        "paper_type": "associated",
                        "keywords": [],
                        "has_image": "0",
                        "has_video": "0",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    },
                    {
                        "slot_id": "a-scivis-contest-1010",
                        "session_id": "c3",
                        "title": "An Interactive Visualization for Neuronal Network Simulations of Plasticity Changes in the Human Brain",
                        "contributors": [
                            "Lawonn, Kai"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "a-scivis-contest-1010",
                        "time_stamp": "2023-10-23T04:00:00Z",
                        "time_start": "2023-10-23T04:00:00Z",
                        "time_end": "2023-10-23T05:15:00Z",
                        "paper_type": "associated",
                        "keywords": [],
                        "has_image": "0",
                        "has_video": "0",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    },
                    {
                        "slot_id": "a-scivis-contest-1011",
                        "session_id": "c3",
                        "title": "Exploring Synaptic Plasticity with NeuroCavePlus",
                        "contributors": [
                            "Chukhman, Morris"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "a-scivis-contest-1011",
                        "time_stamp": "2023-10-23T04:00:00Z",
                        "time_start": "2023-10-23T04:00:00Z",
                        "time_end": "2023-10-23T05:15:00Z",
                        "paper_type": "associated",
                        "keywords": [],
                        "has_image": "0",
                        "has_video": "0",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    },
                    {
                        "slot_id": "a-scivis-contest-1012",
                        "session_id": "c3",
                        "title": "VisAnywhere: Developing Multi-platform Scientific Visualization Applications",
                        "contributors": [
                            "Marrinan, Thomas"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "a-scivis-contest-1012",
                        "time_stamp": "2023-10-23T04:00:00Z",
                        "time_start": "2023-10-23T04:00:00Z",
                        "time_end": "2023-10-23T05:15:00Z",
                        "paper_type": "associated",
                        "keywords": [],
                        "has_image": "0",
                        "has_video": "0",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    }
                ]
            }
        ]
    },
    "t-analysis": {
        "event": "Visualization Analysis and Design",
        "long_name": "Visualization Analysis and Design",
        "event_type": "",
        "event_prefix": "t-analysis",
        "event_description": "",
        "event_url": "",
        "organizers": [
            "Tamara Munzner"
        ],
        "sessions": [
            {
                "title": "Visualization Analysis and Design",
                "session_id": "t2",
                "event_prefix": "t-analysis",
                "track": "101-102(140)",
                "session_image": "t2.png",
                "chair": [
                    "Tamara Munzner"
                ],
                "organizers": [],
                "time_start": "2023-10-22T00:00:00Z",
                "time_end": "2023-10-22T00:15:00Z",
                "discord_category": "",
                "discord_channel": "",
                "discord_channel_id": "",
                "discord_link": "",
                "zoom_meeting": "",
                "zoom_password": "",
                "zoom_link": "",
                "time_slots": []
            }
        ]
    },
    "t-ttk": {
        "event": "A Hands-on TTK Tutorial for Absolute Beginners",
        "long_name": "A Hands-on TTK Tutorial for Absolute Beginners",
        "event_type": "",
        "event_prefix": "t-ttk",
        "event_description": "",
        "event_url": "",
        "organizers": [
            "Julien Tierny"
        ],
        "sessions": [
            {
                "title": "A Hands-on TTK Tutorial for Absolute Beginners",
                "session_id": "t5",
                "event_prefix": "t-ttk",
                "track": "106(234)",
                "session_image": "t5.png",
                "chair": [
                    "Julien Tierny"
                ],
                "organizers": [],
                "time_start": "2023-10-22T00:00:00Z",
                "time_end": "2023-10-22T00:15:00Z",
                "discord_category": "",
                "discord_channel": "",
                "discord_channel_id": "",
                "discord_link": "",
                "zoom_meeting": "",
                "zoom_password": "",
                "zoom_link": "",
                "time_slots": []
            }
        ]
    },
    "w-nlviz": {
        "event": "NLVIZ Workshop: Exploring Research Opportunities for Natural Language, Text, and Data Visualization",
        "long_name": "NLVIZ Workshop: Exploring Research Opportunities for Natural Language, Text, and Data Visualization",
        "event_type": "",
        "event_prefix": "w-nlviz",
        "event_description": "",
        "event_url": "",
        "organizers": [
            "Vidya Setlur",
            "Arjun Srinivasan"
        ],
        "sessions": [
            {
                "title": "NLVIZ Workshop: Exploring Research Opportunities for Natural Language, Text, and Data Visualization",
                "session_id": "w5",
                "event_prefix": "w-nlviz",
                "track": "110(234)",
                "session_image": "w5.png",
                "chair": [
                    "Vidya Setlur",
                    "Arjun Srinivasan"
                ],
                "organizers": [],
                "time_start": "2023-10-22T00:00:00Z",
                "time_end": "2023-10-22T05:15:00Z",
                "discord_category": "",
                "discord_channel": "",
                "discord_channel_id": "",
                "discord_link": "",
                "zoom_meeting": "",
                "zoom_password": "",
                "zoom_link": "",
                "time_slots": [
                    {
                        "slot_id": "w-nlviz-1011",
                        "session_id": "w5",
                        "title": "WEC-Explainer: A Descriptive Framework for Exploring Word Embedding Contextualization",
                        "contributors": [
                            "Rita Sevastjanova"
                        ],
                        "authors": [],
                        "abstract": "Contextual word embeddings -- high-dimensional vectors that encode the semantic similarity between words -- are proven to be effective for diverse natural language processing applications. These embeddings originate in large language models and are updated throughout the model's architecture (i.e., the model's layers). Given their intricacy, the explanation of embedding characteristics and limitations -- their contextualization -- has emerged as a widely investigated research subject. To provide an overview of the existing explanation methods and motivate researchers to design new approaches, we present a descriptive framework that connects data, features, tasks, and users involved in the word embedding explanation process. We use the framework as theoretical groundwork and implement a data processing pipeline that we use to solve three different tasks related to word embedding contextualization. These tasks enable answering questions about the encoded context properties in the embedding vectors, captured semantic concepts and their similarity, and masked-prediction meaningfulness and their relation to embedding characteristics. We show that divergent research questions can be analyzed by combining different data curation methods with a similar set of features.",
                        "uid": "w-nlviz-1011",
                        "time_stamp": "2023-10-22T04:00:00Z",
                        "time_start": "2023-10-22T04:00:00Z",
                        "time_end": "2023-10-22T05:15:00Z",
                        "paper_type": "workshop",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "0",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    },
                    {
                        "slot_id": "w-nlviz-1014",
                        "session_id": "w5",
                        "title": "From Natural Language to Data Visualization (NL2VIS) with Large Language Model and Pattern Matching",
                        "contributors": [
                            "Dr. Zana Vosough"
                        ],
                        "authors": [],
                        "abstract": "In the rapidly evolving domain of artificial intelligence, Large Language Models (LLMs) such as ChatGPT have made remarkable advancements, revolutionizing how users consume and generate content. The current research on LLM-based generative tools has primarily been concentrated on the generation of text, codes, or images. This paper presents an investigation into the application of LLMs for automating the creation of data visualizations from natural language queries. The proposed approach leverages LLMs for generating data queries, coupling this capability with a pattern matching strategy aimed at creating visualizations ( eventually auto-generated dashboards) based on the extracted data. Furthermore, we introduce a novel domain-specific data visualization language (DVL). This DVL can be effortlessly translated into executable code compatible with various data visualization libraries. This exploration into the confluence of visualization, NLP, and human-computer interaction, is substantiated by our work in a practical industrial context, enriching a live product. Our findings aspire to contribute to the ongoing dialogue of how NLP techniques and interactive visualizations can be harmonized to bolster data-driven communication and analytical discourse.",
                        "uid": "w-nlviz-1014",
                        "time_stamp": "2023-10-22T04:00:00Z",
                        "time_start": "2023-10-22T04:00:00Z",
                        "time_end": "2023-10-22T05:15:00Z",
                        "paper_type": "workshop",
                        "keywords": [],
                        "has_image": "0",
                        "has_video": "0",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    },
                    {
                        "slot_id": "w-nlviz-1010",
                        "session_id": "w5",
                        "title": "Unveiling Insights: Surfacing Fine-Grained Discourse Acts in Short Free-Form Public Input Text through Visual Analytics",
                        "contributors": [
                            "Mahmood Jasim"
                        ],
                        "authors": [],
                        "abstract": "Exploratory analysis of public-generated short free-form online text often plays a critical role in facilitating decision-making across various domains. Particularly, in the civic domain, analyzing and understanding the thoughts, opinions, and comments shared by the public on social media or online engagement platforms on various civic issues are crucial for tracking consensus and making informed policy decisions. However, the public inputs are often short, redundant, unstructured, and full of nuances and ambiguity with a lack of clear boundary between positive and negative stances, which demands significant time and effort to analyze. Coupled with a lack of guidelines to visualize short free-form text, designing visualizations to show meaningful insights from public input while preserving the context and semantic values remains a challenging task. In this work, we explored discourse acts as a fine-grained categorization to characterize public input beyond positive or negative stances. Furthermore, we applied McDonald's greedy approximation of global inference to prioritize the relevance and negate the redundancy present in the text. We integrated these approaches with an interactive prototype called Matryona that employs visualization techniques to provide a contextual summary of unstructured public input and enables multilayered exploration from different angles by controlling the information content. Our initial evaluation of the prototype suggests Matryona's potential for reducing ambiguity and eliciting nuances present in the short free-form text by using discourse acts and accelerating the analysis process by ranking text comments.",
                        "uid": "w-nlviz-1010",
                        "time_stamp": "2023-10-22T04:00:00Z",
                        "time_start": "2023-10-22T04:00:00Z",
                        "time_end": "2023-10-22T05:15:00Z",
                        "paper_type": "workshop",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "0",
                        "paper_award": "",
                        "image_caption": "Matryona's detail view provides a dropdown to select discussions. The bar on the left represents the topic percentage contribution of the extracted topic towards the discussion. The detail view also present the topic key phrases and keywords. Next, there is a set of circles representing the discourse act distribution for each topic. There is a slider bar to control the number of ranked text comments to display. Finally, the comments associated with the filters are shown.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    },
                    {
                        "slot_id": "w-nlviz-1018",
                        "session_id": "w5",
                        "title": "An Explainable AI Approach to Large Language Model Assisted Causal Model Auditing and Development",
                        "contributors": [
                            "Klaus Mueller"
                        ],
                        "authors": [],
                        "abstract": "Causal networks are widely used in many fields, including epidemiology, social science, medicine, and engineering, to model the complex relationships between variables. While it can be convenient to algorithmically infer these models directly from observational data, the resulting networks are often plagued with erroneous edges. Auditing and correcting these networks may require domain expertise frequently unavailable to the analyst. We propose the use of large language models such as ChatGPT as an auditor for causal networks. Our method presents ChatGPT witha causal network, one edge at a time, to produce insights about edge directionality, possible confounders, and mediating variables. We ask ChatGPT to reflect on various aspects of each causal link and we then produce visualizations that summarize these viewpoints for the human analyst to direct the edge, gather more data, or test further hypotheses. We envision a system where large language models, automated causal inference, and the human analyst and domain expert work hand in hand as a team to derive holistic and comprehensive causal models for any given case scenario. This paper presents first results obtained with an emerging prototype.",
                        "uid": "w-nlviz-1018",
                        "time_stamp": "2023-10-22T04:00:00Z",
                        "time_start": "2023-10-22T04:00:00Z",
                        "time_end": "2023-10-22T05:15:00Z",
                        "paper_type": "workshop",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "0",
                        "paper_award": "",
                        "image_caption": "ChatGPT has rich knowledge about the real world. We propose to use it as an auditor for causal networks. Our method presents ChatGPT with a causal network and prompts it for insights about edge directionality, confounders, and mediating variables, resulting in a more accurate and detailed causal model. To summarize the large text produced by ChatGPT we designed several visualizations: The Causal Debate Chart, the Causal Environment Chart, and the Confounder/Mediator Chart.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    },
                    {
                        "slot_id": "w-nlviz-1017",
                        "session_id": "w5",
                        "title": "Discourse Lines: Visualising Current Policy and Media Storylines of Opportunity and Disadvantage with Narrative Exploration Maps",
                        "contributors": [
                            "Sarah Goodwin"
                        ],
                        "authors": [],
                        "abstract": "Topics of disadvantage are often discussed in the media. The discourse of disadvantage is multidimensional and has many intersecting elements, with some issues more common than others (e.g. violence, addiction), and some tending to co-occur, like human rights, criminal justice, and health to name just a few common themes. Here, we introduce and describe \u201cDiscourse Lines\u201d, an online interactive visualisation to discover which co-occurring disadvantage issues are being discussed in the media, and which ones are left out and obscured. The visualisation presents an AI-assisted analysis of news articles on topics of discourse. Our multi-scale architecture metro map visualisation invites users to drill down from a topics overview landing map to topic-specific metro maps until individual news articles. This dynamic platform allows users to see how discourse on topics of disadvantage unfolds and how news conflates or separates various issues over time.",
                        "uid": "w-nlviz-1017",
                        "time_stamp": "2023-10-22T04:00:00Z",
                        "time_start": "2023-10-22T04:00:00Z",
                        "time_end": "2023-10-22T05:15:00Z",
                        "paper_type": "workshop",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "0",
                        "paper_award": "",
                        "image_caption": "We introduce and describe \"Discourse Lines\", an online interactive visualisation to discover co-occurring topic groups in the Australian media, akin to the metro map metaphor. The visualisation presents an AI-assisted analysis that has enabled the efficient processing of very large bodies of text about the information on disadvantages. Our multi-scale architecture metro map visualisation invites users to drill down from a topics overview landing map to topic-specific metro maps until individual news articles. This dynamic platform allows users to see how discourse on topics of disadvantage unfolds and how news conflates or separates various issues over time.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    },
                    {
                        "slot_id": "w-nlviz-1013",
                        "session_id": "w5",
                        "title": "Visualizing LLM Text Style Transfer: visually dissecting how to talk like a pirate",
                        "contributors": [
                            "Richard Brath"
                        ],
                        "authors": [],
                        "abstract": "Text Style Transfer (TST) retains semantic content while modifying stylistic features. Exploratory visualization of LLM-generated TST via semantically aligned text visualization reveals advanced stylistic techniques such as use of metaphors. LLM style inquiry can be used to articulate advanced stylistic devices such as interjections, idioms and rhetorical devices and visually depicted as multivariate style heatmaps.",
                        "uid": "w-nlviz-1013",
                        "time_stamp": "2023-10-22T04:00:00Z",
                        "time_start": "2023-10-22T04:00:00Z",
                        "time_end": "2023-10-22T05:15:00Z",
                        "paper_type": "workshop",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "0",
                        "paper_award": "",
                        "image_caption": "Text Style Transfer (TST) retains semantic content while modifying stylistic features from e.g. a pirate to a detective. We semantically align transfered text which reveals techniques such as  metaphors. We do LLM style inquiry to articulate retorical devices as multivariate style heatmaps.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    },
                    {
                        "slot_id": "w-nlviz-1012",
                        "session_id": "w5",
                        "title": "Visualizing textual distributions of repeated LLM responses to characterize LLM knowledge",
                        "contributors": [
                            "Richard Brath"
                        ],
                        "authors": [],
                        "abstract": "The breadth and depth of knowledge learned by Large Language Models (LLMs) can be assessed through repetitive prompting and visual analysis of commonality across the responses. We show levels of LLM verbatim completions of prompt text through aligned responses, mind-maps of knowledge across several areas in general topics, and an association graph of topics generated directly from recursive prompting of the LLM.",
                        "uid": "w-nlviz-1012",
                        "time_stamp": "2023-10-22T04:00:00Z",
                        "time_start": "2023-10-22T04:00:00Z",
                        "time_end": "2023-10-22T05:15:00Z",
                        "paper_type": "workshop",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "0",
                        "paper_award": "",
                        "image_caption": "Issuing the same prompt to an LLM many times generates similar responses that approximate distributions. These distributions can be visualized with proportionally encoded correct words to depict exact matches; or processed into mind-maps to show LLM knowledge breadth.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    },
                    {
                        "slot_id": "w-nlviz-1009",
                        "session_id": "w5",
                        "title": "A Vega-Lite Dataset and Natural Language Generation Pipeline with Large Language Models",
                        "contributors": [
                            "Hyung-Kwon Ko"
                        ],
                        "authors": [],
                        "abstract": "There is a growing trend of utilizing Visualization-oriented Natural Language Interfaces (V-NLIs) to author charts. However, researchers consistently highlight the lack of high-quality chart and natural language datasets, which impedes the development of more sophisticated and data-driven systems using V-NLIs. In this study, we present a meticulously curated collection of human-generated 1,981 Vega-Lite specifications, derived from real-world data, and use Large Language Models (LLMs) for generating natural language queries for chart generation tasks. Unlike previous datasets that relied on relatively simple and homogeneous templates, our Vega-Lite dataset contains more complex and diverse (i.e., varying interactions, multiple plots/views, and different chart types). Using this dataset, we demonstrate generating natural language queries for chart generation, and how the results can be different when different input types are used (e.g., Vega-Lite, Image, both Vega-Lite and Image).",
                        "uid": "w-nlviz-1009",
                        "time_stamp": "2023-10-22T04:00:00Z",
                        "time_start": "2023-10-22T04:00:00Z",
                        "time_end": "2023-10-22T05:15:00Z",
                        "paper_type": "workshop",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "1",
                        "paper_award": "",
                        "image_caption": "This is sample charts of Vega-Lite specifications we present in our work. This charts includes multiple chart types such as map, heatmap, distribution, bar, line and so on. They have multiple interaction techniques like selection, panning, zooming, and brushing. They also include composite views so that many plots are connected with interaction techniques.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    }
                ]
            }
        ]
    },
    "w-energyvis": {
        "event": "EnergyVis 2023: 3rd Workshop on Energy Data Visualization",
        "long_name": "EnergyVis 2023: 3rd Workshop on Energy Data Visualization",
        "event_type": "",
        "event_prefix": "w-energyvis",
        "event_description": "",
        "event_url": "",
        "organizers": [
            "Kenny Gruchalla",
            "Arnaud Prouzeau",
            "Lyn Bartram",
            "Sarah Goodwin"
        ],
        "sessions": [
            {
                "title": "EnergyVis 2023: 3rd Workshop on Energy Data Visualization",
                "session_id": "w10",
                "event_prefix": "w-energyvis",
                "track": "103(132)",
                "session_image": "w10.png",
                "chair": [
                    "Kenny Gruchalla",
                    "Arnaud Prouzeau",
                    "Lyn Bartram",
                    "Sarah Goodwin"
                ],
                "organizers": [],
                "time_start": "2023-10-22T00:00:00Z",
                "time_end": "2023-10-22T00:15:00Z",
                "discord_category": "",
                "discord_channel": "",
                "discord_channel_id": "",
                "discord_link": "",
                "zoom_meeting": "",
                "zoom_password": "",
                "zoom_link": "",
                "time_slots": [
                    {
                        "slot_id": "w-energyvis-1001",
                        "session_id": "w10",
                        "title": "Alternatives to Contour Visualizations for Power Systems Data",
                        "contributors": [
                            "Isaiah Lyons-Galante"
                        ],
                        "authors": [],
                        "abstract": "Electrical grids are geographical and topological structures whose voltage states are challenging to represent accurately and efficiently for visual analysis. The current common practice is to use colored contour maps, yet these can misrepresent the data. We examine the suitability of four alternative visualization methods for depicting voltage data in a geographically dense distribution system\u2014Voronoi polygons, H3 tessellations, S2 tessellations, and a network-weighted contour map. We find that Voronoi tessellations and network-weighted contour maps more accurately represent the statistical distribution of the data than regular contour maps.",
                        "uid": "w-energyvis-1001",
                        "time_stamp": "2023-10-21T23:00:00Z",
                        "time_start": "2023-10-21T23:00:00Z",
                        "time_end": "2023-10-22T00:15:00Z",
                        "paper_type": "workshop",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "0",
                        "paper_award": "",
                        "image_caption": "A synthetic electrical grid covers a neighborhood in Oakland, CA. This network includes over 24,000 junctions, or buses, each with a given voltage at a single snapshot in time. Here, we surrounded each bus with a Voronoi polygon. The electrical state of the network is represented by coloring each polygon by the bus voltage. A blue-white-red diverging color scale shows deviations from the expected voltage. Giving each bus its own polygon increases the fidelity of the visualization to the real data distribution by avoiding averaging. We explore this and other alternatives to contour visualizations for power systems data.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    },
                    {
                        "slot_id": "w-energyvis-1002",
                        "session_id": "w10",
                        "title": "Topological Guided Detection of Extreme Wind Phenomena: Implications for Wind Energy",
                        "contributors": [
                            "Yu Qin"
                        ],
                        "authors": [],
                        "abstract": "Extreme wind phenomena play a crucial role in the efficient operation of wind farms for renewable energy generation. However, existing detection methods are computationally expensive, limited to specific coordinate. In real-world scenarios, understanding the occurrence of these phenomena over a large area is essential. Therefore, there is a significant demand for a fast and accurate approach to forecast such events. In this paper, we propose a novel method for detecting wind phenomena using topological analysis, leveraging the gradient of wind speed or critical points in a topological framework. By extracting topological features from the wind speed profile within a defined region, we employ topological distance to identify extreme wind phenomena. Our results demonstrate the effectiveness of utilizing topological features derived from regional wind speed profiles. We validate our approach using high-resolution simulations with the Weather Research and Forecasting model (WRF) over a month in the US East Coast.",
                        "uid": "w-energyvis-1002",
                        "time_stamp": "2023-10-21T23:00:00Z",
                        "time_start": "2023-10-21T23:00:00Z",
                        "time_end": "2023-10-22T00:15:00Z",
                        "paper_type": "workshop",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "0",
                        "paper_award": "",
                        "image_caption": "Extreme wind phenomena play a crucial role in the efficient operation of wind farms for renewable energy generation. However, existing detection methods are computationally expensive, limited to specific coordinate. In real-world scenarios, understanding the occurrence of these phenomena over a large area is essential. Therefore, there is a significant demand for a fast and accurate approach to forecast such events. In this paper, we propose a novel method for detecting wind phenomena using topological analysis, leveraging the gradient of wind speed or critical points in a topological framework. By extracting topological features from the wind speed profile within a defined region, we employ topological distance to identify extreme wind phenomena. Our results demonstrate the effectiveness of utilizing topological features derived from regional wind speed profiles. We validate our approach using high-resolution simulations with the Weather Research and Forecasting model (WRF) over a month in the US East Coast.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    },
                    {
                        "slot_id": "w-energyvis-1003",
                        "session_id": "w10",
                        "title": "An Interactive, Scenario-Based Visualization Dashboard for Model-to-Model Comparison",
                        "contributors": [
                            "Erica Attard"
                        ],
                        "authors": [],
                        "abstract": "Energy models can have a large impact on policy options and decarbonization pathways, however, they are currently stunted by obstacles including the need for transparency, accessibility of model inputs and outputs, increased stakeholder engagement, and open-source tools. The Energy Modeling Hub has responded to this need with the development of an integrated modeling platform consisting of (1) a Canadian database for energy systems, (2) a suite of energy system models, and (3) a visualization dashboard. This paper focuses on the requirements and development of a visualization dashboard used to facilitate scenario comparison, multi-model comparison, and the co-development of scenarios. The design consists of a series of tabs supporting side-by-side visualizations with a multitude of interactive features including a hover tool, toolbar, data filters, buttons, and an interactive legend. The dashboard has been used in presentations to Ministers in the British Colombia Government and received positive feedback for transparently displaying results and supplemental information as well as allowing users to interact with the data in a way that can drive their own decisions. The visualization dashboard serves as a solid foundation to facilitate scenario-based and multi-model comparisons and close the communication gap between energy modelers and stakeholders. Future work includes expanding the visualization and interactive elements implemented, specifically for input data visualization.",
                        "uid": "w-energyvis-1003",
                        "time_stamp": "2023-10-21T23:00:00Z",
                        "time_start": "2023-10-21T23:00:00Z",
                        "time_end": "2023-10-22T00:15:00Z",
                        "paper_type": "workshop",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "0",
                        "paper_award": "",
                        "image_caption": "The Visualization Dashboard showing the generation capacity for two models and the same scenario.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    },
                    {
                        "slot_id": "w-energyvis-1005",
                        "session_id": "w10",
                        "title": "Virtual Reality for Enhancing Engagement with Net Zero Transitions",
                        "contributors": [
                            "Mrs Amal Hussain Alshardy"
                        ],
                        "authors": [],
                        "abstract": "Immersive experiences can increase engagement and improve data understanding. We explore the use of virtual reality to visualise the complexities of electric networks and facilitate a greater understand- ing of net zero initiatives. We propose that an abstract metaphorical immersive experience can provide an intuitive overview of the complex components of energy networks and how they behave over time in response to fluctuating supply and demand in order to achieve stability and sustainability. This work aims to enable a variety of stakeholders to better understand and associate the complexities in the systems and be aware of the connections over time and space. Using the Monash University electric network system as a case study, we explore the network dynamics and fluctuations over time. The virtual reality experience allows users to view campus electricity assets and local and remote energy sources, as well as witness the dynamics of the network. Thus, the complexities of the energy system and its impact on sustainability can be more readily understood.",
                        "uid": "w-energyvis-1005",
                        "time_stamp": "2023-10-21T23:00:00Z",
                        "time_start": "2023-10-21T23:00:00Z",
                        "time_end": "2023-10-22T00:15:00Z",
                        "paper_type": "workshop",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "0",
                        "paper_award": "",
                        "image_caption": "Immersive VR environment showing an overview of Monash Clayton electric network dynamics: (A) Batteries supply the Clayton campus with stored energy at night. (B) With the wind turbines slowed in early morning, the connecting edge between the Murra Warra Wind Farm and the campus becomes thinner. (C) The campus relies on solar and wind power for daytime consumption. (D) At certain times the campus relies on grid generation, the simulation shows coal powered generation. A thin edge means the campus energy is off the grid.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    },
                    {
                        "slot_id": "w-energyvis-1007",
                        "session_id": "w10",
                        "title": "Visualization of the Oscillatory Dynamics of an Island Power System",
                        "contributors": [
                            "Sam Molnar"
                        ],
                        "authors": [],
                        "abstract": "In this work, we discuss the design of visualizations for understand- ing the complex oscillatory dynamics of an island power system with renewable generation sources after the loss of a large oil power plant. As more renewable generation sources are added to power systems, the oscillatory dynamics will change, which requires new visualization techniques to determine causes and strategies to avoid unwanted behaviors in the future. Our approach integrates geo- graphic views, time-series plots, and novel oscillatory-trajectory curves, providing unique insights into the interdependent oscillatory behaviors of multiple state variables and generators over time. By enabling multi-node and multivariate comparisons over time, users can qualitatively determine drivers of oscillations and differences in generator dynamics, which is not possible with other commonly used visualization techniques.",
                        "uid": "w-energyvis-1007",
                        "time_stamp": "2023-10-21T23:00:00Z",
                        "time_start": "2023-10-21T23:00:00Z",
                        "time_end": "2023-10-22T00:15:00Z",
                        "paper_type": "workshop",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "0",
                        "paper_award": "",
                        "image_caption": "A sequence of snapshots of our oscillatory trajectory curves for the real power and frequency of generators connected to an island power sytem. In addition to showing the relationship between to timeseries variables, we also overlay the network structure to provide structural context.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    },
                    {
                        "slot_id": "w-energyvis-1008",
                        "session_id": "w10",
                        "title": "Exploring the Benefits of Geography on Power Network Diagrams",
                        "contributors": [
                            "Sarah Goodwin"
                        ],
                        "authors": [],
                        "abstract": "This paper introduces an interactive visualisation that combines a spatial element to the single line diagram (SLD). SLDs are conceptual maps of the power network used by power engineers to understand the connectivity between assets of the network, study power flow, and maintain grid stability and security. Enabling users with varying degrees of electrical knowledge to understand the geographical aspect of the SLD was the key design goal. Developed through an iterative process, the visualisation intuitively transitions from an SLD view to a map view. Evaluation of the visualisation during, and following development revealed that the prototype was well-liked and that having a spatial element to the SLD was useful in understanding the geographical relationships in the power network. The evaluation also helped identify stakeholders with an interest in the hybrid view and showed the prototype\u2019s potential utility in communicating technical data to electrical non-technical users.",
                        "uid": "w-energyvis-1008",
                        "time_stamp": "2023-10-21T23:00:00Z",
                        "time_start": "2023-10-21T23:00:00Z",
                        "time_end": "2023-10-22T00:15:00Z",
                        "paper_type": "workshop",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "0",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    }
                ]
            }
        ]
    },
    "w-cityvis": {
        "event": "5th Workshop on Urban Data Visualization (CityVis) - Focus: The Role of Data Governance",
        "long_name": "5th Workshop on Urban Data Visualization (CityVis) - Focus: The Role of Data Governance",
        "event_type": "",
        "event_prefix": "w-cityvis",
        "event_description": "",
        "event_url": "",
        "organizers": [
            "Jessica Bou Nassar",
            "Lyn Bartram",
            "Sebastian Meier",
            "Darren Sharp",
            "Leonard Higi",
            "Sarah Goodwin"
        ],
        "sessions": [
            {
                "title": "5th Workshop on Urban Data Visualization (CityVis) - Focus: The Role of Data Governance",
                "session_id": "w12",
                "event_prefix": "w-cityvis",
                "track": "103(132)",
                "session_image": "w12.png",
                "chair": [
                    "Jessica Bou Nassar",
                    "Lyn Bartram",
                    "Sebastian Meier",
                    "Darren Sharp",
                    "Leonard Higi",
                    "Sarah Goodwin"
                ],
                "organizers": [],
                "time_start": "2023-10-22T00:00:00Z",
                "time_end": "2023-10-22T05:15:00Z",
                "discord_category": "",
                "discord_channel": "",
                "discord_channel_id": "",
                "discord_link": "",
                "zoom_meeting": "",
                "zoom_password": "",
                "zoom_link": "",
                "time_slots": [
                    {
                        "slot_id": "w-cityvis-1001",
                        "session_id": "w12",
                        "title": "Enhancing Collaboration in Urban Data Governance: A Measurement Framework for Applied Data Visualization",
                        "contributors": [
                            "Chien-Yu Lin"
                        ],
                        "authors": [],
                        "abstract": "The integration of data science, machine learning, and artificial intelligence in urban studies and design promises transformative impacts on cities. While acknowledging that urban complexities transcend data, the concepts of datafication and dataism emphasize the potential to sample, model, and predict urban phenomena through data. This study explores the synergy of collaboration and data visualization in urban data governance. An analytical framework, rooted in the multidimensional collaboration model and guided by theories, elucidates dimensions like Governance, Administration, Autonomy, Mutuality, Norms, and Equality. A combination of qualitative and quantitative research complements the framework, generating indicators to assess the impact of data visualization on data governance. This study contributes to structuring the framework to examine the symbiotic relationship between data visualization, collaboration, and decision-making, propelling transformative urban data governance.",
                        "uid": "w-cityvis-1001",
                        "time_stamp": "2023-10-22T04:00:00Z",
                        "time_start": "2023-10-22T04:00:00Z",
                        "time_end": "2023-10-22T05:15:00Z",
                        "paper_type": "workshop",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "1",
                        "paper_award": "",
                        "image_caption": "We all recognize the crucial role of collaboration in urban data. Our study's framework is built upon insights from existing literature. However, our objective goes beyond presenting methodology. Through an extensive literature review, we have meticulously defined six key dimensions: Governance, Administration, Autonomy, Mutuality, Norm, and Equality. Our goal is to emphasize the importance of collaborative research in the urban data profession. We are excited to share this framework with you, as it will allow us to apply indicators to assess the impact of these dimensions on data governance.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    },
                    {
                        "slot_id": "w-cityvis-1006",
                        "session_id": "w12",
                        "title": "Syracuse Interstate 81 Data Visualization in VR: Unveiling the Transformative Role in Data Governance",
                        "contributors": [
                            "Chien-Yu Lin"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "w-cityvis-1006",
                        "time_stamp": "2023-10-22T04:00:00Z",
                        "time_start": "2023-10-22T04:00:00Z",
                        "time_end": "2023-10-22T05:15:00Z",
                        "paper_type": "workshop",
                        "keywords": [],
                        "has_image": "0",
                        "has_video": "0",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    },
                    {
                        "slot_id": "w-cityvis-1017",
                        "session_id": "w12",
                        "title": "The Use of Causal Loop Diagrams to Explore People-Centred Data Governance in Australian Cities",
                        "contributors": [
                            "Ms. Jessica Bou Nassar"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "w-cityvis-1017",
                        "time_stamp": "2023-10-22T04:00:00Z",
                        "time_start": "2023-10-22T04:00:00Z",
                        "time_end": "2023-10-22T05:15:00Z",
                        "paper_type": "workshop",
                        "keywords": [],
                        "has_image": "0",
                        "has_video": "0",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    },
                    {
                        "slot_id": "w-cityvis-1013",
                        "session_id": "w12",
                        "title": "Visualizing Scalar Effects of Urban Data Aggregation",
                        "contributors": [
                            "Jonathan Nelson"
                        ],
                        "authors": [],
                        "abstract": "The process of geospatial data aggregation provides a means for abstracting the complexity of urban systems to not just better understand them, but also protect the privacy of the individuals within them. However, level of aggregation and the arbitrary sizes, shapes, and arrangements of areal units may lead to statistical and visual bias that affects the reliability and validity of findings derived from the analysis of areally aggregated urban data. This bias and resulting analytical uncertainty \u2013 known as the Modifiable Areal Unit Problem (MAUP) \u2013 has implications for public policy implementation and allocation of critical resources in both urban and rural areas. Despite a wealth of geographic research on MAUP and development of advanced statistical approaches to quantifying its effects, many of these insights and techniques remain largely inaccessible and subsequently unadopted by GIS professionals working on city planning applications. This paper introduces a simple vector-to-raster choropleth mapping workflow that enables a broad range of urban analysts to visually assess the scalar effects of the modifiable areal unit problem.",
                        "uid": "w-cityvis-1013",
                        "time_stamp": "2023-10-22T04:00:00Z",
                        "time_start": "2023-10-22T04:00:00Z",
                        "time_end": "2023-10-22T05:15:00Z",
                        "paper_type": "workshop",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "0",
                        "paper_award": "",
                        "image_caption": "Creating choropleth maps of area deprivation indices for Wisconsin State (USA) at the census block group (left), tract (middle), and county (right) levels of aggregation as an initial step in a workflow for visualizing the scalar effects of urban data aggregation.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    },
                    {
                        "slot_id": "w-cityvis-1014",
                        "session_id": "w12",
                        "title": "How Far Can Public Transport Take You?",
                        "contributors": [
                            "Markus Trainer"
                        ],
                        "authors": [],
                        "abstract": "Sustainable mobility is crucial in our current era. Our proposed interactive web application provides a user-friendly way to evaluate public transport networks and analyze how well connected a user defined location is. The current implementation comprises data from all Austrian public transport systems but can be extended with data from any provider. We made the code available on github: github.com/jku-vds-lab/publictransport. The tool can be tested in the deployed version: publictransport.jku-vds-lab.at.",
                        "uid": "w-cityvis-1014",
                        "time_stamp": "2023-10-22T04:00:00Z",
                        "time_start": "2023-10-22T04:00:00Z",
                        "time_end": "2023-10-22T05:15:00Z",
                        "paper_type": "workshop",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "1",
                        "paper_award": "",
                        "image_caption": "A proposed web application that allows users to determine how well connected a location in Austria is through public transport.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    }
                ]
            }
        ]
    },
    "w-mercado": {
        "event": "MERCADO: Multimodal Experiences for Remote Communication Around Data Online",
        "long_name": "MERCADO: Multimodal Experiences for Remote Communication Around Data Online",
        "event_type": "",
        "event_prefix": "w-mercado",
        "event_description": "",
        "event_url": "",
        "organizers": [
            "Matthew Brehmer",
            "Maxime Cordeil",
            "Christophe Hurter",
            "Takayuki Itoh"
        ],
        "sessions": [
            {
                "title": "MERCADO: Multimodal Experiences for Remote Communication Around Data Online",
                "session_id": "w13",
                "event_prefix": "w-mercado",
                "track": "105(234)",
                "session_image": "w13.png",
                "chair": [
                    "Matthew Brehmer",
                    "Maxime Cordeil",
                    "Christophe Hurter",
                    "Takayuki Itoh"
                ],
                "organizers": [],
                "time_start": "2023-10-22T00:00:00Z",
                "time_end": "2023-10-22T00:15:00Z",
                "discord_category": "",
                "discord_channel": "",
                "discord_channel_id": "",
                "discord_link": "",
                "zoom_meeting": "",
                "zoom_password": "",
                "zoom_link": "",
                "time_slots": [
                    {
                        "slot_id": "w-mercado-8665",
                        "session_id": "w13",
                        "title": "Talking to Data Visualizations: Opportunities and Challenges",
                        "contributors": [
                            "Gabriela Molina Le\u00f3n"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "w-mercado-8665",
                        "time_stamp": "2023-10-21T23:00:00Z",
                        "time_start": "2023-10-21T23:00:00Z",
                        "time_end": "2023-10-22T00:15:00Z",
                        "paper_type": "workshop",
                        "keywords": [],
                        "has_image": "0",
                        "has_video": "0",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    },
                    {
                        "slot_id": "w-mercado-8642",
                        "session_id": "w13",
                        "title": "Combining Voice and Gesture for Presenting Data to Remote Audiences",
                        "contributors": [
                            "Arjun Srinivasan"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "w-mercado-8642",
                        "time_stamp": "2023-10-21T23:00:00Z",
                        "time_start": "2023-10-21T23:00:00Z",
                        "time_end": "2023-10-22T00:15:00Z",
                        "paper_type": "workshop",
                        "keywords": [],
                        "has_image": "0",
                        "has_video": "0",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    },
                    {
                        "slot_id": "w-mercado-5242",
                        "session_id": "w13",
                        "title": "Hanstreamer: an Open-source Webcam-based Live Data Presentation System",
                        "contributors": [
                            "Maxime Cordeil"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "w-mercado-5242",
                        "time_stamp": "2023-10-21T23:00:00Z",
                        "time_start": "2023-10-21T23:00:00Z",
                        "time_end": "2023-10-22T00:15:00Z",
                        "paper_type": "workshop",
                        "keywords": [],
                        "has_image": "0",
                        "has_video": "0",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    },
                    {
                        "slot_id": "w-mercado-1322",
                        "session_id": "w13",
                        "title": "CommunityClick-Virtual: Multi-Modal Interactions for Enhancing Participation in Virtual Meetings",
                        "contributors": [
                            "Mahmood Jasim"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "w-mercado-1322",
                        "time_stamp": "2023-10-21T23:00:00Z",
                        "time_start": "2023-10-21T23:00:00Z",
                        "time_end": "2023-10-22T00:15:00Z",
                        "paper_type": "workshop",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "0",
                        "paper_award": "",
                        "image_caption": "A snapshot of CommunityClick-Virtual\u2019s real-time component provides organizers with quick statistics, a set of real-time visualizations to track attendee responses, including a bar chart that shows the number of reactions for each feedback category alongside the unique number of attendees who generated those reactions, accumulated and interval bar and line charts to track how attendees have been providing feedback to  the meeting discussion. Finally, the organizers have access to CMS controls on the attendee visualization and the chat option.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    },
                    {
                        "slot_id": "w-mercado-8593",
                        "session_id": "w13",
                        "title": "Asymmetric Immersive Presentation System for Financial Data Visualization",
                        "contributors": [
                            "Mengyu Chen"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "w-mercado-8593",
                        "time_stamp": "2023-10-21T23:00:00Z",
                        "time_start": "2023-10-21T23:00:00Z",
                        "time_end": "2023-10-22T00:15:00Z",
                        "paper_type": "workshop",
                        "keywords": [],
                        "has_image": "0",
                        "has_video": "0",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    },
                    {
                        "slot_id": "w-mercado-2570",
                        "session_id": "w13",
                        "title": "Echoes in the Gallery: A Collaborative Immersive Analytics System for Analyzing Audience Reactions in Virtual Reality Exhibitions",
                        "contributors": [
                            "Linping Yuan"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "w-mercado-2570",
                        "time_stamp": "2023-10-21T23:00:00Z",
                        "time_start": "2023-10-21T23:00:00Z",
                        "time_end": "2023-10-22T00:15:00Z",
                        "paper_type": "workshop",
                        "keywords": [],
                        "has_image": "0",
                        "has_video": "0",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    }
                ]
            }
        ]
    },
    "w-visxvision": {
        "event": "VisxVision: Workshop on Novel Directions in Vision Science and Visualization Research",
        "long_name": "VisxVision: Workshop on Novel Directions in Vision Science and Visualization Research",
        "event_type": "",
        "event_prefix": "w-visxvision",
        "event_description": "",
        "event_url": "",
        "organizers": [
            "Ghulam Jilani Quadri",
            "Clementine Zimnicki",
            "Racquel Fygenson",
            "Madeline Awad",
            "Ouxun Jiang"
        ],
        "sessions": [
            {
                "title": "VisxVision: Workshop on Novel Directions in Vision Science and Visualization Research",
                "session_id": "w14",
                "event_prefix": "w-visxvision",
                "track": "109(234)",
                "session_image": "w14.png",
                "chair": [
                    "Ghulam Jilani Quadri",
                    "Clementine Zimnicki",
                    "Racquel Fygenson",
                    "Madeline Awad",
                    "Ouxun Jiang"
                ],
                "organizers": [],
                "time_start": "2023-10-22T00:00:00Z",
                "time_end": "2023-10-23T05:15:00Z",
                "discord_category": "",
                "discord_channel": "",
                "discord_channel_id": "",
                "discord_link": "",
                "zoom_meeting": "",
                "zoom_password": "",
                "zoom_link": "",
                "time_slots": [
                    {
                        "slot_id": "w-visxvision-1015",
                        "session_id": "w14",
                        "title": "Adjusting Point Size to Facilitate More Accurate Correlation Perception in Scatterplots",
                        "contributors": [
                            "Mr Gabriel Strain"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "w-visxvision-1015",
                        "time_stamp": "2023-10-23T04:00:00Z",
                        "time_start": "2023-10-23T04:00:00Z",
                        "time_end": "2023-10-23T05:15:00Z",
                        "paper_type": "workshop",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "0",
                        "paper_award": "",
                        "image_caption": "Examples of the experimental stimuli used, with an r value of 0.6. When 150 participants were asked to rate correlation in scatterplots, they were most accurate when the non-linear decay condition was used across a range of 45 r values.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    },
                    {
                        "slot_id": "w-visxvision-1020",
                        "session_id": "w14",
                        "title": "Can AI Mitigate Human Perceptual Biases?  A Pilot Study",
                        "contributors": [
                            "Jian Chen"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "w-visxvision-1020",
                        "time_stamp": "2023-10-23T04:00:00Z",
                        "time_start": "2023-10-23T04:00:00Z",
                        "time_end": "2023-10-23T05:15:00Z",
                        "paper_type": "workshop",
                        "keywords": [],
                        "has_image": "0",
                        "has_video": "0",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    }
                ]
            }
        ]
    },
    "w-visxprov": {
        "event": "(Vis + Prov) x Domain: Workshop on Visualization and Provenance Across Domains",
        "long_name": "(Vis + Prov) x Domain: Workshop on Visualization and Provenance Across Domains",
        "event_type": "",
        "event_prefix": "w-visxprov",
        "event_description": "",
        "event_url": "",
        "organizers": [
            "Kai Xu",
            "Michelle Dowling",
            "John Wenskovitch",
            "Yilin Xia",
            "Jeremy E Block"
        ],
        "sessions": [
            {
                "title": "(Vis + Prov) x Domain: Workshop on Visualization and Provenance Across Domains",
                "session_id": "w15",
                "event_prefix": "w-visxprov",
                "track": "110(234)",
                "session_image": "w15.png",
                "chair": [
                    "Kai Xu",
                    "Michelle Dowling",
                    "John Wenskovitch",
                    "Yilin Xia",
                    "Jeremy E Block"
                ],
                "organizers": [],
                "time_start": "2023-10-22T00:00:00Z",
                "time_end": "2023-10-23T00:15:00Z",
                "discord_category": "",
                "discord_channel": "",
                "discord_channel_id": "",
                "discord_link": "",
                "zoom_meeting": "",
                "zoom_password": "",
                "zoom_link": "",
                "time_slots": [
                    {
                        "slot_id": "w-visxprov-1003",
                        "session_id": "w15",
                        "title": "Modeling the Dashboard Provenance",
                        "contributors": [
                            "Johne Marcus Jarske"
                        ],
                        "authors": [],
                        "abstract": "Organizations of all kinds, whether public or private, profit-driven or non-profit, and across various industries and sectors, rely on dashboards for effective data visualization. However, the reliability and efficacy of these dashboards rely on the quality of the visual and data they present. Studies show that less than a quarter of dashboards provide information about their sources, which is just one of the expected metadata when provenance is seriously considered. Provenance is a record that describes people, organizations, entities, and activities that had a role in the production, influence, or delivery of a piece of data or an object. This paper aims to provide a provenance representation model, that entitles standardization, modeling, generation, capture, and visualization, specifically designed for dashboards and its visual and data components. The proposed model will offer a comprehensive set of essential provenance metadata that enables users to evaluate the quality, consistency, and reliability of the information presented on dashboards. This will allow a clear and precise understanding of the context in which a specific dashboard was developed, ultimately leading to better decision-making.",
                        "uid": "w-visxprov-1003",
                        "time_stamp": "2023-10-22T23:00:00Z",
                        "time_start": "2023-10-22T23:00:00Z",
                        "time_end": "2023-10-23T00:15:00Z",
                        "paper_type": "workshop",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "1",
                        "paper_award": "",
                        "image_caption": "Provenance Dashboard Model to improve reliability and effectiveness of a dashboard.  Organizations across various sectors rely on dashboards for effective data visualization. Nevertheless, the reliability and effectiveness of these dashboards hinge on the quality of the visual elements and data they present. But how can we ensure the reliability and effectiveness of a dashboard?  By integrating provenance into dashboards, users can significantly improve their decision-making processes by gaining a more comprehensive understanding of the dashboard's context and reliability. Provenance-driven dashboards empower users to explore a richer narrative of the data and visualizations, unveiling their meticulously curated history.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    },
                    {
                        "slot_id": "w-visxprov-1002",
                        "session_id": "w15",
                        "title": "Visualising category recoding and numeric redistributions",
                        "contributors": [
                            "Cynthia A Huang"
                        ],
                        "authors": [],
                        "abstract": "This paper proposes graphical representations of data and rationale provenance in workflows that convert both category labels and associated numeric data between distinct but semantically related taxonomies. We motivate the graphical representations with a new task abstraction, the cross-taxonomy transformation, and associated graph-based information structure, the crossmap. The task abstraction supports the separation of category recoding and numeric redistribution decisions from the specifics of data manipulation in ex-post data harmonisation. The crossmap structure is illustrated using an example conversion of numeric statistics from a country-specific taxonomy to an international classification standard. We discuss the opportunities and challenges of using visualisation to audit and communicate cross-taxonomy transformations and present candidate graphical representations.",
                        "uid": "w-visxprov-1002",
                        "time_stamp": "2023-10-22T23:00:00Z",
                        "time_start": "2023-10-22T23:00:00Z",
                        "time_end": "2023-10-23T00:15:00Z",
                        "paper_type": "workshop",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "0",
                        "paper_award": "",
                        "image_caption": "A crossmap for converting values observed using Australian occupation categories (ANZSCO22) into observations under the International Standard Classification of Occupations (ISCO8). Crossmaps are directed multipartite graph structures for capturing details of cross-taxonomy transformation. This crossmap is visualised using a two-layer bigraph layout, with ANZSCO22 codes forming the first source layer, and ISCO8 codes the second target layer. The weights on the links indicate what share of ANZSCO22 observed values are redistributed to corresponding ISCO8 codes.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    },
                    {
                        "slot_id": "w-visxprov-1001",
                        "session_id": "w15",
                        "title": "When Provenance Aids and Complicates Reproducibility Judgments",
                        "contributors": [
                            "David Koop"
                        ],
                        "authors": [],
                        "abstract": "It is well-established that the provenance of a scientific result is important, sometimes more important than the actual result. For computational analyses that involve visualization, this provenance information may contain the steps involved in generating visualizations from raw data. Specifically, data provenance tracks the lineage of data and process provenance tracks the steps executed. In this paper, we argue that the utility of computational provenance may not be as clear-cut as we might like. One common use case for provenance is that the information can be used to reproduce the original result. However, in visualization, the goal is often to communicate results to a user or viewer, and thus the insights obtained are ultimately most important. Viewers can miss important changes or react to unimportant ones. Here, interaction provenance, which tracks a user's actions with a visualization, or insight provenance, which tracks the decision-making process, can help capture what happened but don't remove the issues. In this paper, we present scenarios where provenance impacts reproducibility in different ways. We also explore how provenance and visualizations can be better related.",
                        "uid": "w-visxprov-1001",
                        "time_stamp": "2023-10-22T23:00:00Z",
                        "time_start": "2023-10-22T23:00:00Z",
                        "time_end": "2023-10-23T00:15:00Z",
                        "paper_type": "workshop",
                        "keywords": [],
                        "has_image": "0",
                        "has_video": "0",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    }
                ]
            }
        ]
    },
    "a-visinpractice": {
        "event": "VisInPractice",
        "long_name": "VisInPractice",
        "event_type": "",
        "event_prefix": "a-visinpractice",
        "event_description": "",
        "event_url": "",
        "organizers": [
            "Alexander Bock",
            "Ayan Biswas"
        ],
        "sessions": [
            {
                "title": "VisInPractice",
                "session_id": "ae1",
                "event_prefix": "a-visinpractice",
                "track": "109(234)",
                "session_image": "ae1.png",
                "chair": [
                    "Alexander Bock",
                    "Ayan Biswas"
                ],
                "organizers": [],
                "time_start": "2023-10-22T00:00:00Z",
                "time_end": "2023-10-23T00:15:00Z",
                "discord_category": "",
                "discord_channel": "",
                "discord_channel_id": "",
                "discord_link": "",
                "zoom_meeting": "",
                "zoom_password": "",
                "zoom_link": "",
                "time_slots": []
            }
        ]
    },
    "a-vizsec": {
        "event": "VizSec",
        "long_name": "VizSec",
        "event_type": "",
        "event_prefix": "a-vizsec",
        "event_description": "",
        "event_url": "",
        "organizers": [
            "Lyndsey Franklin",
            "Xumeng Wang",
            "Aritra Dasgupta",
            "Adrian Komandina",
            "Kuhu Gupta"
        ],
        "sessions": [
            {
                "title": "VizSec",
                "session_id": "ae4",
                "event_prefix": "a-vizsec",
                "track": "101-102(140)",
                "session_image": "ae4.png",
                "chair": [
                    "Lyndsey Franklin",
                    "Xumeng Wang",
                    "Aritra Dasgupta",
                    "Adrian Komandina",
                    "Kuhu Gupta"
                ],
                "organizers": [],
                "time_start": "2023-10-22T00:00:00Z",
                "time_end": "2023-10-22T05:15:00Z",
                "discord_category": "",
                "discord_channel": "",
                "discord_channel_id": "",
                "discord_link": "",
                "zoom_meeting": "",
                "zoom_password": "",
                "zoom_link": "",
                "time_slots": [
                    {
                        "slot_id": "a-vizsec-1350",
                        "session_id": "ae4",
                        "title": "Vis-SAGA: Visual Analytics for Situational Awareness of Grid Anomalies",
                        "contributors": [
                            "Graham Johnson"
                        ],
                        "authors": [],
                        "abstract": "We describe supporting near real-time situational awareness of the electric distribution system by visualizing novel data from voltage sensors deployed on existing broadband cable television network equipment. Our scalable web-based visual analytics platform supports interactive geospatial exploration, time-series analysis, and summarization of grid behavior during potentially anomalous events. The broadband cable television sensor network provides observability of the electrical distribution system at a higher local spatial resolution than is typically available to most utilities, revealing the operational state of the network and aiding in the detection of abnormal behaviors or deviations from expected patterns, particularly across electric utility service areas. We outline the design and development of interactive geospatial and time-series visualization components and the scalable data services that supply metadata, historical, and real-time streams of sensor data across the network. We present our platform during periods of extreme weather, demonstrating its ability to assist in detecting patterns of operation that affect power availability, quality, resiliency, and service restoration.",
                        "uid": "a-vizsec-1350",
                        "time_stamp": "2023-10-22T04:00:00Z",
                        "time_start": "2023-10-22T04:00:00Z",
                        "time_end": "2023-10-22T05:15:00Z",
                        "paper_type": "associated",
                        "keywords": [],
                        "has_image": "0",
                        "has_video": "0",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    },
                    {
                        "slot_id": "a-vizsec-5220",
                        "session_id": "ae4",
                        "title": "FuzzPlanner: Visually Assisting the Design of Firmware Fuzzing Campaigns",
                        "contributors": [
                            "Alessio Izzillo"
                        ],
                        "authors": [],
                        "abstract": "Embedded devices are pivotal in many aspects to our everyday life, acting as key elements within our critical infrastructures, e-health sector, and the IoT ecosystem. These devices ship with custom software, dubbed firmware, whose development may not have followed strict security-by-design guidelines and for which no detailed documentation may be available. Given their critical role, testing their software before deploying them is crucial. Software fuzzing is a popular software testing technique that has shown to be quite effective in the last decade. However, the firmware may contain thousands of subcomponents with unexpected interplays. Moreover, operators may have a tight time budget to perform a security evaluation, requiring focused fuzzing on the most critical subcomponents. Also, considering the lack of accurate documentation for a device, it is quite hard for a security operator to understand what to fuzz and how to fuzz a specific device firmware. In this paper, we present FuzzPlanner, a visual analytics solution that enables security operators during the design of a fuzzing campaign over a device firmware. FuzzPlanner helps the operator identify the best candidates for fuzzing using several innovative visual aids. Our contributions include introducing FuzzPlanner, exploring diverse analytical tools to pinpoint critical binaries, and showing its efficacy with two real-world firmware image scenarios.",
                        "uid": "a-vizsec-5220",
                        "time_stamp": "2023-10-22T04:00:00Z",
                        "time_start": "2023-10-22T04:00:00Z",
                        "time_end": "2023-10-22T05:15:00Z",
                        "paper_type": "associated",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "0",
                        "paper_award": "",
                        "image_caption": "FuzzPlanner is a visual tool that assists security operators in designing fuzzing campaigns for device firmware. It employs dynamic analysis to monitor inter-binary data interactions and process interactions, collecting information about firmware binary components. Operators utilize this information to prioritize their testing efforts, as they often need to conduct security assessments within tight timeframes, such as a week. This underscores the importance of efficient fuzzing campaign design due to the time-consuming nature of fuzzing.",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    },
                    {
                        "slot_id": "a-vizsec-7061",
                        "session_id": "ae4",
                        "title": "Exploring the Representation of Cyber-Risk Data Through Sketching",
                        "contributors": [
                            "Mr Thomas Miller"
                        ],
                        "authors": [],
                        "abstract": "Dealing with complex information regarding cyber security risks is increasingly important as attacks rise in frequency. Visualisation techniques are used to support decision-making and insight. However, the use of visualisations across different stakeholders and cyber security risk data is not well explored. This work presents an exploratory study in which participants use sketching to represent cyber-risk data. We critically discuss the method and our results demonstrate the usefulness of the method to identify new, diverse visualisation approaches, as well as the richness of stakeholder visualisation conceptualisation.",
                        "uid": "a-vizsec-7061",
                        "time_stamp": "2023-10-22T04:00:00Z",
                        "time_start": "2023-10-22T04:00:00Z",
                        "time_end": "2023-10-22T05:15:00Z",
                        "paper_type": "associated",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "0",
                        "paper_award": "",
                        "image_caption": "A data representation continuum that represents data from numerical to abstract (left to right). Light-blue dots represent the categories populated by the originally develop continuum. Red  dots represent visualisation techniques identified in industry. Green dots represent participants\u2019 first sketches. Dark-blue dots represent  participants\u2019 second sketches",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    },
                    {
                        "slot_id": "a-vizsec-8322",
                        "session_id": "ae4",
                        "title": "PassViz: An Interactive Visualisation System for Analysing Leaked Passwords",
                        "contributors": [
                            "Mr Samuel Charles Parker"
                        ],
                        "authors": [],
                        "abstract": "Passwords remain the most widely used form of user authentication, despite advancements in other methods. However, their limitations, such as susceptibility to attacks, especially weak passwords defined by human users, are well-documented. The existence of weak human-defined passwords has led to repeated password leaks from websites, many of which are of large scale. While such password leaks are unfortunate security incidents, they provide security researchers and practitioners with good opportunities to learn valuable insights from such leaked passwords, in order to identify ways to improve password policies and other security controls on passwords. Researchers have proposed different data visualisation techniques to help analyse leaked passwords. However, many approaches rely solely on frequency analysis, with limited exploration of distance-based graphs. This paper reports PassViz, a novel method that combines the edit distance with the t-SNE (t-distributed stochastic neighbour embedding) dimensionality reduction algorithm [19] for visualising and analysing leaked passwords in a 2-D space. We implemented PassViz as an easy-to-use command-line tool for visualising large-scale password databases, and also as a graphical user interface (GUI) to support interactive visual analytics of small password databases. Using the \u201c000webhost\u201d leaked database as an example, we show how PassViz can be used to visually analyse different aspects of leaked passwords and to facilitate the discovery of previously unknown password patterns. Overall, our approach empowers researchers and practitioners to gain valuable insights and improve password security through effective data visualisation and analysis.",
                        "uid": "a-vizsec-8322",
                        "time_stamp": "2023-10-22T04:00:00Z",
                        "time_start": "2023-10-22T04:00:00Z",
                        "time_end": "2023-10-22T05:15:00Z",
                        "paper_type": "associated",
                        "keywords": [],
                        "has_image": "1",
                        "has_video": "0",
                        "paper_award": "",
                        "image_caption": "Examples of visualisation of different clusters for 000webhost leaked password database",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    },
                    {
                        "slot_id": "a-vizsec-8611",
                        "session_id": "ae4",
                        "title": "Visualizing Comparisons of Bill of Materials",
                        "contributors": [
                            "Rebecca Jones"
                        ],
                        "authors": [],
                        "abstract": "The complexity of distributed manufacturing and software development coupled with the increasing prevalence of cyber and supply chain attacks necessitates a greater understanding of the hardware and software components that comprise equipment in critical infrastructure. When a vulnerability in a single software library can have disastrous consequences, being able to identify where that library may exist in equipment or software becomes a prerequisite for protecting the overall infrastructure. This need has sparked a large effort around the development and incorporation of bill-of-materials(BOM) into security, asset management, and procurement practices to aid in mitigating, and responding to future attacks. While much of the current research is devoted to creating BOMs, it is equally important to develop methods for comparing them to answer questions, such as: How has my software changed? Are two pieces of equipment equivalent? Does this piece of equipment that just arrived match my historical information? In this work, we demonstrate how BOMs can be represented by graph structures.  We then describe how these structures can be fed into a graph comparison algorithm to produce a novel interactive visualization that allows us to not only identify differences in BOMs but show exactly where they are in the product.",
                        "uid": "a-vizsec-8611",
                        "time_stamp": "2023-10-22T04:00:00Z",
                        "time_start": "2023-10-22T04:00:00Z",
                        "time_end": "2023-10-22T05:15:00Z",
                        "paper_type": "associated",
                        "keywords": [],
                        "has_image": "0",
                        "has_video": "0",
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "has_pdf": false,
                        "ff_link": "",
                        "ff_id": "",
                        "prerecorded_video_link": "",
                        "prerecorded_video_id": "",
                        "live_video_link": "",
                        "live_video_id": ""
                    }
                ]
            }
        ]
    },
    "t-taurus": {
        "event": "TAURUS: a unified framework for creating graph layouts",
        "long_name": "TAURUS: a unified framework for creating graph layouts",
        "event_type": "",
        "event_prefix": "t-taurus",
        "event_description": "",
        "event_url": "",
        "organizers": [
            "Yunhai Wang"
        ],
        "sessions": [
            {
                "title": "TAURUS: a unified framework for creating graph layouts",
                "session_id": "t3",
                "event_prefix": "t-taurus",
                "track": "101-102(140)",
                "session_image": "t3.png",
                "chair": [
                    "Yunhai Wang"
                ],
                "organizers": [],
                "time_start": "2023-10-22T00:00:00Z",
                "time_end": "2023-10-23T05:15:00Z",
                "discord_category": "",
                "discord_channel": "",
                "discord_channel_id": "",
                "discord_link": "",
                "zoom_meeting": "",
                "zoom_password": "",
                "zoom_link": "",
                "time_slots": []
            }
        ]
    },
    "t-network": {
        "event": "Mining Useful Information Via Complex Network Visualization",
        "long_name": "Mining Useful Information Via Complex Network Visualization",
        "event_type": "",
        "event_prefix": "t-network",
        "event_description": "",
        "event_url": "",
        "organizers": [
            "Sonali Agarwal"
        ],
        "sessions": [
            {
                "title": "Mining Useful Information Via Complex Network Visualization",
                "session_id": "t4",
                "event_prefix": "t-network",
                "track": "101-102(140)",
                "session_image": "t4.png",
                "chair": [
                    "Sonali Agarwal"
                ],
                "organizers": [],
                "time_start": "2023-10-22T00:00:00Z",
                "time_end": "2023-10-23T00:15:00Z",
                "discord_category": "",
                "discord_channel": "",
                "discord_channel_id": "",
                "discord_link": "",
                "zoom_meeting": "",
                "zoom_password": "",
                "zoom_link": "",
                "time_slots": []
            }
        ]
    },
    "t-empirical": {
        "event": "Transparent Practices for Quantitative Empirical Research",
        "long_name": "Transparent Practices for Quantitative Empirical Research",
        "event_type": "",
        "event_prefix": "t-empirical",
        "event_description": "",
        "event_url": "",
        "organizers": [
            "Abhraneel Sarma"
        ],
        "sessions": [
            {
                "title": "Transparent Practices for Quantitative Empirical Research",
                "session_id": "t6",
                "event_prefix": "t-empirical",
                "track": "103(132)",
                "session_image": "t6.png",
                "chair": [
                    "Abhraneel Sarma"
                ],
                "organizers": [],
                "time_start": "2023-10-22T00:00:00Z",
                "time_end": "2023-10-23T00:15:00Z",
                "discord_category": "",
                "discord_channel": "",
                "discord_channel_id": "",
                "discord_link": "",
                "zoom_meeting": "",
                "zoom_password": "",
                "zoom_link": "",
                "time_slots": []
            }
        ]
    },
    "t-design": {
        "event": "Design Sprints for Visualization",
        "long_name": "Design Sprints for Visualization",
        "event_type": "",
        "event_prefix": "t-design",
        "event_description": "",
        "event_url": "",
        "organizers": [
            "Carolina Nobre"
        ],
        "sessions": [
            {
                "title": "Design Sprints for Visualization",
                "session_id": "t7",
                "event_prefix": "t-design",
                "track": "104(132)",
                "session_image": "t7.png",
                "chair": [
                    "Carolina Nobre"
                ],
                "organizers": [],
                "time_start": "2023-10-22T00:00:00Z",
                "time_end": "2023-10-23T05:15:00Z",
                "discord_category": "",
                "discord_channel": "",
                "discord_channel_id": "",
                "discord_link": "",
                "zoom_meeting": "",
                "zoom_password": "",
                "zoom_link": "",
                "time_slots": []
            }
        ]
    }

    
}