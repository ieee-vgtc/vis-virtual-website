{
    "conf": {
        "event": "Conference Events",
        "long_name": "Conference Events",
        "event_type": "vis",
        "event_prefix": "conf",
        "event_description": "",
        "event_url": "",
        "organizers": [],
        "sessions": [
            {
                "title": "VIS Welcome (8:45am-9:00am)| VGTC Awards (9:00am-9:45am)| Test of Time Awards (9:45am-10:30am)",
                "session_id": "conf1",
                "event_prefix": "conf",
                "track": "plenary",
                "session_image": "conf1.png",
                "chair": [],
                "time_start": "2023-10-23T21:45:00Z",
                "time_end": "2023-10-23T23:30:00Z",
                "discord_category": "",
                "discord_channel": "plenary-1",
                "discord_channel_id": "1161407618322014349",
                "discord_link": "https://discordapp.com/channels/1161400451619627068/1161407618322014349",
                "zoom_private_meeting": "",
                "zoom_private_password": "",
                "zoom_private_link": "",
                "zoom_broadcast_link": "",
                "ff_link": "",
                "time_slots": []
            },
            {
                "title": "VIS Keynote (11:00am-12:00pm)",
                "session_id": "conf2",
                "event_prefix": "conf",
                "track": "plenary",
                "session_image": "conf2.png",
                "chair": [],
                "time_start": "2023-10-24T00:00:00Z",
                "time_end": "2023-10-24T01:00:00Z",
                "discord_category": "",
                "discord_channel": "plenary-1",
                "discord_channel_id": "1161407618322014349",
                "discord_link": "https://discordapp.com/channels/1161400451619627068/1161407618322014349",
                "zoom_private_meeting": "",
                "zoom_private_password": "",
                "zoom_private_link": "",
                "zoom_broadcast_link": "",
                "ff_link": "",
                "time_slots": []
            },
            {
                "title": "VIS Capstone (10:45am-11:45am)| VIS Closing (11:45am-12:00pm)",
                "session_id": "conf3",
                "event_prefix": "conf",
                "track": "oneohfive",
                "session_image": "conf3.png",
                "chair": [],
                "time_start": "2023-10-26T23:45:00Z",
                "time_end": "2023-10-27T01:00:00Z",
                "discord_category": "",
                "discord_channel": "105",
                "discord_channel_id": "1161741309346861067",
                "discord_link": "https://discordapp.com/channels/1161400451619627068/1161741309346861067",
                "zoom_private_meeting": "",
                "zoom_private_password": "",
                "zoom_private_link": "",
                "zoom_broadcast_link": "",
                "ff_link": "",
                "time_slots": []
            },
            {
                "title": "VIS Poster Session (4:45pm-6:15pm)| -- includes all Associated Event Posters| -- Exhibit Hall Location: Foyer",
                "session_id": "conf4",
                "event_prefix": "conf",
                "track": "plenary",
                "session_image": "conf4.png",
                "chair": [],
                "time_start": "2023-10-24T05:45:00Z",
                "time_end": "2023-10-24T07:15:00Z",
                "discord_category": "",
                "discord_channel": "plenary-1",
                "discord_channel_id": "1161407618322014349",
                "discord_link": "https://discordapp.com/channels/1161400451619627068/1161407618322014349",
                "zoom_private_meeting": "",
                "zoom_private_password": "",
                "zoom_private_link": "",
                "zoom_broadcast_link": "",
                "ff_link": "",
                "time_slots": []
            },
            {
                "title": "VISAP Opening Reception (6:00pm-8:00pm)| -- Art Exhibit Location: Library at the Dock Gallery, 107 Victoria Harbour Promenade, Docklands",
                "session_id": "conf5",
                "event_prefix": "conf",
                "track": "plenary",
                "session_image": "conf5.png",
                "chair": [],
                "time_start": "2023-10-24T07:00:00Z",
                "time_end": "2023-10-24T09:00:00Z",
                "discord_category": "",
                "discord_channel": "plenary-1",
                "discord_channel_id": "1161407618322014349",
                "discord_link": "https://discordapp.com/channels/1161400451619627068/1161407618322014349",
                "zoom_private_meeting": "",
                "zoom_private_password": "",
                "zoom_private_link": "",
                "zoom_broadcast_link": "",
                "ff_link": "",
                "time_slots": []
            },
            {
                "title": "VIS Banquet (6:00pm-10:00pm)| -- Location: Melbourne Planetarium| -- Shows at 6, 6:45, and 7:30pm| -- Transport on your own / via myki",
                "session_id": "conf6",
                "event_prefix": "conf",
                "track": "plenary",
                "session_image": "conf6.png",
                "chair": [],
                "time_start": "2023-10-25T07:00:00Z",
                "time_end": "2023-10-25T11:00:00Z",
                "discord_category": "",
                "discord_channel": "plenary-1",
                "discord_channel_id": "1161407618322014349",
                "discord_link": "https://discordapp.com/channels/1161400451619627068/1161407618322014349",
                "zoom_private_meeting": "",
                "zoom_private_password": "",
                "zoom_private_link": "",
                "zoom_broadcast_link": "",
                "ff_link": "",
                "time_slots": []
            }
        ]
    },
    "v-full": {
        "event": "VIS Full Papers",
        "long_name": "VIS Full Papers",
        "event_type": "full",
        "event_prefix": "v-full",
        "event_description": "",
        "event_url": "",
        "organizers": [],
        "sessions": [
            {
                "title": "VIS Opening (2:00pm-2:20pm)| ACC Summary (2:20pm-2:30pm)| Best Papers 1 (2:30pm-3:10pm)",
                "session_id": "full0",
                "event_prefix": "v-full",
                "track": "plenary",
                "session_image": "full0.png",
                "chair": [
                    "Michael Wybrow",
                    "Tamara Munzner"
                ],
                "time_start": "2023-10-24T03:00:00Z",
                "time_end": "2023-10-24T04:10:00Z",
                "discord_category": "",
                "discord_channel": "plenary-1",
                "discord_channel_id": "1161407618322014349",
                "discord_link": "https://discordapp.com/channels/1161400451619627068/1161407618322014349",
                "zoom_private_meeting": "",
                "zoom_private_password": "",
                "zoom_private_link": "",
                "zoom_broadcast_link": "",
                "ff_link": "https://youtu.be/w3wENT9y2dA",
                "time_slots": [
                    {
                        "slot_id": "v-full-1009",
                        "session_id": "full0",
                        "title": "Affective Visualization Design: Leveraging the Emotional Impact of Data",
                        "contributors": [
                            "Xingyu Lan"
                        ],
                        "authors": [
                            "Xingyu Lan",
                            "Yanqiu Wu",
                            "Nan Cao"
                        ],
                        "abstract": "In recent years, more and more researchers have reflected on the undervaluation of emotion in data visualization and highlighted the importance of considering human emotion in visualization design. Meanwhile, an increasing number of studies have been conducted to explore emotion-related factors. However, so far, this research area is still in its early stages and faces a set of challenges, such as the unclear definition of key concepts, the insufficient justification of why emotion is important in visualization design, and the lack of characterization of the design space of affective visualization design. To address these challenges, first, we conducted a literature review and identified three research lines that examined both emotion and data visualization. We clarified the differences between these research lines and kept 109 papers that studied or discussed how data visualization communicates and influences emotion. Then, we coded the 109 papers in terms of how they justified the legitimacy of considering emotion in visualization design (i.e., why emotion is important) and identified five argumentative perspectives. Based on these papers, we also identified 61 projects that practiced affective visualization design. We coded these design projects in three dimensions, including design fields (where), design tasks (what), and design methods (how), to explore the design space of affective visualization design.",
                        "uid": "v-full-1009",
                        "time_stamp": "2023-10-24T03:00:00Z",
                        "time_start": "2023-10-24T03:00:00Z",
                        "time_end": "2023-10-24T03:12:00Z",
                        "paper_type": "full",
                        "keywords": [
                            "Information Visualization;Affective Design;Visual Communication;User Experience;Storytelling"
                        ],
                        "doi": "10.1109/TVCG.2023.3327385",
                        "fno": "",
                        "has_image": true,
                        "has_pdf": true,
                        "paper_award": "best",
                        "image_caption": "The image has 3 panels: (i)examples of affective visualization design projects; (ii)six identified genres of affective visualization design projects (interactive interface, video, static image/painting, installation, artifact, event), and (iii) the design space proposed by this work, showing WHERE to apply affective visualization design, WHAT design tasks it undertakes, and HOW to perform the design.",
                        "external_paper_link": "",
                        "youtube_ff_link": "https://youtu.be/Q7KBPKW85qc",
                        "youtube_ff_id": "Q7KBPKW85qc",
                        "bunny_ff_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1009/v-full-1009_Preview.mp4?token=TQoD13Vo0XHcdym-oh4_RYw2E36BvLYHUIDCXj7tjAU&expires=1706590800",
                        "bunny_ff_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1009/v-full-1009_Preview.vtt?token=RLrPi3rtJoA1wGjXw1BUUTZf0PFU1FINs7FDfDWZmlI&expires=1706590800",
                        "youtube_prerecorded_link": "https://youtu.be/cPMVHC2cAWE",
                        "youtube_prerecorded_id": "cPMVHC2cAWE",
                        "bunny_prerecorded_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1009/v-full-1009_Presentation.mp4?token=OEwUdugcYoiG3ymrJ1I1pfuA1vxtRMw0sTMe2EAHsts&expires=1706590800",
                        "bunny_prerecorded_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1009/v-full-1009_Presentation.vtt?token=7JetXIhkbfwWRs8WaTTcX8jBmORmUTvf9zukG-QREYU&expires=1706590800"
                    },
                    {
                        "slot_id": "v-full-1392",
                        "session_id": "full0",
                        "title": "Fast Compressed Segmentation Volumes for Scientific Visualization",
                        "contributors": [
                            "Max Piochowiak"
                        ],
                        "authors": [
                            "Max Piochowiak",
                            "Carsten Dachsbacher"
                        ],
                        "abstract": "Voxel-based segmentation volumes often store a large number of labels and voxels, and the resulting amount of data can make storage, transfer, and interactive visualization difficult. We present a lossless compression technique which addresses these challenges. It processes individual small bricks of a segmentation volume and compactly encodes the labelled regions and their boundaries by an iterative refinement scheme. The result for each brick is a list of labels, and a sequence of operations to reconstruct the brick which is further compressed using rANS-entropy coding. As the relative frequencies of operations are very similar across bricks, the entropy coding can use global frequency tables for an entire data set which enables efficient and effective parallel (de)compression. Our technique achieves high throughput (up to gigabytes per second both for compression and decompression) and strong compression ratios of about 1% to 3% of the original data set size while being applicable to GPU-based rendering. We evaluate our method for various data sets from different fields and demonstrate GPU-based volume visualization with on-the-fly decompression, level-of-detail rendering (with optional on-demand streaming of detail coefficients to the GPU), and a caching strategy for decompressed bricks for further performance improvement.",
                        "uid": "v-full-1392",
                        "time_stamp": "2023-10-24T03:12:00Z",
                        "time_start": "2023-10-24T03:12:00Z",
                        "time_end": "2023-10-24T03:24:00Z",
                        "paper_type": "full",
                        "keywords": [
                            "Segmentation volumes, lossless compression, volume rendering"
                        ],
                        "doi": "10.1109/TVCG.2023.3326573",
                        "fno": "",
                        "has_image": true,
                        "has_pdf": true,
                        "paper_award": "best",
                        "image_caption": "Our novel segmentation volume compression can be used to interactively render large data sets. The compression builds on a brick-wise hierarchical multi-grid of segmentation labels which are compactly stored as a stream of operations. Compression rates are between 1% and 3% on complex segmentation volumes.",
                        "external_paper_link": "",
                        "youtube_ff_link": "https://youtu.be/2BN2Lktbu4U",
                        "youtube_ff_id": "2BN2Lktbu4U",
                        "bunny_ff_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1392/v-full-1392_Preview.mp4?token=HliOynftLK8vnxdHHFY06S7hCm6VwLYd7Pyi54D2s5A&expires=1706590800",
                        "bunny_ff_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1392/v-full-1392_Preview.vtt?token=SXJ7lIXouz05wqACHL4pmw0NPj7F1fuLs0L-C6ZQh44&expires=1706590800",
                        "youtube_prerecorded_link": "https://youtu.be/pZfV6o7V7SU",
                        "youtube_prerecorded_id": "pZfV6o7V7SU",
                        "bunny_prerecorded_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1392/v-full-1392_Presentation.mp4?token=r-_tF1H1OEn3DeL47eq9G9jWPvNwkgfgk9VLkvTMgpE&expires=1706590800",
                        "bunny_prerecorded_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1392/v-full-1392_Presentation.vtt?token=rpNnr_Y8ThdhDTYCvCjULlLjBe8X6KC1ZmyXTSUtdLI&expires=1706590800"
                    },
                    {
                        "slot_id": "v-full-1256",
                        "session_id": "full0",
                        "title": "Swaying the Public? Impacts of Election Forecast Visualizations on Emotion, Trust, and Intention in the 2022 U.S. Midterms",
                        "contributors": [
                            "Fumeng Yang"
                        ],
                        "authors": [
                            "Fumeng Yang",
                            "Mandi Cai",
                            "Chloe Rose Mortenson",
                            "Hoda Fakhari",
                            "Ayse Deniz Lokmanoglu",
                            "Jessica Hullman",
                            "Steven L Franconeri",
                            "Nicholas Diakopoulos",
                            "Erik Nisbet",
                            "Matthew Kay"
                        ],
                        "abstract": "We conducted a longitudinal study during the 2022 U.S. midterm elections, investigating the real-world impacts of uncertainty visualizations. Using our forecast model of the governor elections in 33 states, we created a website and deployed four uncertainty visualizations for the election forecasts: single quantile dotplot (1-Dotplot), dual quantile dotplots (2-Dotplot), dual histogram intervals (2-Interval), and Plinko quantile dotplot (Plinko), an animated design with a physical and probabilistic analogy. Our online experiment ran from Oct. 18, 2022, to Nov. 23, 2022, involving 1,327 participants from 15 states. We use Bayesian multilevel modeling and post-stratification to produce demographically-representative estimates of people\u2019s emotions, trust in forecasts, and political participation intention. We find that election forecast visualizations can heighten emotions, increase trust, and slightly affect people\u2019s intentions to participate in elections. 2-Interval shows the strongest effects across all measures; 1-Dotplot increases trust the most after elections. Both visualizations create emotional and trust gaps between different partisan identities, especially when a Republican candidate is predicted to win. Our qualitative analysis uncovers the complex political and social contexts of election forecast visualizations, showcasing that visualizations may provoke polarization. This intriguing interplay between visualization types, partisanship, and trust exemplifies the fundamental challenge of disentangling visualization from its context, underscoring a need for deeper investigation into the real-world impacts of visualizations. Our preprint and supplements are available at https://doi.org/osf.io/ajq8f.",
                        "uid": "v-full-1256",
                        "time_stamp": "2023-10-24T03:24:00Z",
                        "time_start": "2023-10-24T03:24:00Z",
                        "time_end": "2023-10-24T03:36:00Z",
                        "paper_type": "full",
                        "keywords": [
                            "Uncertainty visualization, Probabilistic forecasts, Elections, Emotions, Trust, Political participation, Longitudinal study"
                        ],
                        "doi": "10.1109/TVCG.2023.3327356",
                        "fno": "",
                        "has_image": true,
                        "has_pdf": true,
                        "paper_award": "best",
                        "image_caption": "We conducted a longitudinal study during the 2022 U.S. midterm elections, investigating the real-world impacts of uncertainty visualizations. Using our forecast model of the governor elections in 33 states, we created a website and deployed four uncertainty visualizations for the election forecasts. Our online experiment ran from Oct. 18, 2022 to Nov. 23, 2022, involving 1,327 participants from 15 states. We find that election forecast visualizations can heighten emotions, increase trust, and slightly affect people\u2019s intentions to participate in elections. Our qualitative analysis uncovers the complex political and social contexts of election forecast visualizations, showcasing that visualizations may provoke polarization.",
                        "external_paper_link": "",
                        "youtube_ff_link": "https://youtu.be/4-XECJkfMec",
                        "youtube_ff_id": "4-XECJkfMec",
                        "bunny_ff_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1256/v-full-1256_Preview.mp4?token=Ks9qNCUUoJH0g8-Ijbpx9Oluh5TFAc-3XzpWab8kzTc&expires=1706590800",
                        "bunny_ff_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1256/v-full-1256_Preview.vtt?token=BItzz4QX-kEMoxlUXsmJU2gqUEWwmnBU5PZ11jLsASg&expires=1706590800",
                        "youtube_prerecorded_link": "https://youtu.be/LK9bxqOrrmw",
                        "youtube_prerecorded_id": "LK9bxqOrrmw",
                        "bunny_prerecorded_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1256/v-full-1256_Presentation.mp4?token=TGoPRi-Gkm2fDe2pzjZAop11RE_AelkxM9Ebue5N8lE&expires=1706590800",
                        "bunny_prerecorded_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1256/v-full-1256_Presentation.vtt?token=DAq-82cwFq4q8rDu5WwjKR9sX8li2sbV2mYCAXBeJQ8&expires=1706590800"
                    }
                ]
            },
            {
                "title": "Best Papers 2 (3:40pm-4:30pm)| -- Includes Best Short Paper",
                "session_id": "full1",
                "event_prefix": "v-full",
                "track": "plenary",
                "session_image": "full1.png",
                "chair": [
                    "G. Elisabeta Marai"
                ],
                "time_start": "2023-10-24T04:40:00Z",
                "time_end": "2023-10-24T05:30:00Z",
                "discord_category": "",
                "discord_channel": "plenary-1",
                "discord_channel_id": "1161407618322014349",
                "discord_link": "https://discordapp.com/channels/1161400451619627068/1161407618322014349",
                "zoom_private_meeting": "",
                "zoom_private_password": "",
                "zoom_private_link": "",
                "zoom_broadcast_link": "",
                "ff_link": "https://youtu.be/VNGalekn9YM",
                "time_slots": [
                    {
                        "slot_id": "v-full-1439",
                        "session_id": "full1",
                        "title": "TimeSplines: Sketch-based Authoring of Flexible and Idiosyncratic Timelines",
                        "contributors": [
                            "Anna Offenwanger"
                        ],
                        "authors": [
                            "Anna Offenwanger",
                            "Matthew Brehmer",
                            "Fanny Chevalier",
                            "Theophanis Tsandilas"
                        ],
                        "abstract": "Timelines are essential for visually communicating chronological narratives and reflecting on the personal and cultural significance of historical events. Existing visualization tools tend to support conventional linear representations, but fail to capture personal idiosyncratic conceptualizations of time. In response, we built TimeSplines, a visualization authoring tool that allows people to sketch multiple free-form temporal axes and populate them with heterogeneous, time-oriented data via incremental and lazy data binding. Authors can bend, compress, and expand temporal axes to emphasize or de-emphasize intervals based on their personal importance; they can also annotate the axes with text and figurative elements to convey contextual information. The results of two user studies show how people appropriate the concepts in TimeSplines to express their own conceptualization of time, while our curated gallery of images demonstrates the expressive potential of our approach.",
                        "uid": "v-full-1439",
                        "time_stamp": "2023-10-24T04:40:00Z",
                        "time_start": "2023-10-24T04:40:00Z",
                        "time_end": "2023-10-24T04:55:00Z",
                        "paper_type": "full",
                        "keywords": [
                            "Temporal Data, interaction design, communication / presentation, storytelling, sketch-based interface, lazy data binding"
                        ],
                        "doi": "10.1109/TVCG.2023.3326520",
                        "fno": "",
                        "has_image": true,
                        "has_pdf": true,
                        "paper_award": "best",
                        "image_caption": "The TimeSplines interface. Left: The canvas on which the idiosyncratic timelines are created and manipulated. Right: The data drawer which contains multiple heterogeneous datasets.",
                        "external_paper_link": "",
                        "youtube_ff_link": "https://youtu.be/qP0q5ySbN80",
                        "youtube_ff_id": "qP0q5ySbN80",
                        "bunny_ff_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1439/v-full-1439_Preview.mp4?token=5jCN8tTei5Hy1ZJz08MfPjTjvUWG1cMENPJnx-uJnRQ&expires=1706590800",
                        "bunny_ff_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1439/v-full-1439_Preview.vtt?token=f5xGSnvwTl2yuE1jOsW2yLRk0t9q4E57yf6TNMTl2iM&expires=1706590800",
                        "youtube_prerecorded_link": "https://youtu.be/vNWgwHK4ExY",
                        "youtube_prerecorded_id": "vNWgwHK4ExY",
                        "bunny_prerecorded_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1439/v-full-1439_Presentation.mp4?token=TuVxmXGx5SMkuEHAnr-ddW3pBW2G1TyTgfwNq1-2Tvc&expires=1706590800",
                        "bunny_prerecorded_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1439/v-full-1439_Presentation.vtt?token=xJicl9Ny70y3vlE395xOiiKjxydE3XpCJ3NEh_VpXYk&expires=1706590800"
                    },
                    {
                        "slot_id": "v-full-1716",
                        "session_id": "full1",
                        "title": "Visualization of Discontinuous Vector Field Topology",
                        "contributors": [
                            "Egzon Miftari"
                        ],
                        "authors": [
                            "Egzon Miftari",
                            "Daniel Durstewitz",
                            "Filip Sadlo"
                        ],
                        "abstract": "This paper extends the concept and the visualization of vector field topology to vector fields with discontinuities. We address the non-uniqueness of flow in such fields by introduction of a time-reversible concept of equivalence. This concept generalizes streamlines to streamsets and thus vector field topology to discontinuous vector fields in terms of invariant streamsets. We identify respective novel critical structures as well as their manifolds, investigate their interplay with traditional vector field topology, and detail the application and interpretation of our approach using specifically designed synthetic cases and a simulated case from physics.",
                        "uid": "v-full-1716",
                        "time_stamp": "2023-10-24T04:55:00Z",
                        "time_start": "2023-10-24T04:55:00Z",
                        "time_end": "2023-10-24T05:10:00Z",
                        "paper_type": "full",
                        "keywords": [
                            "Discontinuous vector field topology, equivalence in non-unique flow, non-smooth dynamical systems"
                        ],
                        "doi": "10.1109/TVCG.2023.3326519",
                        "fno": "",
                        "has_image": true,
                        "has_pdf": true,
                        "paper_award": "best",
                        "image_caption": "Topological analysis of vector field with discontinuity (transparent gray) exhibiting repelling sliding flow (red LIC). Qualitatively different equivalence streamsets (white) are separated by stable (blue) and unstable (red) manifolds. Equitrices (violet) separate streamsets of different dimensionality.",
                        "external_paper_link": "",
                        "youtube_ff_link": "https://youtu.be/v-jSC58mF-E",
                        "youtube_ff_id": "v-jSC58mF-E",
                        "bunny_ff_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1716/v-full-1716_Preview.mp4?token=TBF5ZI4uImNwHtQS-6TuCE8yG1ZMeAM1fLNFGt50TgE&expires=1706590800",
                        "bunny_ff_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1716/v-full-1716_Preview.vtt?token=inO1TJ2xm7yR3Y9WujzOc9nK0oCWwHFkKNvoHSEUI4I&expires=1706590800",
                        "youtube_prerecorded_link": "https://youtu.be/OuJbldoK7yw",
                        "youtube_prerecorded_id": "OuJbldoK7yw",
                        "bunny_prerecorded_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1716/v-full-1716_Presentation.mp4?token=LkZpW5_dPs-UqgaAumvS0B75RxrNrM8NtYAnvjfbfnA&expires=1706590800",
                        "bunny_prerecorded_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1716/v-full-1716_Presentation.vtt?token=GN4nx0Nj4ZoAVSf1G2mFvTfF5SFlhRSeSsB5bO57fhc&expires=1706590800"
                    },
                    {
                        "slot_id": "v-full-1226",
                        "session_id": "full1",
                        "title": "Vortex Lens: Interactive Vortex Core Line Extraction using Observed Line Integral Convolution",
                        "contributors": [
                            "Peter Rautek"
                        ],
                        "authors": [
                            "Peter Rautek",
                            "Xingdi Zhang",
                            "Bernhard Woschizka",
                            "Thomas Theussl",
                            "Markus Hadwiger"
                        ],
                        "abstract": "This paper describes a novel method for detecting and visualizing vortex structures in unsteady 2D fluid flows. The method is based on an interactive local reference frame estimation that minimizes the observed time derivative of the input flow field v(x, t). A locally optimal reference frame w(x, t) assists the user in the identification of physically observable vortex structures in Observed Line Integral Convolution (LIC) visualizations. The observed LIC visualizations are interactively computed and displayed in a user-steered vortex lens region, embedded in the context of a conventional LIC visualization outside the lens. The locally optimal reference frame is then used to detect observed critical points, where v = w, which are used to seed vortex core lines. Each vortex core line is computed as a solution of the ordinary differential equation (ODE) (see paper for equation), with an observed critical point as initial condition (w(t0), t0). During integration, we enforce a strict error bound on the difference between the extracted core line and the integration of a path line of the input vector field, i.e., a solution to the ODE (see paper for equation). We experimentally verify that this error depends on the step size of the core line integration. This ensures that our method extracts Lagrangian vortex core lines that are the simultaneous solution of both ODEs with a numerical error that is controllable by the integration step size. We show the usability of our method in the context of an interactive system using a lens metaphor, and evaluate the results in comparison to state-of-the-art vortex core line extraction methods.",
                        "uid": "v-full-1226",
                        "time_stamp": "2023-10-24T05:10:00Z",
                        "time_start": "2023-10-24T05:10:00Z",
                        "time_end": "2023-10-24T05:25:00Z",
                        "paper_type": "full",
                        "keywords": [
                            "Flow visualization, vortex detection, objectivity, observers, reference frames, Lie algebras, visual lens metaphors"
                        ],
                        "doi": "10.1109/TVCG.2023.3326915",
                        "fno": "",
                        "has_image": true,
                        "has_pdf": true,
                        "paper_award": "best",
                        "image_caption": "Fluid flow simulation visualized using line integral convolution and color coding of vorticity.   Inside the vortex lens observed line integral convolution is used to show a vortex at its actual position in space and time.",
                        "external_paper_link": "",
                        "youtube_ff_link": "https://youtu.be/dwYel_PpX-0",
                        "youtube_ff_id": "dwYel_PpX-0",
                        "bunny_ff_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1226/v-full-1226_Preview.mp4?token=Oxnm0Jw0s1rJAGMfYnUYRDWGObW3zFIjW_MXgY1dFrM&expires=1706590800",
                        "bunny_ff_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1226/v-full-1226_Preview.vtt?token=09fMfGj2AVNjWfHmy9vbo3PcniW3ZbuNwvEjR_n5h_o&expires=1706590800",
                        "youtube_prerecorded_link": "https://youtu.be/NkWG5RWrWsY",
                        "youtube_prerecorded_id": "NkWG5RWrWsY",
                        "bunny_prerecorded_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1226/v-full-1226_Presentation.mp4?token=IXNBWCxPesIXnITrlNCR_hVwTfgfwrZbSn0Bv_Ku8KI&expires=1706590800",
                        "bunny_prerecorded_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1226/v-full-1226_Presentation.vtt?token=O402TSsJtesdT-Mt_zT8t8gw3Es0H0BsR_ScxO9S_5w&expires=1706590800"
                    },
                    {
                        "slot_id": "v-short-1170",
                        "session_id": "full1",
                        "title": "Gridded Glyphmaps for Supporting Spatial COVID-19 Modelling",
                        "contributors": [
                            "Aidan Slingsby"
                        ],
                        "authors": [
                            "Aidan Slingsby",
                            "Richard Reeve",
                            "Claire Harris"
                        ],
                        "abstract": "We describe our use of gridded glyphmaps to support development of a repurposed COVID-19 infection model during the height of the pandemic. We found that gridded glyphmaps' ability to interactive summarise multivariate model input, intermediate results and outputs across multiple scales supported our model development tasks in ways that the modellers had not previously seen. We recount our experiences, reflect on potential to support more spatial model development more generally and suggest areas of further work.",
                        "uid": "v-short-1170",
                        "time_stamp": "2023-10-24T05:25:00Z",
                        "time_start": "2023-10-24T05:25:00Z",
                        "time_end": "2023-10-24T05:40:00Z",
                        "paper_type": "short",
                        "keywords": [
                            "Modelling, spatial, temporal, COVID-19, interaction."
                        ],
                        "doi": "",
                        "fno": "",
                        "has_image": true,
                        "has_pdf": true,
                        "paper_award": "best",
                        "image_caption": "This gridded glyphmap shows the proportions of population (x-axis) in each grid square, by age group (y-axis; youngest at top) in different COVID-19 infection states (hue).",
                        "external_paper_link": "",
                        "youtube_ff_link": "https://youtu.be/06uuEK8V1QU",
                        "youtube_ff_id": "06uuEK8V1QU",
                        "bunny_ff_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-short/v-short-1170/v-short-1170_Preview.mp4?token=Gijdqh5dq2ay6R9oGoK6Cj6DF4Lwv642nPOiU49jZ8A&expires=1706590800",
                        "bunny_ff_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-short/v-short-1170/v-short-1170_Preview.vtt?token=5mkYTOXfuJwINSQsUdQ0t0vqL1X1QXvk-PphtD5GAhs&expires=1706590800",
                        "youtube_prerecorded_link": "https://youtu.be/Hala28G3-NM",
                        "youtube_prerecorded_id": "Hala28G3-NM",
                        "bunny_prerecorded_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-short/v-short-1170/v-short-1170_Presentation.mp4?token=vurE6Qd1fZ0RuI4ckj8baduvyIgv0f__EIhf5hEEm5c&expires=1706590800",
                        "bunny_prerecorded_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-short/v-short-1170/v-short-1170_Presentation.vtt?token=yoh1PL81DZsKkiegD_vU-YTM7PkkVT2oyxvEcIEuNLM&expires=1706590800"
                    }
                ]
            },
            {
                "title": "Clustering & Scatterplots",
                "session_id": "full2",
                "event_prefix": "v-full",
                "track": "oneohfour",
                "session_image": "full2.png",
                "chair": [
                    "John Wenskovitch"
                ],
                "time_start": "2023-10-25T22:00:00Z",
                "time_end": "2023-10-25T23:15:00Z",
                "discord_category": "",
                "discord_channel": "104",
                "discord_channel_id": "1161741240665124954",
                "discord_link": "https://discordapp.com/channels/1161400451619627068/1161741240665124954",
                "zoom_private_meeting": "",
                "zoom_private_password": "",
                "zoom_private_link": "",
                "zoom_broadcast_link": "",
                "ff_link": "https://youtu.be/aLf-osAwzE0",
                "time_slots": [
                    {
                        "slot_id": "v-tvcg-9826389",
                        "session_id": "full2",
                        "title": "Automatic Scatterplot Design Optimization for Clustering Identification",
                        "contributors": [
                            "Ghulam Jilani Quadri"
                        ],
                        "authors": [
                            "Ghulam Jilani Quadri",
                            "Jennifer Adorno Nieves",
                            "Brenton Wiernik",
                            "Paul Rosen"
                        ],
                        "abstract": "Scatterplots are among the most widely used visualization techniques. Compelling scatterplot visualizations improve understanding of data by leveraging visual perception to boost awareness when performing specific visual analytic tasks. Design choices in scatterplots, such as graphical encodings or data aspects, can directly impact decision-making quality for low-level tasks like clustering. Hence, constructing frameworks that consider both the perceptions of the visual encodings and the task being performed enables optimizing visualizations to maximize efficacy. In this paper, we propose an automatic tool to optimize the design factors of scatterplots to reveal the most salient cluster structure. Our approach leverages the merge tree data structure to identify the clusters and optimize the choice of subsampling algorithm, sampling rate, marker size, and marker opacity used to generate a scatterplot image. We validate our approach with user and case studies that show it efficiently provides high-quality scatterplot designs from a large parameter space.",
                        "uid": "v-tvcg-9826389",
                        "time_stamp": "2023-10-25T22:00:00Z",
                        "time_start": "2023-10-25T22:00:00Z",
                        "time_end": "2023-10-25T22:12:00Z",
                        "paper_type": "full",
                        "keywords": [
                            "Scatterplot;overdraw;clustering;design optimization;perception;topological data analysis"
                        ],
                        "doi": "10.1109/TVCG.2022.3189883",
                        "fno": "9826389",
                        "has_image": true,
                        "has_pdf": false,
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "youtube_ff_link": "https://youtu.be/Xs8XtOFnIkI",
                        "youtube_ff_id": "Xs8XtOFnIkI",
                        "bunny_ff_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-tvcg/v-tvcg-9826389/v-tvcg-9826389_Preview.mp4?token=oslkCj-QcOLwJyMIKgPCAV9gYc3iMzbdZE4yYgL6gWE&expires=1706590800",
                        "bunny_ff_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-tvcg/v-tvcg-9826389/v-tvcg-9826389_Preview.vtt?token=xrUCgPbHenZJZLPH4rcxiKX6a1u9tgJaUrJhuot6xYY&expires=1706590800",
                        "youtube_prerecorded_link": "",
                        "youtube_prerecorded_id": "",
                        "bunny_prerecorded_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-tvcg/v-tvcg-9826389/v-tvcg-9826389_Presentation.mp4?token=s20aJfH-2g2ycHRjSpe1IKzukgnlp6CssYGfZIouhNY&expires=1706590800",
                        "bunny_prerecorded_subtitles": ""
                    },
                    {
                        "slot_id": "v-tvcg-10068257",
                        "session_id": "full2",
                        "title": "Interactive Subspace Cluster Analysis Guided by Semantic Attribute Associations",
                        "contributors": [
                            "Klaus Mueller"
                        ],
                        "authors": [
                            "Salman Mahmood",
                            "Klaus Mueller"
                        ],
                        "abstract": "Multivariate datasets with many variables are increasingly common in many application areas. Most methods approach multivariate data from a singular perspective. Subspace analysis techniques, on the other hand. provide the user a set of subspaces which can be used to view the data from multiple perspectives. However, many subspace analysis methods produce a huge amount of subspaces, a number of which are usually redundant. The enormity of the number of subspaces can be overwhelming to analysts, making it difficult for them to find informative patterns in the data. In this paper, we propose a new paradigm that constructs semantically consistent subspaces. These subspaces can then be expanded into more general subspaces by ways of conventional techniques. Our framework uses the labels/meta-data of a dataset to learn the semantic meanings and associations of the attributes. We employ a neural network to learn a semantic word embedding of the attributes and then divide this attribute space into semantically consistent subspaces. The user is provided with a visual analytics interface that guides the analysis process. We show via various examples that these semantic subspaces can help organize the data and guide the user in finding interesting patterns in the dataset.",
                        "uid": "v-tvcg-10068257",
                        "time_stamp": "2023-10-25T22:12:00Z",
                        "time_start": "2023-10-25T22:12:00Z",
                        "time_end": "2023-10-25T22:24:00Z",
                        "paper_type": "full",
                        "keywords": [
                            "High-dimensional data;multivariate data;subspace clustering;subspace analysis;cluster analysis"
                        ],
                        "doi": "10.1109/TVCG.2023.3256376",
                        "fno": "10068257",
                        "has_image": true,
                        "has_pdf": false,
                        "paper_award": "",
                        "image_caption": "The Semantic Subspace Explorer learns the semantic associations among data attributes, exposing interesting data patterns. The interface consists of:  the Control Panel, where users can determine the number of subspaces; the Semantic Space View, which visualizes the semantic space of all attributes; the Subspace Organizer, which shows an overview of the generated subspaces; the Subspace View which, shows a user-selected subspace in more detail as a biplot.",
                        "external_paper_link": "",
                        "youtube_ff_link": "https://youtu.be/0S5JSgF0ajE",
                        "youtube_ff_id": "0S5JSgF0ajE",
                        "bunny_ff_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-tvcg/v-tvcg-10068257/v-tvcg-10068257_Preview.mp4?token=DMoeF5TzAm-1U-eiPut_JJNAXbVDoP7CaVLBZsnsxpI&expires=1706590800",
                        "bunny_ff_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-tvcg/v-tvcg-10068257/v-tvcg-10068257_Preview.vtt?token=MdEU5fDUCnftaIstQii_2blVpDLsxlpvbmNOx3NPvK8&expires=1706590800",
                        "youtube_prerecorded_link": "",
                        "youtube_prerecorded_id": "",
                        "bunny_prerecorded_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-tvcg/v-tvcg-10068257/v-tvcg-10068257_Presentation.mp4?token=DpY3GvM6bl49nxZ7OVV8ZRm2rgyMuW-Y9ZYrMZ54RRU&expires=1706590800",
                        "bunny_prerecorded_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-tvcg/v-tvcg-10068257/v-tvcg-10068257_Presentation.vtt?token=nGDSRJhDR572r6dUrITJBLT-TUZXR-SFrgnjGcddQKw&expires=1706590800"
                    },
                    {
                        "slot_id": "v-tvcg-10173631",
                        "session_id": "full2",
                        "title": "Investigating the Visual Utility of Differentially Private Scatterplots",
                        "contributors": [
                            "Liudas Panavas"
                        ],
                        "authors": [
                            "Liudas Panavas",
                            "Tarik Crnovrsanin",
                            "Jane Adams",
                            "Jonathan Ullman",
                            "Ali Sargavad",
                            "Melanie Tory",
                            "Cody Dunne"
                        ],
                        "abstract": "Increasingly, visualization practitioners are working with, using, and studying private and sensitive data. There can be many stakeholders interested in the resulting analyses\u2014but widespread sharing of the data can cause harm to individuals, companies, and organizations. Practitioners are increasingly turning to differential privacy to enable public sharing of data with a guaranteed amount of privacy. Differential privacy algorithms do this by aggregating data statistics with noise, and this now-private data can be released visually with differentially private scatterplots. While the private visual output is affected by the algorithm choice, privacy level, bin number, data distribution, and user task, there is little guidance on how to choose and balance the effect of these parameters. To address this gap, we had experts examine 1,200 differentially private scatterplots created with a variety of parameter choices and tested their ability to see aggregate patterns in the private output (i.e. the visual utility of the chart). We synthesized these results to provide easy-to-use guidance for visualization practitioners releasing private data through scatterplots. Our findings also provide a ground truth for visual utility, which we use to benchmark automated utility metrics from a variety of fields. We demonstrate how multi-scale structural similarity (MS-SSIM), the metric most strongly correlated with our study\u2019s utility results, can be used to optimize parameter selection. A free copy of this paper along with all supplemental materials is available at https://osf.io/wej4s/.",
                        "uid": "v-tvcg-10173631",
                        "time_stamp": "2023-10-25T22:24:00Z",
                        "time_start": "2023-10-25T22:24:00Z",
                        "time_end": "2023-10-25T22:36:00Z",
                        "paper_type": "full",
                        "keywords": [
                            "Scatterplots;differential privacy;data study;visual utility"
                        ],
                        "doi": "10.1109/TVCG.2023.3292391",
                        "fno": "10173631",
                        "has_image": true,
                        "has_pdf": false,
                        "paper_award": "",
                        "image_caption": "Illustration of how a differentially private algorithm generates private data from the original data. The data is binned through count queries, denoted F(x). Noise is added from Laplace distributions dictated by epsilon. The output is a differentially private scatterplot composed of F(x) + noise  = M(x).",
                        "external_paper_link": "",
                        "youtube_ff_link": "https://youtu.be/bybDbzL-EWs",
                        "youtube_ff_id": "bybDbzL-EWs",
                        "bunny_ff_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-tvcg/v-tvcg-10173631/v-tvcg-10173631_Preview.mp4?token=dQHeYiFknw3J7ksz51dYUT4ivyH0vJxF_5-ZdHY0nFE&expires=1706590800",
                        "bunny_ff_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-tvcg/v-tvcg-10173631/v-tvcg-10173631_Preview.vtt?token=TpNknZgncbwR5BM3i6OSVAmEAYrsESut6wHW-nM_GaI&expires=1706590800",
                        "youtube_prerecorded_link": "",
                        "youtube_prerecorded_id": "",
                        "bunny_prerecorded_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-tvcg/v-tvcg-10173631/v-tvcg-10173631_Presentation.mp4?token=hxOzCT1Vydy3tzXOMn1ZsrgQ83eFHRGittzBqsLrjM8&expires=1706590800",
                        "bunny_prerecorded_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-tvcg/v-tvcg-10173631/v-tvcg-10173631_Presentation.vtt?token=iLkasxXbeSsU8qqpQOseUgjfH6JeaioapXax8vaC32Y&expires=1706590800"
                    },
                    {
                        "slot_id": "v-full-1026",
                        "session_id": "full2",
                        "title": "CLAMS: Cluster Ambiguity Measure for Estimating Perceptual Variability in Visual Clustering",
                        "contributors": [
                            "Hyeon Jeon"
                        ],
                        "authors": [
                            "Hyeon Jeon",
                            "Ghulam Jilani Quadri",
                            "Hyunwook Lee",
                            "Paul Rosen",
                            "Danielle Albers Szafir",
                            "Jinwook Seo"
                        ],
                        "abstract": "Visual clustering is a common perceptual task in scatterplots that supports diverse analytics tasks (e.g., cluster identification). However, even with the same scatterplot, the ways of perceiving clusters (i.e., conducting visual clustering) can differ due to the differences among individuals and ambiguous cluster boundaries. Although such perceptual variability casts doubt on the reliability of data analysis based on visual clustering, we lack a systematic way to efficiently assess this variability. In this research, we study perceptual variability in conducting visual clustering, which we call Cluster Ambiguity. To this end, we introduce CLAMS, a data-driven visual quality measure for automatically predicting cluster ambiguity in monochrome scatterplots. We first conduct a qualitative study to identify key factors that affect the visual separation of clusters (e.g., proximity or size difference between clusters). Based on study findings, we deploy a regression module that estimates the human-judged separability of two clusters. Then, CLAMS predicts cluster ambiguity by analyzing the aggregated results of all pairwise separability between clusters that are generated by the module. CLAMS outperforms widely-used clustering techniques in predicting ground truth cluster ambiguity. Meanwhile, CLAMS exhibits performance on par with human annotators. We conclude our work by presenting two applications for optimizing and benchmarking data mining techniques using CLAMS. The interactive demo of CLAMS is available at clusterambiguity.dev.",
                        "uid": "v-full-1026",
                        "time_stamp": "2023-10-25T22:36:00Z",
                        "time_start": "2023-10-25T22:36:00Z",
                        "time_end": "2023-10-25T22:48:00Z",
                        "paper_type": "full",
                        "keywords": [
                            "Cluster, scatterplot, perception, cluster analysis, cluster ambiguity, visual quality measure"
                        ],
                        "doi": "10.1109/TVCG.2023.3327201",
                        "fno": "",
                        "has_image": true,
                        "has_pdf": true,
                        "paper_award": "honorable",
                        "image_caption": "The comparison between the way of estimating the perceptual variability in conducting visual clustering, i.e., cluster ambiguity, of monochrome scatterplots.",
                        "external_paper_link": "",
                        "youtube_ff_link": "https://youtu.be/Jd3naMKyScU",
                        "youtube_ff_id": "Jd3naMKyScU",
                        "bunny_ff_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1026/v-full-1026_Preview.mp4?token=05vLOW1g6VhsyqZQSwKKEI91N3euUh_-HvqDeoFTRZc&expires=1706590800",
                        "bunny_ff_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1026/v-full-1026_Preview.vtt?token=QrzrFy-NhStBk4Z06Sl9JoYhZvo6WRuNz6DKNJA7BYY&expires=1706590800",
                        "youtube_prerecorded_link": "https://youtu.be/-Lx8iElkiDg",
                        "youtube_prerecorded_id": "-Lx8iElkiDg",
                        "bunny_prerecorded_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1026/v-full-1026_Presentation.mp4?token=Uzuw2orQXzp1cQw7ASg8a1chSXP6pufo2AfoEg8Uba0&expires=1706590800",
                        "bunny_prerecorded_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1026/v-full-1026_Presentation.vtt?token=5C1AKktPrsbIhN-l3gFiwPW8tvMLDiDgWIbXmD0g014&expires=1706590800"
                    },
                    {
                        "slot_id": "v-full-1025",
                        "session_id": "full2",
                        "title": "Classes are not Clusters: Improving Label-based Evaluation of Dimensionality Reduction",
                        "contributors": [
                            "Hyeon Jeon"
                        ],
                        "authors": [
                            "Hyeon Jeon",
                            "Yun-Hsin Kuo",
                            "Michael Aupetit",
                            "Kwan-Liu Ma",
                            "Jinwook Seo"
                        ],
                        "abstract": "A common way to evaluate the reliability of dimensionality reduction (DR) embeddings is to quantify how well labeled classes form compact, mutually separated clusters in the embeddings. This approach is based on the assumption that the classes stay as clear clusters in the original high-dimensional space. However, in reality, this assumption can be violated; a single class can be fragmented into multiple separated clusters, and multiple classes can be merged into a single cluster. We thus cannot always assure the credibility of the evaluation using class labels. In this paper, we introduce two novel quality measures\u2014Label-Trustworthiness and Label-Continuity (Label-T&C)\u2014advancing the process of DR evaluation based on class labels. Instead of assuming that classes are well-clustered in the original space, Label-T&C work by (1) estimating the extent to which classes form clusters in the original and embedded spaces and (2) evaluating the difference between the two. A quantitative evaluation showed that Label-T&C outperform widely used DR evaluation measures (e.g., Trustworthiness and Continuity, Kullback-Leibler divergence) in terms of the accuracy in assessing how well DR embeddings preserve the cluster structure, and are also scalable. Moreover, we present case studies demonstrating that Label-T&C can be successfully used for revealing the intrinsic characteristics of DR techniques and their hyperparameters.",
                        "uid": "v-full-1025",
                        "time_stamp": "2023-10-25T22:48:00Z",
                        "time_start": "2023-10-25T22:48:00Z",
                        "time_end": "2023-10-25T23:00:00Z",
                        "paper_type": "full",
                        "keywords": [
                            "Dimensionality Reduction, Reliability, Clustering, Clustering Validation Measures, Dimensionality Reduction Evaluation"
                        ],
                        "doi": "10.1109/TVCG.2023.3327187",
                        "fno": "",
                        "has_image": true,
                        "has_pdf": true,
                        "paper_award": "",
                        "image_caption": "Guidelines to infer the Cluster-Label Matching (CLM) of the high-dimensional data based on the CLM of the embedded data (left column) and the scores given by Label-T (Trustworthiness) and Label-C (Continuity) (first row)",
                        "external_paper_link": "",
                        "youtube_ff_link": "https://youtu.be/2z0zTS7lSMo",
                        "youtube_ff_id": "2z0zTS7lSMo",
                        "bunny_ff_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1025/v-full-1025_Preview.mp4?token=uzvMPLP9cnMQhwBsdodG2F4opdUgoObDYWX3cxETcig&expires=1706590800",
                        "bunny_ff_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1025/v-full-1025_Preview.vtt?token=oBGtA9nveuIrJmnbHRF5CWzy37RlOrh04Z6DqpvcqYk&expires=1706590800",
                        "youtube_prerecorded_link": "https://youtu.be/_lIlinnyHkA",
                        "youtube_prerecorded_id": "_lIlinnyHkA",
                        "bunny_prerecorded_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1025/v-full-1025_Presentation.mp4?token=8T2fvxnUCt9UrgRbCb8tLDtkLCbliPmgu9j_YJ0QgK0&expires=1706590800",
                        "bunny_prerecorded_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1025/v-full-1025_Presentation.vtt?token=XebHQU3EuIfB2a9-KVtvK1SRZ3S8AHTBO1fa9dVgaAE&expires=1706590800"
                    },
                    {
                        "slot_id": "v-full-1473",
                        "session_id": "full2",
                        "title": "Guaranteed Visibility in Scatterplots with Tolerance",
                        "contributors": [
                            "Loann Giovannangeli"
                        ],
                        "authors": [
                            "Loann Giovannangeli",
                            "Fr\u00e9d\u00e9ric Lalanne",
                            "Romain Giot",
                            "Romain Bourqui"
                        ],
                        "abstract": "In 2D visualizations, visibility of every datum's representation is crucial to ease the completion of visual tasks. Such a guarantee is barely respected in complex visualizations, mainly because of overdraws between datum representations that hide parts of the information (e.g., outliers). The literature proposes various Layout Adjustment algorithms to improve the readability of visualizations that suffer from this issue. Manipulating the data in high-dimensional, geometric or visual space; they rely on different strategies with their own strengths and weaknesses. Moreover, most of these algorithms are computationally expensive as they search for an exact solution in the geometric space and do not scale well to large datasets. This article proposes GIST, a layout adjustment algorithm that aims at optimizing three criteria: (i) node visibility guarantee (at least 1 pixel), (ii) node size maximization, and (iii) the original layout preservation. This is achieved by combining a search for the maximum node size that enables to draw all the data points without overlaps, with a limited budget of movements (i.e., limiting the distortions of the original layout). The method's basis relies on the idea that it is not necessary for two data representations to be strictly not overlapping in order to guarantee their visibility in visual space.  Our algorithm therefore uses a tolerance in the geometric space to determine the overlaps between pairs of data.  The tolerance is optimized such that the approximation computed in the geometric space can lead to visualization without noticeable overdraw after the data rendering rasterization. In addition, such an approximation helps to ease the algorithm's convergence as it reduces the number of constraints to resolve, enabling it to handle large datasets. We demonstrate the effectiveness of our approach by comparing its results to those of state-of-the-art methods on several large datasets.",
                        "uid": "v-full-1473",
                        "time_stamp": "2023-10-25T23:00:00Z",
                        "time_start": "2023-10-25T23:00:00Z",
                        "time_end": "2023-10-25T23:12:00Z",
                        "paper_type": "full",
                        "keywords": [
                            "Guaranteed visibility, Layout adjustment, Overlap removal, Scatterplots"
                        ],
                        "doi": "10.1109/TVCG.2023.3326596",
                        "fno": "",
                        "has_image": true,
                        "has_pdf": true,
                        "paper_award": "",
                        "image_caption": "GIST is a Layout Adjustment method to improve data visibility in scatter plots where their visual representations overlap.  It searches for the optimal node diameters that enable to remove overlaps with a limited quantity of movements. To speed up the process and minimize movements, it tolerates some overlap that do not hinder the visualization.  On the top left side, the initial layout represents 10000 points of the Fashion-MNIST dataset projected by t-SNE. On the bottom right side, the layout is post-processed by GIST. Data visual representations remain large enough, the overlaping issues are solved, and the cluster shapes are preserved.",
                        "external_paper_link": "",
                        "youtube_ff_link": "https://youtu.be/StvLgjDdixU",
                        "youtube_ff_id": "StvLgjDdixU",
                        "bunny_ff_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1473/v-full-1473_Preview.mp4?token=DVJQTgm5Reh6kbbPvpfjEeqDCTyZ3YpfbADrPUYAJ8k&expires=1706590800",
                        "bunny_ff_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1473/v-full-1473_Preview.vtt?token=m_zwyVBJgFsWOmiyq-UKM6FtLXFi7M-3vDXDvjtu2NA&expires=1706590800",
                        "youtube_prerecorded_link": "https://youtu.be/4rXA8Nn2M70",
                        "youtube_prerecorded_id": "4rXA8Nn2M70",
                        "bunny_prerecorded_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1473/v-full-1473_Presentation.mp4?token=xADn38Hd08YLnmiedM3m3sp4EF8spLG-Cu9zmDnbnxI&expires=1706590800",
                        "bunny_prerecorded_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1473/v-full-1473_Presentation.vtt?token=ggXl38NLkZIoMUyPXeb_RZoNXGKl9fNzrGX8xZAG3oU&expires=1706590800"
                    }
                ]
            },
            {
                "title": "Color and Accessibility",
                "session_id": "full3",
                "event_prefix": "v-full",
                "track": "oneohfive",
                "session_image": "full3.png",
                "chair": [
                    "Kim Marriott"
                ],
                "time_start": "2023-10-25T22:00:00Z",
                "time_end": "2023-10-25T23:15:00Z",
                "discord_category": "",
                "discord_channel": "105",
                "discord_channel_id": "1161741309346861067",
                "discord_link": "https://discordapp.com/channels/1161400451619627068/1161741309346861067",
                "zoom_private_meeting": "",
                "zoom_private_password": "",
                "zoom_private_link": "",
                "zoom_broadcast_link": "",
                "ff_link": "https://youtu.be/HycJWTgITVg",
                "time_slots": [
                    {
                        "slot_id": "v-tvcg-9919390",
                        "session_id": "full3",
                        "title": "Rainbow Colormaps: What are they good and bad for?",
                        "contributors": [
                            "Khairi Reda"
                        ],
                        "authors": [
                            "Khairi Reda"
                        ],
                        "abstract": "Guidelines for color use in quantitative visualizations have strongly discouraged the use of rainbow colormaps, arguing instead for smooth designs that do not induce visual discontinuities or implicit color categories. However, the empirical evidence behind this argument has been mixed and, at times, even contradictory. In practice, rainbow colormaps are widely used, raising questions about the true utility or dangers of such designs. We study how color categorization impacts the interpretation of scalar fields. We first introduce an approach to detect latent categories in colormaps. We hypothesize that the appearance of color categories in scalar visualizations can be beneficial in that they enhance the perception of certain features, although at the cost of rendering other features less noticeable. In three crowdsourced experiments, we show that observers are more likely to discriminate global, distributional features when viewing colorful scales that induce categorization (e.g., rainbow or diverging schemes). Conversely, when seeing the same data through a less colorful representation, observers are more likely to report localized features defined by small variations in the data. Participants showed awareness of these different affordances, and exhibited bias for exploiting the more discriminating colormap, given a particular feature type. Our results demonstrate costs and benefits for rainbows (and similarly colorful schemes), suggesting that their complementary utility for analyzing scalar data should not be dismissed. In addition to explaining potentially valid uses of rainbow, our study provides actionable guidelines, including on when such designs can be more harmful than useful. Data and materials are available at https://osf.io/xjhtf",
                        "uid": "v-tvcg-9919390",
                        "time_stamp": "2023-10-25T22:00:00Z",
                        "time_start": "2023-10-25T22:00:00Z",
                        "time_end": "2023-10-25T22:12:00Z",
                        "paper_type": "full",
                        "keywords": [
                            "Quantitative color encoding;rainbow colormaps;scalar fields;perception"
                        ],
                        "doi": "10.1109/TVCG.2022.3214771",
                        "fno": "9919390",
                        "has_image": true,
                        "has_pdf": false,
                        "paper_award": "",
                        "image_caption": "Analysis of color nameability in rainbow vs. other perceptual colormaps. This metric predicts the type of data features people attend to.",
                        "external_paper_link": "",
                        "youtube_ff_link": "https://youtu.be/-6tN0YRiRXQ",
                        "youtube_ff_id": "-6tN0YRiRXQ",
                        "bunny_ff_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-tvcg/v-tvcg-9919390/v-tvcg-9919390_Preview.mp4?token=5ryoTx-mdDvI_PwarkpMNigHvpEt4tySO7sORN9HyEE&expires=1706590800",
                        "bunny_ff_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-tvcg/v-tvcg-9919390/v-tvcg-9919390_Preview.vtt?token=8DMoohfAJDtltjCmh9bzTLomW7nsyukl0_P_s8LN6Xo&expires=1706590800",
                        "youtube_prerecorded_link": "https://youtu.be/uqIVnWz_eOY",
                        "youtube_prerecorded_id": "uqIVnWz_eOY",
                        "bunny_prerecorded_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-tvcg/v-tvcg-9919390/v-tvcg-9919390_Presentation.mp4?token=VvH1qBZlI6nKlXYVxSiBOn2sv6uF23kGvhNZMOlujTo&expires=1706590800",
                        "bunny_prerecorded_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-tvcg/v-tvcg-9919390/v-tvcg-9919390_Presentation.vtt?token=9ntZKmiRDFpA0vI7hefyTdzKW1qAkrcqNouP6L7oat4&expires=1706590800"
                    },
                    {
                        "slot_id": "v-tvcg-9904859",
                        "session_id": "full3",
                        "title": "Sensemaking Sans Power: Interactive Data Visualization Using Color-Changing Ink",
                        "contributors": [
                            "Biswaksen Patnaik"
                        ],
                        "authors": [
                            "Biswaksen Patnaik",
                            "Huaishu Peng",
                            "Niklas Elmqvist"
                        ],
                        "abstract": "We present an approach for interactively visualizing data using color-changing inks without the need for electronic displays or computers. Color-changing inks are a family of physical inks that change their color characteristics in response to an external stimulus such as heat, UV light, water, and pressure. Visualizations created using color-changing inks can embed interactivity in printed material without external computational media. In this paper, we survey current color-changing ink technology and then use these findings to derive a framework for how it can be used to construct interactive data representations. We also enumerate the interaction techniques possible using this technology. We then show some examples of how to use color-changing ink to create interactive visualizations on paper. While obviously limited in scope to situations where no power or computing is present, or as a complement to digital displays, our findings can be employed for paper, data physicalization, and embedded visualizations.",
                        "uid": "v-tvcg-9904859",
                        "time_stamp": "2023-10-25T22:12:00Z",
                        "time_start": "2023-10-25T22:12:00Z",
                        "time_end": "2023-10-25T22:24:00Z",
                        "paper_type": "full",
                        "keywords": [
                            "Physical computing;color-changing inks;design space;data physicalization;data visualization"
                        ],
                        "doi": "10.1109/TVCG.2022.3209631",
                        "fno": "9904859",
                        "has_image": true,
                        "has_pdf": false,
                        "paper_award": "",
                        "image_caption": "Changing the distribution on printed media though touch without electronic computation or power. Color changing ink is used that changes it's appearance from colored (right-top) to colorless (right-bottom) upon human touch. This changes the distribution (right-top to right-bottom) upon touch.",
                        "external_paper_link": "",
                        "youtube_ff_link": "https://youtu.be/Efb4LQuK6cg",
                        "youtube_ff_id": "Efb4LQuK6cg",
                        "bunny_ff_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-tvcg/v-tvcg-9904859/v-tvcg-9904859_Preview.mp4?token=6ImPRxeoJ9c6BCnd_HDHadLSVY3M6WJBqtwm3IWNjv8&expires=1706590800",
                        "bunny_ff_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-tvcg/v-tvcg-9904859/v-tvcg-9904859_Preview.vtt?token=MqF5EQgureiJKXs5Wsg3E-VvQaEwrd5rGSlYaCLps30&expires=1706590800",
                        "youtube_prerecorded_link": "https://youtu.be/3fuul3q2McY",
                        "youtube_prerecorded_id": "3fuul3q2McY",
                        "bunny_prerecorded_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-tvcg/v-tvcg-9904859/v-tvcg-9904859_Presentation.mp4?token=RLkM95Zrq3H6RWCDgV8yM0a25IgJjNKHVvltRDks1Jo&expires=1706590800",
                        "bunny_prerecorded_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-tvcg/v-tvcg-9904859/v-tvcg-9904859_Presentation.vtt?token=T4eYhp82ytqOR9qKySBgwbMVPf0e3VCdmH-w4xS2FY4&expires=1706590800"
                    },
                    {
                        "slot_id": "v-full-1580",
                        "session_id": "full3",
                        "title": "Data Navigator: An accessibility-centered data navigation toolkit",
                        "contributors": [
                            "Frank Elavsky"
                        ],
                        "authors": [
                            "Frank Elavsky",
                            "Lucas Nadolskis",
                            "Dominik Moritz"
                        ],
                        "abstract": "Making data visualizations accessible for people with disabilities remains a significant challenge in current practitioner efforts. Existing visualizations often lack an underlying navigable structure, fail to engage necessary input modalities, and rely heavily on visual-only rendering practices. These limitations exclude people with disabilities, especially users of assistive technologies. To address these challenges, we present Data Navigator: a system built on a dynamic graph structure, enabling developers to construct navigable lists, trees, graphs, and flows as well as spatial, diagrammatic, and geographic relations. Data Navigator supports a wide range of input modalities: screen reader, keyboard, speech, gesture detection, and even fabricated assistive devices. We present 3 case examples with Data Navigator, demonstrating we can provide accessible navigation structures on top of raster images, integrate with existing toolkits at scale, and rapidly develop novel prototypes. Data Navigator is a step towards making accessible data visualizations easier to design and implement.",
                        "uid": "v-full-1580",
                        "time_stamp": "2023-10-25T22:24:00Z",
                        "time_start": "2023-10-25T22:24:00Z",
                        "time_end": "2023-10-25T22:36:00Z",
                        "paper_type": "full",
                        "keywords": [
                            "accessibility, visualization, tools, technical materials, platforms, data interaction"
                        ],
                        "doi": "10.1109/TVCG.2023.3327393",
                        "fno": "",
                        "has_image": true,
                        "has_pdf": true,
                        "paper_award": "",
                        "image_caption": "Data Navigator provides visualization toolkits with rich, accessible navigation structures, robust input handling, and flexible, semantic rendering.",
                        "external_paper_link": "",
                        "youtube_ff_link": "https://youtu.be/z-J6kjYRohA",
                        "youtube_ff_id": "z-J6kjYRohA",
                        "bunny_ff_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1580/v-full-1580_Preview.mp4?token=0MiKr3HYsCd_dKP6CoxK3cWGlZ4EmLbD3NO7BQVTIuA&expires=1706590800",
                        "bunny_ff_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1580/v-full-1580_Preview.vtt?token=0U_BmMUPGfbB3BoZO6tLmNiBsS5YrLpluKY-SAMXJS4&expires=1706590800",
                        "youtube_prerecorded_link": "https://youtu.be/mxTeCOPT6Vo",
                        "youtube_prerecorded_id": "mxTeCOPT6Vo",
                        "bunny_prerecorded_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1580/v-full-1580_Presentation.mp4?token=ig22JZGCdAW-q9Gj-O0A03qoWJM5JV5cXwQLWOywGWU&expires=1706590800",
                        "bunny_prerecorded_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1580/v-full-1580_Presentation.vtt?token=qzoCPHOJxEa4iDt5SdhB_e-49NO0YfZqGheTlxqkvCQ&expires=1706590800"
                    },
                    {
                        "slot_id": "v-full-1830",
                        "session_id": "full3",
                        "title": "NL2Color: Refining Color Palettes for Charts with Natural Language",
                        "contributors": [
                            "Chuhan Shi"
                        ],
                        "authors": [
                            "Chuhan Shi",
                            "Weiwei Cui",
                            "Chengzhong Liu",
                            "Chengbo Zheng",
                            "Haidong Zhang",
                            "Qiong Luo",
                            "Xiaojuan Ma"
                        ],
                        "abstract": "Choice of color is critical to creating effective charts with an engaging, enjoyable, and informative reading experience. However, designing a good color palette for a chart is a challenging task for novice users who lack related design expertise. For example, they often find it difficult to articulate their abstract intentions and translate these intentions into effective editing actions to achieve a desired outcome. In this work, we present NL2Color, a tool that allows novice users to modify chart color palettes using natural language expressions of their desired outcomes. We first collected and categorized a dataset of 131 triplets, each consisting of an original color palette of a chart, an editing intent, and a new color palette designed by human experts according to the intent. Our tool employs a large language model (LLM) to substitute the colors in original palettes and produce new color palettes by selecting some of the triplets as few-shot prompts. To evaluate our tool, we conducted a comprehensive two-stage evaluation, including a crowd-sourcing study (N=71) and a within-subjects user study (N=12). The results indicate that the quality of the color palettes revised by NL2Color has no significantly large difference from those designed by human experts. The participants who used NL2Color obtained revised color palettes to their satisfaction in a shorter period and with less effort.",
                        "uid": "v-full-1830",
                        "time_stamp": "2023-10-25T22:36:00Z",
                        "time_start": "2023-10-25T22:36:00Z",
                        "time_end": "2023-10-25T22:48:00Z",
                        "paper_type": "full",
                        "keywords": [
                            "chart, color palette, natural language, large language model"
                        ],
                        "doi": "10.1109/TVCG.2023.3326522",
                        "fno": "",
                        "has_image": true,
                        "has_pdf": true,
                        "paper_award": "",
                        "image_caption": "Examples of color palette refinement by NL2Color. (a)-(d) show four pairs of an original chart (left) and a new chart (right) refined by NL2Color according to the request. (e) shows an original chart (left) and three new charts (right) NL2Color generated according to three refinement requests. The color palette of each chart is displayed above the chart. The original charts are collected from Vega-Lite.",
                        "external_paper_link": "",
                        "youtube_ff_link": "https://youtu.be/DdahFCNJnWY",
                        "youtube_ff_id": "DdahFCNJnWY",
                        "bunny_ff_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1830/v-full-1830_Preview.mp4?token=RzEicBBBTJEM_ToUAiBQk94mBgmFGS5I5Vohtnz41gA&expires=1706590800",
                        "bunny_ff_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1830/v-full-1830_Preview.vtt?token=leC8NJW8VtZUCwhDg-EBsYPQHYIZC0qAkeKa5BhGTY8&expires=1706590800",
                        "youtube_prerecorded_link": "https://youtu.be/mnYexe0jdAI",
                        "youtube_prerecorded_id": "mnYexe0jdAI",
                        "bunny_prerecorded_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1830/v-full-1830_Presentation.mp4?token=_0XtU5fRdklTMPJpB_qq5IoLfbcxdmPAP-VfuP-er-g&expires=1706590800",
                        "bunny_prerecorded_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1830/v-full-1830_Presentation.vtt?token=8llEkMV0ZNoVU18bUIVLXraiStZlE_F1wABaGAG5UsQ&expires=1706590800"
                    },
                    {
                        "slot_id": "v-full-1152",
                        "session_id": "full3",
                        "title": "Reducing Ambiguity in Line-based Density Plots by Image-space Colorization",
                        "contributors": [
                            "Yumeng Xue"
                        ],
                        "authors": [
                            "Yumeng Xue",
                            "Patrick Paetzold",
                            "Rebecca Kehlbeck",
                            "Bin Chen",
                            "Kin Chung Kwan",
                            "Yunhai Wang",
                            "Oliver Deussen"
                        ],
                        "abstract": "Line-based density plots are used to reduce visual clutter in line charts with a multitude of individual lines. However, these traditional density plots are often perceived ambiguously, which obstructs the user's identification of underlying trends in complex datasets. Thus, we propose a novel image space coloring method for line-based density plots that enhances their interpretability. Our method employs color not only to visually communicate data density but also to highlight similar regions in the plot, allowing users to identify and distinguish trends easily. We achieve this by performing hierarchical clustering based on the lines passing through each region and mapping the identified clusters to the hue circle using circular MDS. Additionally, we propose a heuristic approach to assign each line to the most probable cluster, enabling users to analyze density and individual lines. We motivate our method by conducting a small-scale user study, demonstrating the effectiveness of our method using synthetic and real-world datasets, and providing an interactive online tool for generating colored line-based density plots.",
                        "uid": "v-full-1152",
                        "time_stamp": "2023-10-25T22:48:00Z",
                        "time_start": "2023-10-25T22:48:00Z",
                        "time_end": "2023-10-25T23:00:00Z",
                        "paper_type": "full",
                        "keywords": [
                            "Trajectory data, times series, density-based visualization, clustering, coloring"
                        ],
                        "doi": "10.1109/TVCG.2023.3327149",
                        "fno": "",
                        "has_image": true,
                        "has_pdf": true,
                        "paper_award": "",
                        "image_caption": "Trajectories of taxi rides in Beijing: (a) A line-based visualization of the trajectories is cluttered and convoluted; (b) Using a density plot eliminates clutter, but the continuation of the trends is ambiguous, e.g., it looks like taxis follow the prominent, circular route around the city center; (c) Pixel-based colorization reveals clusters, we see that taxis mostly stay in one part of the city.",
                        "external_paper_link": "",
                        "youtube_ff_link": "https://youtu.be/-9BPVyeWbbs",
                        "youtube_ff_id": "-9BPVyeWbbs",
                        "bunny_ff_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1152/v-full-1152_Preview.mp4?token=gT0ikopMWglw07IlQ9RSsffOUdXTqy91n6mN8K6BYR0&expires=1706590800",
                        "bunny_ff_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1152/v-full-1152_Preview.vtt?token=x4tm-xpNrxL25iYX8zP7HmGfL5wScXBRHc5WVvL6l1c&expires=1706590800",
                        "youtube_prerecorded_link": "https://youtu.be/-B164EgzQKs",
                        "youtube_prerecorded_id": "-B164EgzQKs",
                        "bunny_prerecorded_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1152/v-full-1152_Presentation.mp4?token=l1b_cl9zAUSp1NKFxHd2hzllSPs2I27Rslxeg_Va_GY&expires=1706590800",
                        "bunny_prerecorded_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1152/v-full-1152_Presentation.vtt?token=7_bYHfCveOJYeeVMLWCGX5zYPe5XYfXYPrK_h2SHjac&expires=1706590800"
                    },
                    {
                        "slot_id": "v-full-1245",
                        "session_id": "full3",
                        "title": "TactualPlot: Spatializing Data as Sound using Sensory Substitution for Touchscreen Accessibility",
                        "contributors": [
                            "Pramod Chundury"
                        ],
                        "authors": [
                            "Pramod Chundury",
                            "Yasmin Reyazuddin",
                            "J. Bern Jordan",
                            "Jonathan Lazar",
                            "Niklas Elmqvist"
                        ],
                        "abstract": "Tactile graphics are one of the best ways for a blind person to perceive a chart using touch, but their fabrication is often costly, time-consuming, and does not lend itself to dynamic exploration. Refreshable haptic displays tend to be expensive and thus unavailable to most blind individuals. We propose TactualPlot, an approach to sensory substitution where touch interaction yields auditory (sonified) feedback. The technique relies on embodied cognition for spatial awareness\u2014i.e., individuals can perceive 2D touch locations of their fingers with reference to other 2D locations such as the relative locations of other fingers or chart characteristics that are visualized on touchscreens. Combining touch and sound in this way yields a scalable data exploration method for scatterplots where the data density under the user\u2019s fingertips is sampled. The sample regions can optionally be scaled based on how quickly the user moves their hand. Our development of TactualPlot was informed by formative design sessions with a blind collaborator, whose practice while using tactile scatterplots caused us to expand the technique for multiple fingers. We present results from an evaluation comparing our TactualPlot interaction technique to tactile graphics printed on swell touch paper.",
                        "uid": "v-full-1245",
                        "time_stamp": "2023-10-25T23:00:00Z",
                        "time_start": "2023-10-25T23:00:00Z",
                        "time_end": "2023-10-25T23:12:00Z",
                        "paper_type": "full",
                        "keywords": [
                            "Accessibility, sonification, multimodal interaction, crossmodal interaction, visualization"
                        ],
                        "doi": "10.1109/TVCG.2023.3326937",
                        "fno": "",
                        "has_image": true,
                        "has_pdf": true,
                        "paper_award": "",
                        "image_caption": "Teaser with 3 composite images. Image A shows a mockup of the tactual plot technique. Image B shows a screenshot of the TactualPlot system implemented on an iPad. A square-shaped scatterplot is shown at the bottom of the screen, and contains the data points, and the axes. The top part of the screenshot displays buttons that control the prototype. Image C shows the same scatter plot that has been printed on thermoform paper, and is overlaid on the iPad.",
                        "external_paper_link": "",
                        "youtube_ff_link": "https://youtu.be/ZuHqdff6lb0",
                        "youtube_ff_id": "ZuHqdff6lb0",
                        "bunny_ff_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1245/v-full-1245_Preview.mp4?token=Wb2RtoOPreF9hWOebaJ_pRNkuKi51eWvq1XBlaTX54I&expires=1706590800",
                        "bunny_ff_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1245/v-full-1245_Preview.vtt?token=qDSnmQukt1h1NpHc2skL70of6nn8aUXE5cTP6IBsonU&expires=1706590800",
                        "youtube_prerecorded_link": "https://youtu.be/FGOP_V_r_8c",
                        "youtube_prerecorded_id": "FGOP_V_r_8c",
                        "bunny_prerecorded_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1245/v-full-1245_Presentation.mp4?token=siCoxRrL-Wo6N1azdzYTT34FeKBVjA1W-0l442nd3TQ&expires=1706590800",
                        "bunny_prerecorded_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1245/v-full-1245_Presentation.vtt?token=Alre5e8UK7mo7zVjqCfsHkJEc6B03SaF2hsCoWAJrBs&expires=1706590800"
                    }
                ]
            },
            {
                "title": "Dashboards & Multiple Views",
                "session_id": "full4",
                "event_prefix": "v-full",
                "track": "oneohthree",
                "session_image": "full4.png",
                "chair": [
                    "Jonathan C Roberts"
                ],
                "time_start": "2023-10-25T03:00:00Z",
                "time_end": "2023-10-25T04:15:00Z",
                "discord_category": "",
                "discord_channel": "103",
                "discord_channel_id": "1161741183815524502",
                "discord_link": "https://discordapp.com/channels/1161400451619627068/1161741183815524502",
                "zoom_private_meeting": "",
                "zoom_private_password": "",
                "zoom_private_link": "",
                "zoom_broadcast_link": "",
                "ff_link": "https://youtu.be/VqLJhFHlMW4",
                "time_slots": [
                    {
                        "slot_id": "v-tvcg-10057994",
                        "session_id": "full4",
                        "title": "DMiner: Dashboard Design Mining and Recommendation",
                        "contributors": [
                            "Yanna Lin"
                        ],
                        "authors": [
                            "Yanna Lin",
                            "Haotian Li",
                            "Aoyu Wu",
                            "Yong Wang",
                            "Huamin Qu"
                        ],
                        "abstract": "Dashboards, which comprise multiple views on a single display, help analyze and communicate multiple perspectives of data simultaneously. However, creating effective and elegant dashboards is challenging since it requires careful and logical arrangement and coordination of multiple visualizations. To solve the problem, we propose a data-driven approach for mining design rules from dashboards and automating dashboard organization. Specifically, we focus on two prominent aspects of the organization: arrangement, which describes the position, size, and layout of each view in the display space; and coordination, which indicates the interaction between pairwise views. We build a new dataset containing 854 dashboards crawled online, and develop feature engineering methods for describing the single views and view-wise relationships in terms of data, encoding, layout, and interactions. Further, we identify design rules among those features and develop a recommender for dashboard design. We demonstrate the usefulness of DMiner through an expert study and a user study. The expert study shows that our extracted design rules are reasonable and conform to the design practice of experts. Moreover, a comparative user study shows that our recommender could help automate dashboard organization and reach human-level performance. In summary, our work offers a promising starting point for design mining visualizations to build recommenders.",
                        "uid": "v-tvcg-10057994",
                        "time_stamp": "2023-10-25T03:00:00Z",
                        "time_start": "2023-10-25T03:00:00Z",
                        "time_end": "2023-10-25T03:12:00Z",
                        "paper_type": "full",
                        "keywords": [
                            "Design Mining;Visualization Recommendation;Multiple-view Visualization;Dashboards"
                        ],
                        "doi": "10.1109/TVCG.2023.3251344",
                        "fno": "10057994",
                        "has_image": true,
                        "has_pdf": false,
                        "paper_award": "",
                        "image_caption": "The workflow of DMiner. This paper proposes DMiner as a framework for dashboard design mining and automatic recommendation. With the dashboard dataset as the input, DMiner: (A) first surveys a set of features important for dashboard design, and then extracts those features to delineate dashboard designs comprehensively. These are categorized into two types, i.e., single-view features such as data and encoding and pairwise-view features such as coordination and relative position; (B) then mines design rules using decision rule approach, and further filters them; and (C) finally leverages these rules for recommending dashboard arrangement and coordination.",
                        "external_paper_link": "",
                        "youtube_ff_link": "https://youtu.be/UdHc3hzkmSA",
                        "youtube_ff_id": "UdHc3hzkmSA",
                        "bunny_ff_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-tvcg/v-tvcg-10057994/v-tvcg-10057994_Preview.mp4?token=jloBkD9j8ZzK5FwWJcoVNZTOdn5kpXxaj842gC0cjE0&expires=1706590800",
                        "bunny_ff_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-tvcg/v-tvcg-10057994/v-tvcg-10057994_Preview.vtt?token=-EptFftJbwVWp-qHx9yQ2fBOC81JO5jV3FjOcUGUqnE&expires=1706590800",
                        "youtube_prerecorded_link": "https://youtu.be/H-7CWuvNU7M",
                        "youtube_prerecorded_id": "H-7CWuvNU7M",
                        "bunny_prerecorded_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-tvcg/v-tvcg-10057994/v-tvcg-10057994_Presentation.mp4?token=Q27DVjXDT1b5fE8dLkugbwsovgrxV_6tbCqp0jTVk-s&expires=1706590800",
                        "bunny_prerecorded_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-tvcg/v-tvcg-10057994/v-tvcg-10057994_Presentation.vtt?token=aTBX4ivzrGjKmp8HGd0TjyZJh6ynoav5rcI9zwQLQU0&expires=1706590800"
                    },
                    {
                        "slot_id": "v-tvcg-9916137",
                        "session_id": "full4",
                        "title": "Revisiting the Design Patterns of Composite Visualizations",
                        "contributors": [
                            "Dazhen Deng"
                        ],
                        "authors": [
                            "Dazhen Deng",
                            "Weiwei Cui",
                            "Xiyu Meng",
                            "Mengye Xu",
                            "Yu Liao",
                            "Haidong Zhang",
                            "Yingcai Wu"
                        ],
                        "abstract": "Composite visualization is a popular design strategy that represents complex datasets by integrating multiple visualizations in a meaningful and aesthetic layout, such as juxtaposition, overlay, and nesting. With this strategy, numerous novel designs have been proposed in visualization publications to accomplish various visual analytic tasks. However, there is a lack of understanding of design patterns of composite visualization, thus failing to provide holistic design space and concrete examples for practical use. In this paper, we opted to revisit the composite visualizations in IEEE VIS publications and answered what and how visualizations of different types are composed together. To achieve this, we first constructed a corpus of composite visualizations from the publications and analyzed common practices, such as the pattern distributions and co-occurrence of visualization types. From the analysis, we obtained insights into different design patterns on the utilities and their potential pros and cons. Furthermore, we discussed usage scenarios of our taxonomy and corpus and how future research on visualization composition can be conducted on the basis of this study.",
                        "uid": "v-tvcg-9916137",
                        "time_stamp": "2023-10-25T03:12:00Z",
                        "time_start": "2023-10-25T03:12:00Z",
                        "time_end": "2023-10-25T03:24:00Z",
                        "paper_type": "full",
                        "keywords": [
                            "Datasets;visual analytics;visualization specification;visualization design"
                        ],
                        "doi": "10.1109/TVCG.2022.3213565",
                        "fno": "9916137",
                        "has_image": true,
                        "has_pdf": false,
                        "paper_award": "",
                        "image_caption": "Design Patterns of Composite Visualizations",
                        "external_paper_link": "",
                        "youtube_ff_link": "https://youtu.be/96cTmyjtBg4",
                        "youtube_ff_id": "96cTmyjtBg4",
                        "bunny_ff_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-tvcg/v-tvcg-9916137/v-tvcg-9916137_Preview.mp4?token=N2vw7Hf-96mO0BDbhgBNZJscESFBZyOEypujP904J54&expires=1706590800",
                        "bunny_ff_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-tvcg/v-tvcg-9916137/v-tvcg-9916137_Preview.vtt?token=BYVTqIdQDSNTXPpCXhcKKbNoQvVBTbBHG2trUPo6PH0&expires=1706590800",
                        "youtube_prerecorded_link": "https://youtu.be/sF8Vzh7iIWQ",
                        "youtube_prerecorded_id": "sF8Vzh7iIWQ",
                        "bunny_prerecorded_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-tvcg/v-tvcg-9916137/v-tvcg-9916137_Presentation.mp4?token=ruSRtE3oIEq39q49_i9VWljKMjNRQQYHyI8BSLI-x7o&expires=1706590800",
                        "bunny_prerecorded_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-tvcg/v-tvcg-9916137/v-tvcg-9916137_Presentation.vtt?token=kVuHWHULiLqdf5CyWyq9LQGXuJ7gldRFpEI1_MCQr2g&expires=1706590800"
                    },
                    {
                        "slot_id": "v-tvcg-10029921",
                        "session_id": "full4",
                        "title": "Semi-Automatic Layout Adaptation for Responsive Multiple-View Visualization Design",
                        "contributors": [
                            "Wei Zeng"
                        ],
                        "authors": [
                            "Wei Zeng",
                            "Xi Chen",
                            "Yihan Hou",
                            "Lingdan Shao",
                            "Zhe Chu",
                            "Remco Chang"
                        ],
                        "abstract": "Multiple-view (MV) visualizations have become ubiquitous for visual communication and exploratory data visualization. However, most existing MV visualizations are designed for the desktop, which can be unsuitable for the continuously evolving displays of varying screen sizes. In this paper, we present a two-stage adaptation framework that supports the automated retargeting and semi-automated tailoring of a desktop MV visualization for rendering on devices with displays of varying sizes. First, we cast layout retargeting as an optimization problem and propose a simulated annealing technique that can automatically preserve the layout of multiple views. Second, we enable fine-tuning for the visual appearance of each view, using a rule-based auto configuration method complemented with an interactive interface for chart-oriented encoding adjustment. To demonstrate the feasibility and expressivity of our proposed approach, we present a gallery of MV visualizations that have been adapted from the desktop to small displays. We also report the result of a user study comparing visualizations generated using our approach with those by existing methods. The outcome indicates that the participants generally prefer visualizations generated using our approach and find them to be easier to use.",
                        "uid": "v-tvcg-10029921",
                        "time_stamp": "2023-10-25T03:24:00Z",
                        "time_start": "2023-10-25T03:24:00Z",
                        "time_end": "2023-10-25T03:36:00Z",
                        "paper_type": "full",
                        "keywords": [
                            "Layout adaptation;mobile devices;multiple-view visualization;responsive design"
                        ],
                        "doi": "10.1109/TVCG.2023.3240356",
                        "fno": "10029921",
                        "has_image": true,
                        "has_pdf": false,
                        "paper_award": "",
                        "image_caption": "This work considers the design space of a MV visualization with regards to visualization space, display property, and interaction.",
                        "external_paper_link": "",
                        "youtube_ff_link": "https://youtu.be/ajN1aMBt2ZA",
                        "youtube_ff_id": "ajN1aMBt2ZA",
                        "bunny_ff_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-tvcg/v-tvcg-10029921/v-tvcg-10029921_Preview.mp4?token=dv0ZGjOVqkGrLL2Vil2BdTfo1zKEk2mVLQBmTHCfieI&expires=1706590800",
                        "bunny_ff_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-tvcg/v-tvcg-10029921/v-tvcg-10029921_Preview.vtt?token=G85_Gyn_2XucnVXO5Tp3-0NjbHszEMymrfEZX_4VjgE&expires=1706590800",
                        "youtube_prerecorded_link": "https://youtu.be/CVM5BKn5sgw",
                        "youtube_prerecorded_id": "CVM5BKn5sgw",
                        "bunny_prerecorded_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-tvcg/v-tvcg-10029921/v-tvcg-10029921_Presentation.mp4?token=gj6ZL0ne6rfPFSaJySrYtJDZIFfs9y4Qw8EeIQLHXLs&expires=1706590800",
                        "bunny_prerecorded_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-tvcg/v-tvcg-10029921/v-tvcg-10029921_Presentation.vtt?token=5uQ0To-tnkL9jzOeOIxESk9Mh62J_dwJHA22FXHo_FQ&expires=1706590800"
                    },
                    {
                        "slot_id": "v-full-1015",
                        "session_id": "full4",
                        "title": "From Information to Choice: A Critical Inquiry Into Visualization Tools for Decision Making",
                        "contributors": [
                            "Emre Oral"
                        ],
                        "authors": [
                            "Emre Oral",
                            "Ria Chawla",
                            "Michel Wijkstra",
                            "Narges Mahyar",
                            "Evanthia Dimara"
                        ],
                        "abstract": "In the face of complex decisions, people often engage in a three-stage process that spans from (1) exploring and analyzing pertinent information (intelligence); (2) generating and exploring alternative options (design); and ultimately culminating in (3) selecting the optimal decision by evaluating discerning criteria (choice). We can fairly assume that all good visualizations aid in the \u201cintelligence\u201d stage by enabling data exploration and analysis. Yet, to what degree and how do visualization systems currently support the other decision making stages, namely \u201cdesign\u201d and \u201cchoice\u201d? To further explore this question, we conducted a comprehensive review of decision-focused visualization tools by examining publications in major visualization journals and conferences, including VIS, EuroVis, and CHI, spanning all available years. We employed a deductive coding method and in-depth analysis to assess whether and how visualization tools support design and choice. Specifically, we examined each visualization tool by (i) its degree of visibility for displaying decision alternatives, criteria, and preferences, and (ii) its degree of flexibility for offering means to manipulate the decision alternatives, criteria, and preferences with interactions such as adding, modifying, changing mapping, and filtering. Our review highlights the opportunities and challenges that decision-focused visualization tools face in realizing their full potential to support all stages of the decision making process. It reveals a surprising scarcity of tools that support all stages, and while most tools excel in offering visibility for decision criteria and alternatives, the degree of flexibility to manipulate these elements is often limited, and the lack of tools that accommodate decision preferences and their elicitation is notable. Based on our findings, to better support the choice stage, future research could explore enhancing flexibility levels and variety, exploring novel visualization paradigms, increasing algorithmic support, and ensuring that this automation is user-controlled via the enhanced flexibility levels. Our curated list of the 88 surveyed visualization tools is available in the OSF link (https://osf.io/nrasz/view_only=b92a90a34ae241449b5f2cd33383bfcb).",
                        "uid": "v-full-1015",
                        "time_stamp": "2023-10-25T03:36:00Z",
                        "time_start": "2023-10-25T03:36:00Z",
                        "time_end": "2023-10-25T03:48:00Z",
                        "paper_type": "full",
                        "keywords": [
                            "Decision making;visualization;state of the art;review;survey;design;interaction;multi-criteria decision making;MCDM"
                        ],
                        "doi": "10.1109/TVCG.2023.3326593",
                        "fno": "",
                        "has_image": true,
                        "has_pdf": true,
                        "paper_award": "",
                        "image_caption": "At the top, our research goal is depicted. To the left, a mosaic of 27 decision-focused visualization tools identified in our research. On the right, an image of a woman holding a magnifying glass, showcasing our evaluation metrics, including flexibility and visibility. DR, AI, domain, and MCDM are attached to the magnifier to highlight the additional tags regarding our investigation.",
                        "external_paper_link": "",
                        "youtube_ff_link": "https://youtu.be/X9M-YXH3vrI",
                        "youtube_ff_id": "X9M-YXH3vrI",
                        "bunny_ff_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1015/v-full-1015_Preview.mp4?token=yZqu5nhzABHMFMq0pif7vh5zE2KukKAu0sgBvkdYhpw&expires=1706590800",
                        "bunny_ff_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1015/v-full-1015_Preview.vtt?token=mVRzx9TjSzkxYqT1arYLvXGJVyoSmZi1JcnzKxJOojs&expires=1706590800",
                        "youtube_prerecorded_link": "https://youtu.be/tgE2x-2mq78",
                        "youtube_prerecorded_id": "tgE2x-2mq78",
                        "bunny_prerecorded_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1015/v-full-1015_Presentation.mp4?token=_J1-KBHpmcwwv2csMw42qXWzUDfvJBE3k1vdgFtnjhw&expires=1706590800",
                        "bunny_prerecorded_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1015/v-full-1015_Presentation.vtt?token=hk3xkQYLgzQIGFuny4gpnwq93fgRTesLkm50EgeKhD8&expires=1706590800"
                    },
                    {
                        "slot_id": "v-full-1122",
                        "session_id": "full4",
                        "title": "Heuristics for Supporting Cooperative Dashboard Design",
                        "contributors": [
                            "Vidya Setlur"
                        ],
                        "authors": [
                            "Vidya Setlur",
                            "Michael Correll",
                            "Arvind Satyanarayan",
                            "Melanie Tory"
                        ],
                        "abstract": "Dashboards are no longer mere static displays of metrics; through functionality such as interaction and storytelling, they have evolved to support analytic and communicative goals like monitoring and reporting. Existing dashboard design guidelines, however, are often unable to account for this expanded scope as they largely focus on best practices for visual design. In contrast, we frame dashboard design as facilitating an analytical conversation: a cooperative, interactive experience where a user may interact with, reason about, or freely query the underlying data. By drawing on established principles of conversational flow and communication, we define the concept of a cooperative dashboard as one that enables a fruitful and productive analytical conversation, and derive a set of 39 dashboard design heuristics to support effective analytical conversations. To assess the utility of this framing, we asked 52 computer science and engineering graduate students to apply our heuristics to critique and design dashboards as part of an ungraded, opt-in homework assignment. Feedback from participants demonstrates that our heuristics surface new reasons dashboards may fail, and encourage a more fluid, supportive, and responsive style of dashboard design. Our approach suggests several compelling directions for future work, including dashboard authoring tools that better anticipate conversational turn-taking, repair, and refinement and extending cooperative principles to other analytical workflows.",
                        "uid": "v-full-1122",
                        "time_stamp": "2023-10-25T03:48:00Z",
                        "time_start": "2023-10-25T03:48:00Z",
                        "time_end": "2023-10-25T04:00:00Z",
                        "paper_type": "full",
                        "keywords": [
                            "Gricean maxims, interactive visualization, conversation initiation, grounding, turn-taking, repair and refinement."
                        ],
                        "doi": "10.1109/TVCG.2023.3327158",
                        "fno": "",
                        "has_image": true,
                        "has_pdf": true,
                        "paper_award": "",
                        "image_caption": "Successes and failures of cooperative dashboard design throughout the five stages of an analytical conversation: initiation, grounding, turn taking, repair & refinement, and close.",
                        "external_paper_link": "",
                        "youtube_ff_link": "https://youtu.be/ESoh9DNeFgs",
                        "youtube_ff_id": "ESoh9DNeFgs",
                        "bunny_ff_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1122/v-full-1122_Preview.mp4?token=KyrgyGh9FePgxDfQUJn_m04IMngHrvSeSXShxHCwGOg&expires=1706590800",
                        "bunny_ff_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1122/v-full-1122_Preview.vtt?token=8FbFu3NcZDUKf-1BovZTWMoFvICeg31QO72vZD2K-dE&expires=1706590800",
                        "youtube_prerecorded_link": "https://youtu.be/zDR8CbsQznM",
                        "youtube_prerecorded_id": "zDR8CbsQznM",
                        "bunny_prerecorded_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1122/v-full-1122_Presentation.mp4?token=Q3m426YrG4_yxnfIEF5pnQBkMTgCHorFXHuPbjGrTzI&expires=1706590800",
                        "bunny_prerecorded_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1122/v-full-1122_Presentation.vtt?token=mgGxJJEVh_g_aSF_hlxz5JrYQqW8avGuSQ6aEU5dDdU&expires=1706590800"
                    },
                    {
                        "slot_id": "v-full-1642",
                        "session_id": "full4",
                        "title": "Transitioning to a Commercial Dashboarding System: Socio-technical Observations and Opportunities",
                        "contributors": [
                            "Conny Walchshofer"
                        ],
                        "authors": [
                            "Conny Walchshofer",
                            "Vaishali Dhanoa",
                            "Marc Streit",
                            "Miriah Meyer"
                        ],
                        "abstract": "Many long-established, traditional manufacturing businesses are becoming more digital and data-driven to improve their production. These companies are embracing visual analytics in these transitions through their adoption of commercial dashboarding systems. Although a number of studies have looked at the technical challenges of adopting these systems, very few have focused on the socio-technical issues that arise. In this paper, we report on the results of an interview study with 17 participants working in a range of roles at a long-established, traditional manufacturing company as they adopted Microsoft Power BI. The results highlight a number of socio-technical challenges the employees faced, including difficulties in training, using and creating dashboards, and transitioning to a modern digital company. Based on these results, we propose a number of opportunities for both companies and visualization researchers to improve these difficult transitions, as well as opportunities for rethinking how we design dashboarding systems for real-world use.",
                        "uid": "v-full-1642",
                        "time_stamp": "2023-10-25T04:00:00Z",
                        "time_start": "2023-10-25T04:00:00Z",
                        "time_end": "2023-10-25T04:12:00Z",
                        "paper_type": "full",
                        "keywords": [
                            "Interview study, socio-technical challenges, visual analytics"
                        ],
                        "doi": "10.1109/TVCG.2023.3326525",
                        "fno": "",
                        "has_image": true,
                        "has_pdf": true,
                        "paper_award": "",
                        "image_caption": "A summary of our 14 socio-technical observations and three opportunities from an interview study conducted at a large, conventional company as they transitioned to use Power BI.",
                        "external_paper_link": "",
                        "youtube_ff_link": "https://youtu.be/zt5TCRnhpko",
                        "youtube_ff_id": "zt5TCRnhpko",
                        "bunny_ff_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1642/v-full-1642_Preview.mp4?token=cAa46c8L67w9N1n0ziTijH6dKALtwEICPZPf_t3WxxI&expires=1706590800",
                        "bunny_ff_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1642/v-full-1642_Preview.vtt?token=deSdxKxOcFQdD02c_U_SPKtRJhqMSwoeNq9d0WpbaKo&expires=1706590800",
                        "youtube_prerecorded_link": "https://youtu.be/-1ydaDEiVg0",
                        "youtube_prerecorded_id": "-1ydaDEiVg0",
                        "bunny_prerecorded_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1642/v-full-1642_Presentation.mp4?token=N4hjX94SqIaVsPslIsudp0tJZ7MOyoayi211k6tNsmA&expires=1706590800",
                        "bunny_prerecorded_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1642/v-full-1642_Presentation.vtt?token=Ab8L7ZaNLYE1NZQjyAx081FtCaKe-OOfBgTZ4wT_ku8&expires=1706590800"
                    }
                ]
            },
            {
                "title": "Education and Assessment",
                "session_id": "full5",
                "event_prefix": "v-full",
                "track": "oneohsix",
                "session_image": "full5.png",
                "chair": [
                    "Niklas Elmqvist"
                ],
                "time_start": "2023-10-25T04:45:00Z",
                "time_end": "2023-10-25T06:00:00Z",
                "discord_category": "",
                "discord_channel": "106",
                "discord_channel_id": "1161741373410644119",
                "discord_link": "https://discordapp.com/channels/1161400451619627068/1161741373410644119",
                "zoom_private_meeting": "",
                "zoom_private_password": "",
                "zoom_private_link": "",
                "zoom_broadcast_link": "",
                "ff_link": "https://youtu.be/jKpJ4xBRVRQ",
                "time_slots": [
                    {
                        "slot_id": "v-full-1368",
                        "session_id": "full5",
                        "title": "SpeechMirror: A Multimodal Visual Analytics System for Personalized Reflection of Online Public Speaking Effectiveness",
                        "contributors": [
                            "Zeyuan Huang"
                        ],
                        "authors": [
                            "Zeyuan Huang",
                            "Qiang He",
                            "Kevin Maher",
                            "Xiaoming Deng",
                            "Yu-Kun Lai",
                            "Cuixia Ma",
                            "Shengfeng Qin",
                            "Yong-Jin Liu",
                            "Hongan Wang"
                        ],
                        "abstract": "As communications are increasingly taking place virtually, the ability to present well online is becoming an indispensable skill. Online speakers are facing unique challenges in engaging with remote audiences. However, there has been a lack of evidence-based  analytical systems for people to comprehensively evaluate online speeches and further discover possibilities for improvement. This paper introduces SpeechMirror, a visual analytics system facilitating reflection on a speech based on insights from a collection of online speeches. The system estimates the impact of different speech techniques on effectiveness and applies them to a speech to give users awareness of the performance of speech techniques. A similarity recommendation approach based on speech factors or script content supports guided exploration to expand knowledge of presentation evidence and accelerate the discovery of speech delivery possibilities. SpeechMirror provides intuitive visualizations and interactions for users to understand speech factors. Among them, SpeechTwin, a novel multimodal visual summary of speech, supports rapid understanding of critical speech factors and comparison of different speech samples, and SpeechPlayer augments the speech video by integrating visualization of the speaker\u2019s body language with interaction, for focused analysis. The system utilizes visualizations suited to the distinct nature of different speech factors for user comprehension. The proposed system and visualization techniques were evaluated with domain experts and amateurs, demonstrating usability for users with low visualization literacy and its efficacy in assisting users to develop insights for potential improvement.",
                        "uid": "v-full-1368",
                        "time_stamp": "2023-10-25T04:45:00Z",
                        "time_start": "2023-10-25T04:45:00Z",
                        "time_end": "2023-10-25T04:57:00Z",
                        "paper_type": "full",
                        "keywords": [
                            "Visual Analytics, Multimodal Analysis, Public Speaking, Online Presentation"
                        ],
                        "doi": "10.1109/TVCG.2023.3326932",
                        "fno": "",
                        "has_image": true,
                        "has_pdf": true,
                        "paper_award": "",
                        "image_caption": "Our SpeechMirror system facilitates reflection on a speech based on insights from a collection of online speeches.",
                        "external_paper_link": "",
                        "youtube_ff_link": "https://youtu.be/8HGPaautdlk",
                        "youtube_ff_id": "8HGPaautdlk",
                        "bunny_ff_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1368/v-full-1368_Preview.mp4?token=qhtFrLDc3R87CMjv3MAt_5G7-s6ML62GF44-ang4tOg&expires=1706590800",
                        "bunny_ff_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1368/v-full-1368_Preview.vtt?token=JV2uXuRbOLJ4IBTSiQr-jX_NduGpEpGsKKUlIO9cpS8&expires=1706590800",
                        "youtube_prerecorded_link": "https://youtu.be/80PUu8Ll7Q8",
                        "youtube_prerecorded_id": "80PUu8Ll7Q8",
                        "bunny_prerecorded_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1368/v-full-1368_Presentation.mp4?token=G_-XA5H8W8VMPRA5aQ71-x9oicI6g3puxSLdGzsvtBI&expires=1706590800",
                        "bunny_prerecorded_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1368/v-full-1368_Presentation.vtt?token=FuzbQwc8bi9tPT3KGXfX6hX6Gdzu5NP382RnGMJBHR8&expires=1706590800"
                    },
                    {
                        "slot_id": "v-full-1217",
                        "session_id": "full5",
                        "title": "VisGrader: Automatic Grading of D3 Visualizations",
                        "contributors": [
                            "Matthew Hull"
                        ],
                        "authors": [
                            "Matthew Hull",
                            "Vivian Pednekar",
                            "Hannah Murray",
                            "Nimisha Roy",
                            "Emmanuel Tung",
                            "Susanta Kumar Routray",
                            "Connor Guerin",
                            "Justin Lu Chen",
                            "Zijie J. Wang",
                            "Seongmin Lee",
                            "Max Mahdi Roozbahani",
                            "Duen Horng Chau"
                        ],
                        "abstract": "Manually grading D3 data visualizations is a challenging endeavor, and is especially difficult for large classes with hundreds of students. Grading an interactive visualization requires a combination of interactive, quantitative, and qualitative evaluation that are conventionally done manually and are difficult to scale up as the visualization complexity, data size, and number of students increase. We present VISGRADER, a first-of-its kind automatic grading method for D3 visualizations that scalably and precisely evaluates the data bindings, visual encodings, interactions, and design specifications used in a visualization. Our method enhances students\u2019 learning experience, enabling them to submit their code frequently and receive rapid feedback to better inform iteration and improvement to their code and visualization design. We have successfully deployed our method and auto-graded D3 submissions from more than 4000 students in a visualization course at Georgia Tech, and received positive feedback for expanding its adoption.",
                        "uid": "v-full-1217",
                        "time_stamp": "2023-10-25T04:57:00Z",
                        "time_start": "2023-10-25T04:57:00Z",
                        "time_end": "2023-10-25T05:09:00Z",
                        "paper_type": "full",
                        "keywords": [
                            "Automatic grading, D3 visualization, large class, Selenium, Gradescope grading platform"
                        ],
                        "doi": "10.1109/TVCG.2023.3327181",
                        "fno": "",
                        "has_image": true,
                        "has_pdf": true,
                        "paper_award": "",
                        "image_caption": "VisGrader: Automatic Grading of D3 Visualizations",
                        "external_paper_link": "",
                        "youtube_ff_link": "https://youtu.be/ip7L9Wfrnvs",
                        "youtube_ff_id": "ip7L9Wfrnvs",
                        "bunny_ff_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1217/v-full-1217_Preview.mp4?token=RwSp0ba1RoNYr57_8e63j4cRkaGW83QYYBEJHllTns4&expires=1706590800",
                        "bunny_ff_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1217/v-full-1217_Preview.vtt?token=rydRbDadDBiIJ97EdC5pPY-TsOyhXC5KLzpePKm5kgA&expires=1706590800",
                        "youtube_prerecorded_link": "https://youtu.be/4E6cevh6ol0",
                        "youtube_prerecorded_id": "4E6cevh6ol0",
                        "bunny_prerecorded_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1217/v-full-1217_Presentation.mp4?token=wqI4wfp8LVYzZ2VLRNU0LsSHNgjlfS0lfDQdMtbLs9g&expires=1706590800",
                        "bunny_prerecorded_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1217/v-full-1217_Presentation.vtt?token=9fdo4tOaEO7JAabgZwlNK7leOsYGdBh8YUKhPt6bvTA&expires=1706590800"
                    },
                    {
                        "slot_id": "v-tvcg-10045801",
                        "session_id": "full5",
                        "title": "Anchorage: Visual Analysis of Satisfaction in Customer Service Videos via Anchor Events",
                        "contributors": [
                            "Jason Wong"
                        ],
                        "authors": [
                            "Kam Kwai Wong",
                            "Xingbo Wang",
                            "Yong Wang",
                            "Jianben He",
                            "Rong Zhang",
                            "Huamin Qu"
                        ],
                        "abstract": "Delivering customer services through video communications has brought new opportunities to analyze customer satisfaction for quality management. However, due to the lack of reliable self-reported responses, service providers are troubled by the inadequate estimation of customer services and the tedious investigation into multimodal video recordings. We introduce Anchorage , a visual analytics system to evaluate customer satisfaction by summarizing multimodal behavioral features in customer service videos and revealing abnormal operations in the service process. We leverage the semantically meaningful operations to introduce structured event understanding into videos which help service providers quickly navigate to events of their interest. Anchorage supports a comprehensive evaluation of customer satisfaction from the service and operation levels and efficient analysis of customer behavioral dynamics via multifaceted visualization views. We extensively evaluate Anchorage through a case study and a carefully-designed user study. The results demonstrate its effectiveness and usability in assessing customer satisfaction using customer service videos. We found that introducing event contexts in assessing customer satisfaction can enhance its performance without compromising annotation precision. Our approach can be adapted in situations where unlabelled and unstructured videos are collected along with sequential records.",
                        "uid": "v-tvcg-10045801",
                        "time_stamp": "2023-10-25T05:09:00Z",
                        "time_start": "2023-10-25T05:09:00Z",
                        "time_end": "2023-10-25T05:21:00Z",
                        "paper_type": "full",
                        "keywords": [
                            "Customer satisfaction;video data;video visualization;visual analytics"
                        ],
                        "doi": "10.1109/TVCG.2023.3245609",
                        "fno": "10045801",
                        "has_image": true,
                        "has_pdf": false,
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "youtube_ff_link": "https://youtu.be/_X-cCCHUuvI",
                        "youtube_ff_id": "_X-cCCHUuvI",
                        "bunny_ff_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-tvcg/v-tvcg-10045801/v-tvcg-10045801_Preview.mp4?token=Md4s7Y21RQQtal2Ujxdm_cMPThUQKuumOKHh7DogWPI&expires=1706590800",
                        "bunny_ff_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-tvcg/v-tvcg-10045801/v-tvcg-10045801_Preview.vtt?token=Jt56ulq06zWVkNa7CtDaGi2bWGi2TP0WrRMUdLusYek&expires=1706590800",
                        "youtube_prerecorded_link": "https://youtu.be/WPAW0zrjppE",
                        "youtube_prerecorded_id": "WPAW0zrjppE",
                        "bunny_prerecorded_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-tvcg/v-tvcg-10045801/v-tvcg-10045801_Presentation.mp4?token=mkWwJNcv-82CfNTeQ19-skU04iQsmsaocucIUKkR8GY&expires=1706590800",
                        "bunny_prerecorded_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-tvcg/v-tvcg-10045801/v-tvcg-10045801_Presentation.vtt?token=ciqSv0BSC5_0NnCs-deNKgrVgiHqcR3OHzs2lI2clGQ&expires=1706590800"
                    },
                    {
                        "slot_id": "v-full-1210",
                        "session_id": "full5",
                        "title": "Adaptive Assessment of Visualization Literacy",
                        "contributors": [
                            "Yuan Cui"
                        ],
                        "authors": [
                            "Yuan Cui",
                            "Lily W. Ge",
                            "Yiren Ding",
                            "Fumeng Yang",
                            "Lane Harrison",
                            "Matthew Kay"
                        ],
                        "abstract": "Visualization literacy is an essential skill for accurately interpreting data to inform critical decisions. Consequently, it is vital to understand the evolution of this ability and devise targeted interventions to enhance it, requiring concise and repeatable assessments of visualization literacy for individuals. However, current assessments, such as the Visualization Literacy Assessment Test (VLAT), are time-consuming due to their fixed, lengthy format. To address this limitation, we develop two streamlined computerized adaptive tests (CATs) for visualization literacy, A-VLAT and A-CALVI, which measure the same set of skills as their original versions in half the number of questions. Specifically, we (1) employ item response theory (IRT) and non-psychometric constraints to construct adaptive versions of the assessments, (2) finalize the configurations of adaptation through simulation, (3) refine the composition of test items of A-CALVI via a qualitative study, and (4) demonstrate the test-retest reliability (ICC: 0.98 and 0.98) and convergent validity (correlation: 0.81 and 0.66) of both CATs via four online studies. We discuss practical recommendations for using our CATs and opportunities for further customization to leverage the full potential of adaptive assessments. All supplemental materials are available at https://osf.io/a6258/.",
                        "uid": "v-full-1210",
                        "time_stamp": "2023-10-25T05:21:00Z",
                        "time_start": "2023-10-25T05:21:00Z",
                        "time_end": "2023-10-25T05:33:00Z",
                        "paper_type": "full",
                        "keywords": [
                            "Visualization literacy, computerized adaptive testing, item response theory"
                        ],
                        "doi": "10.1109/TVCG.2023.3327165",
                        "fno": "",
                        "has_image": true,
                        "has_pdf": true,
                        "paper_award": "",
                        "image_caption": "The process of developing adaptive visualization literacy assessments: A-VLAT and A-CALVI. We start with the item banks of VLAT and CALVI and use the parameters from item analysis to construct the CAT algorithms for the adaptive assessments. We then evaluate their validity and reliability via four online studies. The annotations in blue are the components\u2019 corresponding sections.",
                        "external_paper_link": "",
                        "youtube_ff_link": "https://youtu.be/fKJifGleSYY",
                        "youtube_ff_id": "fKJifGleSYY",
                        "bunny_ff_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1210/v-full-1210_Preview.mp4?token=N1-k1AV4OfP2-DyRF_kGW3KrumdzmaeZyYECVM8zXq0&expires=1706590800",
                        "bunny_ff_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1210/v-full-1210_Preview.vtt?token=MyU6n5C6nv1MB1cYDpocjjaitvs-fA-v4QKXEN9rnH4&expires=1706590800",
                        "youtube_prerecorded_link": "https://youtu.be/4JWm4X8JQQQ",
                        "youtube_prerecorded_id": "4JWm4X8JQQQ",
                        "bunny_prerecorded_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1210/v-full-1210_Presentation.mp4?token=FyCcNu_won1W-G3N-8rVLxgfAzcTmwi5tXompZbCdHU&expires=1706590800",
                        "bunny_prerecorded_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1210/v-full-1210_Presentation.vtt?token=po0sm7r_s3gp9mlzo8Ccx9jSbTdhpsaIbUfP08gD6d4&expires=1706590800"
                    },
                    {
                        "slot_id": "v-full-1519",
                        "session_id": "full5",
                        "title": "Causality-Based Visual Analysis of Questionnaire Responses",
                        "contributors": [
                            "Renzhong Li"
                        ],
                        "authors": [
                            "Renzhong Li",
                            "Weiwei Cui",
                            "Tianqi Song",
                            "Xiao Xie",
                            "Rui Ding",
                            "Yun Wang",
                            "Haidong Zhang",
                            "Hong Zhou",
                            "Yingcai Wu"
                        ],
                        "abstract": "As the final stage of questionnaire analysis, causal reasoning is the key to turning responses into valuable insights and actionable items for decision-makers. During the questionnaire analysis, classical statistical methods (e.g., Differences-in-Differences) have been widely exploited to evaluate causality between questions. However, due to the huge search space and complex causal structure in data, causal reasoning is still extremely challenging and time-consuming, and often conducted in a trial-and-error manner. On the other hand, existing visual methods of causal reasoning face the challenge of bringing scalability and expert knowledge together and can hardly be used in the questionnaire scenario. In this work, we present a systematic solution to help analysts effectively and efficiently explore questionnaire data and derive causality. Based on the association mining algorithm, we dig question combinations with potential inner causality and help analysts interactively explore the causal sub-graph of each question combination. Furthermore, leveraging the requirements collected from the experts, we built a visualization tool and conducted a comparative study with the state-of-the-art system to show the usability and efficiency of our system.",
                        "uid": "v-full-1519",
                        "time_stamp": "2023-10-25T05:33:00Z",
                        "time_start": "2023-10-25T05:33:00Z",
                        "time_end": "2023-10-25T05:45:00Z",
                        "paper_type": "full",
                        "keywords": [
                            "Causal analysis, Questionnaire, Design study"
                        ],
                        "doi": "10.1109/TVCG.2023.3327376",
                        "fno": "",
                        "has_image": true,
                        "has_pdf": true,
                        "paper_award": "",
                        "image_caption": "The interface of QE: (A) The question list view displays all questions. (B) The question combination view provides an overview of the whole dataset. (C) The causal view presents the causality in a relevant question combination. (D) The respondent view visualizes the clusters of respondents divided by a set of relevant questions for users to deep dive.",
                        "external_paper_link": "",
                        "youtube_ff_link": "https://youtu.be/W4wfW9-WWLw",
                        "youtube_ff_id": "W4wfW9-WWLw",
                        "bunny_ff_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1519/v-full-1519_Preview.mp4?token=fb6Qwezus9M3IfFprqyH9Frue0qxuykTTh7xmILs_ZQ&expires=1706590800",
                        "bunny_ff_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1519/v-full-1519_Preview.vtt?token=Q0Y0vjQivualQLoob_ktJ7WtjAnIjNm86BNU2g00UgA&expires=1706590800",
                        "youtube_prerecorded_link": "",
                        "youtube_prerecorded_id": "",
                        "bunny_prerecorded_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1519/v-full-1519_Presentation.mp4?token=5UR28PHZehyboYW0bcQ59r27HWGhi1vd2n1KD6YWOYI&expires=1706590800",
                        "bunny_prerecorded_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1519/v-full-1519_Presentation.vtt?token=t2grVF7bzPHuRtGqe6OXcdVVC0vubKLdACq6kGLuphE&expires=1706590800"
                    },
                    {
                        "slot_id": "v-full-1428",
                        "session_id": "full5",
                        "title": "Challenges and Opportunities in Data Visualization Education: A Call to Action",
                        "contributors": [
                            "Benjamin Bach"
                        ],
                        "authors": [
                            "Benjamin Bach",
                            "Mandy Keck",
                            "Fateme Rajabiyazdi",
                            "Tatiana Losev",
                            "Isabel Meirelles",
                            "Jason Dykes",
                            "Robert S. Laramee",
                            "Mashael AlKadi",
                            "Christina Stoiber",
                            "Samuel Huron",
                            "Charles Perin",
                            "Luiz Morais",
                            "Wolfgang Aigner",
                            "Doris Kosminsky",
                            "Magdalena Boucher",
                            "S\u00f8ren Knudsen",
                            "Areti Manataki",
                            "Jan Aerts",
                            "Uta Hinrichs",
                            "Jonathan C Roberts",
                            "Sheelagh Carpendale"
                        ],
                        "abstract": "This paper is a call to action for research and discussion on data visualization education. As visualization evolves and spreads through our professional and personal lives, we need to understand how to support and empower a broad and diverse community of learners in visualization. Data Visualization is a diverse and dynamic discipline that combines knowledge from different fields, is tailored to suit diverse audiences and contexts, and frequently incorporates tacit knowledge. This complex nature leads to a series of interrelated challenges for data visualization education. Driven by a lack of consolidated knowledge, overview, and orientation for visualization education, the 21 authors of this paper\u2014educators and researchers in data visualization\u2014identify and describe 19 challenges informed by our collective practical experience. We organize these challenges around seven themes People, Goals & Assessment, Environment, Motivation, Methods, Materials, and Change. Across these themes, we formulate 43 research questions to address these challenges. As part of our call to action, we then conclude with 5 cross-cutting opportunities and respective action items: embrace DIVERSITY+INCLUSION, build COMMUNITIES, conduct RESEARCH, act AGILE, and relish RESPONSIBILITY. We aim to inspire researchers, educators and learners to drive visualization education forward and discuss why, how, who and where we educate, as we learn to use visualization to address challenges across many scales and many domains in a rapidly changing world: viseducationchallenges.github.io",
                        "uid": "v-full-1428",
                        "time_stamp": "2023-10-25T05:45:00Z",
                        "time_start": "2023-10-25T05:45:00Z",
                        "time_end": "2023-10-25T05:57:00Z",
                        "paper_type": "full",
                        "keywords": [
                            "Data Visualization, Education, Challenges"
                        ],
                        "doi": "10.1109/TVCG.2023.3327378",
                        "fno": "",
                        "has_image": true,
                        "has_pdf": true,
                        "paper_award": "",
                        "image_caption": "In this paper, we identify 19 challenges informed by our collective practical experience in data visualization education. We organize these challenges around seven themes and formulate 43 research questions to address these challenges. In a call to action, we conclude with 5 cross-cutting opportunities. We aim to inspire researchers and educators to drive visualization education forward and discuss why, how, who and where we educate.",
                        "external_paper_link": "",
                        "youtube_ff_link": "https://youtu.be/_xNwKJV4w2M",
                        "youtube_ff_id": "_xNwKJV4w2M",
                        "bunny_ff_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1428/v-full-1428_Preview.mp4?token=5dzEnQpXxEjexY9z1uT61TjQdKuNxdLhAAL0742sQnw&expires=1706590800",
                        "bunny_ff_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1428/v-full-1428_Preview.vtt?token=MSE-YmEKHEOlGjVAWNGx3fBXygzNvl_P1KOXlPhZBKU&expires=1706590800",
                        "youtube_prerecorded_link": "https://youtu.be/MtsLr5b9pDM",
                        "youtube_prerecorded_id": "MtsLr5b9pDM",
                        "bunny_prerecorded_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1428/v-full-1428_Presentation.mp4?token=2EAzKcjE4AP0M7iPRRydrMGn9DZ8hN7HnrPhwZeFFV0&expires=1706590800",
                        "bunny_prerecorded_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1428/v-full-1428_Presentation.vtt?token=bA9VYZb4WF4534UkdBo86-gfBO8YW13eLdUdJptkpMY&expires=1706590800"
                    }
                ]
            },
            {
                "title": "Evaluation",
                "session_id": "full6",
                "event_prefix": "v-full",
                "track": "oneohnine",
                "session_image": "full6.png",
                "chair": [
                    "Matthew Kay"
                ],
                "time_start": "2023-10-25T23:45:00Z",
                "time_end": "2023-10-26T01:00:00Z",
                "discord_category": "",
                "discord_channel": "109",
                "discord_channel_id": "1161741434647482379",
                "discord_link": "https://discordapp.com/channels/1161400451619627068/1161741434647482379",
                "zoom_private_meeting": "",
                "zoom_private_password": "",
                "zoom_private_link": "",
                "zoom_broadcast_link": "",
                "ff_link": "https://youtu.be/H_7LLGaA4ys",
                "time_slots": [
                    {
                        "slot_id": "v-tvcg-10034850",
                        "session_id": "full6",
                        "title": "A Qualitative Interview Study of Distributed Tracing Visualisation: A Characterisation of Challenges and Opportunities in Visualisation Research",
                        "contributors": [
                            "Thomas Davidson"
                        ],
                        "authors": [
                            "Thomas Davidson",
                            "Emily Wall",
                            "Jonathan Mace"
                        ],
                        "abstract": "Distributed tracing tools have emerged in recent years to enable the operators of modern internet applications to troubleshoot cross-component problems in deployed applications. Due to the rich, detailed diagnostic data captured by distributed tracing tools, effectively presenting this data is important. However, use of visualisation to enable perceptual sensemaking of this complex data in today\u2019s distributed tracing tools has received relatively little attention. Consequently, operators struggle to make effective use of existing tools. In this paper we present the first characterisation of distributed tracing visualisation through a qualitative interview study with six practitioners from two large internet companies. Across two rounds of 1-on-1 interviews we use grounded theory coding to establish users, extract concrete use cases and identify shortcomings of existing distributed tracing tools. We derive guidelines for development of future distributed tracing tools and expose several open research problems that have wide reaching implications for visualisation research and other domain applications.",
                        "uid": "v-tvcg-10034850",
                        "time_stamp": "2023-10-25T23:45:00Z",
                        "time_start": "2023-10-25T23:45:00Z",
                        "time_end": "2023-10-25T23:57:00Z",
                        "paper_type": "full",
                        "keywords": [
                            "Visualisation;Distributed Tracing;Systems"
                        ],
                        "doi": "10.1109/TVCG.2023.3241596",
                        "fno": "10034850",
                        "has_image": true,
                        "has_pdf": false,
                        "paper_award": "",
                        "image_caption": "A diagram depicting distributed systems and existing analytical solutions,  the work we carried out in this project - a qualitative interview study  and grounded theory coding, and the outputs of the project: a  characterisation of distributed tracing, 5 key challenges, and 8 design  guidelines",
                        "external_paper_link": "",
                        "youtube_ff_link": "https://youtu.be/pwhzN-8o5mo",
                        "youtube_ff_id": "pwhzN-8o5mo",
                        "bunny_ff_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-tvcg/v-tvcg-10034850/v-tvcg-10034850_Preview.mp4?token=NPJjKOjFiOYcUOodbVBAUWLy4ouTG_yHK-5u5VmPcyQ&expires=1706590800",
                        "bunny_ff_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-tvcg/v-tvcg-10034850/v-tvcg-10034850_Preview.vtt?token=Dz5HhnlZQzkrCXNMffxwf8sNN74q317RqbY61Uo1fyU&expires=1706590800",
                        "youtube_prerecorded_link": "https://youtu.be/VswMm2SEvWs",
                        "youtube_prerecorded_id": "VswMm2SEvWs",
                        "bunny_prerecorded_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-tvcg/v-tvcg-10034850/v-tvcg-10034850_Presentation.mp4?token=mWw1UPlT31QiX9MT_yzRFC2tk3_n5XdT6NY599GGB10&expires=1706590800",
                        "bunny_prerecorded_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-tvcg/v-tvcg-10034850/v-tvcg-10034850_Presentation.vtt?token=8PRnguSJ6hUgQW5zL5NhBGSycXF_6Ne8fO-Gnd5PBl4&expires=1706590800"
                    },
                    {
                        "slot_id": "v-tvcg-10002316",
                        "session_id": "full6",
                        "title": "Evaluating Glyph Design for Showing Large-Magnitude-Range Quantum Spins",
                        "contributors": [
                            "Jian Chen"
                        ],
                        "authors": [
                            "Henan Zhao",
                            "Garnett W. Bryant",
                            "Wesley Griffin",
                            "Judith E. Terrill",
                            "Jian Chen"
                        ],
                        "abstract": "We present experimental results to explore a form of bivariate glyphs for representing large-magnitude-range vectors. The glyphs meet two conditions: (1) two visual dimensions are separable; and (2) one of the two visual dimensions uses a categorical representation (e.g., a categorical colormap). We evaluate how much these two conditions determine the bivariate glyphs\u2019 effectiveness. The first experiment asks participants to perform three local tasks requiring reading no more than two glyphs. The second experiment scales up the search space in global tasks when participants must look at the entire scene of hundreds of vector glyphs to get an answer. Our results support that the first condition is necessary for local tasks when a few items are compared. But it is not enough for understanding a large amount of data. The second condition is necessary for perceiving global structures of examining very complex datasets. Participants\u2019 comments reveal that the categorical features in the bivariate glyphs trigger emergent optimal viewers\u2019 behaviors. This work contributes to perceptually accurate glyph representations for revealing patterns from large scientific results. We release source code, quantum physics data, training documents, participants\u2019 answers, and statistical analyses for reproducible science at https : //osf.io/4xcf5/?viewonly = 94123139df9c4ac984a1e0df811cd580.",
                        "uid": "v-tvcg-10002316",
                        "time_stamp": "2023-10-25T23:57:00Z",
                        "time_start": "2023-10-25T23:57:00Z",
                        "time_end": "2023-10-26T00:09:00Z",
                        "paper_type": "full",
                        "keywords": [
                            "Separable and integral dimension pairs;bivariate glyph;3D glyph;quantitative visualization;large-magnitude-range"
                        ],
                        "doi": "10.1109/TVCG.2022.3232591",
                        "fno": "10002316",
                        "has_image": true,
                        "has_pdf": false,
                        "paper_award": "",
                        "image_caption": "We introduce a novel glyph design capable of conveying both overall scene structures and local numerical values with precision akin to a text display. The glyph scene has been employed to analyze highly complex three-dimensional quantum spins at the National Institute of Standards and Technology.  In this design, colors and glyph sizes are employed to cluster similar data, facilitating the visual comprehension of the distribution of spin charge densities, with a very large dynamic range. Our use of size visual attributes simplifies the search process of locating specific items, enabling quantum physicists to quickly extract quantitative data from the glyphs, thereby enhancing their research efficiency.",
                        "external_paper_link": "",
                        "youtube_ff_link": "",
                        "youtube_ff_id": "",
                        "bunny_ff_link": "",
                        "bunny_ff_subtitles": "",
                        "youtube_prerecorded_link": "",
                        "youtube_prerecorded_id": "",
                        "bunny_prerecorded_link": "",
                        "bunny_prerecorded_subtitles": ""
                    },
                    {
                        "slot_id": "v-tvcg-10058545",
                        "session_id": "full6",
                        "title": "Evaluating the Impact of Uncertainty Visualization on Model Reliance",
                        "contributors": [
                            "Yixuan Wang"
                        ],
                        "authors": [
                            "Jieqiong Zhao",
                            "Yixuan Wang",
                            "Michelle V. Mancenido",
                            "Erin K. Chiou",
                            "Ross Maciejewski"
                        ],
                        "abstract": "Machine learning models have gained traction as decision support tools for tasks that require processing copious amounts of data. However, to achieve the primary benefits of automating this part of decision-making, people must be able to trust the machine learning model's outputs. In order to enhance people's trust and promote appropriate reliance on the model, visualization techniques such as interactive model steering, performance analysis, model comparison, and uncertainty visualization have been proposed. In this study, we tested the effects of two uncertainty visualization techniques in a college admissions forecasting task, under two task difficulty levels, using Amazon's Mechanical Turk platform. Results show that (1) people's reliance on the model depends on the task difficulty and level of machine uncertainty and (2) ordinal forms of expressing model uncertainty are more likely to calibrate model usage behavior. These outcomes emphasize that reliance on decision support tools can depend on the cognitive accessibility of the visualization technique and perceptions of model performance and task difficulty.",
                        "uid": "v-tvcg-10058545",
                        "time_stamp": "2023-10-26T00:09:00Z",
                        "time_start": "2023-10-26T00:09:00Z",
                        "time_end": "2023-10-26T00:21:00Z",
                        "paper_type": "full",
                        "keywords": [
                            "Uncertainty;model reliance;trust;human-machine collaborations"
                        ],
                        "doi": "10.1109/TVCG.2023.3251950",
                        "fno": "10058545",
                        "has_image": true,
                        "has_pdf": false,
                        "paper_award": "",
                        "image_caption": "In a human-AI teaming collaborative environment, humans have to appropriately incorporate AI/ML model predictions to fully leverage the benefits of automation. We conducted an experiment using a crowdsourcing platform to assess the impact of the presence and absence of uncertainty information on a college admission forecasting task. Furthermore, we examined how uncertainty information influences model adoption behavior using two uncertainty visualization techniques and two levels of task difficulty. Our findings indicate that ordinal forms of expressing model uncertainty are more likely to calibrate humans' adoption of model predictions.",
                        "external_paper_link": "",
                        "youtube_ff_link": "https://youtu.be/wUfmJPtPe-c",
                        "youtube_ff_id": "wUfmJPtPe-c",
                        "bunny_ff_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-tvcg/v-tvcg-10058545/v-tvcg-10058545_Preview.mp4?token=Eb22NMCn7hLbgL_lyVtPzwhcXOHXb_5zOK90OvCcQdY&expires=1706590800",
                        "bunny_ff_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-tvcg/v-tvcg-10058545/v-tvcg-10058545_Preview.vtt?token=Xo_Lh_DYfDFBJvY9JqsYdHO-jwPmxXSgePCKbGNmhVM&expires=1706590800",
                        "youtube_prerecorded_link": "https://youtu.be/7f0Kh2BiYJw",
                        "youtube_prerecorded_id": "7f0Kh2BiYJw",
                        "bunny_prerecorded_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-tvcg/v-tvcg-10058545/v-tvcg-10058545_Presentation.mp4?token=qA3qcw0ajE-_AXGFAsdjxEC5t0ZRVuNrihheQdAP0do&expires=1706590800",
                        "bunny_prerecorded_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-tvcg/v-tvcg-10058545/v-tvcg-10058545_Presentation.vtt?token=d1cPWe7om4-Gb6wdjEqu1D5sdqVW5qunu0N6r5W4w6A&expires=1706590800"
                    },
                    {
                        "slot_id": "v-tvcg-9905944",
                        "session_id": "full6",
                        "title": "Fitting Bell Curves to Data Distributions using Visualization",
                        "contributors": [
                            "Eric Newburger"
                        ],
                        "authors": [
                            "Eric Newburger",
                            "Michael Correll",
                            "Niklas Elmqvist"
                        ],
                        "abstract": "Idealized probability distributions, such as normal or other curves, lie at the root of confirmatory statistical tests. But how well do people understand these idealized curves? In practical terms, does the human visual system allow us to match sample data distributions with hypothesized population distributions from which those samples might have been drawn? And how do different visualization techniques impact this capability? This paper shares the results of a crowdsourced experiment that tested the ability of respondents to fit normal curves to four different data distribution visualizations: bar histograms, dotplot histograms, strip plots, and boxplots. We find that the crowd can estimate the center (mean) of a distribution with some success and little bias. We also find that people generally overestimate the standard deviation---which we dub the \"umbrella effect\" because people tend to want to cover the whole distribution using the curve, as if sheltering it from the heavens above---and that strip plots yield the best accuracy.",
                        "uid": "v-tvcg-9905944",
                        "time_stamp": "2023-10-26T00:21:00Z",
                        "time_start": "2023-10-26T00:21:00Z",
                        "time_end": "2023-10-26T00:33:00Z",
                        "paper_type": "full",
                        "keywords": [
                            "Graphical inference;visual statistics;statistics by eye;fitting distributions;crowdsourcing"
                        ],
                        "doi": "10.1109/TVCG.2022.3210763",
                        "fno": "9905944",
                        "has_image": true,
                        "has_pdf": false,
                        "paper_award": "",
                        "image_caption": "Can people fit a normal curve to a data distribution through visual interactions alone?   In other words, do we have the visual intuitions to see the connection between an image of a sample distribution and the population distribution from which those samples might have been drawn?    And does graphic design matter?  Our experiment sought answers to these questions.",
                        "external_paper_link": "",
                        "youtube_ff_link": "https://youtu.be/PU5Xd0fgit0",
                        "youtube_ff_id": "PU5Xd0fgit0",
                        "bunny_ff_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-tvcg/v-tvcg-9905944/v-tvcg-9905944_Preview.mp4?token=-z_RELQlxvR12eNFYYzyspCwjDHn3UxoRQ0ZaVNaduE&expires=1706590800",
                        "bunny_ff_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-tvcg/v-tvcg-9905944/v-tvcg-9905944_Preview.vtt?token=PHgSl7fc_fcIIczy_t8b_UrPQ5DyOhx59e2rl4ewrt0&expires=1706590800",
                        "youtube_prerecorded_link": "https://youtu.be/F9-aROXAhDw",
                        "youtube_prerecorded_id": "F9-aROXAhDw",
                        "bunny_prerecorded_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-tvcg/v-tvcg-9905944/v-tvcg-9905944_Presentation.mp4?token=GNXcpxys7Pcn_vl1jzCfVeVPWptTImhTHfAukuNQJns&expires=1706590800",
                        "bunny_prerecorded_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-tvcg/v-tvcg-9905944/v-tvcg-9905944_Presentation.vtt?token=MzNf0XnXHByLZjTgHkvmjXfHTgLeNAB6lf7l3tgMcWI&expires=1706590800"
                    },
                    {
                        "slot_id": "v-full-1048",
                        "session_id": "full6",
                        "title": "A Heuristic Approach for Dual Expert/End-User Evaluation of Guidance in Visual Analytics",
                        "contributors": [
                            "Alessio Arleo"
                        ],
                        "authors": [
                            "Davide Ceneda",
                            "Christopher Collins",
                            "Mennatallah El-Assady",
                            "Silvia Miksch",
                            "Christian Tominski",
                            "Alessio Arleo"
                        ],
                        "abstract": "Guidance can support users during the exploration and analysis of complex data. Previous research focused on characterizing the theoretical aspects of guidance in visual analytics and implementing guidance in different scenarios. However, the evaluation of guidance-enhanced visual analytics solutions remains an open research question. We tackle this question by introducing and validating a practical evaluation methodology for guidance in visual analytics. We identify eight quality criteria to be fulfilled and collect expert feedback on their validity. To facilitate actual evaluation studies, we derive two sets of heuristics. The first set targets heuristic evaluations conducted by expert evaluators. The second set facilitates end-user studies where participants actually use a guidance-enhanced system. By following such a dual approach, the different quality criteria of guidance can be examined from two different perspectives, enhancing the overall value of evaluation studies. To test the practical utility of our methodology, we employ it in two studies to gain insight into the quality of two guidance-enhanced visual analytics solutions, one being a work-in-progress research prototype, and the other being a publicly available visualization recommender system. Based on these two evaluations, we derive good practices for conducting evaluations of guidance in visual analytics and identify pitfalls to be avoided during such studies.",
                        "uid": "v-full-1048",
                        "time_stamp": "2023-10-26T00:33:00Z",
                        "time_start": "2023-10-26T00:33:00Z",
                        "time_end": "2023-10-26T00:45:00Z",
                        "paper_type": "full",
                        "keywords": [
                            "Guidance, heuristics, evaluation, visual analytics."
                        ],
                        "doi": "10.1109/TVCG.2023.3327152",
                        "fno": "",
                        "has_image": true,
                        "has_pdf": true,
                        "paper_award": "",
                        "image_caption": "We introduce and validate a practical evaluation methodology for guidance in visual analytics. We identify eight quality criteria to be fulfilled and to facilitate actual evaluation studies, we derive two sets of heuristics, to be used by VA Users and VA experts.",
                        "external_paper_link": "",
                        "youtube_ff_link": "https://youtu.be/P0LmNH6Sl34",
                        "youtube_ff_id": "P0LmNH6Sl34",
                        "bunny_ff_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1048/v-full-1048_Preview.mp4?token=0iWyC9kDmmgLdp9NU5W847EokCofbxHNAXn4w8MqkxY&expires=1706590800",
                        "bunny_ff_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1048/v-full-1048_Preview.vtt?token=XeYc60eMdGSElamHmzAORefySAx12CH1tuJsZIAXcVE&expires=1706590800",
                        "youtube_prerecorded_link": "https://youtu.be/e7Hu3C8Mj7M",
                        "youtube_prerecorded_id": "e7Hu3C8Mj7M",
                        "bunny_prerecorded_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1048/v-full-1048_Presentation.mp4?token=GpLtc2-oQbm3a9Nd_cV6SomRz3XPEkVUsoUEYHU0FXI&expires=1706590800",
                        "bunny_prerecorded_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1048/v-full-1048_Presentation.vtt?token=v3CF3k5sJ74LHSvGvNml7HlTjTPIwh9bDfOtoe830SI&expires=1706590800"
                    },
                    {
                        "slot_id": "v-full-1542",
                        "session_id": "full6",
                        "title": "The Arrangement of Marks Impacts Afforded Messages: Ordering, Partitioning, Spacing, and Coloring in Bar Charts",
                        "contributors": [
                            "Racquel Fygenson"
                        ],
                        "authors": [
                            "Racquel Fygenson",
                            "Steven L Franconeri",
                            "Enrico Bertini"
                        ],
                        "abstract": "Data visualizations present a massive number of potential messages to an observer. One might notice that one group's average is larger than another's, or that a difference in values is smaller than a difference between two others, or any of a combinatorial explosion of other possibilities. The message that a viewer tends to notice \u2013 the message that a visualization \u2018affords\u2019 \u2013 is strongly affected by how values are arranged in a chart, e.g., how the values are colored or positioned. Although understanding the mapping between a chart\u2019s arrangement and what viewers tend to notice is critical for creating guidelines and recommendation systems, current empirical work is insufficient to lay out clear rules. We present a set of empirical evaluations of how different messages--including ranking, grouping, and part-to-whole relationships--are afforded by variations in ordering, partitioning, spacing, and coloring of values, within the ubiquitous case study of bar graphs. In doing so, we introduce a quantitative method that is easily scalable, reviewable, and replicable, laying groundwork for further investigation of the effects of arrangement on message affordances across other visualizations and tasks. Pre-registration and all supplemental materials are available at https://osf.io/np3q7 and https://osf.io/bvy95, respectively.",
                        "uid": "v-full-1542",
                        "time_stamp": "2023-10-26T00:45:00Z",
                        "time_start": "2023-10-26T00:45:00Z",
                        "time_end": "2023-10-26T00:57:00Z",
                        "paper_type": "full",
                        "keywords": [
                            "Perception & cognition, Methodologies, Human-subjects qualitative studies, Human-subjects quantitative studies, Charts, diagrams and plots, General public"
                        ],
                        "doi": "10.1109/TVCG.2023.3326590",
                        "fno": "",
                        "has_image": true,
                        "has_pdf": true,
                        "paper_award": "",
                        "image_caption": "A 2x2 grid of bar chart arrangements. Top left: ORDER shows two bar charts. One sorted in ascending order, the other in descending order. Top right: PARTITIONS shows two bar charts. One has bars stacked into two columns, each consisting of three smaller bars. The other shows the bars side-by-side. Bottom left: COLOR V SPACING shows two bar charts. One is uniformly spaced, using color to group the bars, while the other is uniformly colored, using spacing to group the bars. Bottom right: SPACING shows two bar charts. One uniformly spaced, the other spaced to make two groups of bars.",
                        "external_paper_link": "",
                        "youtube_ff_link": "https://youtu.be/4JA9qD06WH4",
                        "youtube_ff_id": "4JA9qD06WH4",
                        "bunny_ff_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1542/v-full-1542_Preview.mp4?token=vjsowTM9yykFB7zGSRAa8BeNKp7D896Hwr7ZsKrN__o&expires=1706590800",
                        "bunny_ff_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1542/v-full-1542_Preview.vtt?token=IAHuytYTok9Q8_Z4khRZx4tGstqeY3G1lZLtmOIvdqE&expires=1706590800",
                        "youtube_prerecorded_link": "https://youtu.be/xOuQs8lYO1s",
                        "youtube_prerecorded_id": "xOuQs8lYO1s",
                        "bunny_prerecorded_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1542/v-full-1542_Presentation.mp4?token=SaqCOau8cEkG5VD9v9i1iI-y9flWMM22PcahtkajfPk&expires=1706590800",
                        "bunny_prerecorded_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1542/v-full-1542_Presentation.vtt?token=rKv7GmwwXzhjuLbkrG2e-s7CVwMfpA8Uo6ZJC-QKGyA&expires=1706590800"
                    }
                ]
            },
            {
                "title": "GeoVis| (Rooms 105/106 combined)",
                "session_id": "full7",
                "event_prefix": "v-full",
                "track": "oneohfive",
                "session_image": "full7.png",
                "chair": [
                    "Di Weng"
                ],
                "time_start": "2023-10-26T22:00:00Z",
                "time_end": "2023-10-26T23:15:00Z",
                "discord_category": "",
                "discord_channel": "105",
                "discord_channel_id": "1161741309346861067",
                "discord_link": "https://discordapp.com/channels/1161400451619627068/1161741309346861067",
                "zoom_private_meeting": "",
                "zoom_private_password": "",
                "zoom_private_link": "",
                "zoom_broadcast_link": "",
                "ff_link": "https://youtu.be/W2BIOCeo6Z4",
                "time_slots": [
                    {
                        "slot_id": "v-tvcg-10026242",
                        "session_id": "full7",
                        "title": "IF-City: Intelligible Fair City Planning to Measure, Explain and Mitigate Inequality",
                        "contributors": [
                            "Yan Lyu"
                        ],
                        "authors": [
                            "Yan Lyu",
                            "Hangxin Lu",
                            "Min Kyung Lee",
                            "Gerhard Schmitt",
                            "Brian Y. Lim"
                        ],
                        "abstract": "With the increasing pervasiveness of Artificial Intelligence (AI), many visual analytics tools have been proposed to examine fairness, but they mostly focus on data scientist users. Instead, tackling fairness must be inclusive and involve domain experts with specialized tools and workflows. Thus, domain-specific visualizations are needed for algorithmic fairness. Furthermore, while much work on AI fairness has focused on predictive decisions, less has been done for fair allocation and planning, which require human expertise and iterative design to integrate myriad constraints. We propose the Intelligible Fair Allocation (IF-Alloc) Framework that leverages explanations of causal attribution (Why), contrastive (Why Not) and counterfactual reasoning (What If, How To) to aid domain experts to assess and alleviate unfairness in allocation problems. We apply the framework to fair urban planning for designing cities that provide equal access to amenities and benefits for diverse resident types. Specifically, we propose an interactive visual tool, Intelligible Fair City Planner (IF-City), to help urban planners to perceive inequality across groups, identify and attribute sources of inequality, and mitigate inequality with automatic allocation simulations and constraint-satisfying recommendations (IF-Plan). We demonstrate and evaluate the usage and usefulness of IF-City on a real neighborhood in New York City, US, with practicing urban planners from multiple countries, and discuss generalizing our findings, application, and framework to other use cases and applications of fair allocation.",
                        "uid": "v-tvcg-10026242",
                        "time_stamp": "2023-10-26T22:00:00Z",
                        "time_start": "2023-10-26T22:00:00Z",
                        "time_end": "2023-10-26T22:12:00Z",
                        "paper_type": "full",
                        "keywords": [
                            "Explainable artificial intelligence;fairness;intelligibility;resource allocation;urban planning"
                        ],
                        "doi": "10.1109/TVCG.2023.3239909",
                        "fno": "10026242",
                        "has_image": true,
                        "has_pdf": false,
                        "paper_award": "",
                        "image_caption": "Intelligible Fair City Planner (IF-City) is an interactive visualization tool to help urban planners to perceive inequality across groups, identify and attribute sources of inequality, and mitigate inequality with automatic allocation simulations and constraint-satisfying recommendations.",
                        "external_paper_link": "",
                        "youtube_ff_link": "https://youtu.be/NGyb7Q0PShs",
                        "youtube_ff_id": "NGyb7Q0PShs",
                        "bunny_ff_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-tvcg/v-tvcg-10026242/v-tvcg-10026242_Preview.mp4?token=OceIqow_MowiZISjvO5gpWPkxbnQzJYgfnXYcIwfVNA&expires=1706590800",
                        "bunny_ff_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-tvcg/v-tvcg-10026242/v-tvcg-10026242_Preview.vtt?token=WbJO6PfMv2p8aODKzYX5fNdu8B6WmL_oy1t0Zkl6_Vk&expires=1706590800",
                        "youtube_prerecorded_link": "https://youtu.be/Bv49LJ7Ziiw",
                        "youtube_prerecorded_id": "Bv49LJ7Ziiw",
                        "bunny_prerecorded_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-tvcg/v-tvcg-10026242/v-tvcg-10026242_Presentation.mp4?token=qxjJxuN7vWPb3wKagKuB1sAwGGHiv8gj3mKrduJ4JPE&expires=1706590800",
                        "bunny_prerecorded_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-tvcg/v-tvcg-10026242/v-tvcg-10026242_Presentation.vtt?token=cqe0WFHQICnG9GFwBmVBbY41O5Mox7eTj8yQtbuAoC0&expires=1706590800"
                    },
                    {
                        "slot_id": "v-tvcg-10057127",
                        "session_id": "full7",
                        "title": "MoReVis: A Visual Summary for Spatiotemporal Moving Regions",
                        "contributors": [
                            "Giovani Valdrighi"
                        ],
                        "authors": [
                            "Giovani Valdrighi",
                            "Nivan Ferreira",
                            "Jorge Poco"
                        ],
                        "abstract": "Spatial and temporal interactions are central and fundamental in many activities in our world. A common problem faced when visualizing this type of data is how to provide an overview that helps users navigate efficiently. Traditional approaches use coordinated views or 3D metaphors like the Space-time cube to tackle this problem. However, they suffer from overplotting and often lack spatial context, hindering data exploration. More recent techniques, such as MotionRugs, propose compact temporal summaries based on 1D projection. While powerful, these techniques do not support the situation for which the spatial extent of the objects and their intersections is relevant, such as the analysis of surveillance videos or tracking weather storms. In this paper, we propose MoReVis, a visual overview of spatiotemporal data that considers the objects' spatial extent and strives to show spatial interactions among these objects by displaying spatial intersections. Like previous techniques, our method involves projecting the spatial coordinates to 1D to produce compact summaries. However, our solution's core consists of performing a layout optimization step that sets the size and positions of the visual marks on the summary to resemble the actual values on the original space. We also provide multiple interactive mechanisms to make interpreting the results more straightforward for the user. We perform an extensive experimental evaluation and usage scenarios. Moreover, we evaluated the usefulness of MoReVis in a study with 9 participants. The results point out the effectiveness and suitability of our method in representing different datasets compared to traditional techniques.",
                        "uid": "v-tvcg-10057127",
                        "time_stamp": "2023-10-26T22:12:00Z",
                        "time_start": "2023-10-26T22:12:00Z",
                        "time_end": "2023-10-26T22:24:00Z",
                        "paper_type": "full",
                        "keywords": [
                            "Spatiotemporal visualization;spatial interactions;spatial abstraction"
                        ],
                        "doi": "10.1109/TVCG.2023.3250166",
                        "fno": "10057127",
                        "has_image": true,
                        "has_pdf": false,
                        "paper_award": "",
                        "image_caption": "MoReVis is a visualization technique designed for spatiotemporal datasets of Moving Regions, entities with a spatial extension that evolves through time.  MoReVis uses a metaphor in which time is represented on one axis and space on another, and a curved ribbon of variable height represents each entity.  The resulting visualization is able to represent:  The movement of entities by the vertical movement of the ribbons,  The variable area of entities by the ribbon heights,  and the intersection between spatial regions through the intersection of ribbons.  A visual interface with linked panels and intersections was developed to support MoReVis.",
                        "external_paper_link": "",
                        "youtube_ff_link": "https://youtu.be/g0wuG4HuF4w",
                        "youtube_ff_id": "g0wuG4HuF4w",
                        "bunny_ff_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-tvcg/v-tvcg-10057127/v-tvcg-10057127_Preview.mp4?token=ES0NVBZKC4BbFSLtFLUMKqxXbZACeYa8NFiBinughDI&expires=1706590800",
                        "bunny_ff_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-tvcg/v-tvcg-10057127/v-tvcg-10057127_Preview.vtt?token=Kr0SMP8LzDWxKiCu2JohSccIVXruiDLk_E0u3wQurXI&expires=1706590800",
                        "youtube_prerecorded_link": "https://youtu.be/qPCx6ohVN_w",
                        "youtube_prerecorded_id": "qPCx6ohVN_w",
                        "bunny_prerecorded_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-tvcg/v-tvcg-10057127/v-tvcg-10057127_Presentation.mp4?token=dOeG4eVSE61LvSDLDu705eMWACCtBiJDq0_w1RXv2po&expires=1706590800",
                        "bunny_prerecorded_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-tvcg/v-tvcg-10057127/v-tvcg-10057127_Presentation.vtt?token=qXBxoiKFyL9ANA5VO1vbXfsaIFmNL4ewj1ty88-4qMg&expires=1706590800"
                    },
                    {
                        "slot_id": "v-tvcg-9991899",
                        "session_id": "full7",
                        "title": "Multilevel Visual Analysis of Aggregate Geo-Networks",
                        "contributors": [
                            "Zikun Deng"
                        ],
                        "authors": [
                            "Zikun Deng",
                            "Shifu Chen",
                            "Xiao Xie",
                            "Guodao Sun",
                            "Mingliang Xu",
                            "Di Weng",
                            "Yingcai Wu"
                        ],
                        "abstract": "Numerous patterns found in urban phenomena, such as air pollution and human mobility, can be characterized as many directed geospatial networks (geo-networks) that represent spreading processes in urban space. These geo-networks can be analyzed from multiple levels, ranging from the macro-level of summarizing all geo-networks, meso-level of comparing or summarizing parts of geo-networks, and micro-level of inspecting individual geo-networks. Most of the existing visualizations cannot support multilevel analysis well. These techniques work by: 1) showing geo-networks separately with multiple maps leads to heavy context switching costs between different maps; 2) summarizing all geo-networks into a single network can lead to the loss of individual information; 3) drawing all geo-networks onto one map might suffer from the visual scalability issue in distinguishing individual geo-networks. In this study, we propose GeoNetverse, a novel visualization technique for analyzing aggregate geo-networks from multiple levels. Inspired by metro maps, GeoNetverse balances the overview and details of the geo-networks by placing the edges shared between geo-networks in a stacked manner. To enhance the visual scalability, GeoNetverse incorporates a level-of-detail rendering, a progressive crossing minimization, and a coloring technique. A set of evaluations was conducted to evaluate GeoNetverse from multiple perspectives.",
                        "uid": "v-tvcg-9991899",
                        "time_stamp": "2023-10-26T22:24:00Z",
                        "time_start": "2023-10-26T22:24:00Z",
                        "time_end": "2023-10-26T22:36:00Z",
                        "paper_type": "full",
                        "keywords": [
                            "Geospatial network;multilevel analysis;information visualization;graph drawing"
                        ],
                        "doi": "10.1109/TVCG.2022.3229953",
                        "fno": "9991899",
                        "has_image": true,
                        "has_pdf": false,
                        "paper_award": "",
                        "image_caption": "A novel visualization is proposed for depicting mutliple geo-network on the map. First, the geo-networks are aggregated as one and each geo-network is embbed into the aggregation by stacking the edges. The visualization is equipped with level-of-detail rendering, crossing minimization, and coloring for improved visual scalablity.",
                        "external_paper_link": "",
                        "youtube_ff_link": "https://youtu.be/aI6vnjmqUPA",
                        "youtube_ff_id": "aI6vnjmqUPA",
                        "bunny_ff_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-tvcg/v-tvcg-9991899/v-tvcg-9991899_Preview.mp4?token=5XfwobgNzbBLvfeSXpOqRRkvUDyWzlOhXCmnpZLbKyQ&expires=1706590800",
                        "bunny_ff_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-tvcg/v-tvcg-9991899/v-tvcg-9991899_Preview.vtt?token=WN9sOQchJDJdXSm26lUpUMVQ2d1DSznaEvBEZgT-NgY&expires=1706590800",
                        "youtube_prerecorded_link": "https://youtu.be/Iy_0s44jrgs",
                        "youtube_prerecorded_id": "Iy_0s44jrgs",
                        "bunny_prerecorded_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-tvcg/v-tvcg-9991899/v-tvcg-9991899_Presentation.mp4?token=-KDmZPY5FZtueNMN1W6_nGk78Rm5rzESL8QjYfda8Nw&expires=1706590800",
                        "bunny_prerecorded_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-tvcg/v-tvcg-9991899/v-tvcg-9991899_Presentation.vtt?token=CmKlhKlkOqpVNdrqNH1AvQDuf_z3-EbBMwFKIwDdM5M&expires=1706590800"
                    },
                    {
                        "slot_id": "v-tvcg-9870679",
                        "session_id": "full7",
                        "title": "When, Where and How does it fail? A Spatial-temporal Visual Analytics Approach for Interpretable Object Detection in Autonomous Driving",
                        "contributors": [
                            "Siming Chen"
                        ],
                        "authors": [
                            "Junhong Wang",
                            "Yun Li",
                            "Zhaoyu Zhou",
                            "Chengshun Wang",
                            "Yijie Hou",
                            "Li Zhang",
                            "Xiangyang Xue",
                            "Michael Kamp",
                            "Xiaolong (Luke) Zhang",
                            "Siming Chen"
                        ],
                        "abstract": "Arguably the most representative application of artificial intelligence, autonomous driving systems usually rely on computer vision techniques to detect the situations of the external environment. Object detection underpins the ability of scene understanding in such systems. However, existing object detection algorithms often behave as a black box, so when a model fails, no information is available on When, Where and How the failure happened. In this paper, we propose a visual analytics approach to help model developers interpret the model failures. The system includes the micro- and macro-interpreting modules to address the interpretability problem of object detection in autonomous driving. The micro-interpreting module extracts and visualizes the features of a convolutional neural network (CNN) algorithm with density maps, while the macro-interpreting module provides spatial-temporal information of an autonomous driving vehicle and its environment. With the situation awareness of the spatial, temporal and neural network information, our system facilitates the understanding of the results of object detection algorithms, and helps the model developers better understand, tune and develop the models. We use real-world autonomous driving data to perform case studies by involving domain experts in computer vision and autonomous driving to evaluate our system. The results from our interviews with them show the effectiveness of our approach.",
                        "uid": "v-tvcg-9870679",
                        "time_stamp": "2023-10-26T22:36:00Z",
                        "time_start": "2023-10-26T22:36:00Z",
                        "time_end": "2023-10-26T22:48:00Z",
                        "paper_type": "full",
                        "keywords": [
                            "Autonomous driving;spatial-temporal visual analytics;interpretability"
                        ],
                        "doi": "10.1109/TVCG.2022.3201101",
                        "fno": "9870679",
                        "has_image": true,
                        "has_pdf": false,
                        "paper_award": "",
                        "image_caption": "System User Interface: (a) Micro-interpreting module: feature visualization including Density Map and Object Projection; (b) Macro-interpreting module: temporal visualization including Autonomous Driving Vehicle States and Object-level Density Maps; (c) Macro-interpreting module: spatial visualization including Scene and Trajectory; (d) Control for selecting and filtering object classes, results and locations. (e) A legend referring to all views introduces color encoding. (f) A guidance example to use our analysis workflow.",
                        "external_paper_link": "",
                        "youtube_ff_link": "https://youtu.be/P2cqhJ6EsFk",
                        "youtube_ff_id": "P2cqhJ6EsFk",
                        "bunny_ff_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-tvcg/v-tvcg-9870679/v-tvcg-9870679_Preview.mp4?token=lH2oTjB-gRr1P4bdRd8UgIM_SCyA-KJHFC4FpMTvhZA&expires=1706590800",
                        "bunny_ff_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-tvcg/v-tvcg-9870679/v-tvcg-9870679_Preview.vtt?token=r1MnV6QsRwk_QOzBn3jRSriSG0llJ6Tv0LiF-mTFGo8&expires=1706590800",
                        "youtube_prerecorded_link": "https://youtu.be/xi6dcpoi8-M",
                        "youtube_prerecorded_id": "xi6dcpoi8-M",
                        "bunny_prerecorded_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-tvcg/v-tvcg-9870679/v-tvcg-9870679_Presentation.mp4?token=yg00a3nMZhvhxMBWb-AbZ-P1Wf3_uBd6P-LzIdAYRck&expires=1706590800",
                        "bunny_prerecorded_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-tvcg/v-tvcg-9870679/v-tvcg-9870679_Presentation.vtt?token=xmGpZMadiy44ylWuzD72dKf7L45wfjVTtd3AqND0cLE&expires=1706590800"
                    },
                    {
                        "slot_id": "v-full-1343",
                        "session_id": "full7",
                        "title": "GeoExplainer: A Visual Analytics Framework for Spatial Modeling Contextualization and Report Generation",
                        "contributors": [
                            "Fan Lei"
                        ],
                        "authors": [
                            "Fan Lei",
                            "Yuxin Ma",
                            "Stewart Fotheringham",
                            "Elizabeth Mack",
                            "Ziqi Li",
                            "Mehak Sachdeva",
                            "Sarah Bardin",
                            "Ross Maciejewski"
                        ],
                        "abstract": "Geographic regression models of various descriptions are often applied to identify patterns and anomalies in the determinants of spatially distributed observations. These types of analyses focus on answering why questions about underlying spatial phenomena, e.g., why is crime higher in this locale, why do children in one school district outperform those in another, etc.? Answers to these questions require explanations of the model structure, the choice of parameters, and contextualization of the findings with respect to their geographic context. This is particularly true for local forms of regression models which are focused on the role of locational context in determining human behavior. In this paper, we present GeoExplainer, a visual analytics framework designed to support analysts in creating explanative documentation that summarizes and contextualizes their spatial analyses. As analysts create their spatial models, our framework flags potential issues with model parameter selections, utilizes template-based text generation to summarize model outputs, and links with external knowledge repositories to provide annotations that help to explain the model results. As analysts explore the model results, all visualizations and annotations can be captured in an interactive report generation widget. We demonstrate our framework using a case study modeling the determinants of voting in the 2016 US Presidential Election.",
                        "uid": "v-full-1343",
                        "time_stamp": "2023-10-26T22:48:00Z",
                        "time_start": "2023-10-26T22:48:00Z",
                        "time_end": "2023-10-26T23:00:00Z",
                        "paper_type": "full",
                        "keywords": [
                            "Spatial data analysis, local models, multiscale geographically weighted regression, model explanation, visual analytics"
                        ],
                        "doi": "10.1109/TVCG.2023.3327359",
                        "fno": "",
                        "has_image": true,
                        "has_pdf": true,
                        "paper_award": "",
                        "image_caption": "The model analysis and report authoring interface of GeoExplainer.",
                        "external_paper_link": "",
                        "youtube_ff_link": "https://youtu.be/mkM05Rwi8Pw",
                        "youtube_ff_id": "mkM05Rwi8Pw",
                        "bunny_ff_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1343/v-full-1343_Preview.mp4?token=uT-fML8N4tSQhnHBwzp4WHUs6wg_z3CgshA3fPr_lNY&expires=1706590800",
                        "bunny_ff_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1343/v-full-1343_Preview.vtt?token=4-MZghNOAZOr7wREbUPP5JWkGyuZy6cqK6ujZMOsGro&expires=1706590800",
                        "youtube_prerecorded_link": "",
                        "youtube_prerecorded_id": "",
                        "bunny_prerecorded_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1343/v-full-1343_Presentation.mp4?token=5XADYm3VyNNY9Tn9dGd_eUOl8BvVn7MNmXPZRJHuxTc&expires=1706590800",
                        "bunny_prerecorded_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1343/v-full-1343_Presentation.vtt?token=rXHKT9tn3HFGk459NIWTgw9cOdxs3rNhgg2yIfIy9d8&expires=1706590800"
                    },
                    {
                        "slot_id": "v-full-1797",
                        "session_id": "full7",
                        "title": "The Urban Toolkit: A Grammar-based Framework for Urban Visual Analytics",
                        "contributors": [
                            "Fabio Miranda"
                        ],
                        "authors": [
                            "Gustavo Moreira",
                            "Maryam Hosseini",
                            "Md Nafiul Alam Nipu",
                            "Marcos Lage",
                            "Nivan Ferreira",
                            "Fabio Miranda"
                        ],
                        "abstract": "While cities around the world are looking for smart ways to use new advances in data collection, management, and analysis to address their problems, the complex nature of urban issues and the overwhelming amount of available data have posed significant challenges in translating these efforts into actionable insights. In the past few years, urban visual analytics tools have significantly helped tackle these challenges. When analyzing a feature of interest, an urban expert must transform, integrate, and visualize different thematic (e.g., sunlight access, demographic) and physical (e.g., buildings, street networks) data layers, oftentimes across multiple spatial and temporal scales. However, integrating and analyzing these layers require expertise in different fields, increasing development time and effort. This makes the entire visual data exploration and system implementation difficult for programmers and also sets a high entry barrier for urban experts outside of computer science. With this in mind, in this paper, we present the Urban Toolkit (UTK), a flexible and extensible visualization framework that enables the easy authoring of web-based visualizations through a new high-level grammar specifically built with common urban use cases in mind. In order to facilitate the integration and visualization of different urban data, we also propose the concept of knots to merge thematic and physical urban layers. We evaluate our approach through use cases and a series of interviews with experts and practitioners from different domains, including urban accessibility, urban planning, architecture, and climate science. UTK is available at urbantk.org.",
                        "uid": "v-full-1797",
                        "time_stamp": "2023-10-26T23:00:00Z",
                        "time_start": "2023-10-26T23:00:00Z",
                        "time_end": "2023-10-26T23:12:00Z",
                        "paper_type": "full",
                        "keywords": [
                            "Urban visual analytics, Urban analytics, Urban data, Visualization toolkit"
                        ],
                        "doi": "10.1109/TVCG.2023.3326598",
                        "fno": "",
                        "has_image": true,
                        "has_pdf": true,
                        "paper_award": "",
                        "image_caption": "While cities around the world are looking for smart ways to use new advances in data collection, management, and analysis to address their day-to-day problems, the complex nature of urban issues and the overwhelming amount of available structured and unstructured data have posed significant challenges in translating these efforts into actionable insights. Recently, urban visual analytics tools have significantly helped tackle these challenges. With this in mind, we present the Urban Toolkit, a flexible and extensible visualization framework that enables the easy authoring of web-based visualizations through a new high-level grammar specifically built with common urban cases in mind.",
                        "external_paper_link": "",
                        "youtube_ff_link": "https://youtu.be/1g0s4PpPM6s",
                        "youtube_ff_id": "1g0s4PpPM6s",
                        "bunny_ff_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1797/v-full-1797_Preview.mp4?token=9BQ2G9WPcAw-WRr3s6axpGrg96yazD5k07MQkK9-ucM&expires=1706590800",
                        "bunny_ff_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1797/v-full-1797_Preview.vtt?token=77IW_U2mVGe0dsHExkd5p3Oh5vpAuI8bGvRNnjPVyRc&expires=1706590800",
                        "youtube_prerecorded_link": "https://youtu.be/LF27VgtUGQ4",
                        "youtube_prerecorded_id": "LF27VgtUGQ4",
                        "bunny_prerecorded_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1797/v-full-1797_Presentation.mp4?token=Mn7tbImRSAKi9bQGsleqTkI8GECWvvc9HzP-WNy0KE0&expires=1706590800",
                        "bunny_prerecorded_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1797/v-full-1797_Presentation.vtt?token=PgRxrGv4oisSmxItu5iELvSCgj4xD62CmHzwiyFhp8I&expires=1706590800"
                    }
                ]
            },
            {
                "title": "Grammars",
                "session_id": "full8",
                "event_prefix": "v-full",
                "track": "oneohfive",
                "session_image": "full8.png",
                "chair": [
                    "Dominik Moritz"
                ],
                "time_start": "2023-10-25T03:00:00Z",
                "time_end": "2023-10-25T04:15:00Z",
                "discord_category": "",
                "discord_channel": "105",
                "discord_channel_id": "1161741309346861067",
                "discord_link": "https://discordapp.com/channels/1161400451619627068/1161741309346861067",
                "zoom_private_meeting": "",
                "zoom_private_password": "",
                "zoom_private_link": "",
                "zoom_broadcast_link": "",
                "ff_link": "https://youtu.be/FSJK5p4HM6I",
                "time_slots": [
                    {
                        "slot_id": "v-full-1307",
                        "session_id": "full8",
                        "title": "Visual Analytics for Understanding Draco's Knowledge Base",
                        "contributors": [
                            "Johanna Schmidt"
                        ],
                        "authors": [
                            "Johanna Schmidt",
                            "Bernhard Pointner",
                            "Silvia Miksch"
                        ],
                        "abstract": "Draco has been developed as an automated visualization recommendation system formalizing design knowledge as logical constraints in ASP (Answer-Set Programming). With an increasing set of constraints and incorporated design knowledge, even visualization experts lose overview in Draco and struggle to retrace the automated recommendation decisions made by the system. Our paper proposes an Visual Analytics (VA) approach to visualize and analyze Draco's constraints. Our VA approach is supposed to enable visualization experts to accomplish identified tasks regarding the knowledge base and support them in better understanding Draco. We extend the existing data extraction strategy of Draco with a data processing architecture capable of extracting features of interest from the knowledge base. A revised version of the ASP grammar provides the basis for this data processing strategy. The resulting incorporated and shared features of the constraints are then visualized using a hypergraph structure inside the radial-arranged constraints of the elaborated visualization. The hierarchical categories of the constraints are indicated by arcs surrounding the constraints. Our approach is supposed to enable visualization experts to interactively explore the design rules' violations based on highlighting respective constraints or recommendations. A qualitative and quantitative evaluation of the prototype confirms the prototype's effectiveness and value in acquiring insights into Draco's recommendation process and design constraints.",
                        "uid": "v-full-1307",
                        "time_stamp": "2023-10-25T03:00:00Z",
                        "time_start": "2023-10-25T03:00:00Z",
                        "time_end": "2023-10-25T03:12:00Z",
                        "paper_type": "full",
                        "keywords": [
                            "Visual Analytics, Hypergraph visualization, Rule-based recommendation systems"
                        ],
                        "doi": "10.1109/TVCG.2023.3326912",
                        "fno": "",
                        "has_image": true,
                        "has_pdf": true,
                        "paper_award": "",
                        "image_caption": "Visually inspecting Draco's recommendations. Our interactive Visual Analytics solution allows users to explore the set of rules the visualization recommendation system Draco is built on. On the left side, four recommended visualizations are shown. Every recommendation has costs assigned, which relates to how many rules have been violated by this recommendation. On the right, side we present our hypergraph-based visualization of the set of rules and constraints that are used by Draco. By selecting recommendations on the left (A, blue, and B, red), the rules violated by these visualizations are highlighted in the graph (red and blue dashed lines).",
                        "external_paper_link": "",
                        "youtube_ff_link": "https://youtu.be/Jkaz0DJ25bs",
                        "youtube_ff_id": "Jkaz0DJ25bs",
                        "bunny_ff_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1307/v-full-1307_Preview.mp4?token=SXN7C4s_OhtiGu7nkvhSKs3wl63QPW_xAzc7UPN9iI4&expires=1706590800",
                        "bunny_ff_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1307/v-full-1307_Preview.vtt?token=xCC79vLi7oj2wR69DRVpboLPfGGyWRxhAIr-3b5u8Tg&expires=1706590800",
                        "youtube_prerecorded_link": "https://youtu.be/n3SVTfXvJc8",
                        "youtube_prerecorded_id": "n3SVTfXvJc8",
                        "bunny_prerecorded_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1307/v-full-1307_Presentation.mp4?token=XZ4iSdiD27R-HNrTWQZTsd07QyIxXq95cyIve6sQ3Ss&expires=1706590800",
                        "bunny_prerecorded_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1307/v-full-1307_Presentation.vtt?token=pY-SWs15FxKbfRfxAPnHzM7r2Ptxwk_fW7S_JU1yCWM&expires=1706590800"
                    },
                    {
                        "slot_id": "v-full-1116",
                        "session_id": "full8",
                        "title": "DIVI: Dynamically Interactive Visualization",
                        "contributors": [
                            "Luke S Snyder"
                        ],
                        "authors": [
                            "Luke S. Snyder",
                            "Jeffrey Heer"
                        ],
                        "abstract": "Dynamically Interactive Visualization (DIVI) is a novel approach for orchestrating interactions within and across static visualizations. DIVI deconstructs Scalable Vector Graphics charts at runtime to infer content and coordinate user input, decoupling interaction from specification logic. This decoupling allows interactions to extend and compose freely across different tools, chart types, and analysis goals. DIVI exploits positional relations of marks to detect chart components such as axes and legends, reconstruct scales and view encodings, and infer data fields. DIVI then enumerates candidate transformations across inferred data to perform linking between views. To support dynamic interaction without prior specification, we introduce a taxonomy that formalizes the space of standard interactions by chart element, interaction type, and input event. We demonstrate DIVI\u2019s usefulness for rapid data exploration and analysis through a usability study with 13 participants and a diverse gallery of dynamically interactive visualizations, including single chart, multi-view, and cross-tool configurations.",
                        "uid": "v-full-1116",
                        "time_stamp": "2023-10-25T03:12:00Z",
                        "time_start": "2023-10-25T03:12:00Z",
                        "time_end": "2023-10-25T03:24:00Z",
                        "paper_type": "full",
                        "keywords": [
                            "Interaction, Visualization Tools, Charts, SVG, Exploratory Data Analysis"
                        ],
                        "doi": "10.1109/TVCG.2023.3327172",
                        "fno": "",
                        "has_image": false,
                        "has_pdf": true,
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "youtube_ff_link": "https://youtu.be/VKOqZ2iihnE",
                        "youtube_ff_id": "VKOqZ2iihnE",
                        "bunny_ff_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1116/v-full-1116_Preview.mp4?token=_QqIqQbRi0hn01cdpU_tpPXuc4MnpoIojY0eQ4ZvJ0Q&expires=1706590800",
                        "bunny_ff_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1116/v-full-1116_Preview.vtt?token=jeBzrcwXL2hr6WY8yz-NmeLv48qm_1RfN2oRzCTTaOM&expires=1706590800",
                        "youtube_prerecorded_link": "",
                        "youtube_prerecorded_id": "",
                        "bunny_prerecorded_link": "",
                        "bunny_prerecorded_subtitles": ""
                    },
                    {
                        "slot_id": "v-full-1212",
                        "session_id": "full8",
                        "title": "ggdist: Visualizations of Distributions and Uncertainty in the Grammar of Graphics",
                        "contributors": [
                            "Matthew Kay"
                        ],
                        "authors": [
                            "Matthew Kay"
                        ],
                        "abstract": "The grammar of graphics is ubiquitous, providing the foundation for a variety of popular visualization tools and toolkits. Yet support for uncertainty visualization in the grammar graphics\u2014beyond simple variations of error bars, uncertainty bands, and density plots\u2014remains rudimentary. Research in uncertainty visualization has developed a rich variety of improved uncertainty visualizations, most of which are difficult to create in existing grammar of graphics implementations. ggdist, an extension to the popular ggplot2 grammar of graphics toolkit, is an attempt to rectify this situation. ggdist unifies a variety of uncertainty visualization types through the lens of distributional visualization, allowing functions of distributions to be mapped to directly to visual channels (aesthetics), making it straightforward to express a variety of (sometimes weird!) uncertainty visualization types. This distributional lens also offers a way to unify Bayesian and frequentist uncertainty visualization by formalizing the latter with the help of confidence distributions. In this paper, I offer a description of this uncertainty visualization paradigm and lessons learned from its development and adoption: ggdist has existed in some form for about six years (originally as part of the tidybayes R package for post-processing Bayesian models), and it has evolved substantially over that time, with several rewrites and API re-organizations as it changed in response to user feedback and expanded to cover increasing varieties of uncertainty visualization types. Ultimately, given the huge expressive power of the grammar of graphics and the popularity of tools built on it, I hope a catalog of my experience with ggdist will provide a catalyst for further improvements to formalizations and implementations of uncertainty visualization in grammar of graphics ecosystems. A free copy of this paper is available at https://osf.io/2gsz6. All supplemental materials are available at https://github.com/mjskay/ggdist-paper and are archived on Zenodo at doi:10.5281/zenodo.7770984.",
                        "uid": "v-full-1212",
                        "time_stamp": "2023-10-25T03:24:00Z",
                        "time_start": "2023-10-25T03:24:00Z",
                        "time_end": "2023-10-25T03:36:00Z",
                        "paper_type": "full",
                        "keywords": [
                            "Uncertainty visualization, probability distributions, confidence distributions, grammar of graphics"
                        ],
                        "doi": "10.1109/TVCG.2023.3327195",
                        "fno": "",
                        "has_image": true,
                        "has_pdf": true,
                        "paper_award": "",
                        "image_caption": "Examples from the three major classes of geometries in ggdist: (A) slabintervals, such as density plots, CDFs, intervals, and gradient plots; (B) lineribbons, such as uncertainty bands and fan charts; and (C) dots, such as dotplots and beeswarm charts. Myriad combinations of these are possible, leading to charts like (D) logit dotplots and (E) raincloud plots.",
                        "external_paper_link": "",
                        "youtube_ff_link": "https://youtu.be/j_0vSP7HldQ",
                        "youtube_ff_id": "j_0vSP7HldQ",
                        "bunny_ff_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1212/v-full-1212_Preview.mp4?token=8bB5tTwUVA6e5pqnRhRwul0AtivvxM3YC02sKCQfB68&expires=1706590800",
                        "bunny_ff_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1212/v-full-1212_Preview.vtt?token=6zMeYXnAC6ww3rMg8KR-yiuh3Ctz87JwgYPHMlUO1MQ&expires=1706590800",
                        "youtube_prerecorded_link": "https://youtu.be/htJORACnb54",
                        "youtube_prerecorded_id": "htJORACnb54",
                        "bunny_prerecorded_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1212/v-full-1212_Presentation.mp4?token=L-KPaEfRAFineqoYcNOqG6r4HoCo7MVIf4GuxwJGH5E&expires=1706590800",
                        "bunny_prerecorded_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1212/v-full-1212_Presentation.vtt?token=L4eoC_XCpOHtrydU9PgbsPG292N-xtrShHy9Bc5I5Es&expires=1706590800"
                    },
                    {
                        "slot_id": "v-full-1328",
                        "session_id": "full8",
                        "title": "Metrics-Based Evaluation and Comparison of Visualization Notations",
                        "contributors": [
                            "Michael J. McGuffin"
                        ],
                        "authors": [
                            "Nicolas Kruchten",
                            "Andrew M McNutt",
                            "Michael McGuffin"
                        ],
                        "abstract": "A visualization notation is a recurring pattern of symbols used to author specifications of visualizations, from data transformation to visual mapping. Programmatic notations use symbols defined by grammars or domain-specific languages (e.g., ggplot2, dplyr, Vega-Lite) or libraries (e.g., Matplotlib, Pandas).   Designers and prospective users of grammars and libraries often evaluate visualization notations by inspecting galleries of examples.  While such collections demonstrate usage and expressiveness, their construction and evaluation are usually ad hoc, making comparisons of different notations difficult.  More rarely, experts analyze notations via usability heuristics, such as the Cognitive Dimensions of Notations framework. These analyses, akin to structured close readings of text, can reveal design deficiencies, but place a burden on the expert to simultaneously consider many facets of often complex systems.  To alleviate these issues, we introduce a metrics-based approach to usability evaluation and comparison of notations in which metrics are computed for a gallery of examples across a suite of notations.  While applicable to any visualization domain, we explore the utility of our approach via a case study considering statistical graphics that explores 40 visualizations across 9 widely used notations. We facilitate the computation of appropriate metrics and analysis via a new tool called NotaScope.  We gathered feedback via interviews with authors or maintainers of prominent charting libraries  (n=6).  We find that this approach is a promising way to formalize, externalize, and extend evaluations and comparisons of visualization notations.",
                        "uid": "v-full-1328",
                        "time_stamp": "2023-10-25T03:36:00Z",
                        "time_start": "2023-10-25T03:36:00Z",
                        "time_end": "2023-10-25T03:48:00Z",
                        "paper_type": "full",
                        "keywords": [
                            "Notation, Usability, Evaluation, Language design, API design, Domain-specific languages."
                        ],
                        "doi": "10.1109/TVCG.2023.3326907",
                        "fno": "",
                        "has_image": true,
                        "has_pdf": true,
                        "paper_award": "",
                        "image_caption": "Evaluating or comparing a visualization notation (such as ggplot2 or Matplotlib) with others is typically a qualitative and ad hoc process. We introduce a metrics-based approach to help readers structure that analysis via quantitative measurements based on Cognitive Dimensions of Notation-inspired properties. This approach allows analysts a measure of distance in their considerations of new notations, and provides some distance to previously close reading of textual systems for expressing visualizations.",
                        "external_paper_link": "",
                        "youtube_ff_link": "https://youtu.be/pwkYONAD3e0",
                        "youtube_ff_id": "pwkYONAD3e0",
                        "bunny_ff_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1328/v-full-1328_Preview.mp4?token=hBlgJcFbyl1604BdDie-9WmIyDnxEwbAPeANkTjKdek&expires=1706590800",
                        "bunny_ff_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1328/v-full-1328_Preview.vtt?token=9XXEut_CtOQGrMO1oRR1RfyRbOTYfHSb0cuqriDK8Q4&expires=1706590800",
                        "youtube_prerecorded_link": "https://youtu.be/ZAFdNC4SSr8",
                        "youtube_prerecorded_id": "ZAFdNC4SSr8",
                        "bunny_prerecorded_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1328/v-full-1328_Presentation.mp4?token=VoWXWHr4MZQnZ05UdcJnbMxMpkUWyKwEpOPmNeyNMwk&expires=1706590800",
                        "bunny_prerecorded_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1328/v-full-1328_Presentation.vtt?token=GZVdNu0uh3qLIh7QBlfKqXztecYd1rCeC7197c47juw&expires=1706590800"
                    },
                    {
                        "slot_id": "v-full-1150",
                        "session_id": "full8",
                        "title": "Mosaic: An Architecture for Scalable & Interoperable Data Views",
                        "contributors": [
                            "Jeffrey Heer"
                        ],
                        "authors": [
                            "Jeffrey Heer",
                            "Dominik Moritz"
                        ],
                        "abstract": "Mosaic is an architecture for greater scalability, extensibility, and interoperability of interactive data views. Mosaic decouples data processing from specification logic: clients publish their data needs as declarative queries that are then managed and automatically optimized by a coordinator that proxies access to a scalable data store. Mosaic generalizes Vega-Lite\u2019s selection abstraction to enable rich integration and linking across visualizations and components such as menus, text search, and tables. We demonstrate Mosaic\u2019s expressiveness, extensibility, and interoperability through examples that compose diverse visualization, interaction, and optimization techniques\u2014many constructed using vgplot, a grammar of interactive graphics in which graphical marks act as Mosaic clients. To evaluate scalability, we present benchmark studies with order-of-magnitude performance improvements over existing web-based visualization systems\u2014enabling flexible, real-time visual exploration of billion+ record datasets. We conclude by discussing Mosaic\u2019s potential as an open platform that bridges visualization languages, scalable visualization, and interactive data systems more broadly.",
                        "uid": "v-full-1150",
                        "time_stamp": "2023-10-25T03:48:00Z",
                        "time_start": "2023-10-25T03:48:00Z",
                        "time_end": "2023-10-25T04:00:00Z",
                        "paper_type": "full",
                        "keywords": [
                            "Visualization, Interaction, Scalability, Grammar of Graphics, Software Architecture, Databases"
                        ],
                        "doi": "10.1109/TVCG.2023.3327189",
                        "fno": "",
                        "has_image": true,
                        "has_pdf": true,
                        "paper_award": "",
                        "image_caption": "A Mosaic-based interface for interactive visual exploration of all 1.8 billion stars in the Gaia star catalog. A high-resolution density map of the sky reveals our Milky Way and satellite galaxies.",
                        "external_paper_link": "",
                        "youtube_ff_link": "https://youtu.be/4VPQmScA4Fg",
                        "youtube_ff_id": "4VPQmScA4Fg",
                        "bunny_ff_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1150/v-full-1150_Preview.mp4?token=kuDft_yvQUqcT68PfJtAvS0-uRfIrQqba6dOnHUr_TU&expires=1706590800",
                        "bunny_ff_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1150/v-full-1150_Preview.vtt?token=PejYJxifD_0sH7LVvIHp25JGo6D5grfX3WDeqg79wo0&expires=1706590800",
                        "youtube_prerecorded_link": "https://youtu.be/txIvM1dA3EM",
                        "youtube_prerecorded_id": "txIvM1dA3EM",
                        "bunny_prerecorded_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1150/v-full-1150_Presentation.mp4?token=A4lo3DgI3wyh71AaFinTNMNqoI8gWsvXbocP0qJnZtw&expires=1706590800",
                        "bunny_prerecorded_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1150/v-full-1150_Presentation.vtt?token=rJpueSvBXbTELeEL-mcMalNgn7cxx_DWW73MmIVM1lI&expires=1706590800"
                    },
                    {
                        "slot_id": "v-full-1199",
                        "session_id": "full8",
                        "title": "Mystique: Deconstructing SVG Charts for Layout Reuse",
                        "contributors": [
                            "Chen Chen"
                        ],
                        "authors": [
                            "Chen Chen",
                            "Bongshin Lee",
                            "Yunhai Wang",
                            "Yunjeong Chang",
                            "Zhicheng Liu"
                        ],
                        "abstract": "To facilitate the reuse of existing charts, previous research has examined how to obtain a semantic understanding of a chart by deconstructing its visual representation into reusable components, such as encodings. However, existing deconstruction approaches primarily focus on chart styles, handling only basic layouts. In this paper, we investigate how to deconstruct chart layouts, focusing on rectangle-based ones, as they cover not only 17 chart types but also advanced layouts (e.g., small multiples, nested layouts). We develop an interactive tool, called Mystique, adopting a mixed-initiative approach to extract the axes and legend, and deconstruct a chart's layout into four semantic components: mark groups, spatial relationships, data encodings, and graphical constraints. Mystique employs a wizard interface that guides chart authors through a series of steps to specify how the deconstructed components map to their own data. On 150 rectangle-based SVG charts, Mystique achieves above 85% accuracy for axis and legend extraction and 96% accuracy for layout deconstruction. In a chart reproduction study, participants could easily reuse existing charts on new datasets. We discuss the current limitations of Mystique and future research directions.",
                        "uid": "v-full-1199",
                        "time_stamp": "2023-10-25T04:00:00Z",
                        "time_start": "2023-10-25T04:00:00Z",
                        "time_end": "2023-10-25T04:12:00Z",
                        "paper_type": "full",
                        "keywords": [
                            "Chart layout, Reuse, Reverse-engineering, Deconstruction."
                        ],
                        "doi": "10.1109/TVCG.2023.3327354",
                        "fno": "",
                        "has_image": true,
                        "has_pdf": true,
                        "paper_award": "",
                        "image_caption": "Each pair shows an existing chart (left) and a new chart created using Mystique (right). The existing charts are produced using a variety of tools, such as D3, Vega-lite, Mascot, PlotDB, Highcharts, and Data Illustrator.",
                        "external_paper_link": "",
                        "youtube_ff_link": "https://youtu.be/Wd8PGfKSsSM",
                        "youtube_ff_id": "Wd8PGfKSsSM",
                        "bunny_ff_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1199/v-full-1199_Preview.mp4?token=R-eaPHTE1e9MHhP7cCH1VLihctirAKY3Zzz-vY9eG1c&expires=1706590800",
                        "bunny_ff_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1199/v-full-1199_Preview.vtt?token=93akIKaW2caMFMrhwJlRoWI9Icr10400m9nyh74eTeQ&expires=1706590800",
                        "youtube_prerecorded_link": "https://youtu.be/gP8a-DNLhN0",
                        "youtube_prerecorded_id": "gP8a-DNLhN0",
                        "bunny_prerecorded_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1199/v-full-1199_Presentation.mp4?token=kLWWG8FB2GTveAbEL6JuSSUieLFi-aTw0lIwmW2Msac&expires=1706590800",
                        "bunny_prerecorded_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1199/v-full-1199_Presentation.vtt?token=Ly9h3oIF2J0oQPn7xa0twhDtdK21NBKaSyL4elAo8Eg&expires=1706590800"
                    }
                ]
            },
            {
                "title": "Graph Visualization",
                "session_id": "full9",
                "event_prefix": "v-full",
                "track": "oneohfive",
                "session_image": "full9.png",
                "chair": [
                    "Daniel Archambault"
                ],
                "time_start": "2023-10-25T04:45:00Z",
                "time_end": "2023-10-25T06:00:00Z",
                "discord_category": "",
                "discord_channel": "105",
                "discord_channel_id": "1161741309346861067",
                "discord_link": "https://discordapp.com/channels/1161400451619627068/1161741309346861067",
                "zoom_private_meeting": "",
                "zoom_private_password": "",
                "zoom_private_link": "",
                "zoom_broadcast_link": "",
                "ff_link": "https://youtu.be/e2CsGVUEtYY",
                "time_slots": [
                    {
                        "slot_id": "v-full-1289",
                        "session_id": "full9",
                        "title": "Calliope-Net: Automatic Generation of Graph Data Facts via Annotated Node-link Diagrams",
                        "contributors": [
                            "Qing Chen"
                        ],
                        "authors": [
                            "Qing Chen",
                            "Nan Chen",
                            "Wei Shuai",
                            "Guande Wu",
                            "Zhe Xu",
                            "Hanghang Tong",
                            "Nan Cao"
                        ],
                        "abstract": "Graph or network data are widely studied in both data mining and visualization communities to review the relationship among different entities and groups. The data facts derived from graph visual analysis are important to help understand the social structures of complex data, especially for data journalism. However, it is challenging for data journalists to discover graph data facts and manually organize correlated facts around a meaningful topic due to the complexity of graph data and the difficulty to interpret graph narratives. Therefore, we present an automatic graph facts generation system, Calliope-Net, which consists of a fact discovery module, a fact organization module, and a visualization module. It creates annotated node-link diagrams with facts automatically discovered and organized from network data. A novel layout algorithm is designed to present meaningful and visually appealing annotated graphs. We evaluate the proposed system with two case studies and an in-lab user study. The results show that Calliope-Net can benefit users in discovering and understanding graph data facts with visually pleasing annotated visualizations.",
                        "uid": "v-full-1289",
                        "time_stamp": "2023-10-25T04:45:00Z",
                        "time_start": "2023-10-25T04:45:00Z",
                        "time_end": "2023-10-25T04:57:00Z",
                        "paper_type": "full",
                        "keywords": [
                            "Graph Data, Application Motivated Visualization, Automatic Visualization, Narrative Visualization, Authoring Tools"
                        ],
                        "doi": "10.1109/TVCG.2023.3326925",
                        "fno": "",
                        "has_image": false,
                        "has_pdf": true,
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "youtube_ff_link": "https://youtu.be/CI8z4kdshpo",
                        "youtube_ff_id": "CI8z4kdshpo",
                        "bunny_ff_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1289/v-full-1289_Preview.mp4?token=iUutss-HyR38Qw_o8auj4stXBd3rnauvCVVhzSfSSJ8&expires=1706590800",
                        "bunny_ff_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1289/v-full-1289_Preview.vtt?token=RYB9zy1Z4XkO5HauxVuuW-uL47WMC2J5x0vl5OTg-hQ&expires=1706590800",
                        "youtube_prerecorded_link": "",
                        "youtube_prerecorded_id": "",
                        "bunny_prerecorded_link": "",
                        "bunny_prerecorded_subtitles": ""
                    },
                    {
                        "slot_id": "v-full-1061",
                        "session_id": "full9",
                        "title": "Quantivine: A Visualization Approach for Large-scale Quantum Circuit Representation and Analysis",
                        "contributors": [
                            "Zhen Wen"
                        ],
                        "authors": [
                            "Zhen Wen",
                            "Yihan Liu",
                            "Siwei Tan",
                            "Jieyi Chen",
                            "Minfeng Zhu",
                            "Dongming Han",
                            "Jianwei Yin",
                            "Mingliang Xu",
                            "Wei Chen"
                        ],
                        "abstract": "Quantum computing is a rapidly evolving field that enables exponential speed-up over classical algorithms. At the heart of this revolutionary technology are quantum circuits, which serve as vital tools for implementing, analyzing, and optimizing quantum algorithms. Recent advancements in quantum computing and the increasing capability of quantum devices have led to the development of more complex quantum circuits. However, traditional quantum circuit diagrams suffer from scalability and readability issues, which limit the efficiency of analysis and optimization processes. In this research, we propose a novel visualization approach for large-scale quantum circuits by adopting semantic analysis to facilitate the comprehension of quantum circuits. We first exploit meta-data and semantic information extracted from the underlying code of quantum circuits to create component segmentations and pattern abstractions, allowing for easier wrangling of massive circuit diagrams. We then develop Quantivine, an interactive system for exploring and understanding quantum circuits. A series of novel circuit visualizations is designed to uncover contextual details such as qubit provenance, parallelism, and entanglement. The effectiveness of Quantivine is demonstrated through two usage scenarios of quantum circuits with up to 100 qubits and a formal user evaluation with quantum experts. A free copy of this paper and all supplemental materials are available at https://osf.io/2m9yh/?view_only=0aa1618c97244f5093cd7ce15f1431f9.",
                        "uid": "v-full-1061",
                        "time_stamp": "2023-10-25T04:57:00Z",
                        "time_start": "2023-10-25T04:57:00Z",
                        "time_end": "2023-10-25T05:09:00Z",
                        "paper_type": "full",
                        "keywords": [
                            "Quantum circuit, semantic analysis, visual abstraction, context visualization"
                        ],
                        "doi": "10.1109/TVCG.2023.3327148",
                        "fno": "",
                        "has_image": true,
                        "has_pdf": true,
                        "paper_award": "",
                        "image_caption": "The system interface of Quantivine. The Structure View presents the construction of a quantum circuit in a tree diagram including primitive gates, component gates, and repetitive patterns. The Component View provides a flexibly-organized circuit diagram with grouped component gates. The Abstraction View shows a further simplified circuit diagram according to visual abstractions of repetitive patterns. Three Context Views reveal contextual information in the circuit, including qubit provenance, gate placement, and connectivity.",
                        "external_paper_link": "",
                        "youtube_ff_link": "https://youtu.be/jRw1l1VCzjY",
                        "youtube_ff_id": "jRw1l1VCzjY",
                        "bunny_ff_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1061/v-full-1061_Preview.mp4?token=unitCWs95EzIjptTU_qt3qS0AHt614ss5FEycV8T-7Q&expires=1706590800",
                        "bunny_ff_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1061/v-full-1061_Preview.vtt?token=1kdD18cA5VddjRO64ffs4le9CRxmmcMPUpbHxGtz4iU&expires=1706590800",
                        "youtube_prerecorded_link": "https://youtu.be/Zpy-YQEnPS4",
                        "youtube_prerecorded_id": "Zpy-YQEnPS4",
                        "bunny_prerecorded_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1061/v-full-1061_Presentation.mp4?token=8qgZvggcDRLurWKgSRpOeG1wLlM-MFldxY6IJAFX7qk&expires=1706590800",
                        "bunny_prerecorded_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1061/v-full-1061_Presentation.vtt?token=rlN8mXXp4BahyfD33fBeCdq9DN1q5SyG--auaWG2WwE&expires=1706590800"
                    },
                    {
                        "slot_id": "v-tvcg-9966829",
                        "session_id": "full9",
                        "title": "GraphDecoder: Recovering Diverse Network Graphs from Visualization Images via Attention-Aware Learning",
                        "contributors": [
                            "Sicheng Song"
                        ],
                        "authors": [
                            "Sicheng Song",
                            "Chenhui Li",
                            "Dong Li",
                            "Jaunting Chen",
                            "Changbo Wang"
                        ],
                        "abstract": "DNGs are diverse network graphs with texts and different styles of nodes and edges, including mind maps, modeling graphs, and flowcharts. They are high-level visualizations that are easy for humans to understand but difficult for machines. Inspired by the process of human perception of graphs, we propose a method called GraphDecoder to extract data from raster images. Given a raster image, we extract the content based on a neural network. We built a semantic segmentation network based on U-Net. We increase the attention mechanism module, simplify the network model, and design a specific loss function to improve the model's ability to extract graph data. After this semantic segmentation network, we can extract the data of all nodes and edges. We then combine these data to obtain the topological relationship of the entire DNG. We also provide an interactive interface for users to redesign the DNGs. We verify the effectiveness of our method by evaluations and user studies on datasets collected on the Internet and generated datasets.",
                        "uid": "v-tvcg-9966829",
                        "time_stamp": "2023-10-25T05:09:00Z",
                        "time_start": "2023-10-25T05:09:00Z",
                        "time_end": "2023-10-25T05:21:00Z",
                        "paper_type": "full",
                        "keywords": [
                            "Information visualization;Chart mining;Semantic segmentation;Network graph;Attention mechanism"
                        ],
                        "doi": "10.1109/TVCG.2022.3225554",
                        "fno": "9966829",
                        "has_image": true,
                        "has_pdf": false,
                        "paper_award": "",
                        "image_caption": "ThegraphdecodercanextractDNGdatafromrasterimagesandautomaticallyretargetthem.Ourmethodcan be applied to many DNGs, including flowcharts (A), hierarchical diagrams (B), model graphs, hand-drawn sketches, and mind maps (C).",
                        "external_paper_link": "",
                        "youtube_ff_link": "https://youtu.be/S2UNVl0ywsY",
                        "youtube_ff_id": "S2UNVl0ywsY",
                        "bunny_ff_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-tvcg/v-tvcg-9966829/v-tvcg-9966829_Preview.mp4?token=NpFPorj-mz2HjAhtFyuPC8YbGQ4u4ZMBC_t_AARdIMg&expires=1706590800",
                        "bunny_ff_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-tvcg/v-tvcg-9966829/v-tvcg-9966829_Preview.vtt?token=9qglWOe4IH5_Lt46md6tkwMfHrajOp4icMluJS6YP8k&expires=1706590800",
                        "youtube_prerecorded_link": "https://youtu.be/lr4HKxq26bU",
                        "youtube_prerecorded_id": "lr4HKxq26bU",
                        "bunny_prerecorded_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-tvcg/v-tvcg-9966829/v-tvcg-9966829_Presentation.mp4?token=HFAhrWNWMxw5XtKmwrNzQr3Q1Dt0HQEoFdCVr_bxOEE&expires=1706590800",
                        "bunny_prerecorded_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-tvcg/v-tvcg-9966829/v-tvcg-9966829_Presentation.vtt?token=l6jxlhoE0HUl7NiuXCm2YWw6tUvxq_A_CVTTgnFzidA&expires=1706590800"
                    },
                    {
                        "slot_id": "v-tvcg-9829321",
                        "session_id": "full9",
                        "title": "Influence Maximization with Visual Analytics",
                        "contributors": [
                            "Alessio Arleo"
                        ],
                        "authors": [
                            "Alessio Arleo",
                            "Walter Didimo",
                            "Giuseppe Liotta",
                            "Silvia Miksch",
                            "Fabrizio Montecchiani"
                        ],
                        "abstract": "In social networks, individuals\u2019 decisions are strongly influenced by recommendations from their friends, acquaintances, and favorite renowned personalities. The popularity of online social networking platforms makes them the prime venues to advertise products and promote opinions. The Influence Maximization (IM) problem entails selecting a seed set of users that maximizes the influence spread, i.e., the expected number of users positively influenced by a stochastic diffusion process triggered by the seeds. Engineering and analyzing IM algorithms remains a difficult and demanding task due to the NP-hardness of the problem and the stochastic nature of the diffusion processes. Despite several heuristics being introduced, they often fail in providing enough information on how the network topology affects the diffusion process, precious insights that could help researchers improve their seed set selection. In this paper, we present VAIM, a visual analytics system that supports users in analyzing, evaluating, and comparing information diffusion processes determined by different IM algorithms. Furthermore, VAIM provides useful insights that the analyst can use to modify the seed set of an IM algorithm, so to improve its influence spread. We assess our system by: (i) a qualitative evaluation based on a guided experiment with two domain experts on two different data sets; (ii) a quantitative estimation of the value of the proposed visualization through the ICE-T methodology by Wall (IEEE TVCG - 2018). The twofold assessment indicates that VAIM effectively supports our target users in the visual analysis of the performance of IM algorithms.",
                        "uid": "v-tvcg-9829321",
                        "time_stamp": "2023-10-25T05:21:00Z",
                        "time_start": "2023-10-25T05:21:00Z",
                        "time_end": "2023-10-25T05:33:00Z",
                        "paper_type": "full",
                        "keywords": [
                            "Information visualization;visualization systems and software;influence maximization;visual analytics;information diffusion"
                        ],
                        "doi": "10.1109/TVCG.2022.3190623",
                        "fno": "9829321",
                        "has_image": true,
                        "has_pdf": false,
                        "paper_award": "",
                        "image_caption": "A Snapshot of VAIM, a Visual Analytics System for Influence Maximization.",
                        "external_paper_link": "",
                        "youtube_ff_link": "https://youtu.be/SSoCivECraQ",
                        "youtube_ff_id": "SSoCivECraQ",
                        "bunny_ff_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-tvcg/v-tvcg-9829321/v-tvcg-9829321_Preview.mp4?token=Q8TWAp8laAmU2GKi2Ezf4mUEIyFmKqRNAEy52khRTBk&expires=1706590800",
                        "bunny_ff_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-tvcg/v-tvcg-9829321/v-tvcg-9829321_Preview.vtt?token=6VEQI4IBjBxAmCxGhfYAt-2N9pS1UO2Agesqpv5rg00&expires=1706590800",
                        "youtube_prerecorded_link": "https://youtu.be/woxn4A6K4Ng",
                        "youtube_prerecorded_id": "woxn4A6K4Ng",
                        "bunny_prerecorded_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-tvcg/v-tvcg-9829321/v-tvcg-9829321_Presentation.mp4?token=mI4STObcckslDl6ePy2RoyL8D0WNvsufXWdmmaI5IN0&expires=1706590800",
                        "bunny_prerecorded_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-tvcg/v-tvcg-9829321/v-tvcg-9829321_Presentation.vtt?token=J7cE2lBefdc_v5HCpuID0Q1KSjQp7cyE5uqi1zPtVgI&expires=1706590800"
                    },
                    {
                        "slot_id": "v-full-1239",
                        "session_id": "full9",
                        "title": "Knowledge Graphs in Practice: Characterizing their Users, Challenges, and Visualization Opportunities",
                        "contributors": [
                            "Harry Li"
                        ],
                        "authors": [
                            "Harry Li",
                            "Gabriel Appleby",
                            "Camelia D. Brumar",
                            "Remco Chang",
                            "Ashley Suh"
                        ],
                        "abstract": "This study presents insights from interviews with nineteen Knowledge Graph (KG) practitioners who work in both enterprise and academic settings on a wide variety of use cases. Through this study, we identify critical challenges experienced by KG practitioners when creating, exploring, and analyzing KGs that could be alleviated through visualization design. Our findings reveal three major personas among KG practitioners -- KG Builders, Analysts, and Consumers -- each of whom have their own distinct expertise and needs. We discover that KG Builders would benefit from schema enforcers, while KG Analysts need customizable query builders that provide interim query results. For KG Consumers, we identify a lack of efficacy for node-link diagrams, and the need for tailored domain-specific visualizations to promote KG adoption and comprehension. Lastly, we find that implementing KGs effectively in practice requires both technical and social solutions that are not addressed with current tools, technologies, and collaborative workflows. From the analysis of our interviews, we distill several visualization research directions to improve KG usability, including knowledge cards that balance digestibility and discoverability, timeline views to track temporal changes, interfaces that support organic discovery, and semantic explanations for AI and machine learning predictions.",
                        "uid": "v-full-1239",
                        "time_stamp": "2023-10-25T05:33:00Z",
                        "time_start": "2023-10-25T05:33:00Z",
                        "time_end": "2023-10-25T05:45:00Z",
                        "paper_type": "full",
                        "keywords": [
                            "Knowledge graphs, visualization techniques and methodologies, human factors, visual communication"
                        ],
                        "doi": "",
                        "fno": "",
                        "has_image": true,
                        "has_pdf": true,
                        "paper_award": "",
                        "image_caption": "We interviewed nineteen Knowledge Graph (KG) practitioners to identify critical challenges that could be alleviated through visualization design. We found three KG personas \u2013 Builders, Analysts, and Consumers \u2013 with distinct expertise, tasks, and needs. We identify limitations with node-link diagrams, and the need for domain-specific visualizations. Finally, we distill several visualization research directions to improve organic discovery and explainable AI with KGs",
                        "external_paper_link": "",
                        "youtube_ff_link": "https://youtu.be/qm_Wu-KaQxU",
                        "youtube_ff_id": "qm_Wu-KaQxU",
                        "bunny_ff_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1239/v-full-1239_Preview.mp4?token=o_Zjemv95ovC1p07SQSxcEbfyNZSLl-eWRRYmSsSBP0&expires=1706590800",
                        "bunny_ff_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1239/v-full-1239_Preview.vtt?token=JPK1oBKOYpR_v42GNBG4ZnJ2t_iDLXG-Hrpp-UlEA44&expires=1706590800",
                        "youtube_prerecorded_link": "https://youtu.be/Lhh7CUjdJA4",
                        "youtube_prerecorded_id": "Lhh7CUjdJA4",
                        "bunny_prerecorded_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1239/v-full-1239_Presentation.mp4?token=Adh39-i5rbx9cuVIDfaoObKAYNMVRBIo1j-Wz1d31V8&expires=1706590800",
                        "bunny_prerecorded_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1239/v-full-1239_Presentation.vtt?token=UQYWWomx404pFO9m8kn3DyQ_eEMGuL3hW7jj2m7c0Q8&expires=1706590800"
                    },
                    {
                        "slot_id": "v-full-1564",
                        "session_id": "full9",
                        "title": "Scalable Hypergraph Visualization",
                        "contributors": [
                            "Peter Oliver"
                        ],
                        "authors": [
                            "Peter D Oliver",
                            "Eugene Zhang",
                            "Yue Zhang"
                        ],
                        "abstract": "Hypergraph visualization has many applications in network data analysis. Recently, a polygon-based representation for hypergraphs has been proposed with demonstrated benefits. However, the polygon-based layout often suffers from excessive self-intersections when the input dataset is relatively large. In this paper, we propose a framework in which the hypergraph is iteratively simplified through a set of atomic operations. Then, the layout of the simplest hypergraph is optimized and used as the foundation for a reverse process that brings the simplest hypergraph back to the original one, but with an improved layout. At the core of our approach is the set of atomic simplification operations and an operation priority measure to guide the simplification process. In addition, we introduce necessary definitions and conditions for hypergraph planarity within the polygon representation. We extend our approach to handle simultaneous simplification and layout optimization for both the hypergraph and its dual. We demonstrate the utility of our approach with datasets from a number of real-world applications.",
                        "uid": "v-full-1564",
                        "time_stamp": "2023-10-25T05:45:00Z",
                        "time_start": "2023-10-25T05:45:00Z",
                        "time_end": "2023-10-25T05:57:00Z",
                        "paper_type": "full",
                        "keywords": [
                            "Hypergraph visualization, scalable visualization, polygon layout, hypergraph embedding, primal-dual visualization"
                        ],
                        "doi": "10.1109/TVCG.2023.3326599",
                        "fno": "",
                        "has_image": true,
                        "has_pdf": true,
                        "paper_award": "",
                        "image_caption": "We present a scalable layout optimization framework for polygon visualizations of hypergraphs. Our framework achieves near-optimal polygon layouts for large hypergraphs (left) by first iteratively applying vertex and hyperedge-based simplification operations to scale down the input hypergraph. The coarsest simplified scale is determined by some user-specified criteria (right). After the layout of this simplified scale is optimized, the applied operations are iteratively inverted, and the layout is refined at each intermediate scale until the original scale is reached. An example of an intermediate scale is also shown (middle).",
                        "external_paper_link": "",
                        "youtube_ff_link": "https://youtu.be/DYQn0OaNgN4",
                        "youtube_ff_id": "DYQn0OaNgN4",
                        "bunny_ff_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1564/v-full-1564_Preview.mp4?token=dp4q4jHS50vbSQ_k5CHrNnpnMPW8tH4DskryWpkPcDE&expires=1706590800",
                        "bunny_ff_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1564/v-full-1564_Preview.vtt?token=SMHZQusIw_ahjzcVIEwN9KfVrlwwTIcXK5vfHgcPYzk&expires=1706590800",
                        "youtube_prerecorded_link": "https://youtu.be/Q0aOWOBRCUk",
                        "youtube_prerecorded_id": "Q0aOWOBRCUk",
                        "bunny_prerecorded_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1564/v-full-1564_Presentation.mp4?token=qLNpWRIv-HbxBJunpfZVn98J0fcBCDOYy3ywIoCWebA&expires=1706590800",
                        "bunny_prerecorded_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1564/v-full-1564_Presentation.vtt?token=PK-kvagiQgMKrIy_RlDi12oKKPqpWu8Eo7FMdjF4lzg&expires=1706590800"
                    }
                ]
            },
            {
                "title": "High Dimensional Data",
                "session_id": "full10",
                "event_prefix": "v-full",
                "track": "oneohsix",
                "session_image": "full10.png",
                "chair": [
                    "Helwig Hauser"
                ],
                "time_start": "2023-10-24T22:00:00Z",
                "time_end": "2023-10-24T23:15:00Z",
                "discord_category": "",
                "discord_channel": "106",
                "discord_channel_id": "1161741373410644119",
                "discord_link": "https://discordapp.com/channels/1161400451619627068/1161741373410644119",
                "zoom_private_meeting": "",
                "zoom_private_password": "",
                "zoom_private_link": "",
                "zoom_broadcast_link": "",
                "ff_link": "https://youtu.be/HJ-q-LploO0",
                "time_slots": [
                    {
                        "slot_id": "v-full-1712",
                        "session_id": "full10",
                        "title": "A Parallel Framework for Streaming Dimensionality Reduction",
                        "contributors": [
                            "Jiazhi Xia"
                        ],
                        "authors": [
                            "Jiazhi Xia",
                            "Linquan Huang",
                            "Yiping Sun",
                            "Zhiwei Deng",
                            "Xiaolong (Luke) Zhang",
                            "Minfeng Zhu"
                        ],
                        "abstract": "The visualization of streaming high-dimensional data often needs to consider the speed in dimensionality reduction algorithms, the quality of visualized data patterns, and the stability of view graphs that usually change over time with new data. Existing methods of streaming high-dimensional data visualization primarily line up essential modules in a serial manner and often face challenges in satisfying all these design considerations. In this research, we propose a novel parallel framework for streaming high-dimensional data visualization to achieve high data processing speed, high quality in data patterns, and good stability in visual presentations. This framework arranges all essential modules in parallel to mitigate the delays caused by module waiting in serial setups. In addition, to facilitate the parallel pipeline, we redesign these modules with a parametric non-linear embedding method for new data embedding, an incremental learning method for online embedding function updating, and a hybrid strategy for optimized embedding updating. We also improve the coordination mechanism among these modules. Our experiments show that our method has advantages in embedding speed, quality, and stability over other existing methods to visualize streaming high-dimensional data.",
                        "uid": "v-full-1712",
                        "time_stamp": "2023-10-24T22:00:00Z",
                        "time_start": "2023-10-24T22:00:00Z",
                        "time_end": "2023-10-24T22:12:00Z",
                        "paper_type": "full",
                        "keywords": [
                            "High-dimensional data visualization, dimensionality reduction, streaming data visualization"
                        ],
                        "doi": "10.1109/TVCG.2023.3326515",
                        "fno": "",
                        "has_image": true,
                        "has_pdf": true,
                        "paper_award": "",
                        "image_caption": "We show the parallel framework proposed in this paper in the red dashed box, and the three core modules of the framework are shown in the blue dotted boxes: (a) an incremental learning method for online embedding function updating, (b) a parametric non-linear embedding method for new data embedding, and (c) a hybrid strategy for local and global embedding updating. With this parallel framework, we proposed SDR to achieve realtime, trustworthy and stable embedding results.",
                        "external_paper_link": "",
                        "youtube_ff_link": "https://youtu.be/lHQAMgYuyvA",
                        "youtube_ff_id": "lHQAMgYuyvA",
                        "bunny_ff_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1712/v-full-1712_Preview.mp4?token=IHjAcuWyA9uiTFjXr3aTLugtxFcfwcd8xN9_yJHmEck&expires=1706590800",
                        "bunny_ff_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1712/v-full-1712_Preview.vtt?token=4HJ3JaevAzmu9dUgOSm9nQ8-lIkNluXk0bIxZIl85Yk&expires=1706590800",
                        "youtube_prerecorded_link": "https://youtu.be/L39askTrxh8",
                        "youtube_prerecorded_id": "L39askTrxh8",
                        "bunny_prerecorded_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1712/v-full-1712_Presentation.mp4?token=WF49HbE-C1eMAHgev1YIIdJRsQxaOILXewquaKj2_os&expires=1706590800",
                        "bunny_prerecorded_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1712/v-full-1712_Presentation.vtt?token=5EiBGnVgo46bK16WnC03Dqc2nJ4Kd89wcl-Y8Bx-55w&expires=1706590800"
                    },
                    {
                        "slot_id": "v-full-1314",
                        "session_id": "full10",
                        "title": "QEVIS: Multi-grained Visualizing of Distributed Query Execution",
                        "contributors": [
                            "Qiaomu Shen"
                        ],
                        "authors": [
                            "Qiaomu Shen",
                            "Zhengxin You",
                            "Xiao Yan",
                            "Chaozu Zhang",
                            "Ke Xu",
                            "Jianbin Qin",
                            "Dan Zeng",
                            "Bo Tang"
                        ],
                        "abstract": "Distributed query processing systems such as Apache Hive and Spark are widely-used in many organizations for large-scale data analytics. Analyzing and understanding the query execution process of these systems are daily routines for engineers and crucial for identifying performance problems, optimizing system configurations, and rectifying errors. However, existing visualization tools for distributed query execution are insufficient because (i) most of them (if not all) do not provide fine-grained visualization (i.e., the atomic task level), which can be crucial for understanding query performance and reasoning about the underlying execution anomalies, and (ii) they do not support proper linkages between system status and query execution, which makes it difficult to identify the causes of execution problems. To tackle these limitations, we propose QEVIS, which visualizes distributed query execution process with multiple views that focus on different granularities and complement each other. Specifically, we first devise a query logical plan layout algorithm to visualize the overall query execution progress compactly and clearly. We then propose two novel scoring methods to summarize the anomaly degrees of the jobs and machines during query execution, and visualize the anomaly scores intuitively, which allow users to easily identify the components that are worth paying attention to. Moreover, we devise a scatter plot-based task view to show a massive number of atomic tasks, where task distribution patterns are informative for execution problems. We also equip QEVIS with a suite of auxiliary views and interaction methods to support easy and effective cross-view exploration, which makes it convenient to track the causes of execution problems. QEVIS has been used in the production environment of our industry partner, and we present three use cases from real-world applications and user interview to demonstrate its effectiveness. QEVIS is open-source at https://github.com/DBGroup-SUSTech/QEVIS.",
                        "uid": "v-full-1314",
                        "time_stamp": "2023-10-24T22:12:00Z",
                        "time_start": "2023-10-24T22:12:00Z",
                        "time_end": "2023-10-24T22:24:00Z",
                        "paper_type": "full",
                        "keywords": [
                            "visual analytics system, distributed query execution, performance analysis"
                        ],
                        "doi": "10.1109/TVCG.2023.3326930",
                        "fno": "",
                        "has_image": true,
                        "has_pdf": true,
                        "paper_award": "",
                        "image_caption": "QEVIS is a visual analytics system designed for a comprehensive understanding of distributed query execution. It offers a suite of coordinated views, showcasing execution at various granularities. The job view (a) uses a new TDAG layout to depict Map/Reducer job processes and dependencies. Unique anomaly scores in the performance matrix (b) pinpoint suspicious jobs and machines. The task view (c), with a scatter-plot visualization, reveals patterns and highlights significant tasks. Entity list (d) provide additional insights, including detailed statistics on job, task, and machine components, and a profiling view (e) aligns machine status with task execution.",
                        "external_paper_link": "",
                        "youtube_ff_link": "https://youtu.be/pg5fqxrqgFc",
                        "youtube_ff_id": "pg5fqxrqgFc",
                        "bunny_ff_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1314/v-full-1314_Preview.mp4?token=SL4MXllCeZ_tRINqlq-8IPzDP7ZuH5LXDT-LmveJIaw&expires=1706590800",
                        "bunny_ff_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1314/v-full-1314_Preview.vtt?token=bHHLKVi2a4RX94jA9A0wfrUWu2smEeISXQhGt5h-TcQ&expires=1706590800",
                        "youtube_prerecorded_link": "https://youtu.be/fYXKIJmLs0g",
                        "youtube_prerecorded_id": "fYXKIJmLs0g",
                        "bunny_prerecorded_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1314/v-full-1314_Presentation.mp4?token=QjjbImhI18vkUvMRycEntJWsCKPMKbPAH1Z3460IElM&expires=1706590800",
                        "bunny_prerecorded_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1314/v-full-1314_Presentation.vtt?token=hPSt-SfmlQSFQ0gNm85V3FoC-2NPFiRCNYLxQvOAS_c&expires=1706590800"
                    },
                    {
                        "slot_id": "v-tvcg-9956753",
                        "session_id": "full10",
                        "title": "EVNet: An Explainable Deep Network for Dimension Reduction",
                        "contributors": [
                            "Zelin Zang"
                        ],
                        "authors": [
                            "Zelin Zang",
                            "Shenghui Cheng",
                            "Linyan Lu",
                            "Hanchen Xia",
                            "Liangyu Li",
                            "Yaoting Sun",
                            "Yongjie Xu",
                            "Lei Shang",
                            "Baigui Sun",
                            "Stan Z. Li"
                        ],
                        "abstract": "Dimension reduction (DR) is commonly utilized to capture the intrinsic structure and transform high-dimensional data into low-dimensional space while retaining meaningful properties of the original data. It is used in various applications, such as image recognition, single-cell sequencing analysis, and biomarker discovery. However, contemporary parametric-free and parametric DR techniques suffer from several significant shortcomings, such as the inability to preserve global and local features and the pool generalization performance. On the other hand, regarding explainability, it is crucial to comprehend the embedding process, especially the contribution of each part to the embedding process, while understanding how each feature affects the embedding results that identify critical components and help diagnose the embedding process. To address these problems, we have developed a deep neural network method called EVNet, which provides not only excellent performance in structural maintainability but also explainability to the DR therein. EVNet starts with data augmentation and a manifold-based loss function to improve embedding performance. The explanation is based on saliency maps and aims to examine the trained EVNet parameters and contributions of components during the embedding process. The proposed techniques are integrated with a visual interface to help the user to adjust EVNet to achieve better DR performance and explainability. The interactive visual interface makes it easier to illustrate the data features, compare different DR techniques, and investigate DR. An in-depth experimental comparison shows that EVNet consistently outperforms the state-of-the-art methods in both performance measures and explainability.",
                        "uid": "v-tvcg-9956753",
                        "time_stamp": "2023-10-24T22:24:00Z",
                        "time_start": "2023-10-24T22:24:00Z",
                        "time_end": "2023-10-24T22:36:00Z",
                        "paper_type": "full",
                        "keywords": [
                            "Dimension Reduction;Explainability of DR Models;Deep Learning;Parametric Model"
                        ],
                        "doi": "10.1109/TVCG.2022.3223399",
                        "fno": "9956753",
                        "has_image": false,
                        "has_pdf": false,
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "youtube_ff_link": "",
                        "youtube_ff_id": "",
                        "bunny_ff_link": "",
                        "bunny_ff_subtitles": "",
                        "youtube_prerecorded_link": "",
                        "youtube_prerecorded_id": "",
                        "bunny_prerecorded_link": "",
                        "bunny_prerecorded_subtitles": ""
                    },
                    {
                        "slot_id": "v-tvcg-10158903",
                        "session_id": "full10",
                        "title": "FS/DS: A Theoretical Framework for the Dual Analysis of Feature Space and Data Space",
                        "contributors": [
                            "Frederik Dennig"
                        ],
                        "authors": [
                            "Frederik L. Dennig",
                            "Matthias Miller",
                            "Daniel A. Keim",
                            "Mennatallah El-Assady"
                        ],
                        "abstract": "With the surge of data-driven analysis techniques, there is a rising demand for enhancing the exploration of large high-dimensional data by enabling interactions for the joint analysis of features (i.e., dimensions). Such a dual analysis of the feature space and data space is characterized by three components, (1) a view visualizing feature summaries, (2) a view that visualizes the data records, and (3) a bidirectional linking of both plots triggered by human interaction in one of both visualizations, e.g., Linking & Brushing. Dual analysis approaches span many domains, e.g., medicine, crime analysis, and biology. The proposed solutions encapsulate various techniques, such as feature selection or statistical analysis. However, each approach establishes a new definition of dual analysis. To address this gap, we systematically reviewed published dual analysis methods to investigate and formalize the key elements, such as the techniques used to visualize the feature space and data space, as well as the interaction between both spaces. From the information elicited during our review, we propose a unified theoretical framework for dual analysis, encompassing all existing approaches extending the field. We apply our proposed formalization describing the interactions between each component and relate them to the addressed tasks. Additionally, we categorize the existing approaches using our framework and derive future research directions to advance dual analysis by including state-of-the-art visual analysis techniques to improve data exploration.",
                        "uid": "v-tvcg-10158903",
                        "time_stamp": "2023-10-24T22:36:00Z",
                        "time_start": "2023-10-24T22:36:00Z",
                        "time_end": "2023-10-24T22:48:00Z",
                        "paper_type": "full",
                        "keywords": [
                            "Visual analytics;dual analysis;feature space;data space;feature exploration;mixed data;high-dimensional data"
                        ],
                        "doi": "10.1109/TVCG.2023.3288356",
                        "fno": "10158903",
                        "has_image": true,
                        "has_pdf": false,
                        "paper_award": "",
                        "image_caption": "Due to the rising demand for enhancing the exploration of large high-dimensional data, enabling interactions for the joint analysis of features or dimensions is crucial. Such a dual analysis of the feature space and data space is characterized by three components: A view visualizing feature summaries, a view that visualizes the data items, and a bidirectional linking of both plots. Existing solutions encapsulate techniques, such as feature selection or statistical analysis, but establish idiosyncratic definitions of dual analysis. Thus, we propose a unified theoretical framework for dual analysis, encompassing all existing approaches extending the field.",
                        "external_paper_link": "",
                        "youtube_ff_link": "https://youtu.be/u3iGvwNwEno",
                        "youtube_ff_id": "u3iGvwNwEno",
                        "bunny_ff_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-tvcg/v-tvcg-10158903/v-tvcg-10158903_Preview.mp4?token=i5uGjR7jd79AlHtiGLhbgBZaXpfJQm8_KgbCbcLzNnY&expires=1706590800",
                        "bunny_ff_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-tvcg/v-tvcg-10158903/v-tvcg-10158903_Preview.vtt?token=Oc6_rrI6UBOEYfwbjy9wP7CEYy-dIERERocmO8wxhGM&expires=1706590800",
                        "youtube_prerecorded_link": "https://youtu.be/Nc0iW7q8J_Y",
                        "youtube_prerecorded_id": "Nc0iW7q8J_Y",
                        "bunny_prerecorded_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-tvcg/v-tvcg-10158903/v-tvcg-10158903_Presentation.mp4?token=7-bjTsDh4aDpi6uh5pM2q-gflsAqRGN1yoMy-cPYbuM&expires=1706590800",
                        "bunny_prerecorded_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-tvcg/v-tvcg-10158903/v-tvcg-10158903_Presentation.vtt?token=NebEeWGfhRKWRi1dJfOWutRGM7Fn-KRNu_jd07Ohs2A&expires=1706590800"
                    },
                    {
                        "slot_id": "v-full-1507",
                        "session_id": "full10",
                        "title": "Class-constrained t-SNE: Combining Data Features and Class Probabilities",
                        "contributors": [
                            "Linhao Meng"
                        ],
                        "authors": [
                            "Linhao Meng",
                            "Stef van den Elzen",
                            "Nicola Pezzotti",
                            "Anna Vilanova"
                        ],
                        "abstract": "Data features and class probabilities are two main perspectives when, e.g., evaluating model results and identifying problematic items. Class probabilities represent the likelihood that each instance belongs to a particular class, which can be produced by probabilistic classifiers or even human labeling with uncertainty. Since both perspectives are multi-dimensional data, dimensionality reduction (DR) techniques are commonly used to extract informative characteristics from them. However, existing methods either focus solely on the data feature perspective or rely on class probability estimates to guide the DR process. In contrast to previous work where separate views are linked to conduct the analysis, we propose a novel approach, class-constrained t-SNE, that combines data features and class probabilities in the same DR result. Specifically, we combine them by balancing two corresponding components in a cost function to optimize the positions of data points and iconic representation of classes -- class landmarks. Furthermore, an interactive user-adjustable parameter balances these two components so that users can focus on the weighted perspectives of interest and also empowers a smooth visual transition between varying perspectives to preserve the mental map. We illustrate its application potential in model evaluation and visual-interactive labeling. A comparative analysis is performed to evaluate the DR results.",
                        "uid": "v-full-1507",
                        "time_stamp": "2023-10-24T22:48:00Z",
                        "time_start": "2023-10-24T22:48:00Z",
                        "time_end": "2023-10-24T23:00:00Z",
                        "paper_type": "full",
                        "keywords": [
                            "Dimensionality reduction, t-distributed stochastic neighbor embedding, constraint integration"
                        ],
                        "doi": "10.1109/TVCG.2023.3326600",
                        "fno": "",
                        "has_image": true,
                        "has_pdf": true,
                        "paper_award": "",
                        "image_caption": "Class-constrained t-SNE is a dimensionality reduction-based visualization method which integrates both data feature and class probability structures into a single projection view. Users can control the balance between the two structures with an interactive parameter.",
                        "external_paper_link": "",
                        "youtube_ff_link": "https://youtu.be/91nAgNhpHIg",
                        "youtube_ff_id": "91nAgNhpHIg",
                        "bunny_ff_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1507/v-full-1507_Preview.mp4?token=WfTQRSbstmMjpLlNMC261Kba4hTR6_AQSnucqu8cx9o&expires=1706590800",
                        "bunny_ff_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1507/v-full-1507_Preview.vtt?token=54SIadU7c7QF4TxUr3EizmdbqYnw6NyZH5Ce2QikkHE&expires=1706590800",
                        "youtube_prerecorded_link": "https://youtu.be/O2xRwsUpIc0",
                        "youtube_prerecorded_id": "O2xRwsUpIc0",
                        "bunny_prerecorded_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1507/v-full-1507_Presentation.mp4?token=aUdezZHlPmM0G4HASf1GLCNuO6FPngqXRA_BGZtQ3M4&expires=1706590800",
                        "bunny_prerecorded_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1507/v-full-1507_Presentation.vtt?token=jp79glH7WzHnsKf0m4B3tJFzR_AspnbgBgAso2CrDFI&expires=1706590800"
                    },
                    {
                        "slot_id": "v-full-1469",
                        "session_id": "full10",
                        "title": "ManiVault: A Flexible and Extensible Visual Analytics Framework for High-Dimensional Data",
                        "contributors": [
                            "Alexander Vieth"
                        ],
                        "authors": [
                            "Alexander Vieth",
                            "Thomas Kroes",
                            "Julian Thijssen",
                            "Baldur van Lew",
                            "Jeroen Eggermont",
                            "Soumyadeep Basu",
                            "Elmar Eisemann",
                            "Anna Vilanova",
                            "Thomas H\u00f6llt",
                            "Boudewijn Lelieveldt"
                        ],
                        "abstract": "Exploration and analysis of high-dimensional data are important tasks in many fields that produce large and complex data, like the financial sector, systems biology, or cultural heritage. Tailor-made visual analytics software is developed for each specific application, limiting their applicability in other fields. However, as diverse as these fields are, their characteristics and requirements for data analysis are conceptually similar. Many applications share abstract tasks and data types and are often constructed with similar building blocks. Developing such applications, even when based mostly on existing building blocks, requires significant engineering efforts. We developed ManiVault, a flexible and extensible open-source visual analytics framework for analyzing high-dimensional data. The primary objective of ManiVault is to facilitate rapid prototyping of visual analytics workflows for visualization software developers and practitioners alike. ManiVault is built using a plugin-based architecture that offers easy extensibility. While our architecture deliberately keeps plugins self-contained, to guarantee maximum flexibility and re-usability, we have designed and implemented a messaging API for tight integration and linking of modules to support common visual analytics design patterns. We provide several visualization and analytics plugins, and ManiVault's API makes the integration of new plugins easy for developers. ManiVault facilitates the distribution of visualization and analysis pipelines and results for practitioners through saving and reproducing complete application states. As such, ManiVault can be used as a communication tool among researchers to discuss workflows and results. A copy of this paper and all supplemental material is available at https://osf.io/9k6jw and source code at https://github.com/ManiVaultStudio.",
                        "uid": "v-full-1469",
                        "time_stamp": "2023-10-24T23:00:00Z",
                        "time_start": "2023-10-24T23:00:00Z",
                        "time_end": "2023-10-24T23:12:00Z",
                        "paper_type": "full",
                        "keywords": [
                            "High-dimensional data, Visual analytics, Visualization framework, Progressive analytics, Prototyping system"
                        ],
                        "doi": "10.1109/TVCG.2023.3326582",
                        "fno": "",
                        "has_image": true,
                        "has_pdf": true,
                        "paper_award": "honorable",
                        "image_caption": "Example screenshot of ManiVault used for the exploration of a hyperspectral imaging data. The high-dimensional data set is explored using an image viewer, two scatterplot views that show different levels a hierarchical embedding, and a line chart that depicts average spectra of three data clusters.",
                        "external_paper_link": "",
                        "youtube_ff_link": "https://youtu.be/KFUyjH8dBsk",
                        "youtube_ff_id": "KFUyjH8dBsk",
                        "bunny_ff_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1469/v-full-1469_Preview.mp4?token=l7fe3Y6nK9eOzIAVsfBrRhvYcjDyX2CDKvfBtuU1Ubc&expires=1706590800",
                        "bunny_ff_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1469/v-full-1469_Preview.vtt?token=DK17jqAkrhua1ZWGvIExAYRH88Qifv-No6NibKhzUaY&expires=1706590800",
                        "youtube_prerecorded_link": "https://youtu.be/zR0CAUMdGEM",
                        "youtube_prerecorded_id": "zR0CAUMdGEM",
                        "bunny_prerecorded_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1469/v-full-1469_Presentation.mp4?token=9FvU3cfrS-SugfgTuMH3s9HGHQlGEDC3CHqxdHzcTQE&expires=1706590800",
                        "bunny_prerecorded_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1469/v-full-1469_Presentation.vtt?token=OgVVEHsP4sfMe2L5mZiNB0Pd24hzDit4yOWvTb2Mr3Y&expires=1706590800"
                    }
                ]
            },
            {
                "title": "Image and Video",
                "session_id": "full11",
                "event_prefix": "v-full",
                "track": "oneohthree",
                "session_image": "full11.png",
                "chair": [
                    "Quan Li"
                ],
                "time_start": "2023-10-24T22:00:00Z",
                "time_end": "2023-10-24T23:15:00Z",
                "discord_category": "",
                "discord_channel": "103",
                "discord_channel_id": "1161741183815524502",
                "discord_link": "https://discordapp.com/channels/1161400451619627068/1161741183815524502",
                "zoom_private_meeting": "",
                "zoom_private_password": "",
                "zoom_private_link": "",
                "zoom_broadcast_link": "",
                "ff_link": "https://youtu.be/sp2xH_WqZGg",
                "time_slots": [
                    {
                        "slot_id": "v-full-1515",
                        "session_id": "full11",
                        "title": "Guided Visual Analytics for Image Selection in Time and Space",
                        "contributors": [
                            "Ignacio P\u00e9rez-Messina"
                        ],
                        "authors": [
                            "Ignacio P\u00e9rez-Messina",
                            "Davide Ceneda",
                            "Silvia Miksch"
                        ],
                        "abstract": "Unexploded Ordnance (UXO) detection, the identification of remnant active bombs buried underground from archival aerial images, implies a complex workflow involving decision-making at each stage. An essential phase in UXO detection is the task of image selection, where a small subset of images must be chosen from archives to reconstruct an area of interest (AOI) and identify craters. The selected image set must comply with good spatial and temporal coverage over the AOI, particularly in the temporal vicinity of recorded aerial attacks, and do so with minimal images for resource optimization. This paper presents a guidance-enhanced visual analytics prototype to select images for UXO detection. In close collaboration with domain experts, our design process involved analyzing user tasks, eliciting expert knowledge, modeling quality metrics, and choosing appropriate guidance. We report on a user study with two real-world scenarios of image selection performed with and without guidance. Our solution was well-received and deemed highly usable. Through the lens of our task-based design and developed quality measures, we observed guidance-driven changes in user behavior and improved quality of analysis results. An expert evaluation of the study allowed us to improve our guidance-enhanced prototype further and discuss new possibilities for user-adaptive guidance.",
                        "uid": "v-full-1515",
                        "time_stamp": "2023-10-24T22:00:00Z",
                        "time_start": "2023-10-24T22:00:00Z",
                        "time_end": "2023-10-24T22:12:00Z",
                        "paper_type": "full",
                        "keywords": [
                            "Application Motivated Visualization, Geospatial Data, Mixed Initiative Human-Machine Analysis, Process/Workflow Design, Task Abstractions & Application Domains, Temporal Data"
                        ],
                        "doi": "10.1109/TVCG.2023.3326572",
                        "fno": "",
                        "has_image": true,
                        "has_pdf": true,
                        "paper_award": "",
                        "image_caption": "To enhance the UXO detection expert's decision-making process, our guided VA system for image selection in UXO detection visualizes image metadata in time while providing orientation (blue) and recommendations (red), employing an optimisation model that adapts to the user's selection (green).",
                        "external_paper_link": "",
                        "youtube_ff_link": "https://youtu.be/3P6-uz4Cun8",
                        "youtube_ff_id": "3P6-uz4Cun8",
                        "bunny_ff_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1515/v-full-1515_Preview.mp4?token=uB-kzDLiMYHDHwSPmwCpRF8N9apGjIfASUVeOtLOCSE&expires=1706590800",
                        "bunny_ff_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1515/v-full-1515_Preview.vtt?token=BAyFPIepM-M3vG-kNRPmXp2nGSDhL3f222Fq7p8nyME&expires=1706590800",
                        "youtube_prerecorded_link": "https://youtu.be/HzGQ6yczMPI",
                        "youtube_prerecorded_id": "HzGQ6yczMPI",
                        "bunny_prerecorded_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1515/v-full-1515_Presentation.mp4?token=ulr95_1wD_EP5_rc_Q5L1d8a_7Z062bwwWK0uICGuNo&expires=1706590800",
                        "bunny_prerecorded_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1515/v-full-1515_Presentation.vtt?token=vvJ31naklxlO2yjtX7zxLn8IkrZT1jNjnug2dhc5INE&expires=1706590800"
                    },
                    {
                        "slot_id": "v-tvcg-9984953",
                        "session_id": "full11",
                        "title": "An Image-based Exploration and Query System for Large Visualization Collections via Neural Image Embedding",
                        "contributors": [
                            "Yilin Ye"
                        ],
                        "authors": [
                            "Yilin Ye",
                            "Rong Huang",
                            "Wei Zeng"
                        ],
                        "abstract": "High-quality visualization collections are beneficial for a variety of applications including visualization reference and data-driven visualization design. The visualization community has created many visualization collections, and developed interactive exploration systems for the collections. However, the systems are mainly based on extrinsic attributes like authors and publication years, whilst neglect intrinsic property ( i.e ., visual appearance) of visualizations, hindering visual comparison and query of visualization designs. This paper presents VISAtlas , an image-based approach empowered by neural image embedding, to facilitate exploration and query for visualization collections. To improve embedding accuracy, we create a comprehensive collection of synthetic and real-world visualizations, and use it to train a convolutional neural network (CNN) model with a triplet loss for taxonomical classification of visualizations. Next, we design a coordinated multiple view (CMV) system that enables multi-perspective exploration and design retrieval based on visualization embeddings. Specifically, we design a novel embedding overview that leverages contextual layout framework to preserve the context of the embedding vectors with the associated visualization taxonomies, and density plot and sampling techniques to address the overdrawing problem. We demonstrate in three case studies and one user study the effectiveness of VISAtlas in supporting comparative analysis of visualization collections, exploration of composite visualizations, and image-based retrieval of visualization designs. The studies reveal that real-world visualization collections ( e.g ., Beagle and VIS30K) better accord with the richness and diversity of visualization designs than synthetic collections ( e.g ., Data2Vis), inspiring composite visualizations are identified in real-world collections, and distinct design patterns exist in visualizations from different sources.",
                        "uid": "v-tvcg-9984953",
                        "time_stamp": "2023-10-24T22:12:00Z",
                        "time_start": "2023-10-24T22:12:00Z",
                        "time_end": "2023-10-24T22:24:00Z",
                        "paper_type": "full",
                        "keywords": [
                            "Visualization collection;image embedding;visual query;image visualization;design pattern"
                        ],
                        "doi": "10.1109/TVCG.2022.3229023",
                        "fno": "9984953",
                        "has_image": true,
                        "has_pdf": false,
                        "paper_award": "",
                        "image_caption": "Visualization image collections are valuable for learning visualization, training AI, and inspiring design.  Many existing exploration system of such collections rely on attribute metadata, limiting application to various collections with divergent metadata.  We propose image embedding method to allow for interpretable image-based exploration.   The method combines neural image embeddings based on classification model and contextual projection with density plot and point sampling.  A system is developed based on the method to assist visual comparative analysis and exploration of different collections.",
                        "external_paper_link": "",
                        "youtube_ff_link": "https://youtu.be/H6NW6PvYn1A",
                        "youtube_ff_id": "H6NW6PvYn1A",
                        "bunny_ff_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-tvcg/v-tvcg-9984953/v-tvcg-9984953_Preview.mp4?token=A_dpRpMp0XAV7afzwkqv2YwsEEUbCTsoAc_Zp9huS3M&expires=1706590800",
                        "bunny_ff_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-tvcg/v-tvcg-9984953/v-tvcg-9984953_Preview.vtt?token=rUh0Q1jMAbYMiobjTA1cKW8sXiswcUC4vvsye_DcuTY&expires=1706590800",
                        "youtube_prerecorded_link": "https://youtu.be/-BXxwETo01s",
                        "youtube_prerecorded_id": "-BXxwETo01s",
                        "bunny_prerecorded_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-tvcg/v-tvcg-9984953/v-tvcg-9984953_Presentation.mp4?token=wRzf7fyUzmVAFcYCpINYphO1zh_OAhSfaeJTVS90FiM&expires=1706590800",
                        "bunny_prerecorded_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-tvcg/v-tvcg-9984953/v-tvcg-9984953_Presentation.vtt?token=pB6GbhLsKpKGky8kXl6CXDHM3C9JV0HSyh0UdtxcoJk&expires=1706590800"
                    },
                    {
                        "slot_id": "v-tvcg-10081386",
                        "session_id": "full11",
                        "title": "Image Collage on Arbitrary Shape via Shape-Aware Slicing and Optimization",
                        "contributors": [
                            "Dong-Yi Wu"
                        ],
                        "authors": [
                            "Dong-Yi Wu",
                            "Thi-Ngoc-Hanh Le",
                            "Sheng-Yi Yao",
                            "Yun-Chen Lin",
                            "Tong-Yee Lee"
                        ],
                        "abstract": "Image collage is a very useful tool for visualizing an image collection. Most of the existing methods and commercial applications for generating image collages are designed on simple shapes, such as rectangular and circular layouts. This greatly limits the use of image collages in some artistic and creative settings. Although there are some methods that can generate irregularly-shaped image collages, they often suffer from severe image overlapping and excessive blank space. This prevents such methods from being effective information communication tools. In this paper, we present a shape slicing algorithm and an optimization scheme that can create image collages of arbitrary shapes in an informative and visually pleasing manner given an input shape and an image collection. To overcome the challenge of irregular shapes, we propose a novel algorithm, called Shape-Aware Slicing, which partitions the input shape into cells based on medial axis and binary slicing tree. Shape-Aware Slicing, which is designed specifically for irregular shapes, takes human perception and shape structure into account to generate visually pleasing partitions. Then, the layout is optimized by analyzing input images with the goal of maximizing the total salient regions of the images. To evaluate our method, we conduct extensive experiments and compare our results against previous work. The evaluations show that our proposed algorithm can efficiently arrange image collections on irregular shapes and create visually superior results than prior work and existing commercial tools.",
                        "uid": "v-tvcg-10081386",
                        "time_stamp": "2023-10-24T22:24:00Z",
                        "time_start": "2023-10-24T22:24:00Z",
                        "time_end": "2023-10-24T22:36:00Z",
                        "paper_type": "full",
                        "keywords": [
                            "Image collection visualization;image collage;irregular shape layout"
                        ],
                        "doi": "10.1109/TVCG.2023.3262039",
                        "fno": "10081386",
                        "has_image": true,
                        "has_pdf": false,
                        "paper_award": "",
                        "image_caption": "Image Collage on Arbitrary Shape via Shape-Aware Slicing and Optimization",
                        "external_paper_link": "",
                        "youtube_ff_link": "https://youtu.be/ANQ590xHKwA",
                        "youtube_ff_id": "ANQ590xHKwA",
                        "bunny_ff_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-tvcg/v-tvcg-10081386/v-tvcg-10081386_Preview.mp4?token=wd70LPRp_bQXPPx02hlnhjidsKPvXYqFFO-MIWSc5To&expires=1706590800",
                        "bunny_ff_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-tvcg/v-tvcg-10081386/v-tvcg-10081386_Preview.vtt?token=Cu-_QfJZGeRAgmXGPmGXeNW1xmmOD1LeJPiC-hlFNMQ&expires=1706590800",
                        "youtube_prerecorded_link": "https://youtu.be/K86NPBRc0xA",
                        "youtube_prerecorded_id": "K86NPBRc0xA",
                        "bunny_prerecorded_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-tvcg/v-tvcg-10081386/v-tvcg-10081386_Presentation.mp4?token=eR0snRlNrqOhRYvH-CW7uxYLb7YTkDXyWb0JPJNxDRg&expires=1706590800",
                        "bunny_prerecorded_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-tvcg/v-tvcg-10081386/v-tvcg-10081386_Presentation.vtt?token=_J_FV9mmU3kW0vYS0P7_y9k9FPXqCbOgujhyXXjD78s&expires=1706590800"
                    },
                    {
                        "slot_id": "v-tvcg-9729541",
                        "session_id": "full11",
                        "title": "VisImages: A Fine-Grained Expert-Annotated Visualization Dataset",
                        "contributors": [
                            "Dazhen Deng"
                        ],
                        "authors": [
                            "Dazhen Deng",
                            "Yihong Wu",
                            "Xinhuan Shu",
                            "Jiang Wu",
                            "Siwei Fu",
                            "Weiwei Cui",
                            "Yingcai Wu"
                        ],
                        "abstract": "Images in visualization publications contain rich information, e.g., novel visualization designs and implicit design patterns of visualizations. A systematic collection of these images can contribute to the community in many aspects, such as literature analysis and automated tasks for visualization. In this paper, we build and make public a dataset, VisImages, which collects 12,267 images with captions from 1,397 papers in IEEE InfoVis and VAST. Built upon a comprehensive visualization taxonomy, the dataset includes 35,096 visualizations and their bounding boxes in the images. We demonstrate the usefulness of VisImages through three use cases: 1) investigating the use of visualizations in the publications with VisImages Explorer, 2) training and benchmarking models for visualization classification, and 3) localizing visualizations in the visual analytics systems automatically.",
                        "uid": "v-tvcg-9729541",
                        "time_stamp": "2023-10-24T22:36:00Z",
                        "time_start": "2023-10-24T22:36:00Z",
                        "time_end": "2023-10-24T22:48:00Z",
                        "paper_type": "full",
                        "keywords": [
                            "Visualization dataset;crowdsourcing;literature analysis;visualization classification;visualization detection"
                        ],
                        "doi": "10.1109/TVCG.2022.3155440",
                        "fno": "9729541",
                        "has_image": true,
                        "has_pdf": false,
                        "paper_award": "",
                        "image_caption": "VisImages: A Fine-Grained Expert-Annotated Visualization Dataset",
                        "external_paper_link": "",
                        "youtube_ff_link": "https://youtu.be/UAMTr28Qwvs",
                        "youtube_ff_id": "UAMTr28Qwvs",
                        "bunny_ff_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-tvcg/v-tvcg-9729541/v-tvcg-9729541_Preview.mp4?token=Jl4jLOPTsw6zjLZkPaVgevfB3BsDK59RNsvjqUxKStA&expires=1706590800",
                        "bunny_ff_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-tvcg/v-tvcg-9729541/v-tvcg-9729541_Preview.vtt?token=H9rD1ZWE4riM8HFGzjpajFhfczMJX0AtPqMRkiI6_j8&expires=1706590800",
                        "youtube_prerecorded_link": "https://youtu.be/4AuXXeA_8F0",
                        "youtube_prerecorded_id": "4AuXXeA_8F0",
                        "bunny_prerecorded_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-tvcg/v-tvcg-9729541/v-tvcg-9729541_Presentation.mp4?token=_fultBZzfOUvZ_9gyeXfnsI0sDx5CK3zdgXWIYZkML0&expires=1706590800",
                        "bunny_prerecorded_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-tvcg/v-tvcg-9729541/v-tvcg-9729541_Presentation.vtt?token=YTps9j2pbz0i-edKhjODNb7LH3F-IlRoTL5s2wB_Zbw&expires=1706590800"
                    },
                    {
                        "slot_id": "v-full-1405",
                        "session_id": "full11",
                        "title": "A Unified Interactive Model Evaluation for Classification, Object Detection, and Instance Segmentation in Computer Vision",
                        "contributors": [
                            "Yukai Guo"
                        ],
                        "authors": [
                            "Changjian Chen",
                            "Yukai Guo",
                            "Fengyuan Tian",
                            "Shilong Liu",
                            "Weikai Yang",
                            "Zhaowei Wang",
                            "Jing Wu",
                            "Hang Su",
                            "Hanspeter Pfister",
                            "Shixia Liu"
                        ],
                        "abstract": "Existing model evaluation tools mainly focus on evaluating classification models, leaving a gap in evaluating more complex models, such as object detection. In this paper, we develop an open-source visual analysis tool, Uni-Evaluator, to support a unified model evaluation for classification, object detection, and instance segmentation in computer vision. The key idea behind our method is to formulate both discrete and continuous predictions in different tasks as unified probability distributions. Based on these distributions, we develop 1) a matrix-based visualization to provide an overview of model performance; 2) a table visualization to identify the problematic data subsets where the model performs poorly; 3) a grid visualization to display the samples of interest. These visualizations work together to facilitate the model evaluation from a global overview to individual samples. Two case studies demonstrate the effectiveness of Uni-Evaluator in evaluating model performance and making informed improvements.",
                        "uid": "v-full-1405",
                        "time_stamp": "2023-10-24T22:48:00Z",
                        "time_start": "2023-10-24T22:48:00Z",
                        "time_end": "2023-10-24T23:00:00Z",
                        "paper_type": "full",
                        "keywords": [
                            "Model evaluation, computer vision, classification, object detection, instance segmentation"
                        ],
                        "doi": "10.1109/TVCG.2023.3326588",
                        "fno": "",
                        "has_image": true,
                        "has_pdf": true,
                        "paper_award": "",
                        "image_caption": "Uni-Evaluator is an open-source visual analysis tool to support a unified interactive model evaluation for classification, object detection, and instance segmentation in computer vision. The tool consists of (a) the filtering panel; (b) the matrix-based visualization that provides an overview of model performance; (c) the table visualization that helps identify problematic data subsets; and (d) the grid visualization that displays the samples of interest. These modules work together to facilitate the model evaluation from a global overview to individual samples.",
                        "external_paper_link": "",
                        "youtube_ff_link": "https://youtu.be/4JK2zn0LYdI",
                        "youtube_ff_id": "4JK2zn0LYdI",
                        "bunny_ff_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1405/v-full-1405_Preview.mp4?token=6ksoOQBamOdztE2UtHpY1VI-xxG74qNrQVIoxYzJxbs&expires=1706590800",
                        "bunny_ff_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1405/v-full-1405_Preview.vtt?token=yOATEVqg5gvli32TnNwv2JzxnvRBI0jzXn1s0m1JfyE&expires=1706590800",
                        "youtube_prerecorded_link": "https://youtu.be/zM0FXNzq6PM",
                        "youtube_prerecorded_id": "zM0FXNzq6PM",
                        "bunny_prerecorded_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1405/v-full-1405_Presentation.mp4?token=5aqF8TpchsB0SoWjplBYE767AbzlLrgqHJR1Kz3NrLc&expires=1706590800",
                        "bunny_prerecorded_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1405/v-full-1405_Presentation.vtt?token=gwfOUiwYkkoh3jQZqYSvjKhfWcp2Oh_U08tTTD_V8XA&expires=1706590800"
                    },
                    {
                        "slot_id": "v-full-1408",
                        "session_id": "full11",
                        "title": "VideoPro: A Visual Analytics Approach for Interactive Video Programming",
                        "contributors": [
                            "Jianben He"
                        ],
                        "authors": [
                            "Jianben He",
                            "Xingbo Wang",
                            "Kam Kwai Wong",
                            "Xijie Huang",
                            "Changjian Chen",
                            "Zixin Chen",
                            "Fengjie Wang",
                            "Min Zhu",
                            "Huamin Qu"
                        ],
                        "abstract": "Constructing supervised machine learning models for real-world video analysis require substantial labeled data, which is costly to acquire due to scarce domain expertise and laborious manual inspection. While data programming shows promise in generating labeled data at scale with user-defined labeling functions, the high dimensional and complex temporal information in videos poses additional challenges for effectively composing and evaluating labeling functions. In this paper, we propose VideoPro, a visual analytics approach to support flexible and scalable video data programming for model steering with reduced human effort. We first extract human-understandable events from videos using computer vision techniques and treat them as atomic components of labeling functions. We further propose a two-stage template mining algorithm that characterizes the sequential patterns of these events to serve as labeling function templates for efficient data labeling. The visual interface of VideoPro facilitates multifaceted exploration, examination, and application of the labeling templates, allowing for effective programming of video data at scale. Moreover, users can monitor the impact of programming on model performance and make informed adjustments during the iterative programming process. We demonstrate the efficiency and effectiveness of our approach with two case studies and expert interviews.",
                        "uid": "v-full-1408",
                        "time_stamp": "2023-10-24T23:00:00Z",
                        "time_start": "2023-10-24T23:00:00Z",
                        "time_end": "2023-10-24T23:12:00Z",
                        "paper_type": "full",
                        "keywords": [
                            "Interactive machine learning, data programming, video exploration and analysis"
                        ],
                        "doi": "10.1109/TVCG.2023.3326586",
                        "fno": "",
                        "has_image": true,
                        "has_pdf": true,
                        "paper_award": "",
                        "image_caption": "The VideoPro interface consists of three major views. The Template View (A) offers descriptive statistics and rich interactions to facilitate multi-faceted exploration and comprehension of labeling templates. The Labeling View (B) provides a summary of the nuanced event compositions within the selected template to allow effective template validation and refinement. It also displays retrieved matching videos for efficient examination and at-scale programming. The Info View (C) presents comprehensive information regarding data embedding distribution in latent space and the model iteration process.",
                        "external_paper_link": "",
                        "youtube_ff_link": "https://youtu.be/FCb64peiqBA",
                        "youtube_ff_id": "FCb64peiqBA",
                        "bunny_ff_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1408/v-full-1408_Preview.mp4?token=GyFI8un6us-O71_D5f8zt-yTHXw3Kv99FBE49unayG4&expires=1706590800",
                        "bunny_ff_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1408/v-full-1408_Preview.vtt?token=jJWQnshTLXQAUH9lydcC9cJU28-YOyUNV2mkvXHn0ws&expires=1706590800",
                        "youtube_prerecorded_link": "https://youtu.be/Hn2_KqSMDaY",
                        "youtube_prerecorded_id": "Hn2_KqSMDaY",
                        "bunny_prerecorded_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1408/v-full-1408_Presentation.mp4?token=-08G5MaFzK_VyJWl52ImHdlUFzgeJm1suDOreFDqcD4&expires=1706590800",
                        "bunny_prerecorded_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1408/v-full-1408_Presentation.vtt?token=F5hvfua12u_fZu_gHsTWyk8trJ0CVv0qtUTboH_os2I&expires=1706590800"
                    }
                ]
            },
            {
                "title": "Immersive Analytics and Virtual Reality",
                "session_id": "full12",
                "event_prefix": "v-full",
                "track": "oneohsix",
                "session_image": "full12.png",
                "chair": [
                    "Dieter Schmalstieg"
                ],
                "time_start": "2023-10-25T03:00:00Z",
                "time_end": "2023-10-25T04:15:00Z",
                "discord_category": "",
                "discord_channel": "106",
                "discord_channel_id": "1161741373410644119",
                "discord_link": "https://discordapp.com/channels/1161400451619627068/1161741373410644119",
                "zoom_private_meeting": "",
                "zoom_private_password": "",
                "zoom_private_link": "",
                "zoom_broadcast_link": "",
                "ff_link": "https://youtu.be/0rGtRii_znA",
                "time_slots": [
                    {
                        "slot_id": "v-full-1103",
                        "session_id": "full12",
                        "title": "VIRD: Immersive Match Video Analysis for High-Performance Badminton Coaching",
                        "contributors": [
                            "Tica Lin"
                        ],
                        "authors": [
                            "Tica Lin",
                            "Alexandre Aouididi",
                            "Zhutian Chen",
                            "Johanna Beyer",
                            "Hanspeter Pfister",
                            "Jui-Hsien Wang"
                        ],
                        "abstract": "Badminton is a fast-paced sport that requires a strategic combination of spatial, temporal, and technical tactics. To gain a competitive edge at high-level competitions, badminton professionals frequently analyze match videos to gain insights and develop game strategies. However, the current process for analyzing matches is time-consuming and relies heavily on manual note-taking, due to the lack of automatic data collection and appropriate visualization tools. As a result, there is a gap in effectively analyzing matches and communicating insights among badminton coaches and players. This work proposes an end-to-end immersive match analysis pipeline designed in close collaboration with badminton professionals, including Olympic and national coaches and players. We present VIRD, a VR Bird (i.e., shuttle) immersive analysis tool, that supports interactive badminton game analysis in an immersive environment based on 3D reconstructed game views of the match video. We propose a top-down analytic workflow that allows users to seamlessly move from a high-level match overview to a detailed game view of individual rallies and shots, using situated 3D visualizations and video. We collect 3D spatial and dynamic shot data and player poses with computer vision models and visualize them in VR. Through immersive visualizations, coaches can interactively analyze situated spatial data (player positions, poses, and shot trajectories) with flexible viewpoints while navigating between shots and rallies effectively with embodied interaction. We evaluated the usefulness of VIRD with Olympic and national-level coaches and players in real matches. Results show that immersive analytics supports effective badminton match analysis with reduced context-switching costs and enhances spatial understanding with a high sense of presence.",
                        "uid": "v-full-1103",
                        "time_stamp": "2023-10-25T03:00:00Z",
                        "time_start": "2023-10-25T03:00:00Z",
                        "time_end": "2023-10-25T03:12:00Z",
                        "paper_type": "full",
                        "keywords": [
                            "Sports Analytics, Immersive Analytics, Data Visualization"
                        ],
                        "doi": "10.1109/TVCG.2023.3327161",
                        "fno": "",
                        "has_image": true,
                        "has_pdf": true,
                        "paper_award": "",
                        "image_caption": "{\\rtf1\\ansi\\ansicpg1252\\cocoartf2709 \\cocoatextscaling0\\cocoaplatform0{\\fonttbl\\f0\\fswiss\\fcharset0 ArialMT;\\f1\\froman\\fcharset0 Times-Roman;} {\\colortbl;\\red255\\green255\\blue255;\\red0\\green0\\blue0;} {\\*\\expandedcolortbl;;\\cssrgb\\c0\\c0\\c0;} \\margl1440\\margr1440\\vieww24000\\viewh16320\\viewkind0 \\deftab720 \\pard\\pardeftab720\\partightenfactor0  \\f0\\fs29\\fsmilli14667 \\cf0 \\expnd0\\expndtw0\\kerning0 \\outl0\\strokewidth0 \\strokec2 VIRD, VR Bird, is a VR video analysis tool for badminton coaches and players to effectively analyze badminton match with multi-modal data. \\f1\\fs24 \\  \\f0\\fs29\\fsmilli14667 Users can experience the dynamic 3D poses and ball trajectories in VR, and analyze game data with situated visualizations.\\'a0 \\f1\\fs24 \\  \\f0\\fs29\\fsmilli14667 We contributed to a semi-automatic pipeline, where the coaches only need to provide a game video, and can later analyze the game in VR with all the supporting 3D views and data. \\f1\\fs24 \\ }",
                        "external_paper_link": "",
                        "youtube_ff_link": "",
                        "youtube_ff_id": "",
                        "bunny_ff_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1103/v-full-1103_Preview.mp4?token=zTXIziVBmdfHH2Di-ZMUE0Y2QTZX16V36DByu-KZ4Pk&expires=1706590800",
                        "bunny_ff_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1103/v-full-1103_Preview.vtt?token=frdGya5ZxP7iAN9AdNfA5efq2HJzfhJr1gxc575m1_Q&expires=1706590800",
                        "youtube_prerecorded_link": "https://youtu.be/LxEuKLl9JJA",
                        "youtube_prerecorded_id": "LxEuKLl9JJA",
                        "bunny_prerecorded_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1103/v-full-1103_Presentation.mp4?token=I6x3PQn7TCGA3ZT-mT67d82Vkh6DFzZUwBivlFQBCiE&expires=1706590800",
                        "bunny_prerecorded_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1103/v-full-1103_Presentation.vtt?token=mMA8E2Pr32fZj8DiqeljLiPiaW6ECgNcLWsTuMTB64c&expires=1706590800"
                    },
                    {
                        "slot_id": "v-tvcg-9834145",
                        "session_id": "full12",
                        "title": "Visual Cue Effects on a Classification Accuracy Estimation Task in Immersive Scatterplots",
                        "contributors": [
                            "Fumeng Yang"
                        ],
                        "authors": [
                            "Fumeng Yang",
                            "James Tompkin",
                            "Lane Harrison",
                            "David H. Laidlaw"
                        ],
                        "abstract": "Immersive visualization in virtual reality (VR) allows us to exploit visual cues for perception in 3D space, yet few existing studies have measured the effects of visual cues. Across a desktop monitor and a head-mounted display (HMD), we assessed scatterplot designs which vary their use of visual cues\u2014motion, shading, perspective (graphical projection), and dimensionality\u2014on two sets of data. We conducted a user study with a summary task in which 32 participants estimated the classification accuracy of an artificial neural network from the scatterplots. With Bayesian multilevel modeling, we capture the intricate visual effects and find that no cue alone explains all the variance in estimation error. Visual motion cues generally reduce participants\u2019 estimation error; besides this motion, using other cues may increase participants\u2019 estimation error. Using an HMD, adding visual motion cues, providing a third data dimension, or showing a more complicated dataset leads to longer response times. We speculate that most visual cues may not strongly affect perception in immersive analytics unless they change people\u2019s mental model about data. In summary, by studying participants as they interpret the output from a complicated machine learning model, we advance our understanding of how to use the visual cues in immersive analytics.",
                        "uid": "v-tvcg-9834145",
                        "time_stamp": "2023-10-25T03:12:00Z",
                        "time_start": "2023-10-25T03:12:00Z",
                        "time_end": "2023-10-25T03:24:00Z",
                        "paper_type": "full",
                        "keywords": [
                            "Virtual reality;cluster perception;information visualization;immersive analytics;dimension reduction;classification"
                        ],
                        "doi": "10.1109/TVCG.2022.3192364",
                        "fno": "9834145",
                        "has_image": true,
                        "has_pdf": false,
                        "paper_award": "",
                        "image_caption": "Immersive visualization in virtual reality (VR) allows us to exploit visual cues for perception in 3D space, yet few existing studies have measured the effects of visual cues. Across a desktop monitor (Left) and a head-mounted display (HMD; Right), we assessed scatterplot designs which vary their use of visual cues\u2014motion, shading, perspective (graphical projection), and dimensionality\u2014on two sets of data. We advance our understanding of how to use the visual cues in immersive analytics.",
                        "external_paper_link": "",
                        "youtube_ff_link": "https://youtu.be/WWA3TMcCyUg",
                        "youtube_ff_id": "WWA3TMcCyUg",
                        "bunny_ff_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-tvcg/v-tvcg-9834145/v-tvcg-9834145_Preview.mp4?token=bCLpQaC9ytQTSHXaFn9tuPkaZC-UzmSR5Ry9ioJtcRs&expires=1706590800",
                        "bunny_ff_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-tvcg/v-tvcg-9834145/v-tvcg-9834145_Preview.vtt?token=vOyLZOtjKZufaHtvCnhEB2Lw4BGe8UPMc6W5omnEL8w&expires=1706590800",
                        "youtube_prerecorded_link": "https://youtu.be/T614LKfo9fY",
                        "youtube_prerecorded_id": "T614LKfo9fY",
                        "bunny_prerecorded_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-tvcg/v-tvcg-9834145/v-tvcg-9834145_Presentation.mp4?token=u5xxH0TgfT-FnoDO6XyWy5bZKO101mg6hUu_TD_Ivtc&expires=1706590800",
                        "bunny_prerecorded_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-tvcg/v-tvcg-9834145/v-tvcg-9834145_Presentation.vtt?token=t4kgfklz8Ud0JyHL-1oIk8_J4Auc5NMh3_xZ0QwZTY0&expires=1706590800"
                    },
                    {
                        "slot_id": "v-full-1161",
                        "session_id": "full12",
                        "title": "2D, 2.5D, or 3D? An Exploratory Study on Multilayer Network Visualisations in Virtual Reality",
                        "contributors": [
                            "Stefan Paul Feyer"
                        ],
                        "authors": [
                            "Stefan Paul Feyer",
                            "Bruno Pinaud",
                            "Stephen Kobourov",
                            "Nicolas Brich",
                            "Michael Krone",
                            "Andreas Kerren",
                            "Michael Behrisch",
                            "Falk Schreiber",
                            "Karsten Klein"
                        ],
                        "abstract": "Relational information between different types of entities is often modelled by a multilayer network (MLN) -- a network with subnetworks represented by layers. The layers of an MLN can be arranged in different ways in a visual representation, however, the impact of the arrangement on the readability of the network is an open question. Therefore, we studied this impact for several commonly occurring tasks related to MLN analysis. Additionally, layer arrangements with a dimensionality beyond 2D, which are common in this scenario, motivate the use of stereoscopic displays. We ran a human subject study utilising a Virtual Reality headset to evaluate 2D, 2.5D, and 3D layer arrangements. The study employs six analysis tasks that cover the spectrum of an MLN task taxonomy, from path finding and pattern identification to comparisons between and across layers. We found no clear overall winner. However, we explore the task-to-arrangement space and derive empirical-based recommendations on the effective use of 2D, 2.5D, and 3D layer arrangements for MLNs.",
                        "uid": "v-full-1161",
                        "time_stamp": "2023-10-25T03:24:00Z",
                        "time_start": "2023-10-25T03:24:00Z",
                        "time_end": "2023-10-25T03:36:00Z",
                        "paper_type": "full",
                        "keywords": [
                            "Network, Guidelines, VisDesign, HumanQuant, CompSystems."
                        ],
                        "doi": "10.1109/TVCG.2023.3327402",
                        "fno": "",
                        "has_image": true,
                        "has_pdf": true,
                        "paper_award": "",
                        "image_caption": "The layers of a multilayer network (MLN) can be arranged in different ways in a visual representation, however, the impact of the arrangement on the readability of the network is an open question. Therefore, we studied this impact for several commonly occurring tasks related to MLN analysis. We ran a human subject study utilising a Virtual Reality headset to evaluate 2D, 2.5D, and 3D layer arrangements. We found no clear overall winner. However, we explore the task-to-arrangement space and derive empirical-based recommendations on the effective use of 2D, 2.5D, and 3D layer arrangements for MLNs.",
                        "external_paper_link": "",
                        "youtube_ff_link": "https://youtu.be/76CcqIms1KM",
                        "youtube_ff_id": "76CcqIms1KM",
                        "bunny_ff_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1161/v-full-1161_Preview.mp4?token=_scHK8t19MM6x4q2ivsmJDJqQSnuMjwlzMwXQy2wyiQ&expires=1706590800",
                        "bunny_ff_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1161/v-full-1161_Preview.vtt?token=Tc5AN4JhGe8USXC7-80u0KQTh9SMTvYJFcxBCGxjFw8&expires=1706590800",
                        "youtube_prerecorded_link": "https://youtu.be/0kP3NhrRoSk",
                        "youtube_prerecorded_id": "0kP3NhrRoSk",
                        "bunny_prerecorded_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1161/v-full-1161_Presentation.mp4?token=6oVO0HErEi-15F6Exa7PmRj7mCUuTOsH9TxjqHqAD4s&expires=1706590800",
                        "bunny_prerecorded_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1161/v-full-1161_Presentation.vtt?token=63x3YvCGTfwiAiomJLIn6tJ5qDfj5q7AHDXppITiOsc&expires=1706590800"
                    },
                    {
                        "slot_id": "v-full-1431",
                        "session_id": "full12",
                        "title": "MeTACAST: Target- and Context-aware Spatial Selection in VR",
                        "contributors": [
                            "Lixiang Zhao"
                        ],
                        "authors": [
                            "Lixiang Zhao",
                            "Tobias Isenberg",
                            "Fuqi Xie",
                            "Hai-Ning Liang",
                            "Lingyun Yu"
                        ],
                        "abstract": "We propose three novel spatial data selection techniques for particle data in VR visualization environments. They are designed to be target- and context-aware and be suitable for a wide range of data features and complex scenarios. Each technique is designed to be adjusted to particular selection intents: the selection of consecutive dense regions, the selection of filament-like structures, and the selection of clusters\u2014with all of them facilitating post-selection threshold adjustment. These techniques allow users to precisely select those regions of space for further exploration\u2014with simple and approximate 3D pointing, brushing, or drawing input\u2014using flexible point- or path-based input and without being limited by 3D occlusions, non-homogeneous feature density, or complex data shapes. These new techniques are evaluated in a controlled experiment and compared with the Baseline method, a region-based 3D painting selection. Our results indicate that our techniques are effective in handling a wide range of scenarios and allow users to select data based on their comprehension of crucial features. Furthermore, we analyze the attributes, requirements, and strategies of our spatial selection methods and compare them with existing state-of-the-art selection methods to handle diverse data features and situations. Based on this analysis we provide guidelines for choosing the most suitable 3D spatial selection techniques based on the interaction environment, the given data characteristics, or the need for interactive post-selection threshold adjustment.",
                        "uid": "v-full-1431",
                        "time_stamp": "2023-10-25T03:36:00Z",
                        "time_start": "2023-10-25T03:36:00Z",
                        "time_end": "2023-10-25T03:48:00Z",
                        "paper_type": "full",
                        "keywords": [
                            "Spatial selection, immersive analytics, virtual reality (VR), target-aware and context-aware interaction for visualization"
                        ],
                        "doi": "10.1109/TVCG.2023.3326517",
                        "fno": "",
                        "has_image": true,
                        "has_pdf": true,
                        "paper_award": "",
                        "image_caption": "MeTACAST is a family of three target- and context-aware spatial selection techniques for 3D point cloud data in VR environments. Each technique is designed to be adjusted to particular selection intents: the selection of consecutive dense regions, the selection of filament-like structures, and the selection of clusters. These techniques allow users to precisely select those regions of space for further exploration---with simple and approximate 3D pointing, brushing, or drawing input---using flexible point- or path-based input and without being limited by 3D occlusions, non-homogeneous feature density, or complex data shapes.",
                        "external_paper_link": "",
                        "youtube_ff_link": "https://youtu.be/rPIlDZSqKs4",
                        "youtube_ff_id": "rPIlDZSqKs4",
                        "bunny_ff_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1431/v-full-1431_Preview.mp4?token=dJ5l8IydIF7caxnZOyhTcsaDAVE7tgEZqntMwdXc3jg&expires=1706590800",
                        "bunny_ff_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1431/v-full-1431_Preview.vtt?token=lGu0tNB3VwClY9TLSSXUVvb_dTra9hjtcvAlmRQ_sso&expires=1706590800",
                        "youtube_prerecorded_link": "https://youtu.be/cVDWVGbMTc8",
                        "youtube_prerecorded_id": "cVDWVGbMTc8",
                        "bunny_prerecorded_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1431/v-full-1431_Presentation.mp4?token=IunojNVWtwekUxhQxRRa3OxOJfgW_hWoJI5o-wv0AYo&expires=1706590800",
                        "bunny_prerecorded_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1431/v-full-1431_Presentation.vtt?token=X4lzJkofk6LzJ81kDTVBYkEeBKmpeh5vkDJ4CqMtOdM&expires=1706590800"
                    },
                    {
                        "slot_id": "v-full-1606",
                        "session_id": "full12",
                        "title": "Unraveling the Design Space of Immersive Analytics: A Systematic Review",
                        "contributors": [
                            "David Saffo"
                        ],
                        "authors": [
                            "David Saffo",
                            "Sara Di Bartolomeo",
                            "Tarik Crnovrsanin",
                            "Laura South",
                            "Justin Raynor",
                            "Caglar Yildirim",
                            "Cody Dunne"
                        ],
                        "abstract": "Immersive analytics has emerged as a promising research area, leveraging advances in immersive display technologies and techniques, such as virtual and augmented reality, to facilitate data exploration and decision-making. This paper presents a systematic literature review of 73 studies published between 2013-2022 on immersive analytics systems and visualizations, aiming to identify and categorize the primary dimensions influencing their design. We identified five key dimensions: Academic Theory and Contribution, Immersive Technology, Data, Spatial Presentation, and Visual Presentation. Academic Theory and Contribution assess the motivations behind the works and their theoretical frameworks. Immersive Technology examines the display and input modalities, while Data dimension focuses on dataset types and generation. Spatial Presentation discusses the environment, space, embodiment, and collaboration aspects in IA, and Visual Presentation explores the visual elements, facet and position, and manipulation of views. By examining each dimension individually and cross-referencing them, this review uncovers trends and relationships that help inform the design of immersive systems visualizations. This analysis provides valuable insights for researchers and practitioners, offering guidance in designing future immersive analytics systems and shaping the trajectory of this rapidly evolving field. A free copy of this paper and all supplemental materials are available at osf.io/5ewaj.",
                        "uid": "v-full-1606",
                        "time_stamp": "2023-10-25T03:48:00Z",
                        "time_start": "2023-10-25T03:48:00Z",
                        "time_end": "2023-10-25T04:00:00Z",
                        "paper_type": "full",
                        "keywords": [
                            "Immersive Analytics, Systematic Review, Survey, Augmented Reality, Virtual Reality, Design Space"
                        ],
                        "doi": "10.1109/TVCG.2023.3327368",
                        "fno": "",
                        "has_image": true,
                        "has_pdf": true,
                        "paper_award": "",
                        "image_caption": "Five represenative images inspired by works surveyed in our systematic review. Each image is accompanied by several of the design aspects that sum up their unique final design.",
                        "external_paper_link": "",
                        "youtube_ff_link": "https://youtu.be/AQzEyNdhwoo",
                        "youtube_ff_id": "AQzEyNdhwoo",
                        "bunny_ff_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1606/v-full-1606_Preview.mp4?token=wAzcL9-qvONEwvvoBt-AH1tDRo2Ya0mvbDIQ3TUxZ2I&expires=1706590800",
                        "bunny_ff_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1606/v-full-1606_Preview.vtt?token=_Zz8rJJ7cE9-TdPpgCgzhe8P04vEE5-j_GKZiMzn-uk&expires=1706590800",
                        "youtube_prerecorded_link": "https://youtu.be/BSLMlmUF-tk",
                        "youtube_prerecorded_id": "BSLMlmUF-tk",
                        "bunny_prerecorded_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1606/v-full-1606_Presentation.mp4?token=_SGcdm_w_c12B7CQiqUTCS5rYF0F4ZvK5OyJd4JsdsQ&expires=1706590800",
                        "bunny_prerecorded_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1606/v-full-1606_Presentation.vtt?token=O1lzaa-9FSzeoYKCW8eO4ndSURCcNuYPSN6h4OLzL8s&expires=1706590800"
                    },
                    {
                        "slot_id": "v-full-1492",
                        "session_id": "full12",
                        "title": "Wizualization: A \"Hard Magic\" Visualization System for Immersive and Ubiquitous Analytics",
                        "contributors": [
                            "Andrea Batch"
                        ],
                        "authors": [
                            "Andrea Batch",
                            "Peter William Scott Butcher",
                            "Panagiotis D. Ritsos",
                            "Niklas Elmqvist"
                        ],
                        "abstract": "What if magic could be used as an effective metaphor to perform data visualization and analysis using speech and gestures while mobile and on-the-go? In this paper, we introduce Wizualization, a visual analytics system for eXtended Reality (XR) that enables an analyst to author and interact with visualizations using such a magic system through gestures, speech commands, and touch interaction. Wizualization is a rendering system for current XR headsets that comprises several components: a cross-device (or Arcane Focuses) infrastructure for signalling and view control (Weave), a code notebook (Spellbook), and a grammar of graphics for XR (Optomancy). The system offers users three modes of input: gestures, spoken commands, and materials. We demonstrate Wizualization and its components using a motivating scenario on collaborative data analysis of pandemic data across time and space",
                        "uid": "v-full-1492",
                        "time_stamp": "2023-10-25T04:00:00Z",
                        "time_start": "2023-10-25T04:00:00Z",
                        "time_end": "2023-10-25T04:12:00Z",
                        "paper_type": "full",
                        "keywords": [
                            "immersive analytics, situated analytics, ubiquitous analytics, gestural interaction, voice interaction"
                        ],
                        "doi": "10.1109/TVCG.2023.3326580",
                        "fno": "",
                        "has_image": true,
                        "has_pdf": true,
                        "paper_award": "",
                        "image_caption": "Data scientists at the CDC use the Wizualization system in augmented reality for faster, more intuitive data visualization.",
                        "external_paper_link": "",
                        "youtube_ff_link": "https://youtu.be/trKRCtbFTUM",
                        "youtube_ff_id": "trKRCtbFTUM",
                        "bunny_ff_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1492/v-full-1492_Preview.mp4?token=kyP2zF4kYpcariTVgDJcQQTg3ZA_BZ4FzTY30h4XlSs&expires=1706590800",
                        "bunny_ff_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1492/v-full-1492_Preview.vtt?token=4zECS92TaeqcAWXQFVm9mZL_RlYZf1hkRBJ9hBrV8os&expires=1706590800",
                        "youtube_prerecorded_link": "https://youtu.be/Pv6glacsfgg",
                        "youtube_prerecorded_id": "Pv6glacsfgg",
                        "bunny_prerecorded_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1492/v-full-1492_Presentation.mp4?token=5D1OQRSVPccdyEZPAoxiAFo7yuHkw7Ie6B-JXaXd6vk&expires=1706590800",
                        "bunny_prerecorded_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1492/v-full-1492_Presentation.vtt?token=FQMbVOoeoHQjCV_6HYlUqUkDvrC0b8kn9pMIaJM40k4&expires=1706590800"
                    }
                ]
            },
            {
                "title": "Journalism & the Public",
                "session_id": "full13",
                "event_prefix": "v-full",
                "track": "oneohnine",
                "session_image": "full13.png",
                "chair": [
                    "Xavier Ho"
                ],
                "time_start": "2023-10-26T22:00:00Z",
                "time_end": "2023-10-26T23:15:00Z",
                "discord_category": "",
                "discord_channel": "109",
                "discord_channel_id": "1161741434647482379",
                "discord_link": "https://discordapp.com/channels/1161400451619627068/1161741434647482379",
                "zoom_private_meeting": "",
                "zoom_private_password": "",
                "zoom_private_link": "",
                "zoom_broadcast_link": "",
                "ff_link": "https://youtu.be/A1lqAXJCXfA",
                "time_slots": [
                    {
                        "slot_id": "v-full-1144",
                        "session_id": "full13",
                        "title": "From shock to shift: Data visualization for constructive climate journalism",
                        "contributors": [
                            "Francesca Morini"
                        ],
                        "authors": [
                            "Francesca Morini",
                            "Johanna Hartmann",
                            "Anna Eschenbacher",
                            "Marian D\u00f6rk"
                        ],
                        "abstract": "We present a multi-dimensional, multi-level, and multi-channel approach to data visualization for the purpose of constructive climate journalism. Data visualization has assumed a central role in environmental journalism and is often used in data stories to convey the dramatic consequences of climate change and other ecological crises. However, the emphasis on the catastrophic impacts of climate change tends to induce feelings of fear, anxiety, and apathy in readers. Climate mitigation, adaptation, and protection \u2013 all highly urgent in the face of the climate crisis \u2013 are at risk of being overlooked. These topics are more difficult to communicate as they are hard to convey on varying levels of locality, involve multiple interconnected sectors, and need to be mediated across various channels from the printed newspaper to social media platforms. So far, there has been little research on data visualization to enhance affective engagement with data about climate protection as part of solution-oriented reporting of climate change. With this research we characterize the unique challenges of constructive climate journalism for data visualization and share findings from a research and design study in collaboration with a national newspaper in Germany. Using the affordances and aesthetics of travel postcards, we present Klimakarten, a data journalism project on the progress of climate protection at multiple spatial scales (from national to local), across five key sectors (agriculture, buildings, energy, mobility, and waste), and for print and online use. The findings from quantitative and qualitative analysis of reader feedback confirm our overall approach and suggest implications for future work.",
                        "uid": "v-full-1144",
                        "time_stamp": "2023-10-26T22:00:00Z",
                        "time_start": "2023-10-26T22:00:00Z",
                        "time_end": "2023-10-26T22:12:00Z",
                        "paper_type": "full",
                        "keywords": [
                            "Constructive Climate Journalism, Frameworks, Storytelling, Journalism"
                        ],
                        "doi": "10.1109/TVCG.2023.3327185",
                        "fno": "",
                        "has_image": true,
                        "has_pdf": true,
                        "paper_award": "",
                        "image_caption": "Four postcards are laid on top of a newspaper page. The postcards show visualizations about climate protection.",
                        "external_paper_link": "",
                        "youtube_ff_link": "https://youtu.be/Q_G6d5enbZI",
                        "youtube_ff_id": "Q_G6d5enbZI",
                        "bunny_ff_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1144/v-full-1144_Preview.mp4?token=daKomu-REozpa2hZ5LFEi61zt79-F6nt-cqGYsg0tGU&expires=1706590800",
                        "bunny_ff_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1144/v-full-1144_Preview.vtt?token=F3Dw8dZF3fSCH632O9ToQrI-r6ku2QG5kmQXUU6OqhU&expires=1706590800",
                        "youtube_prerecorded_link": "https://youtu.be/35q-7hUec0Y",
                        "youtube_prerecorded_id": "35q-7hUec0Y",
                        "bunny_prerecorded_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1144/v-full-1144_Presentation.mp4?token=5ICkYpSf01FxhFa_p_lBAYXIHt91gDAetdg4B4K9Un8&expires=1706590800",
                        "bunny_prerecorded_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1144/v-full-1144_Presentation.vtt?token=I7InnxbmBxp6CLknvUtQ1lQWxhWkjvHDyufrFOUtSTI&expires=1706590800"
                    },
                    {
                        "slot_id": "v-tvcg-10130316",
                        "session_id": "full13",
                        "title": "Towards Visualization Thumbnail Designs that Entice Reading Data-driven Articles",
                        "contributors": [
                            "Hwiyeon Kim"
                        ],
                        "authors": [
                            "Hwiyeon Kim",
                            "Joohee Kim",
                            "Yunha Han",
                            "Hwajung Hong",
                            "Oh-Sang Kwon",
                            "Young-Woo Park",
                            "Niklas Elmqvist",
                            "Sungahn Ko",
                            "Bum Chul Kwon"
                        ],
                        "abstract": "As online news increasingly include data journalism, there is a corresponding increase in the incorporation of visualization in article thumbnail images. However, little research exists on the design rationale for visualization thumbnails, such as resizing, cropping, simplifying, and embellishing charts that appear within the body of the associated article. Therefore, in this paper we aim to understand these design choices and determine what makes a visualization thumbnail inviting and interpretable. To this end, we first survey visualization thumbnails collected online and discuss visualization thumbnail practices with data journalists and news graphics designers. Based on the survey and discussion results, we then define a design space for visualization thumbnails and conduct a user study with four types of visualization thumbnails derived from the design space. The study results indicate that different chart components play different roles in attracting reader attention and enhancing reader understandability of the visualization thumbnails. We also find various thumbnail design strategies for effectively combining the charts' components, such as a data summary with highlights and data labels, and a visual legend with text labels and Human Recognizable Objects (HROs), into thumbnails. Ultimately, we distill our findings into design implications that allow effective visualization thumbnail designs for data-rich news articles. Our work can thus be seen as a first step toward providing structured guidance on how to design compelling thumbnails for data stories.",
                        "uid": "v-tvcg-10130316",
                        "time_stamp": "2023-10-26T22:12:00Z",
                        "time_start": "2023-10-26T22:12:00Z",
                        "time_end": "2023-10-26T22:24:00Z",
                        "paper_type": "full",
                        "keywords": [
                            "Data journalism;data stories;data-driven storytelling;online news;thumbnail images;visualization"
                        ],
                        "doi": "10.1109/TVCG.2023.3278304",
                        "fno": "10130316",
                        "has_image": true,
                        "has_pdf": false,
                        "paper_award": "",
                        "image_caption": "This study is about design choices of visualization thumbnail for news articles.",
                        "external_paper_link": "",
                        "youtube_ff_link": "https://youtu.be/fC93yY3r2Js",
                        "youtube_ff_id": "fC93yY3r2Js",
                        "bunny_ff_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-tvcg/v-tvcg-10130316/v-tvcg-10130316_Preview.mp4?token=kwQMZGPMyqOigk_POoh6j-nrA_ZSe7-8vZ6IINFDfDM&expires=1706590800",
                        "bunny_ff_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-tvcg/v-tvcg-10130316/v-tvcg-10130316_Preview.vtt?token=JLRUYMU7JkShQ1gYCW_LeupWLC5C2jzLiMaKMYaJhvk&expires=1706590800",
                        "youtube_prerecorded_link": "https://youtu.be/0zHERZNI6NY",
                        "youtube_prerecorded_id": "0zHERZNI6NY",
                        "bunny_prerecorded_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-tvcg/v-tvcg-10130316/v-tvcg-10130316_Presentation.mp4?token=qunw6NdKy8mvO2P235WZzST7Qajy6Hc0SA8LZ_g7sdk&expires=1706590800",
                        "bunny_prerecorded_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-tvcg/v-tvcg-10130316/v-tvcg-10130316_Presentation.vtt?token=phQmniXQywppiEIJumc8mTF1vKa8_qP6xt5KiKm0r1M&expires=1706590800"
                    },
                    {
                        "slot_id": "v-full-1383",
                        "session_id": "full13",
                        "title": "Embellishments Revisited: Perceptions of Embellished Visualisations Through the Viewer's Lens",
                        "contributors": [
                            "Muna Alebri"
                        ],
                        "authors": [
                            "Muna Alebri",
                            "Enrico Costanza",
                            "Georgia Panagiotidou",
                            "Duncan P Brumby"
                        ],
                        "abstract": "Embellishments are features commonly used in everyday visualisations which are demonstrated to enhance assimilation and memorability. Despite their popularity, little is known about their impact on enticing readers to explore visualisations. To address this gap, we conducted 18 interviews with a diverse group of participants who were consumers of news media but non-experts in visualisation and design. Participants were shown ten embellished and plain visualisations collected from the news and asked to rank them based on enticement and ease of understanding. Extending prior work, our interview results suggest that visualisations with multiple embellishment types might make a visualisation perceived as more enticing. An important finding from our study is that the widespread of certain embellishments in the media might have made them part of visualisation conventions, making a visualisation appear more objective but less enticing. Based on these findings, we ran a follow-up online user study showing participants variations of the visualisations with multiple embellishments to isolate each embellishment type and investigate its effect. We found that variations with salient embellishments were perceived as more enticing. We argue that to unpack the concept of embellishments; we must consider two factors: embellishment saliency and editorial styles. Our study contributes concept and design considerations to the literature concerned with visualisation design for non-experts in visualisation and design.",
                        "uid": "v-full-1383",
                        "time_stamp": "2023-10-26T22:24:00Z",
                        "time_start": "2023-10-26T22:24:00Z",
                        "time_end": "2023-10-26T22:36:00Z",
                        "paper_type": "full",
                        "keywords": [
                            "Visualizations, Embellishments, Non-experts in visualisation and design"
                        ],
                        "doi": "10.1109/TVCG.2023.3326914",
                        "fno": "",
                        "has_image": true,
                        "has_pdf": true,
                        "paper_award": "",
                        "image_caption": "Revisiting Embellishments: bar charts with salient icons and arrows instead of bars. Small icon labels in charts that have a semantic meaning to the labels. Icon arrays and background images in charts.",
                        "external_paper_link": "",
                        "youtube_ff_link": "https://youtu.be/-DSGK_yrsL0",
                        "youtube_ff_id": "-DSGK_yrsL0",
                        "bunny_ff_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1383/v-full-1383_Preview.mp4?token=XKyEAbIALTfoUJ-ZoCdBR4Of4RdMSNVkcCA2x15shAE&expires=1706590800",
                        "bunny_ff_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1383/v-full-1383_Preview.vtt?token=-T_Iyh2Sb-TdQtlJL68t7bbnD9-r1yOpF4Q6SFs38cA&expires=1706590800",
                        "youtube_prerecorded_link": "https://youtu.be/zKUENG1MSzA",
                        "youtube_prerecorded_id": "zKUENG1MSzA",
                        "bunny_prerecorded_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1383/v-full-1383_Presentation.mp4?token=jBD0PDIz0wn5-pzmctNEqM9QHN-G9-yR5ohztDk3b9c&expires=1706590800",
                        "bunny_prerecorded_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1383/v-full-1383_Presentation.vtt?token=92-2btDSKfbSrTVPkZzr3cF4PR7wT4j4yck1b3Dse5U&expires=1706590800"
                    },
                    {
                        "slot_id": "v-full-1254",
                        "session_id": "full13",
                        "title": "Enthusiastic and Grounded, Avoidant and Cautious: Understanding Public Receptivity to Data and Visualizations",
                        "contributors": [
                            "Wesley Willett"
                        ],
                        "authors": [
                            "Helen Ai He",
                            "Jagoda Walny",
                            "Sonja Thoma",
                            "Sheelagh Carpendale",
                            "Wesley Willett"
                        ],
                        "abstract": "Despite an abundance of open data initiatives aimed to inform and empower \u201cgeneral\u201d audiences, we still know little about the ways people outside of traditional data analysis communities experience and engage with public data and visualizations. To investigate this gap, we present results from an in-depth qualitative interview study with 19 participants from diverse ethnic, occupational, and demographic backgrounds. Our findings characterize a set of lived experiences with open data and visualizations in the domain of energy consumption, production, and transmission. This work exposes information receptivity \u2014 an individual\u2019s transient state of willingness or openness to receive information \u2014 as a blind spot for the data visualization community, complementary to but distinct from previous notions of data visualization literacy and engagement. We observed four clusters of receptivity responses to data-and visualization-based rhetoric: Information-Avoidant, Data-Cautious, Data-Enthusiastic, and Domain-Grounded. Based on our findings, we highlight research opportunities for the visualization community. This exploratory work identifies the existence of diverse receptivity responses, highlighting the need to consider audiences with varying levels of openness to new information.",
                        "uid": "v-full-1254",
                        "time_stamp": "2023-10-26T22:36:00Z",
                        "time_start": "2023-10-26T22:36:00Z",
                        "time_end": "2023-10-26T22:48:00Z",
                        "paper_type": "full",
                        "keywords": [
                            "Diverse audiences, Information receptivity, Information visualization, Open data"
                        ],
                        "doi": "10.1109/TVCG.2023.3326917",
                        "fno": "",
                        "has_image": true,
                        "has_pdf": true,
                        "paper_award": "honorable",
                        "image_caption": "A 2D Information Receptivity space, which characterizes interview participants based on their receptivity to open energy information when presented as data and as interpretation. The space shows four clusters: Data-Cautious (receptive to interpretation but not data), Data-Enthusiastic (receptive to interpretation and data), Domain-Grounded (receptive to data but not interpretation), and Information-Avoidant (not receptive to data or interpretation).",
                        "external_paper_link": "",
                        "youtube_ff_link": "https://youtu.be/fwOwzoapPMw",
                        "youtube_ff_id": "fwOwzoapPMw",
                        "bunny_ff_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1254/v-full-1254_Preview.mp4?token=zzDfvYFFnA89cMKavPt_ca9UsOeQ2LWDINUirhSLMHo&expires=1706590800",
                        "bunny_ff_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1254/v-full-1254_Preview.vtt?token=KiznLYnRFAQ-zEBtXmD2UhyQiJbhhUiVIsNndLRIJuY&expires=1706590800",
                        "youtube_prerecorded_link": "https://youtu.be/QvGw-9SIL54",
                        "youtube_prerecorded_id": "QvGw-9SIL54",
                        "bunny_prerecorded_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1254/v-full-1254_Presentation.mp4?token=j6RCr5f84dZwcdag89ovjafKU-VItqfZRO3WIgJfXyY&expires=1706590800",
                        "bunny_prerecorded_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1254/v-full-1254_Presentation.vtt?token=OF_lpwgc_2doJ1oafbz7SlS4NGU26airc3c2mmDlTOw&expires=1706590800"
                    },
                    {
                        "slot_id": "v-full-1513",
                        "session_id": "full13",
                        "title": "Polarizing Political Polls: Visualization Design Choices Can Shape Public Opinion and Increase Political Polarization",
                        "contributors": [
                            "Eli Holder"
                        ],
                        "authors": [
                            "Eli Holder",
                            "Cindy Xiong Bearfield"
                        ],
                        "abstract": "While we typically focus on data visualization as a tool for facilitating cognitive tasks (e.g.\\ learning facts, making decisions), we know relatively little about their second-order impacts on our opinions, attitudes, and values. For example, could design or framing choices interact with viewers' social cognitive biases in ways that promote political polarization? When reporting on U.S. attitudes toward public policies, it is popular to highlight the gap between Democrats and Republicans (e.g.\\ with blue vs red connected dot plots). But these charts may encourage social-normative conformity, influencing viewers' attitudes to match the divided opinions shown in the visualization. We conducted three experiments examining visualization framing in the context of social conformity and polarization. Crowdworkers viewed charts showing simulated polling results for public policy proposals. We varied framing (aggregating data as non-partisan \u201cAll US Adults,\u201d or partisan \u201cDemocrat\u201d / \u201cRepublican\u201d) and the visualized groups' support levels. Participants then reported their own support for each policy. We found that participants' attitudes biased significantly toward the group attitudes shown in the stimuli and this can increase inter-party attitude divergence.These results demonstrate that data visualizations can induce social conformity and accelerate political polarization. Choosing to visualize partisan divisions can divide us further.",
                        "uid": "v-full-1513",
                        "time_stamp": "2023-10-26T22:48:00Z",
                        "time_start": "2023-10-26T22:48:00Z",
                        "time_end": "2023-10-26T23:00:00Z",
                        "paper_type": "full",
                        "keywords": [
                            "Political Polarization, Public Opinion, Social Categorization, Survey Data, Social Influence, Attitude Change"
                        ],
                        "doi": "10.1109/TVCG.2023.3326512",
                        "fno": "",
                        "has_image": true,
                        "has_pdf": true,
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "youtube_ff_link": "https://youtu.be/RmrWjeji25Y",
                        "youtube_ff_id": "RmrWjeji25Y",
                        "bunny_ff_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1513/v-full-1513_Preview.mp4?token=NrJUq7m6Xv2DAWV__jCG2BJVQOrkF0Qk5JwAN737S-w&expires=1706590800",
                        "bunny_ff_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1513/v-full-1513_Preview.vtt?token=26npaP-gZxWQhK2lKjobJNCrQb3GKvnQkUwPrTmySM4&expires=1706590800",
                        "youtube_prerecorded_link": "https://youtu.be/_Ux-vx35Iu0",
                        "youtube_prerecorded_id": "_Ux-vx35Iu0",
                        "bunny_prerecorded_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1513/v-full-1513_Presentation.mp4?token=0_FcLwgN0RF7ahg2ELpyH_iQthDzYXPOrppbZudnEnM&expires=1706590800",
                        "bunny_prerecorded_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1513/v-full-1513_Presentation.vtt?token=uMB7_5YpGEBcEJpi4D4t6bDBQjTJH5NDrwizXRbhnoY&expires=1706590800"
                    }
                ]
            },
            {
                "title": "Layout Algorithms (Full+Short)",
                "session_id": "full14",
                "event_prefix": "v-full",
                "track": "oneohfive",
                "session_image": "full14.png",
                "chair": [
                    "Helen Purchase"
                ],
                "time_start": "2023-10-24T23:45:00Z",
                "time_end": "2023-10-25T01:00:00Z",
                "discord_category": "",
                "discord_channel": "105",
                "discord_channel_id": "1161741309346861067",
                "discord_link": "https://discordapp.com/channels/1161400451619627068/1161741309346861067",
                "zoom_private_meeting": "",
                "zoom_private_password": "",
                "zoom_private_link": "",
                "zoom_broadcast_link": "",
                "ff_link": "https://youtu.be/xEAEsqyKvrs",
                "time_slots": [
                    {
                        "slot_id": "v-tvcg-10122175",
                        "session_id": "full14",
                        "title": "A Scalable Method for Readable Tree Layouts",
                        "contributors": [
                            "Reyan Ahmed"
                        ],
                        "authors": [
                            "Kathryn Gray",
                            "Mingwei Li",
                            "Reyan Ahmed",
                            "Md. Khaledur Rahman",
                            "Ariful Azad",
                            "Stephen Kobourov",
                            "Katy B\u00f6rner"
                        ],
                        "abstract": "Large tree structures are ubiquitous and real-world relational datasets often have information associated with nodes (e.g., labels or other attributes) and edges (e.g., weights or distances) that need to be communicated to the viewers. Yet, scalable, easy to read tree layouts are difficult to achieve. We consider tree layouts to be readable if they meet some basic requirements: node labels should not overlap, edges should not cross, edge lengths should be preserved, and the output should be compact. There are many algorithms for drawing trees, although very few take node labels or edge lengths into account, and none optimizes all requirements above. With this in mind, we propose a new scalable method for readable tree layouts. The algorithm guarantees that the layout has no edge crossings and no label overlaps, and optimizing one of the remaining aspects: desired edge lengths and compactness. We evaluate the performance of the new algorithm by comparison with related earlier approaches using several real-world datasets, ranging from a few thousand nodes to hundreds of thousands of nodes. Tree layout algorithms can be used to visualize large general graphs, by extracting a hierarchy of progressively larger trees. We illustrate this functionality by presenting several map-like visualizations generated by the new tree layout algorithm.",
                        "uid": "v-tvcg-10122175",
                        "time_stamp": "2023-10-24T23:45:00Z",
                        "time_start": "2023-10-24T23:45:00Z",
                        "time_end": "2023-10-24T23:57:00Z",
                        "paper_type": "full",
                        "keywords": [
                            "Tree layouts;force-directed;readability"
                        ],
                        "doi": "10.1109/TVCG.2023.3274572",
                        "fno": "10122175",
                        "has_image": true,
                        "has_pdf": false,
                        "paper_award": "",
                        "image_caption": "Overview of the readable tree (RT) method. The input is a node-labeled tree with pre-specified edge lengths from which a multi-level Steiner tree (MLST) is computed. (1) RT initializes with a crossing-free layout, with options targeting compactness or edge length preservation.  (2) A force-directed improvement removes label overlaps, preserves desired edge lengths, and minimizes the area. (3) Remaining label overlaps are removed through resizing and position fine tuning. Note that we have two options for prioritizing either compactness or edge length preservation, matching the corresponding initialization. The tree layout together with the MLST drive a Map-like visualizations with semantic zooming.",
                        "external_paper_link": "",
                        "youtube_ff_link": "https://youtu.be/DpY_LMaLH4s",
                        "youtube_ff_id": "DpY_LMaLH4s",
                        "bunny_ff_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-tvcg/v-tvcg-10122175/v-tvcg-10122175_Preview.mp4?token=2_qTT10ym9TH1TkQ84lCAhf9Mke1piRXjpwaZCBNRYc&expires=1706590800",
                        "bunny_ff_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-tvcg/v-tvcg-10122175/v-tvcg-10122175_Preview.vtt?token=CLHLkBMEuwJGisLPEXmeYW3PZ5T3AO6zpLUISFZe0Ng&expires=1706590800",
                        "youtube_prerecorded_link": "https://youtu.be/puRshwHG7Z0",
                        "youtube_prerecorded_id": "puRshwHG7Z0",
                        "bunny_prerecorded_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-tvcg/v-tvcg-10122175/v-tvcg-10122175_Presentation.mp4?token=idTltTJHwOi-2hC02fVosmaRd61mR1W804fcJu6lHMY&expires=1706590800",
                        "bunny_prerecorded_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-tvcg/v-tvcg-10122175/v-tvcg-10122175_Presentation.vtt?token=CELGqaLZtmzUVa_GixFLosIWDOKDN9XRfCB725vTcvQ&expires=1706590800"
                    },
                    {
                        "slot_id": "v-tvcg-10024360",
                        "session_id": "full14",
                        "title": "Force-directed graph layouts revisited: a new force based on the t-Distribution",
                        "contributors": [
                            "Mingliang Xue"
                        ],
                        "authors": [
                            "Fahai Zhong",
                            "Mingliang Xue",
                            "Jian Zhang",
                            "Rui Ban",
                            "Oliver Deussen",
                            "Yunhai Wang"
                        ],
                        "abstract": "In this paper, we propose the t-FDP model, a force-directed placement method based on a novel bounded short-range force (t-force) defined by Student\u2019s t-distribution. Our formulation is flexible, exerts limited repulsive forces for nearby nodes and can be adapted separately in its short- and long-range effects. Using such forces in force-directed graph layouts yields better neighborhood preservation than current methods, while maintaining low stress errors. Our efficient implementation using a Fast Fourier Transform is one order of magnitude faster than state-of-the-art methods and two orders faster on the GPU, enabling us to perform parameter tuning by globally and locally adjusting the t-force in real-time even for complex graphs of up to 10.000 nodes. We demonstrate the quality of our approach by numerical evaluation against state-of-the-art approaches and extensions for interactive exploration.",
                        "uid": "v-tvcg-10024360",
                        "time_stamp": "2023-10-24T23:57:00Z",
                        "time_start": "2023-10-24T23:57:00Z",
                        "time_end": "2023-10-25T00:09:00Z",
                        "paper_type": "full",
                        "keywords": [
                            "FFT;force directed placement;graph layout;student\u201a\u00c4\u00f4s t-distribution"
                        ],
                        "doi": "10.1109/TVCG.2023.3238821",
                        "fno": "10024360",
                        "has_image": true,
                        "has_pdf": false,
                        "paper_award": "",
                        "image_caption": "In this paper, we propose the t-FDP model, a force-directed placement method based on a bounded short-range force (t-force) defined by Student's t-distribution. Our formulation exerts limited repulsive forces for nearby nodes and can be adapted separately in its short- and long-range effects. (A) Using such forces yields better neighborhood preservation than current methods. (B) Our efficient implementation using a Fast Fourier Transform is one order of magnitude faster than state-of-the-art methods and two orders faster on the GPU (C), enabling us to perform parameter tuning by globally and locally adjusting the t-force in real-time for complex graphs. (D)",
                        "external_paper_link": "",
                        "youtube_ff_link": "https://youtu.be/wO5egj8Prnw",
                        "youtube_ff_id": "wO5egj8Prnw",
                        "bunny_ff_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-tvcg/v-tvcg-10024360/v-tvcg-10024360_Preview.mp4?token=z_bHaMt4_dz62StR5A4gAKMhzNH3-Pr4U2XqRkQ7xPU&expires=1706590800",
                        "bunny_ff_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-tvcg/v-tvcg-10024360/v-tvcg-10024360_Preview.vtt?token=fmZYHxSeXspd2cSUuHrkYRAX3HIcIaQew7s1LGp8IbA&expires=1706590800",
                        "youtube_prerecorded_link": "https://youtu.be/SdkEYDI5YEc",
                        "youtube_prerecorded_id": "SdkEYDI5YEc",
                        "bunny_prerecorded_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-tvcg/v-tvcg-10024360/v-tvcg-10024360_Presentation.mp4?token=v-XmeMlBahcTIo_89fJ3W3JljqtogestJYa1TMRGsk4&expires=1706590800",
                        "bunny_prerecorded_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-tvcg/v-tvcg-10024360/v-tvcg-10024360_Presentation.vtt?token=wfi5C8Bpavv52wduhkMcKSDitiEvNGQq90YClC7bqmE&expires=1706590800"
                    },
                    {
                        "slot_id": "v-tvcg-9814874",
                        "session_id": "full14",
                        "title": "Target Netgrams: An Annulus-constrained Stress Model for Radial Graph Visualization",
                        "contributors": [
                            "Mingliang Xue"
                        ],
                        "authors": [
                            "Mingliang Xue",
                            "Yunhai Wang",
                            "Chang Han",
                            "Jian Zhang",
                            "Zheng Wang",
                            "Kaiyi Zhang",
                            "Christophe Hurter",
                            "Jian Zhao",
                            "Oliver Deussen"
                        ],
                        "abstract": "We present Target Netgrams as a visualization technique for radial layouts of graphs. Inspired by manually created target sociograms, we propose an annulus-constrained stress model that aims to position nodes onto the annuli between adjacent circles for indicating their radial hierarchy, while maintaining the network structure (clusters and neighborhoods) and improving readability as much as possible. This is achieved by having more space on the annuli than traditional layout techniques. By adapting stress majorization to this model, the layout is computed as a constrained least square optimization problem. Additional constraints (e.g., parent-child preservation, attribute-based clusters and structure-aware radii) are provided for exploring nodes, edges, and levels of interest. We demonstrate the effectiveness of our method through a comprehensive evaluation, a user study, and a case study.",
                        "uid": "v-tvcg-9814874",
                        "time_stamp": "2023-10-25T00:09:00Z",
                        "time_start": "2023-10-25T00:09:00Z",
                        "time_end": "2023-10-25T00:21:00Z",
                        "paper_type": "full",
                        "keywords": [
                            "Radial visualization;stress model;hierarchy constraint;graph"
                        ],
                        "doi": "10.1109/TVCG.2022.3187425",
                        "fno": "9814874",
                        "has_image": true,
                        "has_pdf": false,
                        "paper_award": "",
                        "image_caption": "In this paper, we present Target Netgrams, a visualization technique for radial graph layouts. We propose an annulus-constrained stress model to position nodes on the annuli between adjacent circles, indicating their radial hierarchy while maintaining network structure and improving readability. (A) This is achieved by providing more space on the annuli compared to traditional layouts, computed through a constrained least square optimization problem using stress majorization. (B) Additional constraints facilitate exploration of nodes, edges, and levels of interest. (C) We demonstrate the effectiveness of our method through comprehensive evaluation, a user study, and a case study. (D)",
                        "external_paper_link": "",
                        "youtube_ff_link": "https://youtu.be/ItWytv-QIiY",
                        "youtube_ff_id": "ItWytv-QIiY",
                        "bunny_ff_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-tvcg/v-tvcg-9814874/v-tvcg-9814874_Preview.mp4?token=_haHkOU8I2eVwRaT7x6GId5Amgz9Ih3gqlbTWG9VDRg&expires=1706590800",
                        "bunny_ff_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-tvcg/v-tvcg-9814874/v-tvcg-9814874_Preview.vtt?token=gYGZQNUxkqeZL0PIxDxaFJ3etkCfQ6EdD68B8Alrzj4&expires=1706590800",
                        "youtube_prerecorded_link": "https://youtu.be/b8qdAK3T9P0",
                        "youtube_prerecorded_id": "b8qdAK3T9P0",
                        "bunny_prerecorded_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-tvcg/v-tvcg-9814874/v-tvcg-9814874_Presentation.mp4?token=NP72nFPiEkAGkH6a7R4gwzGc-BGKT58gpLTDfYiYIE4&expires=1706590800",
                        "bunny_prerecorded_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-tvcg/v-tvcg-9814874/v-tvcg-9814874_Presentation.vtt?token=VlXZJueYXxvfzoCxvUyaieB_ibBTpRTqs4JulLP4l-k&expires=1706590800"
                    },
                    {
                        "slot_id": "v-full-1377",
                        "session_id": "full14",
                        "title": "Cluster-Aware Grid Layout",
                        "contributors": [
                            "Weikai Yang"
                        ],
                        "authors": [
                            "Yuxing Zhou",
                            "Weikai Yang",
                            "Jiashu Chen",
                            "Changjian Chen",
                            "Zhiyang Shen",
                            "Xiaonan Luo",
                            "Lingyun Yu",
                            "Shixia Liu"
                        ],
                        "abstract": "Grid visualizations are widely used in many applications to visually explain a set of data and their proximity relationships. However, existing layout methods face difficulties when dealing with the inherent cluster structures within the data. To address this issue, we propose a cluster-aware grid layout method that aims to better preserve cluster structures by simultaneously considering proximity, compactness, and convexity in the optimization process. Our method utilizes a hybrid optimization strategy that consists of two phases. The global phase aims to balance proximity and compactness within each cluster, while the local phase ensures the convexity of cluster shapes. We evaluate the proposed grid layout method through a series of quantitative experiments and two use cases, demonstrating its effectiveness in preserving cluster structures and facilitating analysis tasks.",
                        "uid": "v-full-1377",
                        "time_stamp": "2023-10-25T00:21:00Z",
                        "time_start": "2023-10-25T00:21:00Z",
                        "time_end": "2023-10-25T00:33:00Z",
                        "paper_type": "full",
                        "keywords": [
                            "Grid layout, similarity, convexity, compactness, optimization"
                        ],
                        "doi": "10.1109/TVCG.2023.3326934",
                        "fno": "",
                        "has_image": true,
                        "has_pdf": true,
                        "paper_award": "",
                        "image_caption": "Grid visualizations are widely used in many applications to visually explain a set of data and their proximity relationships. However, existing layout methods face difficulties when dealing with the inherent cluster structures within the data. To address this issue, we propose a cluster-aware grid layout method that aims to better preserve cluster structures by simultaneously considering proximity, compactness, and convexity in the optimization process. Our method utilizes a hybrid optimization strategy that consists of two phases. The global phase aims to balance proximity and compactness within each cluster, while the local phase ensures the convexity of cluster shapes. We evaluate the proposed grid layout method through a series of quantitative experiments and two use cases, demonstrating its effectiveness in preserving cluster structures and facilitating analysis tasks.",
                        "external_paper_link": "",
                        "youtube_ff_link": "https://youtu.be/UTG1UIbfOVc",
                        "youtube_ff_id": "UTG1UIbfOVc",
                        "bunny_ff_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1377/v-full-1377_Preview.mp4?token=_E1vW8orXMGWfxieXKhL8cE5ppQzD-r-Jl-fEp_7JnM&expires=1706590800",
                        "bunny_ff_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1377/v-full-1377_Preview.vtt?token=LniG4-1vDmRs10fa0X0u5W-cm_tuDZwaFJV91QOmT5c&expires=1706590800",
                        "youtube_prerecorded_link": "https://youtu.be/j3RC42_HyeU",
                        "youtube_prerecorded_id": "j3RC42_HyeU",
                        "bunny_prerecorded_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1377/v-full-1377_Presentation.mp4?token=X7EZemZ14KGPLqCx9VbAQEFISWRGBzzL8L337bTIJY4&expires=1706590800",
                        "bunny_prerecorded_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1377/v-full-1377_Presentation.vtt?token=tsScnQSp11gsVCPIKLqNxLUisHcsVSYeeMN8I0Vrb9s&expires=1706590800"
                    },
                    {
                        "slot_id": "v-full-1128",
                        "session_id": "full14",
                        "title": "Radial Icicle Tree (RIT): Node Separation and Area Constancy",
                        "contributors": [
                            "Yuanzhe Jin"
                        ],
                        "authors": [
                            "Yuanzhe Jin",
                            "Tim de Jong",
                            "Martijn Tennekes",
                            "Min Chen"
                        ],
                        "abstract": "Icicles and sunbursts are two commonly-used visual representations of trees. While icicle trees can map data values faithfully to rectangles of different sizes, often some rectangles are too narrow to be noticed easily. When an icicle tree is transformed into a sunburst tree, the width of each rectangle becomes the length of an annular sector that is usually longer than the original width. While sunburst trees alleviate the problem of narrow rectangles in icicle trees, it no longer maintains the consistency of size encoding. At different tree depths, nodes of the same data values are displayed in annular sections of different sizes in a sunburst tree, though they are represented by rectangles of the same size in an icicle tree. Furthermore, two nodes from different subtrees could sometimes appear as a single node in both icicle trees and sunburst trees. In this paper, we propose a new visual representation, referred to as radial icicle tree (RIT), which transforms the rectangular bounding box of an icicle tree into a circle, circular sector, or annular sector while introducing gaps between nodes and maintaining area constancy for nodes of the same size. We applied the new visual design to several datasets. Both the analytical design process and user-centered evaluation have confirmed that this new design has improved the design of icicles and sunburst trees without introducing any relative demerit.",
                        "uid": "v-full-1128",
                        "time_stamp": "2023-10-25T00:33:00Z",
                        "time_start": "2023-10-25T00:33:00Z",
                        "time_end": "2023-10-25T00:45:00Z",
                        "paper_type": "full",
                        "keywords": [
                            "Tree visualization, icicle tree, sunburst tree, size encoding, area constancy, node separation, radial icicle tree, RIT"
                        ],
                        "doi": "10.1109/TVCG.2023.3327178",
                        "fno": "",
                        "has_image": true,
                        "has_pdf": true,
                        "paper_award": "",
                        "image_caption": "The Radial Icicle Tree (RIT) elucidates the connections and distinctions between various categories within the dataset. Through the implementation of RIT, we visualized data from the CBS dataset, which was aggregated from wearable devices adorned by volunteers in a laboratory setting. RIT plots were utilized for eight categories of human physical activity, unveiling inter-category relationships. This underscores one of the key benefits of RIT - the ability to swiftly and lucidly reveal intrinsic data relationships when structural connections are present. It also facilitates simple comparisons between diverse plot categories concurrently, enabling users to uncover intriguing relations among them.",
                        "external_paper_link": "",
                        "youtube_ff_link": "https://youtu.be/hBAaiCz18EI",
                        "youtube_ff_id": "hBAaiCz18EI",
                        "bunny_ff_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1128/v-full-1128_Preview.mp4?token=GVlEEHnSkLRfg4TVYHSpVl_chlOpjur3ohmAv3QDVZc&expires=1706590800",
                        "bunny_ff_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1128/v-full-1128_Preview.vtt?token=gVs3P2T1PgHdm7NV8s7o7YU6CXiuQW8ucQ56-cppiX0&expires=1706590800",
                        "youtube_prerecorded_link": "https://youtu.be/Qg6gOYXhW_Q",
                        "youtube_prerecorded_id": "Qg6gOYXhW_Q",
                        "bunny_prerecorded_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1128/v-full-1128_Presentation.mp4?token=N5PqGO6koh5CV9oNo77mBvXqlv8KRV4E_3pU8IU67lc&expires=1706590800",
                        "bunny_prerecorded_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1128/v-full-1128_Presentation.vtt?token=MvAUZ8m0DDjKEiFlTuctvKC1xhcsEQlql2af30rNr_Y&expires=1706590800"
                    },
                    {
                        "slot_id": "v-short-1051",
                        "session_id": "full14",
                        "title": "Projection Ensemble: Visualizing the Robust Structures of Multidimensional Projections",
                        "contributors": [
                            "Myeongwon Jung"
                        ],
                        "authors": [
                            "Myeongwon Jung",
                            "Jiwon Choi",
                            "Jaemin Jo"
                        ],
                        "abstract": "We introduce Projection Ensemble, a novel approach for identifying and visualizing robust structures across multidimensional projec- tions. Although multidimensional projections, such as t-Stochastic Neighbor Embedding (t-SNE), have gained popularity, their stochastic nature often leads the user to interpret the structures that arise by chance and make erroneous findings. To overcome this limitation, we present a frequent subgraph mining algorithm and a visualization interface to extract and visualize the consistent structures across multiple projections. We demonstrate that our system not only identifies trustworthy structures but also detects accidental clustering or separation of data points.",
                        "uid": "v-short-1051",
                        "time_stamp": "2023-10-25T00:45:00Z",
                        "time_start": "2023-10-25T00:45:00Z",
                        "time_end": "2023-10-25T00:57:00Z",
                        "paper_type": "short",
                        "keywords": [
                            "Human-centered computing\u2014Visualization\u2014Visu- alization systems and tools"
                        ],
                        "doi": "",
                        "fno": "",
                        "has_image": true,
                        "has_pdf": true,
                        "paper_award": "",
                        "image_caption": "Projection Ensemble recognizes robust structures in multidimensional projections. A) A randomly initialized t-SNE projection for the MNIST dataset. The viewer may interpret groups of points (a) and (b) as individual clusters, which, in fact, have intricate structures. B) Projection Ensemble visualizes two robust structures identified by extracting common subgraphs between ten randomly initialized projections. This reveals that (a) actually consists of two entangled structures (group (c) and the other points), and a subgroup of the cluster (b) is found to be closer to a distant cluster (see (d)). C) The ground-truth class labels are shown as the color of points.",
                        "external_paper_link": "",
                        "youtube_ff_link": "https://youtu.be/yRTFqYREZ18",
                        "youtube_ff_id": "yRTFqYREZ18",
                        "bunny_ff_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-short/v-short-1051/v-short-1051_Preview.mp4?token=0XHSWUaiGMLJn04h9KdbvX26bor1iRsWpU-1fy1iWqc&expires=1706590800",
                        "bunny_ff_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-short/v-short-1051/v-short-1051_Preview.vtt?token=dEG9SUi9WmRaEv42UDCMIkG_MvjoJ4OM0wMKgxi8qNY&expires=1706590800",
                        "youtube_prerecorded_link": "https://youtu.be/i1_CNi18axc",
                        "youtube_prerecorded_id": "i1_CNi18axc",
                        "bunny_prerecorded_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-short/v-short-1051/v-short-1051_Presentation.mp4?token=ZEAxCaPfJF4M5635JLZkqKEa2gpnf18-scw_y1QQPi8&expires=1706590800",
                        "bunny_prerecorded_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-short/v-short-1051/v-short-1051_Presentation.vtt?token=uwmRm5ZXLpeP-2Dg8Z4G2NYDZDwB4B1rTSCQoXGcZfM&expires=1706590800"
                    }
                ]
            },
            {
                "title": "LLMs and Generative Models",
                "session_id": "full15",
                "event_prefix": "v-full",
                "track": "oneohsix",
                "session_image": "full15.png",
                "chair": [
                    "Vidya Setlur"
                ],
                "time_start": "2023-10-24T23:45:00Z",
                "time_end": "2023-10-25T01:00:00Z",
                "discord_category": "",
                "discord_channel": "106",
                "discord_channel_id": "1161741373410644119",
                "discord_link": "https://discordapp.com/channels/1161400451619627068/1161741373410644119",
                "zoom_private_meeting": "",
                "zoom_private_password": "",
                "zoom_private_link": "",
                "zoom_broadcast_link": "",
                "ff_link": "https://youtu.be/vBWk_jPZaig",
                "time_slots": [
                    {
                        "slot_id": "v-full-1114",
                        "session_id": "full15",
                        "title": "AttentionViz: A Global View of Transformer Attention",
                        "contributors": [
                            "Catherine Yeh"
                        ],
                        "authors": [
                            "Catherine Yeh",
                            "Yida Chen",
                            "Aoyu Wu",
                            "Cynthia Chen",
                            "Fernanda Viegas",
                            "Martin Wattenberg"
                        ],
                        "abstract": "Transformer models are revolutionizing machine learning, but their inner workings remain mysterious. In this work, we present a new visualization technique designed to help researchers understand the self-attention mechanism in transformers that allows these models to learn rich, contextual relationships between elements of a sequence. The main idea behind our method is to visualize a joint embedding of the query and key vectors used by transformer models to compute attention. Unlike previous attention visualization techniques, our approach enables the analysis of global patterns across multiple input sequences. We create an interactive visualization tool, AttentionViz (demo: http://attentionviz.com), based on these joint query-key embeddings, and use it to study attention mechanisms in both language and vision transformers. We demonstrate the utility of our approach in improving model understanding and offering new insights about query-key interactions through several application scenarios and expert feedback.",
                        "uid": "v-full-1114",
                        "time_stamp": "2023-10-24T23:45:00Z",
                        "time_start": "2023-10-24T23:45:00Z",
                        "time_end": "2023-10-24T23:57:00Z",
                        "paper_type": "full",
                        "keywords": [
                            "Transformer, Attention, NLP, Computer Vision, Visual Analytics"
                        ],
                        "doi": "10.1109/TVCG.2023.3327163",
                        "fno": "",
                        "has_image": true,
                        "has_pdf": true,
                        "paper_award": "",
                        "image_caption": "AttentionViz, our interactive visualization tool, allows users to explore transformer self-attention at scale by creating a joint embedding space for queries and keys. (a) In language transformers, these visualizations reveal striking visual traces that can be linked to attention patterns. Each point represents the query (green) or key (pink) version of a word. Users can explore individual attention heads (left) or zoom out for a \u201cglobal\u201d view of attention (right). (b) Our visualizations also divulge interesting insights in vision transformers, such as attention heads that group image patches by hue and brightness. (c) Sample input sentences and (d) images.",
                        "external_paper_link": "",
                        "youtube_ff_link": "https://youtu.be/uVoPKrRy3ik",
                        "youtube_ff_id": "uVoPKrRy3ik",
                        "bunny_ff_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1114/v-full-1114_Preview.mp4?token=k_vy-R48sPZCPmCNKCf1OflSKsb8v7tfz4gjGqX1vCg&expires=1706590800",
                        "bunny_ff_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1114/v-full-1114_Preview.vtt?token=Ge6Ly5QY0BInlzotLd5nj3Zfew7rUMtTDbm9a9rvVy4&expires=1706590800",
                        "youtube_prerecorded_link": "https://youtu.be/YBxRfWTFb3U",
                        "youtube_prerecorded_id": "YBxRfWTFb3U",
                        "bunny_prerecorded_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1114/v-full-1114_Presentation.mp4?token=1P6e9ndycE4fQeIVc_fcfTu261LdnD9ezbM4q1N-mlM&expires=1706590800",
                        "bunny_prerecorded_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1114/v-full-1114_Presentation.vtt?token=_M3Go6VX-5X5ZKhAmc1T7imeYYt79XcwISjnQoIvi8Q&expires=1706590800"
                    },
                    {
                        "slot_id": "v-tvcg-9964397",
                        "session_id": "full15",
                        "title": "PhraseMap: Attention-based Keyphrases Recommendation for Information Seeking",
                        "contributors": [
                            "Yamei Tu"
                        ],
                        "authors": [
                            "Yamei Tu",
                            "Rui Qiu",
                            "Yu-Shuen Wang",
                            "Po-Yin Yen",
                            "Han-Wei Shen"
                        ],
                        "abstract": "Many Information Retrieval (IR) approaches have been proposed to extract relevant information from a large corpus. Among these methods, phrase-based retrieval methods have been proven to capture more concrete and concise information than word-based and paragraph-based methods. However, due to the complex relationship among phrases and a lack of proper visual guidance, achieving user-driven interactive information-seeking and retrieval remains challenging. In this study, we present a visual analytic approach for users to seek information from an extensive collection of documents efficiently. The main component of our approach is a PhraseMap, where nodes and edges represent the extracted keyphrases and their relationships, respectively, from a large corpus. To build the PhraseMap, we extract keyphrases from each document and link the phrases according to word attention determined using modern language models, i.e., BERT. As can be imagined, the graph is complex due to the extensive volume of information and the massive amount of relationships. Therefore, we develop a navigation algorithm to facilitate information seeking. It includes (1) a question-answering (QA) model to identify phrases related to users\u2019 queries and (2) updating relevant phrases based on users\u2019 feedback. To better present the PhraseMap, we introduce a resource-controlled self-organizing map (RC-SOM) to evenly and regularly display phrases on grid cells while expecting phrases with similar semantics to stay close in the visualization. To evaluate our approach, we conducted case studies with three domain experts in diverse literature. The results and feedback demonstrate its effectiveness, usability, and intelligence.",
                        "uid": "v-tvcg-9964397",
                        "time_stamp": "2023-10-24T23:57:00Z",
                        "time_start": "2023-10-24T23:57:00Z",
                        "time_end": "2023-10-25T00:09:00Z",
                        "paper_type": "full",
                        "keywords": [
                            "Machine learning;natural language processing;textual data;user-in-the-loop;visual analytics"
                        ],
                        "doi": "10.1109/TVCG.2022.3225114",
                        "fno": "9964397",
                        "has_image": true,
                        "has_pdf": false,
                        "paper_award": "",
                        "image_caption": "In this study, we introduce a visual analytics approach for efficient information retrieval from extensive document collections. We transform large corpora into an attention-based phrase graph, known as PhraseMap, where nodes represent keyphrases and edges illustrate their relationships. Our method employs the PhraseMap as a backend knowledge base and offers multiple user interactions. To enhance the visualization of the PhraseMap, we employ a resource-controlled self-organizing map (RC-SOM) to evenly display phrases on a grid while ensuring semantically similar phrases are located close together (A). We also devise a navigation algorithm to support information retrieval, consisting of (1) a question-answering (QA) model that identifies phrases relevant to user queries (A') and (2) the option to accept/decline top-recommended phrases (B), followed by further exploration through documents (C).",
                        "external_paper_link": "",
                        "youtube_ff_link": "https://youtu.be/QmFpNCFwd3k",
                        "youtube_ff_id": "QmFpNCFwd3k",
                        "bunny_ff_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-tvcg/v-tvcg-9964397/v-tvcg-9964397_Preview.mp4?token=BIXpDHgNJEZKaXeHSqJPUe3FhDrmQnWqwSsvmaMuwF4&expires=1706590800",
                        "bunny_ff_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-tvcg/v-tvcg-9964397/v-tvcg-9964397_Preview.vtt?token=ThUhFMzWtdMmY5BGuznZuKJwUrXTMigyHc476SukZbM&expires=1706590800",
                        "youtube_prerecorded_link": "https://youtu.be/NNSGOr6Y3Nw",
                        "youtube_prerecorded_id": "NNSGOr6Y3Nw",
                        "bunny_prerecorded_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-tvcg/v-tvcg-9964397/v-tvcg-9964397_Presentation.mp4?token=FD8whdZAb33tcalYBMQUCGJVeUZqpy8zdV93lsPh5x8&expires=1706590800",
                        "bunny_prerecorded_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-tvcg/v-tvcg-9964397/v-tvcg-9964397_Presentation.vtt?token=BNNZ7s5r0Tp7_Eu4FbhaLbBhWFGtk13G37wgIz--8dw&expires=1706590800"
                    },
                    {
                        "slot_id": "v-tvcg-10056593",
                        "session_id": "full15",
                        "title": "Visual Explanation for Open-domain Question Answering with BERT",
                        "contributors": [
                            "Zekai Shao"
                        ],
                        "authors": [
                            "Zekai Shao",
                            "Shuran Sun",
                            "Yuheng Zhao",
                            "Siyuan Wang",
                            "Zhongyu Wei",
                            "Tao Gui",
                            "Cagatay Turkay",
                            "Siming Chen"
                        ],
                        "abstract": "Open-domain question answering (OpenQA) is an essential but challenging task in natural language processing that aims to answer questions in natural language formats on the basis of large-scale unstructured passages. Recent research has taken the performance of benchmark datasets to new heights, especially when these datasets are combined with techniques for machine reading comprehension based on Transformer models. However, as identified through our ongoing collaboration with domain experts and our review of literature, three key challenges limit their further improvement: (i) complex data with multiple long texts, (ii) complex model architecture with multiple modules, and (iii) semantically complex decision process. In this paper, we present VEQA, a visual analytics system that helps experts understand the decision reasons of OpenQA and provides insights into model improvement. The system summarizes the data flow within and between modules in the OpenQA model as the decision process takes place at the summary, instance and candidate levels. Specifically, it guides users through a summary visualization of dataset and module response to explore individual instances with a ranking visualization that incorporates context. Furthermore, VEQA supports fine-grained exploration of the decision flow within a single module through a comparative tree visualization. We demonstrate the effectiveness of VEQA in promoting interpretability and providing insights into model enhancement through a case study and expert evaluation.",
                        "uid": "v-tvcg-10056593",
                        "time_stamp": "2023-10-25T00:09:00Z",
                        "time_start": "2023-10-25T00:09:00Z",
                        "time_end": "2023-10-25T00:21:00Z",
                        "paper_type": "full",
                        "keywords": [
                            "Open-domain Question Answering;Explainable Machine Learning;Visual Analytics"
                        ],
                        "doi": "10.1109/TVCG.2023.3243676",
                        "fno": "10056593",
                        "has_image": true,
                        "has_pdf": false,
                        "paper_award": "",
                        "image_caption": "Understanding the decision process of a neural model for OpenQA. The User Panel (A) displays the statistical information about the model and the dataset, as well as the color legends. The Summary View (B) provides a global summary of performance and module behavior for subsets. The Context View (C) presents questions from the selected subset and retrieved passage for a selected question. The Instance View (D) summarizes the keywords of each candidate passage in different modules with ranking visualization incorporating text to analyze the selected instance. The Tree View (E) explains the local data flow within a single module or multiple modules in the model with comparable Sankey-tree layout.",
                        "external_paper_link": "",
                        "youtube_ff_link": "https://youtu.be/Ed5HD9UvT9w",
                        "youtube_ff_id": "Ed5HD9UvT9w",
                        "bunny_ff_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-tvcg/v-tvcg-10056593/v-tvcg-10056593_Preview.mp4?token=Xxs9KMF4BbkHLq4nI52gwO7NrwUvvAy26ZHB-Yy9t0s&expires=1706590800",
                        "bunny_ff_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-tvcg/v-tvcg-10056593/v-tvcg-10056593_Preview.vtt?token=1yRp4QY7vp1HvAeHjd0UR3-u-lBc6fYJ6HuSU01MElc&expires=1706590800",
                        "youtube_prerecorded_link": "https://youtu.be/mbosHKXS99Q",
                        "youtube_prerecorded_id": "mbosHKXS99Q",
                        "bunny_prerecorded_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-tvcg/v-tvcg-10056593/v-tvcg-10056593_Presentation.mp4?token=DIxEiXQsoZWOVHC-bhP9XpPS8Fn44G7PQ9226irapDM&expires=1706590800",
                        "bunny_prerecorded_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-tvcg/v-tvcg-10056593/v-tvcg-10056593_Presentation.vtt?token=3JWPuBTB-VUwqjO4wkmLOsZcwOc-w-smme_qLte7cRE&expires=1706590800"
                    },
                    {
                        "slot_id": "v-full-1197",
                        "session_id": "full15",
                        "title": "CommonsenseVIS: Visualizing and Understanding Commonsense Reasoning Capabilities of Natural Language Models",
                        "contributors": [
                            "Xingbo Wang"
                        ],
                        "authors": [
                            "Xingbo Wang",
                            "Renfei Huang",
                            "Zhihua Jin",
                            "Tianqing Fang",
                            "Huamin Qu"
                        ],
                        "abstract": "Recently, large pretrained language models have achieved compelling performance on commonsense benchmarks. Nevertheless, it is unclear what commonsense knowledge the models learn and whether they solely exploit spurious patterns. Feature attributions are popular explainability techniques that identify important input concepts for model outputs. However, commonsense knowledge tends to be implicit and rarely explicitly presented in inputs. These methods cannot infer models\u2019 implicit reasoning over mentioned concepts. We present CommonsenseVIS, a visual explanatory system that utilizes external commonsense knowledge bases to contextualize model behavior for commonsense question-answering. Specifically, we extract relevant commonsense knowledge in inputs as references to align model behavior with human knowledge. Our system features multi-level visualization and interactive model probing and editing for different concepts and their underlying relations. Through a user study, we show that CommonsenseVIS helps NLP experts conduct a systematic and scalable visual analysis of models\u2019 relational reasoning over concepts in different situations.",
                        "uid": "v-full-1197",
                        "time_stamp": "2023-10-25T00:21:00Z",
                        "time_start": "2023-10-25T00:21:00Z",
                        "time_end": "2023-10-25T00:33:00Z",
                        "paper_type": "full",
                        "keywords": [
                            "Commonsense reasoning, visual analytics, XAI, natural language processing"
                        ],
                        "doi": "10.1109/TVCG.2023.3327153",
                        "fno": "",
                        "has_image": true,
                        "has_pdf": true,
                        "paper_award": "",
                        "image_caption": "Since commonsense knowledge is not explicitly stated, it is challenging to conduct a scalable analysis of what commonsense knowledge NLP models do (not) learn. We employ a knowledge graph to derive implicit commonsense in the model input as context information. Then, we use it to align model behavior with human reasoning through multi-level interactive visualizations. Thereafter, users can understand, diagnose, and edit specific knowledge areas where models do not perform well.",
                        "external_paper_link": "",
                        "youtube_ff_link": "https://youtu.be/Ko6di7FUGVc",
                        "youtube_ff_id": "Ko6di7FUGVc",
                        "bunny_ff_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1197/v-full-1197_Preview.mp4?token=T-OJMB-DI38AOvgWu8g7lpqUmrc_gG2DjZ2dm01pWGg&expires=1706590800",
                        "bunny_ff_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1197/v-full-1197_Preview.vtt?token=a6lvCsKcHMhKm3AJaxkm0VQLYoBtYbYOasBuuXr1Zz8&expires=1706590800",
                        "youtube_prerecorded_link": "https://youtu.be/Jvsw0VE3Bko",
                        "youtube_prerecorded_id": "Jvsw0VE3Bko",
                        "bunny_prerecorded_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1197/v-full-1197_Presentation.mp4?token=Imr4zCN02p7jFGjfhqNvYYzz8yWGNVHuHzjkGWOO-pA&expires=1706590800",
                        "bunny_prerecorded_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1197/v-full-1197_Presentation.vtt?token=8K4ulSSL0SFS8m5ArYkpS8QNc6JsxCwlmvlajDVVGYM&expires=1706590800"
                    },
                    {
                        "slot_id": "v-full-1292",
                        "session_id": "full15",
                        "title": "Let the Chart Spark: Embedding Semantic Context into Chart with Generative Model",
                        "contributors": [
                            "Shishi Xiao"
                        ],
                        "authors": [
                            "Shishi Xiao",
                            "Suizi Huang",
                            "Yue LIN",
                            "Yilin Ye",
                            "Wei Zeng"
                        ],
                        "abstract": "Pictorial visualization seamlessly integrates data and semantic context into visual representation, conveying complex information in an engaging and informative manner. Extensive studies have been devoted to developing authoring tools to simplify the creation of pictorial visualizations. However, mainstream works follow a retrieving-and-editing pipeline that heavily relies on retrieved visual elements from a dedicated corpus, which often compromise data integrity. Text-guided generation methods are emerging, but may have limited applicability due to its predefined entities. In this work, we propose ChartSpark, a novel system that embeds semantic context into chart based on text-to-image generative model. ChartSpark generates pictorial visualizations conditioned on both semantic context conveyed in textual inputs and data information embedded in plain charts. The method is generic for both foreground and background pictorial generation, satisfying the design practices identified from an empirical research into existing pictorial visualizations. We further develop an interactive visual interface that integrates a text analyzer, editing module, and evaluation module to enable users to generate, modify, and assess pictorial visualizations. We experimentally demonstrate the usability of our tool, and conclude with a discussion of the potential of using text-to-image generative model combined with interactive interface for visualization design.",
                        "uid": "v-full-1292",
                        "time_stamp": "2023-10-25T00:33:00Z",
                        "time_start": "2023-10-25T00:33:00Z",
                        "time_end": "2023-10-25T00:45:00Z",
                        "paper_type": "full",
                        "keywords": [
                            "pictorial visualization, generative model, authoring tool"
                        ],
                        "doi": "10.1109/TVCG.2023.3326913",
                        "fno": "",
                        "has_image": true,
                        "has_pdf": true,
                        "paper_award": "",
                        "image_caption": "ChartSpark\uff1a An authoring tool encompassing generation and evaluation to create faithful pictorial visualization.",
                        "external_paper_link": "",
                        "youtube_ff_link": "https://youtu.be/CiwtQhI1o5E",
                        "youtube_ff_id": "CiwtQhI1o5E",
                        "bunny_ff_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1292/v-full-1292_Preview.mp4?token=1xAA4AGJDGLcKO4x4QBPwQjMgx3iKkD-7AHtU2wU1eA&expires=1706590800",
                        "bunny_ff_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1292/v-full-1292_Preview.vtt?token=OneSG1FiNZlWkuEC5Ai221Ha5IzOoUBKk91Wv0OST4M&expires=1706590800",
                        "youtube_prerecorded_link": "https://youtu.be/OXno64ZJhoE",
                        "youtube_prerecorded_id": "OXno64ZJhoE",
                        "bunny_prerecorded_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1292/v-full-1292_Presentation.mp4?token=KS_Iwg2Vksa8iW1yc6RuJblhvYOtrzPUl3msLlTyA6I&expires=1706590800",
                        "bunny_prerecorded_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1292/v-full-1292_Presentation.vtt?token=8PhffU_LU1smshZVFkR9WA_gLVHVHWeHxP0gl8Qs1zo&expires=1706590800"
                    },
                    {
                        "slot_id": "v-full-1066",
                        "session_id": "full15",
                        "title": "PromptMagician: Interactive Prompt Engineering for Text-to-Image Creation",
                        "contributors": [
                            "Yingchaojie Feng"
                        ],
                        "authors": [
                            "Yingchaojie Feng",
                            "Xingbo Wang",
                            "Kam Kwai Wong",
                            "Sijia Wang",
                            "Yuhong Lu",
                            "Minfeng Zhu",
                            "Baicheng Wang",
                            "Wei Chen"
                        ],
                        "abstract": "Generative text-to-image models have gained great popularity among the public for their powerful capability to generate high-quality images based on natural language prompts. However, developing effective prompts for desired images can be challenging due to the complexity and ambiguity of natural language. This research proposes PromptMagician, a visual analysis system that helps users explore the image results and refine the input prompts. The backbone of our system is a prompt recommendation model that takes user prompts as input, retrieves similar prompt-image pairs from DiffusionDB, and identifies special (important and relevant) prompt keywords. To facilitate interactive prompt refinement, PromptMagician introduces a multi-level visualization for the cross-modal embedding of the retrieved images and recommended keywords, and supports users in specifying multiple criteria for personalized exploration. Two usage scenarios, a user study, and expert interviews demonstrate the effectiveness and usability of our system, suggesting it facilitates prompt engineering and improves the creativity support of the generative text-to-image model.",
                        "uid": "v-full-1066",
                        "time_stamp": "2023-10-25T00:45:00Z",
                        "time_start": "2023-10-25T00:45:00Z",
                        "time_end": "2023-10-25T00:57:00Z",
                        "paper_type": "full",
                        "keywords": [
                            "Prompt engineering, text-to-image generation, image visualization"
                        ],
                        "doi": "10.1109/TVCG.2023.3327168",
                        "fno": "",
                        "has_image": true,
                        "has_pdf": true,
                        "paper_award": "",
                        "image_caption": "The user interface of PromptMagician consists of four views. The Model Input View (A) configures the prompts and model parameters for image creation. The Image Browser View (B) visualizes the generated and retrieved images and the recommended prompt keywords. The Image Evaluation View (C) helps evaluate and filter images based on multiple criteria. The Local Exploration View (D) helps users explore and validate the prompt keywords and guidance scales for images of interest.",
                        "external_paper_link": "",
                        "youtube_ff_link": "https://youtu.be/zO6bF0BPSQQ",
                        "youtube_ff_id": "zO6bF0BPSQQ",
                        "bunny_ff_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1066/v-full-1066_Preview.mp4?token=viWTVGqJm1CJLyXtAhLEeIAuYZsYTQzraWazaKpD_qA&expires=1706590800",
                        "bunny_ff_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1066/v-full-1066_Preview.vtt?token=K-_7W8nVHDg6-8CUm-7nQDNN5NOjK4SUG0FlzLO4fM0&expires=1706590800",
                        "youtube_prerecorded_link": "",
                        "youtube_prerecorded_id": "",
                        "bunny_prerecorded_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1066/v-full-1066_Presentation.mp4?token=HOmaX4BFh0HXmAByqGeIALphGyisMfruw8RkF_8xJM8&expires=1706590800",
                        "bunny_prerecorded_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1066/v-full-1066_Presentation.vtt?token=emFwqOrtYosanaWpM4wYzsg3Pf7vGv2xG6lv1EatDi4&expires=1706590800"
                    }
                ]
            },
            {
                "title": "Machine Learning for Volume Visualization",
                "session_id": "full16",
                "event_prefix": "v-full",
                "track": "oneohsix",
                "session_image": "full16.png",
                "chair": [
                    "Joshua A. Levine"
                ],
                "time_start": "2023-10-25T23:45:00Z",
                "time_end": "2023-10-26T01:00:00Z",
                "discord_category": "",
                "discord_channel": "106",
                "discord_channel_id": "1161741373410644119",
                "discord_link": "https://discordapp.com/channels/1161400451619627068/1161741373410644119",
                "zoom_private_meeting": "",
                "zoom_private_password": "",
                "zoom_private_link": "",
                "zoom_broadcast_link": "",
                "ff_link": "https://youtu.be/NVeMVeheJBk",
                "time_slots": [
                    {
                        "slot_id": "v-tvcg-9852325",
                        "session_id": "full16",
                        "title": "CoordNet: Data Generation and Visualization Generation for Time-Varying Volumes via a Coordinate-Based Neural Network",
                        "contributors": [
                            "Jun Han"
                        ],
                        "authors": [
                            "Jun Han",
                            "Chaoli Wang"
                        ],
                        "abstract": "Although deep learning has demonstrated its capability in solving diverse scientific visualization problems, it still lacks generalization power across different tasks. To address this challenge, we propose CoordNet, a single coordinate-based framework that tackles various tasks relevant to time-varying volumetric data visualization without modifying the network architecture. The core idea of our approach is to decompose diverse task inputs and outputs into a unified representation (i.e., coordinates and values) and learn a function from coordinates to their corresponding values. We achieve this goal using a residual block-based implicit neural representation architecture with periodic activation functions. We evaluate CoordNet on data generation (i.e., temporal super-resolution and spatial super-resolution) and visualization generation (i.e., view synthesis and ambient occlusion prediction) tasks using time-varying volumetric data sets of various characteristics. The experimental results indicate that CoordNet achieves better quantitative and qualitative results than the state-of-the-art approaches across all the evaluated tasks.",
                        "uid": "v-tvcg-9852325",
                        "time_stamp": "2023-10-25T23:45:00Z",
                        "time_start": "2023-10-25T23:45:00Z",
                        "time_end": "2023-10-25T23:57:00Z",
                        "paper_type": "full",
                        "keywords": [
                            "Volume visualization;implicit neural representation;data generation;visualization generation"
                        ],
                        "doi": "10.1109/TVCG.2022.3197203",
                        "fno": "9852325",
                        "has_image": true,
                        "has_pdf": false,
                        "paper_award": "",
                        "image_caption": "We propose an implicit neural representation for processing diverse scientific data generation and visualization tasks without changing network architecture.",
                        "external_paper_link": "",
                        "youtube_ff_link": "https://youtu.be/a-J8VHBwRxk",
                        "youtube_ff_id": "a-J8VHBwRxk",
                        "bunny_ff_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-tvcg/v-tvcg-9852325/v-tvcg-9852325_Preview.mp4?token=gFVHL0NbNad12J6-DNtCUNoCuEchBYXYG5Ci8p0uvD8&expires=1706590800",
                        "bunny_ff_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-tvcg/v-tvcg-9852325/v-tvcg-9852325_Preview.vtt?token=KPE_035zqNz34Pj4LaYvs8pznvTFu6LPasJozJtevfU&expires=1706590800",
                        "youtube_prerecorded_link": "https://youtu.be/3MWuv95BLTQ",
                        "youtube_prerecorded_id": "3MWuv95BLTQ",
                        "bunny_prerecorded_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-tvcg/v-tvcg-9852325/v-tvcg-9852325_Presentation.mp4?token=0qeZb66n9h2ZZPsmtU7jVjxXmiKI0knUS1HnYXASWGA&expires=1706590800",
                        "bunny_prerecorded_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-tvcg/v-tvcg-9852325/v-tvcg-9852325_Presentation.vtt?token=uQ7NV3rAEA1QZGTMqNCzINQXP015xAl4dxDk4-LwxYc&expires=1706590800"
                    },
                    {
                        "slot_id": "v-tvcg-9920542",
                        "session_id": "full16",
                        "title": "Deep Hierarchical Super Resolution for Scientific Data",
                        "contributors": [
                            "Skylar W. Wurster"
                        ],
                        "authors": [
                            "Skylar W. Wurster",
                            "Hanqi Guo",
                            "Han-Wei Shen",
                            "Thomas Peterka",
                            "Jiayi Xu"
                        ],
                        "abstract": "We present a novel technique for hierarchical super resolution (SR) with neural networks (NNs), which upscales volumetric data represented with an octree data structure to a high-resolution uniform grid with minimal seam artifacts on octree node boundaries. Our method uses existing state-of-the-art SR models and adds flexibility to upscale input data with varying levels of detail across the domain, instead of only uniform grid data that are supported in previous approaches. The key is to use a hierarchy of SR NNs, each trained to perform 2x SR between two levels of detail, with a hierarchical SR algorithm that minimizes seam artifacts by starting from the coarsest level of detail and working up. We show that our hierarchical approach outperforms baseline interpolation and hierarchical upscaling methods, and demonstrate the usefulness of our proposed approach across three use cases including data reduction using hierarchical downsampling+SR instead of uniform downsampling+SR, computation savings for hierarchical finite-time Lyapunov exponent field calculation, and super-resolving low-resolution simulation results for a high-resolution approximation visualization.",
                        "uid": "v-tvcg-9920542",
                        "time_stamp": "2023-10-25T23:57:00Z",
                        "time_start": "2023-10-25T23:57:00Z",
                        "time_end": "2023-10-26T00:09:00Z",
                        "paper_type": "full",
                        "keywords": [
                            "Deep learning;super-resolution;hierarchical data"
                        ],
                        "doi": "10.1109/TVCG.2022.3214420",
                        "fno": "9920542",
                        "has_image": true,
                        "has_pdf": false,
                        "paper_award": "",
                        "image_caption": "Deep Hierarchical Super Resolution combines the data efficiency of hierarchical data formats with the inference power of state-of-the-art super resolution models. We build a pyramid of super resolution networks and develop a hierarchical super resolution algorithm to upscale octree-like data.",
                        "external_paper_link": "",
                        "youtube_ff_link": "https://youtu.be/f7UCT5BfCZg",
                        "youtube_ff_id": "f7UCT5BfCZg",
                        "bunny_ff_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-tvcg/v-tvcg-9920542/v-tvcg-9920542_Preview.mp4?token=GpSqNlSnhr9opV6kXK4L1GwXkotPF0O8vS6pLYigyfo&expires=1706590800",
                        "bunny_ff_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-tvcg/v-tvcg-9920542/v-tvcg-9920542_Preview.vtt?token=Tn3qwaZeSi-mXiM_45S9hC52bOQ9_9Dp6buJ20WtfBc&expires=1706590800",
                        "youtube_prerecorded_link": "https://youtu.be/l-UdNYfw8kg",
                        "youtube_prerecorded_id": "l-UdNYfw8kg",
                        "bunny_prerecorded_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-tvcg/v-tvcg-9920542/v-tvcg-9920542_Presentation.mp4?token=4tgxgfqARSStAwkl1QImaCabysNbFjKKmOBBzdccGyk&expires=1706590800",
                        "bunny_prerecorded_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-tvcg/v-tvcg-9920542/v-tvcg-9920542_Presentation.vtt?token=oHmQfBnNbHMe9xw1kSHG4MRJokFz-ytqkfjy2P19GHo&expires=1706590800"
                    },
                    {
                        "slot_id": "v-tvcg-10175377",
                        "session_id": "full16",
                        "title": "Interactive Volume Visualization via Multi-Resolution Hash Encoding based Neural Representation",
                        "contributors": [
                            "Qi Wu"
                        ],
                        "authors": [
                            "Qi Wu",
                            "David Bauer",
                            "Michael J. Doyle",
                            "Kwan-Liu Ma"
                        ],
                        "abstract": "Neural networks have shown great potential in compressing volume data for visualization. However, due to the high cost of training and inference, such volumetric neural representations have thus far only been applied to offline data processing and non-interactive rendering. In this paper, we demonstrate that by simultaneously leveraging modern GPU tensor cores, a native CUDA neural network framework, and a well-designed rendering algorithm with macro-cell acceleration, we can interactively ray trace volumetric neural representations (10-60fps). Our neural representations are also high-fidelity (PSNR > 30dB) and compact (10-1000x smaller). Additionally, we show that it is possible to fit the entire training step inside a rendering loop and skip the pre-training process completely. To support extreme-scale volume data, we also develop an efficient out-of-core training strategy, which allows our volumetric neural representation training to potentially scale up to terascale using only an NVIDIA RTX 3090 workstation.",
                        "uid": "v-tvcg-10175377",
                        "time_stamp": "2023-10-26T00:09:00Z",
                        "time_start": "2023-10-26T00:09:00Z",
                        "time_end": "2023-10-26T00:21:00Z",
                        "paper_type": "full",
                        "keywords": [
                            "Implicit neural representation;path tracing;ray marching;volume visualization"
                        ],
                        "doi": "10.1109/TVCG.2023.3293121",
                        "fno": "10175377",
                        "has_image": true,
                        "has_pdf": false,
                        "paper_award": "",
                        "image_caption": "This paper showcases the use of modern GPU tensor cores, a CUDA neural network framework, and an optimized rendering algorithm to interactively ray trace volumetric neural representations at 10-60fps. These neural representations are of high quality (PSNR > 30dB) and are significantly compact (10-1000x smaller). The study also reveals that the entire training phase can be integrated into a rendering loop, eliminating the need for pre-training. Moreover, this method can be scaled to terascale using just an NVIDIA RTX 3090 workstation.",
                        "external_paper_link": "",
                        "youtube_ff_link": "https://youtu.be/kX5GggkjpdA",
                        "youtube_ff_id": "kX5GggkjpdA",
                        "bunny_ff_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-tvcg/v-tvcg-10175377/v-tvcg-10175377_Preview.mp4?token=p2MlUIpKbI3wSOcc0Uph2Yd_PyJs4Gw8Pi15vmm3sUk&expires=1706590800",
                        "bunny_ff_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-tvcg/v-tvcg-10175377/v-tvcg-10175377_Preview.vtt?token=HLGkGa2dBJJuT45YXqZ_E5FVrGCKD5Gt8PThzrx2WMI&expires=1706590800",
                        "youtube_prerecorded_link": "https://youtu.be/_jrB2gX8a6Q",
                        "youtube_prerecorded_id": "_jrB2gX8a6Q",
                        "bunny_prerecorded_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-tvcg/v-tvcg-10175377/v-tvcg-10175377_Presentation.mp4?token=kIxHPZbIZSwG04_n_CxGzN7ZCqlV0u3TsyddZS1W5Yw&expires=1706590800",
                        "bunny_prerecorded_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-tvcg/v-tvcg-10175377/v-tvcg-10175377_Presentation.vtt?token=5GDDq0ynwd_E4FRbGYLfn_bRL9XW4ZeJaHUKSj7eT9E&expires=1706590800"
                    },
                    {
                        "slot_id": "v-full-1036",
                        "session_id": "full16",
                        "title": "Adaptively Placed Multi-Grid Scene Representation Networks for Large-Scale Data Visualization",
                        "contributors": [
                            "Skylar Wolfgang Wurster"
                        ],
                        "authors": [
                            "Skylar Wolfgang Wurster",
                            "Tianyu Xiong",
                            "Han-Wei Shen",
                            "Hanqi Guo",
                            "Tom Peterka"
                        ],
                        "abstract": "Scene representation networks (SRNs) have been recently proposed for compression and visualization of scientific data. However, state-of-the-art SRNs do not adapt the allocation of available network parameters to the complex features found in scientific data, leading to a loss in reconstruction quality. We address this shortcoming with an adaptively placed multi-grid SRN (APMGSRN) and propose a domain decomposition training and inference technique for accelerated parallel training on multi-GPU systems. We also release an open-source neural volume rendering application that allows plug-and-play rendering with any PyTorch-based SRN. Our proposed APMGSRN architecture uses multiple spatially adaptive feature grids that learn where to be placed within the domain to dynamically allocate more neural network resources where error is high in the volume, improving state-of-the-art reconstruction accuracy of SRNs for scientific data without requiring expensive octree refining, pruning, and traversal like previous adaptive models. In our domain decomposition approach for representing large-scale data, we train an set of APMGSRNs in parallel on separate bricks of the volume to reduce training time while avoiding overhead necessary for an out-of-core solution for volumes too large to fit in GPU memory. After training, the lightweight SRNs are used for realtime neural volume rendering in our open-source renderer, where arbitrary view angles and transfer functions can be explored. A copy of this paper, all code, all models used in our experiments, and all supplemental materials and videos are available at https://github.com/skywolf829/APMGSRN.",
                        "uid": "v-full-1036",
                        "time_stamp": "2023-10-26T00:21:00Z",
                        "time_start": "2023-10-26T00:21:00Z",
                        "time_end": "2023-10-26T00:33:00Z",
                        "paper_type": "full",
                        "keywords": [
                            "Scene representation network, deep learning, scientific visualization, volume rendering"
                        ],
                        "doi": "10.1109/TVCG.2023.3327194",
                        "fno": "",
                        "has_image": true,
                        "has_pdf": true,
                        "paper_award": "",
                        "image_caption": "APMGSRN is a novel scene representation network that introduces flexible feature grids that learn their position within the scene to maximize the use of the network parameters regardless of the features within the data.",
                        "external_paper_link": "",
                        "youtube_ff_link": "https://youtu.be/CM2h8KUgsMg",
                        "youtube_ff_id": "CM2h8KUgsMg",
                        "bunny_ff_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1036/v-full-1036_Preview.mp4?token=g3Wfwsg86T8NcbHrRU2MsW8zW2ci5fhOZp7HjnxstIs&expires=1706590800",
                        "bunny_ff_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1036/v-full-1036_Preview.vtt?token=mWkrd9k37lJPiu4TajeNspZP-dPeMeB19OxLexfiyzw&expires=1706590800",
                        "youtube_prerecorded_link": "https://youtu.be/DNV-hfpiBLI",
                        "youtube_prerecorded_id": "DNV-hfpiBLI",
                        "bunny_prerecorded_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1036/v-full-1036_Presentation.mp4?token=7vnVn9rYtDguxTN85B8MurTzk0RUuSoi_KmaEFCZJOw&expires=1706590800",
                        "bunny_prerecorded_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1036/v-full-1036_Presentation.vtt?token=RBd4lpABNRLqLOjPtqVfKKHOfJ60LvBe9MZMeUywWLY&expires=1706590800"
                    },
                    {
                        "slot_id": "v-full-1039",
                        "session_id": "full16",
                        "title": "Photon Field Networks for Dynamic Real-Time Volumetric Global Illumination",
                        "contributors": [
                            "David Bauer"
                        ],
                        "authors": [
                            "David Bauer",
                            "Qi Wu",
                            "Kwan-Liu Ma"
                        ],
                        "abstract": "Volume data is commonly found in many scientific disciplines, like medicine, physics, and biology. Experts rely on robust scientific visualization techniques to extract valuable insights from the data. Recent years have shown path tracing to be the preferred approach for volumetric rendering, given its high levels of realism. However, real-time volumetric path tracing often suffers from stochastic noise and long convergence times, limiting interactive exploration. In this paper, we present a novel method to enable real-time global illumination for volume data visualization. We develop Photon Field Networks - a phase-function-aware, multi-light neural representation of indirect volumetric global illumination. The fields are trained on multi-phase photon caches that we compute a priori. Training can be done within seconds, after which the fields can be used in various rendering tasks. To showcase their potential, we develop a custom neural path tracer, with which our photon fields achieve interactive framerates even on large datasets. We conduct in-depth evaluations of the method's performance, including visual quality, stochastic noise, inference and rendering speeds, and accuracy regarding illumination and phase function awareness. Results are compared to ray marching, path tracing and photon mapping. Our findings show that Photon Field Networks can faithfully represent indirect global illumination within the boundaries of the trained phase spectrum while exhibiting less stochastic noise and rendering at a significantly faster rate than traditional methods.",
                        "uid": "v-full-1039",
                        "time_stamp": "2023-10-26T00:33:00Z",
                        "time_start": "2023-10-26T00:33:00Z",
                        "time_end": "2023-10-26T00:45:00Z",
                        "paper_type": "full",
                        "keywords": [
                            "Volume data, volume rendering, volume visualization, deep learning, global illumination, neural rendering, path tracing"
                        ],
                        "doi": "10.1109/TVCG.2023.3327107",
                        "fno": "",
                        "has_image": true,
                        "has_pdf": true,
                        "paper_award": "",
                        "image_caption": "Photon Field Networks are implicit neural represenations trained on photon caches traced in volume datasets. They can represent indirect radiance parameterized for sample position and view direction. Moreover, they can be trained on multiple caches simultaneously to learn non-isotropic scattering effects. In this paper, we introduce the concept of Photon Fields, show how to efficiently train them, and evaluate them in a proof-of-concept path tracing application. Our results show that Photon Fields can faithfully represent the photon caches and create approximate global illumination effects several times faster than a comparable path tracer.",
                        "external_paper_link": "",
                        "youtube_ff_link": "https://youtu.be/Q_c9v8F6BwM",
                        "youtube_ff_id": "Q_c9v8F6BwM",
                        "bunny_ff_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1039/v-full-1039_Preview.mp4?token=65f6F8CzhQSGe3C6kNGUKbiriuKPdWhnkbUrHY_9SrE&expires=1706590800",
                        "bunny_ff_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1039/v-full-1039_Preview.vtt?token=7DtPeuNa0AYoRUtADAM10_2lU6uL1w8WxqCSpuReR9E&expires=1706590800",
                        "youtube_prerecorded_link": "https://youtu.be/CSWbuK0rwoc",
                        "youtube_prerecorded_id": "CSWbuK0rwoc",
                        "bunny_prerecorded_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1039/v-full-1039_Presentation.mp4?token=TW35FC8MCh0qp0Vt668B0xv28MWfNZ9g-6Yfygn86hc&expires=1706590800",
                        "bunny_prerecorded_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1039/v-full-1039_Presentation.vtt?token=TW1lwuT_8pSOJFPEG9aQ8Cdz0O4kaggdbR41cdoMKsw&expires=1706590800"
                    },
                    {
                        "slot_id": "v-full-1215",
                        "session_id": "full16",
                        "title": "PSRFlow: Probabilistic Super Resolution with Flow-Based Models for Scientific Data",
                        "contributors": [
                            "JINGYI SHEN"
                        ],
                        "authors": [
                            "JINGYI SHEN",
                            "Han-Wei Shen"
                        ],
                        "abstract": "Although many deep-learning-based super-resolution approaches have been proposed in recent years, because no ground truth is available in the inference stage, few can quantify the errors and uncertainties of the super-resolved results. For scientific visualization applications, however, conveying uncertainties of the results to scientists is crucial to avoid generating misleading or incorrect information. In this paper, we propose PSRFlow, a novel normalizing flow-based generative model for scientific data super-resolution that incorporates uncertainty quantification into the super-resolution process. PSRFlow learns the conditional distribution of the high-resolution data based on the low-resolution counterpart. By sampling from a Gaussian latent space that captures the missing information in the high-resolution data, one can generate different plausible super-resolution outputs. The efficient sampling in the Gaussian latent space allows our model to perform uncertainty quantification for the super-resolved results. During model training, we augment the training data with samples across various scales to make the model adaptable to data of different scales, achieving flexible super-resolution for a given input. Our results demonstrate superior performance and robust uncertainty quantification compared with existing methods such as interpolation and GAN-based super-resolution networks.",
                        "uid": "v-full-1215",
                        "time_stamp": "2023-10-26T00:45:00Z",
                        "time_start": "2023-10-26T00:45:00Z",
                        "time_end": "2023-10-26T00:57:00Z",
                        "paper_type": "full",
                        "keywords": [
                            "Super resolution, latent space, normalizing flow, uncertainty visualization"
                        ],
                        "doi": "10.1109/TVCG.2023.3327171",
                        "fno": "",
                        "has_image": true,
                        "has_pdf": true,
                        "paper_award": "",
                        "image_caption": "We propose PSRFlow, a novel deep learning based super-resolution algorithm with uncertainty quantification. Our work is based on normalizing flows to capture the intricate relationships between low and high-resolution data. The missing high-frequency details and the low-resolution information are modeled separately in the latent space of a conditional normalizing flow. The high-frequency latent follows a Gaussian distribution conditioned on the low-resolution information. During testing, given a low-resolution input, one can sample from the conditional Gaussian distribution and utilize the inverse of the normalizing flow to obtain high-resolution outputs. The generated high-resolution outputs are then used for uncertainty estimation.",
                        "external_paper_link": "",
                        "youtube_ff_link": "https://youtu.be/_7X25uEUyz0",
                        "youtube_ff_id": "_7X25uEUyz0",
                        "bunny_ff_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1215/v-full-1215_Preview.mp4?token=8omjSH3e7hsyzyf0ygC_JVb8hSNkitDWCK-TL7pbuUQ&expires=1706590800",
                        "bunny_ff_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1215/v-full-1215_Preview.vtt?token=BDNV18k2UsLWQvNpzmnClSV33zs0UMKemfrR9HOBKbo&expires=1706590800",
                        "youtube_prerecorded_link": "",
                        "youtube_prerecorded_id": "",
                        "bunny_prerecorded_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1215/v-full-1215_Presentation.mp4?token=JRDHAEcbw-3I5ZIYo2NsUoE6mXaq5lcIfgnSbWWvuYw&expires=1706590800",
                        "bunny_prerecorded_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1215/v-full-1215_Presentation.vtt?token=z7N3H-FITeNGVCRDldMPnpAAZBPJR2oqWyLhntJuD2Y&expires=1706590800"
                    }
                ]
            },
            {
                "title": "Medical and Biomedical Applications",
                "session_id": "full17",
                "event_prefix": "v-full",
                "track": "oneohfive",
                "session_image": "full17.png",
                "chair": [
                    "Alexander Lex"
                ],
                "time_start": "2023-10-26T04:45:00Z",
                "time_end": "2023-10-26T06:00:00Z",
                "discord_category": "",
                "discord_channel": "105",
                "discord_channel_id": "1161741309346861067",
                "discord_link": "https://discordapp.com/channels/1161400451619627068/1161741309346861067",
                "zoom_private_meeting": "",
                "zoom_private_password": "",
                "zoom_private_link": "",
                "zoom_broadcast_link": "",
                "ff_link": "https://youtu.be/kM8ZAMRAx10",
                "time_slots": [
                    {
                        "slot_id": "v-full-1290",
                        "session_id": "full17",
                        "title": "HealthPrism: A Visual Analytics System for Exploring Children's Physical and Mental Health Profiles with Multimodal Data",
                        "contributors": [
                            "Zhihan Jiang"
                        ],
                        "authors": [
                            "Zhihan Jiang",
                            "Handi Chen",
                            "Rui Zhou",
                            "Jing Deng",
                            "Xinchen Zhang",
                            "Running Zhao",
                            "Cong Xie",
                            "Yifang Wang",
                            "Edith Ngai"
                        ],
                        "abstract": "The correlation between children\u2019s personal and family characteristics (e.g., demographics and socioeconomic status) and their physical and mental health status has been extensively studied across various research domains, such as public health, medicine, and data science. Such studies can provide insights into the underlying factors affecting children\u2019s health and aid in the development of targeted interventions to improve their health outcomes. However, with the availability of multiple data sources, including context data (i.e., the background information of children) and motion data (i.e., sensor data measuring activities of children), new challenges have arisen due to the large-scale, heterogeneous, and multimodal nature of the data. Existing statistical hypothesis-based and learning model-based approaches have been inadequate for comprehensively analyzing the complex correlation between multimodal features and multi-dimensional health outcomes due to the limited information revealed. In this work, we first distill a set of design requirements from multiple levels through conducting a literature review and iteratively interviewing 11 experts from multiple domains (e.g., public health and medicine). Then, we propose HealthPrism, an interactive visual and analytics system for assisting researchers in exploring the importance and influence of various context and motion features on children\u2019s health status from multi-level perspectives. Within HealthPrism, a multimodal learning model with a gate mechanism is proposed for health profiling and cross-modality feature importance comparison. A set of visualization components is designed for experts to explore and understand multimodal data freely. We demonstrate the effectiveness and usability of HealthPrism through quantitative evaluation of the model performance, case studies, and expert interviews in associated domains.",
                        "uid": "v-full-1290",
                        "time_stamp": "2023-10-26T04:45:00Z",
                        "time_start": "2023-10-26T04:45:00Z",
                        "time_end": "2023-10-26T04:57:00Z",
                        "paper_type": "full",
                        "keywords": [
                            "Visual Analytics, Health Profiling, Multimodal Learning, Context Data, Motion Data"
                        ],
                        "doi": "10.1109/TVCG.2023.3326943",
                        "fno": "",
                        "has_image": true,
                        "has_pdf": true,
                        "paper_award": "",
                        "image_caption": "The overview of HealthPrism. The Summary View (A) showcases overall context and motion features, including categorical context feature flows (A1), numerical context feature correlation (A2), motion features distribution (A4), and context and motion feature importance and influence (A3). The Group View (B) presents a network graph (B1) showing health profile clusters based on health indicators, genders, and age. It also presents the feature importance and influence (B2) and feature overview (B3) by groups. The Individual View (C) presents health profile (C1), motion and context feature (C3, C4), and feature importance and influence (C2) for up to two individuals for comparison.",
                        "external_paper_link": "",
                        "youtube_ff_link": "https://youtu.be/C95-YcpnIr8",
                        "youtube_ff_id": "C95-YcpnIr8",
                        "bunny_ff_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1290/v-full-1290_Preview.mp4?token=p8sGviIwPO87UJofJQADhBVQDd7gNBadvXDSLA_4SyA&expires=1706590800",
                        "bunny_ff_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1290/v-full-1290_Preview.vtt?token=g2pdRah92eQ_4fNn9HsGP4rlNGbvc1hJ76VltZgl3GQ&expires=1706590800",
                        "youtube_prerecorded_link": "https://youtu.be/sYpgwC54wcU",
                        "youtube_prerecorded_id": "sYpgwC54wcU",
                        "bunny_prerecorded_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1290/v-full-1290_Presentation.mp4?token=8mfiOPHFvYLfSfe7fV6YSQ7C9CcuaTlFfE87d4rIR7s&expires=1706590800",
                        "bunny_prerecorded_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1290/v-full-1290_Presentation.vtt?token=7wzzxXQmcKfs8gOm2jTJlSyrkAddPdEqQNL6J3F0Q0U&expires=1706590800"
                    },
                    {
                        "slot_id": "v-full-1279",
                        "session_id": "full17",
                        "title": "Marjorie: Visualizing Type 1 Diabetes Data to Support Pattern Exploration",
                        "contributors": [
                            "Andreas Hinterreiter"
                        ],
                        "authors": [
                            "Anna Scimone",
                            "Klaus Eckelt",
                            "Marc Streit",
                            "Andreas Hinterreiter"
                        ],
                        "abstract": "This work involves joint efforts between visualization and humanities researchers, aiming at building a holistic view of the cultural exchange and integration between China and Japan brought about by the overseas circulation of Chinese classics. Book circulation data consist of uncertain spatiotemporal trajectories, with multiple dimensions, and movement across hierarchical spaces forms a compound network. LiberRoad visualizes the circulation of books collected in the Imperial Household Agency of Japan, and can be generalized to other book movement data. The LiberRoad system enables a smooth transition between three views (Location Graph, map, and timeline) according to the desired perspectives (spatial or temporal), as well as flexible filtering and selection. The Location Graph is a novel uncertainty-aware visualization method that employs improved circle packing to represent spatial hierarchy. The map view intuitively shows the overall circulation by clustering and allows zooming into single book trajectory with lenses magnifying local movements. The timeline view ranks dynamically in response to user interaction to facilitate the discovery of temporal events. The evaluation and feedback from the expert users demonstrate that LiberRoad is helpful in revealing movement patterns and comparing circulation characteristics of different times and spaces.",
                        "uid": "v-full-1279",
                        "time_stamp": "2023-10-26T04:57:00Z",
                        "time_start": "2023-10-26T04:57:00Z",
                        "time_end": "2023-10-26T05:09:00Z",
                        "paper_type": "full",
                        "keywords": [
                            "Design study, task analysis, diabetes, time series data, visual analytics, clustering"
                        ],
                        "doi": "10.1109/TVCG.2023.3326936",
                        "fno": "",
                        "has_image": true,
                        "has_pdf": true,
                        "paper_award": "",
                        "image_caption": "Marjorie is a visual analytics solution for finding and analyzing patterns in type 1 diabetes data. Designed in consultation with diabetologists, Marjorie features a unique representation of glucose data based on modified horizon graphs and performs automated clustering of interesting glucose patterns.",
                        "external_paper_link": "",
                        "youtube_ff_link": "https://youtu.be/zOfOEm4NU3g",
                        "youtube_ff_id": "zOfOEm4NU3g",
                        "bunny_ff_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1279/v-full-1279_Preview.mp4?token=iixYYZENsIkbBJAj4tFOcB_NqXODB7zXoviOODPliM4&expires=1706590800",
                        "bunny_ff_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1279/v-full-1279_Preview.vtt?token=FSr3fH44QAZDjZBrQLe-hqjGu6y6i75CrVPLYqo_H9s&expires=1706590800",
                        "youtube_prerecorded_link": "https://youtu.be/MYspNQH2fi8",
                        "youtube_prerecorded_id": "MYspNQH2fi8",
                        "bunny_prerecorded_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1279/v-full-1279_Presentation.mp4?token=8AFW0YtYvmAzLiJPhMZbcKz1uY3L7-6QuQdG60N3gHM&expires=1706590800",
                        "bunny_prerecorded_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1279/v-full-1279_Presentation.vtt?token=Zm-NaMlKToLBMFFU6Wp_DWZy2GGIG2YHoBfCWaxnRjM&expires=1706590800"
                    },
                    {
                        "slot_id": "v-full-1389",
                        "session_id": "full17",
                        "title": "Roses Have Thorns: Understanding the Downside of Oncological Care Delivery Through Visual Analytics and Sequential Rule Mining",
                        "contributors": [
                            "Carla Gabriela Floricel"
                        ],
                        "authors": [
                            "Carla Gabriela Floricel",
                            "Andrew Wentzel",
                            "Abdallah Mohamed",
                            "Clifton David Fuller",
                            "Guadalupe Canahuate",
                            "G. Elisabeta Marai"
                        ],
                        "abstract": "Personalized head and neck cancer therapeutics have greatly improved survival rates for patients, but are often leading to understudied long-lasting symptoms which affect quality of life. Sequential rule mining (SRM) is a promising unsupervised machine learning method for predicting longitudinal patterns in temporal data which, however, can output many repetitive patterns that are difficult to interpret without the assistance of visual analytics. We present a data-driven, human-machine analysis visual system developed in collaboration with SRM model builders in cancer symptom research, which facilitates mechanistic knowledge discovery in large scale, multivariate cohort symptom data. Our system supports multivariate predictive modeling of post-treatment symptoms based on during-treatment symptoms. It supports this goal through an SRM, clustering, and aggregation back end, and a custom front end to help develop and tune the predictive models. The system also explains the resulting predictions in the context of therapeutic decisions typical in personalized care delivery. We evaluate the resulting models and system with an interdisciplinary group of modelers and head and neck oncology researchers. The results demonstrate that our system effectively supports clinical and symptom research.",
                        "uid": "v-full-1389",
                        "time_stamp": "2023-10-26T05:09:00Z",
                        "time_start": "2023-10-26T05:09:00Z",
                        "time_end": "2023-10-26T05:21:00Z",
                        "paper_type": "full",
                        "keywords": [
                            "Temporal Data; Life Sciences; Mixed Initiative Human-Machine Analysis; Data Clustering and Aggregation"
                        ],
                        "doi": "10.1109/TVCG.2023.3326939",
                        "fno": "",
                        "has_image": true,
                        "has_pdf": true,
                        "paper_award": "",
                        "image_caption": "We present an interactive visual analytics system that supports sequential rule mining for model builders who work in cancer symptom research. The system facilitates mechanistic knowledge discovery of treatment-related toxicities (i.e., \"roses have thorns\") in large scale cohort data.",
                        "external_paper_link": "",
                        "youtube_ff_link": "https://youtu.be/uH5MNOblrbI",
                        "youtube_ff_id": "uH5MNOblrbI",
                        "bunny_ff_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1389/v-full-1389_Preview.mp4?token=ndinoFYERwtYqC9cTPY1IFJD0mDXpOemlsmfrxsQvdY&expires=1706590800",
                        "bunny_ff_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1389/v-full-1389_Preview.vtt?token=qg47OtNzvidh6FMEOcvltN4mbr3YuzUuhb7MMY7m5KA&expires=1706590800",
                        "youtube_prerecorded_link": "https://youtu.be/0ctRQ3mF4AE",
                        "youtube_prerecorded_id": "0ctRQ3mF4AE",
                        "bunny_prerecorded_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1389/v-full-1389_Presentation.mp4?token=0nGG1e01lTe1c4p9NSfLzmnR_vCD8hCkJdMGGKPzcMM&expires=1706590800",
                        "bunny_prerecorded_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1389/v-full-1389_Presentation.vtt?token=oKpJ8UbjueNzeKDdGLMpmDBffKnZRBslU1QlLhQRRvw&expires=1706590800"
                    },
                    {
                        "slot_id": "v-tvcg-10005035",
                        "session_id": "full17",
                        "title": "MitoVis: A Unified Visual Analytics System for End-to-End Neuronal Mitochondria Analysis",
                        "contributors": [
                            "JunYoung Choi"
                        ],
                        "authors": [
                            "JunYoung Choi",
                            "Hyun-Jic Oh",
                            "Hakjun Lee",
                            "Suyeon Kim",
                            "Seok-Kyu Kwon",
                            "Won-Ki Jeong"
                        ],
                        "abstract": "Neurons have a polarized structure, with dendrites and axons, and compartment-specific functions can be affected by the dwelling mitochondria. Recent studies have shown that the morphology of mitochondria is closely related to the functions of neurons and neurodegenerative diseases. However, the conventional mitochondria analysis workflow mainly relies on manual annotations and generic image-processing software. Moreover, even though there have been recent developments in automatic mitochondria analysis using deep learning, the application of existing methods in a daily analysis remains challenging because the performance of a pretrained deep learning model can vary depending on the target data, and there are always errors in inference time, requiring human proofreading. To address these issues, we introduce MitoVis, a novel visualization system for end-to-end data processing and an interactive analysis of the morphology of neuronal mitochondria. MitoVis introduces a novel active learning framework based on recent contrastive learning, which allows accurate fine-tuning of the neural network model. MitoVis also provides novel visual guides for interactive proofreading so that users can quickly identify and correct errors in the result with minimal effort. We demonstrate the usefulness and efficacy of the system via case studies conducted by neuroscientists. The results show that MitoVis achieved up to 13.3\u00d7 faster total analysis time in the case study compared to the conventional manual analysis workflow.",
                        "uid": "v-tvcg-10005035",
                        "time_stamp": "2023-10-26T05:21:00Z",
                        "time_start": "2023-10-26T05:21:00Z",
                        "time_end": "2023-10-26T05:33:00Z",
                        "paper_type": "full",
                        "keywords": [
                            "Biomedical and medical visualization;intelligence analysis;machine learning;task and requirements analysis;user interfaces"
                        ],
                        "doi": "10.1109/TVCG.2022.3233548",
                        "fno": "10005035",
                        "has_image": true,
                        "has_pdf": false,
                        "paper_award": "",
                        "image_caption": "MitoVis, a unified visual analytics system for end-to-end neuronal mitochondria analysis.",
                        "external_paper_link": "",
                        "youtube_ff_link": "https://youtu.be/_h1rsXq9NhU",
                        "youtube_ff_id": "_h1rsXq9NhU",
                        "bunny_ff_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-tvcg/v-tvcg-10005035/v-tvcg-10005035_Preview.mp4?token=tF2szkJNMnO7KKwEsRAskT3CwhqMySKDYQoTC2g_7NM&expires=1706590800",
                        "bunny_ff_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-tvcg/v-tvcg-10005035/v-tvcg-10005035_Preview.vtt?token=X6HLVCN6Ca5WsYs7rk77jDTtkWiTmdgA-lOS0RD1Akw&expires=1706590800",
                        "youtube_prerecorded_link": "https://youtu.be/sjwjvEJ1nwI",
                        "youtube_prerecorded_id": "sjwjvEJ1nwI",
                        "bunny_prerecorded_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-tvcg/v-tvcg-10005035/v-tvcg-10005035_Presentation.mp4?token=r9nwooBhCUXw5j0Inq6fZZZ47mPMebybFv-cxcwrdLI&expires=1706590800",
                        "bunny_prerecorded_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-tvcg/v-tvcg-10005035/v-tvcg-10005035_Presentation.vtt?token=Z2tDyVyHL8vbI81USvg4Tank2IGTzPCAajK4Cd66MhI&expires=1706590800"
                    },
                    {
                        "slot_id": "v-tvcg-10143227",
                        "session_id": "full17",
                        "title": "PanVA: Pangenomic Variant Analysis",
                        "contributors": [
                            "Astrid van den Brandt"
                        ],
                        "authors": [
                            "Astrid van den Brandt",
                            "Eef M. Jonkheer",
                            "Dirk-Jan M. van Workum",
                            "Huub van de Wetering",
                            "Sandra Smit",
                            "Anna Vilanova"
                        ],
                        "abstract": "Genomics researchers increasingly use multiple reference genomes to comprehensively explore genetic variants underlying differences in detectable characteristics between organisms. Pangenomes allow for an efficient data representation of multiple related genomes and their associated metadata. However, current visual analysis approaches for exploring these complex genotype-phenotype relationships are often based on single reference approaches or lack adequate support for interpreting the variants in the genomic context with heterogeneous (meta)data. This design study introduces PanVA, a visual analytics design for pangenomic variant analysis developed with the active participation of genomics researchers. The design uniquely combines tailored visual representations with interactions such as sorting, grouping, and aggregation, allowing users to navigate and explore different perspectives on complex genotype-phenotype relations. Through evaluation in the context of plants and pathogen research, we show that PanVA helps researchers explore variants in genes and generate hypotheses about their role in phenotypic variation.",
                        "uid": "v-tvcg-10143227",
                        "time_stamp": "2023-10-26T05:33:00Z",
                        "time_start": "2023-10-26T05:33:00Z",
                        "time_end": "2023-10-26T05:45:00Z",
                        "paper_type": "full",
                        "keywords": [
                            "Visual analytics;design study;pangenomics;comparative genomics;variant analysis"
                        ],
                        "doi": "10.1109/TVCG.2023.3282364",
                        "fno": "10143227",
                        "has_image": true,
                        "has_pdf": false,
                        "paper_award": "",
                        "image_caption": "Overview of PanVA, annotated to show the main components: the Control Panel (A) to select a gene, review peripheral information, and select groups; the Gene Overview (B) to slice an interesting region within the gene for further analysis; and the Locus View (C), the core view to analyze genetic variation in the sliced region (1), and explore association with metadata (2) and hierarchical relations (3).",
                        "external_paper_link": "",
                        "youtube_ff_link": "https://youtu.be/Mlt48tFIqho",
                        "youtube_ff_id": "Mlt48tFIqho",
                        "bunny_ff_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-tvcg/v-tvcg-10143227/v-tvcg-10143227_Preview.mp4?token=3_nwFzMbLzw-iKpJMyO78aNqTBe-W20CmvVeWPpml3U&expires=1706590800",
                        "bunny_ff_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-tvcg/v-tvcg-10143227/v-tvcg-10143227_Preview.vtt?token=NaCgvAg6T9FzcMvXOOw2mXgFspCjm8a1v1XFayu2Rrg&expires=1706590800",
                        "youtube_prerecorded_link": "https://youtu.be/nxm5OgvK5_I",
                        "youtube_prerecorded_id": "nxm5OgvK5_I",
                        "bunny_prerecorded_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-tvcg/v-tvcg-10143227/v-tvcg-10143227_Presentation.mp4?token=ARbpyojxowbEG-iAM9zCEVRwC28n7rG-IyvYBuWiIw4&expires=1706590800",
                        "bunny_prerecorded_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-tvcg/v-tvcg-10143227/v-tvcg-10143227_Presentation.vtt?token=QnBBDcdqiZvK7cB6Q0nfZO2G0tsIvdWC2Dv5s9JC-TE&expires=1706590800"
                    },
                    {
                        "slot_id": "v-full-1346",
                        "session_id": "full17",
                        "title": "Leveraging Historical Medical Records as a Proxy via Multimodal Modeling and Visualization to Enrich Medical Diagnostic Learning",
                        "contributors": [
                            "Yang Ouyang"
                        ],
                        "authors": [
                            "Yang Ouyang",
                            "Yuchen Wu",
                            "He Wang",
                            "Chenyang Zhang",
                            "Furui Cheng",
                            "Chang Jiang",
                            "Lixia Jin",
                            "Yuanwu Cao",
                            "Quan Li"
                        ],
                        "abstract": "Simulation-based Medical Education (SBME) has been developed as a cost-effective means of enhancing the diagnostic skills of novice physicians and interns, thereby mitigating the need for resource-intensive mentor-apprentice training. However, feedback provided in most SBME is often directed towards improving the operational proficiency of learners, rather than providing summative medical diagnoses that result from experience and time. Additionally, the multimodal nature of medical data during diagnosis poses significant challenges for interns and novice physicians, including the tendency to overlook or over-rely on data from certain modalities, and difficulties in comprehending potential associations between modalities. To address these challenges, we present DiagnosisAssistant, a visual analytics system that leverages historical medical records as a proxy for multimodal modeling and visualization to enhance the learning experience of interns and novice physicians. The system employs elaborately designed visualizations to explore different modality data, offer diagnostic interpretive hints based on the constructed model, and enable comparative analyses of specific patients. Our approach is validated through two case studies and expert interviews, demonstrating its effectiveness in enhancing medical training.",
                        "uid": "v-full-1346",
                        "time_stamp": "2023-10-26T05:45:00Z",
                        "time_start": "2023-10-26T05:45:00Z",
                        "time_end": "2023-10-26T05:57:00Z",
                        "paper_type": "full",
                        "keywords": [
                            "Multimodal Medical Dataset, Visual Analytics, Explainable Machine Learning"
                        ],
                        "doi": "10.1109/TVCG.2023.3326929",
                        "fno": "",
                        "has_image": true,
                        "has_pdf": true,
                        "paper_award": "",
                        "image_caption": "The system interface of DiagnosisAssistant contains (A) the User panel, (B) the Embedding Transition View, (C) the Modality Exploration View, and (D) the Comparison View.",
                        "external_paper_link": "",
                        "youtube_ff_link": "https://youtu.be/Kpvq91vRi4Q",
                        "youtube_ff_id": "Kpvq91vRi4Q",
                        "bunny_ff_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1346/v-full-1346_Preview.mp4?token=tUHqgDiJ_7IISRcWzENyqSmoXzXN1WtuWjPRX-AdCys&expires=1706590800",
                        "bunny_ff_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1346/v-full-1346_Preview.vtt?token=25xpdkbdj0TjVLMCGPpG5qzHwyEo_n7aeLSuQ1UZSRY&expires=1706590800",
                        "youtube_prerecorded_link": "https://youtu.be/RAOZsUGql9E",
                        "youtube_prerecorded_id": "RAOZsUGql9E",
                        "bunny_prerecorded_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1346/v-full-1346_Presentation.mp4?token=bJXrUnLrLzjC4UHnQKoYsHIRkxQHBBd-FKRR7WddxEk&expires=1706590800",
                        "bunny_prerecorded_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1346/v-full-1346_Presentation.vtt?token=Yyv4JYDlkvzXTosO7giKR9jSGReiz7KMaavLgYXjPAM&expires=1706590800"
                    }
                ]
            },
            {
                "title": "ML for VIS",
                "session_id": "full18",
                "event_prefix": "v-full",
                "track": "oneohnine",
                "session_image": "full18.png",
                "chair": [
                    "Nan Cao"
                ],
                "time_start": "2023-10-26T03:00:00Z",
                "time_end": "2023-10-26T04:15:00Z",
                "discord_category": "",
                "discord_channel": "109",
                "discord_channel_id": "1161741434647482379",
                "discord_link": "https://discordapp.com/channels/1161400451619627068/1161741434647482379",
                "zoom_private_meeting": "",
                "zoom_private_password": "",
                "zoom_private_link": "",
                "zoom_broadcast_link": "",
                "ff_link": "https://youtu.be/SjlZ-YBpbto",
                "time_slots": [
                    {
                        "slot_id": "v-full-1194",
                        "session_id": "full18",
                        "title": "Data Type Agnostic Visual Sensitivity Analysis",
                        "contributors": [
                            "Markus B\u00f6gl"
                        ],
                        "authors": [
                            "Nikolaus Piccolotto",
                            "Markus B\u00f6gl",
                            "Christoph Muehlmann",
                            "Klaus Nordhausen",
                            "Peter Filzmoser",
                            "Johanna Schmidt",
                            "Silvia Miksch"
                        ],
                        "abstract": "Modern science and industry rely on computational models for simulation, prediction, and data analysis. Spatial blind source separation (SBSS) is a model used to analyze spatial data. Designed explicitly for spatial data analysis, it is superior to popular non-spatial methods, like PCA. However, a challenge to its practical use is setting two complex tuning parameters, which requires parameter space analysis. In this paper, we focus on sensitivity analysis (SA). SBSS parameters and outputs are spatial data, which makes SA difficult as few SA approaches in the literature assume such complex data on both sides of the model. Based on the requirements in our design study with statistics experts, we developed a visual analytics prototype for data type agnostic visual sensitivity analysis that fits SBSS and other contexts. The main advantage of our approach is that it requires only dissimilarity measures for parameter settings and outputs. We evaluated the prototype heuristically with visualization experts and through interviews with two SBSS experts. In addition, we show the transferability of our approach by applying it to microclimate simulations. Study participants could confirm suspected and known parameter-output relations, find surprising associations, and identify parameter subspaces to examine in the future. During our design study and evaluation, we identified challenging future research opportunities.",
                        "uid": "v-full-1194",
                        "time_stamp": "2023-10-26T03:00:00Z",
                        "time_start": "2023-10-26T03:00:00Z",
                        "time_end": "2023-10-26T03:12:00Z",
                        "paper_type": "full",
                        "keywords": [
                            "Visual analytics, parameter space analysis, sensitivity analysis, spatial blind source separation."
                        ],
                        "doi": "10.1109/TVCG.2023.3327203",
                        "fno": "",
                        "has_image": true,
                        "has_pdf": true,
                        "paper_award": "",
                        "image_caption": "The schematized illustration of our proposed visualization shows a dendrogram with clusters colored in a blue-red diverging scale. The red cluster points to sensitive parameter settings, the blue cluster to stable settings.",
                        "external_paper_link": "",
                        "youtube_ff_link": "https://youtu.be/uM-t5wrikFs",
                        "youtube_ff_id": "uM-t5wrikFs",
                        "bunny_ff_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1194/v-full-1194_Preview.mp4?token=YitHc8HCsrwH-oNY4mVXpOuk2_XawuWYm8y7BPYe4zM&expires=1706590800",
                        "bunny_ff_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1194/v-full-1194_Preview.vtt?token=MSzsgVaywjq26Imp4T6xD9uLkWd8zNhw5BSAsYR8fjc&expires=1706590800",
                        "youtube_prerecorded_link": "https://youtu.be/qlz1lzLI48Y",
                        "youtube_prerecorded_id": "qlz1lzLI48Y",
                        "bunny_prerecorded_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1194/v-full-1194_Presentation.mp4?token=kRuL5Weg4IJHqZxr-vqA4cABBMxD_IMy1yOza0l2R2Q&expires=1706590800",
                        "bunny_prerecorded_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1194/v-full-1194_Presentation.vtt?token=tKUrVpouZA6wPLGIJ7PyGNGfaaTeWDWiBeMinkuoD5k&expires=1706590800"
                    },
                    {
                        "slot_id": "v-full-1223",
                        "session_id": "full18",
                        "title": "LiveRetro: Visual Analytics for Strategic Retrospect in Livestream E-Commerce",
                        "contributors": [
                            "Yuchen Wu"
                        ],
                        "authors": [
                            "Yuchen Wu",
                            "Yuansong Xu",
                            "Shenghan Gao",
                            "Xingbo Wang",
                            "Wenkai Song",
                            "Zhiheng Nie",
                            "Xiaomeng Fan",
                            "Quan Li"
                        ],
                        "abstract": "Livestream e-commerce integrates live streaming and online shopping, allowing viewers to make purchases while watching. However, effective marketing strategies remain a challenge due to limited empirical research and subjective biases from the absence of quantitative data. Current tools fail to capture the interdependence between live performances and feedback. This study identified computational features, formulated design requirements, and developed LiveRetro, an interactive visual analytics system. It enables comprehensive retrospective analysis of livestream e-commerce for streamers, viewers, and merchandise. LiveRetro employs enhanced visualization and time-series forecasting models to align performance features and feedback, identifying influences at channel, merchandise, feature, and segment levels. Through case studies and expert interviews, the system provides deep insights into the relationship between live performance and streaming statistics, enabling efficient strategic analysis from multiple perspectives.",
                        "uid": "v-full-1223",
                        "time_stamp": "2023-10-26T03:12:00Z",
                        "time_start": "2023-10-26T03:12:00Z",
                        "time_end": "2023-10-26T03:24:00Z",
                        "paper_type": "full",
                        "keywords": [
                            "Livestream E-commerce, Visual Analytics, Multimodal Video Analysis, Marketing Strategy, Time-series Modeling."
                        ],
                        "doi": "10.1109/TVCG.2023.3326911",
                        "fno": "",
                        "has_image": true,
                        "has_pdf": true,
                        "paper_award": "",
                        "image_caption": "LiveRetro system, consisting of Session View, Segment View, Exploration View, and Record View.",
                        "external_paper_link": "",
                        "youtube_ff_link": "https://youtu.be/BroIyMJvTmc",
                        "youtube_ff_id": "BroIyMJvTmc",
                        "bunny_ff_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1223/v-full-1223_Preview.mp4?token=eqkdfWvOtLlELmyF-H6HbN9ksShCXozSvZrmDPzPjUQ&expires=1706590800",
                        "bunny_ff_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1223/v-full-1223_Preview.vtt?token=x6_PerMojHBXmNW38c4C5az_RVdzj0D2fHRyYI_rhWA&expires=1706590800",
                        "youtube_prerecorded_link": "https://youtu.be/G_ofhrgeANw",
                        "youtube_prerecorded_id": "G_ofhrgeANw",
                        "bunny_prerecorded_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1223/v-full-1223_Presentation.mp4?token=wW5eV5MdJfROws95msD3oCw_-h5egpxPIadhrRI7sc0&expires=1706590800",
                        "bunny_prerecorded_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1223/v-full-1223_Presentation.vtt?token=6VO2mo8R5AUVzn7kPWt9uaQia2bINJz-3GpVSsi1rgU&expires=1706590800"
                    },
                    {
                        "slot_id": "v-tvcg-10149378",
                        "session_id": "full18",
                        "title": "Towards Better Modeling with Missing Data: A Contrastive Learning-based Visual Analytics Perspective",
                        "contributors": [
                            "Laixin Xie"
                        ],
                        "authors": [
                            "Laixin Xie",
                            "Yang Ouyang",
                            "Longfei Chen",
                            "Ziming Wu",
                            "Quan Li"
                        ],
                        "abstract": "Missing data can pose a challenge for machine learning (ML) modeling. To address this, current approaches are categorized into feature imputation and label prediction and are primarily focused on handling missing data to enhance ML performance. These approaches rely on the observed data to estimate the missing values and therefore encounter three main shortcomings in imputation, including the need for different imputation methods for various missing data mechanisms, heavy dependence on the assumption of data distribution, and potential introduction of bias. This study proposes a Contrastive Learning (CL) framework to model observed data with missing values, where the ML model learns the similarity between an incomplete sample and its complete counterpart and the dissimilarity between other samples. Our proposed approach demonstrates the advantages of CL without requiring any imputation. To enhance interpretability, we introduce CIVis, a visual analytics system that incorporates interpretable techniques to visualize the learning process and diagnose the model status. Users can leverage their domain knowledge through interactive sampling to identify negative and positive pairs in CL. The output of CIVis is an optimized model that takes specified features and predicts downstream tasks. We provide two usage scenarios in regression and classification tasks and conduct quantitative experiments, expert interviews, and a qualitative user study to demonstrate the effectiveness of our approach. In short, this study offers a valuable contribution to addressing the challenges associated with ML modeling in the presence of missing data by providing a practical solution that achieves high predictive accuracy and model interpretability.",
                        "uid": "v-tvcg-10149378",
                        "time_stamp": "2023-10-26T03:24:00Z",
                        "time_start": "2023-10-26T03:24:00Z",
                        "time_end": "2023-10-26T03:36:00Z",
                        "paper_type": "full",
                        "keywords": [
                            "Contrastive learning;data imputation;explainable AI;missing data"
                        ],
                        "doi": "10.1109/TVCG.2023.3285210",
                        "fno": "10149378",
                        "has_image": true,
                        "has_pdf": false,
                        "paper_award": "",
                        "image_caption": "Interaction and observation in CIVis. The notation 1 - 3 demonstrate how to configure positive and negative sampling based on visual cues; notation 4 - 7 highlight and explain the benefits that CIVis brings. The loss curves in (4) look like an area because the loss sharply and frequently jumps up and down.",
                        "external_paper_link": "",
                        "youtube_ff_link": "https://youtu.be/00BS3k2FvAY",
                        "youtube_ff_id": "00BS3k2FvAY",
                        "bunny_ff_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-tvcg/v-tvcg-10149378/v-tvcg-10149378_Preview.mp4?token=T8hRkYa2rZBS1-YBPkrMiUrcp14QEzU1HDWWJLIor0A&expires=1706590800",
                        "bunny_ff_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-tvcg/v-tvcg-10149378/v-tvcg-10149378_Preview.vtt?token=-DXRs30341c-KasnUQWSAx3Zh8wCvJ_DMrZQ1VkqfIU&expires=1706590800",
                        "youtube_prerecorded_link": "https://youtu.be/LyN8CXInM5g",
                        "youtube_prerecorded_id": "LyN8CXInM5g",
                        "bunny_prerecorded_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-tvcg/v-tvcg-10149378/v-tvcg-10149378_Presentation.mp4?token=TJBj7JlIuPj0D6iWtBPSBU6JuyLUWN0lfHZLb4SEzos&expires=1706590800",
                        "bunny_prerecorded_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-tvcg/v-tvcg-10149378/v-tvcg-10149378_Presentation.vtt?token=o-KP0eSi-VELDrA-Uoj9Iz1gp8xc7PkecM94dAyL0FI&expires=1706590800"
                    },
                    {
                        "slot_id": "v-tvcg-10034833",
                        "session_id": "full18",
                        "title": "uxSense: Supporting User Experience Analysis with Visualization and Computer Vision",
                        "contributors": [
                            "Andrea Batch"
                        ],
                        "authors": [
                            "Andrea Batch",
                            "Yipeng Ji",
                            "Mingming Fan",
                            "Jian Zhao",
                            "Niklas Elmqvist"
                        ],
                        "abstract": "Analyzing user behavior from usability evaluation can be a challenging and time-consuming task, especially as the number of participants and the scale and complexity of the evaluation grows. We propose uxSense, a visual analytics system using machine learning methods to extract user behavior from audio and video recordings as parallel time-stamped data streams. Our implementation draws on pattern recognition, computer vision, natural language processing, and machine learning to extract user sentiment, actions, posture, spoken words, and other features from such recordings. These streams are visualized as parallel timelines in a web-based front-end, enabling the researcher to search, filter, and annotate data across time and space. We present the results of a user study involving professional UX researchers evaluating user data using uxSense. In fact, we used uxSense itself to evaluate their sessions.",
                        "uid": "v-tvcg-10034833",
                        "time_stamp": "2023-10-26T03:36:00Z",
                        "time_start": "2023-10-26T03:36:00Z",
                        "time_end": "2023-10-26T03:48:00Z",
                        "paper_type": "full",
                        "keywords": [
                            "Visualization;visual analytics;evaluation;video analytics;machine learning;deep learning;computer vision"
                        ],
                        "doi": "10.1109/TVCG.2023.3241581",
                        "fno": "10034833",
                        "has_image": true,
                        "has_pdf": false,
                        "paper_award": "",
                        "image_caption": "An interface with a video playback component at the top left, a transcript in the top middle, and an annotations table at the top right. The bottom half of the screen is a collection of timeline visualizations.",
                        "external_paper_link": "",
                        "youtube_ff_link": "https://youtu.be/D1NMyMAG1iE",
                        "youtube_ff_id": "D1NMyMAG1iE",
                        "bunny_ff_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-tvcg/v-tvcg-10034833/v-tvcg-10034833_Preview.mp4?token=9Afo9mu63iMVKWL9Ts8bBnGg7IHGkKVKOK1EuSSu3aU&expires=1706590800",
                        "bunny_ff_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-tvcg/v-tvcg-10034833/v-tvcg-10034833_Preview.vtt?token=pMjFlGJDheuK5maJMwx8KWnKlD8OLDmX5GilVfer7tU&expires=1706590800",
                        "youtube_prerecorded_link": "",
                        "youtube_prerecorded_id": "",
                        "bunny_prerecorded_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-tvcg/v-tvcg-10034833/v-tvcg-10034833_Presentation.mp4?token=LhPlllYkPK47Ek0yao2qnp5ANWrtbDj0PncdYFffzVc&expires=1706590800",
                        "bunny_prerecorded_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-tvcg/v-tvcg-10034833/v-tvcg-10034833_Presentation.vtt?token=C99id-Yf5z30lAH2NLpPLg6MK8S7HQ6hUYDnvpwOEaU&expires=1706590800"
                    },
                    {
                        "slot_id": "v-full-1567",
                        "session_id": "full18",
                        "title": "Data Formulator: AI-powered Concept-driven Visualization Authoring",
                        "contributors": [
                            "Bongshin Lee"
                        ],
                        "authors": [
                            "Chenglong Wang",
                            "John R Thompson",
                            "Bongshin Lee"
                        ],
                        "abstract": "With most modern visualization tools, authors need to transform their data into tidy formats to create visualizations they want. Because this requires experience with programming or separate data processing tools, data transformation remains a barrier in visualization authoring. To address this challenge, we present a new visualization paradigm, concept binding, that separates high-level visualization intents and low-level data transformation steps, leveraging an AI agent. We realize this paradigm in Data Formulator, an interactive visualization authoring tool. With Data Formulator, authors first define data concepts they plan to visualize using natural languages or examples, and then bind them to visual channels. Data Formulator then dispatches its AI-agent to automatically transform the input data to surface these concepts and generate desired visualizations. When presenting the results (transformed table and output visualizations) from the AI-agent, Data Formulator provides feedback to help authors inspect and understand them. A user study with 10 participants shows that participants could learn and use Data Formulator to create visualizations that involve challenging data transformations, and presents interesting future research directions.",
                        "uid": "v-full-1567",
                        "time_stamp": "2023-10-26T03:48:00Z",
                        "time_start": "2023-10-26T03:48:00Z",
                        "time_end": "2023-10-26T04:00:00Z",
                        "paper_type": "full",
                        "keywords": [
                            "AI, visualization authoring, data transformation, programming by example, natural language, large language model"
                        ],
                        "doi": "10.1109/TVCG.2023.3326585",
                        "fno": "",
                        "has_image": true,
                        "has_pdf": true,
                        "paper_award": "honorable",
                        "image_caption": "Data Formulator User Interface. After loading the input data, the authors interact with Data Formulator in four steps: (1) in the Concept Shelf, create new data concepts they plan to visualize (e.g., Seattle and Atlanta) or derive (e.g., Difference, Warmer), (2) encode data concepts to visual channels of a chart using Chart Builder and formulate the chart, (3) inspect the derived data automatically generated by Data Formulator, and (4) examine and save generated visualizations. Throughout the process, Data Formulator provides feedback to help authors understand generated data and visualizations.",
                        "external_paper_link": "",
                        "youtube_ff_link": "https://youtu.be/gc2fOZ3E-1c",
                        "youtube_ff_id": "gc2fOZ3E-1c",
                        "bunny_ff_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1567/v-full-1567_Preview.mp4?token=6CiAlhYYhCLb_EI2-a2qvE9XR6DsL5YHHzAaBYHdYJs&expires=1706590800",
                        "bunny_ff_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1567/v-full-1567_Preview.vtt?token=yFZqC3xylcVnB5NiPeo63qJTpBLiuSz1H1j7rYyURdY&expires=1706590800",
                        "youtube_prerecorded_link": "https://youtu.be/2CmYytcpoPg",
                        "youtube_prerecorded_id": "2CmYytcpoPg",
                        "bunny_prerecorded_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1567/v-full-1567_Presentation.mp4?token=DNcHdemyn632sWameLri1IJld1l-MG574y96hF6PiBo&expires=1706590800",
                        "bunny_prerecorded_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1567/v-full-1567_Presentation.vtt?token=OIOOH91HyanLANauSu-gNSUH-v8FC6w3RhePq1k8PXs&expires=1706590800"
                    },
                    {
                        "slot_id": "v-full-1420",
                        "session_id": "full18",
                        "title": "InvVis: Large-Scale Data Embedding for Invertible Visualization",
                        "contributors": [
                            "Huayuan Ye"
                        ],
                        "authors": [
                            "Huayuan Ye",
                            "Chenhui Li",
                            "Yang Li",
                            "Changbo Wang"
                        ],
                        "abstract": "We present InvVis, a new approach for invertible visualization, which is reconstructing or further modifying a visualization from an image. InvVis allows the embedding of a significant amount of data, such as chart data, chart information, source code, etc., into visualization images. The encoded image is perceptually indistinguishable from the original one. We propose a new method to efficiently express chart data in the form of images, enabling large-capacity data embedding. We also outline a model based on the invertible neural network to achieve high-quality data concealing and revealing. We explore and implement a variety of application scenarios of InvVis. Additionally, we conduct a series of evaluation experiments to assess our method from multiple perspectives, including data embedding quality, data restoration accuracy, data encoding capacity, etc. The result of our experiments demonstrates the great potential of InvVis in invertible visualization.",
                        "uid": "v-full-1420",
                        "time_stamp": "2023-10-26T04:00:00Z",
                        "time_start": "2023-10-26T04:00:00Z",
                        "time_end": "2023-10-26T04:12:00Z",
                        "paper_type": "full",
                        "keywords": [
                            "Information visualization, information steganography, invertible visualization, invertible neural network."
                        ],
                        "doi": "10.1109/TVCG.2023.3326597",
                        "fno": "",
                        "has_image": true,
                        "has_pdf": true,
                        "paper_award": "",
                        "image_caption": "InvVis can embed a large amount of data into visualization images, users can decode the embedded data and perform rich exploration, such as redesigning (a) or reconstructing (b) visualizations, rebuilding a visualization dashboard based on the decoded source code and chart data (c), or visualizing volume data based on the decoded raw data and rendering parameters (d).",
                        "external_paper_link": "",
                        "youtube_ff_link": "https://youtu.be/O6gAlsCxrWU",
                        "youtube_ff_id": "O6gAlsCxrWU",
                        "bunny_ff_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1420/v-full-1420_Preview.mp4?token=7Y5sD340CFVh5h073q_dfymGAC-fxpZeCX2kONGb_9Y&expires=1706590800",
                        "bunny_ff_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1420/v-full-1420_Preview.vtt?token=WpU8IkTud_INuSpMLFFUqeXO4u0yEt8ygTFEhnLv0F4&expires=1706590800",
                        "youtube_prerecorded_link": "https://youtu.be/SalRGyajgds",
                        "youtube_prerecorded_id": "SalRGyajgds",
                        "bunny_prerecorded_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1420/v-full-1420_Presentation.mp4?token=auFdI-tZ-Xp3Ao5VN0a347l_oiOpY12bMRSXvVlkpZI&expires=1706590800",
                        "bunny_prerecorded_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1420/v-full-1420_Presentation.vtt?token=j-4Yi8zMVwYgMceLoyqw9SIiFAWMFfoMxUON4ELWOEM&expires=1706590800"
                    }
                ]
            },
            {
                "title": "Natural Language",
                "session_id": "full19",
                "event_prefix": "v-full",
                "track": "oneohnine",
                "session_image": "full19.png",
                "chair": [
                    "Andreas Kerren"
                ],
                "time_start": "2023-10-25T22:00:00Z",
                "time_end": "2023-10-25T23:15:00Z",
                "discord_category": "",
                "discord_channel": "109",
                "discord_channel_id": "1161741434647482379",
                "discord_link": "https://discordapp.com/channels/1161400451619627068/1161741434647482379",
                "zoom_private_meeting": "",
                "zoom_private_password": "",
                "zoom_private_link": "",
                "zoom_broadcast_link": "",
                "ff_link": "https://youtu.be/IH61fsQvTmw",
                "time_slots": [
                    {
                        "slot_id": "v-full-1284",
                        "session_id": "full19",
                        "title": "TransforLearn: Interactive Visual Tutorial for the Transformer Model",
                        "contributors": [
                            "Lin Gao"
                        ],
                        "authors": [
                            "Lin Gao",
                            "Zekai Shao",
                            "Ziqin LUO",
                            "Haibo Hu",
                            "Cagatay Turkay",
                            "Siming Chen"
                        ],
                        "abstract": "The widespread adoption of Transformers in deep learning, serving as the core framework for numerous large-scale language models, has sparked significant interest in understanding their underlying mechanisms. However, beginners face difficulties in comprehending and learning Transformers due to its complex structure and abstract data representation. We present TransforLearn, the first interactive visual tutorial designed for deep learning beginners and non-experts to comprehensively learn about Transformers. TransforLearn supports interactions for architecture-driven exploration and task-driven exploration, providing insight into different levels of model details and their working processes. It accommodates interactive views of each layer's operation and mathematical formula, helping users to understand the data flow of long text sequences. By altering the current decoder-based recursive prediction results and combining the downstream task abstractions, users can deeply explore model processes. Our user study revealed that the interactions of TransforLearn are positively received. We observe that TransforLearn facilitates users' accomplishment of study tasks and a grasp of key concepts in Transformer effectively.",
                        "uid": "v-full-1284",
                        "time_stamp": "2023-10-25T22:00:00Z",
                        "time_start": "2023-10-25T22:00:00Z",
                        "time_end": "2023-10-25T22:12:00Z",
                        "paper_type": "full",
                        "keywords": [
                            "Deep learning, Transformer, Visual tutorial, Explorable explanations"
                        ],
                        "doi": "10.1109/TVCG.2023.3327353",
                        "fno": "",
                        "has_image": true,
                        "has_pdf": true,
                        "paper_award": "",
                        "image_caption": "With TransforLearn, learners can gain an understanding of the Transformer structure and the process of machine translation. Input view (A) provides an interface for the text to be translated. Translation view (B) displays the translation results and current translation progress, helping users in task-driven exploration. Architecture view (C) provides an overview of model structure and data flow, with sub-views (C1-C4) that support computational processes. Once enabled, the Detailed view (C3) displays the Attention mechanism view (D), Layer normalization view (E), and Feed-forward network view (F). These views not only show the operational details but also support multiple interactions.",
                        "external_paper_link": "",
                        "youtube_ff_link": "https://youtu.be/IN6SyphYpZ0",
                        "youtube_ff_id": "IN6SyphYpZ0",
                        "bunny_ff_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1284/v-full-1284_Preview.mp4?token=4lrLGAiEE_smzWd6Ks9vWfTDL1nVykS1p7ixfVDThm4&expires=1706590800",
                        "bunny_ff_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1284/v-full-1284_Preview.vtt?token=44a_QSWX7yujfFZFLE5JMFQmUIZqRjMMEiq1DmT_6zM&expires=1706590800",
                        "youtube_prerecorded_link": "https://youtu.be/cfbs70RaxvA",
                        "youtube_prerecorded_id": "cfbs70RaxvA",
                        "bunny_prerecorded_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1284/v-full-1284_Presentation.mp4?token=xeLSFr4EGDNIcBKZurwpXluigmErGlHT7N5Zpn-ihgs&expires=1706590800",
                        "bunny_prerecorded_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1284/v-full-1284_Presentation.vtt?token=AIhdWm9oEdmcNfJy0c-ox09FgpcuDBpjcRI8DerTrfo&expires=1706590800"
                    },
                    {
                        "slot_id": "v-tvcg-10153659",
                        "session_id": "full19",
                        "title": "Creating Emordle: Animating Word Cloud for Emotion Expression",
                        "contributors": [
                            "Liwenhan Xie"
                        ],
                        "authors": [
                            "Liwenhan Xie",
                            "Xinhuan Shu",
                            "Jeon Cheol Su",
                            "Yun Wang",
                            "Siming Chen",
                            "Huamin Qu"
                        ],
                        "abstract": "We propose emordle, a conceptual design that animates wordles (compact word clouds) to deliver their emotional context to the audiences. To inform the design, we first reviewed online examples of animated texts and animated wordles, and summarized strategies for injecting emotion into the animations. We introduced a composite approach that extends an existing animation scheme for one word to multiple words in a wordle with two global factors: the randomness of text animation (entropy) and the animation speed (speed). To create an emordle, general users can choose one predefined animated scheme that matches the intended emotion class and fine-tune the emotion intensity with the two parameters. We designed proof-of-concept emordle examples for four basic emotion classes, namely happiness, sadness, anger, and fear. We conducted two controlled crowdsourcing studies to evaluate our approach. The first study confirmed that people generally agreed on the conveyed emotions from well-crafted animations, and the second one demonstrated that our identified factors helped fine-tune the delivered emotion extent. We also invited general users to create emordles on their own based on our proposed framework. Through this user study, we confirmed the effectiveness of the approach. We concluded with implications for future research opportunities of supporting emotion expression in visualizations.",
                        "uid": "v-tvcg-10153659",
                        "time_stamp": "2023-10-25T22:12:00Z",
                        "time_start": "2023-10-25T22:12:00Z",
                        "time_end": "2023-10-25T22:24:00Z",
                        "paper_type": "full",
                        "keywords": [
                            "Wordle;Animation;Affective Visualization;Authoring;Casual Visualization"
                        ],
                        "doi": "10.1109/TVCG.2023.3286392",
                        "fno": "10153659",
                        "has_image": true,
                        "has_pdf": false,
                        "paper_award": "",
                        "image_caption": "This paper presents an approach to create animated word cloud that matches intended emotion based on a simple kinetic typography.",
                        "external_paper_link": "",
                        "youtube_ff_link": "https://youtu.be/W2pP4SHV6kI",
                        "youtube_ff_id": "W2pP4SHV6kI",
                        "bunny_ff_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-tvcg/v-tvcg-10153659/v-tvcg-10153659_Preview.mp4?token=3dUPMPXNX7K-O69l35NXw6R5Q5oK_owHYGSShivQy5s&expires=1706590800",
                        "bunny_ff_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-tvcg/v-tvcg-10153659/v-tvcg-10153659_Preview.vtt?token=ip1OZISYQrbJ9HSsI-lIOMa_JaCSvzres3nud5F0VMI&expires=1706590800",
                        "youtube_prerecorded_link": "",
                        "youtube_prerecorded_id": "",
                        "bunny_prerecorded_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-tvcg/v-tvcg-10153659/v-tvcg-10153659_Presentation.mp4?token=Smg9yovHy0gHBIRC8YGw525o-Q7iUwiXrHZUxDz4X_0&expires=1706590800",
                        "bunny_prerecorded_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-tvcg/v-tvcg-10153659/v-tvcg-10153659_Presentation.vtt?token=3LbmpP-rVKu3IyId5J_daRUgeSPK7Msf8I-1iAl0Myc&expires=1706590800"
                    },
                    {
                        "slot_id": "v-tvcg-9939115",
                        "session_id": "full19",
                        "title": "DocFlow: A Visual Analytics System for Question-based Document Retrieval and Categorization",
                        "contributors": [
                            "Rui Qiu"
                        ],
                        "authors": [
                            "Rui Qiu",
                            "Yamei Tu",
                            "Yu-Shuen Wang",
                            "Po-Yin Yen",
                            "Han-Wei Shen"
                        ],
                        "abstract": "A systematic review (SR) is essential with up-to-date research evidence to support clinical decisions and practices. However, the growing literature volume makes it challenging for SR reviewers and clinicians to discover useful information efficiently. Many human-in-the-loop information retrieval approaches (HIR) have been proposed to rank documents semantically similar to users' queries and provide interactive visualizations to facilitate document retrieval. Given that the queries are mainly composed of keywords and keyphrases retrieving documents that are semantically similar to a query does not necessarily respond to the clinician's need. Clinicians still have to review many documents to find the solution. The problem motivates us to develop a visual analytics system, DocFlow, to facilitate information-seeking. One of the features of our DocFlow is accepting natural language questions. The detailed description enables retrieving documents that can answer users' questions. Additionally, clinicians often categorize documents based on their backgrounds and with different purposes (e.g., populations, treatments). Since the criteria are unknown and cannot be pre-defined in advance, existing methods can only achieve categorization by considering the entire information in documents. In contrast, by locating answers in each document, our DocFlow can intelligently categorize documents based on users' questions. The second feature of our DocFlow is a flexible interface where users can arrange a sequence of questions to customize their rules for document retrieval and categorization. The two features of this visual analytics system support a flexible information-seeking process. The case studies and the feedback from domain experts demonstrate the usefulness and effectiveness of our DocFlow.",
                        "uid": "v-tvcg-9939115",
                        "time_stamp": "2023-10-25T22:24:00Z",
                        "time_start": "2023-10-25T22:24:00Z",
                        "time_end": "2023-10-25T22:36:00Z",
                        "paper_type": "full",
                        "keywords": [
                            "Biomedical systematic review;evidence-based-practice;human-in-the-loop information retrieval;question-based document retrieval;question-based document categorization"
                        ],
                        "doi": "10.1109/TVCG.2022.3219762",
                        "fno": "9939115",
                        "has_image": true,
                        "has_pdf": false,
                        "paper_award": "",
                        "image_caption": "We introduce DocFlow, a component-based visual analytics system tailored for nuanced information seeking. Designed with modularity in mind, DocFlow empowers users to effortlessly assemble information-seeking pipelines by interlinking functional components, facilitating retrieval, categorization, and in-depth visual analysis tasks. To enhance the efficiency of retrieval, DocFlow incorporates a query-based retrieval model, ensuring documents with a high likelihood of answering the user's query are prioritized. Furthermore, with its integrated query-based categorization model, DocFlow provides users with the flexibility to dynamically group documents based on specific perspectives of interest.",
                        "external_paper_link": "",
                        "youtube_ff_link": "https://youtu.be/KolLPY6DUGI",
                        "youtube_ff_id": "KolLPY6DUGI",
                        "bunny_ff_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-tvcg/v-tvcg-9939115/v-tvcg-9939115_Preview.mp4?token=0TXexBC_zHAbv79HZW-6XuddgTZzs6yl53qIu2y8u5g&expires=1706590800",
                        "bunny_ff_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-tvcg/v-tvcg-9939115/v-tvcg-9939115_Preview.vtt?token=Eb-P_OHU39Zh4H-SAyemPPRGMsd-EC61CGc76jEsa9I&expires=1706590800",
                        "youtube_prerecorded_link": "",
                        "youtube_prerecorded_id": "",
                        "bunny_prerecorded_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-tvcg/v-tvcg-9939115/v-tvcg-9939115_Presentation.mp4?token=AkpNpjU5Ef_fzg4BJ60oBfbmJwdwgNeejIKAxI2ie8Q&expires=1706590800",
                        "bunny_prerecorded_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-tvcg/v-tvcg-9939115/v-tvcg-9939115_Presentation.vtt?token=a75j4BNmfb83bJxHD0oolq0JRmjLrStpS3cUMpEyU_M&expires=1706590800"
                    },
                    {
                        "slot_id": "v-tvcg-10015807",
                        "session_id": "full19",
                        "title": "ShortcutLens: A Visual Analytics Approach for Exploring Shortcuts in Natural Language Understanding Dataset",
                        "contributors": [
                            "Zhihua Jin"
                        ],
                        "authors": [
                            "Zhihua Jin",
                            "Xingbo Wang",
                            "Furui Cheng",
                            "Chunhui Sun",
                            "Qun Liu",
                            "Huamin Qu"
                        ],
                        "abstract": "Benchmark datasets play an important role in evaluating Natural Language Understanding (NLU) models. However, shortcuts\u2014unwanted biases in the benchmark datasets\u2014can damage the effectiveness of benchmark datasets in revealing models\u2019 real capabilities. Since shortcuts vary in coverage, productivity, and semantic meaning, it is challenging for NLU experts to systematically understand and avoid them when creating benchmark datasets. In this paper, we develop a visual analytics system, ShortcutLens, to help NLU experts explore shortcuts in NLU benchmark datasets. The system allows users to conduct multi-level exploration of shortcuts. Specifically, Statistics View helps users grasp the statistics such as coverage and productivity of shortcuts in the benchmark dataset. Template View employs hierarchical and interpretable templates to summarize different types of shortcuts. Instance View allows users to check the corresponding instances covered by the shortcuts. We conduct case studies and expert interviews to evaluate the effectiveness and usability of the system. The results demonstrate that ShortcutLens supports users in gaining a better understanding of benchmark dataset issues through shortcuts, inspiring them to create challenging and pertinent benchmark datasets.",
                        "uid": "v-tvcg-10015807",
                        "time_stamp": "2023-10-25T22:36:00Z",
                        "time_start": "2023-10-25T22:36:00Z",
                        "time_end": "2023-10-25T22:48:00Z",
                        "paper_type": "full",
                        "keywords": [
                            "Visual Analytics;Natural Language Understanding;Shortcut"
                        ],
                        "doi": "10.1109/TVCG.2023.3236380",
                        "fno": "10015807",
                        "has_image": true,
                        "has_pdf": false,
                        "paper_award": "",
                        "image_caption": "ShortcutLens is a visual analytics tool that assists NLU experts in conducting the multi-level exploration of shortcuts in NLU benchmark datasets. ShortcutLens consists of three visualization components. The Statistics View (b) helps users inspect the statistics about the benchmark dataset and shortcuts. It also allows users to conduct what-if analysis on shortcuts of interest. The Template View (c) enables users to check the relationship of shortcuts and inspect the statistics about individual shortcuts.  The Instance View (d) displays the instances covered by selected shortcuts from the Template View. They enable users to gain a better understanding of benchmark dataset issues and inspire the creation of more challenging and pertinent benchmark datasets.",
                        "external_paper_link": "",
                        "youtube_ff_link": "https://youtu.be/ZoCAiwZN4o0",
                        "youtube_ff_id": "ZoCAiwZN4o0",
                        "bunny_ff_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-tvcg/v-tvcg-10015807/v-tvcg-10015807_Preview.mp4?token=QJ0NEIlN7ywrR7gUxgTGISMXPBauLMR73WRo1rUj0YM&expires=1706590800",
                        "bunny_ff_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-tvcg/v-tvcg-10015807/v-tvcg-10015807_Preview.vtt?token=OiXQVwr0CjcCukxr2s2nTZj6-YMLF07pOzVOR4a2axQ&expires=1706590800",
                        "youtube_prerecorded_link": "https://youtu.be/TQ7mNp19Ky4",
                        "youtube_prerecorded_id": "TQ7mNp19Ky4",
                        "bunny_prerecorded_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-tvcg/v-tvcg-10015807/v-tvcg-10015807_Presentation.mp4?token=-jG3XNiubqmAeOOfb01hm9FPgKADkUkx1tPgdtUDtrM&expires=1706590800",
                        "bunny_prerecorded_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-tvcg/v-tvcg-10015807/v-tvcg-10015807_Presentation.vtt?token=haCnWwcQviexXrhiLz9R8R5s13PIa5VSOX5LEYFa3mg&expires=1706590800"
                    },
                    {
                        "slot_id": "v-tvcg-10026499",
                        "session_id": "full19",
                        "title": "XNLI: Explaining and Diagnosing NLI-based Visual Data Analysis",
                        "contributors": [
                            "Xingbo Wang"
                        ],
                        "authors": [
                            "Yingchaojie Feng",
                            "Xingbo Wang",
                            "Bo Pan",
                            "Kam Kwai Wong",
                            "Yi Ren",
                            "Shi Liu",
                            "Zihan Yan",
                            "Yuxin Ma",
                            "Huamin Qu",
                            "Wei Chen"
                        ],
                        "abstract": "Natural language interfaces (NLIs) enable users to flexibly specify analytical intentions in data visualization. However, diagnosing the visualization results without understanding the underlying generation process is challenging. Our research explores how to provide explanations for NLIs to help users locate the problems and further revise the queries. We present XNLI, an explainable NLI system for visual data analysis. The system introduces a Provenance Generator to reveal the detailed process of visual transformations, a suite of interactive widgets to support error adjustments, and a Hint Generator to provide query revision hints based on the analysis of user queries and interactions. Two usage scenarios of XNLI and a user study verify the effectiveness and usability of the system. Results suggest that XNLI can significantly enhance task accuracy without interrupting the NLI-based analysis process.",
                        "uid": "v-tvcg-10026499",
                        "time_stamp": "2023-10-25T22:48:00Z",
                        "time_start": "2023-10-25T22:48:00Z",
                        "time_end": "2023-10-25T23:00:00Z",
                        "paper_type": "full",
                        "keywords": [
                            "Natural language interface;visual data analysis;explainability"
                        ],
                        "doi": "10.1109/TVCG.2023.3240003",
                        "fno": "10026499",
                        "has_image": true,
                        "has_pdf": false,
                        "paper_award": "",
                        "image_caption": "The user interface of XNLI consists of two views, including the Data View and the Query View. Users can select or upload the dataset and explore the data attributes in the Data View. Then, they can use Query View to enter natural language queries, analyze data via charts, understand and diagnose the NLI process through interactive widgets, and gain hint feedback for query revision.",
                        "external_paper_link": "",
                        "youtube_ff_link": "https://youtu.be/5HTIcQswHls",
                        "youtube_ff_id": "5HTIcQswHls",
                        "bunny_ff_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-tvcg/v-tvcg-10026499/v-tvcg-10026499_Preview.mp4?token=1OGb2U1Y-tOk-xQNsFkKEnNZq8jUlmRHyLh3Q_g-S6E&expires=1706590800",
                        "bunny_ff_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-tvcg/v-tvcg-10026499/v-tvcg-10026499_Preview.vtt?token=Kemr0Fxn6I4p99KSR-OUnHT7nB83l9vrjn6p2-Exk60&expires=1706590800",
                        "youtube_prerecorded_link": "https://youtu.be/bCM6E5iRgts",
                        "youtube_prerecorded_id": "bCM6E5iRgts",
                        "bunny_prerecorded_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-tvcg/v-tvcg-10026499/v-tvcg-10026499_Presentation.mp4?token=I-pXWq1e_RxNQj4w1CNAGW4RuLhoS0PkkVBIvP9XsTc&expires=1706590800",
                        "bunny_prerecorded_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-tvcg/v-tvcg-10026499/v-tvcg-10026499_Presentation.vtt?token=J9HrjAHZIC-aSk1c_gmp9joy6qyoUrapP5DH32sCrdY&expires=1706590800"
                    },
                    {
                        "slot_id": "v-full-1510",
                        "session_id": "full19",
                        "title": "Large-Scale Evaluation of Topic Models and Dimensionality Reductions for 2D Text Spatialization",
                        "contributors": [
                            "Daniel Atzberger"
                        ],
                        "authors": [
                            "Daniel Atzberger",
                            "Tim Cech",
                            "Rico Richter",
                            "Matthias Trapp",
                            "Willy Scheibel",
                            "J\u00fcrgen D\u00f6llner",
                            "Tobias Schreck"
                        ],
                        "abstract": "Topic models are a class of unsupervised learning algorithms for detecting the semantic structure within a text corpus. Together with a subsequent dimensionality reduction algorithm, topic models can be used for deriving spatializations for text corpora as two-dimensional scatter plots, reflecting semantic similarity between the documents and supporting corpus analysis. Although the choice of the topic model, the dimensionality reduction, and their underlying hyperparameters significantly impact the resulting layout, it is unknown which particular combinations result in high-quality layouts with respect to accuracy and perception metrics. To investigate the effectiveness of topic models and dimensionality reduction methods for the spatialization of corpora as two-dimensional scatter plots (or basis for landscape-type visualizations), we present a large-scale, benchmark-based computational evaluation. Our evaluation consists of (1) a set of corpora, (2) a set of layout algorithms that are combinations of topic models and dimensionality reductions, and (3) quality metrics for quantifying the resulting layout. The corpora are given as  document-term matrices, and each document is assigned to a thematic class. The chosen metrics quantify the preservation of local and global properties and the perceptual effectiveness of the two-dimensional scatter plots. By evaluating the benchmark on a computing cluster, we derived a multivariate dataset with over 45000 individual layouts and corresponding quality metrics. Based on the results, we propose guidelines for the effective design of text spatializations that are based on topic models and dimensionality reductions. As a main result, we show that interpretable topic models are beneficial for capturing the structure of text corpora. We furthermore recommend the use of t-SNE as a subsequent dimensionality reduction.",
                        "uid": "v-full-1510",
                        "time_stamp": "2023-10-25T23:00:00Z",
                        "time_start": "2023-10-25T23:00:00Z",
                        "time_end": "2023-10-25T23:12:00Z",
                        "paper_type": "full",
                        "keywords": [
                            "Text visualization, spatialization, dimensionality reduction algorithms, topic modeling"
                        ],
                        "doi": "10.1109/TVCG.2023.3326569",
                        "fno": "",
                        "has_image": true,
                        "has_pdf": true,
                        "paper_award": "",
                        "image_caption": "Two-dimensional layout for the 20 Newsgroups dataset. Each point represents a document within the corpus and  the color its class. The layout originates from applying Latent Semantic Indexing to the term-document matrix and  a subsequent application of t-SNE on the topics, which are then aggregated to the document positions.",
                        "external_paper_link": "",
                        "youtube_ff_link": "https://youtu.be/gLMIy-ea8qU",
                        "youtube_ff_id": "gLMIy-ea8qU",
                        "bunny_ff_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1510/v-full-1510_Preview.mp4?token=hqSLl0iCL-2FfUozCxfClP6Dm42y9d6VfeWj6a72hFg&expires=1706590800",
                        "bunny_ff_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1510/v-full-1510_Preview.vtt?token=IiW9jWUz05ap43z214H_le4i1QZ9psEVJ5yIrHYDYdU&expires=1706590800",
                        "youtube_prerecorded_link": "https://youtu.be/E2EacKmpdhI",
                        "youtube_prerecorded_id": "E2EacKmpdhI",
                        "bunny_prerecorded_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1510/v-full-1510_Presentation.mp4?token=U-FpMTEIn4y1sTPr3uZAVApYM4VVptECgZNiSRjqaC4&expires=1706590800",
                        "bunny_prerecorded_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1510/v-full-1510_Presentation.vtt?token=6CewsmDUBKwzs428d78356VTSFmJuKJbjawyZn8DblE&expires=1706590800"
                    }
                ]
            },
            {
                "title": "Perception",
                "session_id": "full20",
                "event_prefix": "v-full",
                "track": "oneohfive",
                "session_image": "full20.png",
                "chair": [
                    "Cindy Xiong Bearfield"
                ],
                "time_start": "2023-10-26T03:00:00Z",
                "time_end": "2023-10-26T04:15:00Z",
                "discord_category": "",
                "discord_channel": "105",
                "discord_channel_id": "1161741309346861067",
                "discord_link": "https://discordapp.com/channels/1161400451619627068/1161741309346861067",
                "zoom_private_meeting": "",
                "zoom_private_password": "",
                "zoom_private_link": "",
                "zoom_broadcast_link": "",
                "ff_link": "https://youtu.be/ZNjYvA7m7l0",
                "time_slots": [
                    {
                        "slot_id": "v-tvcg-9978718",
                        "session_id": "full20",
                        "title": "The Risks of Ranking: Revisiting Graphical Perception to Model Individual Differences in Visualization Performance",
                        "contributors": [
                            "Yiren Ding"
                        ],
                        "authors": [
                            "Russell Davis",
                            "Xiaoying Pu",
                            "Yiren Ding",
                            "Brian D. Hall",
                            "Karen Bonilla",
                            "Mi Feng",
                            "Matthew Kay",
                            "Lane Harrison"
                        ],
                        "abstract": "Graphical perception studies typically measure visualization encoding effectiveness using the error of an \u201caverage observer\u201d, leading to canonical rankings of encodings for numerical attributes: e.g., position > area > angle > volume. Yet different people may vary in their ability to read different visualization types, leading to variance in this ranking across individuals not captured by population-level metrics using \u201caverage observer\u201d models. One way we can bridge this gap is by recasting classic visual perception tasks as tools for assessing individual performance, in addition to overall visualization performance. In this paper we replicate and extend Cleveland and McGill\u2019s graphical comparison experiment using Bayesian multilevel regression, using these models to explore individual differences in visualization skill from multiple perspectives. The results from experiments and modeling indicate that some people show patterns of accuracy that credibly deviate from the canonical rankings of visualization effectiveness. We discuss implications of these findings, such as a need for new ways to communicate visualization effectiveness to designers, how patterns in individuals\u2019 responses may show systematic biases and strategies in visualization judgment, and how recasting classic visual perception tasks as tools for assessing individual performance may offer new ways to quantify aspects of visualization literacy. Experiment data, source code, and analysis scripts are available at the following repository: https://osf.io/8ub7t/?view_only=9be4798797404a4397be3c6fc2a68cc0.",
                        "uid": "v-tvcg-9978718",
                        "time_stamp": "2023-10-26T03:00:00Z",
                        "time_start": "2023-10-26T03:00:00Z",
                        "time_end": "2023-10-26T03:12:00Z",
                        "paper_type": "full",
                        "keywords": [
                            "visualization;graphical perception;individual differences"
                        ],
                        "doi": "10.1109/TVCG.2022.3226463",
                        "fno": "9978718",
                        "has_image": true,
                        "has_pdf": false,
                        "paper_award": "",
                        "image_caption": "Graphical perception studies traditionally assess visualization encoding effectiveness using an \u201caverage observer,\u201d establishing classic rankings like position > area > angle > volume for numerical attributes. However, people\u2019s individual abilities to interpret different visualizations vary, challenging this one-size-fits-all approach.  To address this, this study replicates a classic graphical perception tasks and uses Bayesian modeling to evaluate individual differences in chart performance. The results reveal diverse visualization skills among individuals and suggests a need for better ways to communicate visualization effectiveness to designers.",
                        "external_paper_link": "",
                        "youtube_ff_link": "https://youtu.be/v29ewjbLXK4",
                        "youtube_ff_id": "v29ewjbLXK4",
                        "bunny_ff_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-tvcg/v-tvcg-9978718/v-tvcg-9978718_Preview.mp4?token=bJe8OTbV9Q-NOkQG_9ejnpYxoP1SjGQXM6Gg4Wt0T6k&expires=1706590800",
                        "bunny_ff_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-tvcg/v-tvcg-9978718/v-tvcg-9978718_Preview.vtt?token=NagTcsrHLB-waoZcoflSCHKtvDzBLHtzYHCsPQlI8qE&expires=1706590800",
                        "youtube_prerecorded_link": "https://youtu.be/dJlG2qgpndU",
                        "youtube_prerecorded_id": "dJlG2qgpndU",
                        "bunny_prerecorded_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-tvcg/v-tvcg-9978718/v-tvcg-9978718_Presentation.mp4?token=EMM-xRSnMQRE1iHtkIX_Bb4YeiaWuAZ8y0qpTYOdxEs&expires=1706590800",
                        "bunny_prerecorded_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-tvcg/v-tvcg-9978718/v-tvcg-9978718_Presentation.vtt?token=vWvIawT1fJkgD1_Me09iwcPfLfLjZ9F7PV9gSCJed2s&expires=1706590800"
                    },
                    {
                        "slot_id": "v-full-1229",
                        "session_id": "full20",
                        "title": "Design Characterization for Black-and-White Textures in Visualization",
                        "contributors": [
                            "Tingying He"
                        ],
                        "authors": [
                            "Tingying He",
                            "Yuanyang Zhong",
                            "Petra Isenberg",
                            "Tobias Isenberg"
                        ],
                        "abstract": "We investigate the use of 2D black-and-white textures for the visualization of categorical data and contribute a summary of texture attributes, and the results of three experiments that elicited design strategies as well as aesthetic and effectiveness measures. Black-and-white textures are useful, for instance, as a visual channel for categorical data on low-color displays, in 2D/3D print, to achieve the aesthetic of historic visualizations, or to retain the color hue channel for other visual mappings. We specifically study how to use what we call geometric and iconic textures. Geometric textures use patterns of repeated abstract geometric shapes, while iconic textures use repeated icons that may stand for data categories. We parameterized both types of textures and developed a tool for designers to create textures on simple charts by adjusting texture parameters. 30 visualization experts used our tool and designed 66 textured bar charts, pie charts, and maps. We then had 150 participants rate these designs for aesthetics. Finally, with the top-rated geometric and iconic textures, our perceptual assessment experiment with 150 participants revealed that textured charts perform about equally well as non-textured charts, and that there are some differences depending on the type of chart.",
                        "uid": "v-full-1229",
                        "time_stamp": "2023-10-26T03:12:00Z",
                        "time_start": "2023-10-26T03:12:00Z",
                        "time_end": "2023-10-26T03:24:00Z",
                        "paper_type": "full",
                        "keywords": [
                            "Aesthetics, textures, icons, black and white, visualization, visual representations, categorical data, design, perception"
                        ],
                        "doi": "10.1109/TVCG.2023.3326941",
                        "fno": "",
                        "has_image": true,
                        "has_pdf": true,
                        "paper_award": "",
                        "image_caption": "The bar chart, pie chart designs with geometric and iconic textures with the highest ratings in Experiment 2.",
                        "external_paper_link": "",
                        "youtube_ff_link": "https://youtu.be/YSZAgpoddtA",
                        "youtube_ff_id": "YSZAgpoddtA",
                        "bunny_ff_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1229/v-full-1229_Preview.mp4?token=_r_kbWAp-y4mPaicz9InD2xzoz56bvDZ8tvlq7q2fEY&expires=1706590800",
                        "bunny_ff_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1229/v-full-1229_Preview.vtt?token=0_XPY3OaMNRKSKuC-utnkw9FSnEGCx2Iq4xZBDMsNmI&expires=1706590800",
                        "youtube_prerecorded_link": "https://youtu.be/5nZ_v7C8xog",
                        "youtube_prerecorded_id": "5nZ_v7C8xog",
                        "bunny_prerecorded_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1229/v-full-1229_Presentation.mp4?token=VLmkbq2nu93OYC86bfbHtgVg3jsGgHg_SNFvkZ2hvdw&expires=1706590800",
                        "bunny_prerecorded_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1229/v-full-1229_Presentation.vtt?token=vMdvlzLYVvtYtEGQ5FolgHgaANJ0Q-NP07djwN2c9us&expires=1706590800"
                    },
                    {
                        "slot_id": "v-full-1250",
                        "session_id": "full20",
                        "title": "Image or Information? Examining the Nature and Impact of Visualization Perceptual Classification",
                        "contributors": [
                            "Anjana Arunkumar"
                        ],
                        "authors": [
                            "Anjana Arunkumar",
                            "Lace M. Padilla",
                            "Gi-Yeul Bae",
                            "Chris Bryan"
                        ],
                        "abstract": "How do people internalize visualizations: as images or information? In this study, we investigate the nature of internalization for visualizations (i.e., how the mind encodes visualizations in memory) and how memory encoding affects its retrieval. This exploratory work examines the influence of various design elements on a user's perception of a chart. Specifically, which design elements lead to perceptions of visualization as an image (aims to provide visual references, evoke emotions, express creativity, and inspire philosophic thought) or as information (aims to present complex data, information, or ideas concisely and promote analytical thinking)? Understanding how design elements contribute to viewers perceiving a visualization more as an image or information will help designers decide which elements to include to achieve their communication goals. For this study, we annotated 500 visualizations and analyzed the responses of 250 online participants, who rated the visualizations on a bilinear scale as 'image' or 'information.' We then conducted an in-person study (n = 101) using a free recall task to examine how the image/information ratings and design elements impacted memory. The results revealed several interesting findings: Image-rated visualizations were perceived as more aesthetically 'appealing,' 'enjoyable,' and 'pleasing.'  Information-rated visualizations were perceived as less 'difficult to understand' and more aesthetically 'likable' and 'nice,' though participants expressed higher 'positive' sentiment when viewing image-rated visualizations and felt less 'guided to a conclusion.' The presence of axes and text annotations heavily influenced the likelihood of participants rating the visualization as 'information.' We also found different patterns among participants that were older. Importantly, we show that visualizations internalized as 'images' are less effective in conveying trends and messages, though they elicit a more positive emotional judgment, while 'informative' visualizations exhibit annotation focused recall and elicit a more positive design judgment. We discuss the implications of this dissociation between aesthetic pleasure and perceived ease of use in visualization design.",
                        "uid": "v-full-1250",
                        "time_stamp": "2023-10-26T03:24:00Z",
                        "time_start": "2023-10-26T03:24:00Z",
                        "time_end": "2023-10-26T03:36:00Z",
                        "paper_type": "full",
                        "keywords": [
                            "Information Visualization; Human-Centered Computing; Perception & Cognition; Takeaways."
                        ],
                        "doi": "10.1109/TVCG.2023.3326919",
                        "fno": "",
                        "has_image": true,
                        "has_pdf": true,
                        "paper_award": "",
                        "image_caption": "Visualization design faces challenges in reconciling good empirical results and rules of thumb with good exemplars of real-world visualizations. To disentangle these confounding factors, we set out to answer a fundamental question: how do people actually internalize visualizations: as image or information? We present visualization to be rated on a bilinear scale as 'image' or 'information', and investigate the agreement between externalized ratings and internalized memory through free recall.",
                        "external_paper_link": "",
                        "youtube_ff_link": "",
                        "youtube_ff_id": "",
                        "bunny_ff_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1250/v-full-1250_Preview.mp4?token=Y7-TpQpnwjGbMr-5TKUzjbIHKtB2Oz2WxQaLiiG1f-E&expires=1706590800",
                        "bunny_ff_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1250/v-full-1250_Preview.vtt?token=YqSCUWMQoki9CXiBjH5t1rsPtsw_cTIF8RuA-XCfClg&expires=1706590800",
                        "youtube_prerecorded_link": "https://youtu.be/XaOcp1Cdb04",
                        "youtube_prerecorded_id": "XaOcp1Cdb04",
                        "bunny_prerecorded_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1250/v-full-1250_Presentation.mp4?token=fAcFCOCCi0hFC0DAr3DUfe_YNgRQOsvfpblYHQ1DQXc&expires=1706590800",
                        "bunny_prerecorded_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1250/v-full-1250_Presentation.vtt?token=xuj6V4f9OIciVPf57lyXaUr5jvDFh6DjzT7zqHY4q9U&expires=1706590800"
                    },
                    {
                        "slot_id": "v-full-1655",
                        "session_id": "full20",
                        "title": "Perception of Line Attributes for Visualization",
                        "contributors": [
                            "Anna Sterzik"
                        ],
                        "authors": [
                            "Anna Sterzik",
                            "Nils Lichtenberg",
                            "Jana Wilms",
                            "Michael Krone",
                            "Douglas Cunningham",
                            "Kai Lawonn"
                        ],
                        "abstract": "Line attributes such as width and dashing are commonly used to encode information. However, many questions on the perception of line attributes remain, such as how many levels of attribute variation can be distinguished or which line attributes are the preferred choices for which tasks. We conducted three studies to develop guidelines for using stylized lines to encode scalar data. In our first study, participants drew stylized lines to encode uncertainty information. Uncertainty is usually visualized alongside other data. Therefore, alternative visual channels are important for the visualization of uncertainty. Additionally, uncertainty\u2014e.g., in weather forecasts\u2014is a familiar topic to most people. Thus, we picked it for our visualization scenarios in study 1. We used the results of our study to determine the most common line attributes for drawing uncertainty: Dashing, luminance, wave amplitude, and width. While those line attributes were especially common for drawing uncertainty, they are also commonly used in other areas. In studies 2 and 3, we investigated the discriminability of the line attributes determined in study 1. Studies 2 and 3 did not require specific application areas; thus, their results apply to visualizing any scalar data in line attributes. We evaluated the just-noticeable differences (JND) and derived recommendations for perceptually distinct line levels. We found that participants could discriminate considerably more levels for the line attribute width than for wave amplitude, dashing, or luminance.",
                        "uid": "v-full-1655",
                        "time_stamp": "2023-10-26T03:36:00Z",
                        "time_start": "2023-10-26T03:36:00Z",
                        "time_end": "2023-10-26T03:48:00Z",
                        "paper_type": "full",
                        "keywords": [
                            "Line Drawings, Line Stylization, Perceptual Evaluation, Uncertainty Visualization."
                        ],
                        "doi": "10.1109/TVCG.2023.3326523",
                        "fno": "",
                        "has_image": true,
                        "has_pdf": true,
                        "paper_award": "",
                        "image_caption": "We investigated the perception of line attributes for visualization.  The line attributes in the figure were the most popular in our drawing study.  We investigated their discriminability in two further studies.  From top to bottom: two types of dashing, luminance, waves, and width.",
                        "external_paper_link": "",
                        "youtube_ff_link": "https://youtu.be/O-mD127e7nY",
                        "youtube_ff_id": "O-mD127e7nY",
                        "bunny_ff_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1655/v-full-1655_Preview.mp4?token=q2IG9lmZXbHKyuww-iIc1qWW20xvCEYfrwY1O-16Gj0&expires=1706590800",
                        "bunny_ff_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1655/v-full-1655_Preview.vtt?token=--vKQC5GOr4Hmgv_zP59HURrD1CtugS3hR7Ga6q-hxU&expires=1706590800",
                        "youtube_prerecorded_link": "https://youtu.be/BwN9YtwhkcM",
                        "youtube_prerecorded_id": "BwN9YtwhkcM",
                        "bunny_prerecorded_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1655/v-full-1655_Presentation.mp4?token=bPfnoJi4QIMTCID2Y0BdpFEG4F95sAdoVBhKXFbhZR8&expires=1706590800",
                        "bunny_prerecorded_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1655/v-full-1655_Presentation.vtt?token=EdT3bmAxWzskKnlhTdUqoqr7l7AFJlqOa4Jfj_H8SkU&expires=1706590800"
                    },
                    {
                        "slot_id": "v-full-1509",
                        "session_id": "full20",
                        "title": "Perceptually Uniform Construction of Illustrative Textures",
                        "contributors": [
                            "Anna Sterzik"
                        ],
                        "authors": [
                            "Anna Sterzik",
                            "Monique Meuschke",
                            "Douglas Cunningham",
                            "Kai Lawonn"
                        ],
                        "abstract": "Illustrative textures, such as stippling or hatching, were predominantly used as an alternative to conventional Phong rendering. Recently, the potential of encoding information on surfaces or maps using different densities has also been recognized. This has the significant advantage that additional color can be used as another visual channel and the illustrative textures can then be overlaid. Effectively, it is thus possible to display multiple information, such as two different scalar fields on surfaces simultaneously. In previous work, these textures were manually generated and the choice of density was unempirically determined. Here, we first want to determine and understand the perceptual space of illustrative textures. We chose a succession of simplices with increasing dimensions as primitives for our textures: Dots, lines, and triangles. Thus, we explore the texture types of stippling, hatching, and triangles. We create a range of textures by sampling the density space uniformly. Then, we conduct three perceptual studies in which the participants performed pairwise comparisons for each texture type. We use multidimensional scaling (MDS) to analyze the perceptual spaces per category. The perception of stippling and triangles seems relatively similar. Both are adequately described by a 1D manifold in 2D space. The perceptual space of hatching consists of two main clusters: Crosshatched textures, and textures with only one hatching direction. However, the perception of hatching textures with only one hatching direction is similar to the perception of stippling and triangles. Based on our findings, we construct perceptually uniform illustrative textures. Afterwards, we provide concrete application examples for the constructed textures.",
                        "uid": "v-full-1509",
                        "time_stamp": "2023-10-26T03:48:00Z",
                        "time_start": "2023-10-26T03:48:00Z",
                        "time_end": "2023-10-26T04:00:00Z",
                        "paper_type": "full",
                        "keywords": [
                            "Illustrative Visualization, Perceptual Evaluation, Hatching, Stippling."
                        ],
                        "doi": "10.1109/TVCG.2023.3326574",
                        "fno": "",
                        "has_image": true,
                        "has_pdf": true,
                        "paper_award": "",
                        "image_caption": "We studied the perceptual spaces of illustrative textures (stippling, hatching, triangles) and extracted perceptually uniform texture levels by reparameterizing them.  The image displays five texture levels generated for each texture type using our reparameterization.",
                        "external_paper_link": "",
                        "youtube_ff_link": "https://youtu.be/CGu5KzFO7-w",
                        "youtube_ff_id": "CGu5KzFO7-w",
                        "bunny_ff_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1509/v-full-1509_Preview.mp4?token=Ls5Ykz0G_Xh_M5fu9F87BoVjEDcB_UhRWuV6UqMSuvw&expires=1706590800",
                        "bunny_ff_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1509/v-full-1509_Preview.vtt?token=dHuFEvSwEvw95f09zFw2kRouqeBu-3n9cfFEXyhwxt8&expires=1706590800",
                        "youtube_prerecorded_link": "https://youtu.be/pWsjetlz5pQ",
                        "youtube_prerecorded_id": "pWsjetlz5pQ",
                        "bunny_prerecorded_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1509/v-full-1509_Presentation.mp4?token=qqcXEK2sO-kGYddrrxY9mZcrErv9r6wRA7oMSWSaYJI&expires=1706590800",
                        "bunny_prerecorded_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1509/v-full-1509_Presentation.vtt?token=819bh5wGgxiKdsb7EXvOdDKri7HMrnpZ-pdWhmxdjvE&expires=1706590800"
                    },
                    {
                        "slot_id": "v-full-1016",
                        "session_id": "full20",
                        "title": "Too Many Cooks: Exploring How Graphical Perception Studies Influence Visualization Recommendations in Draco",
                        "contributors": [
                            "Leilani Battle"
                        ],
                        "authors": [
                            "Zehua Zeng",
                            "Junran Yang",
                            "Dominik Moritz",
                            "Jeffrey Heer",
                            "Leilani Battle"
                        ],
                        "abstract": "Findings from graphical perception can guide visualization recommendation algorithms in identifying effective visualization designs. However, existing algorithms use knowledge from, at best, a few studies, limiting our understanding of how complementary (or contradictory) graphical perception results influence generated recommendations. In this paper, we present a pipeline of applying a large body of graphical perception results to develop new visualization recommendation algorithms and conduct an exploratory study to investigate how results from graphical perception can alter the behavior of downstream algorithms. Specifically, we model graphical perception results from 30 papers in Draco---a framework to model visualization knowledge---to develop new recommendation algorithms. By analyzing Draco-generated algorithms, we showcase the feasibility of our method to (1) identify gaps in existing graphical perception literature informing recommendation algorithms, (2) cluster papers by their preferred design rules and constraints, and (3) investigate why certain studies can dominate Draco's recommendations, whereas others may have little influence. Given our findings, we discuss the potential for mutually reinforcing advancements in graphical perception and visualization recommendation research.",
                        "uid": "v-full-1016",
                        "time_stamp": "2023-10-26T04:00:00Z",
                        "time_start": "2023-10-26T04:00:00Z",
                        "time_end": "2023-10-26T04:12:00Z",
                        "paper_type": "full",
                        "keywords": [
                            "Graphical Perception Studies;Visualization Recommendation Algorithms"
                        ],
                        "doi": "10.1109/TVCG.2023.3326527",
                        "fno": "",
                        "has_image": true,
                        "has_pdf": true,
                        "paper_award": "",
                        "image_caption": "In this paper, we present a pipeline of applying a large body of graphical perception results to develop new visualization recommendation algorithms and conduct an exploratory study to investigate how results from graphical perception can alter the behaviors and outputs of downstream algorithms. Given our findings, we discuss the potential for mutually reinforcing advancements in graphical perception and visualization recommendation research.",
                        "external_paper_link": "",
                        "youtube_ff_link": "https://youtu.be/VDA1aW1tTfY",
                        "youtube_ff_id": "VDA1aW1tTfY",
                        "bunny_ff_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1016/v-full-1016_Preview.mp4?token=3KH3Ft6QWy3IGyFFe3joDPb7KNlwyo3jS2Ye1-xW8tc&expires=1706590800",
                        "bunny_ff_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1016/v-full-1016_Preview.vtt?token=xlFYlaoT4X-63PvNRRRYk5MpcaiwQrPTFWp2wJcpGSs&expires=1706590800",
                        "youtube_prerecorded_link": "https://youtu.be/9oZ6MiFDud8",
                        "youtube_prerecorded_id": "9oZ6MiFDud8",
                        "bunny_prerecorded_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1016/v-full-1016_Presentation.mp4?token=XDsRIVMGfg0q40N-X80pVWwWCWavMAI4D-j8qFT5Qh4&expires=1706590800",
                        "bunny_prerecorded_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1016/v-full-1016_Presentation.vtt?token=OfLHq8fmZ50MbyHtqcXvNXBF2fMR-subJEeaGi62DYQ&expires=1706590800"
                    }
                ]
            },
            {
                "title": "Scientific Visualization",
                "session_id": "full21",
                "event_prefix": "v-full",
                "track": "oneohthree",
                "session_image": "full21.png",
                "chair": [
                    "Ingrid Hotz"
                ],
                "time_start": "2023-10-26T22:00:00Z",
                "time_end": "2023-10-26T23:15:00Z",
                "discord_category": "",
                "discord_channel": "103",
                "discord_channel_id": "1161741183815524502",
                "discord_link": "https://discordapp.com/channels/1161400451619627068/1161741183815524502",
                "zoom_private_meeting": "",
                "zoom_private_password": "",
                "zoom_private_link": "",
                "zoom_broadcast_link": "",
                "ff_link": "https://youtu.be/yYYony0nBEQ",
                "time_slots": [
                    {
                        "slot_id": "v-tvcg-9905473",
                        "session_id": "full21",
                        "title": "Electromechanical Coupling in Electroactive Polymers - a Visual Analysis of a Third-Order Tensor Field",
                        "contributors": [
                            "Chiara Hergl"
                        ],
                        "authors": [
                            "Chiara Hergl",
                            "Carina Witt",
                            "Baldwin Nsonga",
                            "Andreas Menzel",
                            "Gerik Scheuermann"
                        ],
                        "abstract": "Electroactive polymers are frequently used in engineering applications due to their ability to change their shape and properties under the influence of an electric field. This process also works vice versa such that a mechanical deformation of the material induces an electric field in the EAP device. This specific behaviour makes such materials highly attractive for the construction of actuators and sensors in various application areas. The electromechanical behaviour of electroactive polymers can be described by a third-order coupling tensor which represents the sensitivity of mechanical stresses with respect to the electric field, i.e. it establishes a relation between a second-order and a first-order tensor field. Due to the complexity of this coupling tensor and to the lack of meaningful visualization methods for third-order tensors in general, an interpretation of the tensor is rather difficult. Thus, the central engineering research question that this contribution deals with, is a deeper understanding of the electromechanical coupling by analyzing the third-order coupling tensor with the help of specific visualization methods. Starting with a deviatoric decomposition of the tensor, the multipoles of each deviator are visualized, which allows a first insight into this highly complex third-order tensor. In the present contribution, four examples including electromechanical coupling are simulated within a finite element framework and subsequently analyzed by using the tensor visualization method.",
                        "uid": "v-tvcg-9905473",
                        "time_stamp": "2023-10-26T22:00:00Z",
                        "time_start": "2023-10-26T22:00:00Z",
                        "time_end": "2023-10-26T22:12:00Z",
                        "paper_type": "full",
                        "keywords": [
                            "Tensor visualization;third-order tensor;deviatoric decomposition;electro-active polymer"
                        ],
                        "doi": "10.1109/TVCG.2022.3209328",
                        "fno": "9905473",
                        "has_image": true,
                        "has_pdf": false,
                        "paper_award": "",
                        "image_caption": "Multipole Glyph visualization of a bioinspired tunable lens out of electroactive polymers. Electroactive polymers are materials that change their shape under the influence of an electric field. This material can be described by the third-order coupling tensor. Three-dimensional tensors of arbitrary order or symmetry can be decomposed using the deviatoric decomposition. Deviators can then be represented by a set of so-called multipoles and a scalar. These multipoles are, except for an even number of sign changes, unique vectors that are used to design a glyph in a split view.",
                        "external_paper_link": "",
                        "youtube_ff_link": "https://youtu.be/xPLdI5T8IiY",
                        "youtube_ff_id": "xPLdI5T8IiY",
                        "bunny_ff_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-tvcg/v-tvcg-9905473/v-tvcg-9905473_Preview.mp4?token=YbQ-bYRTm7P4L_gH0H7ugrmTyAhDDZyO6Bgqga9mLhA&expires=1706590800",
                        "bunny_ff_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-tvcg/v-tvcg-9905473/v-tvcg-9905473_Preview.vtt?token=6J7OtHi3lJKAjch9Z11uBE_OdyAem9c8_96dIOifAPo&expires=1706590800",
                        "youtube_prerecorded_link": "https://youtu.be/LBhm-WORfmc",
                        "youtube_prerecorded_id": "LBhm-WORfmc",
                        "bunny_prerecorded_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-tvcg/v-tvcg-9905473/v-tvcg-9905473_Presentation.mp4?token=avlnNtnyWsEi3IOCgmNJEmrlzLNVxZY4fulX4-1TfdU&expires=1706590800",
                        "bunny_prerecorded_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-tvcg/v-tvcg-9905473/v-tvcg-9905473_Presentation.vtt?token=5QRMI4rCqX9Fj2EIpR5Q1n2tZUCXu3hr8ZmHGKETucE&expires=1706590800"
                    },
                    {
                        "slot_id": "v-tvcg-9992117",
                        "session_id": "full21",
                        "title": "GPU Accelerated 3D Tomographic Reconstruction and Visualization from Noisy Electron Microscopy Tilt-Series",
                        "contributors": [
                            "Julio A Rey Ramirez"
                        ],
                        "authors": [
                            "Julio Rey Ramirez,Peter Rautek,Ciril Bohak,Ondrej Strnad,Zheyuan Zhang,Sai Li,Ivan Viola,Wolfgang Heidrich"
                        ],
                        "abstract": "We present a novel framework for 3D tomographic reconstruction and visualization of tomograms from noisy electron microscopy tilt-series. Our technique takes as an input aligned tilt-series from cryogenic electron microscopy and creates denoised 3D tomograms using a proximal jointly-optimized approach that iteratively performs reconstruction and denoising, relieving the users of the need to select appropriate denoising algorithms in the pre-reconstruction or post-reconstruction steps. The whole process is accelerated by exploiting parallelism on modern GPUs, and the results can be visualized immediately after the reconstruction using volume rendering tools incorporated in the framework. We show that our technique can be used with multiple combinations of reconstruction algorithms and regularizers, thanks to the flexibility provided by proximal algorithms. Additionally, the reconstruction framework is open-source and can be easily extended with additional reconstruction and denoising methods. Furthermore, our approach enables visualization of reconstruction error throughout the iterative process within the reconstructed tomogram and on projection planes of the input tilt-series. We evaluate our approach in comparison with state-of-the-art approaches and additionally show how our error visualization can be used for reconstruction evaluation.",
                        "uid": "v-tvcg-9992117",
                        "time_stamp": "2023-10-26T22:12:00Z",
                        "time_start": "2023-10-26T22:12:00Z",
                        "time_end": "2023-10-26T22:24:00Z",
                        "paper_type": "full",
                        "keywords": [
                            "tomographic reconstruction;electron tomography;tilt-series;visualization;cryo-ET;GPU acceleration"
                        ],
                        "doi": "10.1109/TVCG.2022.3230445",
                        "fno": "9992117",
                        "has_image": true,
                        "has_pdf": false,
                        "paper_award": "",
                        "image_caption": "Left: 3D rendering of a volume obtained with standard cryo-ET reconstruction techniques. Right: 3D rendering of a reconstruction of the same tilt-series using our framework.",
                        "external_paper_link": "",
                        "youtube_ff_link": "https://youtu.be/PFfV91NSiYw",
                        "youtube_ff_id": "PFfV91NSiYw",
                        "bunny_ff_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-tvcg/v-tvcg-9992117/v-tvcg-9992117_Preview.mp4?token=NY7za_0LjEzD6FtLlKY1rI_O3vambzHAMg5ZGujX0yg&expires=1706590800",
                        "bunny_ff_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-tvcg/v-tvcg-9992117/v-tvcg-9992117_Preview.vtt?token=3ydDyoTL-FojH63fZhebx3tvvra3sgDkyIrwogg1ets&expires=1706590800",
                        "youtube_prerecorded_link": "https://youtu.be/xb16AVEAYtw",
                        "youtube_prerecorded_id": "xb16AVEAYtw",
                        "bunny_prerecorded_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-tvcg/v-tvcg-9992117/v-tvcg-9992117_Presentation.mp4?token=xd59kWQt-t87Chsy09appGkxPCl7U-z2u9R_AcsPhuE&expires=1706590800",
                        "bunny_prerecorded_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-tvcg/v-tvcg-9992117/v-tvcg-9992117_Presentation.vtt?token=vWoTPuytvEhZdg6op7dGX29kB9YbXCvmvg-yHkaUaWM&expires=1706590800"
                    },
                    {
                        "slot_id": "v-tvcg-10091196",
                        "session_id": "full21",
                        "title": "RadVolViz: An Information Display-Inspired Transfer Function Editor for Multivariate Volume Visualization",
                        "contributors": [
                            "Klaus Mueller"
                        ],
                        "authors": [
                            "Ayush Kumar",
                            "Xinyu Zhang",
                            "Huolin L. Xin",
                            "Hanfei Yan",
                            "Xiaojing Huang",
                            "Wei Xu",
                            "Klaus Mueller"
                        ],
                        "abstract": "In volume visualization transfer functions are widely used for mapping voxel properties to color and opacity. Typically, volume density data are scalars which require simple 1D transfer functions to achieve this mapping. If the volume densities are vectors of three channels, one can straightforwardly map each channel to either red, green or blue, which requires a trivial extension of the 1D transfer function editor. We devise a new method that applies to volume data with more than three channels. These types of data often arise in scientific scanning applications, where the data are separated into spectral bands or chemical elements. Our method expands on prior work in which a multivariate information display, RadViz, was fused with a radial color map, in order to visualize multi-band 2D images. In this work, we extend this joint interface to blended volume rendering. The information display allows users to recognize the presence and value distribution of the multivariate voxels and the joint volume rendering display visualizes their spatial distribution. We design a set of operators and lenses that allow users to interactively control the mapping of the multivariate voxels to opacity and color. This enables users to isolate or emphasize volumetric structures with desired multivariate properties. Furthermore, it turns out that our method also enables more insightful displays even for RGB data. We demonstrate our method with three datasets obtained from spectral electron microscopy, high energy X-ray scanning, and atmospheric science.",
                        "uid": "v-tvcg-10091196",
                        "time_stamp": "2023-10-26T22:24:00Z",
                        "time_start": "2023-10-26T22:24:00Z",
                        "time_end": "2023-10-26T22:36:00Z",
                        "paper_type": "full",
                        "keywords": [
                            "Battery;color mapping;multi channel data;multivariate data;transfer function;volume rendering;volume visualization"
                        ],
                        "doi": "10.1109/TVCG.2023.3263856",
                        "fno": "10091196",
                        "has_image": true,
                        "has_pdf": false,
                        "paper_award": "",
                        "image_caption": "An Information Display-Inspired Transfer Function Editor for Multivariate Volume Visualization",
                        "external_paper_link": "",
                        "youtube_ff_link": "https://youtu.be/5OcGTXHWdBI",
                        "youtube_ff_id": "5OcGTXHWdBI",
                        "bunny_ff_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-tvcg/v-tvcg-10091196/v-tvcg-10091196_Preview.mp4?token=x2xxeL14Gs-4DVBoiR0aWtX9nPIGs5AODxQ9uIyi76M&expires=1706590800",
                        "bunny_ff_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-tvcg/v-tvcg-10091196/v-tvcg-10091196_Preview.vtt?token=bBN5asvwKGBDAlibI6eNpS5vV9RllSbtxZC1TQAk4_w&expires=1706590800",
                        "youtube_prerecorded_link": "https://youtu.be/PRMYHiBTaZA",
                        "youtube_prerecorded_id": "PRMYHiBTaZA",
                        "bunny_prerecorded_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-tvcg/v-tvcg-10091196/v-tvcg-10091196_Presentation.mp4?token=uh2_thHHN6tEWrDCe1KevrThz_2sBDFv1-F4QyVP7xg&expires=1706590800",
                        "bunny_prerecorded_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-tvcg/v-tvcg-10091196/v-tvcg-10091196_Presentation.vtt?token=RKcQNIRqnvwnsgWmxKE7-v4t7c3HQAdHx8wMcQxXvpE&expires=1706590800"
                    },
                    {
                        "slot_id": "v-full-1209",
                        "session_id": "full21",
                        "title": "A General Framework for Progressive Data Compression and Retrieval",
                        "contributors": [
                            "Victor A. P. Magri"
                        ],
                        "authors": [
                            "Victor A. P. Magri",
                            "Peter Lindstrom"
                        ],
                        "abstract": "In scientific simulations, observations, and experiments, the transfer of data to and from disk and across networks has become a major bottleneck for data analysis and visualization. Compression techniques have been employed to tackle this challenge, but traditional lossy methods often demand conservative error tolerances to meet the numerical accuracy requirements of both anticipated and unknown data analysis tasks. Progressive data compression and retrieval has emerged as a promising solution, where each analysis task dictates its own accuracy needs. However, few analysis algorithms inherently support progressive data processing, and adapting compression techniques, file formats, client/server frameworks, and APIs to support progressivity can be challenging. This paper presents a framework that enables progressive-precision data queries for any data compressor or numerical representation. Our strategy hinges on a multi-component representation that successively reduces the error between the original and compressed field, allowing each field in the progressive sequence to be expressed as a partial sum of components. We have implemented this approach with four established scientific data compressors and assessed its effectiveness using real-world data sets from the SDRBench collection. The results show that our framework competes in accuracy with the standalone compressors it is based upon. Additionally, (de)compression time is proportional to the number of components requested by the user. Finally, our framework allows for fully lossless compression using lossy compressors when a sufficient number of components are employed.",
                        "uid": "v-full-1209",
                        "time_stamp": "2023-10-26T22:36:00Z",
                        "time_start": "2023-10-26T22:36:00Z",
                        "time_end": "2023-10-26T22:48:00Z",
                        "paper_type": "full",
                        "keywords": [
                            "Lossy to lossless compression, progressive precision, multi-component expansion, floating-point data"
                        ],
                        "doi": "10.1109/TVCG.2023.3327186",
                        "fno": "",
                        "has_image": true,
                        "has_pdf": true,
                        "paper_award": "",
                        "image_caption": "This work presents a framework that enables progressive-precision data queries for any data compressor. Our strategy hinges on a multi-component representation that successively reduces the error between the original and compressed fields, allowing each field in the progressive sequence to be expressed as a partial sum of components. We have implemented this approach with four established scientific data compressors and assessed its effectiveness using real-world data sets from the SDRBench collection. The results show that our framework competes in accuracy and performance with other methods that natively support progressive data compression.",
                        "external_paper_link": "",
                        "youtube_ff_link": "https://youtu.be/RWsdS44wpm4",
                        "youtube_ff_id": "RWsdS44wpm4",
                        "bunny_ff_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1209/v-full-1209_Preview.mp4?token=27xtaQs4Vigurjgw9EH8gltldMEThqMDcwXWqP3h6Rw&expires=1706590800",
                        "bunny_ff_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1209/v-full-1209_Preview.vtt?token=yOF3otLr3ael4UCWfdCAdhMfj1tcRPHp6Sk5f5n_Ymc&expires=1706590800",
                        "youtube_prerecorded_link": "https://youtu.be/MASGoL1NZMM",
                        "youtube_prerecorded_id": "MASGoL1NZMM",
                        "bunny_prerecorded_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1209/v-full-1209_Presentation.mp4?token=t2LDordy7_TPGd5VbxLwpQHlhizX296-LPA0qWumZTw&expires=1706590800",
                        "bunny_prerecorded_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1209/v-full-1209_Presentation.vtt?token=RCIuYKP3eZSK6cf52Q5yrGpBmY3CV82ZTjT7waa1Q1U&expires=1706590800"
                    },
                    {
                        "slot_id": "v-full-1288",
                        "session_id": "full21",
                        "title": "Differentiable Design Galleries: A Differentiable Approach to Explore the Design Space of Transfer Functions",
                        "contributors": [
                            "Bo Pan"
                        ],
                        "authors": [
                            "Bo Pan",
                            "Jiaying Lu",
                            "Haoxuan Li",
                            "Weifeng Chen",
                            "Yiyao Wang",
                            "Minfeng Zhu",
                            "Chenhao Yu",
                            "Wei Chen"
                        ],
                        "abstract": "The transfer function is crucial for direct volume rendering (DVR) to create an informative visual representation of volumetric data. However, manually adjusting the transfer function to achieve the desired DVR result can be time-consuming and unintuitive. In this paper, we propose Differentiable Design Galleries, an image-based transfer function design approach to help users explore the design space of transfer functions by taking advantage of the recent advances in deep learning and differentiable rendering. Specifically, we leverage neural rendering to learn a latent design space, which is a continuous manifold representing various types of implicit transfer functions. We further provide a set of interactive tools to support intuitive query, navigation, and modification to obtain the target design, which is represented as a neural-rendered design exemplar. The explicit transfer function can be reconstructed from the target design with a differentiable direct volume renderer. Experimental results on real volumetric data demonstrate the effectiveness of our method.",
                        "uid": "v-full-1288",
                        "time_stamp": "2023-10-26T22:48:00Z",
                        "time_start": "2023-10-26T22:48:00Z",
                        "time_end": "2023-10-26T23:00:00Z",
                        "paper_type": "full",
                        "keywords": [
                            "Transfer function, direct volume rendering, deep learning, generative models, differentiable rendering"
                        ],
                        "doi": "10.1109/TVCG.2023.3327371",
                        "fno": "",
                        "has_image": true,
                        "has_pdf": true,
                        "paper_award": "",
                        "image_caption": "We propose Differentiable Design Galleries, a transfer function design approach based on deep learning and differentiable rendering to assist users in exploring the design space of transfer functions.",
                        "external_paper_link": "",
                        "youtube_ff_link": "https://youtu.be/U2tRFLO5jtk",
                        "youtube_ff_id": "U2tRFLO5jtk",
                        "bunny_ff_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1288/v-full-1288_Preview.mp4?token=XBIiEJaosd8JuQ1dxZ1KraxY9hHt6nqIi2el7W83PpU&expires=1706590800",
                        "bunny_ff_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1288/v-full-1288_Preview.vtt?token=n74Qe0hn013d9-aon4keiMwIaTzKo7r395XZzHgoOPc&expires=1706590800",
                        "youtube_prerecorded_link": "https://youtu.be/oErDe_2ScnU",
                        "youtube_prerecorded_id": "oErDe_2ScnU",
                        "bunny_prerecorded_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1288/v-full-1288_Presentation.mp4?token=ZL8vDDuF6Qn5tqM3vvy0zqKfkVOhlikCJLQ_N3w5-zc&expires=1706590800",
                        "bunny_prerecorded_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1288/v-full-1288_Presentation.vtt?token=yVP6S6Ko1htSjXUfUZTOYjhHDKJq3QJ8tmvVuhjSB8o&expires=1706590800"
                    },
                    {
                        "slot_id": "v-full-1202",
                        "session_id": "full21",
                        "title": "Residency Octree: A Hybrid Approach for Scalable Web-Based Multi-Volume Rendering",
                        "contributors": [
                            "Lukas Herzberger"
                        ],
                        "authors": [
                            "Lukas Herzberger",
                            "Markus Hadwiger",
                            "Robert Kr\u00fcger",
                            "Peter Sorger",
                            "Hanspeter Pfister",
                            "Eduard Gr\u00f6ller",
                            "Johanna Beyer"
                        ],
                        "abstract": "We present a hybrid multi-volume rendering approach based on a novel Residency Octree that combines the advantages of out-of-core volume rendering using page tables with those of standard octrees. Octree approaches work by performing hierarchical tree traversal. However, in octree volume rendering, tree traversal and the selection of data resolution are intrinsically coupled. This makes fine-grained empty-space skipping costly. Page tables, on the other hand, allow access to any cached brick from any resolution. However, they do not offer a clear and efficient strategy for substituting missing high-resolution data with lower-resolution data. We enable flexible mixed-resolution out-of-core multi-volume rendering by decoupling the cache residency of multi-resolution data from a resolution-independent spatial subdivision determined by the tree. Instead of one-to-one node-to-brick correspondences, each residency octree node is mapped to a set of bricks from different resolution levels. This makes it possible to efficiently and adaptively choose and mix resolutions, adapt sampling rates, and compensate for cache misses. At the same time, residency octrees support fine-grained empty-space skipping, independent of the data subdivision used for caching. Finally, to facilitate collaboration and outreach, and to eliminate local data storage, our implementation is a web-based, pure client-side renderer using WebGPU and WebAssembly. Our method is faster than prior approaches and efficient for many data channels with a flexible and adaptive choice of data resolution.",
                        "uid": "v-full-1202",
                        "time_stamp": "2023-10-26T23:00:00Z",
                        "time_start": "2023-10-26T23:00:00Z",
                        "time_end": "2023-10-26T23:12:00Z",
                        "paper_type": "full",
                        "keywords": [
                            "Volume rendering, ray-guided rendering, large-scale data, out-of-core rendering, multi-resolution, multi-channel, web-based visualization"
                        ],
                        "doi": "10.1109/TVCG.2023.3327193",
                        "fno": "",
                        "has_image": true,
                        "has_pdf": true,
                        "paper_award": "",
                        "image_caption": "In contrast to octree-based out-of-core approaches (left) which employ a one-to-one mapping between octree nodes and bricks, the residency octree nodes in our approach (right) represent geometric spatial regions, with each node mapping to multiple bricks and vice versa. This decoupling of resolution levels in data set from the spatial subdivision of the tree allows for more fine-grained empty space skipping than previous approaches and makes it possible to directly access any resolution from any node in the residency octree.",
                        "external_paper_link": "",
                        "youtube_ff_link": "https://youtu.be/WtaUcnuG-cY",
                        "youtube_ff_id": "WtaUcnuG-cY",
                        "bunny_ff_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1202/v-full-1202_Preview.mp4?token=Z6u7TPe-Pck6A8vi041zrR9yWSBZknaFUL81y-9C-BM&expires=1706590800",
                        "bunny_ff_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1202/v-full-1202_Preview.vtt?token=zX6CW_AaqtZAvS6ahzfj266HW7k67YkrcypekNm7WDU&expires=1706590800",
                        "youtube_prerecorded_link": "https://youtu.be/JCqZaePlqmw",
                        "youtube_prerecorded_id": "JCqZaePlqmw",
                        "bunny_prerecorded_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1202/v-full-1202_Presentation.mp4?token=qZe3HdGhOsbUOkIP38Vi1_6WI_87zisZTeuK_QGmze8&expires=1706590800",
                        "bunny_prerecorded_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1202/v-full-1202_Presentation.vtt?token=TvsQxAcTmo5ZjF7c6NmggaqF0LkwACJ4yWTd4EZzbsg&expires=1706590800"
                    }
                ]
            },
            {
                "title": "Situated Analytics and Augmented Reality (Full+Short)",
                "session_id": "full22",
                "event_prefix": "v-full",
                "track": "oneohnine",
                "session_image": "full22.png",
                "chair": [
                    "Christophe Hurter"
                ],
                "time_start": "2023-10-26T04:45:00Z",
                "time_end": "2023-10-26T06:00:00Z",
                "discord_category": "",
                "discord_channel": "109",
                "discord_channel_id": "1161741434647482379",
                "discord_link": "https://discordapp.com/channels/1161400451619627068/1161741434647482379",
                "zoom_private_meeting": "",
                "zoom_private_password": "",
                "zoom_private_link": "",
                "zoom_broadcast_link": "",
                "ff_link": "https://youtu.be/M_kq8lKHDxc",
                "time_slots": [
                    {
                        "slot_id": "v-full-1795",
                        "session_id": "full22",
                        "title": "ARGUS: Visualization of AI-assisted Task Guidance in AR",
                        "contributors": [
                            "Claudio Silva"
                        ],
                        "authors": [
                            "Sonia Castelo Quispe",
                            "Jo\u00e3o Rulff",
                            "Erin McGowan",
                            "Bea Steers",
                            "Guande Wu",
                            "Shaoyu Chen",
                            "Iran Roman",
                            "Roque Lopez",
                            "Ethan Brewer",
                            "Chen Zhao",
                            "Jing Qian",
                            "Kyunghyun Cho",
                            "He He",
                            "Qi Sun",
                            "Huy T. Vo",
                            "Juan Pablo Bello",
                            "Michael Krone",
                            "Claudio Silva"
                        ],
                        "abstract": "The concept of augmented reality (AR) assistants has captured the human imagination for decades, becoming a staple of modern science fiction. To pursue this goal, it is necessary to develop artificial intelligence (AI)-based methods that simultaneously perceive the 3D environment, reason about physical tasks, and model the performer, all in real-time. Within this framework, a wide variety of sensors are needed to generate data across different modalities, such as audio, video, depth, speech, and time-of-flight. The required sensors are typically part of the AR headset, providing performer sensing and interaction through visual, audio, and haptic feedback. AI assistants not only record the performer as they perform activities, but also require machine learning (ML) models to understand and assist the performer as they interact with the physical world. Therefore, developing such assistants is a challenging task. We propose ARGUS, a visual analytics system to support the development of intelligent AR assistants. Our system was designed as part of a multi-year-long collaboration between visualization researchers and ML and AR experts. This co-design process has led to advances in the visualization of ML in AR. Our system allows for online visualization of object, action, and step detection as well as offline analysis of previously recorded AR sessions. It visualizes not only the multimodal sensor data streams but also the output of the ML models. This allows developers to gain insights into the performer activities as well as the ML models, helping them troubleshoot, improve, and fine-tune the components of the AR assistant.",
                        "uid": "v-full-1795",
                        "time_stamp": "2023-10-26T04:45:00Z",
                        "time_start": "2023-10-26T04:45:00Z",
                        "time_end": "2023-10-26T04:57:00Z",
                        "paper_type": "full",
                        "keywords": [
                            "Data Models; Image and Video Data; Temporal Data; Application Motivated Visualization; AR/VR/Immersive."
                        ],
                        "doi": "10.1109/TVCG.2023.3327396",
                        "fno": "",
                        "has_image": true,
                        "has_pdf": true,
                        "paper_award": "honorable",
                        "image_caption": "ARGUS is a visual analytics tool for real-time and historical evaluation of sensor and model outputs of AR task assistants. Our system allows for online visualization of object, action, and step detection as well as offline analysis of previously recorded AR sessions. It visualizes not only the multimodal sensor data streams but also the output of the ML models. This allows developers to gain insights into the performer activities as well as the ML models, helping them troubleshoot, improve, and fine-tune the components of the AR assistant.",
                        "external_paper_link": "",
                        "youtube_ff_link": "https://youtu.be/QIPtDJ57SK4",
                        "youtube_ff_id": "QIPtDJ57SK4",
                        "bunny_ff_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1795/v-full-1795_Preview.mp4?token=Df4aAC5PBVib-N6Pu97SHerFGYVdtSByrrVI_NbIdwU&expires=1706590800",
                        "bunny_ff_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1795/v-full-1795_Preview.vtt?token=3DSkGcNe6KvaXByZCw1XkyogiHjAH5vyNDJ206b1wS8&expires=1706590800",
                        "youtube_prerecorded_link": "https://youtu.be/qBDonJbkDjQ",
                        "youtube_prerecorded_id": "qBDonJbkDjQ",
                        "bunny_prerecorded_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1795/v-full-1795_Presentation.mp4?token=jv554PXtGZABM636oEAJWjTjuwL4xOjc7cO_HZArK3E&expires=1706590800",
                        "bunny_prerecorded_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1795/v-full-1795_Presentation.vtt?token=k-pF90eeFM_01-8MgltkMn_EnnNkHd81US1WSLy-MCk&expires=1706590800"
                    },
                    {
                        "slot_id": "v-tvcg-10149486",
                        "session_id": "full22",
                        "title": "The Reality of the Situation: A Survey of Situated Analytics",
                        "contributors": [
                            "Sungbok Shin"
                        ],
                        "authors": [
                            "Sungbok Shin",
                            "Andrea Batch",
                            "Peter W. S. Butcher",
                            "Panagiotis D. Ritsos",
                            "Niklas Elmqvist"
                        ],
                        "abstract": "The advent of low-cost, accessible, and high-performance augmented reality (AR) has shed light on a situated form of analytics where in-situ visualizations embedded in the real world can facilitate sensemaking based on the user\u2019s physical location. In this work, we identify prior literature in this emerging field with a focus on the technologies enabling such situated analytics. After collecting 47 relevant situated analytics systems, we classify them using a taxonomy of three dimensions: situating triggers, view situatedness, and data depiction. We then identify four archetypical patterns in our classification using an ensemble cluster analysis. Finally, we discuss several insights and design guidelines that we learned from our analysis.",
                        "uid": "v-tvcg-10149486",
                        "time_stamp": "2023-10-26T04:57:00Z",
                        "time_start": "2023-10-26T04:57:00Z",
                        "time_end": "2023-10-26T05:09:00Z",
                        "paper_type": "full",
                        "keywords": [
                            "situated analytics;situated visualization;augmented reality;immersive analytics;data visualization"
                        ],
                        "doi": "10.1109/TVCG.2023.3285546",
                        "fno": "10149486",
                        "has_image": true,
                        "has_pdf": false,
                        "paper_award": "",
                        "image_caption": "This figure shows the design space of the taxonomy to describe Situated Analytics Systems. Below we show the four most commonly-appearing patterns extracted from a list of 47 situated analytics systems.",
                        "external_paper_link": "",
                        "youtube_ff_link": "https://youtu.be/bmYNk1EsLsg",
                        "youtube_ff_id": "bmYNk1EsLsg",
                        "bunny_ff_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-tvcg/v-tvcg-10149486/v-tvcg-10149486_Preview.mp4?token=mgZYSgncCFghy8syVbivp8oddn-F2Nw6S6F2uIyFm5A&expires=1706590800",
                        "bunny_ff_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-tvcg/v-tvcg-10149486/v-tvcg-10149486_Preview.vtt?token=69I-U_tI6LzSmFVPQwc4I4CH7BH14eFcygpH2vk-O3A&expires=1706590800",
                        "youtube_prerecorded_link": "https://youtu.be/6A-Uk8h9AO0",
                        "youtube_prerecorded_id": "6A-Uk8h9AO0",
                        "bunny_prerecorded_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-tvcg/v-tvcg-10149486/v-tvcg-10149486_Presentation.mp4?token=UJMopd2gbXvegO6D1D4oimKxnOV2MN2pKR0bO_kfOa4&expires=1706590800",
                        "bunny_prerecorded_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-tvcg/v-tvcg-10149486/v-tvcg-10149486_Presentation.vtt?token=PdTTvTTU_lhFc3BvtGAzjW76ArUHP4A4uCC-x_95nbA&expires=1706590800"
                    },
                    {
                        "slot_id": "v-full-1297",
                        "session_id": "full22",
                        "title": "Design Patterns for Situated Visualization in Augmented Reality",
                        "contributors": [
                            "Benjamin Lee"
                        ],
                        "authors": [
                            "Benjamin Lee",
                            "Michael Sedlmair",
                            "Dieter Schmalstieg"
                        ],
                        "abstract": "Situated visualization has become an increasingly popular research area in the visualization community, fueled by advancements in augmented reality (AR) technology and immersive analytics. Visualizing data in spatial proximity to their physical referents affords new design opportunities and considerations not present in traditional visualization, which researchers are now beginning to explore. However, the AR research community has an extensive history of designing graphics that are displayed in highly physical contexts. In this work, we leverage the richness of AR research and apply it to situated visualization. We derive design patterns which summarize common approaches of visualizing data in situ. The design patterns are based on a survey of 293 papers published in the AR and visualization communities, as well as our own expertise. We discuss design dimensions that help to describe both our patterns and previous work in the literature. This discussion is accompanied by several guidelines which explain how to apply the patterns given the constraints imposed by the real world. We conclude by discussing future research directions that will help establish a complete understanding of the design of situated visualization, including the role of interactivity, tasks, and workflows.",
                        "uid": "v-full-1297",
                        "time_stamp": "2023-10-26T05:09:00Z",
                        "time_start": "2023-10-26T05:09:00Z",
                        "time_end": "2023-10-26T05:21:00Z",
                        "paper_type": "full",
                        "keywords": [
                            "Augmented reality, immersive analytics, situated visualization, design patterns, design space"
                        ],
                        "doi": "10.1109/TVCG.2023.3327398",
                        "fno": "",
                        "has_image": true,
                        "has_pdf": true,
                        "paper_award": "",
                        "image_caption": "We present 10 design patterns for visualising data in the context of physical referents. Our design patterns include embedded views that encode data directly on the physical referent, such as glyphs, trajectories, and decals. They also include situated views such as panels and proxies. We describe common uses of each pattern, and characterise them through design dimensions and constraints.",
                        "external_paper_link": "",
                        "youtube_ff_link": "https://youtu.be/Ya5dInAbZE0",
                        "youtube_ff_id": "Ya5dInAbZE0",
                        "bunny_ff_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1297/v-full-1297_Preview.mp4?token=sur-FajtBEsOEf5Y2nnpxpUSAfDh84r7sNfMLeu-D-o&expires=1706590800",
                        "bunny_ff_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1297/v-full-1297_Preview.vtt?token=51kzP_4WytzwcVv0mdJqBZ9TDI0t9Cj6zICTWgYTXNQ&expires=1706590800",
                        "youtube_prerecorded_link": "https://youtu.be/hgf7X_uKG6g",
                        "youtube_prerecorded_id": "hgf7X_uKG6g",
                        "bunny_prerecorded_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1297/v-full-1297_Presentation.mp4?token=8Eo_56pHdmPvQ8iU-VB5noGNqasdtRVKmaRZa9SFLIo&expires=1706590800",
                        "bunny_prerecorded_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1297/v-full-1297_Presentation.vtt?token=Xl2ievsORIPl-eoFa_Lqd9SZ3JZy0Qo9hyzmGA7QhkQ&expires=1706590800"
                    },
                    {
                        "slot_id": "v-full-1299",
                        "session_id": "full22",
                        "title": "Handling Non-Visible Referents in Situated Visualizations",
                        "contributors": [
                            "Ambre Assor"
                        ],
                        "authors": [
                            "Ambre Assor",
                            "Arnaud Prouzeau",
                            "Martin Hachet",
                            "Pierre Dragicevic"
                        ],
                        "abstract": "Situated visualizations are a type of visualization where data is presented next to its physical referent (i.e., the physical object, space, or person it refers to), often using augmented-reality displays. While situated visualizations can be benefcial in various contexts and have received research attention, they are typically designed with the assumption that the physical referent is visible. However, in practice, a physical referent may be obscured by another object, such as a wall, or may be outside the user\u2019s visual feld. In this paper, we propose a conceptual framework and a design space to help researchers and user interface designers handle non-visible referents in situated visualizations. We frst provide an overview of techniques proposed in the past for dealing with non-visible objects in the areas of 3D user interfaces, 3D visualization, and mixed reality. From this overview, we derive a design space that applies to situated visualizations and employ it to examine various trade-offs, challenges, and opportunities for future research in this area.",
                        "uid": "v-full-1299",
                        "time_stamp": "2023-10-26T05:21:00Z",
                        "time_start": "2023-10-26T05:21:00Z",
                        "time_end": "2023-10-26T05:33:00Z",
                        "paper_type": "full",
                        "keywords": [
                            "Taxonomy, Models, Frameworks, Theory ; Mobile, AR/VR/Immersive, Specialized Input/Display Hardware"
                        ],
                        "doi": "10.1109/TVCG.2023.3327361",
                        "fno": "",
                        "has_image": true,
                        "has_pdf": true,
                        "paper_award": "",
                        "image_caption": "The user, a firefighter, has to save a victim. He uses a situated visualization: near the fire victim (the referent), stands a visualization of his vitals. However, a wall occludes the firefighter\u2019s view from the referent. To perform his task, he uses a XR system that handles the non-visibility of the physical referent. This system shows an overlay on the wall showing a representation of the victim as well as the visualization.",
                        "external_paper_link": "",
                        "youtube_ff_link": "https://youtu.be/tvE8z_llpqg",
                        "youtube_ff_id": "tvE8z_llpqg",
                        "bunny_ff_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1299/v-full-1299_Preview.mp4?token=uSyiq72LqtD4ghS_FV_ijFrnWhBvnANSCmnVdvcyEQA&expires=1706590800",
                        "bunny_ff_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1299/v-full-1299_Preview.vtt?token=iwJ9DfBQ7kH24liRtNTKsawOa_xavJYxoOFkl-GN8xE&expires=1706590800",
                        "youtube_prerecorded_link": "https://youtu.be/5GViRMjaUoU",
                        "youtube_prerecorded_id": "5GViRMjaUoU",
                        "bunny_prerecorded_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1299/v-full-1299_Presentation.mp4?token=0VkXlDKmOFGfjqlcg_oGCSC85OQV8mHh4oeTniIdr8s&expires=1706590800",
                        "bunny_prerecorded_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1299/v-full-1299_Presentation.vtt?token=yCJ2yDb4oPm3jhvA6FZ15CV24W_KlELXnwHbWk9SMvU&expires=1706590800"
                    },
                    {
                        "slot_id": "v-full-1532",
                        "session_id": "full22",
                        "title": "RL-LABEL: A Deep Reinforcement Learning Approach Intended for AR Label Placement in Dynamic Scenarios",
                        "contributors": [
                            "Zhutian Chen"
                        ],
                        "authors": [
                            "Zhutian Chen",
                            "Daniele Chiappalupi",
                            "Tica Lin",
                            "Yalong Yang",
                            "Johanna Beyer",
                            "Hanspeter Pfister"
                        ],
                        "abstract": "Labels are widely used in augmented reality (AR) to display digital information. Ensuring the readability of AR labels requires placing them occlusion-free manner while keeping visual linkings legible, especially when multiple labels exist in the scene. Although existing optimization-based methods, such as force-based methods, are effective in managing AR labels in static scenarios, they often struggle in dynamic scenarios with constantly moving objects. This is due to their focus on generating layouts optimal for the current moment, neglecting future moments and leading to sub-optimal or unstable layouts over time. In this work, we present RL-LABEL, a deep reinforcement learning-based method intended for managing the placement of AR labels in scenarios involving moving objects. RL-LABEL considers both the current and predicted future states of objects and labels, such as positions and velocities, as well as the user\u2019s viewpoint, to make informed decisions about label placement. It balances the trade-offs between immediate and long-term objectives. We tested RL-LABEL in simulated AR scenarios on two real-world datasets, showing that it effectively learns the decision-making process for long-term optimization, outperforming two baselines (i.e., no view management and a force-based method) by minimizing label occlusions, line intersections, and label movement distance. Additionally, a user study involving 18 participants indicates that, within our simulated environment, RL-LABEL excels over the baselines in aiding users to identify, compare, and summarize data on labels in dynamic scenes.",
                        "uid": "v-full-1532",
                        "time_stamp": "2023-10-26T05:33:00Z",
                        "time_start": "2023-10-26T05:33:00Z",
                        "time_end": "2023-10-26T05:45:00Z",
                        "paper_type": "full",
                        "keywords": [
                            "Augmented Reality, Reinforcement Learning, Label Placement, Dynamic Scenarios"
                        ],
                        "doi": "10.1109/TVCG.2023.3326568",
                        "fno": "",
                        "has_image": true,
                        "has_pdf": true,
                        "paper_award": "",
                        "image_caption": "RL-LABEL not only adapts label placement considering players\u2019 current motion status (e.g., speed, direction) and long-term outcomes but also ensures label stability over time. (a) Both players move left, with the rear player moving faster. A label is attached to the front player. (b)-(c) Using a force-based method, the label shifts left to avoid immediate occlusion but results in future occlusion. (d)-(e) With our method, the label moves right, sacrificing some immediate occlusion-free space for preventing future occlusion and ensuring stable visibility.",
                        "external_paper_link": "",
                        "youtube_ff_link": "https://youtu.be/BeWKBFH2xhA",
                        "youtube_ff_id": "BeWKBFH2xhA",
                        "bunny_ff_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1532/v-full-1532_Preview.mp4?token=9BsfuTihhnlVaE0NtcsHZ8azAF5pt5LsHxRmJ-jbGP8&expires=1706590800",
                        "bunny_ff_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1532/v-full-1532_Preview.vtt?token=15rQPFN3_u7CopxyEfPylfV8tu7NrcS7CfUdK9KT7Qw&expires=1706590800",
                        "youtube_prerecorded_link": "",
                        "youtube_prerecorded_id": "",
                        "bunny_prerecorded_link": "",
                        "bunny_prerecorded_subtitles": ""
                    },
                    {
                        "slot_id": "v-short-1191",
                        "session_id": "full22",
                        "title": "Quantifying the Impact of XR Visual Guidance on User Performance Using a Large-Scale Virtual Assembly Experiment",
                        "contributors": [
                            "Leon Pietschmann"
                        ],
                        "authors": [
                            "Leon Pietschmann",
                            "Paul-David Zuercher",
                            "Erik Bub\u00edk",
                            "Zhutian Chen",
                            "Hanspeter Pfister",
                            "Thomas Bohn\u00e9"
                        ],
                        "abstract": "The combination of Visual Guidance and Extended Reality (XR) technology holds the potential to greatly improve the performance of human workforces in numerous areas, particularly industrial environments. Focusing on virtual assembly tasks and making use of different forms of supportive visualisations, this study investigates the potential of XR Visual Guidance. Set in a web-based immersive environment, our results draw from a heterogeneous pool of 199 participants. This research is designed to significantly differ from previous exploratory studies, which yielded conflicting results on user performance and associated human factors. Our results clearly show the advantages of XR Visual Guidance based on an over 50\\% reduction in task completion times and mistakes made; this may further be enhanced and refined using specific frameworks and other forms of visualisations/Visual Guidance. Discussing the role of other factors, such as cognitive load, motivation, and usability, this paper also seeks to provide concrete avenues for future research and practical takeaways for practitioners.",
                        "uid": "v-short-1191",
                        "time_stamp": "2023-10-26T05:45:00Z",
                        "time_start": "2023-10-26T05:45:00Z",
                        "time_end": "2023-10-26T05:57:00Z",
                        "paper_type": "short",
                        "keywords": [
                            "[Human-centered computing]: Visualization, Human computer interaction (HCI), Interaction design\u2014Empirical studies in visualization, HCI, and interaction design, AR/VR/XR, Visual Guidance"
                        ],
                        "doi": "",
                        "fno": "",
                        "has_image": false,
                        "has_pdf": true,
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "youtube_ff_link": "https://youtu.be/2Lz8nw8FKY4",
                        "youtube_ff_id": "2Lz8nw8FKY4",
                        "bunny_ff_link": "",
                        "bunny_ff_subtitles": "",
                        "youtube_prerecorded_link": "",
                        "youtube_prerecorded_id": "",
                        "bunny_prerecorded_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-short/v-short-1191/v-short-1191_Presentation.mp4?token=evtwL9jtMAlZxgkL0Q6q5LDKb7pRMfYAXC9NH-kTtfY&expires=1706590800",
                        "bunny_prerecorded_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-short/v-short-1191/v-short-1191_Presentation.vtt?token=7MscsmMVP-iQ_xw7fJ8jErx9CR5lhMpIDVv_nBb8AoU&expires=1706590800"
                    }
                ]
            },
            {
                "title": "Sports and Spatial Management",
                "session_id": "full23",
                "event_prefix": "v-full",
                "track": "oneohsix",
                "session_image": "full23.png",
                "chair": [
                    "Charles Perin"
                ],
                "time_start": "2023-10-25T22:00:00Z",
                "time_end": "2023-10-25T23:15:00Z",
                "discord_category": "",
                "discord_channel": "106",
                "discord_channel_id": "1161741373410644119",
                "discord_link": "https://discordapp.com/channels/1161400451619627068/1161741373410644119",
                "zoom_private_meeting": "",
                "zoom_private_password": "",
                "zoom_private_link": "",
                "zoom_broadcast_link": "",
                "ff_link": "https://youtu.be/kaouYtZRqK4",
                "time_slots": [
                    {
                        "slot_id": "v-full-1329",
                        "session_id": "full23",
                        "title": "FSLens: A Visual Analytics Approach to Evaluating and Optimizing the Spatial Layout of Fire Stations",
                        "contributors": [
                            "Longfei Chen"
                        ],
                        "authors": [
                            "Longfei Chen",
                            "He Wang",
                            "Yang Ouyang",
                            "Yang Zhou",
                            "Naiyu Wang",
                            "Quan Li"
                        ],
                        "abstract": "The provision of fire services plays a vital role in ensuring the safety of residents' lives and property. The spatial layout of fire stations is closely linked to the efficiency of fire rescue operations. Traditional approaches have primarily relied on mathematical planning models to generate appropriate layouts by summarizing relevant evaluation criteria. However, this optimization process presents significant challenges due to the extensive decision space, inherent conflicts among criteria, and decision-makers' preferences. To address these challenges, we propose FSLens, an interactive visual analytics system that enables in-depth evaluation and rational optimization of fire station layout. Our approach integrates fire records and correlation features to reveal fire occurrence patterns and influencing factors using spatiotemporal sequence forecasting. We design an interactive visualization method to explore areas within the city that are potentially under-resourced for fire service based on the fire distribution and existing fire station layout. Moreover, we develop a collaborative human-computer multi-criteria decision model that generates multiple candidate solutions for optimizing firefighting resources within these areas. We simulate and compare the impact of different solutions on the original layout through well-designed visualizations, providing decision-makers with the most satisfactory solution. We demonstrate the effectiveness of our approach through one case study with real-world datasets. The feedback from domain experts indicates that our system helps them to better identify and improve potential gaps in the current fire station layout.",
                        "uid": "v-full-1329",
                        "time_stamp": "2023-10-25T22:00:00Z",
                        "time_start": "2023-10-25T22:00:00Z",
                        "time_end": "2023-10-25T22:12:00Z",
                        "paper_type": "full",
                        "keywords": [
                            "Spatiotemporal Analysis, Multi-criteria Decision Making, Visualization."
                        ],
                        "doi": "10.1109/TVCG.2023.3327077",
                        "fno": "",
                        "has_image": true,
                        "has_pdf": true,
                        "paper_award": "",
                        "image_caption": "System Overview of FSLens: (A) The Statistics Overview displays the statistical information of historical fires and fire stations. (B) The Fire Service S&D View serves as a tool for experts to comprehend the fluctuations in the supply and demand of firefighting resources over time. (C) The Spatiotemporal View employs a map-based exploration method to exhibit the spatial distribution of fire incidents and the spatial layout of fire stations. (D) The Optimization View offers a set of interactions to support the user in generating multiple optimization scenarios for consideration. (E) The Simulation and Comparison View aids in the assessment of the effects of the optimization solutions on the original layout and offers a comparative evaluation of the efficacy among solutions.",
                        "external_paper_link": "",
                        "youtube_ff_link": "https://youtu.be/W0zFHKKAPs0",
                        "youtube_ff_id": "W0zFHKKAPs0",
                        "bunny_ff_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1329/v-full-1329_Preview.mp4?token=KGbAGtsPfY4i1mY2nyxxGwpQBMD_IEhgEJz9OZULMVk&expires=1706590800",
                        "bunny_ff_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1329/v-full-1329_Preview.vtt?token=XlbHEb254GhyXv-yipqXIrZ3vDenb4V_ceW3OnHdkew&expires=1706590800",
                        "youtube_prerecorded_link": "",
                        "youtube_prerecorded_id": "",
                        "bunny_prerecorded_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1329/v-full-1329_Presentation.mp4?token=5RZE_CE2LTjE-kz4FrrSouVk7xI6qE3vVl8_WAIL8DU&expires=1706590800",
                        "bunny_prerecorded_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1329/v-full-1329_Presentation.vtt?token=Y3ZdEGMXNmevxNkb5dVfWocEa2XusW2cv_bV_2WgHOY&expires=1706590800"
                    },
                    {
                        "slot_id": "v-full-1344",
                        "session_id": "full23",
                        "title": "HoopInSight: Analyzing and Comparing Basketball Shooting Performance Through Visualization",
                        "contributors": [
                            "Yu Fu"
                        ],
                        "authors": [
                            "Yu Fu",
                            "John Stasko"
                        ],
                        "abstract": "Data visualization has the power to revolutionize sports. For example, the rise of shot maps has changed basketball strategy by visually illustrating where \u201cgood/bad\u201d shots are taken from. As a result, professional basketball teams today take shots from very different positions on the court than they did 20 years ago. Although the shot map has transformed many facets of the game, there is still much room for improvement to support richer and more complex analytical tasks. More specifically, we believe that the lack of sufficient interactivity to support various analytical queries and the inability to visually compare differences across situations are significant limitations of current shot maps. To address these limitations and showcase new possibilities, we designed and developed HoopInSight, an interactive visualization system that centers around a novel spatial comparison visual technique, enhancing the capabilities of shot maps in basketball analytics. This article presents the system, with a focus on our proposed visual technique and its accompanying interactions, all designed to promote comparison of two different scenarios. Furthermore, we provide reflections on and a discussion of relevant issues, including considerations for designing spatial comparison techniques, the scalability and transferability of this approach, and the benefits and pitfalls of designing as domain experts.",
                        "uid": "v-full-1344",
                        "time_stamp": "2023-10-25T22:12:00Z",
                        "time_start": "2023-10-25T22:12:00Z",
                        "time_end": "2023-10-25T22:24:00Z",
                        "paper_type": "full",
                        "keywords": [
                            "sports data visualization, sports analytics, visual comparison, basketball"
                        ],
                        "doi": "10.1109/TVCG.2023.3326910",
                        "fno": "",
                        "has_image": true,
                        "has_pdf": true,
                        "paper_award": "",
                        "image_caption": "The interface of HoopInSight. It consists of three large columns \u2014 two selection views and the comparison view in the middle. Each selection view has a shot chart and multiple supplementary views. The Comparison view is divided into two sub-views. The top subview shows where the frequency increases and the bottom subview shows where the frequency decreases.",
                        "external_paper_link": "",
                        "youtube_ff_link": "https://youtu.be/LnZdCpbHWt4",
                        "youtube_ff_id": "LnZdCpbHWt4",
                        "bunny_ff_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1344/v-full-1344_Preview.mp4?token=kXFpkIafvileCxWajUTL-PVy8LyCGdbWv8d--7jNYDE&expires=1706590800",
                        "bunny_ff_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1344/v-full-1344_Preview.vtt?token=7aGbu2bstL5b7k9RWtj15ys5MG4i3ZIzJfQRVl3CUTY&expires=1706590800",
                        "youtube_prerecorded_link": "https://youtu.be/lSxRinkaVH4",
                        "youtube_prerecorded_id": "lSxRinkaVH4",
                        "bunny_prerecorded_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1344/v-full-1344_Presentation.mp4?token=Lde9HG0g76U4NP5ZNsAYB2fcDgpF9er-asMr-OkirtU&expires=1706590800",
                        "bunny_prerecorded_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1344/v-full-1344_Presentation.vtt?token=Sm3NYyJXxv4hVzCU4W6WLEwVJZuxc9RGb0Uolqr_rmM&expires=1706590800"
                    },
                    {
                        "slot_id": "v-full-1240",
                        "session_id": "full23",
                        "title": "SkiVis: Visual Exploration and Route Planning in Ski Resorts",
                        "contributors": [
                            "Julius Rauscher"
                        ],
                        "authors": [
                            "Julius Rauscher",
                            "Raphael Buchm\u00fcller",
                            "Daniel Keim",
                            "Matthias Miller"
                        ],
                        "abstract": "Optimal ski route selection is a challenge based on a multitude of factors, such as the steepness, compass direction, or crowdedness. The personal preferences of every skier towards these factors require individual adaptations, which aggravate this task. Current approaches within this domain do not combine automated routing capabilities with user preferences, missing out on the possibility of integrating domain knowledge in the analysis process. We introduce SkiVis, a visual analytics application to interactively explore ski slopes and provide routing recommendations based on user preferences. In collaboration with ski guides and enthusiasts, we elicited requirements and guidelines for such an application and propose different workflows depending on the skiers' familiarity with the resort. In a case study on the resort of Ski Arlberg, we illustrate how to leverage volunteered geographic information to enable a numerical comparison between slopes. We evaluated our approach through a pair-analytics study and demonstrate how it supports skiers in discovering relevant and preference-based ski routes. Besides the tasks investigated in the study, we derive additional use cases from the interviews that showcase the further potential of SkiVis, and contribute directions for further research opportunities.",
                        "uid": "v-full-1240",
                        "time_stamp": "2023-10-25T22:24:00Z",
                        "time_start": "2023-10-25T22:24:00Z",
                        "time_end": "2023-10-25T22:36:00Z",
                        "paper_type": "full",
                        "keywords": [
                            "Geographic Visualization, Routing"
                        ],
                        "doi": "10.1109/TVCG.2023.3326940",
                        "fno": "",
                        "has_image": true,
                        "has_pdf": true,
                        "paper_award": "",
                        "image_caption": "SkiVis: Visual Exploration and Route Planning  in Ski Resorts",
                        "external_paper_link": "",
                        "youtube_ff_link": "https://youtu.be/DCy63Q2QC-4",
                        "youtube_ff_id": "DCy63Q2QC-4",
                        "bunny_ff_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1240/v-full-1240_Preview.mp4?token=j5VI19s9rp_LbdW4s4m3jxQnR7NEsJv2y4Gpt43vNDs&expires=1706590800",
                        "bunny_ff_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1240/v-full-1240_Preview.vtt?token=WGLRj7VueoM3q5egJBABEbdSxmIjFhhgTKZ25F46uP0&expires=1706590800",
                        "youtube_prerecorded_link": "https://youtu.be/kAR_SMveFoI",
                        "youtube_prerecorded_id": "kAR_SMveFoI",
                        "bunny_prerecorded_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1240/v-full-1240_Presentation.mp4?token=AJsBeSz6G6CF3-xIGS2oXwU3Q7IgotmTWhgwZOSoYgY&expires=1706590800",
                        "bunny_prerecorded_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1240/v-full-1240_Presentation.vtt?token=PoWV4Jjy1v_IRroOD4hDSxwWp60cgL0POBthRZERuEg&expires=1706590800"
                    },
                    {
                        "slot_id": "v-tvcg-10076255",
                        "session_id": "full23",
                        "title": "Analysis of Wildfire Visualization Systems for Research and Training: Are They up for the Challenge of the Current State of Wildfires?",
                        "contributors": [
                            "Carlos Tirado Cortes"
                        ],
                        "authors": [
                            "Carlos A. Tirado Cortes",
                            "Susanne Thurow",
                            "Alex Ong",
                            "Jason J. Sharples",
                            "Tomasz Bednarz",
                            "Grant Stevens",
                            "Dennis Del Favero"
                        ],
                        "abstract": "Wildfires affect many regions across the world. The accelerated progression of global warming has amplified their frequency and scale, deepening their impact on human life, the economy, and the environment. The temperature rise has been driving wildfires to behave unpredictably compared to those previously observed, challenging researchers and fire management agencies to understand the factors behind this behavioral change. Furthermore, this change has rendered fire personnel training outdated and lost its ability to adequately prepare personnel to respond to these new fires. Immersive visualization can play a key role in tackling the growing issue of wildfires. Therefore, this survey reviews various studies that use immersive and non-immersive data visualization techniques to depict wildfire behavior and train first responders and planners. This paper identifies the most useful characteristics of these systems. While these studies support knowledge creation for certain situations, there is still scope to comprehensively improve immersive systems to address the unforeseen dynamics of wildfires.",
                        "uid": "v-tvcg-10076255",
                        "time_stamp": "2023-10-25T22:36:00Z",
                        "time_start": "2023-10-25T22:36:00Z",
                        "time_end": "2023-10-25T22:48:00Z",
                        "paper_type": "full",
                        "keywords": [
                            "Immersive wildfire training;immersive wildfire visualization;modelling;simulation;wildfire;wildfire visualization"
                        ],
                        "doi": "10.1109/TVCG.2023.3258440",
                        "fno": "10076255",
                        "has_image": true,
                        "has_pdf": false,
                        "paper_award": "",
                        "image_caption": "Analysis of Wildfire Visualization Systems for Research and Training  Are They Up for the Challenge of the Current State of Wildfires?",
                        "external_paper_link": "",
                        "youtube_ff_link": "https://youtu.be/R6a55I-JcWo",
                        "youtube_ff_id": "R6a55I-JcWo",
                        "bunny_ff_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-tvcg/v-tvcg-10076255/v-tvcg-10076255_Preview.mp4?token=npXc6yzhjiPq-e1_ZNOKq6ENw9t0oKRhWFi-hpQNO2s&expires=1706590800",
                        "bunny_ff_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-tvcg/v-tvcg-10076255/v-tvcg-10076255_Preview.vtt?token=rMOC5oUD4hSNUEKmSC26LLoCzNgZZzDF5lnVIZUvTtw&expires=1706590800",
                        "youtube_prerecorded_link": "https://youtu.be/uk1-ec8Z4Zs",
                        "youtube_prerecorded_id": "uk1-ec8Z4Zs",
                        "bunny_prerecorded_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-tvcg/v-tvcg-10076255/v-tvcg-10076255_Presentation.mp4?token=ULPUD-PEV3t_MhyHRQbHhWt6IpNoxVQ0iZYYiIYbT8I&expires=1706590800",
                        "bunny_prerecorded_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-tvcg/v-tvcg-10076255/v-tvcg-10076255_Presentation.vtt?token=kS6LX1KFeTBT4vTsPn0cvdhLungjNmQA92nPjeNNfAo&expires=1706590800"
                    },
                    {
                        "slot_id": "v-tvcg-9894103",
                        "session_id": "full23",
                        "title": "Team-Builder: Toward More Effective Lineup Selection in Soccer",
                        "contributors": [
                            "Anqi Cao"
                        ],
                        "authors": [
                            "Anqi Cao",
                            "Ji Lan",
                            "Xiao Xie",
                            "Hongyu Chen",
                            "Xiaolong (Luke) Zhang",
                            "Hui Zhang",
                            "Yingcai Wu"
                        ],
                        "abstract": "Lineup selection is an essential and important task in soccer matches. To win a match, coaches must consider various factors and select appropriate players for a planned formation. Computation-based tools have been proposed to help coaches on this complex task, but they are usually based on over-simplified models on player performances, do not support interactive analysis, and overlook the inputs by coaches. In this paper, we propose a method for visual analytics of soccer lineup selection by tackling two challenges: characterizing essential factors involved in generating optimal lineup, and supporting coach-driven visual analytics of lineup selection. We develop a lineup selection model that integrates such important factors, such as spatial regions of player actions and defensive interactions with opponent players. A visualization system, Team-Builder, is developed to help coaches control the process of lineup generation, explanation, and comparison through multiple coordinated views. The usefulness and effectiveness of our system are demonstrated by two case studies on a real-world soccer event dataset.",
                        "uid": "v-tvcg-9894103",
                        "time_stamp": "2023-10-25T22:48:00Z",
                        "time_start": "2023-10-25T22:48:00Z",
                        "time_end": "2023-10-25T23:00:00Z",
                        "paper_type": "full",
                        "keywords": [
                            "Sports visualization;lineup selection;design study"
                        ],
                        "doi": "10.1109/TVCG.2022.3207147",
                        "fno": "9894103",
                        "has_image": true,
                        "has_pdf": false,
                        "paper_award": "",
                        "image_caption": "System user interface. The interface consists of three views: a tactic view (A), a player view (B), and a lineup view (C). The tactic view provides confrontation tactic lists (A1, A2, A3, A4) to navigate tactics used in the lineup. The player view contains a lineup edit board (B1) for lineup generation, a candidate player list (B2) for player constraint identification, and an explanation component for comprehension of the reason of the selection of a player. The lineup view includes a candidate lineup list (C1) and lineup thumbnails (C2) for comparing multiple lineups.",
                        "external_paper_link": "",
                        "youtube_ff_link": "https://youtu.be/oVjUUzMjCSQ",
                        "youtube_ff_id": "oVjUUzMjCSQ",
                        "bunny_ff_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-tvcg/v-tvcg-9894103/v-tvcg-9894103_Preview.mp4?token=e19UJYO3XXavAV6nTNKVAM6_FquxtytfwiB1IBU0gNQ&expires=1706590800",
                        "bunny_ff_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-tvcg/v-tvcg-9894103/v-tvcg-9894103_Preview.vtt?token=rtZjJasfiYFc0C5TVcXMpWx1XtVMeIaCzrhbHAMltis&expires=1706590800",
                        "youtube_prerecorded_link": "https://youtu.be/wmzB4gIqZQI",
                        "youtube_prerecorded_id": "wmzB4gIqZQI",
                        "bunny_prerecorded_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-tvcg/v-tvcg-9894103/v-tvcg-9894103_Presentation.mp4?token=C83dz4Cv_ZNexqr5FJDsyh5PSG9qQljq54msIV3uH-g&expires=1706590800",
                        "bunny_prerecorded_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-tvcg/v-tvcg-9894103/v-tvcg-9894103_Presentation.vtt?token=aTpb7G8Z_mzHYmR3Of3GSl6I4XE0w0kAzR1uKTDdNJ8&expires=1706590800"
                    },
                    {
                        "slot_id": "v-full-1002",
                        "session_id": "full23",
                        "title": "Action-Evaluator: A Visualization Approach for Player Action Evaluation in Soccer",
                        "contributors": [
                            "Anqi Cao"
                        ],
                        "authors": [
                            "Anqi Cao",
                            "Xiao Xie",
                            "Mingxu Zhou",
                            "Hui Zhang",
                            "Mingliang Xu",
                            "Yingcai Wu"
                        ],
                        "abstract": "In soccer, player action evaluation provides a fine-grained method to analyze player performance and plays an important role in improving winning chances in future matches. However, previous studies on action evaluation only provide a score for each action, and hardly support inspecting and comparing player actions integrated with complex match context information such as team tactics and player locations. In this work, we collaborate with soccer analysts and coaches to characterize the domain problems of evaluating player performance based on action scores. We design a tailored visualization of soccer player actions that places the action choice together with the tactic it belongs to as well as the player locations in the same view. Based on the design, we introduce a visual analytics system, Action-Evaluator, to facilitate a comprehensive player action evaluation through player navigation, action investigation, and action explanation. With the system, analysts can find players to be analyzed efficiently, learn how they performed under various match situations, and obtain valuable insights to improve their action choices. The usefulness and effectiveness of this work are demonstrated by two case studies on a real-world dataset and an expert interview.",
                        "uid": "v-full-1002",
                        "time_stamp": "2023-10-25T23:00:00Z",
                        "time_start": "2023-10-25T23:00:00Z",
                        "time_end": "2023-10-25T23:12:00Z",
                        "paper_type": "full",
                        "keywords": [
                            "Soccer Visualization;Player Evaluation;Design Study"
                        ],
                        "doi": "10.1109/TVCG.2023.3326524",
                        "fno": "",
                        "has_image": true,
                        "has_pdf": true,
                        "paper_award": "",
                        "image_caption": "System user interface. The interface contains three views: a player view (A), an action view (B), and an explanation view (C). The player view consists of a player ranking list (A1) to navigate players by importance and a player projection component (A2) to navigate players by similarity. The action view includes a match situation list (B1) to investigate action scores by match situations and an action score list (B2) to present those of different action choices. The adjustment view is composed of a record list (C1) and a ghost pitch (C2) to explain action scores to players.",
                        "external_paper_link": "",
                        "youtube_ff_link": "https://youtu.be/FMmI8pz3e5M",
                        "youtube_ff_id": "FMmI8pz3e5M",
                        "bunny_ff_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1002/v-full-1002_Preview.mp4?token=0AVF31hpzIbTY26XxF472ZfRwgH9ix5b9dVHUfF5C_E&expires=1706590800",
                        "bunny_ff_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1002/v-full-1002_Preview.vtt?token=D6LjW50DL-lDuzL2Vbbh4GSPtX3ckP-iacklHQk_xr4&expires=1706590800",
                        "youtube_prerecorded_link": "https://youtu.be/gfn4zJemyaQ",
                        "youtube_prerecorded_id": "gfn4zJemyaQ",
                        "bunny_prerecorded_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1002/v-full-1002_Presentation.mp4?token=kRXQPJj8zUY_ToSscK99YRoxhuTXEA1o3r2fMglZluI&expires=1706590800",
                        "bunny_prerecorded_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1002/v-full-1002_Presentation.vtt?token=KQ5L5Nk0lHegzTn0H-BpR5O9Nmp-PQZalOD8vP1d2FQ&expires=1706590800"
                    }
                ]
            },
            {
                "title": "Storytelling",
                "session_id": "full24",
                "event_prefix": "v-full",
                "track": "oneohfive",
                "session_image": "full24.png",
                "chair": [
                    "Bongshin Lee"
                ],
                "time_start": "2023-10-24T22:00:00Z",
                "time_end": "2023-10-24T23:15:00Z",
                "discord_category": "",
                "discord_channel": "105",
                "discord_channel_id": "1161741309346861067",
                "discord_link": "https://discordapp.com/channels/1161400451619627068/1161741309346861067",
                "zoom_private_meeting": "",
                "zoom_private_password": "",
                "zoom_private_link": "",
                "zoom_broadcast_link": "",
                "ff_link": "https://youtu.be/l0qYTKMUI74",
                "time_slots": [
                    {
                        "slot_id": "v-tvcg-9998319",
                        "session_id": "full24",
                        "title": "From Invisible to Visible: Impacts of Metadata in Communicative Data Visualization",
                        "contributors": [
                            "Alyxander Burns"
                        ],
                        "authors": [
                            "Alyxander Burns",
                            "Christiana Lee",
                            "Thai On",
                            "Cindy Xiong Bearfield",
                            "Evan Peck",
                            "Narges Mahyar"
                        ],
                        "abstract": "Leaving the context of visualizations invisible can have negative impacts on understanding and transparency. While common wisdom suggests that recontextualizing visualizations with metadata (e.g., disclosing the data source or instructions for decoding the visualizations' encoding) may counter these effects, the impact remains largely unknown. To fill this gap, we conducted two experiments. In Experiment 1, we explored how chart type, topic, and user goal impacted which categories of metadata participants deemed most relevant. We presented 64 participants with four real-world visualizations. For each visualization, participants were given four goals and selected the type of metadata they most wanted from a set of 18 types. Our results indicated that participants were most interested in metadata which explained the visualization's encoding for goals related to understanding and metadata about the source of the data for assessing trustworthiness. In Experiment 2, we explored how these two types of metadata impact transparency, trustworthiness and persuasiveness, information relevance, and understanding. We asked 144 participants to explain the main message of two pairs of visualizations (one with metadata and one without); rate them on scales of transparency and relevance; and then predict the likelihood that they were selected for a presentation to policymakers. Our results suggested that visualizations with metadata were perceived as more thorough than those without metadata, but similarly relevant, accurate, clear, and complete. Additionally, we found that metadata did not impact the accuracy of the information extracted from visualizations, but may have influenced which information participants remembered as important or interesting.",
                        "uid": "v-tvcg-9998319",
                        "time_stamp": "2023-10-24T22:00:00Z",
                        "time_start": "2023-10-24T22:00:00Z",
                        "time_end": "2023-10-24T22:12:00Z",
                        "paper_type": "full",
                        "keywords": [
                            "Visualization;metadata;understanding;transparency;trust"
                        ],
                        "doi": "10.1109/TVCG.2022.3231716",
                        "fno": "9998319",
                        "has_image": true,
                        "has_pdf": false,
                        "paper_award": "",
                        "image_caption": "In this paper, we explore what kinds of metadata people want and the impacts of having access to metadata on experience and understanding. Our results indicate that participants wanted different kinds of metadata depending on their reason for needing the data, but not the topic or chart. Further, We found that the presence of metadata did not affect the correctness of the statements made by participants, but may have re-directed their attention and impacted how transparent they thought the visualization was.",
                        "external_paper_link": "",
                        "youtube_ff_link": "https://youtu.be/Q-BPtDVwwaA",
                        "youtube_ff_id": "Q-BPtDVwwaA",
                        "bunny_ff_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-tvcg/v-tvcg-9998319/v-tvcg-9998319_Preview.mp4?token=8yul5Pk5NBky4RYdgbDBQt3rTt5pfHbc6_ChI4f1Qog&expires=1706590800",
                        "bunny_ff_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-tvcg/v-tvcg-9998319/v-tvcg-9998319_Preview.vtt?token=vcCgYaATCqjBBeZW1m39FHVdT5OAAHExy3bJsF3eL9I&expires=1706590800",
                        "youtube_prerecorded_link": "https://youtu.be/6F44xcHZxAc",
                        "youtube_prerecorded_id": "6F44xcHZxAc",
                        "bunny_prerecorded_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-tvcg/v-tvcg-9998319/v-tvcg-9998319_Presentation.mp4?token=V2P-LYdw1dJr3kkhiLJnqZbXTRM_p834B8TpcMWynYE&expires=1706590800",
                        "bunny_prerecorded_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-tvcg/v-tvcg-9998319/v-tvcg-9998319_Presentation.vtt?token=H0Sew5PGJIKXf-fFmX_ipXYuwEvkaFWOvINV3Qnhjrg&expires=1706590800"
                    },
                    {
                        "slot_id": "v-tvcg-9887905",
                        "session_id": "full24",
                        "title": "ScrollyVis: Interactive Visual Authoring of Guided Dynamic Narratives for Scientific Scrollytelling",
                        "contributors": [
                            "Eric M\u00f6rth"
                        ],
                        "authors": [
                            "Eric M\u00f6rth",
                            "Stefan Bruckner",
                            "Noeska N. Smit"
                        ],
                        "abstract": "Visual stories are an effective and powerful tool to convey specific information to a diverse public. Scrollytelling is a recent visual storytelling technique extensively used on the web, where content appears or changes as users scroll up or down a page. By employing the familiar gesture of scrolling as its primary interaction mechanism, it provides users with a sense of control, exploration and discoverability while still offering a simple and intuitive interface. In this paper, we present a novel approach for authoring, editing, and presenting data-driven scientific narratives using scrollytelling. Our method flexibly integrates common sources such as images, text, and video, but also supports more specialized visualization techniques such as interactive maps as well as scalar field and mesh data visualizations. We show that scrolling navigation can be used to traverse dynamic narratives and demonstrate how it can be combined with interactive parameter exploration. The resulting system consists of an extensible web-based authoring tool capable of exporting stand-alone stories that can be hosted on any web server. We demonstrate the power and utility of our approach with case studies from several diverse scientific fields and with a user study including 12 participants of diverse professional backgrounds. Furthermore, an expert in creating interactive articles assessed the usefulness of our approach and the quality of the created stories.",
                        "uid": "v-tvcg-9887905",
                        "time_stamp": "2023-10-24T22:12:00Z",
                        "time_start": "2023-10-24T22:12:00Z",
                        "time_end": "2023-10-24T22:24:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "doi": "10.1109/TVCG.2022.3205769",
                        "fno": "9887905",
                        "has_image": true,
                        "has_pdf": false,
                        "paper_award": "",
                        "image_caption": "Visual stories are an effective and powerful tool to convey specific information to a diverse public. Scrollytelling is a recent visual storytelling technique extensively used on the web, where content appears or changes as users scroll up or down a page. By employing the familiar gesture of scrolling as its primary interaction mechanism, it provides users with a sense of control, exploration and discoverability while still offering a simple and intuitive interface. In this paper, we present a novel approach for authoring, editing, and presenting data-driven scientific narratives using scrollytelling. Our method flexibly integrates common sources such as images, text, and video, but also supports more specialized visualization techniques such as interactive maps as well as scalar field and mesh data visualizations. We show that scrolling navigation can be used to traverse dynamic narratives and demonstrate how it can be combined with interactive parameter exploration. The resulting system consists of an extensible web-based authoring tool capable of exporting stand-alone stories that can be hosted on any web server. We demonstrate the power and utility of our approach with case studies from several diverse scientific fields and with a user study including 12 participants of diverse professional backgrounds. Furthermore, an expert in creating interactive articles assessed the usefulness of our approach and the quality of the created stories.",
                        "external_paper_link": "",
                        "youtube_ff_link": "https://youtu.be/GEc4BQGQKRE",
                        "youtube_ff_id": "GEc4BQGQKRE",
                        "bunny_ff_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-tvcg/v-tvcg-9887905/v-tvcg-9887905_Preview.mp4?token=d4RAB_f19Xo_M_QO58CHsyzKbgpLri1bXPH2xt3Gwd8&expires=1706590800",
                        "bunny_ff_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-tvcg/v-tvcg-9887905/v-tvcg-9887905_Preview.vtt?token=c4Fnc7EWQzZNrGCgfngLADhsfvaTND8peClUjYe3FvU&expires=1706590800",
                        "youtube_prerecorded_link": "https://youtu.be/QlZsUpFo6rg",
                        "youtube_prerecorded_id": "QlZsUpFo6rg",
                        "bunny_prerecorded_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-tvcg/v-tvcg-9887905/v-tvcg-9887905_Presentation.mp4?token=jepr_PmyjlbIJWFBDCfTsT5ukRo0Zzjg08JQN4iwSsA&expires=1706590800",
                        "bunny_prerecorded_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-tvcg/v-tvcg-9887905/v-tvcg-9887905_Presentation.vtt?token=BwTIm6sLQ8liIeBU6eNkBnnudy7x8C7VmeNY_hTr6JM&expires=1706590800"
                    },
                    {
                        "slot_id": "v-full-1758",
                        "session_id": "full24",
                        "title": "Character-Oriented Design for Visual Data Storytelling",
                        "contributors": [
                            "Keshav Dasu"
                        ],
                        "authors": [
                            "Keshav Dasu",
                            "Yun-Hsin Kuo",
                            "Kwan-Liu Ma"
                        ],
                        "abstract": "When telling a data story, an author has an intention they seek to convey to an audience. This intention can be of many forms such as to persuade, to educate, to inform, or even to entertain. In addition to expressing their intention, the story plot must balance being consumable and enjoyable while preserving scientific integrity. In data stories, numerous methods have been identified for constructing and presenting a plot. However, there is an opportunity to expand how we think and create the visual elements that present the story. Stories are brought to life by characters; often they are what make a story captivating, enjoyable, memorable, and facilitate following the plot until the end. Through the analysis of 160 existing data stories, we systematically investigate and identify distinguishable features of characters in data stories, and we illustrate how they feed into the broader concept of \u201ccharacter-oriented design\u201d. We identify the roles and visual representations data characters assume as well as the types of relationships these roles have with one another. We identify characteristics of antagonists as well as define conflict in data stories. We find the need for an identifiable central character that the audience latches on to in order to follow the narrative and identify their visual representations. We then illustrate \u201ccharacter-oriented design\u201d by showing how to develop data characters with common data story plots. With this work, we present a framework for data characters derived from our analysis; we then offer our extension to the data storytelling process using character-oriented design. To access our supplemental materials please visit https://chaorientdesignds.github.io/",
                        "uid": "v-full-1758",
                        "time_stamp": "2023-10-24T22:24:00Z",
                        "time_start": "2023-10-24T22:24:00Z",
                        "time_end": "2023-10-24T22:36:00Z",
                        "paper_type": "full",
                        "keywords": [
                            "Storytelling, Explanatory, Narrative visualization, Visual metaphor"
                        ],
                        "doi": "10.1109/TVCG.2023.3326578",
                        "fno": "",
                        "has_image": true,
                        "has_pdf": true,
                        "paper_award": "",
                        "image_caption": "In other media, characters are often used as a bridge for the audience to cross into an unfamiliar and perhaps complex new worlds.  Through the lens of characters, the audience can gain an understanding of a world without prior knowledge.   We are inspired to investigate the possibility of applying characters to convey scientific insights in data stories.  A deeper understanding of data characters could address open data storytelling opportunities.",
                        "external_paper_link": "",
                        "youtube_ff_link": "https://youtu.be/cV6Oro-gfBg",
                        "youtube_ff_id": "cV6Oro-gfBg",
                        "bunny_ff_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1758/v-full-1758_Preview.mp4?token=4A4zaZcGqln01GdBYedceyxfsNFBWK7J5CKzBqk3XKE&expires=1706590800",
                        "bunny_ff_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1758/v-full-1758_Preview.vtt?token=dW1u8lZP6Sx0abA197RBcTDKleIj8VBWQvsBWIkcP9c&expires=1706590800",
                        "youtube_prerecorded_link": "https://youtu.be/om-urGmmd5o",
                        "youtube_prerecorded_id": "om-urGmmd5o",
                        "bunny_prerecorded_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1758/v-full-1758_Presentation.mp4?token=EZ_Wb9-S7O65-Lh30R6K3Lm6SJMaTY0kzWg_7KBWXBU&expires=1706590800",
                        "bunny_prerecorded_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1758/v-full-1758_Presentation.vtt?token=BKQkXT4HV_tVaLGH6mvJ6ScLU3JfyMzazBCCNFg9MKE&expires=1706590800"
                    },
                    {
                        "slot_id": "v-full-1049",
                        "session_id": "full24",
                        "title": "Data Player: Automatic Generation of Data Videos with Narration-Animation Interplay",
                        "contributors": [
                            "Leixian Shen"
                        ],
                        "authors": [
                            "Leixian Shen",
                            "yizhi zhang",
                            "Haidong Zhang",
                            "Yun Wang"
                        ],
                        "abstract": "Data visualizations and narratives are often integrated to convey data stories effectively. Among various data storytelling formats, data videos have been garnering increasing attention. These videos provide an intuitive interpretation of data charts while vividly articulating the underlying data insights. However, the production of data videos demands a diverse set of professional skills and considerable manual labor, including understanding narratives, linking visual elements with narration segments, designing and crafting animations, recording audio narrations, and synchronizing audio with visual animations. To simplify this process, our paper introduces a novel method, referred to as Data Player, capable of automatically generating dynamic data videos with narration-animation interplay. This approach lowers the technical barriers associated with creating data videos rich in narration. To enable narration-animation interplay, Data Player constructs references between visualizations and text input. Specifically, it first extracts data into tables from the visualizations. Subsequently, it utilizes large language models to form semantic connections between text and visuals. Finally, Data Player encodes animation design knowledge as computational low-level constraints, allowing for the recommendation of suitable animation presets that align with the audio narration produced by text-to-speech technologies. We assessed Data Player\u2019s efficacy through an example gallery, a user study, and expert interviews. The evaluation results demonstrated that Data Player can generate high-quality data videos that are comparable to human-composed ones.",
                        "uid": "v-full-1049",
                        "time_stamp": "2023-10-24T22:36:00Z",
                        "time_start": "2023-10-24T22:36:00Z",
                        "time_end": "2023-10-24T22:48:00Z",
                        "paper_type": "full",
                        "keywords": [
                            "Visualization, Narration-animation interplay, Data video, Human-AI collaboration"
                        ],
                        "doi": "10.1109/TVCG.2023.3327197",
                        "fno": "",
                        "has_image": true,
                        "has_pdf": true,
                        "paper_award": "",
                        "image_caption": "The pipeline of automatic generation of data videos with narration-animation interplay.",
                        "external_paper_link": "",
                        "youtube_ff_link": "https://youtu.be/WkHHY7haJYI",
                        "youtube_ff_id": "WkHHY7haJYI",
                        "bunny_ff_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1049/v-full-1049_Preview.mp4?token=s6LRQKH6ZgIriQ-e1vYm1XD_wwWaTttB6A96oT7BfbI&expires=1706590800",
                        "bunny_ff_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1049/v-full-1049_Preview.vtt?token=8-QjRK7I0qF-zSopQON22UoewIUQUOggWp4acu5xQrg&expires=1706590800",
                        "youtube_prerecorded_link": "https://youtu.be/wrtAbOrr3m8",
                        "youtube_prerecorded_id": "wrtAbOrr3m8",
                        "bunny_prerecorded_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1049/v-full-1049_Presentation.mp4?token=AwEWb-BKR5IEC1pM85tVppe9iPDc_vuCbYXWWc4dlQc&expires=1706590800",
                        "bunny_prerecorded_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1049/v-full-1049_Presentation.vtt?token=TPAPJ1IfIEtT2ldONE_U-_JNmhnYb1BV5HbaHH1eYwE&expires=1706590800"
                    },
                    {
                        "slot_id": "v-full-1158",
                        "session_id": "full24",
                        "title": "EmphasisChecker: A Tool for Guiding Chart and Caption Emphasis",
                        "contributors": [
                            "Dae Hyun Kim"
                        ],
                        "authors": [
                            "Dae Hyun Kim",
                            "Seulgi Choi",
                            "Juho Kim",
                            "Vidya Setlur",
                            "Maneesh Agrawala"
                        ],
                        "abstract": "Recent work has shown that when both the chart and caption emphasize the same aspects of the data, readers tend to remember the doubly-emphasized features as takeaways; when there is a mismatch, readers rely on the chart to form takeaways and can miss information in the caption text. Through a survey of 280 chart-caption pairs in real-world sources (e.g., news media, poll reports, government reports, academic articles, and Tableau Public), we find that captions often do not emphasize the same information in practice, which could limit how effectively readers take away the authors\u2019 intended messages. Motivated by the survey findings, we present EmphasisChecker, an interactive tool that highlights visually prominent chart features as well as the features emphasized by the caption text along with any mismatches in the emphasis. The tool implements a time-series prominent feature detector based on the Ramer-Douglas-Peucker algorithm and a text reference extractor that identifies time references and data descriptions in the caption and matches them with chart data. This information enables authors to compare features emphasized by these two modalities, quickly see mismatches, and make necessary revisions. A user study confirms that our tool is both useful and easy to use when authoring charts and captions.",
                        "uid": "v-full-1158",
                        "time_stamp": "2023-10-24T22:48:00Z",
                        "time_start": "2023-10-24T22:48:00Z",
                        "time_end": "2023-10-24T23:00:00Z",
                        "paper_type": "full",
                        "keywords": [
                            "Chart and text takeaways, visual prominence, authoring, captions"
                        ],
                        "doi": "10.1109/TVCG.2023.3327150",
                        "fno": "",
                        "has_image": true,
                        "has_pdf": true,
                        "paper_award": "",
                        "image_caption": "The results of running EmphasisChecker at each stage of authoring a chart-caption pair about the real home price index between 1890 and 2006. (a) Prominent features are shown on top with a basic caption not describing any feature. (b) Caption text matches the most prominent visual feature (sharp rise at the end; blue). (c) Typo in the caption indicated by a red squiggly underline on \u2018declined since 1984\u2019. (d) Caption matching a less prominent feature, indicated by a blue squiggly underline on \u2018declined since 1894\u2019.",
                        "external_paper_link": "",
                        "youtube_ff_link": "https://youtu.be/uk-gt_dGXDI",
                        "youtube_ff_id": "uk-gt_dGXDI",
                        "bunny_ff_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1158/v-full-1158_Preview.mp4?token=z_9WAv6Rh9S_5s3MLCeZgG3XD0sQioUlNuFkP8cPNCg&expires=1706590800",
                        "bunny_ff_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1158/v-full-1158_Preview.vtt?token=0FUEMJG_Opjr16xhSOL8g9Pg2BE__CsBxdlJf3EoLNQ&expires=1706590800",
                        "youtube_prerecorded_link": "https://youtu.be/moe_-ROvDsc",
                        "youtube_prerecorded_id": "moe_-ROvDsc",
                        "bunny_prerecorded_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1158/v-full-1158_Presentation.mp4?token=IEB1NpyhDHuLSAyzPZYm0eiloj3bHp2puZTymhAshY0&expires=1706590800",
                        "bunny_prerecorded_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1158/v-full-1158_Presentation.vtt?token=wCXrhCcIC4AM0FqjScGD_4GvZzNB1zsHtJZJ-fHXHDU&expires=1706590800"
                    },
                    {
                        "slot_id": "v-full-1311",
                        "session_id": "full24",
                        "title": "Socrates: Data Story Generation via Adaptive Machine-Guided Elicitation of User Feedback",
                        "contributors": [
                            "Guande Wu"
                        ],
                        "authors": [
                            "Guande Wu",
                            "Shunan Guo",
                            "Jane Hoffswell",
                            "Gromit Yeuk-Yin Chan",
                            "Ryan Rossi",
                            "Eunyee Koh"
                        ],
                        "abstract": "Visual data stories can effectively convey insights from data, yet their creation often necessitates intricate data exploration, insight discovery, narrative organization, and customization to meet the communication objectives of the storyteller. Existing automated data storytelling system, however, tends to overlook the importance of user customization during the data story authoring process, limiting the system's ability to create tailored narratives that reflect the user's intentions. We present a novel data story generation workflow that leverages adaptive machine-guided elicitation of user feedback to customize the story. Our approach employs an adaptive plug-in module for existing story generation systems, which incorporates user feedback through interactive questioning based on the conversation history and dataset. This adaptability refines the system's understanding of the user's intentions, ensuring the final narrative aligns with their goals. We demonstrate the feasibility of our approach through the implementation of an interactive prototype: Socrates. Through a quantitative user study with 18 participants that compares our method to a state-of-the-art data story generation algorithm, we show that Socrates produces more relevant stories with a larger overlap of insights compared to human-generated stories. We also demonstrate the usability of Socrates via interviews with three data analysts and highlight areas of future work.",
                        "uid": "v-full-1311",
                        "time_stamp": "2023-10-24T23:00:00Z",
                        "time_start": "2023-10-24T23:00:00Z",
                        "time_end": "2023-10-24T23:12:00Z",
                        "paper_type": "full",
                        "keywords": [
                            "Narrative visualization, visual storytelling, conversational agent"
                        ],
                        "doi": "10.1109/TVCG.2023.3327363",
                        "fno": "",
                        "has_image": true,
                        "has_pdf": true,
                        "paper_award": "",
                        "image_caption": "Socrates: Data Story Generation via Adaptive Machine-Guided Elicitation of User Feedback. The figure includes the paper title, author information and an overview of the method. This paper presents a novel data story generation workflow called Socrates, that leverages adaptive machine-guided elicitation of user feedback to customize the story. The machine (Socrates) adaptively proposes questions to collect the user\u2019s feedback, which is incorporated into story generation.",
                        "external_paper_link": "",
                        "youtube_ff_link": "",
                        "youtube_ff_id": "",
                        "bunny_ff_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1311/v-full-1311_Preview.mp4?token=IGaqF7Tr1GewqngmBBs_x6SU0j5w5-woKZQ-5YsbNGc&expires=1706590800",
                        "bunny_ff_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1311/v-full-1311_Preview.vtt?token=VtjmiCsCgG11CMO7TevzOdNTgflCq3ATc2V8rkxrdAw&expires=1706590800",
                        "youtube_prerecorded_link": "https://youtu.be/u5bDYCzkIEI",
                        "youtube_prerecorded_id": "u5bDYCzkIEI",
                        "bunny_prerecorded_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1311/v-full-1311_Presentation.mp4?token=SACQiHRO-2_RGs1xS8qgnoCnTaQsPj5YDIojzTHvi-g&expires=1706590800",
                        "bunny_prerecorded_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1311/v-full-1311_Presentation.vtt?token=SSmrrNScnYKKjQyAijS_AnlvfJF89TVVPnGtlpiEfWk&expires=1706590800"
                    }
                ]
            },
            {
                "title": "Time Series Data",
                "session_id": "full25",
                "event_prefix": "v-full",
                "track": "oneohfour",
                "session_image": "full25.png",
                "chair": [
                    "Silvia Miksch"
                ],
                "time_start": "2023-10-26T04:45:00Z",
                "time_end": "2023-10-26T06:00:00Z",
                "discord_category": "",
                "discord_channel": "104",
                "discord_channel_id": "1161741240665124954",
                "discord_link": "https://discordapp.com/channels/1161400451619627068/1161741240665124954",
                "zoom_private_meeting": "",
                "zoom_private_password": "",
                "zoom_private_link": "",
                "zoom_broadcast_link": "",
                "ff_link": "https://youtu.be/4O1XJ1qM1Y4",
                "time_slots": [
                    {
                        "slot_id": "v-tvcg-9895311",
                        "session_id": "full25",
                        "title": "DOMINO: Visual Causal Reasoning with Time-Dependent Phenomena",
                        "contributors": [
                            "Klaus Mueller"
                        ],
                        "authors": [
                            "Jun Wang",
                            "Klaus Mueller"
                        ],
                        "abstract": "Current work on using visual analytics to determine causal relations among variables has mostly been based on the concept of counterfactuals. As such the derived static causal networks do not take into account the effect of time as an indicator. However, knowing the time delay of a causal relation can be crucial as it instructs how and when actions should be taken. Yet, similar to static causality, deriving causal relations from observational time-series data, as opposed to designed experiments, is not a straightforward process. It can greatly benefit from human insight to break ties and resolve errors.  We hence propose a set of visual analytics methods that allow humans to participate in the discovery of causal relations associated with windows of time delay. Specifically, we leverage a well-established method, logic-based causality, to enable analysts to test the significance of potential causes and measure their influences toward a certain effect. Furthermore, since an effect can be a cause of other effects, we allow users to  aggregate different temporal cause-effect relations found with our method into a visual flow diagram to enable the discovery of temporal causal networks. To demonstrate the effectiveness of our methods we constructed a prototype system named DOMINO and showcase it via a number of case studies using real-world datasets. Finally, we also used DOMINO to conduct several evaluations with human analysts from different science domains in order to gain feedback on the utility of our system in practical scenarios.",
                        "uid": "v-tvcg-9895311",
                        "time_stamp": "2023-10-26T04:45:00Z",
                        "time_start": "2023-10-26T04:45:00Z",
                        "time_end": "2023-10-26T04:57:00Z",
                        "paper_type": "full",
                        "keywords": [
                            "Causality analysis;hypothesis generation;hypothesis testing;time series;visual analytics"
                        ],
                        "doi": "10.1109/TVCG.2022.3207929",
                        "fno": "9895311",
                        "has_image": true,
                        "has_pdf": false,
                        "paper_award": "",
                        "image_caption": "The visual interface of our DOMINO system. DOMINO allows humans to discover causal relations associated with windows of time delay. It consists of the conditional distribution view for manually exploring potential causes of a specified effect, the causal inference panel for the interactive analysis of causal relations under different time delays, the time sequence view for examining the synchronized time series, and the causal flow chart that aggregates the identified relations into a causal network.",
                        "external_paper_link": "",
                        "youtube_ff_link": "https://youtu.be/jFp4v7NDB2E",
                        "youtube_ff_id": "jFp4v7NDB2E",
                        "bunny_ff_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-tvcg/v-tvcg-9895311/v-tvcg-9895311_Preview.mp4?token=46BfoX7m-QlN9PMbXCN4Gfl-wUkET2Ond3cm-eSfdlg&expires=1706590800",
                        "bunny_ff_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-tvcg/v-tvcg-9895311/v-tvcg-9895311_Preview.vtt?token=CG4s5D1h0YlQSHBg5gLwW_Y-CVa6nUsIU9bFXsZ2oD4&expires=1706590800",
                        "youtube_prerecorded_link": "https://youtu.be/06DNJRFKE0I",
                        "youtube_prerecorded_id": "06DNJRFKE0I",
                        "bunny_prerecorded_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-tvcg/v-tvcg-9895311/v-tvcg-9895311_Presentation.mp4?token=5tr8MrVeZYXtIeav9_s_oVu2a4K_nT6kWaMVmePREi4&expires=1706590800",
                        "bunny_prerecorded_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-tvcg/v-tvcg-9895311/v-tvcg-9895311_Presentation.vtt?token=cWt_7Vl0xCVEPokWh32yqohH0s2o1LjnUoYFws1yQSM&expires=1706590800"
                    },
                    {
                        "slot_id": "v-full-1079",
                        "session_id": "full25",
                        "title": "Attribute-Aware RBFs: Interactive Visualization of Time Series Particle Volumes Using RT Core Range Queries",
                        "contributors": [
                            "Nate Morrical"
                        ],
                        "authors": [
                            "Nate Morrical",
                            "Stefan Zellmann",
                            "Alper Sahistan",
                            "Patrick Shriwise",
                            "Valerio Pascucci"
                        ],
                        "abstract": "Smoothed-particle hydrodynamics (SPH) is a mesh-free method used to simulate volumetric media in fluids, astrophysics, and solid mechanics. Visualizing these simulations is problematic because these datasets often contain millions, if not billions of particles carrying physical attributes and moving over time. Radial basis functions (RBFs) are used to model particles, and overlapping particles are interpolated to reconstruct a high-quality volumetric field; however, this interpolation process is expensive and makes interactive visualization difficult. Existing RBF interpolation schemes do not account for color-mapped attributes and are instead constrained to visualizing just the density field. To address these challenges, we exploit ray tracing cores in modern GPU architectures to accelerate scalar field reconstruction. We use a novel RBF interpolation scheme to integrate per-particle colors and densities, and leverage GPU-parallel tree construction and refitting to quickly update the tree as the simulation animates over time or when the user manipulates particle radii. We also propose a Hilbert reordering scheme to cluster particles together at the leaves of the tree to reduce tree memory consumption. Finally, we reduce the noise of volumetric shadows by adopting a spatially temporal blue noise sampling scheme. Our method can provide a more detailed and interactive view of these large, volumetric, time-series particle datasets than traditional methods, leading to new insights into these physics simulations.",
                        "uid": "v-full-1079",
                        "time_stamp": "2023-10-26T04:57:00Z",
                        "time_start": "2023-10-26T04:57:00Z",
                        "time_end": "2023-10-26T05:09:00Z",
                        "paper_type": "full",
                        "keywords": [
                            "Ray Tracing, Volume Rendering, Particle Volumes, Radial Basis Functions, Scientific Visualization"
                        ],
                        "doi": "10.1109/TVCG.2023.3327366",
                        "fno": "",
                        "has_image": true,
                        "has_pdf": true,
                        "paper_award": "",
                        "image_caption": "The \"Cabana Dam Break\" data set, rendered interactively with our method at 46 FPS, 4-samples-per-pixel per-frame with volumetric shadows (left is 1 frame, right is 1024 averaged frames, bottom row are progressing time steps). GPU-accelerated tree construction and our blue noise approach enable interactive animation and improved perception over time.",
                        "external_paper_link": "",
                        "youtube_ff_link": "https://youtu.be/Tf56UeFyPSI",
                        "youtube_ff_id": "Tf56UeFyPSI",
                        "bunny_ff_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1079/v-full-1079_Preview.mp4?token=Hg1jRebFBXm6jjBnce4X0fia-nq__YlWvuDlm2WJm0w&expires=1706590800",
                        "bunny_ff_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1079/v-full-1079_Preview.vtt?token=60OjyN6o9w7znpHlGGwkzQ_39P_jsgz4rIqZbsF6A04&expires=1706590800",
                        "youtube_prerecorded_link": "https://youtu.be/iIDCZbLPzRo",
                        "youtube_prerecorded_id": "iIDCZbLPzRo",
                        "bunny_prerecorded_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1079/v-full-1079_Presentation.mp4?token=R8nec98H0TUFqq_szbDEVKLF3UAsxTC5poH4oPGEcrI&expires=1706590800",
                        "bunny_prerecorded_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1079/v-full-1079_Presentation.vtt?token=Krh5qe9w6-v1zy114efAg8uSZMErwdJQCYAJxx4bVb8&expires=1706590800"
                    },
                    {
                        "slot_id": "v-full-1449",
                        "session_id": "full25",
                        "title": "Reclaiming the Horizon: Novel Visualization Designs for Time-Series Data with Large Value Ranges",
                        "contributors": [
                            "Daniel Braun"
                        ],
                        "authors": [
                            "Daniel Braun",
                            "Rita Borgo",
                            "Max Sondag",
                            "Tatiana von Landesberger"
                        ],
                        "abstract": "We introduce two novel visualization designs to support practitioners in performing identification and discrimination tasks on large value ranges (i.e., several orders of magnitude) in time-series data: (1) The order of magnitude horizon graph, which extends the classic horizon graph; and (2) the order of magnitude line chart, which adapts the log-line chart. These new visualization designs visualize large value ranges by explicitly splitting the mantissa \ud835\udc5a and exponent \ud835\udc52 of a value \ud835\udc63 = \ud835\udc5a \u00b7 10\ud835\udc52. We evaluate our novel designs against the most relevant state-of-the-art visualizations in an empirical user study. It focuses on four main tasks commonly employed in the analysis of time-series and large value ranges visualization: identification, discrimination, estimation, and trend detection. For each task we analyze error, confidence, and response time. The new order of magnitude horizon graph performs better or equal to all other designs in identification, discrimination, and estimation tasks. Only for trend detection tasks, the more traditional horizon graphs reported better performance. Our results are domain-independent, only requiring time-series data with large value ranges.",
                        "uid": "v-full-1449",
                        "time_stamp": "2023-10-26T05:09:00Z",
                        "time_start": "2023-10-26T05:09:00Z",
                        "time_end": "2023-10-26T05:21:00Z",
                        "paper_type": "full",
                        "keywords": [
                            "Visualization techniques, time-series, design study, orders of magnitude, logarithmic scale"
                        ],
                        "doi": "10.1109/TVCG.2023.3326576",
                        "fno": "",
                        "has_image": true,
                        "has_pdf": true,
                        "paper_award": "",
                        "image_caption": "Our two new visualization designs for large value ranges in time-series data: The order of magnitude line chart (top) and the order of magnitude horizon graph (bottom).",
                        "external_paper_link": "",
                        "youtube_ff_link": "https://youtu.be/kqx5KXyM84w",
                        "youtube_ff_id": "kqx5KXyM84w",
                        "bunny_ff_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1449/v-full-1449_Preview.mp4?token=Lvi6SNhhU5OghzOiJAN4-3xokPXX_EfNUJmdxuI3WJU&expires=1706590800",
                        "bunny_ff_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1449/v-full-1449_Preview.vtt?token=isAm6JWt5b5yA4oDEemu0BJFTnaEW_fy54upg3ZiXb0&expires=1706590800",
                        "youtube_prerecorded_link": "https://youtu.be/ATQJQr5dGpQ",
                        "youtube_prerecorded_id": "ATQJQr5dGpQ",
                        "bunny_prerecorded_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1449/v-full-1449_Presentation.mp4?token=9P8uFGit1E3-NBT5O7-XBrF0aZGL2posUFjG6RjBuGk&expires=1706590800",
                        "bunny_prerecorded_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1449/v-full-1449_Presentation.vtt?token=ucfbja_l-Pv9RuBPicfKDxZpeeW2ZrFAdNBr-u5suq0&expires=1706590800"
                    },
                    {
                        "slot_id": "v-full-1082",
                        "session_id": "full25",
                        "title": "Supporting Guided Exploratory Visual Analysis on Time Series Data with Reinforcement Learning",
                        "contributors": [
                            "Yang Shi"
                        ],
                        "authors": [
                            "Yang Shi",
                            "Bingchang Chen",
                            "Ying Chen",
                            "Zhuochen Jin",
                            "Ke Xu",
                            "Xiaohan Jiao",
                            "Tian Gao",
                            "Nan Cao"
                        ],
                        "abstract": "The exploratory visual analysis (EVA) of time series data uses visualization as the main output medium and input interface for exploring new data. However, for users who lack visual analysis expertise, interpreting and manipulating EVA can be challenging. Thus, providing guidance on EVA is necessary and two relevant questions need to be answered. First, how to recommend interesting insights to provide a first glance at data and help develop an exploration goal. Second, how to provide step-by-step EVA suggestions to help identify which parts of the data to explore. In this work, we present a reinforcement learning (RL)-based system, Visail, which generates EVA sequences to guide the exploration of time series data. As a user uploads a time series dataset, Visail can generate step-by-step EVA suggestions, while each step is visualized as an annotated chart combined with textual descriptions. The RL-based algorithm uses exploratory data analysis knowledge to construct the state and action spaces for the agent to imitate human analysis behaviors in data exploration tasks. In this way, the agent learns the strategy of generating coherent EVA sequences through a well-designed network. To evaluate the effectiveness of our system, we conducted an ablation study, a user study, and two case studies. The results of our evaluation suggested that Visail can provide effective guidance on supporting EVA on time series data.",
                        "uid": "v-full-1082",
                        "time_stamp": "2023-10-26T05:21:00Z",
                        "time_start": "2023-10-26T05:21:00Z",
                        "time_end": "2023-10-26T05:33:00Z",
                        "paper_type": "full",
                        "keywords": [
                            "Time Series Data, Exploratory Visual Analysis, Reinforcement Learning"
                        ],
                        "doi": "10.1109/TVCG.2023.3327200",
                        "fno": "",
                        "has_image": true,
                        "has_pdf": true,
                        "paper_award": "",
                        "image_caption": "The exploratory visual analysis (EVA) of time series data uses visualization as the main output medium and input interface for exploring new data. In this work, we present a reinforcement learning (RL)-based system, Visail, which generates EVA sequences to guide the exploration of time series data. As a user uploads a time series dataset, Visail can generate step-by-step EVA suggestions, while each step is visualized as an annotated chart combined with textual descriptions. interface of Visail consists of four components, including (1) Timeline view, (2) Sequence view, (3) Insight panel, (4) Insight tooltip, and (5) Suggestion Panel.",
                        "external_paper_link": "",
                        "youtube_ff_link": "https://youtu.be/WBOV9-xf2RQ",
                        "youtube_ff_id": "WBOV9-xf2RQ",
                        "bunny_ff_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1082/v-full-1082_Preview.mp4?token=1sgumf0CP3SRQrgZMCAIGitp2cCzLeMQKzPPzY2mIko&expires=1706590800",
                        "bunny_ff_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1082/v-full-1082_Preview.vtt?token=wFKZz6C8tG49Uxx1EUdEUkk8zhZXP5g55Sy1bk10MGg&expires=1706590800",
                        "youtube_prerecorded_link": "https://youtu.be/l26n79AiXTE",
                        "youtube_prerecorded_id": "l26n79AiXTE",
                        "bunny_prerecorded_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1082/v-full-1082_Presentation.mp4?token=P7y18MH9GVG5SQAFFQBENeyhghQ7ptGwmugOAPAAOd8&expires=1706590800",
                        "bunny_prerecorded_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1082/v-full-1082_Presentation.vtt?token=Dzoee13dqOGtAanNOd2sG6vNnN4cq5D3i70PxQk8EOA&expires=1706590800"
                    },
                    {
                        "slot_id": "v-full-1321",
                        "session_id": "full25",
                        "title": "TimeTuner: Diagnosing Time Representations for Time-Series Forecasting with Counterfactual Explanations",
                        "contributors": [
                            "Jianing Hao"
                        ],
                        "authors": [
                            "Jianing Hao",
                            "Qing Shi",
                            "Yilin Ye",
                            "Wei Zeng"
                        ],
                        "abstract": "Deep learning (DL) approaches are being increasingly used for time-series forecasting, with many efforts devoted to designing complex DL models. Recent studies have shown that the DL success is often attributed to effective data representations, fostering the fields of feature engineering and representation learning. However, automated approaches for feature learning are typically limited with respect to incorporating prior knowledge, identifying interactions among variables, and choosing evaluation metrics to ensure that the models are reliable. To improve on these limitations, this paper contributes a novel visual analytics framework, namely TimeTuner, designed to help analysts understand how model behaviors are associated with localized correlations, stationarity, and granularity of time-series representations. The system mainly consists of the following two-stage technique: We first leverage counterfactual explanations to connect the relationships among time-series representations, multivariate features and model predictions. Next, we design multiple coordinated views including a partition-based correlation matrix and juxtaposed bivariate stripes, and provide a set of interactions that allow users to step into the transformation selection process, navigate through the feature space, and reason the model performance. We instantiate TimeTuner with two transformation methods of smoothing and sampling, and demonstrate its applicability on real-world time-series forecasting of univariate sunspots and multivariate air pollutants. Feedback from domain experts indicates that our system can help characterize time-series representations and guide the feature engineering processes.",
                        "uid": "v-full-1321",
                        "time_stamp": "2023-10-26T05:33:00Z",
                        "time_start": "2023-10-26T05:33:00Z",
                        "time_end": "2023-10-26T05:45:00Z",
                        "paper_type": "full",
                        "keywords": [
                            "Time-series forecasting, counterfactual explanation, visual analytics"
                        ],
                        "doi": "10.1109/TVCG.2023.3327389",
                        "fno": "",
                        "has_image": true,
                        "has_pdf": true,
                        "paper_award": "",
                        "image_caption": "TimeTuner, a novel visual analytics framework, combines counterfactual explanations with interactive visualizations to enhance user engagement in exploring the feature space, selecting appropriate transformation methods, and gaining intuitive insights. It offers juxtaposed bivariate stripes and partition-based correlation matrices, enabling users to navigate the transformation selection process and feature space interactively. TimeTuner is instantiated with smoothing and sampling transformations, and evaluated on real-world time-series forecasting of univariate sunspots and multivariate air pollutants. Feedback highlights its effectiveness in analyzing the impact of time-series representations and guiding data representation learning.",
                        "external_paper_link": "",
                        "youtube_ff_link": "https://youtu.be/eCNQTStE0l0",
                        "youtube_ff_id": "eCNQTStE0l0",
                        "bunny_ff_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1321/v-full-1321_Preview.mp4?token=WPjzXk-XI50DpK9_s5UKPSju0q91kpMim5RkZw8efIg&expires=1706590800",
                        "bunny_ff_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1321/v-full-1321_Preview.vtt?token=8x35wWgcv_RG3RTl_Tide37Wpq_BJLXE8fOjTdGMDM0&expires=1706590800",
                        "youtube_prerecorded_link": "https://youtu.be/ofNNirHeJeE",
                        "youtube_prerecorded_id": "ofNNirHeJeE",
                        "bunny_prerecorded_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1321/v-full-1321_Presentation.mp4?token=meWcHElkyqtGwhSLN0fFApfxSEwuJMF3A-Pfz4XV30Y&expires=1706590800",
                        "bunny_prerecorded_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1321/v-full-1321_Presentation.vtt?token=IqKFIxtreQuwBXmDPOSDtfZIaIOyeR-Vly03NIG-ems&expires=1706590800"
                    },
                    {
                        "slot_id": "v-full-1179",
                        "session_id": "full25",
                        "title": "Visualizing Large-Scale Spatial Time Series with GeoChron",
                        "contributors": [
                            "Zikun Deng"
                        ],
                        "authors": [
                            "Zikun Deng",
                            "Shifu Chen",
                            "Tobias Schreck",
                            "Dazhen Deng",
                            "Tan Tang",
                            "Mingliang Xu",
                            "Di Weng",
                            "Yingcai Wu"
                        ],
                        "abstract": "In geo-related fields such as urban informatics, atmospheric science, and geography, large-scale spatial time (ST) series (i.e., geo-referred time series) are collected for monitoring and understanding important spatiotemporal phenomena. ST series visualization is an effective means of understanding the data and reviewing spatiotemporal phenomena, which is a prerequisite for in-depth data analysis. However, visualizing these series is challenging due to their large scales, inherent dynamics, and spatiotemporal nature. In this study, we introduce the notion of patterns of evolution in ST series. Each evolution pattern is characterized by 1) a set of ST series that are close in space and 2) a time period when the trends of these ST series are correlated. We then leverage Storyline techniques by considering an analogy between evolution patterns and sessions, and finally design a novel visualization called GeoChron, which is capable of visualizing large-scale ST series in an evolution pattern-aware and narrative-preserving manner. GeoChron includes a mining framework to extract evolution patterns and two-level visualizations to enhance its visual scalability. We evaluate GeoChron with two case studies, an informal user study, an ablation study, parameter analysis, and running time analysis.",
                        "uid": "v-full-1179",
                        "time_stamp": "2023-10-26T05:45:00Z",
                        "time_start": "2023-10-26T05:45:00Z",
                        "time_end": "2023-10-26T05:57:00Z",
                        "paper_type": "full",
                        "keywords": [
                            "Spatiotemporal visualization, spatial time series, Storyline"
                        ],
                        "doi": "10.1109/TVCG.2023.3327162",
                        "fno": "",
                        "has_image": true,
                        "has_pdf": true,
                        "paper_award": "",
                        "image_caption": "A novel Storyline-based visualization is proposed to visualizing large-scale spatial time series. Each curve in the Storyline represents a spatial time series, and each bundle of curves represents an evolution pattern where the spatial time series are close in space and have correlated trends. The Storyline and geographic map is visually linked using colors.",
                        "external_paper_link": "",
                        "youtube_ff_link": "https://youtu.be/HJOANK17sTM",
                        "youtube_ff_id": "HJOANK17sTM",
                        "bunny_ff_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1179/v-full-1179_Preview.mp4?token=93xWbNFB5tBu7iB23HqNHUl46wTsitWKvptKGn--oak&expires=1706590800",
                        "bunny_ff_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1179/v-full-1179_Preview.vtt?token=M3pepyuiohpc7XJ5djnZedxkvyYLt_Y4U-ItyJOYxto&expires=1706590800",
                        "youtube_prerecorded_link": "https://youtu.be/DLJtBFaW6HY",
                        "youtube_prerecorded_id": "DLJtBFaW6HY",
                        "bunny_prerecorded_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1179/v-full-1179_Presentation.mp4?token=EHzvTaIaXwMhzbz4-7EiyCTQxP-qoVei1NsEWypmy3w&expires=1706590800",
                        "bunny_prerecorded_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1179/v-full-1179_Presentation.vtt?token=jPovC_-QWOeIfS0kxmSPF55tjfjFNu1v1cyyQXR_DqY&expires=1706590800"
                    }
                ]
            },
            {
                "title": "Topology and Morse Theory",
                "session_id": "full26",
                "event_prefix": "v-full",
                "track": "oneohsix",
                "session_image": "full26.png",
                "chair": [
                    "Bei Wang Phillips"
                ],
                "time_start": "2023-10-26T03:00:00Z",
                "time_end": "2023-10-26T04:15:00Z",
                "discord_category": "",
                "discord_channel": "106",
                "discord_channel_id": "1161741373410644119",
                "discord_link": "https://discordapp.com/channels/1161400451619627068/1161741373410644119",
                "zoom_private_meeting": "",
                "zoom_private_password": "",
                "zoom_private_link": "",
                "zoom_broadcast_link": "",
                "ff_link": "https://youtu.be/Wn0aTu71Xe4",
                "time_slots": [
                    {
                        "slot_id": "v-tvcg-10021892",
                        "session_id": "full26",
                        "title": "Discrete Morse Sandwich: Fast Computation of Persistence Diagrams for Scalar Data -- An Algorithm and A Benchmark",
                        "contributors": [
                            "Julien Tierny"
                        ],
                        "authors": [
                            "Pierre Guillou",
                            "Jules Vidal",
                            "Julien Tierny"
                        ],
                        "abstract": "This paper introduces an efficient algorithm for persistence diagram computation, given an input piecewise linear scalar field f defined on a d-dimensional simplicial complex K, with $d \\leq 3$. Our work revisits the seminal algorithm \"PairSimplices\" [31], [103] with discrete Morse theory (DMT) [34], [80], which greatly reduces the number of input simplices to consider. Further, we also extend to DMT and accelerate the stratification strategy described in \"PairSimplices\" for the fast computation of the 0th and (d - 1)th diagrams, noted $D_0(f)$ and $D_{d-1}(f)$. Minima-saddle persistence pairs ($D_0(f)$) and saddle-maximum persistence pairs ($D_{d-1}(f)$) are efficiently computed by processing, with a Union-Find, the unstable sets of 1-saddles and the stable sets of (d - 1)-saddles. This fast pre-computation for the dimensions 0 and (d - 1) enables an aggressive specialization of [4] to the 3D case, which results in a drastic reduction of the number of input simplices for the computation of $D_1(f)$, the intermediate layer of the sandwich. Finally, we document several performance improvements via shared-memory parallelism. We provide an open-source implementation of our algorithm for reproducibility purposes. We also contribute a reproducible benchmark package, which exploits three-dimensional data from a public repository and compares our algorithm to a variety of publicly available implementations. Extensive experiments indicate that our algorithm improves by two orders of magnitude the time performance of the seminal \"PairSimplices\" algorithm it extends. Moreover, it also improves memory footprint and time performance over a selection of 14 competing approaches, with a substantial gain over the fastest available approaches, while producing a strictly identical output.",
                        "uid": "v-tvcg-10021892",
                        "time_stamp": "2023-10-26T03:00:00Z",
                        "time_start": "2023-10-26T03:00:00Z",
                        "time_end": "2023-10-26T03:12:00Z",
                        "paper_type": "full",
                        "keywords": [
                            "Topological data analysis;scalar data;persistence diagrams;discrete Morse theory"
                        ],
                        "doi": "10.1109/TVCG.2023.3238008",
                        "fno": "10021892",
                        "has_image": true,
                        "has_pdf": false,
                        "paper_award": "",
                        "image_caption": "This work presents a fast algorithm for the computation of persistence diagrams. Our algorithm can be viewed as a modern interpretation of the standard persistence algorithm, from the perspective of Discrete Morse Theory.  We provide an open-source implementation as well as a benchmark package, which shows that our method leads to faster computations than competing approaches.  Our work enables the interactive inspection of circular patterns in scalar data.",
                        "external_paper_link": "",
                        "youtube_ff_link": "https://youtu.be/VaLZEgKFs-c",
                        "youtube_ff_id": "VaLZEgKFs-c",
                        "bunny_ff_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-tvcg/v-tvcg-10021892/v-tvcg-10021892_Preview.mp4?token=LunNmPPQlZnDDUvtOm2Sk5NpCma8YeChXvdRWpkXxe0&expires=1706590800",
                        "bunny_ff_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-tvcg/v-tvcg-10021892/v-tvcg-10021892_Preview.vtt?token=Xf_0nLAn-XTCgbPKO9mhbM6Z0lmJ5NMH2ytBmh7tAts&expires=1706590800",
                        "youtube_prerecorded_link": "https://youtu.be/pOhxvWMIjr0",
                        "youtube_prerecorded_id": "pOhxvWMIjr0",
                        "bunny_prerecorded_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-tvcg/v-tvcg-10021892/v-tvcg-10021892_Presentation.mp4?token=vZlPAyka-_TUAtz8vgQqPSznvEpM6-J-asrN6uxTW4o&expires=1706590800",
                        "bunny_prerecorded_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-tvcg/v-tvcg-10021892/v-tvcg-10021892_Presentation.vtt?token=N6mI2UXydk47dJUGfUpfI2LCi67HaWWuLd5JS5QlWl0&expires=1706590800"
                    },
                    {
                        "slot_id": "v-tvcg-10081444",
                        "session_id": "full26",
                        "title": "Parallel Computation of Piecewise Linear Morse-Smale Segmentations",
                        "contributors": [
                            "Robin Maack"
                        ],
                        "authors": [
                            "Robin G. C. Maack",
                            "Jonas Lukasczyk",
                            "Julien Tierny",
                            "Hans Hagen",
                            "Ross Maciejewski",
                            "Christoph Garth"
                        ],
                        "abstract": "This paper presents a well-scaling parallel algorithm for the computation of Morse-Smale (MS) segmentations, including the region separators and region boundaries. The segmentation of the domain into ascending and descending manifolds, solely defined on the vertices, improves the computational time using path compression and fully segments the border region. Region boundaries and region separators are generated using a multi-label marching tetrahedra algorithm. This enables a fast and simple solution to find optimal parameter settings in preliminary exploration steps by generating an MS complex preview. It also poses a rapid option to generate a fast visual representation of the region geometries for immediate utilization. Two experiments demonstrate the performance of our approach with speedups of over an order of magnitude in comparison to two publicly available implementations. The example section shows the similarity to the MS complex, the useability of the approach, and the benefits of this method with respect to the presented datasets. We provide our implementation with the paper.",
                        "uid": "v-tvcg-10081444",
                        "time_stamp": "2023-10-26T03:12:00Z",
                        "time_start": "2023-10-26T03:12:00Z",
                        "time_end": "2023-10-26T03:24:00Z",
                        "paper_type": "full",
                        "keywords": [
                            "Morse-Smale complex;segmentation;topology;visualization;watershed transformation"
                        ],
                        "doi": "10.1109/TVCG.2023.3261981",
                        "fno": "10081444",
                        "has_image": true,
                        "has_pdf": false,
                        "paper_award": "",
                        "image_caption": "The image shows the region boundaries of the Morse-Smale Segmentation computed on the Viscous Fingering dataset, simplified with an absolute persistence threshold of 0.1. The boundary interface of viscous fingers is shown as contours of the salt concentration density scalar field, colored by the density from yellow (high concentration) to purple (low concentration). The Morse-Smale segmentation region boundaries can extract the region-separating geometries that separate single viscous fingers without cluttering the visualization. Many Morse-Smale complex implementations would clutter the visualization with additional geometry from the saddle-saddle separatices.",
                        "external_paper_link": "",
                        "youtube_ff_link": "https://youtu.be/OkvrASwC4eE",
                        "youtube_ff_id": "OkvrASwC4eE",
                        "bunny_ff_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-tvcg/v-tvcg-10081444/v-tvcg-10081444_Preview.mp4?token=JuC5R_UU7c3zxvLgEStp6s4xXd5_5cjU9-HgfydvfqE&expires=1706590800",
                        "bunny_ff_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-tvcg/v-tvcg-10081444/v-tvcg-10081444_Preview.vtt?token=CG35t2ap2i3qYOZ4QzA2-O6HTwMk0vcfGyE_Hkbz2ng&expires=1706590800",
                        "youtube_prerecorded_link": "https://youtu.be/V90D4sMN_9A",
                        "youtube_prerecorded_id": "V90D4sMN_9A",
                        "bunny_prerecorded_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-tvcg/v-tvcg-10081444/v-tvcg-10081444_Presentation.mp4?token=oO3oZ_8oImNDUba_HaqhqW-5d1Yt5euUmYFEo4G5rD0&expires=1706590800",
                        "bunny_prerecorded_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-tvcg/v-tvcg-10081444/v-tvcg-10081444_Presentation.vtt?token=_WVYpqnM82lO3VLSW6whusNWdVaxln-o_qzX7zR84EY&expires=1706590800"
                    },
                    {
                        "slot_id": "v-tvcg-9920234",
                        "session_id": "full26",
                        "title": "Principal Geodesic Analysis of Merge Trees (and Persistence Diagrams)",
                        "contributors": [
                            "Mathieu Pont"
                        ],
                        "authors": [
                            "Mathieu Pont",
                            "Jules Vidal",
                            "Julien Tierny"
                        ],
                        "abstract": "This paper presents a computational framework for the Principal Geodesic Analysis of merge trees (MT-PGA), a novel adaptation of the celebrated Principal Component Analysis (PCA) framework [87] to the Wasserstein metric space of merge trees [92]. We formulate MT-PGA computation as a constrained optimization problem, aiming at adjusting a basis of orthogonal geodesic axes, while minimizing a fitting energy. We introduce an efficient, iterative algorithm which exploits shared-memory parallelism, as well as an analytic expression of the fitting energy gradient, to ensure fast iterations. Our approach also trivially extends to extremum persistence diagrams. Extensive experiments on public ensembles demonstrate the efficiency of our approach - with MT-PGA computations in the orders of minutes for the largest examples. We show the utility of our contributions by extending to merge trees two typical PCA applications. First, we apply MT-PGA to data reduction and reliably compress merge trees by concisely representing them by their first coordinates in the MT-PGA basis. Second, we present a dimensionality reduction framework exploiting the first two directions of the MT-PGA basis to generate two-dimensional layouts of the ensemble. We augment these layouts with persistence correlation views, enabling global and local visual inspections of the feature variability in the ensemble. In both applications, quantitative experiments assess the relevance of our framework. Finally, we provide a C++ implementation that can be used to reproduce our results.",
                        "uid": "v-tvcg-9920234",
                        "time_stamp": "2023-10-26T03:24:00Z",
                        "time_start": "2023-10-26T03:24:00Z",
                        "time_end": "2023-10-26T03:36:00Z",
                        "paper_type": "full",
                        "keywords": [
                            "Topological data analysis;ensemble data;merge trees;persistence diagrams"
                        ],
                        "doi": "10.1109/TVCG.2022.3215001",
                        "fno": "9920234",
                        "has_image": true,
                        "has_pdf": false,
                        "paper_award": "",
                        "image_caption": "Merge trees are mathematical objects that summarize the features of interest in the data. This work presents a new method for the variability analysis of ensembles of merge trees (or persistence diagrams) by adapting the celebrated Principal Component Analysis framework to these specific objects. We show the utility of our approach with visualization applications such as data reduction to reliably compress the input merge trees and dimensionality reduction to generate two-dimensional layouts of the ensemble. We augment these layouts with persistence correlation view, enabling visual inspections of the feature variability. And with the reconstruction of user-defined locations for interactive exploration.",
                        "external_paper_link": "",
                        "youtube_ff_link": "https://youtu.be/dmitXW1wWwA",
                        "youtube_ff_id": "dmitXW1wWwA",
                        "bunny_ff_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-tvcg/v-tvcg-9920234/v-tvcg-9920234_Preview.mp4?token=S16AT-ptydnKsLQZR8Z2GsYLM1oUoSMWhM0NLVzE5ak&expires=1706590800",
                        "bunny_ff_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-tvcg/v-tvcg-9920234/v-tvcg-9920234_Preview.vtt?token=k0dXSPXb8ka2eO6ccNJCYwv8G7ewKxuLiOb7euPCrjI&expires=1706590800",
                        "youtube_prerecorded_link": "https://youtu.be/6CZKgVdEwn8",
                        "youtube_prerecorded_id": "6CZKgVdEwn8",
                        "bunny_prerecorded_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-tvcg/v-tvcg-9920234/v-tvcg-9920234_Presentation.mp4?token=A1wkRK8pavPO4ZzWDoD5rZVLKRQRsVOSvovegeSclus&expires=1706590800",
                        "bunny_prerecorded_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-tvcg/v-tvcg-9920234/v-tvcg-9920234_Presentation.vtt?token=ue3CooNEcfWEhWjfdcLBPnWF85cOBVg_0HCrBJ-680c&expires=1706590800"
                    },
                    {
                        "slot_id": "v-full-1554",
                        "session_id": "full26",
                        "title": "A Comparative Study of the Perceptual Sensitivity of Topological Visualizations to Feature Variations",
                        "contributors": [
                            "Dr. Tushar M. Athawale"
                        ],
                        "authors": [
                            "Tushar M. Athawale",
                            "Bryan Triana",
                            "Tanmay Kotha",
                            "David Pugmire",
                            "Paul Rosen"
                        ],
                        "abstract": "Color maps are a commonly used visualization technique in which data are mapped to optical properties, e.g., color or opacity. Color maps, however, do not explicitly convey structures (e.g., positions and scale of features) within data. Topology-based visualizations reveal and explicitly communicate structures underlying data. Although our understanding of what types of features are captured by topological visualizations is good, our understanding of people's perception of those features is not. This paper evaluates the sensitivity of topology-based isocontour, Reeb graph, and persistence diagram visualizations compared to a reference color map visualization for synthetically generated scalar fields on 2-manifold triangular meshes embedded in 3D. In particular, we built and ran a human-subject study that evaluated the perception of data features characterized by Gaussian signals and measured how effectively each visualization technique portrays variations of data features arising from the position and amplitude variation of a mixture of Gaussians. For positional feature variations, the results showed that only the Reeb graph visualization had high sensitivity. For amplitude feature variations, persistence diagrams and color maps demonstrated the highest sensitivity, whereas isocontours showed only weak sensitivity. These results take an important step toward understanding which topology-based tools are best for various data and task scenarios and their effectiveness in conveying topological variations as compared to conventional color mapping.",
                        "uid": "v-full-1554",
                        "time_stamp": "2023-10-26T03:36:00Z",
                        "time_start": "2023-10-26T03:36:00Z",
                        "time_end": "2023-10-26T03:48:00Z",
                        "paper_type": "full",
                        "keywords": [
                            "Perception & cognition, computational topology-based techniques, comparison and similarity."
                        ],
                        "doi": "10.1109/TVCG.2023.3326592",
                        "fno": "",
                        "has_image": true,
                        "has_pdf": true,
                        "paper_award": "",
                        "image_caption": "Sensitivity analysis results for a color map [column (a)] and topological visualizations [columns (b)-(d)]. Bar charts depict the percentage of correctly answered trials for the positional data variation in the center row and amplitude data variation in the bottom row. Reeb graphs showed sensitivity to positional variation. Persistence diagrams and color maps showed sensitivity to amplitude variation. However, no single visualization type effectively conveyed both position and amplitude variation in the data.",
                        "external_paper_link": "",
                        "youtube_ff_link": "https://youtu.be/4IHuBESGvJs",
                        "youtube_ff_id": "4IHuBESGvJs",
                        "bunny_ff_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1554/v-full-1554_Preview.mp4?token=pCiInya-rF4FidLd2IdarEmlWju_lL5BcDbH_3vHAOI&expires=1706590800",
                        "bunny_ff_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1554/v-full-1554_Preview.vtt?token=JU4hlHDH6YkHNj5GzIfVWj6tWd6a1E1BvwrrT0BukN8&expires=1706590800",
                        "youtube_prerecorded_link": "https://youtu.be/e3NtxX19Lwc",
                        "youtube_prerecorded_id": "e3NtxX19Lwc",
                        "bunny_prerecorded_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1554/v-full-1554_Presentation.mp4?token=Gddo-PNabxDkRyvvix1MQGvP2BAJSWIje-LYqAuXFGU&expires=1706590800",
                        "bunny_prerecorded_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1554/v-full-1554_Presentation.vtt?token=v2feQ17L7UDvxytEAS1_Z8Jboj8wibHyQAgC_lkVca0&expires=1706590800"
                    },
                    {
                        "slot_id": "v-full-1663",
                        "session_id": "full26",
                        "title": "ExTreeM: Scalable Augmented Merge Tree Computation via Extremum Graphs",
                        "contributors": [
                            "Jonas Lukasczyk"
                        ],
                        "authors": [
                            "Jonas Lukasczyk",
                            "Michael Will",
                            "Florian Wetzels",
                            "Gunther H Weber",
                            "Christoph Garth"
                        ],
                        "abstract": "Over the last decade merge trees have been proven to support a plethora of visualization and analysis tasks since they effectively abstract complex datasets. This paper describes the ExTreeM-Algorithm: a scalable algorithm for the computation of merge trees via extremum graphs. The core idea of ExTreeM is to first derive the extremum graph G of an input scalar field f defined on a cell complex K, and subsequently compute the unaugmented merge tree of f on G instead of K; which are equivalent. Any merge tree algorithm can be carried out significantly faster on G, since K in general contains substantially more cells than G. To further speed up computation, ExTreeM includes a tailored procedure to derive merge trees of extremum graphs. The computation of the fully augmented merge tree, i.e., a merge tree domain segmentation of K, can then be performed in an optional post processing step. All steps of ExTreeM consist of procedures with high parallel efficiency, and we provide a formal proof of its correctness. Our experiments, performed on publicly available datasets, report a speedup of up to one order of magnitude over the state-of-the-art algorithms included in the TTK and VTK-m software libraries, while also requiring significantly less memory and exhibiting superior scaling behavior.",
                        "uid": "v-full-1663",
                        "time_stamp": "2023-10-26T03:48:00Z",
                        "time_start": "2023-10-26T03:48:00Z",
                        "time_end": "2023-10-26T04:00:00Z",
                        "paper_type": "full",
                        "keywords": [
                            "Scalar field topology, merge trees, persistence pairs, high performance computing."
                        ],
                        "doi": "10.1109/TVCG.2023.3326526",
                        "fno": "",
                        "has_image": true,
                        "has_pdf": true,
                        "paper_award": "",
                        "image_caption": "Merge trees are fundamental data abstractions of scalar field topology that record at which scalar values superlevel set components appear and merge. They can be used for a plethora of visualization and analysis task such as data segmentation (top).  We present ExTreeM, a generic schema using the ascending / descending manifold to generate a smaller extremum graph of the dataset (bottom, middle) and a specialized merge tree algorithm (bottom, right) showing speedups of up to one order of magnitude over the current state of the art.",
                        "external_paper_link": "",
                        "youtube_ff_link": "https://youtu.be/w9bKW5O_Jf0",
                        "youtube_ff_id": "w9bKW5O_Jf0",
                        "bunny_ff_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1663/v-full-1663_Preview.mp4?token=epQ7uNXaQfE5fQYJW77H5QGJaUzM6QLEY4HAUQJ_r2A&expires=1706590800",
                        "bunny_ff_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1663/v-full-1663_Preview.vtt?token=L4uNvs1xi4YAkgJK0r4yZ3KFMOLXZXxxPmE-jA5nIb4&expires=1706590800",
                        "youtube_prerecorded_link": "https://youtu.be/AVHlMbYJnNk",
                        "youtube_prerecorded_id": "AVHlMbYJnNk",
                        "bunny_prerecorded_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1663/v-full-1663_Presentation.mp4?token=AE2IOjGrMsL4UquUp3YPBk4eWGCbPVSKgMZvrlUQu50&expires=1706590800",
                        "bunny_prerecorded_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1663/v-full-1663_Presentation.vtt?token=H-d3t52v06ApXI1Sg0fL9_wUtGv0Zn7kV2qsNfocaXg&expires=1706590800"
                    },
                    {
                        "slot_id": "v-full-1463",
                        "session_id": "full26",
                        "title": "Merge Tree Geodesics and Barycenters with Path Mappings",
                        "contributors": [
                            "Florian Wetzels"
                        ],
                        "authors": [
                            "Florian Wetzels",
                            "Mathieu Pont",
                            "Julien Tierny",
                            "Christoph Garth"
                        ],
                        "abstract": "Comparative visualization of scalar fields is often facilitated using similarity measures such as edit distances. In this paper, we describe a novel approach for similarity analysis of scalar fields that combines two recently introduced techniques: Wasserstein geodesics/barycenters as well as path mappings, a branch decomposition-independent edit distance. Effectively, we are able to leverage the reduced susceptibility of path mappings to small perturbations in the data when compared with the original Wasserstein distance. Our approach therefore exhibits superior performance and quality in typical tasks such as ensemble summarization, ensemble clustering, and temporal reduction of time series, while retaining practically feasible runtimes. Beyond studying theoretical properties of our approach and discussing implementation aspects, we describe a number of case studies that provide empirical insights into its utility for comparative visualization, and demonstrate the advantages of our method in both synthetic and real-world scenarios. We supply a C++ implementation that can be used to reproduce our results.",
                        "uid": "v-full-1463",
                        "time_stamp": "2023-10-26T04:00:00Z",
                        "time_start": "2023-10-26T04:00:00Z",
                        "time_end": "2023-10-26T04:12:00Z",
                        "paper_type": "full",
                        "keywords": [
                            "Topological data analysis, merge trees, scalar data, ensemble data"
                        ],
                        "doi": "10.1109/TVCG.2023.3326601",
                        "fno": "",
                        "has_image": true,
                        "has_pdf": true,
                        "paper_award": "honorable",
                        "image_caption": "A comparison of the Wasserstein interpolation of merge trees with the novel path mapping interpolation, together with the corresponding mappings embedded in the scalar field. The path mapping distance clearly yields a more meaningful interpolated merge tree.",
                        "external_paper_link": "",
                        "youtube_ff_link": "https://youtu.be/vgeBm-pPER0",
                        "youtube_ff_id": "vgeBm-pPER0",
                        "bunny_ff_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1463/v-full-1463_Preview.mp4?token=wpK3yUE8h0HILc5Hqr5_XmQ1C1islggn14I5hFRp5og&expires=1706590800",
                        "bunny_ff_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1463/v-full-1463_Preview.vtt?token=ygY4rJKrnEEeci7mScUQzwXvcfENGQ48dKRjQg4L7S0&expires=1706590800",
                        "youtube_prerecorded_link": "https://youtu.be/bybxAFqWovA",
                        "youtube_prerecorded_id": "bybxAFqWovA",
                        "bunny_prerecorded_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1463/v-full-1463_Presentation.mp4?token=ch17EZRakMKRxtqZVn4CQoXw3tuLew_ZbH1cK8fYdZw&expires=1706590800",
                        "bunny_prerecorded_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1463/v-full-1463_Presentation.vtt?token=Pwj3YX64ZfCTukKmZQVX9PCmFTy12Jir5IAP6ZSNdhw&expires=1706590800"
                    }
                ]
            },
            {
                "title": "Topology Applications",
                "session_id": "full27",
                "event_prefix": "v-full",
                "track": "oneohsix",
                "session_image": "full27.png",
                "chair": [
                    "Filip Sadlo"
                ],
                "time_start": "2023-10-26T04:45:00Z",
                "time_end": "2023-10-26T06:00:00Z",
                "discord_category": "",
                "discord_channel": "106",
                "discord_channel_id": "1161741373410644119",
                "discord_link": "https://discordapp.com/channels/1161400451619627068/1161741373410644119",
                "zoom_private_meeting": "",
                "zoom_private_password": "",
                "zoom_private_link": "",
                "zoom_broadcast_link": "",
                "ff_link": "https://youtu.be/RqFrvQUZEuM",
                "time_slots": [
                    {
                        "slot_id": "v-full-1609",
                        "session_id": "full27",
                        "title": "TROPHY: A Topologically Robust Physics-Informed Tracking Framework for Tropical Cyclone",
                        "contributors": [
                            "Tom Peterka"
                        ],
                        "authors": [
                            "Lin Yan",
                            "Hanqi Guo",
                            "Tom Peterka",
                            "Bei Wang Phillips",
                            "Jiali Wang"
                        ],
                        "abstract": "Tropical cyclones (TCs) are among the most destructive weather systems. Realistically and efficiently detecting and tracking TCs are critical for assessing their impacts and risks. In particular, the eye is a signature feature of a mature TC. Therefore, knowing the eyes' locations and movements is crucial for both operational weather forecasts and climate risk assessments. Recently, a multilevel robustness framework has been introduced to study the critical points of time-varying vector fields. The framework quantifies the robustness (i.e., structural stability) of critical points across varying neighborhoods. By relating the multilevel robustness with critical point tracking, the framework has demonstrated its potential in cyclone tracking. An advantage is that it identifies cyclonic features using only 2D wind vector fields, which is encouraging as most tracking algorithms require multiple dynamic and thermodynamic variables at different altitudes. A disadvantage is that the framework does not scale well computationally for datasets containing a large number of cyclones.  This paper introduces a topologically robust physics-informed tracking framework (TROPHY) for TC tracking. The main idea is to integrate physical knowledge of TC to drastically improve the computational efficiency of multilevel robustness framework for large-scale climate datasets. First, during preprocessing, we propose a physics-informed feature selection strategy to filter 90% of critical points that are short-lived and have low stability, thus preserving good candidates for TC tracking. Second, during in-processing, we impose constraints during the multilevel robustness computation to focus only on physics-informed neighborhoods of TCs. We apply TROPHY to 30 years of 2D wind fields from reanalysis data in ERA5 and generate a number of TC tracks. In comparison with the observed tracks, we demonstrate that TROPHY can capture TC characteristics (e.g., frequency, intensity, duration, latitudes with maximum intensity, and genesis) that are comparable to and sometimes even better than a well-validated TC tracking algorithm that requires multiple dynamic and thermodynamic scalar fields.",
                        "uid": "v-full-1609",
                        "time_stamp": "2023-10-26T04:45:00Z",
                        "time_start": "2023-10-26T04:45:00Z",
                        "time_end": "2023-10-26T04:57:00Z",
                        "paper_type": "full",
                        "keywords": [
                            "Feature tracking, robustness, topology-based methods in visualization, applications, climate science, tropical cyclones"
                        ],
                        "doi": "10.1109/TVCG.2023.3326905",
                        "fno": "",
                        "has_image": true,
                        "has_pdf": true,
                        "paper_award": "",
                        "image_caption": "Tropical cyclones are among the most destructive weather systems.  This paper introduces a physics-informed tropical cyclone tracking framework, TROPHY, that utilizes tools from vector field topology. We demonstrate that TROPHY can capture tropical cyclones' characteristics that are comparable to and sometimes even better than a well-validated tropical cyclone tracking algorithm while requiring far less input data.",
                        "external_paper_link": "",
                        "youtube_ff_link": "https://youtu.be/Phan4CK2sjM",
                        "youtube_ff_id": "Phan4CK2sjM",
                        "bunny_ff_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1609/v-full-1609_Preview.mp4?token=Rc2F_FsreO2cgYcxyb39IawCFOOid1muWQ6Wc6hBbbs&expires=1706590800",
                        "bunny_ff_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1609/v-full-1609_Preview.vtt?token=BrlcCLK4iu3JBx2ijuEYtLDOvV1WBqFHVEEnhNihFAE&expires=1706590800",
                        "youtube_prerecorded_link": "https://youtu.be/9WGLQVcTbGs",
                        "youtube_prerecorded_id": "9WGLQVcTbGs",
                        "bunny_prerecorded_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1609/v-full-1609_Presentation.mp4?token=NaZUdJOvR38U4CUBo0A4f3EHDgY96A17kN-a6KVrw0k&expires=1706590800",
                        "bunny_prerecorded_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1609/v-full-1609_Presentation.vtt?token=RXWMIlosb-vu67ylWeqhu9hS6kHVIr1thLNlzZlQFmE&expires=1706590800"
                    },
                    {
                        "slot_id": "v-full-1295",
                        "session_id": "full27",
                        "title": "A Local Iterative Approach for the Extraction of 2D Manifolds from Strongly Curved and Folded Thin-Layer Structures",
                        "contributors": [
                            "Nicolas Klenert"
                        ],
                        "authors": [
                            "Nicolas Klenert",
                            "Verena Lepper",
                            "Daniel Baum"
                        ],
                        "abstract": "Ridge surfaces represent important features for the analysis of 3-dimensional (3D) datasets in diverse applications and are often derived from varying underlying data including flow fields, geological fault data, and point data, but they can also be present in the original scalar images acquired using a plethora of imaging techniques. Our work is motivated by the analysis of image data acquired using micro-computed tomography (\u03bcCT) of ancient, rolled and folded thin-layer structures such as papyrus, parchment, and paper as well as silver and lead sheets. From these documents we know that they are 2-dimensional (2D) in nature. Hence, we are particularly interested in reconstructing 2D manifolds that approximate the document\u2019s structure. The image data from which we want to reconstruct the 2D manifolds are often very noisy and represent folded, densely-layered structures with many artifacts, such as ruptures or layer splitting and merging. Previous ridge-surface extraction methods fail to extract the desired 2D manifold for such challenging data. We have therefore developed a novel method to extract 2D manifolds. The proposed method uses a local fast marching scheme in combination with a separation of the region covered by fast marching into two sub-regions. The 2D manifold of interest is then extracted as the surface separating the two sub-regions. The local scheme can be applied for both automatic propagation as well as interactive analysis. We demonstrate the applicability and robustness of our method on both artificial data as well as real-world data including folded silver and papyrus sheets.",
                        "uid": "v-full-1295",
                        "time_stamp": "2023-10-26T04:57:00Z",
                        "time_start": "2023-10-26T04:57:00Z",
                        "time_end": "2023-10-26T05:09:00Z",
                        "paper_type": "full",
                        "keywords": [
                            "Ridge surface, crease surface, 2D manifold extraction, fast marching, virtual unfolding, historical documents"
                        ],
                        "doi": "10.1109/TVCG.2023.3327403",
                        "fno": "",
                        "has_image": true,
                        "has_pdf": true,
                        "paper_award": "",
                        "image_caption": "Volume Rendering of a silver sheet package, blended together with surfaces created by the proposed algorithm. The unfolded textured surfaces are also shown. The colored border help associate the folded and unfolded surfaces with each other.",
                        "external_paper_link": "",
                        "youtube_ff_link": "https://youtu.be/nJWJrZd0KFI",
                        "youtube_ff_id": "nJWJrZd0KFI",
                        "bunny_ff_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1295/v-full-1295_Preview.mp4?token=Lsr1w4T2s0vmyMIRf97D2lg0ApE2Aqw7MHxQ3avsWFQ&expires=1706590800",
                        "bunny_ff_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1295/v-full-1295_Preview.vtt?token=l6joYjcg3OrGSbMx8l_-upBwIu19OgzGrv65fD9xjHg&expires=1706590800",
                        "youtube_prerecorded_link": "https://youtu.be/B10fjAOfFMo",
                        "youtube_prerecorded_id": "B10fjAOfFMo",
                        "bunny_prerecorded_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1295/v-full-1295_Presentation.mp4?token=Zka0wDqQfRj9hssEX7uRVIn6SdonJ1RuwHlEm9IJpZ0&expires=1706590800",
                        "bunny_prerecorded_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1295/v-full-1295_Presentation.vtt?token=2dYQm6-2mERGLCFBwPAtIkFZwX8nzETba4dGVP--9IU&expires=1706590800"
                    },
                    {
                        "slot_id": "v-full-1222",
                        "session_id": "full27",
                        "title": "A Task-Parallel Approach for Localized Topological Data Structures",
                        "contributors": [
                            "Guoxi Liu"
                        ],
                        "authors": [
                            "Guoxi Liu",
                            "Federico Iuricich"
                        ],
                        "abstract": "Unstructured meshes are characterized by data points irregularly distributed in the Euclidian space. Due to the irregular nature of these data, computing connectivity information between the mesh elements requires much more time and memory than on uniformly distributed data. To lower storage costs, dynamic data structures have been proposed. These data structures compute connectivity information on the fly and discard them when no longer needed. However, on-the-fly computation slows down algorithms and results in a negative impact on the time performance. To address this issue, we propose a new task-parallel approach to proactively compute mesh connectivity. Unlike previous approaches implementing data-parallel models, where all threads run the same type of instructions, our task-parallel approach allows threads to run different functions. Specifically, some threads run the algorithm of choice while other threads compute connectivity information before they are actually needed. The approach was implemented in the new Accelerated Clustered TOPOlogical (ACTOPO) data structure, which can support any processing algorithm requiring mesh connectivity information. Our experiments show that ACTOPO combines the benefits of state-of-the-art memory-efficient (TTK CompactTriangulation) and time-efficient (TTK ExplicitTriangulation) topological data structures. It occupies a similar amount of memory as TTK CompactTriangulation while providing up to 5x speedup. Moreover, it achieves comparable time performance as TTK ExplicitTriangulation while using only half of the memory space.",
                        "uid": "v-full-1222",
                        "time_stamp": "2023-10-26T05:09:00Z",
                        "time_start": "2023-10-26T05:09:00Z",
                        "time_end": "2023-10-26T05:21:00Z",
                        "paper_type": "full",
                        "keywords": [
                            "Data structures, parallel computation, topological data analysis, simplicial complex"
                        ],
                        "doi": "10.1109/TVCG.2023.3327182",
                        "fno": "",
                        "has_image": true,
                        "has_pdf": true,
                        "paper_award": "",
                        "image_caption": "The image illustrates our proposed task-parallel approach, showcasing the pipeline with the dragon dataset. It highlights three distinct thread roles and outlines the precomputation methods implemented within the ACTOPO data structure.",
                        "external_paper_link": "",
                        "youtube_ff_link": "https://youtu.be/DX1Pi4V7iQw",
                        "youtube_ff_id": "DX1Pi4V7iQw",
                        "bunny_ff_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1222/v-full-1222_Preview.mp4?token=IFeo1zaE1zzfu6g0VyHQ4zTSR0yok6KsooDJXKT2VCU&expires=1706590800",
                        "bunny_ff_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1222/v-full-1222_Preview.vtt?token=MvYAadbiX1p8TQYmEVAmtKJkZ0I60O4kqN3Ab5DU8-c&expires=1706590800",
                        "youtube_prerecorded_link": "https://youtu.be/K7q5fiqmJq8",
                        "youtube_prerecorded_id": "K7q5fiqmJq8",
                        "bunny_prerecorded_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1222/v-full-1222_Presentation.mp4?token=gaFa_dsmLS09Xvwd2v9P1YeSJzg8r_Obr7bCGsZoHco&expires=1706590800",
                        "bunny_prerecorded_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1222/v-full-1222_Presentation.vtt?token=h9bFn52eQP69Uhqpm7htgcXfud97BfA7q1uNb4kO4dg&expires=1706590800"
                    },
                    {
                        "slot_id": "v-full-1360",
                        "session_id": "full27",
                        "title": "Global Topology of 3D Symmetric Tensor Fields",
                        "contributors": [
                            "Professor Eugene Zhang"
                        ],
                        "authors": [
                            "Shih-Hsuan Hung",
                            "Yue Zhang",
                            "Eugene Zhang"
                        ],
                        "abstract": "There have been recent advances in the analysis and visualization of 3D symmetric tensor fields, with a focus on the robust extraction of tensor field topology. However, topological features such as degenerate curves and neutral surfaces do not live in isolation. Instead, they intriguingly interact with each other. In this paper, we introduce the notion of {\\em topological graph} for 3D symmetric tensor fields to facilitate global topological analysis of such fields. The nodes of the graph include degenerate curves and regions bounded by neutral surfaces in the domain. The edges in the graph denote the adjacency information between the regions and degenerate curves. In addition, we observe that a degenerate curve can be a loop and even a knot and that two degenerate curves (whether in the same region or not) can form a link. We provide a definition and theoretical analysis of individual degenerate curves in order to help understand why knots and links may occur. Moreover, we differentiate between wedges and trisectors, thus making the analysis more detailed about degenerate curves. We incorporate this information into the topological graph. Such a graph can not only reveal the global structure in a 3D symmetric tensor field but also allow two symmetric tensor fields to be compared. We demonstrate our approach by applying it to solid mechanics and material science data sets.",
                        "uid": "v-full-1360",
                        "time_stamp": "2023-10-26T05:21:00Z",
                        "time_start": "2023-10-26T05:21:00Z",
                        "time_end": "2023-10-26T05:33:00Z",
                        "paper_type": "full",
                        "keywords": [
                            "Tensor field visualization, 3D symmetric tensor fields, global tensor field topology, topological graphs, degenerate curves, neutral surfaces, wedges and trisectors"
                        ],
                        "doi": "10.1109/TVCG.2023.3326933",
                        "fno": "",
                        "has_image": true,
                        "has_pdf": true,
                        "paper_award": "",
                        "image_caption": "Existing topology-driven tensor field visualization focuses on the robust extraction of individual features such as degenerate curves and neutral surfaces (left: colored curves and surfaces).     In this paper, we introduce the notion of topological graphs for 3D symmetric tensor fields (right), whose nodes represent individual degenerate curves (colored circles) and volumes bounded by neutral surfaces (colored squares). The edges of the graph encode interactions among the nodes, such as linked degenerate curves, adjacent regions, and a degenerate curve and its container region.     The topological graph provides a holistic and global view of 3D tensor field topology.",
                        "external_paper_link": "",
                        "youtube_ff_link": "https://youtu.be/T_uen4YkRxg",
                        "youtube_ff_id": "T_uen4YkRxg",
                        "bunny_ff_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1360/v-full-1360_Preview.mp4?token=jvMS3LvC50H7FbQLM98cCKYSUDp4aPsFpayySZGB8FI&expires=1706590800",
                        "bunny_ff_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1360/v-full-1360_Preview.vtt?token=epNs0-TKf96lEQaomUjm5ZDpsGK_UQ4N_Q1vJ-HW6-g&expires=1706590800",
                        "youtube_prerecorded_link": "https://youtu.be/tXiELTWc-lA",
                        "youtube_prerecorded_id": "tXiELTWc-lA",
                        "bunny_prerecorded_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1360/v-full-1360_Presentation.mp4?token=WqgD6er63JBz_KYZCEQO3kD07ar40ndFE_FO6IYD_N8&expires=1706590800",
                        "bunny_prerecorded_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1360/v-full-1360_Presentation.vtt?token=aIqK7dDW0YQN-7nl9GkY20y4ufvBodBNJ5upF5Jn0HA&expires=1706590800"
                    },
                    {
                        "slot_id": "v-full-1319",
                        "session_id": "full27",
                        "title": "Interactive Design and Optics-Based Visualization of Arbitrary Non-Euclidean Kaleidoscopic Orbifolds",
                        "contributors": [
                            "Professor Eugene Zhang"
                        ],
                        "authors": [
                            "Jinta Zheng",
                            "Eugene Zhang",
                            "Yue Zhang"
                        ],
                        "abstract": "Orbifolds are a modern mathematical concept that arises in the research of hyperbolic geometry with applications in computer graphics and visualization. In this paper, we make use of rooms with mirrors as the visual metaphor for orbifolds. Given any arbitrary two-dimensional kaleidoscopic orbifold, we provide an algorithm to construct a Euclidean, spherical, or hyperbolic polygon to match the orbifold. This polygon is then used to create a room for which the polygon serves as the floor and the ceiling. With our system that implements M\u00f6bius transformations, the user can interactively edit the scene and see the reflections of the edited objects. To correctly visualize non-Euclidean orbifolds, we adapt the rendering algorithms to account for the geodesics in these spaces, which light rays follow. Our interactive orbifold design system allows the user to create arbitrary two-dimensional kaleidoscopic orbifolds. In addition, our mirror-based orbifold visualization approach has the potential of helping our users gain insight on the orbifold, including its orbifold notation as well as its universal cover, which can also be the spherical space and the hyperbolic space.",
                        "uid": "v-full-1319",
                        "time_stamp": "2023-10-26T05:33:00Z",
                        "time_start": "2023-10-26T05:33:00Z",
                        "time_end": "2023-10-26T05:45:00Z",
                        "paper_type": "full",
                        "keywords": [
                            "Kaleidoscopic Orbifolds, Orbifold Visualization, Math Visualization, Orbifold Construction, Spherical Geometry, Hyperbolic Geometry"
                        ],
                        "doi": "10.1109/TVCG.2023.3326927",
                        "fno": "",
                        "has_image": true,
                        "has_pdf": true,
                        "paper_award": "",
                        "image_caption": "In this paper, we provide an algorithm to generate arbitrary two-dimensional non-Euclidean kaleidoscopic orbifolds. The example shown in this figure is a hyperbolic orbifold with six sides. The orders of symmetries at the six corners are respectively 2, 3, 4, 5, 6, and 7. Note that at a corner of order N, there are 2N sectors. Notice the reflections of the words \"Non-Euclidean Orbifold\" in the universal cover of the orbifold, the disk. The polygonal layout generated from our algorithm can be used as the ceiling and floor of a 3D room, thus enabling our mirror-based visualization for kaleidoscopic orbifolds.",
                        "external_paper_link": "",
                        "youtube_ff_link": "https://youtu.be/HZauUsDoKao",
                        "youtube_ff_id": "HZauUsDoKao",
                        "bunny_ff_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1319/v-full-1319_Preview.mp4?token=qaQOnYwgOZR7FiBW9oYhPCSqJ-2gLuKdQrpuU6P1GXg&expires=1706590800",
                        "bunny_ff_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1319/v-full-1319_Preview.vtt?token=DsG0TZXpUwIpiw10H9waqmi4LVTSeNZ6dNZsN2c42-Y&expires=1706590800",
                        "youtube_prerecorded_link": "https://youtu.be/C0cjlLlOM_0",
                        "youtube_prerecorded_id": "C0cjlLlOM_0",
                        "bunny_prerecorded_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1319/v-full-1319_Presentation.mp4?token=o9P65Fci__fq0Nz2__rOGtt0fD9JCbAigx8rmSIvBrs&expires=1706590800",
                        "bunny_prerecorded_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1319/v-full-1319_Presentation.vtt?token=G3_M-Z1LUSVOk1xvs0SCOIAT9nUruW3xZ6mp97i6J-I&expires=1706590800"
                    },
                    {
                        "slot_id": "v-full-1616",
                        "session_id": "full27",
                        "title": "TopoSZ: Preserving Topology in Error-Bounded Lossy Compression",
                        "contributors": [
                            "Bei Wang"
                        ],
                        "authors": [
                            "Lin Yan",
                            "Xin Liang",
                            "Hanqi Guo",
                            "Bei Wang Phillips"
                        ],
                        "abstract": "Existing error-bounded lossy compression techniques control the pointwise error during compression to guarantee the integrity of the decompressed data. However, they typically do not explicitly preserve the topological features in data. When performing post hoc analysis with decompressed data using topological methods, preserving topology in the compression process to obtain topologically consistent and correct scientific insights is desirable. In this paper, we introduce TopoSZ, an error-bounded lossy compression method that preserves the topological features in 2D and 3D scalar fields. Specifically, we aim to preserve the types and locations of local extrema as well as the level set relations among critical points captured by contour trees in the decompressed data. The main idea is to derive topological constraints from contour-tree-induced segmentation from the data domain, and incorporate such constraints with a customized error-controlled quantization strategy from the SZ compressor (version 1.4). Our method allows users to control the pointwise error and the loss of topological features during the compression process with a global error bound and a persistence threshold.",
                        "uid": "v-full-1616",
                        "time_stamp": "2023-10-26T05:45:00Z",
                        "time_start": "2023-10-26T05:45:00Z",
                        "time_end": "2023-10-26T05:57:00Z",
                        "paper_type": "full",
                        "keywords": [
                            "Lossy compression, contour tree, topology preservation, topological data analysis, topology in visualization"
                        ],
                        "doi": "10.1109/TVCG.2023.3326920",
                        "fno": "",
                        "has_image": true,
                        "has_pdf": true,
                        "paper_award": "",
                        "image_caption": "This paper introduces TopoSZ, an error-bounded lossy compression method that preserves the topological features in 2D and 3D scalar fields. The main idea is to derive topological constraints from contour-tree-induced segmentation from the data domain and incorporate such constraints iteratively with a customized error-controlled quantization strategy. TopoSZ allows users to control the pointwise error and the loss of topological features during the compression process with a global error bound and a persistence threshold.",
                        "external_paper_link": "",
                        "youtube_ff_link": "https://youtu.be/FahwYhReces",
                        "youtube_ff_id": "FahwYhReces",
                        "bunny_ff_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1616/v-full-1616_Preview.mp4?token=MhwrH-3M-fJuXzgoGlkaSw5naRcbL2EXoJhVA9evZuI&expires=1706590800",
                        "bunny_ff_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1616/v-full-1616_Preview.vtt?token=I9zOmDNoADPgJTB-qd2ikun9M-24ySNUcQDpWBl4lUU&expires=1706590800",
                        "youtube_prerecorded_link": "https://youtu.be/LfqGC3UEkEY",
                        "youtube_prerecorded_id": "LfqGC3UEkEY",
                        "bunny_prerecorded_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1616/v-full-1616_Presentation.mp4?token=81ywlqgD4TYVPzaN2A2BoSUvPTmk2QGBWER6p4Kqr38&expires=1706590800",
                        "bunny_prerecorded_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1616/v-full-1616_Presentation.vtt?token=aJhOmEZ1PXqBXiMeAe5rqrZjSstuvxZrUUYTkcHSUT8&expires=1706590800"
                    }
                ]
            },
            {
                "title": "Trust and Bias",
                "session_id": "full28",
                "event_prefix": "v-full",
                "track": "oneohnine",
                "session_image": "full28.png",
                "chair": [
                    "Evanthia Dimara"
                ],
                "time_start": "2023-10-24T23:45:00Z",
                "time_end": "2023-10-25T01:00:00Z",
                "discord_category": "",
                "discord_channel": "109",
                "discord_channel_id": "1161741434647482379",
                "discord_link": "https://discordapp.com/channels/1161400451619627068/1161741434647482379",
                "zoom_private_meeting": "",
                "zoom_private_password": "",
                "zoom_private_link": "",
                "zoom_broadcast_link": "",
                "ff_link": "https://youtu.be/fB_TMMLihpM",
                "time_slots": [
                    {
                        "slot_id": "v-tvcg-10002893",
                        "session_id": "full28",
                        "title": "Reasoning Affordances with Tables and Bar Charts",
                        "contributors": [
                            "Cindy Xiong"
                        ],
                        "authors": [
                            "Cindy Xiong Bearfield",
                            "Elsie Lee-Robbins",
                            "Icy Zhang",
                            "Aimen Gaba",
                            "Steven L Franconeri"
                        ],
                        "abstract": "A viewer\u2019s existing beliefs can prevent accurate reasoning with data visualizations. In particular, confirmation bias can cause people to overweigh information that confirms their beliefs, and dismiss information that disconfirms them. We tested whether confirmation bias exists when people reason with visualized data and whether certain visualization designs can elicit less biased reasoning strategies. We asked crowd workers to solve reasoning problems that had the potential to evoke both poor reasoning strategies and confirmation bias. We created two scenarios, one in which we primed people with a belief before asking them to make a decision, and another in which people held pre-existing beliefs. The data was presented as either a table, a bar table, or a bar chart. To correctly solve the problem, participants should use a complex reasoning strategy to compare two ratios, each between two pairs of values. But participants could also be tempted to use simpler, superficial heuristics, shortcuts, or biased strategies to reason about the problem. Presenting the data in a table format helped participants reason with the correct ratio strategy while showing the data as a bar table or a bar chart led participants towards incorrect heuristics. Confirmation bias was not significantly present when beliefs were primed, but it was present when beliefs were pre-existing. Additionally, the table presentation format was more likely to afford the ratio reasoning strategy, and the use of the ratio strategy was more likely to lead to the correct answer. These findings suggest that data presentation formats can affect affordances for reasoning.",
                        "uid": "v-tvcg-10002893",
                        "time_stamp": "2023-10-24T23:45:00Z",
                        "time_start": "2023-10-24T23:45:00Z",
                        "time_end": "2023-10-24T23:57:00Z",
                        "paper_type": "full",
                        "keywords": [
                            "Data visualization;Tabular displays;Empirical evaluation;Reasoning"
                        ],
                        "doi": "10.1109/TVCG.2022.3232959",
                        "fno": "10002893",
                        "has_image": true,
                        "has_pdf": false,
                        "paper_award": "",
                        "image_caption": "We studied confirmation bias in visualizations, asking participants to solve a hard reasoning problem. We tested if using a bar chart, a bar table, or a table would lead to less biased reasoning strategies. Participants were often tempted to use simple heuristics, only comparing a few data points. With a table, participants were more likely to solve it correctly, using a complex reasoning strategy of comparing two ratios.",
                        "external_paper_link": "",
                        "youtube_ff_link": "https://youtu.be/eZmaAmtTMhw",
                        "youtube_ff_id": "eZmaAmtTMhw",
                        "bunny_ff_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-tvcg/v-tvcg-10002893/v-tvcg-10002893_Preview.mp4?token=4TUONu8hDwbkTcZ5j16gmCNFg0YUqZuOvGOQt0sNkQE&expires=1706590800",
                        "bunny_ff_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-tvcg/v-tvcg-10002893/v-tvcg-10002893_Preview.vtt?token=MSDdLEgW6TH_fU0haDgU-w89CXtKN24yyc7ZSFJ96Ok&expires=1706590800",
                        "youtube_prerecorded_link": "https://youtu.be/PhyTrIjhh5Y",
                        "youtube_prerecorded_id": "PhyTrIjhh5Y",
                        "bunny_prerecorded_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-tvcg/v-tvcg-10002893/v-tvcg-10002893_Presentation.mp4?token=dYpme_8LyJYtTGoIzROLY0p6FcNEIk-9KJQH-ulqsEE&expires=1706590800",
                        "bunny_prerecorded_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-tvcg/v-tvcg-10002893/v-tvcg-10002893_Presentation.vtt?token=_wFy6USOuawRBBE3VikhZLla7Kry8MNovTLDzDJu-jE&expires=1706590800"
                    },
                    {
                        "slot_id": "v-full-1531",
                        "session_id": "full28",
                        "title": "Average Estimates in Line Graphs are Biased Towards Areas of Higher Variability",
                        "contributors": [
                            "Dominik Moritz"
                        ],
                        "authors": [
                            "Dominik Moritz",
                            "Lace M. Padilla",
                            "Francis Nguyen",
                            "Steven L Franconeri"
                        ],
                        "abstract": "We investigate variability overweighting, a previously undocumented bias in line graphs, where estimates of average value are biased toward areas of higher variability in that line. We found this effect across two preregistered experiments with 140 and 420 participants. These experiments also show that the bias is reduced when using a dot encoding of the same series. We can model the bias with the average of the data series and the average of the points drawn along the line. This bias might arise because higher variability leads to stronger weighting in the average calculation, either due to the longer line segments (even though those segments contain the same number of data values) or line segments with higher variability being otherwise more visually salient. Understanding and predicting this bias is important for visualization design guidelines, recommendation systems, and tool builders, as the bias can adversely affect estimates of averages and trends.",
                        "uid": "v-full-1531",
                        "time_stamp": "2023-10-24T23:57:00Z",
                        "time_start": "2023-10-24T23:57:00Z",
                        "time_end": "2023-10-25T00:09:00Z",
                        "paper_type": "full",
                        "keywords": [
                            "bias, lines graph, ensemble perception, average"
                        ],
                        "doi": "10.1109/TVCG.2023.3326589",
                        "fno": "",
                        "has_image": true,
                        "has_pdf": true,
                        "paper_award": "honorable",
                        "image_caption": "Demonstration of the bias toward variability. The red line shows the mean estimated averages across all participants in one of two experiments. The line chart shows a bias of the estimated average toward higher variability in the higher y-values. The bias is smallest when the data is shown as points equally spaced along the x-axis and can be simulated using points sampled at equal intervals along the arc of the line.",
                        "external_paper_link": "",
                        "youtube_ff_link": "https://youtu.be/yhMcIVE-hDE",
                        "youtube_ff_id": "yhMcIVE-hDE",
                        "bunny_ff_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1531/v-full-1531_Preview.mp4?token=_v5XxlTK_KANIu5_mUd-kgiIUsEHc5PHsMf4xK93wvw&expires=1706590800",
                        "bunny_ff_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1531/v-full-1531_Preview.vtt?token=g5YLzKvNqDkA5OJImxblVp6s30lbQVmMlLvpFqpzb14&expires=1706590800",
                        "youtube_prerecorded_link": "https://youtu.be/XTA-DNCITtA",
                        "youtube_prerecorded_id": "XTA-DNCITtA",
                        "bunny_prerecorded_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1531/v-full-1531_Presentation.mp4?token=S2CmS-w5NojwTyXDV2mTgv8J_jqTUD-KOZViuBnuSnI&expires=1706590800",
                        "bunny_prerecorded_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1531/v-full-1531_Presentation.vtt?token=QY5Mq4LM184sTVoYCOWrxacKKfsTi8jaUGaw72lNCEE&expires=1706590800"
                    },
                    {
                        "slot_id": "v-full-1196",
                        "session_id": "full28",
                        "title": "Eleven Years of Gender Data Visualization: Towards more Inclusive Gender Representation",
                        "contributors": [
                            "Florent Cabric"
                        ],
                        "authors": [
                            "Florent Cabric",
                            "Margret Vilborg Bjarnadottir",
                            "Meng Ling",
                            "Gudbj\u00f6rg Linda Rafnsd\u00f3ttir",
                            "Petra Isenberg"
                        ],
                        "abstract": "We present an analysis of the representation of gender as a data dimension in data visualizations and propose a set of considerations around visual variables and annotations for gender-related data. Gender is a common demographic dimension of data collected from study or survey participants, passengers, or customers, as well as across academic studies, especially in certain disciplines like sociology. Our work contributes to multiple ongoing discussions on the ethical implications of data visualizations. By choosing specifc data, visual variables, and text labels, visualization designers may, inadvertently or not, perpetuate stereotypes and biases. Here, our goal is to start an evolving discussion on how to represent data on gender in data visualizations and raise awareness of the subtleties of choosing visual variables and words in gender visualizations. In order to ground this discussion, we collected and coded gender visualizations and their captions from fve different scientifc communities (Biology, Politics, Social Studies, Visualisation, and Human-Computer Interaction), in addition to images from Tableau Public and the Information Is Beautiful awards showcase. Overall we found that representation types are community-specifc, color hue is the dominant visual channel for gender data, and nonconforming gender is under-represented. We end our paper with a discussion of considerations for gender visualization derived from our coding and the literature and recommendations for large data collection bodies. A free copy of this paper and all supplemental materials are available at https://osf.io/v9ams/.",
                        "uid": "v-full-1196",
                        "time_stamp": "2023-10-25T00:09:00Z",
                        "time_start": "2023-10-25T00:09:00Z",
                        "time_end": "2023-10-25T00:21:00Z",
                        "paper_type": "full",
                        "keywords": [
                            "Visualization, gender, visual gender representation, ethics"
                        ],
                        "doi": "10.1109/TVCG.2023.3327369",
                        "fno": "",
                        "has_image": true,
                        "has_pdf": true,
                        "paper_award": "",
                        "image_caption": "The colors used by scientists to represent women and men. Links show associations between colors. For example, when women are represented in red, men are almost exclusively represented in blue.",
                        "external_paper_link": "",
                        "youtube_ff_link": "https://youtu.be/nVE_sjdWUok",
                        "youtube_ff_id": "nVE_sjdWUok",
                        "bunny_ff_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1196/v-full-1196_Preview.mp4?token=IfRWV82LcDQJ71DJNWOYAGeUwVacXFNfq6WBUuAsdPc&expires=1706590800",
                        "bunny_ff_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1196/v-full-1196_Preview.vtt?token=QKRHjRP3vusppF5hxRyXQYbDBJtWLR-sCTzySVjnQCs&expires=1706590800",
                        "youtube_prerecorded_link": "https://youtu.be/lqImDkBWpe8",
                        "youtube_prerecorded_id": "lqImDkBWpe8",
                        "bunny_prerecorded_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1196/v-full-1196_Presentation.mp4?token=pY9mDcLhwI5Ge9UeosXtrz27rlkDMD90Yq9ti48RCgU&expires=1706590800",
                        "bunny_prerecorded_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1196/v-full-1196_Presentation.vtt?token=EubUZiAAoDMVi_TBJ6hfgs-kDkZHkRNIwuRzRPpQ2sU&expires=1706590800"
                    },
                    {
                        "slot_id": "v-full-1073",
                        "session_id": "full28",
                        "title": "My Model is Unfair, Do People Even Care? Visual Design Affects Trust and Perceived Bias in Machine Learning",
                        "contributors": [
                            "Aimen Gaba, Zhanna Kaufman"
                        ],
                        "authors": [
                            "Aimen Gaba",
                            "Zhanna Kaufman",
                            "Jason Cheung",
                            "Marie Shvakel",
                            "Kyle Wm Hall",
                            "Yuriy Brun",
                            "Cindy Xiong Bearfield"
                        ],
                        "abstract": "Machine learning technology has become ubiquitous, but, unfortunately, often exhibits bias. As a consequence, disparate stakeholders need to interact with and make informed decisions about using machine learning models in everyday systems. Visualization technology can support stakeholders in understanding and evaluating trade-offs between, for example, accuracy and fairness of models. This paper aims to empirically answer \"Can visualization design choices affect a stakeholder's perception of model bias, trust in a model, and willingness to adopt a model?'\" Through a series of controlled, crowd-sourced experiments with more than 1,500 participants, we identify a set of strategies people follow in deciding which models to trust. Our results show that men and women prioritize fairness and performance differently and that visual design choices significantly affect that prioritization. For example, women trust fairer models more often than men do, participants value fairness more when it is explained using text than as a bar chart, and being explicitly told a model is biased has a bigger impact than showing past biased performance. We test the generalizability of our results by comparing the effect of multiple textual and visual design choices and offer potential explanations of the cognitive mechanisms behind the difference in fairness perception and trust. Our research guides design considerations to support future work developing visualization systems for machine learning.",
                        "uid": "v-full-1073",
                        "time_stamp": "2023-10-25T00:21:00Z",
                        "time_start": "2023-10-25T00:21:00Z",
                        "time_end": "2023-10-25T00:33:00Z",
                        "paper_type": "full",
                        "keywords": [
                            "machine learning, fairness, bias, trust, visual design, gender, human-subjects studies"
                        ],
                        "doi": "10.1109/TVCG.2023.3327192",
                        "fno": "",
                        "has_image": true,
                        "has_pdf": true,
                        "paper_award": "",
                        "image_caption": "We conducted a user study, asking participants to select between two automated investors, one historically producing fair returns for men and women (top left) and the other producing higher returns but exhibiting sexist behavior, favoring either men or women (top right).  We found that women valued fairness more than men, regardless of whether bias hurt men or women (bottom left), and that using text to describe the historical returns, as opposed to bar charts, resulted in more participants selecting fair investors (bottom right).",
                        "external_paper_link": "",
                        "youtube_ff_link": "https://youtu.be/CsugZupQSX0",
                        "youtube_ff_id": "CsugZupQSX0",
                        "bunny_ff_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1073/v-full-1073_Preview.mp4?token=oFqPJQaFDIdDGkBiPhhyWkqGOAYC5mV-m4ixNCbGGn0&expires=1706590800",
                        "bunny_ff_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1073/v-full-1073_Preview.vtt?token=EtIe-qaulAKN7RqYblHexr9HtD4v_QcYW4CCbHb91p4&expires=1706590800",
                        "youtube_prerecorded_link": "https://youtu.be/sjnUF7NZL14",
                        "youtube_prerecorded_id": "sjnUF7NZL14",
                        "bunny_prerecorded_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1073/v-full-1073_Presentation.mp4?token=ubjz-NxLtsmyF2mBaI1EpTFlNnjc9LNllrc2TPY9hcQ&expires=1706590800",
                        "bunny_prerecorded_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1073/v-full-1073_Presentation.vtt?token=hVbV-bE3scjPYjojPzmtMr2UU-UtzlePnn0bO1b8tro&expires=1706590800"
                    },
                    {
                        "slot_id": "v-full-1778",
                        "session_id": "full28",
                        "title": "The Rational Agent Benchmark for Data Visualization",
                        "contributors": [
                            "Yifan Wu"
                        ],
                        "authors": [
                            "Yifan Wu",
                            "Ziyang Guo",
                            "Michalis Mamakos",
                            "Jason Hartline",
                            "Jessica Hullman"
                        ],
                        "abstract": "Understanding how helpful a visualization is from experimental results is difficult because the observed performance is confounded with aspects of the study design, such as how useful the information that is visualized is for the task. We develop a rational agent framework for designing and interpreting visualization experiments. Our framework conceives two experiments with the same setup: one with behavioral agents (human subjects), and the other one with a hypothetical rational agent. A visualization is evaluated by comparing the expected performance of behavioral agents to that of a rational agent under different assumptions. Using recent visualization decision studies from the literature, we demonstrate how the framework can be used to pre-experimentally evaluate the experiment design by bounding the expected improvement in performance from having access to visualizations, and post-experimentally to deconfound errors of information extraction from errors of optimization, among other analyses.",
                        "uid": "v-full-1778",
                        "time_stamp": "2023-10-25T00:33:00Z",
                        "time_start": "2023-10-25T00:33:00Z",
                        "time_end": "2023-10-25T00:45:00Z",
                        "paper_type": "full",
                        "keywords": [
                            "Evaluation, decision-making, rational agent, scoring rule"
                        ],
                        "doi": "10.1109/TVCG.2023.3326513",
                        "fno": "",
                        "has_image": true,
                        "has_pdf": true,
                        "paper_award": "",
                        "image_caption": "Understanding how helpful a visualization is from experimental results is difficult because the observed performance is confounded with aspects of the study design, such as how useful the information that is visualized is for the task. We develop a rational agent framework for designing and interpreting visualization experiments. Our framework conceives two experiments with the same setup: one with behavioral agents (human subjects), and the other one with a hypothetical rational agent. Our framework can be used to pre-experimentally and post-experimentally evaluate the experiment design.",
                        "external_paper_link": "",
                        "youtube_ff_link": "https://youtu.be/ZcqYd0O_7ps",
                        "youtube_ff_id": "ZcqYd0O_7ps",
                        "bunny_ff_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1778/v-full-1778_Preview.mp4?token=0_wMuibBqLhmon6sfbGdsOG6Gs4yvmAb7X_V2Xd44h0&expires=1706590800",
                        "bunny_ff_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1778/v-full-1778_Preview.vtt?token=vPvE6ljAL52MVcwjKbABXs4xyrlJaU9eiPmQd0qh_ug&expires=1706590800",
                        "youtube_prerecorded_link": "https://youtu.be/1dOwlsp-0K0",
                        "youtube_prerecorded_id": "1dOwlsp-0K0",
                        "bunny_prerecorded_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1778/v-full-1778_Presentation.mp4?token=7E2vMgpjc1zKDZxdTaPP_u6e81tjysG6JVBcfuK8M3Y&expires=1706590800",
                        "bunny_prerecorded_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1778/v-full-1778_Presentation.vtt?token=B5rh1DwE63B1Retx8KBbqS8z-nMC0kAo4PyRDSRnb6U&expires=1706590800"
                    },
                    {
                        "slot_id": "v-full-1522",
                        "session_id": "full28",
                        "title": "Vistrust: a Multidimensional Framework and Empirical Study of Trust in Data Visualizations",
                        "contributors": [
                            "Hamza Elhamdadi"
                        ],
                        "authors": [
                            "Hamza Elhamdadi",
                            "Adam Stefkovics",
                            "Johanna Beyer",
                            "Eric Moerth",
                            "Hanspeter Pfister",
                            "Cindy Xiong Bearfield",
                            "Carolina Nobre"
                        ],
                        "abstract": "Trust is an essential aspect of data visualization, as it plays a crucial role in the interpretation and decision-making processes of users. While research in social sciences outlines the multi-dimensional factors that can play a role in trust formation, most data visualization trust researchers employ a single-item scale to measure trust. We address this gap by proposing a comprehensive, multidimensional conceptualization and operationalization of trust in visualization. We do this by applying general theories of trust from social sciences, as well as synthesizing and extending earlier work and factors identified by studies in the visualization field. We apply a two-dimensional approach to trust in visualization, to distinguish between cognitive and affective elements, as well as between visualization and data-specific trust antecedents. We use our framework to design and run a large crowd-sourced study to quantify the role of visual complexity in establishing trust in science visualizations. Our study provides empirical evidence for several aspects of our proposed theoretical framework, most notably the impact of cognition, affective responses, and individual differences when establishing trust in visualizations.",
                        "uid": "v-full-1522",
                        "time_stamp": "2023-10-25T00:45:00Z",
                        "time_start": "2023-10-25T00:45:00Z",
                        "time_end": "2023-10-25T00:57:00Z",
                        "paper_type": "full",
                        "keywords": [
                            "Trust, visualization, science, framework"
                        ],
                        "doi": "10.1109/TVCG.2023.3326579",
                        "fno": "",
                        "has_image": true,
                        "has_pdf": true,
                        "paper_award": "",
                        "image_caption": "An integrated framework, which outlines the development of trust in visualizations. The framework defines the different trust antecedents of the two basic components of trust (cognitive and affective trust). Both cognitive and affective trust can relate to the visualization and the underlying data. Individual characteristics can play a role in shaping one's level of trust in visualizations, and behavioral outcomes can emerge as a results of trust judgements.",
                        "external_paper_link": "",
                        "youtube_ff_link": "https://youtu.be/Mb83yBTxJY4",
                        "youtube_ff_id": "Mb83yBTxJY4",
                        "bunny_ff_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1522/v-full-1522_Preview.mp4?token=tT1Z--igamMRKhOXYV_6geRljhA63O-JS0PJAx6nS0k&expires=1706590800",
                        "bunny_ff_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1522/v-full-1522_Preview.vtt?token=loL0IQ0xwB0XZIRFXnL-I24POa0gyasea7YNynNF0MI&expires=1706590800",
                        "youtube_prerecorded_link": "https://youtu.be/gFXeHdqm6vM",
                        "youtube_prerecorded_id": "gFXeHdqm6vM",
                        "bunny_prerecorded_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1522/v-full-1522_Presentation.mp4?token=9wBB4WS14mNZUPXJEhDO2ihhIelNzyw9nKLXfALiwIQ&expires=1706590800",
                        "bunny_prerecorded_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1522/v-full-1522_Presentation.vtt?token=8Nk87Eo_TRnbf85RHnL05_LgQkMAQ21-lGXpTHAU9Qo&expires=1706590800"
                    }
                ]
            },
            {
                "title": "VIS for Data Scientists",
                "session_id": "full29",
                "event_prefix": "v-full",
                "track": "oneohnine",
                "session_image": "full29.png",
                "chair": [
                    "Kate Isaacs"
                ],
                "time_start": "2023-10-24T22:00:00Z",
                "time_end": "2023-10-24T23:15:00Z",
                "discord_category": "",
                "discord_channel": "109",
                "discord_channel_id": "1161741434647482379",
                "discord_link": "https://discordapp.com/channels/1161400451619627068/1161741434647482379",
                "zoom_private_meeting": "",
                "zoom_private_password": "",
                "zoom_private_link": "",
                "zoom_broadcast_link": "",
                "ff_link": "https://youtu.be/yxHu7Fjiy0Y",
                "time_slots": [
                    {
                        "slot_id": "v-tvcg-10077087",
                        "session_id": "full29",
                        "title": "Are Metrics Enough? Guidelines for Communicating and Visualizing Predictive Models to Subject Matter Experts",
                        "contributors": [
                            "Ashley Suh"
                        ],
                        "authors": [
                            "Ashley Suh",
                            "Gabriel Appleby",
                            "Erik W. Anderson",
                            "Luca Finelli",
                            "Remco Chang",
                            "Dylan Cashman"
                        ],
                        "abstract": "Presenting a predictive model's performance is a communication bottleneck that threatens collaborations between data scientists and subject matter experts.  Accuracy and error metrics alone fail to tell the whole story of a model \u2013 its risks, strengths, and limitations \u2013 making it difficult for subject matter experts to feel confident in their decision to use a model.  As a result, models may fail in unexpected ways or go entirely unused, as subject matter experts disregard poorly presented models in favor of familiar, yet arguably substandard methods.  In this paper, we describe an iterative study conducted with both subject matter experts and data scientists to understand the gaps in communication between these two groups.  We find that, while the two groups share common goals of understanding the data and predictions of the model, friction can stem from unfamiliar terms, metrics, and visualizations \u2013 limiting the transfer of knowledge to SMEs and discouraging clarifying questions being asked during presentations.  Based on our findings, we derive a set of communication guidelines that use visualization as a common medium for communicating the strengths and weaknesses of a model.  We provide a demonstration of our guidelines in a regression modeling scenario and elicit feedback on their use from subject matter experts.  From our demonstration, subject matter experts were more comfortable discussing a model's performance, more aware of the trade-offs for the presented model, and better equipped to assess the model's risks \u2013 ultimately informing and contextualizing the model's use beyond text and numbers.",
                        "uid": "v-tvcg-10077087",
                        "time_stamp": "2023-10-24T22:00:00Z",
                        "time_start": "2023-10-24T22:00:00Z",
                        "time_end": "2023-10-24T22:12:00Z",
                        "paper_type": "full",
                        "keywords": [
                            "Visualization techniques and methodologies;Human factors;Modeling and prediction;Data communications aspects"
                        ],
                        "doi": "10.1109/TVCG.2023.3259341",
                        "fno": "10077087",
                        "has_image": true,
                        "has_pdf": false,
                        "paper_award": "",
                        "image_caption": "An image showing a set of model performance visualizations that can be used by data scientists to communicate predictive models to subject matter experts. Underneath, a ScatterText analysis depicting the differences and commonalities in words spoken during interviews with data scientists and subject matter experts. On the right, a table with communication and visualization guidelines with subject matter experts' feedback on each of them.",
                        "external_paper_link": "",
                        "youtube_ff_link": "https://youtu.be/8GMNolp5v7Y",
                        "youtube_ff_id": "8GMNolp5v7Y",
                        "bunny_ff_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-tvcg/v-tvcg-10077087/v-tvcg-10077087_Preview.mp4?token=MW3Xe1F5lb4dHGAzBR7HS-L2B3K2zAOaQd6Oeq-SrU0&expires=1706590800",
                        "bunny_ff_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-tvcg/v-tvcg-10077087/v-tvcg-10077087_Preview.vtt?token=vnZhJOzuDSJYg7rPcY1lwpLqf64JW6hsl-23dReZFe0&expires=1706590800",
                        "youtube_prerecorded_link": "https://youtu.be/Uya0038s5ho",
                        "youtube_prerecorded_id": "Uya0038s5ho",
                        "bunny_prerecorded_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-tvcg/v-tvcg-10077087/v-tvcg-10077087_Presentation.mp4?token=sGmWPK9AjjuFkDcq_dolOIvsiCuUQsGyqKtHhP06sN4&expires=1706590800",
                        "bunny_prerecorded_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-tvcg/v-tvcg-10077087/v-tvcg-10077087_Presentation.vtt?token=Mn2UhMwi9uOjhkqYFFJJ8xBDnz0UmP4SvlZYJUii3yg&expires=1706590800"
                    },
                    {
                        "slot_id": "v-full-1559",
                        "session_id": "full29",
                        "title": "Dataopsy: Scalable and Fluid Visual Exploration using Aggregate Query Sculpting",
                        "contributors": [
                            "Md Naimul Hoque"
                        ],
                        "authors": [
                            "Md Naimul Hoque",
                            "Niklas Elmqvist"
                        ],
                        "abstract": "We present aggregate query sculpting (AQS), a faceted visual query technique for large-scale multidimensional data. As a \"born scalable\" query technique, AQS starts visualization with a single visual mark representing an aggregation of the entire dataset. The user can then progressively explore the dataset through a sequence of operations abbreviated as P6: pivot (facet an aggregate based on an attribute), partition (lay out a facet in space), peek (see inside a subset using an aggregate visual representation), pile (merge two or more subsets), project (extracting a subset into a new substrate), and prune (discard an aggregate not currently of interest). We validate AQS with Dataopsy, a prototype implementation of AQS that has been designed for fluid interaction on desktop and touch-based mobile devices. We demonstrate AQS and Dataopsy using two case studies and three application examples.",
                        "uid": "v-full-1559",
                        "time_stamp": "2023-10-24T22:12:00Z",
                        "time_start": "2023-10-24T22:12:00Z",
                        "time_end": "2023-10-24T22:24:00Z",
                        "paper_type": "full",
                        "keywords": [
                            "Multidimensional data visualization, multivariate graphs, visual queries, visual exploration."
                        ],
                        "doi": "10.1109/TVCG.2023.3326594",
                        "fno": "",
                        "has_image": true,
                        "has_pdf": true,
                        "paper_award": "",
                        "image_caption": "A screenshot of the Dataopsy system, showing different subsets of a dataset.",
                        "external_paper_link": "",
                        "youtube_ff_link": "https://youtu.be/QQRfR7cqtLk",
                        "youtube_ff_id": "QQRfR7cqtLk",
                        "bunny_ff_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1559/v-full-1559_Preview.mp4?token=V8T5OQIe0duQNxRP-xHxpqdAiaVxWbdOWwGdt8AnL9M&expires=1706590800",
                        "bunny_ff_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1559/v-full-1559_Preview.vtt?token=gVsnrbncg4JPOVzgIEqw7neScaoz0Oi3iikH-mlVoY8&expires=1706590800",
                        "youtube_prerecorded_link": "",
                        "youtube_prerecorded_id": "",
                        "bunny_prerecorded_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1559/v-full-1559_Presentation.mp4?token=hw1V7vMW6HMGyV6CQRDD3rjlKgpfHr0sgD1HnNF1DiE&expires=1706590800",
                        "bunny_prerecorded_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1559/v-full-1559_Presentation.vtt?token=uo5JFwbG5GdyqccgpS8am7BuE3I7uDZ2Ihe3zz5kwqM&expires=1706590800"
                    },
                    {
                        "slot_id": "v-full-1517",
                        "session_id": "full29",
                        "title": "Dead or Alive: Continuous Data Profiling for Interactive Data Science",
                        "contributors": [
                            "Will Epperson"
                        ],
                        "authors": [
                            "Will Epperson",
                            "Vaishnavi Gorantla",
                            "Dominik Moritz",
                            "Adam Perer"
                        ],
                        "abstract": "Profiling data by plotting distributions and analyzing summary statistics is a critical step throughout data analysis. Currently, this process is manual and tedious since analysts must write extra code to examine their data after every transformation. This inefficiency may lead to data scientists profiling their data infrequently, rather than after each transformation, making it easy for them to miss important errors or insights. We propose continuous data profiling as a process that allows analysts to immediately see interactive visual summaries of their data throughout their data analysis to facilitate fast and thorough analysis. Our system, AutoProfiler, presents three ways to support continuous data profiling: (1) it automatically displays data distributions and summary statistics to facilitate data comprehension; (2) it is live, so visualizations are always accessible and update automatically as the data updates; (3) it supports follow up analysis and documentation by authoring code for the user in the notebook. In a user study with 16 participants, we evaluate two versions of our system that integrate different levels of automation: both automatically show data profiles and facilitate code authoring, however, one version updates reactively (\"live\") and the other updates only on demand (\"dead\"). We find that both tools, dead or alive, facilitate insight discovery with 91% of user-generated insights originating from the tools rather than manual profiling code written by users. Participants found live updates intuitive and felt it helped them verify their transformations while those with on-demand profiles liked the ability to look at past visualizations. We also present a longitudinal case study on how AutoProfiler helped domain scientists find serendipitous insights about their data through automatic, live data profiles. Our results have implications for the design of future tools that offer automated data analysis support.",
                        "uid": "v-full-1517",
                        "time_stamp": "2023-10-24T22:24:00Z",
                        "time_start": "2023-10-24T22:24:00Z",
                        "time_end": "2023-10-24T22:36:00Z",
                        "paper_type": "full",
                        "keywords": [
                            "Data Profiling, Data Quality, Exploratory Data Analysis, Interactive Data Science"
                        ],
                        "doi": "10.1109/TVCG.2023.3327367",
                        "fno": "",
                        "has_image": true,
                        "has_pdf": true,
                        "paper_award": "honorable",
                        "image_caption": "We present our system AutoProfiler for continuous data profiling in Jupyter. AutoProfiler helps users understand their data and quality issues through automatic EDA information, live updates, and writing analysis code for users. Learn more at https://github.com/cmudig/AutoProfiler",
                        "external_paper_link": "",
                        "youtube_ff_link": "https://youtu.be/qtSt39Z3tIk",
                        "youtube_ff_id": "qtSt39Z3tIk",
                        "bunny_ff_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1517/v-full-1517_Preview.mp4?token=hSzyhUJqQEp8NFruUrYC93WeMFO2RPj9Z0CLQFIK1eQ&expires=1706590800",
                        "bunny_ff_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1517/v-full-1517_Preview.vtt?token=0QhI6BS3SmwlDt-tndOAR3oHJGtxuX-Hdvel9BNHRv4&expires=1706590800",
                        "youtube_prerecorded_link": "https://youtu.be/eMphMxjoIUA",
                        "youtube_prerecorded_id": "eMphMxjoIUA",
                        "bunny_prerecorded_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1517/v-full-1517_Presentation.mp4?token=l40uI74NrWpez1jfxRzhDyR03d5P4Q430CaVvcNOoow&expires=1706590800",
                        "bunny_prerecorded_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1517/v-full-1517_Presentation.vtt?token=C6JeXjipdMc6CmAiOOarsZsQhYaBfwhRkBkIKwdfsDk&expires=1706590800"
                    },
                    {
                        "slot_id": "v-full-1787",
                        "session_id": "full29",
                        "title": "EVM: Incorporating Model Checking into Exploratory Visual Analysis",
                        "contributors": [
                            "Alex Kale"
                        ],
                        "authors": [
                            "Alex Kale",
                            "Ziyang Guo",
                            "xiaoli Qiao",
                            "Jeffrey Heer",
                            "Jessica Hullman"
                        ],
                        "abstract": "Visual analytics (VA) tools support data exploration by helping analysts quickly and iteratively generate views of data which reveal interesting patterns. However, these tools seldom enable explicit checks of the resulting interpretations of data\u2014e.g., whether patterns can be accounted for by a model that implies a particular structure in the  relationships between variables. We present EVM, a data exploration tool that enables users to express and check provisional interpretations of data in the form of statistical models. EVM integrates support for visualization-based model checks by rendering distributions of model predictions alongside user-generated views of data. In a user study with data scientists practicing in the private and public sector, we evaluate how model checks influence analysts\u2019 thinking during data exploration. Our analysis characterizes how participants use model checks to scrutinize expectations about data generating process and surfaces further opportunities to scaffold model exploration in VA tools.",
                        "uid": "v-full-1787",
                        "time_stamp": "2023-10-24T22:36:00Z",
                        "time_start": "2023-10-24T22:36:00Z",
                        "time_end": "2023-10-24T22:48:00Z",
                        "paper_type": "full",
                        "keywords": [
                            "Visualization, model checks, exploratory analysis"
                        ],
                        "doi": "10.1109/TVCG.2023.3326516",
                        "fno": "",
                        "has_image": true,
                        "has_pdf": true,
                        "paper_award": "",
                        "image_caption": "EVM (Exploratory Visual Modeling) integrates the ability to express and visually check regression models into a drag-and-drop visual analytics interface. In a typical visual analytics workflow, (A and B) analysts use visualizations to discover possible patterns of interest. With EVM's visual model checks, (C) analysts can rule out and compare of provisional data interpretations by scrutinizing the compatibility of observed data with model predictions.",
                        "external_paper_link": "",
                        "youtube_ff_link": "https://youtu.be/fGtd0CzXm0w",
                        "youtube_ff_id": "fGtd0CzXm0w",
                        "bunny_ff_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1787/v-full-1787_Preview.mp4?token=2KPWMwW-gDxPC8LtnuARLg9S3eR30vblLLscOEaUqYk&expires=1706590800",
                        "bunny_ff_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1787/v-full-1787_Preview.vtt?token=CL5wzaB1-WzKWGtKerU6BP-Q-Otwd5AdiEIe3YaKSe8&expires=1706590800",
                        "youtube_prerecorded_link": "https://youtu.be/MYJ-1sKgxbI",
                        "youtube_prerecorded_id": "MYJ-1sKgxbI",
                        "bunny_prerecorded_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1787/v-full-1787_Presentation.mp4?token=t-xkdNGG3uj0kRh0eigcVoEsDUpJPSfTJVIcaUSwM4k&expires=1706590800",
                        "bunny_prerecorded_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1787/v-full-1787_Presentation.vtt?token=yHdLbRX8JKoxIPKmTWsmm8l4X6md9wSBNe1ieX2C-v4&expires=1706590800"
                    },
                    {
                        "slot_id": "v-full-1594",
                        "session_id": "full29",
                        "title": "VISPUR: Visual Aids for Identifying and Interpreting Spurious Associations in Data-Driven Decisions",
                        "contributors": [
                            "Xian Teng"
                        ],
                        "authors": [
                            "Xian Teng",
                            "Yongsu Ahn",
                            "Yu-Ru Lin"
                        ],
                        "abstract": "Big data and machine learning tools have jointly empowered humans in making data-driven decisions. However, many of them capture empirical associations that might be spurious due to confounding factors and subgroup heterogeneity. The famous Simpson\u2019s paradox is such a phenomenon where aggregated and subgroup-level associations contradict with each other, causing cognitive confusions and difficulty in making adequate interpretations and decisions. Existing tools provide little insights for humans to locate, reason about, and prevent pitfalls of spurious association in practice. We propose VISPUR, a visual analytic system that provides a causal analysis framework and a human-centric workflow for tackling spurious associations. These include a CONFOUNDER DASHBOARD, which can automatically identify possible confounding factors, and a SUBGROUP VIEWER, which allows for the visualization and comparison of diverse subgroup patterns that likely or potentially result in a misinterpretation of causality. Additionally, we propose a REASONING STORYBOARD, which uses a flow-based approach to illustrate paradoxical phenomena, as well as an interactive DECISION DIAGNOSIS panel that helps ensure accountable decision-making. Through an expert interview and a controlled user experiment, our qualitative and quantitative results demonstrate that the proposed \u201cde-paradox\u201d workflow and the designed visual analytic system are effective in helping human users to identify and understand spurious associations, as well as to make accountable causal decisions.",
                        "uid": "v-full-1594",
                        "time_stamp": "2023-10-24T22:48:00Z",
                        "time_start": "2023-10-24T22:48:00Z",
                        "time_end": "2023-10-24T23:00:00Z",
                        "paper_type": "full",
                        "keywords": [
                            "Causal Analysis, Simpson\u2019s Paradox, Spurious Associations, Machine Learning, Decision Making"
                        ],
                        "doi": "10.1109/TVCG.2023.3326587",
                        "fno": "",
                        "has_image": true,
                        "has_pdf": true,
                        "paper_award": "",
                        "image_caption": "This work provides a de-paradox workflow to help analyze observational data and overcome spurious and paradoxical associations. Spurious associations, including Simpson's paradox, are prevalent in observational studies. E.g., in a study that investigates the effect of a job training program, the cause (training program) and outcome (earnings) can be distorted by a third variable (ethnicity), leading to a misleading interpretation of the causal effect. We identify two major sources for spuriousness: (1) confounding bias and (2) subgroup heterogeneity, based on causal literature. We develop VISPUR, visualizing spurious associations, a visual analytic system to enable causal analysis of spurious associations. The system incorporates a suite of statistical techniques, algorithms, and visual components to help identify causal roots of spurious associations, as well as modules to reason about association paradox and to make informed decisions.",
                        "external_paper_link": "",
                        "youtube_ff_link": "https://youtu.be/AB8x_Tn7KXw",
                        "youtube_ff_id": "AB8x_Tn7KXw",
                        "bunny_ff_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1594/v-full-1594_Preview.mp4?token=3llvqu7L-mLpFp9YKVPtjL29ArmF2gTXvS9tZTDw7c8&expires=1706590800",
                        "bunny_ff_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1594/v-full-1594_Preview.vtt?token=k7zFVMZd46vDLp0oInH8x1PKBKdmlh2RD5wCDTXOnkE&expires=1706590800",
                        "youtube_prerecorded_link": "https://youtu.be/eEg-_6MA4Zw",
                        "youtube_prerecorded_id": "eEg-_6MA4Zw",
                        "bunny_prerecorded_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1594/v-full-1594_Presentation.mp4?token=WVbBCMZrVBzQd63n5yxVbZfVoFy5Cv7MJagSC5NApwU&expires=1706590800",
                        "bunny_prerecorded_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1594/v-full-1594_Presentation.vtt?token=cDjtQtT8sMfuIVQrWQjX3yVrMrTig8Z0WJNeJb2wSKk&expires=1706590800"
                    },
                    {
                        "slot_id": "v-full-1508",
                        "session_id": "full29",
                        "title": "Visualization According to Statisticians: An Interview Study on the Role of Visualization for Inferential Statistics",
                        "contributors": [
                            "Eric Newburger"
                        ],
                        "authors": [
                            "Eric Newburger",
                            "Niklas Elmqvist"
                        ],
                        "abstract": "Abstract\u2014Statisticians are not only one of the earliest professional adopters of data visualization, but also some of its most prolific users. Understanding how these professionals utilize visual representations in their analytic process may shed light on best practices for visual sensemaking. We present results from an interview study involving 18 professional statisticians (19.7 years average in the profession) on three aspects: (1) their use of visualization in their daily analytic work; (2) their mental models of inferential statistical processes; and (3) their design recommendations for how to best represent statistical inferences. Interview sessions consisted of discussing inferential statistics, eliciting participant sketches of suitable visual designs, and finally, a design intervention with our proposed visual designs. We analyzed interview transcripts using thematic analysis and open coding, deriving thematic codes on statistical mindset, analytic process, and analytic toolkit. The key findings for each aspect are as follows: (1) statisticians make extensive use of visualization during all phases of their work (and not just when reporting results); (2) their mental models of inferential methods tend to be mostly visually based; and (3) many statisticians abhor dichotomous thinking. The latter suggests that a multi-faceted visual display of inferential statistics that includes a visual indicator of analytically important effect sizes may help to balance the attributed epistemic power of traditional statistical testing with an awareness of the uncertainty of sensemaking.",
                        "uid": "v-full-1508",
                        "time_stamp": "2023-10-24T23:00:00Z",
                        "time_start": "2023-10-24T23:00:00Z",
                        "time_end": "2023-10-24T23:12:00Z",
                        "paper_type": "full",
                        "keywords": [
                            "Inferential statistics, qualitative interview study, thematic coding, statistical visualization"
                        ],
                        "doi": "10.1109/TVCG.2023.3326521",
                        "fno": "",
                        "has_image": true,
                        "has_pdf": true,
                        "paper_award": "",
                        "image_caption": "How do professional statisticians use data visualization?  How do they consider visualization within the suite of analytic methods at their disposal? Do they trust visualization methods?  We conducted semi-structured interviews with 18 statisticians from government, academia, and private industry, with more than 350 years collective professional experience, to find out.",
                        "external_paper_link": "",
                        "youtube_ff_link": "https://youtu.be/LstX25H2Uho",
                        "youtube_ff_id": "LstX25H2Uho",
                        "bunny_ff_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1508/v-full-1508_Preview.mp4?token=XmYCrKCl6_JsUzddQL3AyeLBCL_b-6Y7WhFo3Xfqqvs&expires=1706590800",
                        "bunny_ff_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1508/v-full-1508_Preview.vtt?token=7lukTuuknhEx2KtOd1COsxpNQBoypXxf7ROyWLz6dyY&expires=1706590800",
                        "youtube_prerecorded_link": "https://youtu.be/D4z_F2b5aEQ",
                        "youtube_prerecorded_id": "D4z_F2b5aEQ",
                        "bunny_prerecorded_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1508/v-full-1508_Presentation.mp4?token=M_cnK4NaHX5p4z9c8kyHY0SDB2wP-23CvCPNe8ix_5o&expires=1706590800",
                        "bunny_prerecorded_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1508/v-full-1508_Presentation.vtt?token=B1i1Rr9I3NIoH-Qy086gbNfex8X93iyfkTZgv48-9JU&expires=1706590800"
                    }
                ]
            },
            {
                "title": "VIS for ML",
                "session_id": "full30",
                "event_prefix": "v-full",
                "track": "oneohnine",
                "session_image": "full30.png",
                "chair": [
                    "Shixia Liu"
                ],
                "time_start": "2023-10-25T04:45:00Z",
                "time_end": "2023-10-25T06:00:00Z",
                "discord_category": "",
                "discord_channel": "109",
                "discord_channel_id": "1161741434647482379",
                "discord_link": "https://discordapp.com/channels/1161400451619627068/1161741434647482379",
                "zoom_private_meeting": "",
                "zoom_private_password": "",
                "zoom_private_link": "",
                "zoom_broadcast_link": "",
                "ff_link": "https://youtu.be/UbwJtCmd5-g",
                "time_slots": [
                    {
                        "slot_id": "v-full-1227",
                        "session_id": "full30",
                        "title": "A Comparative Visual Analytics Framework for Evaluating Evolutionary Processes in Multi-objective Optimization",
                        "contributors": [
                            "Zherui Zhang"
                        ],
                        "authors": [
                            "Yansong Huang",
                            "Zherui Zhang",
                            "Ao Jiao",
                            "Yuxin Ma",
                            "Ran Cheng"
                        ],
                        "abstract": "Evolutionary multi-objective optimization (EMO) algorithms have been demonstrated to be effective in solving multi-criteria decision-making problems. In real-world applications, analysts often employ several algorithms concurrently and compare their solution sets to gain insight into the characteristics of different algorithms and explore a broader range of feasible solutions. However, EMO algorithms are typically treated as black boxes, leading to difficulties in performing detailed analysis and comparisons between the internal evolutionary processes. Inspired by the successful application of visual analytics tools in explainable AI, we argue that interactive visualization can significantly enhance the comparative analysis between multiple EMO algorithms. In this paper, we present a visual analytics framework that enables the exploration and comparison of evolutionary processes in EMO algorithms. Guided by a literature review and expert interviews, the proposed framework addresses various analytical tasks and establishes a multi-faceted visualization design to support the comparative analysis of intermediate generations in the evolution as well as solution sets. We demonstrate the effectiveness of our framework through case studies on benchmarking and real-world multi-objective optimization problems to elucidate how analysts can leverage our framework to inspect and compare diverse algorithms.",
                        "uid": "v-full-1227",
                        "time_stamp": "2023-10-25T04:45:00Z",
                        "time_start": "2023-10-25T04:45:00Z",
                        "time_end": "2023-10-25T04:57:00Z",
                        "paper_type": "full",
                        "keywords": [
                            "Visual analytics, evolutionary multi-objective optimization"
                        ],
                        "doi": "10.1109/TVCG.2023.3326921",
                        "fno": "",
                        "has_image": true,
                        "has_pdf": true,
                        "paper_award": "",
                        "image_caption": "We propose a visual analytics framework that enables the exploration and comparison of evolutionary processes in EMO algorithms. Our visual analytics framework comprises three primary modules, namely the Algorithm-level Comparison module (V1), Evolution-level Exploration module (V2-4), and Solution-level Inspection module (V5).",
                        "external_paper_link": "",
                        "youtube_ff_link": "https://youtu.be/CuJOVQaVWrA",
                        "youtube_ff_id": "CuJOVQaVWrA",
                        "bunny_ff_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1227/v-full-1227_Preview.mp4?token=vSE-u8yLLb_WJQSumkRCm_ZKXLv_tLILSmUDcKYbNzk&expires=1706590800",
                        "bunny_ff_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1227/v-full-1227_Preview.vtt?token=CgQvjoKBVYghsKAuc9tk0t-5GYtAoO6Dsylh3PmNOaQ&expires=1706590800",
                        "youtube_prerecorded_link": "https://youtu.be/D5EyyPiax-A",
                        "youtube_prerecorded_id": "D5EyyPiax-A",
                        "bunny_prerecorded_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1227/v-full-1227_Presentation.mp4?token=Gjj0Zu7HDLU5nq8UG3ojLKMdiYKXmNsKh7Hf0gpENx4&expires=1706590800",
                        "bunny_prerecorded_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1227/v-full-1227_Presentation.vtt?token=UDapXMQHvUB48QAEF6P5CHc96t_vFsXMGSWLWQjZ3j8&expires=1706590800"
                    },
                    {
                        "slot_id": "v-tvcg-9937145",
                        "session_id": "full30",
                        "title": "The Transform-and-Perform framework: Explainable deep learning beyond classification",
                        "contributors": [
                            "Vidya Prasad"
                        ],
                        "authors": [
                            "Vidya Prasad",
                            "Ruud J. G. van Sloun",
                            "Stef van den Elzen",
                            "Anna Vilanova",
                            "Nicola Pezzotti"
                        ],
                        "abstract": "In recent years, visual analytics (VA) has shown promise in alleviating the challenges of interpreting black-box deep learning (DL) models. While the focus of VA for explainable DL has been mainly on classification problems, DL is gaining popularity in high-dimensional-to-high-dimensional (H-H) problems such as image-to-image translation. In contrast to classification, H-H problems have no explicit instance groups or classes to study. Each output is continuous, high-dimensional, and changes in an unknown non-linear manner with changes in the input. These unknown relations between the input, model and output necessitate the user to analyze them in conjunction, leveraging symmetries between them. Since classification tasks do not exhibit some of these challenges, most existing VA systems and frameworks allow limited control of the components required to analyze models beyond classification. Hence, we identify the need for and present a unified conceptual framework, the Transform-and-Perform framework (T&P), to facilitate the design of VA systems for DL model analysis focusing on H-H problems. T&P provides a checklist to structure and identify workflows and analysis strategies to design new VA systems, and understand existing ones to uncover potential gaps for improvements. The goal is to aid the creation of effective VA systems that support the structuring of model understanding and identifying actionable insights for model improvements. We highlight the growing need for new frameworks like T&P with a real-world image-to image translation application. We illustrate how T&P effectively supports the understanding and identification of potential gaps in existing VA systems.",
                        "uid": "v-tvcg-9937145",
                        "time_stamp": "2023-10-25T04:57:00Z",
                        "time_start": "2023-10-25T04:57:00Z",
                        "time_end": "2023-10-25T05:09:00Z",
                        "paper_type": "full",
                        "keywords": [
                            "Visual Analytics;Explainable AI;XAI;Framework;Deep Learning;High-dimensional-to-high-dimensional translation"
                        ],
                        "doi": "10.1109/TVCG.2022.3219248",
                        "fno": "9937145",
                        "has_image": true,
                        "has_pdf": false,
                        "paper_award": "",
                        "image_caption": "We introduce the Transform-and-Perform (T&P) framework, designed to assist visual analytics (VA) designers in creating VA systems with general applicability to high-dimensional-to-high-dimensional (H-H) problems. T&P helps identify workflows and analysis strategies for designing new VA systems. It also helps reveal potential gaps in existing systems. T&P enables analysis across the \"3Ws\" of model behavior: 1) when a behavior occurs (input analysis), 2) how & why it occurs (model analysis), and 3) what the behavior is (output analysis). By utilizing input-output symmetries, T&P offers a formal approach to understanding a model's inductive biases, crucial for analyzing a range of DL models.",
                        "external_paper_link": "",
                        "youtube_ff_link": "https://youtu.be/NT6ajUlK18c",
                        "youtube_ff_id": "NT6ajUlK18c",
                        "bunny_ff_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-tvcg/v-tvcg-9937145/v-tvcg-9937145_Preview.mp4?token=iOQ4UxkAx4HfqLNy8EF6EntWh0PZjmbXWmnK7s9Pn9I&expires=1706590800",
                        "bunny_ff_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-tvcg/v-tvcg-9937145/v-tvcg-9937145_Preview.vtt?token=TDyYqXPbD-ul5i3CuE1Qjo3uOxYEcDL3JWWQ0au2-EQ&expires=1706590800",
                        "youtube_prerecorded_link": "https://youtu.be/EsEtAxp0QVk",
                        "youtube_prerecorded_id": "EsEtAxp0QVk",
                        "bunny_prerecorded_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-tvcg/v-tvcg-9937145/v-tvcg-9937145_Presentation.mp4?token=4mvSpkiNrcqebwYZe5ZTGXofTh9c-gKa5HA5WLLABmE&expires=1706590800",
                        "bunny_prerecorded_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-tvcg/v-tvcg-9937145/v-tvcg-9937145_Presentation.vtt?token=dx-xgDHRbIVqJJIqL3z9qtTfbJwuTm1oKbKLGrTFyFY&expires=1706590800"
                    },
                    {
                        "slot_id": "v-tvcg-9937064",
                        "session_id": "full30",
                        "title": "Visual Exploration of Machine Learning Model Behavior with Hierarchical Surrogate Rule Sets",
                        "contributors": [
                            "Jun Yuan"
                        ],
                        "authors": [
                            "Jun Yuan",
                            "Brian Barr",
                            "Kyle Overton",
                            "Enrico Bertini"
                        ],
                        "abstract": "One of the potential solutions for model interpretation is to train a surrogate model: a more transparent model that approximates the behavior of the model to be explained. Typically, classification rules or decision trees are used due to their logic-based expressions. However, decision trees can grow too deep, and rule sets can become too large to approximate a complex model. Unlike paths on a decision tree that must share ancestor nodes (conditions), rules are more flexible. However, the unstructured visual representation of rules makes it hard to make inferences across rules. In this paper, we focus on tabular data and present novel algorithmic and interactive solutions to address these issues. First, we present Hierarchical Surrogate Rules (HSR), an algorithm that generates hierarchical rules based on user-defined parameters. We also contribute SURE, a visual analytics (VA) system that integrates HSR and an interactive surrogate rule visualization, the Feature-Aligned Tree, which depicts rules as trees while aligning features for easier comparison. We evaluate the algorithm in terms of parameter sensitivity, time performance, and comparison with surrogate decision trees and find that it scales reasonably well and overcomes the shortcomings of surrogate decision trees. We evaluate the visualization and the system through a usability study and an observational study with domain experts. Our investigation shows that the participants can use feature-aligned trees to perform non-trivial tasks with very high accuracy. We also discuss many interesting findings, including a rule analysis task characterization, that can be used for visualization design and future research.",
                        "uid": "v-tvcg-9937064",
                        "time_stamp": "2023-10-25T05:09:00Z",
                        "time_start": "2023-10-25T05:09:00Z",
                        "time_end": "2023-10-25T05:21:00Z",
                        "paper_type": "full",
                        "keywords": [
                            "visualization;rule set;surrogate model;model understanding"
                        ],
                        "doi": "10.1109/TVCG.2022.3219232",
                        "fno": "9937064",
                        "has_image": true,
                        "has_pdf": false,
                        "paper_award": "",
                        "image_caption": "SuRE is a visual analytics (VA) system that integrates hierarchical surrogate rule generation and an interactive surrogate rule visualization, the Feature-Aligned Tree, which depicts rules as trees while aligning features for easier comparison.",
                        "external_paper_link": "",
                        "youtube_ff_link": "https://youtu.be/Ao8r2hjVils",
                        "youtube_ff_id": "Ao8r2hjVils",
                        "bunny_ff_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-tvcg/v-tvcg-9937064/v-tvcg-9937064_Preview.mp4?token=PpcGyHaOiaB2LtmJ0r8powxm89Z8mTICgLbqHe10uH8&expires=1706590800",
                        "bunny_ff_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-tvcg/v-tvcg-9937064/v-tvcg-9937064_Preview.vtt?token=3cg58dUQA4M3JuRJHnRU2fJMQFmHxLKBwbtYG-z7Izs&expires=1706590800",
                        "youtube_prerecorded_link": "https://youtu.be/BMVvNfxg7O8",
                        "youtube_prerecorded_id": "BMVvNfxg7O8",
                        "bunny_prerecorded_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-tvcg/v-tvcg-9937064/v-tvcg-9937064_Presentation.mp4?token=q23QzRueTvsR0p4ljyCVFYSDZl0JGr6J8uFrcy6mhRs&expires=1706590800",
                        "bunny_prerecorded_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-tvcg/v-tvcg-9937064/v-tvcg-9937064_Presentation.vtt?token=HJhh47W3UD_Dvx8CW8Fv8fewn6X9nfovgbszBeMqebk&expires=1706590800"
                    },
                    {
                        "slot_id": "v-full-1753",
                        "session_id": "full30",
                        "title": "Are We Closing the Loop Yet? Gaps in the Generalizability of VIS4ML Research",
                        "contributors": [
                            "Hariharan Subramonyam"
                        ],
                        "authors": [
                            "Hariharan Subramonyam",
                            "Jessica Hullman"
                        ],
                        "abstract": "Visualization for machine learning (VIS4ML) research aims to help experts apply their prior knowledge to develop, understand, and improve the performance of machine learning models. In conceiving VIS4ML systems, researchers characterize the nature of human knowledge to support human-in-the-loop tasks, design interactive visualizations to make ML components interpretable and elicit knowledge, and evaluate the effectiveness of human-model interchange. We survey recent VIS4ML papers to assess the generalizability of research contributions and claims in enabling human-in-the-loop ML. Our results show potential gaps between the current scope of VIS4ML research and aspirations for its use in practice. We find that while papers motivate that VIS4ML systems are applicable beyond the specific conditions studied, conclusions are often overfitted to non-representative scenarios, are based on interactions with a small set of ML experts and well-understood datasets, fail to acknowledge crucial dependencies, and hinge on decisions that lack justification. We discuss approaches to close the gap between aspirations and research claims and suggest documentation practices to report generality constraints that better acknowledge the exploratory nature of VIS4ML research.",
                        "uid": "v-full-1753",
                        "time_stamp": "2023-10-25T05:21:00Z",
                        "time_start": "2023-10-25T05:21:00Z",
                        "time_end": "2023-10-25T05:33:00Z",
                        "paper_type": "full",
                        "keywords": [
                            "VIS4ML, Visualization, Machine learning, Human-in-the-loop, Human Knowledge, Generalizability, Survey."
                        ],
                        "doi": "10.1109/TVCG.2023.3326591",
                        "fno": "",
                        "has_image": true,
                        "has_pdf": true,
                        "paper_award": "",
                        "image_caption": "Two concentric circles divided into four segments with the four main finding groups including human expertise, hitl tasks, ml components, and vis4ml tools. The segments are distintly colored, and the outer circle segments are slightly desaturated. The outer circle represents the scope of VIS4ML in real world applications, and the inner circle the scope of VIS4ML research.",
                        "external_paper_link": "",
                        "youtube_ff_link": "https://youtu.be/oyxGvCFuobc",
                        "youtube_ff_id": "oyxGvCFuobc",
                        "bunny_ff_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1753/v-full-1753_Preview.mp4?token=kIQLr3JHao5Oc2gfh9xBawDw6JECYzIYxiga_Qts5qU&expires=1706590800",
                        "bunny_ff_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1753/v-full-1753_Preview.vtt?token=R2Pm0JR6j-YTwSk0P6NUldFpZIIU37B1a6hhrSTVHF8&expires=1706590800",
                        "youtube_prerecorded_link": "https://youtu.be/fn5zAUom2Mk",
                        "youtube_prerecorded_id": "fn5zAUom2Mk",
                        "bunny_prerecorded_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1753/v-full-1753_Presentation.mp4?token=zubQVQV0vmqMvDUtZWLPiys113jlZmE9Ir_OsJablj4&expires=1706590800",
                        "bunny_prerecorded_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1753/v-full-1753_Presentation.vtt?token=X4RY2ssGNfvO8EXi_YsO1E_cUXXlKXu3V8rdv_qPPfU&expires=1706590800"
                    },
                    {
                        "slot_id": "v-full-1694",
                        "session_id": "full30",
                        "title": "Explore Your Network in Minutes: A Rapid Prototyping Toolkit for Understanding Neural Networks with Visual Analytics",
                        "contributors": [
                            "Jun Tao"
                        ],
                        "authors": [
                            "Shaoxuan Lai",
                            "Wanna Luan",
                            "Jun Tao"
                        ],
                        "abstract": "Neural networks attract significant attention in almost every field due to their widespread applications in various tasks. However, developers often struggle with debugging due to the black-box nature of neural networks. Visual analytics provides an intuitive way for developers to understand the hidden states and underlying complex transformations in neural networks. Existing visual analytics tools for neural networks have been demonstrated to be effective in providing useful hints for debugging certain network architectures. However, these approaches are often architecture-specific with strong assumptions of how the network should be understood. This limits their use when the network architecture or the exploration goal changes. In this paper, we present a general model and a programming toolkit, Neural Network Visualization Builder (NNVisBuilder), for prototyping visual analytics systems to understand neural networks. NNVisBuilder covers the common data transformation and interaction model involved in existing tools for exploring neural networks. It enables developers to customize a visual analytics interface for answering their specific questions about networks. NNVisBuilder is compatible with PyTorch so that developers can integrate the visualization code into their learning code seamlessly. We demonstrate the applicability by reproducing several existing visual analytics systems for networks with NNVisBuilder. The source code and some example cases can be found at https://github.com/sysuvis/NVB.",
                        "uid": "v-full-1694",
                        "time_stamp": "2023-10-25T05:33:00Z",
                        "time_start": "2023-10-25T05:33:00Z",
                        "time_end": "2023-10-25T05:45:00Z",
                        "paper_type": "full",
                        "keywords": [
                            "Visualization model, toolkit, neural networks, visual diagnosis"
                        ],
                        "doi": "10.1109/TVCG.2023.3326575",
                        "fno": "",
                        "has_image": true,
                        "has_pdf": true,
                        "paper_award": "",
                        "image_caption": "Under the guidance of the NNVisBuilder conceptual model, you can easily use the NNVisBuilder programming toolkit to build visual analytics interfaces for neural networks.",
                        "external_paper_link": "",
                        "youtube_ff_link": "https://youtu.be/tZFpiomSRTE",
                        "youtube_ff_id": "tZFpiomSRTE",
                        "bunny_ff_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1694/v-full-1694_Preview.mp4?token=6IZigkpgKL9XtaJU2wQKOrnRzZRXA5hg9oI3KNz-7wI&expires=1706590800",
                        "bunny_ff_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1694/v-full-1694_Preview.vtt?token=UiO8-ei_cT8KEWOXl-BKr_c9prGIJBa5kCH3vAybIhU&expires=1706590800",
                        "youtube_prerecorded_link": "https://youtu.be/WcAWOeqiRfo",
                        "youtube_prerecorded_id": "WcAWOeqiRfo",
                        "bunny_prerecorded_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1694/v-full-1694_Presentation.mp4?token=orD697qeBx4e81J26IFS5GQdMKxHORh-bhYTrTiu2co&expires=1706590800",
                        "bunny_prerecorded_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1694/v-full-1694_Presentation.vtt?token=1FGsHASEzxNkDdcVyWFArHVnK_MzEAVIjMEXB6cooUg&expires=1706590800"
                    },
                    {
                        "slot_id": "v-full-1550",
                        "session_id": "full30",
                        "title": "OW-Adapter: Human-Assisted Open-World Object Detection with a Few Examples",
                        "contributors": [
                            "Suphanut Jamonnak"
                        ],
                        "authors": [
                            "Suphanut Jamonnak",
                            "Jiajing Guo",
                            "Wenbin He",
                            "Liang Gou",
                            "Liu Ren"
                        ],
                        "abstract": "Open-world object detection (OWOD) is an emerging computer vision problem that involves not only the identification of predefined object classes, like what general object detectors do, but also detects new unknown objects simultaneously. Recently, several end-to-end deep learning models have been proposed to address the OWOD problem. However, these approaches face several challenges: a) significant changes in both network architecture and training procedure are required; b) they are trained from scratch, which can not leverage existing pre-trained general detectors; c) costly annotations for all unknown classes are needed. To overcome these challenges, we present a visual analytic framework called OW-Adapter. It acts as an adaptor to enable pre-trained general object detectors to handle the OWOD problem. Specifically, OW-Adapter is designed to identify, summarize, and annotate unknown examples with minimal human effort. Moreover, we introduce a lightweight classifier to learn newly annotated unknown classes and plug the classifier into pre-trained general detectors to detect unknown objects. We demonstrate the effectiveness of our framework through two case studies of different domains, including common object recognition and autonomous driving. The studies show that a simple yet powerful adaptor can extend the capability of pre-trained general detectors to detect unknown objects and improve the performance on known classes simultaneously.",
                        "uid": "v-full-1550",
                        "time_stamp": "2023-10-25T05:45:00Z",
                        "time_start": "2023-10-25T05:45:00Z",
                        "time_end": "2023-10-25T05:57:00Z",
                        "paper_type": "full",
                        "keywords": [
                            "Open world learning, object detection, continuous learning, human-assisted AI."
                        ],
                        "doi": "10.1109/TVCG.2023.3326577",
                        "fno": "",
                        "has_image": true,
                        "has_pdf": true,
                        "paper_award": "",
                        "image_caption": "Open-world object detection (OWOD) is an emerging computer vision problem that involves not only the identification of predefined object classes, like what general object detectors do, but also detects new unknown objects simultaneously. Recently, several end-to-end deep learning models have been proposed to address the OWOD problem. However, these approaches face several challenges: a) significant changes in both network architecture and training procedure are required; b) they are trained from scratch, which can not leverage existing pre-trained general detectors; c) costly annotations for all unknown classes are needed. To overcome these challenges, we present a visual analytic framework called OW-Adapter. It acts as an adaptor to enable pre-trained general object detectors to handle the OWOD problem. Specifically, OW-Adapter is designed to identify, summarize, and annotate unknown examples with minimal human effort. Moreover, we introduce a lightweight classifier to learn newly annotated unknown classes and plug the classifier into pre-trained general detectors to detect unknown objects. We demonstrate the effectiveness of our framework through two case studies of different domains, including common object recognition and autonomous driving. The studies show that a simple yet powerful adaptor can extend the capability of pre-trained general detectors to detect unknown objects and improve the performance on known classes simultaneously.",
                        "external_paper_link": "",
                        "youtube_ff_link": "https://youtu.be/QNub6PYMp1k",
                        "youtube_ff_id": "QNub6PYMp1k",
                        "bunny_ff_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1550/v-full-1550_Preview.mp4?token=87GDUWCtggaw5M8C9aJywRHheRtTy7IkhW2e1in3Tns&expires=1706590800",
                        "bunny_ff_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1550/v-full-1550_Preview.vtt?token=j3qjCbxO2UMB7hPzGTMS7B3L0AhckkbrexBweLEKYTA&expires=1706590800",
                        "youtube_prerecorded_link": "",
                        "youtube_prerecorded_id": "",
                        "bunny_prerecorded_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1550/v-full-1550_Presentation.mp4?token=jCcP7kBc7QBgP9m6kEh7SJht0AJfFYI4e668zjTPyt0&expires=1706590800",
                        "bunny_prerecorded_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1550/v-full-1550_Presentation.vtt?token=sAtRo3yQXHbz8NtXepJLmI0tvUYAIkM1E57x4bKrwN0&expires=1706590800"
                    }
                ]
            },
            {
                "title": "Visualization Design and User Experience",
                "session_id": "full31",
                "event_prefix": "v-full",
                "track": "oneohfive",
                "session_image": "full31.png",
                "chair": [
                    "Melanie Tory"
                ],
                "time_start": "2023-10-25T23:45:00Z",
                "time_end": "2023-10-26T01:00:00Z",
                "discord_category": "",
                "discord_channel": "105",
                "discord_channel_id": "1161741309346861067",
                "discord_link": "https://discordapp.com/channels/1161400451619627068/1161741309346861067",
                "zoom_private_meeting": "",
                "zoom_private_password": "",
                "zoom_private_link": "",
                "zoom_broadcast_link": "",
                "ff_link": "https://youtu.be/ci8sJ9mvuy0",
                "time_slots": [
                    {
                        "slot_id": "v-tvcg-9855227",
                        "session_id": "full31",
                        "title": "VisRecall: Quantifying Information Visualisation Recallability via Question Answering",
                        "contributors": [
                            "Yao Wang"
                        ],
                        "authors": [
                            "Yao Wang",
                            "Chuhan Jiao",
                            "Mihai B\u00e2ce",
                            "Andreas Bulling"
                        ],
                        "abstract": "Despite its importance for assessing the effectiveness of communicating information visually, fine-grained recallability of information visualisations has not been studied quantitatively so far. In this work, we propose a question-answering paradigm to study visualisation recallability and present VisRecall - a novel dataset consisting of 200 visualisations that are annotated with crowd-sourced human (N = 305) recallability scores obtained from 1,000 questions of five question types. Furthermore, we present the first computational method to predict recallability of different visualisation elements, such as the title or specific data values. We report detailed analyses of our method on VisRecall and demonstrate that it outperforms several baselines in overall recallability and FE-, F-, RV-, and U-question recallability. Our work makes fundamental contributions towards a new generation of methods to assist designers in optimising visualisations.",
                        "uid": "v-tvcg-9855227",
                        "time_stamp": "2023-10-25T23:45:00Z",
                        "time_start": "2023-10-25T23:45:00Z",
                        "time_end": "2023-10-25T23:57:00Z",
                        "paper_type": "full",
                        "keywords": [
                            "Information visualisation;machine learning;memorability;recallability"
                        ],
                        "doi": "10.1109/TVCG.2022.3198163",
                        "fno": "9855227",
                        "has_image": true,
                        "has_pdf": false,
                        "paper_award": "",
                        "image_caption": "\u200bThis work makes three contributions: (1) We adapt a question-answering paradigm to study fine-grained recallability of information visualisations. (2) We present VisRecall \u2013\u2013 a novel dataset consisting of 200 visualisations that are annotated with crowd-sourced human recallability scores obtained from 1,000 questions of five types. (3) We present the first computational method to predict recallability of visualisations.",
                        "external_paper_link": "",
                        "youtube_ff_link": "https://youtu.be/yby3iEztvi0",
                        "youtube_ff_id": "yby3iEztvi0",
                        "bunny_ff_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-tvcg/v-tvcg-9855227/v-tvcg-9855227_Preview.mp4?token=X--qk0fZD-5uCweRJxl3ykskQ1OzWISpzon-nyWy49s&expires=1706590800",
                        "bunny_ff_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-tvcg/v-tvcg-9855227/v-tvcg-9855227_Preview.vtt?token=BxaD5DSgbYY21JobCraK9jV2RhNpRBNA9WBFfDcJs70&expires=1706590800",
                        "youtube_prerecorded_link": "https://youtu.be/xFaM8Y8CAnc",
                        "youtube_prerecorded_id": "xFaM8Y8CAnc",
                        "bunny_prerecorded_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-tvcg/v-tvcg-9855227/v-tvcg-9855227_Presentation.mp4?token=qzTcjKvBDC-BweOjwbamhyfGMCJoZwc0cSUwcnc2vDE&expires=1706590800",
                        "bunny_prerecorded_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-tvcg/v-tvcg-9855227/v-tvcg-9855227_Presentation.vtt?token=LEw6_XEq8K7atNROH4LhAUhcE5dpwGAOHe-JyQIYqA4&expires=1706590800"
                    },
                    {
                        "slot_id": "v-full-1021",
                        "session_id": "full31",
                        "title": "A Computational Design Process for Sensing Network Physicalizations",
                        "contributors": [
                            "S. Sandra Bae"
                        ],
                        "authors": [
                            "S. Sandra Bae",
                            "Takanori Fujiwara",
                            "Anders Ynnerman",
                            "Ellen Yi-Luen Do",
                            "Michael L Rivera",
                            "Danielle Albers Szafir"
                        ],
                        "abstract": "Interaction is critical for data analysis and sensemaking. However, designing interactive physicalizations is challenging as it requires cross-disciplinary knowledge in visualization, fabrication, and electronics. Interactive physicalizations are typically produced in an unstructured manner, resulting in unique solutions for a specific dataset, problem, or interaction that cannot be easily extended or adapted to new scenarios or future physicalizations. To mitigate these challenges, we introduce a computational design pipeline to 3D print network physicalizations with integrated sensing capabilities. Networks are ubiquitous, yet their complex geometry also requires significant engineering considerations to provide intuitive, effective interactions for exploration. Using our pipeline, designers can readily produce network physicalizations supporting selection\u2014the most critical atomic operation for interaction\u2014by touch through capacitive sensing and computational inference. Our computational design pipeline introduces a new design paradigm by concurrently considering the form and interactivity of a physicalization into one cohesive fabrication workflow. We evaluate our approach using (i) computational evaluations, (ii) three usage scenarios focusing on general visualization tasks, and (iii) expert interviews. The design paradigm introduced by our pipeline can lower barriers to physicalization research, creation, and adoption.",
                        "uid": "v-full-1021",
                        "time_stamp": "2023-10-25T23:57:00Z",
                        "time_start": "2023-10-25T23:57:00Z",
                        "time_end": "2023-10-26T00:09:00Z",
                        "paper_type": "full",
                        "keywords": [
                            "Physicalization, tangible interfaces, 3D printing, computational fabrication, design automation, network data"
                        ],
                        "doi": "10.1109/TVCG.2023.3327198",
                        "fno": "",
                        "has_image": true,
                        "has_pdf": true,
                        "paper_award": "honorable",
                        "image_caption": "A sensing network physicalization (N = 20, L = 40). (a) A multi-material 3D printed network physicalization produced by Bae et al\u2019s computational design pipeline. Conductive traces are embedded in the network\u2019s links which enables node selection via capacitive sensing. (b) A computational rendering of the network physicalization showcasing how the conductive traces are distributed throughout the network\u2019s links. The conductive traces use a serpentine pattern.",
                        "external_paper_link": "",
                        "youtube_ff_link": "https://youtu.be/9fexUfzRIjs",
                        "youtube_ff_id": "9fexUfzRIjs",
                        "bunny_ff_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1021/v-full-1021_Preview.mp4?token=zTlxbcoj1utujHq_WCWQEP58OWgOmJbshDz5QNp7-4E&expires=1706590800",
                        "bunny_ff_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1021/v-full-1021_Preview.vtt?token=Yd2n-Ioy_5xpCbdmz4lud7dd-q4e8YF3EMY9KVjwwYs&expires=1706590800",
                        "youtube_prerecorded_link": "https://youtu.be/xhGKUf2PCYw",
                        "youtube_prerecorded_id": "xhGKUf2PCYw",
                        "bunny_prerecorded_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1021/v-full-1021_Presentation.mp4?token=RswiEoV0Xn8jCMVsDt-c7f3XxpYUHc8gcWJYC5hretI&expires=1706590800",
                        "bunny_prerecorded_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1021/v-full-1021_Presentation.vtt?token=pM9sewYaGuwwFE9Trdny3w-3bYw72Mlj7wre_-fkV0w&expires=1706590800"
                    },
                    {
                        "slot_id": "v-full-1573",
                        "session_id": "full31",
                        "title": "Designing for Ambiguity in Visual Analytics: Lessons from Risk Assessment and Prediction",
                        "contributors": [
                            "Stan Nowak"
                        ],
                        "authors": [
                            "Stan Nowak",
                            "Lyn Bartram"
                        ],
                        "abstract": "Ambiguity is pervasive in the complex sensemaking domains of risk assessment and prediction but there remains little research on how to design visual analytics tools to accommodate it. We report on findings from a qualitative study based on a conceptual framework of sensemaking processes to investigate how both new visual analytics designs and existing tools, primarily data tables, support the cognitive work demanded in avalanche forecasting. While both systems yielded similar analytic outcomes we observed differences in ambiguous sensemaking and the analytic actions either afforded. Our findings challenge conventional visualization design guidance in both perceptual and interaction design, highlighting the need for data interfaces that encourage reflection, provoke alternative interpretations, and support the inherently ambiguous nature of sensemaking in this critical application. We review how different visual and interactive forms support or impede analytic processes and introduce \"gisting\" as a significant yet unexplored analytic action for visual analytics research. We conclude with design implications for enabling ambiguity in visual analytics tools to scaffold sensemaking in risk assessment.",
                        "uid": "v-full-1573",
                        "time_stamp": "2023-10-26T00:09:00Z",
                        "time_start": "2023-10-26T00:09:00Z",
                        "time_end": "2023-10-26T00:21:00Z",
                        "paper_type": "full",
                        "keywords": [
                            "Complex Systems, Risk Assessment, Sensemaking, Visualization Design"
                        ],
                        "doi": "10.1109/TVCG.2023.3326571",
                        "fno": "",
                        "has_image": true,
                        "has_pdf": true,
                        "paper_award": "",
                        "image_caption": "An interactive visualization prototype showing reports of observed avalanches developed for snow avalanche forecasters. The system is designed to address issues of ambiguity that forecasters face when making sense of avalanche observations and assessing avalanche hazards.",
                        "external_paper_link": "",
                        "youtube_ff_link": "https://youtu.be/btRIn_f3kKE",
                        "youtube_ff_id": "btRIn_f3kKE",
                        "bunny_ff_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1573/v-full-1573_Preview.mp4?token=29FrLyrV76vdmG33cycCi7pTZK2WpdpOfkInigks69M&expires=1706590800",
                        "bunny_ff_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1573/v-full-1573_Preview.vtt?token=Thcgg6IPQPjedmbuw9jXSKTXj650Gy7Tb3bf-jqpJuo&expires=1706590800",
                        "youtube_prerecorded_link": "https://youtu.be/SBB4WR305xk",
                        "youtube_prerecorded_id": "SBB4WR305xk",
                        "bunny_prerecorded_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1573/v-full-1573_Presentation.mp4?token=5MDhGmKqTfw8GszrBwSVmghCE27nm57VaOyVJwtUBSo&expires=1706590800",
                        "bunny_prerecorded_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1573/v-full-1573_Presentation.vtt?token=dKVwPGB4StFVW3g-WQR_gIKQ-QTst20JXnM-UnrVK_I&expires=1706590800"
                    },
                    {
                        "slot_id": "v-full-1020",
                        "session_id": "full31",
                        "title": "Dupo: A Mixed-Initiative Authoring Tool for Responsive Visualization",
                        "contributors": [
                            "Hyeok Kim"
                        ],
                        "authors": [
                            "Hyeok Kim",
                            "Ryan Rossi",
                            "Jessica Hullman",
                            "Jane Hoffswell"
                        ],
                        "abstract": "Designing responsive visualizations for various screen types can be tedious as authors must manage multiple chart~versions across design iterations. Automated approaches for responsive visualization must take into account the user's need for agency in exploring possible design ideas and applying customizations based on their own goals. We design and implement Dupo, a mixed-initiative approach to creating responsive visualizations that combines the agency afforded by a manual interface with automation provided by a recommender system. Given an initial design, users can browse automated design suggestions for a different screen type and make edits to a chosen design, thereby supporting quick prototyping and customizability. Dupo employs a two-step recommender pipeline that first suggests significant design changes (Exploration) followed by more subtle changes (Alteration). We evaluated Dupo with six expert responsive visualization authors. While creating responsive versions of a source design in Dupo, participants could reason about different design suggestions without having to manually prototype them, and thus avoid prematurely fixating on a particular design. This process led participants to create designs that they were satisfied with but which they had previously overlooked.",
                        "uid": "v-full-1020",
                        "time_stamp": "2023-10-26T00:21:00Z",
                        "time_start": "2023-10-26T00:21:00Z",
                        "time_end": "2023-10-26T00:33:00Z",
                        "paper_type": "full",
                        "keywords": [
                            "Visualization, responsive visualization, mixed-initiative authoring"
                        ],
                        "doi": "10.1109/TVCG.2023.3326583",
                        "fno": "",
                        "has_image": true,
                        "has_pdf": true,
                        "paper_award": "",
                        "image_caption": "Dupo is a mixed-initiative authoring tool for responsive visualization. Using Dupo, a visualization author can make manual edits, such as directly repositioning annotations in the chart. They can explore design alternatives that are automatically generated for different screen types. Design suggestions for mobile screens include simple rescaling, axes transpose, encoding changes, and numbering annotations, for example.",
                        "external_paper_link": "",
                        "youtube_ff_link": "https://youtu.be/kQr6UJQF40g",
                        "youtube_ff_id": "kQr6UJQF40g",
                        "bunny_ff_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1020/v-full-1020_Preview.mp4?token=Pbu2HsS9cB7xtIcKPTUkFIK5MxdgL2KfCCc1v1ViQzE&expires=1706590800",
                        "bunny_ff_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1020/v-full-1020_Preview.vtt?token=2fkEdUVxzo0-L-7PfcIFNGobUi2uJxkowuifTO4dN-Q&expires=1706590800",
                        "youtube_prerecorded_link": "https://youtu.be/GYIqimSJwiY",
                        "youtube_prerecorded_id": "GYIqimSJwiY",
                        "bunny_prerecorded_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1020/v-full-1020_Presentation.mp4?token=kisOqTTMSqfA7AhHfIxYMdhxPOOAQNEIJPxoq4p5iB0&expires=1706590800",
                        "bunny_prerecorded_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1020/v-full-1020_Presentation.vtt?token=KsReZSrxXPIJsNAk4naHm5D8toq_BpRVnrulFMQBxDU&expires=1706590800"
                    },
                    {
                        "slot_id": "v-full-1160",
                        "session_id": "full31",
                        "title": "InkSight: Leveraging Sketch Interaction for Documenting Chart Findings in Computational Notebooks",
                        "contributors": [
                            "Yanna Lin"
                        ],
                        "authors": [
                            "Yanna Lin",
                            "Haotian Li",
                            "Leni Yang",
                            "Aoyu Wu",
                            "Huamin Qu"
                        ],
                        "abstract": "Computational notebooks have become increasingly popular for exploratory data analysis due to their ability to support data exploration and explanation within a single document. Effective documentation for explaining chart findings during the exploration process is essential as it helps recall and share data analysis. However, documenting chart findings remains a challenge due to its time-consuming and tedious nature.  While existing automatic methods alleviate some of the burden on users, they often fail to cater to users' specific interests.  In response to these limitations, we present InkSight, a mixed-initiative computational notebook plugin that generates finding documentation based on the user's intent. InkSight allows users to express their intent in specific data subsets through sketching atop visualizations intuitively. To facilitate this, we designed two types of sketches, i.e., open-path and closed-path sketch. Upon receiving a user's sketch, InkSight identifies the sketch type and corresponding selected data items.  Subsequently, it filters data fact types based on the sketch and selected data items  before employing existing automatic data fact recommendation algorithms to infer data facts.  Using large language models (GPT-3.5), InkSight converts data facts into effective natural language documentation. Users can conveniently fine-tune the generated documentation within InkSight.  A user study with 12 participants demonstrated the usability and effectiveness of InkSight in expressing user intent and facilitating chart finding documentation.",
                        "uid": "v-full-1160",
                        "time_stamp": "2023-10-26T00:33:00Z",
                        "time_start": "2023-10-26T00:33:00Z",
                        "time_end": "2023-10-26T00:45:00Z",
                        "paper_type": "full",
                        "keywords": [
                            "Computational Notebook, Sketch-based Interaction, Documentation, Visualization, Exploratory Data Analysis"
                        ],
                        "doi": "10.1109/TVCG.2023.3327170",
                        "fno": "",
                        "has_image": true,
                        "has_pdf": true,
                        "paper_award": "",
                        "image_caption": "This figure shows the interface of InkSight, including the sketch panel and documentation panel. When the user sketches atop the chart to identify areas of interest, InkSight automatically generates corresponding documentation on the right.",
                        "external_paper_link": "",
                        "youtube_ff_link": "https://youtu.be/pNxS5x-zt5A",
                        "youtube_ff_id": "pNxS5x-zt5A",
                        "bunny_ff_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1160/v-full-1160_Preview.mp4?token=_bGmeZj1w5FYzRRy1_YKDkl68ox0ZG218oU4lGVX9H8&expires=1706590800",
                        "bunny_ff_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1160/v-full-1160_Preview.vtt?token=KvG0zv6yLCAF271lBKjiaVP0vhrDEOjDK07d6ltFvTQ&expires=1706590800",
                        "youtube_prerecorded_link": "https://youtu.be/9aUrbFWiaTk",
                        "youtube_prerecorded_id": "9aUrbFWiaTk",
                        "bunny_prerecorded_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1160/v-full-1160_Presentation.mp4?token=3--dIr_tsjMUvorFvRhlsaB94U_YXsqtsRlf46Hi2u0&expires=1706590800",
                        "bunny_prerecorded_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1160/v-full-1160_Presentation.vtt?token=GTS2-ai-ll26Y-RKC8fU2ALFyyPokFycclASI5_mQ48&expires=1706590800"
                    },
                    {
                        "slot_id": "v-full-1159",
                        "session_id": "full31",
                        "title": "Why Change My Design: Explaining Poorly Constructed Visualization Designs with Explorable Explanations",
                        "contributors": [
                            "Leo Yu-Ho Lo"
                        ],
                        "authors": [
                            "Leo Yu-Ho Lo",
                            "Yi-Fan Cao",
                            "Leni Yang",
                            "Huamin Qu"
                        ],
                        "abstract": "Although visualization tools are widely available and accessible, not everyone knows the best practices and guidelines for creating accurate and honest visual representations of data. Numerous books and articles have been written to expose the misleading potential of poorly constructed charts and teach people how to avoid being deceived by them or making their own mistakes. These readings use various rhetorical devices to explain the concepts to their readers. In our analysis of a collection of books, online materials, and a design workshop, we identified six common explanation methods. To assess the effectiveness of these methods, we conducted two crowdsourced studies (each with N = 125) to evaluate their ability to teach and persuade people to make design changes. In addition to these existing methods, we brought in the idea of Explorable Explanations, which allows readers to experiment with different chart settings and observe how the changes are reflected in the visualization. While we did not find significant differences across explanation methods, the results of our experiments indicate that, following the exposure to the explanations, the participants showed improved proficiency in identifying deceptive charts and were more receptive to proposed alterations of the visualization design. We discovered that participants were willing to accept more than 60% of the proposed adjustments in the persuasiveness assessment. Nevertheless, we found no significant differences among different explanation methods in convincing participants to accept the modifications.",
                        "uid": "v-full-1159",
                        "time_stamp": "2023-10-26T00:45:00Z",
                        "time_start": "2023-10-26T00:45:00Z",
                        "time_end": "2023-10-26T00:57:00Z",
                        "paper_type": "full",
                        "keywords": [
                            "Information Visualization, Deceptive Visualization, Explorable Explanations"
                        ],
                        "doi": "10.1109/TVCG.2023.3327155",
                        "fno": "",
                        "has_image": true,
                        "has_pdf": true,
                        "paper_award": "",
                        "image_caption": "Explorable explanation designs for five different common chart issues studied in the paper \"Why Change My Design: Explaining Poorly Constructed Visualization Designs with Explorable Explanations\"",
                        "external_paper_link": "",
                        "youtube_ff_link": "https://youtu.be/Bwdk5t-Qy0A",
                        "youtube_ff_id": "Bwdk5t-Qy0A",
                        "bunny_ff_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1159/v-full-1159_Preview.mp4?token=ShOEpxKSO4VVZ5Il6WAEUd5Y8L7y7g8vQpzHxciBCTg&expires=1706590800",
                        "bunny_ff_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1159/v-full-1159_Preview.vtt?token=keYUW5B7Ut-CJU-OVpcW9WyDxWhCjfSBZ15E8dItqwY&expires=1706590800",
                        "youtube_prerecorded_link": "https://youtu.be/966i7IF1IEI",
                        "youtube_prerecorded_id": "966i7IF1IEI",
                        "bunny_prerecorded_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1159/v-full-1159_Presentation.mp4?token=x294Gt9CwHlO_a_cWTa9MxTQpKatdJ3QhzzvCSfYTiY&expires=1706590800",
                        "bunny_prerecorded_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1159/v-full-1159_Presentation.vtt?token=r-4bekt0AAoqxxS-zOOyHy-rgUcRYoz8fIk46z27NUo&expires=1706590800"
                    }
                ]
            },
            {
                "title": "Visualization for Humanities and Social Sciences (Full+Short)",
                "session_id": "full32",
                "event_prefix": "v-full",
                "track": "oneohnine",
                "session_image": "full32.png",
                "chair": [
                    "Thomas Chandler"
                ],
                "time_start": "2023-10-25T03:00:00Z",
                "time_end": "2023-10-25T04:15:00Z",
                "discord_category": "",
                "discord_channel": "109",
                "discord_channel_id": "1161741434647482379",
                "discord_link": "https://discordapp.com/channels/1161400451619627068/1161741434647482379",
                "zoom_private_meeting": "",
                "zoom_private_password": "",
                "zoom_private_link": "",
                "zoom_broadcast_link": "",
                "ff_link": "https://youtu.be/HkkO4SwyruE",
                "time_slots": [
                    {
                        "slot_id": "v-full-1104",
                        "session_id": "full32",
                        "title": "InnovationInsights: A Visual Analytics Approach for Understanding the Dual Frontiers between Science and Technology",
                        "contributors": [
                            "Dr. Yifang Wang"
                        ],
                        "authors": [
                            "Yifang Wang",
                            "Yifan Qian",
                            "Xiaoyu Qi",
                            "Nan Cao",
                            "Dashun Wang"
                        ],
                        "abstract": "Science has long been viewed as a key driver of economic growth and rising standards of living. Knowledge about how scientific advances support marketplace inventions is therefore essential for understanding the role of science in propelling real-world applications and technological progress. The increasing availability of large-scale datasets tracing scientific publications and patented inventions and the complex interactions among them offers us new opportunities to explore the evolving dual frontiers of science and technology at an unprecedented level of scale and detail. However, we lack suitable visual analytics approaches to analyze such complex interactions effectively. Here we introduce InnovationInsights, an interactive visual analysis system for researchers, research institutions, and policymakers to explore the complex linkages between science and technology, and to identify critical innovations, inventors, and potential partners. The system first identifies important associations between scientific papers and patented inventions through a set of statistical measures introduced by our experts from the field of the Science of Science. A series of visualization views are then used to present these associations in the data context. In particular, we introduce the Interplay Graph to visualize patterns and insights derived from the data, helping users effectively navigate citation relationships between papers and patents. This visualization thereby helps them identify the origins of technical inventions and the impact of scientific research. We evaluate the system through two case studies with experts followed by expert interviews. We further engage a premier research institution to test-run the system, helping its institution leaders to extract new insights for innovation. Through both the case studies and the engagement project, we find that our system not only meets our original goals of design, allowing users to better identify the sources of technical inventions and to understand the broad impact of scientific research; it also goes beyond these purposes to enable an array of new applications for researchers and research institutions, ranging from identifying untapped innovation potential within an institution to forging new collaboration opportunities between science and industry.",
                        "uid": "v-full-1104",
                        "time_stamp": "2023-10-25T03:00:00Z",
                        "time_start": "2023-10-25T03:00:00Z",
                        "time_end": "2023-10-25T03:12:00Z",
                        "paper_type": "full",
                        "keywords": [
                            "Science of Science, Innovation, Academic Profiles, Patent Data, Publication Data, Visual Analytics"
                        ],
                        "doi": "10.1109/TVCG.2023.3327387",
                        "fno": "",
                        "has_image": true,
                        "has_pdf": true,
                        "paper_award": "honorable",
                        "image_caption": "Science is central to improving the human condition. Not only has science long been recognized as the engine for long-run economic growth and prosperity, but also it has been essential to creating critical solutions to confront emergent threats to humanity, from climate change to the COVID-19 pandemic. While scientific research propels both fundamental understanding and practical applications, there has been a lack of visual analytics approaches to explore the complex linkages (i.e., the dual frontiers) between scientific advances and technical inventions. Here we introduce InnovationInsights, which represents an initial step toward filling this crucial gap.",
                        "external_paper_link": "",
                        "youtube_ff_link": "https://youtu.be/a-pCg9jrhFg",
                        "youtube_ff_id": "a-pCg9jrhFg",
                        "bunny_ff_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1104/v-full-1104_Preview.mp4?token=5DjYZO7iqgO4UMV8bbSPq626QNszUszxthzfFh5yHFU&expires=1706590800",
                        "bunny_ff_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1104/v-full-1104_Preview.vtt?token=mrG3eWiJ0ZORhpG7Mio2cQkF677UaIsZtvQnYL58FuU&expires=1706590800",
                        "youtube_prerecorded_link": "https://youtu.be/06ZsvuyjtjA",
                        "youtube_prerecorded_id": "06ZsvuyjtjA",
                        "bunny_prerecorded_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1104/v-full-1104_Presentation.mp4?token=XqHZaLkED4QfEc9QfMD1q68f8mAxI9epStTltFJ1Qc8&expires=1706590800",
                        "bunny_prerecorded_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1104/v-full-1104_Presentation.vtt?token=GZeMC9lTiL0R9zFON84OPa9ogY-ZxzIIKZogiZJRZ2k&expires=1706590800"
                    },
                    {
                        "slot_id": "v-full-1274",
                        "session_id": "full32",
                        "title": "LiberRoad: Probing into the Journey of Chinese Classics through Visual Analytics",
                        "contributors": [
                            "Yuhan Guo"
                        ],
                        "authors": [
                            "Yuhan Guo",
                            "Yuchu Luo",
                            "Keer Lu",
                            "Linfang Li",
                            "haizheng Yang",
                            "Xiaoru Yuan"
                        ],
                        "abstract": "Books act as a crucial carrier of cultural dissemination in ancient times.",
                        "uid": "v-full-1274",
                        "time_stamp": "2023-10-25T03:12:00Z",
                        "time_start": "2023-10-25T03:12:00Z",
                        "time_end": "2023-10-25T03:24:00Z",
                        "paper_type": "full",
                        "keywords": [
                            "Visual analytics, digital humanities, spatial uncertainty, trajectory visualization, book movement, historical data"
                        ],
                        "doi": "10.1109/TVCG.2023.3326944",
                        "fno": "",
                        "has_image": true,
                        "has_pdf": true,
                        "paper_award": "",
                        "image_caption": "LiberRoad is a visual analytics system for humanities scholars to explore and analyze the book circulation data. Consisting the Location Graph, the Event Timeline, and the Geo Map, LiberRoad provides a clear presentation of the book circulation patterns across different historical periods.",
                        "external_paper_link": "",
                        "youtube_ff_link": "https://youtu.be/rblrPx4OGPU",
                        "youtube_ff_id": "rblrPx4OGPU",
                        "bunny_ff_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1274/v-full-1274_Preview.mp4?token=88ZJDyW9lzyPPDDWeEyXj6dw8BWJH0gNyzr9s2LH_lk&expires=1706590800",
                        "bunny_ff_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1274/v-full-1274_Preview.vtt?token=vvrBSi86l4RBctiDsHpkCtCjTU_oDd70XFVnRV4nQQc&expires=1706590800",
                        "youtube_prerecorded_link": "https://youtu.be/h7Hxc8765ag",
                        "youtube_prerecorded_id": "h7Hxc8765ag",
                        "bunny_prerecorded_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1274/v-full-1274_Presentation.mp4?token=Jb-tvvB3iFygeBdqWFSCA4RUvEpDYlltjNbN_9unhV0&expires=1706590800",
                        "bunny_prerecorded_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1274/v-full-1274_Presentation.vtt?token=t84Qfs027-PjMiSiYefSmdPQragSnVyqHvtAwdKQhYE&expires=1706590800"
                    },
                    {
                        "slot_id": "v-full-1387",
                        "session_id": "full32",
                        "title": "Visualizing Historical Book Trade Data: An Iterative Design Study with Close Collaboration with Domain Experts",
                        "contributors": [
                            "Yiwen Xing"
                        ],
                        "authors": [
                            "Yiwen Xing",
                            "Cristina Dondi",
                            "Rita Borgo",
                            "Alfie Abdul-Rahman"
                        ],
                        "abstract": "The circulation of historical books has always been an area of interest for historians. However, the data used to represent the journey of a book across different places and times can be difficult for domain experts to digest due to buried geographical and chronological features within text-based presentations. This situation provides an opportunity for collaboration between visualization researchers and historians. This paper describes a design study where a variant of the Nine-Stage Framework was employed to develop a Visual Analytics (VA) tool called DanteExploreVis. This tool was designed to aid domain experts in exploring, explaining, and presenting book trade data from multiple perspectives. We discuss the design choices made and how each panel in the interface meets the domain requirements. We also present the results of a qualitative evaluation conducted with domain experts. The main contributions of this paper include: 1) the development of a VA tool to support domain experts in exploring, explaining, and presenting book trade data; 2) a comprehensive documentation of the iterative design, development, and evaluation process following the variant Nine-Stage Framework; 3) a summary of the insights gained and lessons learned from this design study in the context of the humanities field; and 4) reflections on how our approach could be applied in a more generalizable way.",
                        "uid": "v-full-1387",
                        "time_stamp": "2023-10-25T03:24:00Z",
                        "time_start": "2023-10-25T03:24:00Z",
                        "time_end": "2023-10-25T03:36:00Z",
                        "paper_type": "full",
                        "keywords": [
                            "Design study, application motivated visualization, geospatial data"
                        ],
                        "doi": "10.1109/TVCG.2023.3326923",
                        "fno": "",
                        "has_image": true,
                        "has_pdf": true,
                        "paper_award": "",
                        "image_caption": "In our work, we introduce DanteExploreVis, a Visual Analytic (VA) tool designed to assist historians in exploring, explaining, and presenting book trade data. This timeline provides a thorough overview of the design process behind DanteExploreVis. It traces the stages of interface evolution, labeled I1 through I8, and highlights key sketches, indicated as S1 to S7, from each design iteration. Beyond a visual aid, this figure offers an in-depth narrative that emphasizes the crucial moments, decisions, and methodological adaptations we incorporated throughout our design study.",
                        "external_paper_link": "",
                        "youtube_ff_link": "https://youtu.be/hVYdZL4MMqI",
                        "youtube_ff_id": "hVYdZL4MMqI",
                        "bunny_ff_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1387/v-full-1387_Preview.mp4?token=MME9Er-q6qMGe8zHhuiCnuNrrRNotF4jbAaSDSzyicE&expires=1706590800",
                        "bunny_ff_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1387/v-full-1387_Preview.vtt?token=vCjvzuJWkrbP97faKe3Ov4yOnx09fEvyIk-_DL_n_Y8&expires=1706590800",
                        "youtube_prerecorded_link": "https://youtu.be/5EswGZzXctA",
                        "youtube_prerecorded_id": "5EswGZzXctA",
                        "bunny_prerecorded_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1387/v-full-1387_Presentation.mp4?token=vjM9gQCD5tUS0P38enA_9D0hhy0gQKKk2y-gNEjQVhk&expires=1706590800",
                        "bunny_prerecorded_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1387/v-full-1387_Presentation.vtt?token=TZQx7aI5c2PHWEL677t3GH2lA6-FyKGNg21ZgJx70AI&expires=1706590800"
                    },
                    {
                        "slot_id": "v-full-1333",
                        "session_id": "full32",
                        "title": "OldVisOnline: Curating a Dataset of Historical Visualizations",
                        "contributors": [
                            "Yu Zhang"
                        ],
                        "authors": [
                            "Yu Zhang",
                            "Ruike Jiang",
                            "Liwenhan Xie",
                            "Yuheng Zhao",
                            "Can Liu",
                            "Tianhong Ding",
                            "Siming Chen",
                            "Xiaoru Yuan"
                        ],
                        "abstract": "With the increasing adoption of digitization, more and more historical visualizations created hundreds of years ago are accessible in digital libraries online. It provides a unique opportunity for visualization and history research. Meanwhile, there is no large-scale digital collection dedicated to historical visualizations. The visualizations are scattered in various collections, which hinders retrieval. In this study, we curate the first large-scale dataset dedicated to historical visualizations. Our dataset comprises 13K historical visualization images with corresponding processed metadata from seven digital libraries. In curating the dataset, we propose a workflow to scrape and process heterogeneous metadata. We develop a semi-automatic labeling approach to distinguish visualizations from other artifacts. Our dataset can be accessed with OldVisOnline, a system we have built to browse and label historical visualizations. We discuss our vision of usage scenarios and research opportunities with our dataset, such as textual criticism for historical visualizations. Drawing upon our experience, we summarize recommendations for future efforts to improve our dataset.",
                        "uid": "v-full-1333",
                        "time_stamp": "2023-10-25T03:36:00Z",
                        "time_start": "2023-10-25T03:36:00Z",
                        "time_end": "2023-10-25T03:48:00Z",
                        "paper_type": "full",
                        "keywords": [
                            "Historical visualization, dataset, digital humanities, data labeling"
                        ],
                        "doi": "10.1109/TVCG.2023.3326908",
                        "fno": "",
                        "has_image": true,
                        "has_pdf": true,
                        "paper_award": "",
                        "image_caption": "The image gallery interface of OldVisOnline contains images and corresponding metadata of 13K historical visualizations published before 1950 that we curate from various data sources. The user can search and filter metadata fields of historical visualizations.",
                        "external_paper_link": "",
                        "youtube_ff_link": "https://youtu.be/qU9cXKv0Dzk",
                        "youtube_ff_id": "qU9cXKv0Dzk",
                        "bunny_ff_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1333/v-full-1333_Preview.mp4?token=phxAF7O0D_5FxPwHyDseXYXrawAQ3O-nuTTrN4_DQkY&expires=1706590800",
                        "bunny_ff_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1333/v-full-1333_Preview.vtt?token=4pbi4n_FDBoakvYm8w_QJWQXtVxdOodHiGNdtiM92L0&expires=1706590800",
                        "youtube_prerecorded_link": "https://youtu.be/ZyKijYzleR4",
                        "youtube_prerecorded_id": "ZyKijYzleR4",
                        "bunny_prerecorded_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1333/v-full-1333_Presentation.mp4?token=qYYcdd6Z94TEUOWaBqoQ36wbwydBbYIgg27aUHg3CHs&expires=1706590800",
                        "bunny_prerecorded_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1333/v-full-1333_Presentation.vtt?token=6rdFIOZTlEJ4pnAaRhqpEa5zi6oxnKNuJY9tbJbkGDs&expires=1706590800"
                    },
                    {
                        "slot_id": "v-short-1027",
                        "session_id": "full32",
                        "title": "What Exactly is an Insight? A Literature Review",
                        "contributors": [
                            "Leilani Battle"
                        ],
                        "authors": [
                            "Leilani Battle",
                            "Alvitta Ottley"
                        ],
                        "abstract": "Insights are often considered the ideal outcome of visual analysis sessions. However, there is no single definition of what an insight is. Some scholars define insights as correlations, while others define them as hypotheses or aha moments. This lack of a clear definition can make it difficult to build visualization tools that effectively support insight discovery. In this paper, we contribute a comprehensive literature review that maps the landscape of existing insight definitions. We summarize key themes regarding how insight is defined, with the goal of helping readers identify which definitions of insight align closely with their research and tool development goals. Based on our review, we also suggest interesting research directions, such as synthesizing a unified formalism for insight and connecting theories of insight to other critical concepts in visualization research.",
                        "uid": "v-short-1027",
                        "time_stamp": "2023-10-25T03:48:00Z",
                        "time_start": "2023-10-25T03:48:00Z",
                        "time_end": "2023-10-25T04:00:00Z",
                        "paper_type": "short",
                        "keywords": [
                            "Insight Discovery, Visualization Theory"
                        ],
                        "doi": "",
                        "fno": "",
                        "has_image": true,
                        "has_pdf": true,
                        "paper_award": "",
                        "image_caption": "Based on a review of the literature, insights seem to capture knowledge that can be inferred directly from a dataset such as data facts, generalizations of these facts, and hypotheses to be tested (internal knowledge). Insights also link internal knowledge with user domain knowledge, personal experiences, and tool expertise (external knowledge).",
                        "external_paper_link": "",
                        "youtube_ff_link": "https://youtu.be/FaXFIF3DAB0",
                        "youtube_ff_id": "FaXFIF3DAB0",
                        "bunny_ff_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-short/v-short-1027/v-short-1027_Preview.mp4?token=6zOsSXWdKlT4b0oDM3VxNI6cQHWKDhe1GkfKwSKdljk&expires=1706590800",
                        "bunny_ff_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-short/v-short-1027/v-short-1027_Preview.vtt?token=odKaJhJfV3vQmnSgqaNp6cyM_v9Ub_Gevb0xidlH77w&expires=1706590800",
                        "youtube_prerecorded_link": "https://youtu.be/uiYy35T462o",
                        "youtube_prerecorded_id": "uiYy35T462o",
                        "bunny_prerecorded_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-short/v-short-1027/v-short-1027_Presentation.mp4?token=NRZOlc7QtRXKXclF4QThVy8nWC_IP_5OXaqUUR1YV1A&expires=1706590800",
                        "bunny_prerecorded_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-short/v-short-1027/v-short-1027_Presentation.vtt?token=vsP5wMXXLiGfOb8IEeTp0wSU2WmrDD-8MOLN4EUjJVQ&expires=1706590800"
                    },
                    {
                        "slot_id": "v-short-1083",
                        "session_id": "full32",
                        "title": "WhaleVis: Visualizing the History of Commercial Whaling",
                        "contributors": [
                            "Ameya B Patil"
                        ],
                        "authors": [
                            "Ameya B Patil",
                            "Zoe Rand",
                            "Trevor Branch",
                            "Leilani Battle"
                        ],
                        "abstract": "Whales are an important part of the oceanic ecosystem. Although historic commercial whale hunting a.k.a. whaling has severely threatened whale populations, whale researchers are looking at historical whaling data to inform current whale status and future conservation efforts. To facilitate this, we worked with experts in aquatic and fishery sciences to create WhaleVis\u2014\u2013an interactive dashboard for the commercial whaling dataset maintained by the International Whaling Commission (IWC). We characterize key analysis tasks among whale researchers for this database, most important of which is inferring spatial distribution of whale populations over time. In addition to facilitating analysis of whale catches based on the spatio-temporal attributes, we use whaling expedition details to plot the search routes of expeditions. We propose a model of the catch data as a graph, where nodes represent catch locations, and edges represent whaling expedition routes. This model facilitates visual estimation of whale search effort and in turn the spatial distribution of whale populations normalized by the search effort\u2014a well known problem in fisheries research. It further opens up new avenues for graph analysis on the data, including more rigorous computation of spatial distribution of whales normalized by the search effort, and enabling new insight generation. We demonstrate the use of our dashboard through a real life use case.",
                        "uid": "v-short-1083",
                        "time_stamp": "2023-10-25T04:00:00Z",
                        "time_start": "2023-10-25T04:00:00Z",
                        "time_end": "2023-10-25T04:12:00Z",
                        "paper_type": "short",
                        "keywords": [
                            "Whaling, conservation, data visualization dashboards"
                        ],
                        "doi": "",
                        "fno": "",
                        "has_image": true,
                        "has_pdf": true,
                        "paper_award": "",
                        "image_caption": "Selective visualizations from WhaleVis: The map visualization shows pelagic (offshore) whale catches and the routes traversed by whaling expeditions between 1880 and 1986. The bar chart shows the breakdown for pelagic vs land catches and also facilitates setting a filter for pelagic whale catches. The route density in the map visualization enables visual estimation of where whales were searched for. The North Atlantic and South Pacific Oceans have  fewer catches than other regions. Since fewer expeditions traversed those waters, we are aware of a relative reduction in search effort when inferring the whale populations from reported catches in those regions.",
                        "external_paper_link": "",
                        "youtube_ff_link": "https://youtu.be/sxkZSOc59FQ",
                        "youtube_ff_id": "sxkZSOc59FQ",
                        "bunny_ff_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-short/v-short-1083/v-short-1083_Preview.mp4?token=XqEgW5xBYK09gZpRFKd6GHEk-J3i8r5xsNTPJaHh4qM&expires=1706590800",
                        "bunny_ff_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-short/v-short-1083/v-short-1083_Preview.vtt?token=qbFbVrGc_NxTXYUj9cwlYRkVMbGUcfZNrUz5dVCVtzg&expires=1706590800",
                        "youtube_prerecorded_link": "https://youtu.be/BU8FMbzAfpc",
                        "youtube_prerecorded_id": "BU8FMbzAfpc",
                        "bunny_prerecorded_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-short/v-short-1083/v-short-1083_Presentation.mp4?token=e5sEQQdhcIdRe8itiw6Y52jUSyLyD6d8scunSGoMExI&expires=1706590800",
                        "bunny_prerecorded_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-short/v-short-1083/v-short-1083_Presentation.vtt?token=QSsjog1xEVdCBLgqrzlfpGvcHRtjhUQUAVYmAsAWoxU&expires=1706590800"
                    }
                ]
            },
            {
                "title": "Visualization for the Physical Sciences",
                "session_id": "full33",
                "event_prefix": "v-full",
                "track": "oneohthree",
                "session_image": "full33.png",
                "chair": [
                    "Markus Hadwiger"
                ],
                "time_start": "2023-10-25T22:00:00Z",
                "time_end": "2023-10-25T23:15:00Z",
                "discord_category": "",
                "discord_channel": "103",
                "discord_channel_id": "1161741183815524502",
                "discord_link": "https://discordapp.com/channels/1161400451619627068/1161741183815524502",
                "zoom_private_meeting": "",
                "zoom_private_password": "",
                "zoom_private_link": "",
                "zoom_broadcast_link": "",
                "ff_link": "https://youtu.be/nxvvz1y6AwY",
                "time_slots": [
                    {
                        "slot_id": "v-full-1673",
                        "session_id": "full33",
                        "title": "Dr. KID: Direct Remeshing and K-set Isometric Decomposition for Scalable Physicalization of Organic Shapes",
                        "contributors": [
                            "Dawar Khan"
                        ],
                        "authors": [
                            "Dawar Khan",
                            "Ciril Bohak",
                            "Ivan Viola"
                        ],
                        "abstract": "Dr. KID is an algorithm that uses isometric decomposition for the physicalization of potato-shaped organic models in a puzzle fashion. The algorithm begins with creating a simple, regular triangular surface mesh of organic shapes, followed by iterative K-means clustering and remeshing. For clustering, we need similarity between triangles (segments) which is defined as a distance function. The distance function maps each triangle's shape to a single point in the virtual 3D space. Thus, the distance between the triangles indicates their degree of dissimilarity. K-means clustering uses this distance and sorts segments into k classes. After this, remeshing is applied to minimize the distance between triangles within the same cluster by making their shapes identical. Clustering and remeshing are repeated until the distance between triangles in the same cluster reaches an acceptable threshold. We adopt a curvature-aware strategy to determine the surface thickness and finalize puzzle pieces for 3D printing. Identical hinges and holes are created for assembling the puzzle components. For smoother outcomes, we use triangle subdivision along with curvature-aware clustering, generating curved triangular patches for 3D printing. Our algorithm was evaluated using various models, and the 3D-printed results were analyzed. Findings indicate that our algorithm performs reliably on target organic shapes with minimal loss of input geometry.",
                        "uid": "v-full-1673",
                        "time_stamp": "2023-10-25T22:00:00Z",
                        "time_start": "2023-10-25T22:00:00Z",
                        "time_end": "2023-10-25T22:12:00Z",
                        "paper_type": "full",
                        "keywords": [
                            "Physicalization, Physical visualization, 3D printing, Isometric decomposition, Direct remeshing, Biological structures, Intracellular compartments."
                        ],
                        "doi": "10.1109/TVCG.2023.3326595",
                        "fno": "",
                        "has_image": true,
                        "has_pdf": true,
                        "paper_award": "honorable",
                        "image_caption": "The physicalization of potato-shaped biological structures with k types of triangles. Back row: SARS-CoV-2 virion membrane (left) with k = 2, SARS-CoV-2 virion membrane with smooth triangle patches (right), using k = 6, and front row: cell nuclei membrane (left), using k = 5, SARS-CoV-2 virion membrane (center), using k = 2, mitochondria outer membrane (right), using k = 6.",
                        "external_paper_link": "",
                        "youtube_ff_link": "https://youtu.be/Kv9V69zSsgg",
                        "youtube_ff_id": "Kv9V69zSsgg",
                        "bunny_ff_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1673/v-full-1673_Preview.mp4?token=aclp_AwRvcs8ZmdL-lLcsviSey6JdtsccphSk11vD_I&expires=1706590800",
                        "bunny_ff_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1673/v-full-1673_Preview.vtt?token=i_Tf803J3-LNpa2hisLEdbgHpK60lZFXfGPFabXyPgw&expires=1706590800",
                        "youtube_prerecorded_link": "https://youtu.be/8v156eZrfTQ",
                        "youtube_prerecorded_id": "8v156eZrfTQ",
                        "bunny_prerecorded_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1673/v-full-1673_Presentation.mp4?token=uGR037nlG6US5OytWr0ZiDTBDZsy18hC10LenlurUhc&expires=1706590800",
                        "bunny_prerecorded_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1673/v-full-1673_Presentation.vtt?token=raIsTUrDQxLqbpIF57WzGUSbhOCsVjbZryH_dcQp5sg&expires=1706590800"
                    },
                    {
                        "slot_id": "v-full-1669",
                        "session_id": "full33",
                        "title": "Extract and Characterize Hairpin Vortices in Turbulent Flows",
                        "contributors": [
                            "Adeel Zafar"
                        ],
                        "authors": [
                            "Adeel Zafar",
                            "Di Yang",
                            "Guoning Chen"
                        ],
                        "abstract": "Hairpin vortices are one of the most important vortical structures in turbulent flows. Extracting and characterizing hairpin vortices provides useful insight into many behaviors in turbulent flows. However, hairpin vortices have complex configurations and might be entangled with other vortices, making their extraction difficult. In this work, we introduce a framework to extract and separate hairpin vortices in shear driven turbulent flows for their study. Our method first extracts general vortical regions with a region-growing strategy based on certain vortex criteria (e.g., \u03bb2) and then separates those vortices with the help of progressive extraction of (\u03bb2) iso-surfaces in a top-down fashion. This leads to a hierarchical tree representing the spatial proximity and merging relation of vortices. After separating individual vortices, their shape and orientation information is extracted. Candidate hairpin vortices are identified based on their shape and orientation information as well as their physical characteristics. An interactive visualization system is developed to aid the exploration, classification, and analysis of hairpin vortices based on their geometric and physical attributes. We also present additional use cases of the proposed system for the analysis and study of general vortices in other types of flows.",
                        "uid": "v-full-1669",
                        "time_stamp": "2023-10-25T22:12:00Z",
                        "time_start": "2023-10-25T22:12:00Z",
                        "time_end": "2023-10-25T22:24:00Z",
                        "paper_type": "full",
                        "keywords": [
                            "Turbulent flow, vortices, hairpin vortex extraction"
                        ],
                        "doi": "10.1109/TVCG.2023.3326603",
                        "fno": "",
                        "has_image": true,
                        "has_pdf": true,
                        "paper_award": "honorable",
                        "image_caption": "Extract, separate, identify and characterize hairpin vortices in turbulent flows",
                        "external_paper_link": "",
                        "youtube_ff_link": "https://youtu.be/Gemap7uU6_o",
                        "youtube_ff_id": "Gemap7uU6_o",
                        "bunny_ff_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1669/v-full-1669_Preview.mp4?token=4nOlTgO04uVkOoscBRG4ayEJPhuluZWaMZDova-yQKk&expires=1706590800",
                        "bunny_ff_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1669/v-full-1669_Preview.vtt?token=URyo7lYOTReNrhMvY9X2y36rz7trxXvt7cWf3nVL9bY&expires=1706590800",
                        "youtube_prerecorded_link": "https://youtu.be/TpJUFmReA-M",
                        "youtube_prerecorded_id": "TpJUFmReA-M",
                        "bunny_prerecorded_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1669/v-full-1669_Presentation.mp4?token=EKAtISmgh2oXHuRNZgyWASQ8v0zj46twoZKa0zDkYrw&expires=1706590800",
                        "bunny_prerecorded_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1669/v-full-1669_Presentation.vtt?token=00y9sAje2mxfdNVVkiSaiLe3JDtfjXGQ4MRBbdXpQ2E&expires=1706590800"
                    },
                    {
                        "slot_id": "v-full-1530",
                        "session_id": "full33",
                        "title": "MolSieve: A Progressive Visual Analytics System for Molecular Dynamics Simulations",
                        "contributors": [
                            "Rostyslav Hnatyshyn"
                        ],
                        "authors": [
                            "Rostyslav Hnatyshyn",
                            "Jieqiong Zhao",
                            "Danny Perez",
                            "James Ahrens",
                            "Ross Maciejewski"
                        ],
                        "abstract": "Molecular Dynamics (MD) simulations are ubiquitous in cutting-edge physio-chemical research. They provide critical insights into how a physical system evolves over time given a model of interatomic interactions. Understanding a system\u2019s evolution is key to selecting the best candidates for new drugs, materials for manufacturing, and countless other practical applications. With today\u2019s technology, these simulations can encompass millions of unit transitions between discrete molecular structures, spanning up to several milliseconds of real time. Attempting to perform a brute-force analysis with data-sets of this size is not only computationally impractical, but would not shed light on the physically-relevant features of the data. Moreover, there is a need to analyze simulation ensembles in order to compare similar processes in differing environments. These problems call for an approach that is analytically transparent, computationally efficient, and flexible enough to handle the variety found in materials-based research. In order to address these problems, we introduce MolSieve, a progressive visual analytics system that enables the comparison of multiple long-duration simulations. Using MolSieve, analysts are able to quickly identify and compare regions of interest within immense simulations through its combination of control charts, data-reduction techniques, and highly informative visual components. A simple programming interface is provided which allows experts to fit MolSieve to their needs. To demonstrate the efficacy of our approach, we present two case studies of MolSieve and report on findings from domain collaborators.",
                        "uid": "v-full-1530",
                        "time_stamp": "2023-10-25T22:24:00Z",
                        "time_start": "2023-10-25T22:24:00Z",
                        "time_end": "2023-10-25T22:36:00Z",
                        "paper_type": "full",
                        "keywords": [
                            "Molecular dynamics, time-series analysis, visual analytics"
                        ],
                        "doi": "10.1109/TVCG.2023.3326584",
                        "fno": "",
                        "has_image": true,
                        "has_pdf": true,
                        "paper_award": "",
                        "image_caption": "MolSieve is a visual analytics system that reduces large molecular dynamics simulations into their essential components. To accomplish this, GPCCA (Generalized Perron-Cluster-Cluster-Analysis) is applied to a trajectory's transition matrix. The results are then mapped to an easy-to-use and efficient visual representation. MolSieve is scalable and adaptable; multiple large trajectories can be analyzed at once, and many interactions exist to make it easy to compare multiple simulations. MolSieve can be customized through a simple Python programming interface to adapt to any kind of simulation.",
                        "external_paper_link": "",
                        "youtube_ff_link": "https://youtu.be/9tz_7aRFP5o",
                        "youtube_ff_id": "9tz_7aRFP5o",
                        "bunny_ff_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1530/v-full-1530_Preview.mp4?token=LhQcaX8xA_Rm2iX0VZJTSSHFYdm141_Gw75OMFR4R38&expires=1706590800",
                        "bunny_ff_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1530/v-full-1530_Preview.vtt?token=2EYka_jrqZMl1u4guciWiRLfzsRybtApcKNO9UcoLTs&expires=1706590800",
                        "youtube_prerecorded_link": "https://youtu.be/VAxMeWR-1IA",
                        "youtube_prerecorded_id": "VAxMeWR-1IA",
                        "bunny_prerecorded_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1530/v-full-1530_Presentation.mp4?token=C9Q03lSJ1ntx88NCOszxlFKMQYsFdA_rnbQ_t3GYgac&expires=1706590800",
                        "bunny_prerecorded_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1530/v-full-1530_Presentation.vtt?token=YZtlZmioYiTXQqHfa2Mwm_gZQtWWb6yGBFmU4NGCTMQ&expires=1706590800"
                    },
                    {
                        "slot_id": "v-full-1773",
                        "session_id": "full33",
                        "title": "ProWis: A Visual Approach for Building, Managing, and Analyzing Weather Simulation Ensembles at Runtime",
                        "contributors": [
                            "Marcos Lage"
                        ],
                        "authors": [
                            "Carolina Veiga Ferreira de Souza",
                            "Suzanna Maria Bonnet",
                            "Daniel de Oliveira",
                            "Marcio Cataldi",
                            "Fabio Miranda",
                            "Marcos Lage"
                        ],
                        "abstract": "Weather forecasting is essential for decision-making and is usually performed using numerical modeling. Numerical weather models, in turn, are complex tools that require specialized training and laborious setup and are challenging even for weather experts. Moreover, weather simulations are data-intensive computations and may take hours to days to complete. When the simulation is finished, the experts face challenges analyzing its outputs, a large mass of spatiotemporal and multivariate data. From the simulation setup to the analysis of results, working with weather simulations involves several manual and error-prone steps. The complexity of the problem increases exponentially when the experts must deal with ensembles of simulations, a frequent task in their daily duties. To tackle these challenges, we propose ProWis: an interactive and provenance-oriented system to help weather experts build, manage, and analyze simulation ensembles at runtime. Our system follows a human-in-the-loop approach to enable the exploration of multiple atmospheric variables and weather scenarios. ProWis was built in close collaboration with weather experts, and we demonstrate its effectiveness by presenting two case studies of rainfall events in Brazil.",
                        "uid": "v-full-1773",
                        "time_stamp": "2023-10-25T22:36:00Z",
                        "time_start": "2023-10-25T22:36:00Z",
                        "time_end": "2023-10-25T22:48:00Z",
                        "paper_type": "full",
                        "keywords": [
                            "Weather visualization, Ensemble visualization, Provenance management, WRF visual setup"
                        ],
                        "doi": "10.1109/TVCG.2023.3326514",
                        "fno": "",
                        "has_image": true,
                        "has_pdf": true,
                        "paper_award": "honorable",
                        "image_caption": "We present the Provenance-enabled Weather Visualization (ProWis) system, a visual analytics system to assist weather professionals to work with the Weather Research and Forecasting model (WRF). The interactive and provenance-oriented system was designed to help weather experts build, manage, and analyze simulation ensembles at runtime. Our system follows a human-in-the-loop approach to enable the exploration of multiple atmospheric variables and weather scenarios. In collaboration with weather experts, we demonstrate its effectiveness by presenting two case studies of rainfall events in Brazil.",
                        "external_paper_link": "",
                        "youtube_ff_link": "https://youtu.be/tW6WzsnZXE8",
                        "youtube_ff_id": "tW6WzsnZXE8",
                        "bunny_ff_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1773/v-full-1773_Preview.mp4?token=LMHc2pDgzN7VMfROsDd-vsCF4j9jQq7vCfDmoXY_09s&expires=1706590800",
                        "bunny_ff_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1773/v-full-1773_Preview.vtt?token=5Fku0ZtIRaax1ijvVpgMdYh2IL-jslnwJM4IsZHJKZ4&expires=1706590800",
                        "youtube_prerecorded_link": "https://youtu.be/wVpLwvVA5XE",
                        "youtube_prerecorded_id": "wVpLwvVA5XE",
                        "bunny_prerecorded_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1773/v-full-1773_Presentation.mp4?token=T8RxQWt9LFyPxoZk0WF0-OWJM6vGaCfJpP4PDFZgwrw&expires=1706590800",
                        "bunny_prerecorded_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1773/v-full-1773_Presentation.vtt?token=C8wjpqMPf92hOcybIOUbXY9usIlcOeNtuLYcQ-RNT7o&expires=1706590800"
                    },
                    {
                        "slot_id": "v-full-1253",
                        "session_id": "full33",
                        "title": "Vimo: Visual Analysis of Neuronal Connectivity Motifs",
                        "contributors": [
                            "Jakob Troidl"
                        ],
                        "authors": [
                            "Jakob Troidl",
                            "Simon Alexander Warchol",
                            "Jinhan Choi",
                            "Jordan Matelsky",
                            "Nagaraju Dhanyasi",
                            "Xueying Wang",
                            "Brock Wester",
                            "Donglai Wei",
                            "Jeff Lichtman",
                            "Hanspeter Pfister",
                            "Johanna Beyer"
                        ],
                        "abstract": "Recent advances in high-resolution connectomics provide researchers with access to accurate petascale reconstructions of neuronal circuits and brain networks for the first time. Neuroscientists are analyzing these networks to better understand information processing in the brain. In particular, scientists are interested in identifying specific small network motifs, i.e., repeating subgraphs of the larger brain network that are believed to be neuronal building blocks. Although such motifs are typically small (e.g., 2 -6 neurons), the vast data sizes and intricate data complexity present significant challenges to the search and analysis process. To analyze these motifs, it is crucial to review instances of a motif in the brain network and then map the graph structure to detailed 3D reconstructions of the involved neurons and synapses. We present Vimo, an interactive visual approach to analyze neuronal motifs and motif chains in large brain networks. Experts can sketch network motifs intuitively in a visual interface and specify structural properties of the involved neurons and synapses to query large connectomics datasets. Motif instances (MIs) can be explored in high-resolution 3D renderings. To simplify the analysis of MIs, we designed a continuous focus&context metaphor inspired by visual abstractions. This allows users to transition from a highly-detailed rendering of the anatomical structure to views that emphasize the underlying motif structure and synaptic connectivity. Furthermore, Vimo supports the identification of motif chains where a motif is used repeatedly (e.g., 2 -4 times) to form a larger network structure. We evaluate Vimo in a user study and an in-depth case study with seven domain experts on motifs in a large connectome of the fruit fly, including more than 21,000 neurons and 20 million synapses. We find that Vimo enables hypothesis generation and confirmation through fast analysis iterations and connectivity highlighting.",
                        "uid": "v-full-1253",
                        "time_stamp": "2023-10-25T22:48:00Z",
                        "time_start": "2023-10-25T22:48:00Z",
                        "time_end": "2023-10-25T23:00:00Z",
                        "paper_type": "full",
                        "keywords": [
                            "Visual motif analysis, Focus&Context, Scientific visualization, Neuroscience, Connectomics."
                        ],
                        "doi": "10.1109/TVCG.2023.3327388",
                        "fno": "",
                        "has_image": true,
                        "has_pdf": true,
                        "paper_award": "",
                        "image_caption": "Vimo is a visual analysis tool to search and analyze arbitrary connectivity motifs in large brain networks.",
                        "external_paper_link": "",
                        "youtube_ff_link": "https://youtu.be/9OEPANsTtjs",
                        "youtube_ff_id": "9OEPANsTtjs",
                        "bunny_ff_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1253/v-full-1253_Preview.mp4?token=xhPQ4Gw2nzESmn8StseMrBnJMolGQd5pVoi8C_bbHlY&expires=1706590800",
                        "bunny_ff_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1253/v-full-1253_Preview.vtt?token=TArhsDreBtN_YBpUIJ1m1EQzH3N7fMyhhrqIjIJZ0t4&expires=1706590800",
                        "youtube_prerecorded_link": "https://youtu.be/MtAWc_XoilE",
                        "youtube_prerecorded_id": "MtAWc_XoilE",
                        "bunny_prerecorded_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1253/v-full-1253_Presentation.mp4?token=sOK0Rmab9bUTuxmNOhhMjsooeL_1eLAkZG2yFZqzhIY&expires=1706590800",
                        "bunny_prerecorded_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1253/v-full-1253_Presentation.vtt?token=KMMUQKV235aWMVBQ_jB_NcGhgkNIATFGpRONXsCEaLk&expires=1706590800"
                    },
                    {
                        "slot_id": "v-full-1298",
                        "session_id": "full33",
                        "title": "Visual Analysis of Displacement Processes in Porous Media using Spatio-Temporal Flow Graphs",
                        "contributors": [
                            "Alexander Straub"
                        ],
                        "authors": [
                            "Alexander Straub",
                            "Nikolaos Karadimitriou",
                            "Guido Reina",
                            "Steffen Frey",
                            "Holger Steeb",
                            "Thomas Ertl"
                        ],
                        "abstract": "We developed a new approach comprised of different visualizations for the comparative spatio-temporal analysis of displacement processes in porous media. We aim to analyze and compare ensemble datasets from experiments to gain insight into the influence of different parameters on fluid flow. To capture the displacement of a defending fluid by an invading fluid, we first condense an input image series to a single time map. From this map, we generate a spatio-temporal flow graph covering the whole process. This graph is further simplified to only reflect topological changes in the movement of the invading fluid. Our interactive tools allow the visual analysis of these processes by visualizing the graph structure and the context of the experimental setup, as well as by providing charts for multiple metrics. We apply our approach to analyze and compare ensemble datasets jointly with domain experts, where we vary either fluid properties or the solid structure of the porous medium. We finally report the generated insights from the domain experts and discuss our contribution\u2019s advantages, generality, and limitations.",
                        "uid": "v-full-1298",
                        "time_stamp": "2023-10-25T23:00:00Z",
                        "time_start": "2023-10-25T23:00:00Z",
                        "time_end": "2023-10-25T23:12:00Z",
                        "paper_type": "full",
                        "keywords": [
                            "Comparative visualization, ensemble, graph, porous media."
                        ],
                        "doi": "10.1109/TVCG.2023.3326931",
                        "fno": "",
                        "has_image": true,
                        "has_pdf": true,
                        "paper_award": "",
                        "image_caption": "Different abstractions of the displacement process of fluids in a porous material.  Time map showing the invading fluid entering the porous domain on the right side and propagating towards the outlet on the left. The periodic color map provides time information directly (violet to yellow), and velocity from its frequency.  The extracted graphs are either shown in their physical embedding with time information (displacement graph; white to red), or employing a different layout to show the main channel and highlight the number of outgoing edges (breakthrough graph; categorical color map).",
                        "external_paper_link": "",
                        "youtube_ff_link": "https://youtu.be/2jFzP_AuBp0",
                        "youtube_ff_id": "2jFzP_AuBp0",
                        "bunny_ff_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1298/v-full-1298_Preview.mp4?token=b40paE2X6TDNuzATQz2yavaKV_oxW_FwGEhpfNhOGAs&expires=1706590800",
                        "bunny_ff_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1298/v-full-1298_Preview.vtt?token=jH4s5jkv4BtNFMR6jC6nb5wrKP_t38gTyGNaLQ8WaJI&expires=1706590800",
                        "youtube_prerecorded_link": "https://youtu.be/WngF_Xyr87k",
                        "youtube_prerecorded_id": "WngF_Xyr87k",
                        "bunny_prerecorded_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1298/v-full-1298_Presentation.mp4?token=UYxqQtLI1T-r7rDtxXH-kXB14sS-MubMerKF8wgzQSk&expires=1706590800",
                        "bunny_prerecorded_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-full/v-full-1298/v-full-1298_Presentation.vtt?token=MOGj6_1uOtyWpIGP4km9eGK575Ri9YwV3j5Db3kUflo&expires=1706590800"
                    }
                ]
            }
        ]
    },
    "v-short": {
        "event": "VIS Short Papers",
        "long_name": "VIS Short Papers",
        "event_type": "short",
        "event_prefix": "v-short",
        "event_description": "",
        "event_url": "",
        "organizers": [],
        "sessions": [
            {
                "title": "Information Visualization / Interaction",
                "session_id": "short1",
                "event_prefix": "v-short",
                "track": "oneohfour",
                "session_image": "short1.png",
                "chair": [
                    "Andreas Kerren"
                ],
                "time_start": "2023-10-26T03:00:00Z",
                "time_end": "2023-10-26T04:15:00Z",
                "discord_category": "",
                "discord_channel": "104",
                "discord_channel_id": "1161741240665124954",
                "discord_link": "https://discordapp.com/channels/1161400451619627068/1161741240665124954",
                "zoom_private_meeting": "",
                "zoom_private_password": "",
                "zoom_private_link": "",
                "zoom_broadcast_link": "",
                "ff_link": "https://youtu.be/m5VQinFhYLw",
                "time_slots": [
                    {
                        "slot_id": "v-short-1160",
                        "session_id": "short1",
                        "title": "A Simple yet Useful Spiral Visualization of Large Graphs",
                        "contributors": [
                            "Graima Jindal"
                        ],
                        "authors": [
                            "Garima Jindal",
                            "Kamalakar Karlapalem"
                        ],
                        "abstract": "We present a Spiral Visualization that facilitates users to visually comprehend large graphs. Spiral Visualization is a representation that highlights key aspects of networks, including the number, size, and density of communities, important or central nodes within communities, centrality distribution within communities, connections between communities, and connections between nodes. To facilitate analysis and comprehension of networks using various interaction techniques, such as zooming, tooltip, and highlight, we have implemented a Spiral Visualization dashboard. We conducted a qualitative user study incorporating observation, think-aloud protocols, and participant rating of confidence and easiness to assess the usability and suitability of our visualization. The findings suggest that our visualization is appropriate for the network tasks evaluated. However, tasks requiring color comparison, such as identifying the densest community and comparing community densities were found to be more challenging to perform.",
                        "uid": "v-short-1160",
                        "time_stamp": "2023-10-26T03:00:00Z",
                        "time_start": "2023-10-26T03:00:00Z",
                        "time_end": "2023-10-26T03:09:00Z",
                        "paper_type": "short",
                        "keywords": [
                            "Human-centered computing\u2014Visualization\u2014Visualization techniques\u2014Graph Drawing; Human-centered computing\u2014Visualization\u2014Visualization design and evaluation methods"
                        ],
                        "doi": "",
                        "fno": "",
                        "has_image": true,
                        "has_pdf": true,
                        "paper_award": "",
                        "image_caption": "This figure presents a Spiral Visualization on the Facebook dataset having 4,039 nodes and 88,234 edges. The Spiral Visualization uses continuous diverging color coding. Each spiral in a spiral visualization represents a community within the dataset. The radius of the spiral represents the size of the community. Each edge represents the existence of inter-community edges between two communities, and the edge width represents the number of interconnections between them. Nodes are ordered and colored based on degree centrality to visualize and compare degree distribution in spirals.",
                        "external_paper_link": "",
                        "youtube_ff_link": "https://youtu.be/RsvJuDcTSoY",
                        "youtube_ff_id": "RsvJuDcTSoY",
                        "bunny_ff_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-short/v-short-1160/v-short-1160_Preview.mp4?token=8MvNTsr5nzy5h1O_PpKqNmGGlsz2RTQjhXn4PnBtxVI&expires=1706590800",
                        "bunny_ff_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-short/v-short-1160/v-short-1160_Preview.vtt?token=CnxznuSHkPTXxDeTxFPMYzGga3_tFX_2lPJT-qMtKCc&expires=1706590800",
                        "youtube_prerecorded_link": "https://youtu.be/sKv2LDg_394",
                        "youtube_prerecorded_id": "sKv2LDg_394",
                        "bunny_prerecorded_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-short/v-short-1160/v-short-1160_Presentation.mp4?token=y4q3YlTqsi0cFPB6tNIHTBKb6xBUgOnT-mFA0wNCNIg&expires=1706590800",
                        "bunny_prerecorded_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-short/v-short-1160/v-short-1160_Presentation.vtt?token=rcBnOe5OQFWXIi9275P0WJaa7jbd05_4sJJWFLHx8cU&expires=1706590800"
                    },
                    {
                        "slot_id": "v-short-1189",
                        "session_id": "short1",
                        "title": "ProtoGraph: A Non-Expert Toolkit for Creating Animated Graphs",
                        "contributors": [
                            "Carolina Nobre"
                        ],
                        "authors": [
                            "Machiel Daniel Rodrigues",
                            "Joel Dapello",
                            "Priyan Vaithilingam",
                            "Johanna Beyer",
                            "Carolina Nobre"
                        ],
                        "abstract": "Creating intuitive and aesthetically pleasing visualizations and animations of small-to-moderate-sized graphs in the form of node-link diagrams is a common task across many fields, particularly in pedagogical settings. However, creating a graph visualization either requires users to manually construct a graph by hand or programming skills. We present ProtoGraph, an English-like programming language for non-expert users to rapidly specify and animate node-link graph visualizations. The language supports iterative prototyping, thereby allowing non-experts users to intuitively refine their graphs, and to easily create animated graphs. The key features of ProtoGraph include a web-based live coding interface, previews for the different states in an animated graph, integrated user documentation, and an active-learning style tutorial. We have integrated the ProtoGraph language into an open-source JavaScript graph visualization library for rendering and a graphical web interface for rapid prototyping. In a user study, we show that participants with varying coding experiences were able to quickly learn the ProtoGraph language and create real-world pedagogical visualizations, showing that ProtoGraph is easy to learn, efficient to use, and extensible.",
                        "uid": "v-short-1189",
                        "time_stamp": "2023-10-26T03:09:00Z",
                        "time_start": "2023-10-26T03:09:00Z",
                        "time_end": "2023-10-26T03:18:00Z",
                        "paper_type": "short",
                        "keywords": [
                            "Human-centered computing\u2014Visualization\u2014Visualization techniques\u2014Graph drawings; Human-centered computing\u2014Visualization\u2014Visualization systems and tools\u2014Visualization toolkits"
                        ],
                        "doi": "",
                        "fno": "",
                        "has_image": true,
                        "has_pdf": true,
                        "paper_award": "",
                        "image_caption": "ProtoGraph web tool interface. The left column shows the code editor where the user can write ProtoGraph language statements, the right column displays the graph which dynamically renders as the user types in the editor. The bottom right panel  shows the animation timeline previewing each step of the animation. The user can share the current graph using an auto-generated  URL or export the visualization as an image or video.",
                        "external_paper_link": "",
                        "youtube_ff_link": "https://youtu.be/m3YEi_e3Ab8",
                        "youtube_ff_id": "m3YEi_e3Ab8",
                        "bunny_ff_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-short/v-short-1189/v-short-1189_Preview.mp4?token=16ghbk6ZTHBknYxcQZiYhCJ61kvLW4RoqSw8e8cTm64&expires=1706590800",
                        "bunny_ff_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-short/v-short-1189/v-short-1189_Preview.vtt?token=nvSP1EsaZW-q_O2D4hLPplslGbZeHZuwyGGGHUGzDQU&expires=1706590800",
                        "youtube_prerecorded_link": "https://youtu.be/t-rEoqkswZA",
                        "youtube_prerecorded_id": "t-rEoqkswZA",
                        "bunny_prerecorded_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-short/v-short-1189/v-short-1189_Presentation.mp4?token=nYmMli0Ban-A66zzU2Z96q3ukNJHozWGY9K7iSLGd9M&expires=1706590800",
                        "bunny_prerecorded_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-short/v-short-1189/v-short-1189_Presentation.vtt?token=9oQ5vsh-OfCGLMwYrpnJ77hL9glECViW3iICYaxewSI&expires=1706590800"
                    },
                    {
                        "slot_id": "v-short-1103",
                        "session_id": "short1",
                        "title": "Visual Validation versus Visual Estimation: A Study on the Average Value in Scatterplots",
                        "contributors": [
                            "Daniel Braun"
                        ],
                        "authors": [
                            "Daniel Braun",
                            "Ashley Suh",
                            "Remco Chang",
                            "Michael Gleicher",
                            "Tatiana von Landesberger"
                        ],
                        "abstract": "We investigate the ability of individuals to visually validate statistical models in terms of their fit to the data. While visual model estimation has been studied extensively, visual model validation remains under-investigated. It is unknown how well people are able to visually validate models, and how their performance compares to visual and computational estimation. As a starting point, we conducted a study across two populations (crowdsourced and volunteers). Participants had to both visually estimate (i.e, draw) and visually validate (i.e., accept or reject) the frequently studied model of averages. Across both populations, the level of accuracy of the models that were considered valid was lower than the accuracy of the estimated models. We find that participants\u2019 validation and estimation were unbiased. Moreover, their natural critical point between accepting and rejecting a given mean value is close to the boundary of its 95% confidence interval, indicating that the visually perceived confidence interval corresponds to a common statistical standard. Our work contributes to the understanding of visual model validation and opens new research opportunities.",
                        "uid": "v-short-1103",
                        "time_stamp": "2023-10-26T03:18:00Z",
                        "time_start": "2023-10-26T03:18:00Z",
                        "time_end": "2023-10-26T03:27:00Z",
                        "paper_type": "short",
                        "keywords": [
                            "Perception, Visual model validation, Visual model estimation, User study, Information visualization"
                        ],
                        "doi": "",
                        "fno": "",
                        "has_image": true,
                        "has_pdf": true,
                        "paper_award": "",
                        "image_caption": "Example scatterplot shown to participants in our user study to investigate the differences between (a) visually estimating  and (b) visually validating the average value of the shown data. In (c), the red lines indicate the upper border of the statistical 95%  confidence interval (CI) of the average value and the blue line shows the data\u2019s true average value.",
                        "external_paper_link": "",
                        "youtube_ff_link": "https://youtu.be/gteNsqVsz88",
                        "youtube_ff_id": "gteNsqVsz88",
                        "bunny_ff_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-short/v-short-1103/v-short-1103_Preview.mp4?token=sdjZ2sSlFUruVV6xHGJYkfT1HWe9NRyPWvYv0T8nODk&expires=1706590800",
                        "bunny_ff_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-short/v-short-1103/v-short-1103_Preview.vtt?token=Pveb3pHDZ74B342AJOySb6Sg7HQq9uGYP3oQfodPIAk&expires=1706590800",
                        "youtube_prerecorded_link": "https://youtu.be/VHoYANynMKY",
                        "youtube_prerecorded_id": "VHoYANynMKY",
                        "bunny_prerecorded_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-short/v-short-1103/v-short-1103_Presentation.mp4?token=c_lJX9IvGx3_C9bIa9dGJBFQCK5KsuVZLO1IGe69m8A&expires=1706590800",
                        "bunny_prerecorded_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-short/v-short-1103/v-short-1103_Presentation.vtt?token=R0Z-fEpj4ROmH5iNlsryGM_C4R2iRevQ-YyAiRs98ag&expires=1706590800"
                    },
                    {
                        "slot_id": "v-short-1014",
                        "session_id": "short1",
                        "title": "Line Harp: Importance-Driven Sonification for Dense Line Charts",
                        "contributors": [
                            "Stefan Bruckner"
                        ],
                        "authors": [
                            "Egil Bru",
                            "Thomas Trautner",
                            "Stefan Bruckner"
                        ],
                        "abstract": "Accessibility in visualization is an important yet challenging topic. Sonification, in particular, is a valuable yet underutilized technique that can enhance accessibility for people with low vision. However, the lower bandwidth of the auditory channel makes it difficult to fully convey dense visualizations. For this reason, interactivity is key in making full use of its potential. In this paper, we present a novel approach for the sonification of dense line charts. We utilize the metaphor of a string instrument, where individual line segments can be \"plucked\". We propose an importance-driven approach which encodes the directionality of line segments using frequency and dynamically scales amplitude for improved density perception. We discuss the potential of our approach based on a set of examples.",
                        "uid": "v-short-1014",
                        "time_stamp": "2023-10-26T03:27:00Z",
                        "time_start": "2023-10-26T03:27:00Z",
                        "time_end": "2023-10-26T03:36:00Z",
                        "paper_type": "short",
                        "keywords": [
                            "Human-centered computing\u2014Visualization\u2014Visualization application domains\u2014Information visualization; Human-centered computing\u2014Human computer interaction (HCI)\u2014Interaction techniques\u2014Auditory feedback; Human-centered computing\u2014Accessibility\u2014Accessibility systems and tools"
                        ],
                        "doi": "",
                        "fno": "",
                        "has_image": true,
                        "has_pdf": true,
                        "paper_award": "",
                        "image_caption": "Line Harp is an interactive sonification approach for dense line charts. We utilize the metaphor of a string instrument, where individual line segments can be \u201dplucked\u201d. We propose an importance-driven approach which encodes the directionality of line segments using frequency and dynamically scales amplitude for improved density perception, and also provide additional sonified lenses for enhanced data exploration.",
                        "external_paper_link": "",
                        "youtube_ff_link": "https://youtu.be/nqrujOsrmyI",
                        "youtube_ff_id": "nqrujOsrmyI",
                        "bunny_ff_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-short/v-short-1014/v-short-1014_Preview.mp4?token=m4Q1y8lgtpNNfWcGzf-PPkoJYMYRO9_JobKdR1fg8Ig&expires=1706590800",
                        "bunny_ff_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-short/v-short-1014/v-short-1014_Preview.vtt?token=pxTrg298gSKuk304vbeW3vyDUf6WvxWo8fkMx0S4Buc&expires=1706590800",
                        "youtube_prerecorded_link": "",
                        "youtube_prerecorded_id": "",
                        "bunny_prerecorded_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-short/v-short-1014/v-short-1014_Presentation.mp4?token=xq3wxCCh55RVaFR-E-nPVrlCxd5sTkwSFQhrJ0VYfcw&expires=1706590800",
                        "bunny_prerecorded_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-short/v-short-1014/v-short-1014_Presentation.vtt?token=zvyTdCtx8_5hyH2bk_JX2oDFq5-btQn_7foGQihD2II&expires=1706590800"
                    },
                    {
                        "slot_id": "v-short-1030",
                        "session_id": "short1",
                        "title": "Compact Phase Histograms for Guided Exploration of Periodicity",
                        "contributors": [
                            "Max Franke"
                        ],
                        "authors": [
                            "Max Franke",
                            "Steffen Koch"
                        ],
                        "abstract": "Periodically occurring accumulations of events or measured values are present in many time-dependent datasets and can be of interest for analyses. The frequency of such periodic behavior is often not known in advance, making it difficult to detect and tedious to explore. Automated analysis methods exist, but can be too costly for smooth, interactive analysis. We propose a compact visual representation that reveals periodicity by showing a phase histogram for a given period length that can be used standalone or in combination with other linked visualizations. Our approach supports guided, interactive analyses by suggesting other period lengths to explore, which are ranked based on two quality measures. We further describe how the phase can be mapped to visual representations in other views to reveal periodicity there.",
                        "uid": "v-short-1030",
                        "time_stamp": "2023-10-26T03:36:00Z",
                        "time_start": "2023-10-26T03:36:00Z",
                        "time_end": "2023-10-26T03:45:00Z",
                        "paper_type": "short",
                        "keywords": [
                            "Human-centered computing\u2014Visualization\u2014Visualization techniques"
                        ],
                        "doi": "",
                        "fno": "",
                        "has_image": true,
                        "has_pdf": true,
                        "paper_award": "",
                        "image_caption": "Our compact widget visualizes a histogram of the phases of event data for a specific period length, which can be interactively adjusted. If the period length matches a periodicity contained in the data, this shows up as a non-uniform distribution in the histogram. In a matrix visualization, the phase histograms for slightly longer and shorted period lengths are also shown row-wise.",
                        "external_paper_link": "",
                        "youtube_ff_link": "https://youtu.be/7SWeAKjktc8",
                        "youtube_ff_id": "7SWeAKjktc8",
                        "bunny_ff_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-short/v-short-1030/v-short-1030_Preview.mp4?token=r350wEHDKajFdxWhZMHngMqo2egK4F7SSQSCFmLcpjc&expires=1706590800",
                        "bunny_ff_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-short/v-short-1030/v-short-1030_Preview.vtt?token=f976nJEoil6FbHpPTrDgevOAEXS_eqFSnB8Z-T3EmI8&expires=1706590800",
                        "youtube_prerecorded_link": "https://youtu.be/p76hCmHqbfw",
                        "youtube_prerecorded_id": "p76hCmHqbfw",
                        "bunny_prerecorded_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-short/v-short-1030/v-short-1030_Presentation.mp4?token=2w1lOIpCLmUIkD4fbSnxxacNRdh8UbBkJrXBx6ucKk0&expires=1706590800",
                        "bunny_prerecorded_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-short/v-short-1030/v-short-1030_Presentation.vtt?token=xoYQkq8Iz4GJOlwGWUuaLJMo8jGHCOx8NS-JdEm0HEA&expires=1706590800"
                    },
                    {
                        "slot_id": "v-short-1036",
                        "session_id": "short1",
                        "title": "ZADU: A Python Library for Evaluating the Reliability of Dimensionality Reduction Embeddings",
                        "contributors": [
                            "Aeri Cho"
                        ],
                        "authors": [
                            "Hyeon Jeon",
                            "Aeri Cho",
                            "Jinhwa Jang",
                            "Soohyun Lee",
                            "Jake Hyun",
                            "Hyung-Kwon Ko",
                            "Jaemin Jo",
                            "Jinwook Seo"
                        ],
                        "abstract": "Dimensionality reduction (DR) techniques inherently distort the original structure of input high-dimensional data, producing imperfect low-dimensional embeddings. Diverse distortion measures have thus been proposed to evaluate the reliability of DR embeddings. However, implementing and executing distortion measures in practice has so far been time-consuming and tedious. To address this issue, we present ZADU, a Python library that provides distortion measures. ZADU is not only easy to install and execute but also enables comprehensive evaluation of DR embeddings through three key features. First, the library covers a wide range of distortion measures. Second, it automatically optimizes the execution of distortion measures, substantially reducing the running time required to execute multiple measures. Last, the library informs how individual points contribute to the overall distortions, facilitating the detailed analysis of DR embeddings. By simulating a real-world scenario of optimizing DR embeddings, we verify that our optimization scheme substantially reduces the time required to execute distortion measures. Finally, as an application of ZADU, we present another library called ZADUVis that allows users to easily create distortion visualizations that depict the extent to which each region of an embedding suffers from distortions.",
                        "uid": "v-short-1036",
                        "time_stamp": "2023-10-26T03:45:00Z",
                        "time_start": "2023-10-26T03:45:00Z",
                        "time_end": "2023-10-26T03:54:00Z",
                        "paper_type": "short",
                        "keywords": [
                            "Dimensionality reduction, Reliability, Visualization Library, Distortion measures, Distortion visualizations"
                        ],
                        "doi": "",
                        "fno": "",
                        "has_image": true,
                        "has_pdf": true,
                        "paper_award": "",
                        "image_caption": "The UMAP embedding of the MNIST dataset (leftmost column), and two distortion visualizations generated by ZADUVis: CheckViz and the Reliability Map.",
                        "external_paper_link": "",
                        "youtube_ff_link": "https://youtu.be/p7t4tcFMGkc",
                        "youtube_ff_id": "p7t4tcFMGkc",
                        "bunny_ff_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-short/v-short-1036/v-short-1036_Preview.mp4?token=-Q_9nuDE1qnXJDv5cI9Z0mVudJR0dQh2nGJJvfJ4Xb8&expires=1706590800",
                        "bunny_ff_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-short/v-short-1036/v-short-1036_Preview.vtt?token=dP2FaU4Vgxol9QXKTiEBFOAT_q-U3NOkCCOFKNr001Q&expires=1706590800",
                        "youtube_prerecorded_link": "https://youtu.be/c2JvnAg1ZiQ",
                        "youtube_prerecorded_id": "c2JvnAg1ZiQ",
                        "bunny_prerecorded_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-short/v-short-1036/v-short-1036_Presentation.mp4?token=T4Y02k3R_NkTZhPXweboG9sIeo4P_WHDrcRD0CI6dJg&expires=1706590800",
                        "bunny_prerecorded_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-short/v-short-1036/v-short-1036_Presentation.vtt?token=FkgvvQqPEdXsfjGD0gLvy-mMInTOOo2OD8BZrx9e4ww&expires=1706590800"
                    },
                    {
                        "slot_id": "v-short-1149",
                        "session_id": "short1",
                        "title": "TimePool: Visually Answer \"Which and When\" Questions On Univariate Time Series",
                        "contributors": [
                            "Yueqi Hu"
                        ],
                        "authors": [
                            "Tinghao Feng",
                            "Yueqi Hu",
                            "Jing Yang",
                            "Tom Polk",
                            "Ye Zhao",
                            "Shixia Liu",
                            "Zhaocong Yang"
                        ],
                        "abstract": "When exploring time series datasets, analysts often pose \"which and when\" questions. For example, with world life expectancy data over one hundred years, they may inquire about the top 10 countries in life expectancy and the time period when they achieved this status, or which countries have had longer life expectancy than Ireland and when. This paper proposes TimePool, a new visualization prototype, to address this need for univariate time series analysis. It allows users to construct interactive \"which and when\" queries and visually explore the results for insights.",
                        "uid": "v-short-1149",
                        "time_stamp": "2023-10-26T03:54:00Z",
                        "time_start": "2023-10-26T03:54:00Z",
                        "time_end": "2023-10-26T04:03:00Z",
                        "paper_type": "short",
                        "keywords": [
                            "time series data, interaction, dynamic queries"
                        ],
                        "doi": "",
                        "fno": "",
                        "has_image": true,
                        "has_pdf": true,
                        "paper_award": "",
                        "image_caption": "When exploring time series datasets, analysts often pose \u201cwhich and when\u201d questions. For example, with world life expectancy data over one hundred years, they may inquire about the top 10 countries in life expectancy and the time period when they achieved this status, or which countries have had longer life expectancy than Ireland and when. This paper proposes TimePool, a new visualization prototype, to address this need for univariate time series analysis. It allows users to construct interactive \u201cwhich and when\u201d queries and visually explore the results for insights.",
                        "external_paper_link": "",
                        "youtube_ff_link": "https://youtu.be/Eb6NMjePqHc",
                        "youtube_ff_id": "Eb6NMjePqHc",
                        "bunny_ff_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-short/v-short-1149/v-short-1149_Preview.mp4?token=TEZBEqFo6KVk8EeMaJ9SOhl6FYdS3P1_kysJxhTq2eM&expires=1706590800",
                        "bunny_ff_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-short/v-short-1149/v-short-1149_Preview.vtt?token=hh_2EDoGOWlgKru533TuAh4MbIL-A8UGBp3Mt3H93KA&expires=1706590800",
                        "youtube_prerecorded_link": "https://youtu.be/9J4dXA2a86A",
                        "youtube_prerecorded_id": "9J4dXA2a86A",
                        "bunny_prerecorded_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-short/v-short-1149/v-short-1149_Presentation.mp4?token=igqoTJ1qMLAXB9h8Rd5C4xa_eStipuUPCaxyZcjKgXg&expires=1706590800",
                        "bunny_prerecorded_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-short/v-short-1149/v-short-1149_Presentation.vtt?token=dVJzr5Ew9-5Aunbln_aSysq-L4M5cqDcZDVm9x5-TYs&expires=1706590800"
                    },
                    {
                        "slot_id": "v-short-1159",
                        "session_id": "short1",
                        "title": "Two Heads are Better than One: Pair-Interviews for Visualization",
                        "contributors": [
                            "Derya Akbaba"
                        ],
                        "authors": [
                            "derya akbaba",
                            "Miriah Meyer"
                        ],
                        "abstract": "Visualization research methods help us study how visualization systems are used in complex real-world scenarios. One such widely used method is the interview --- researchers asking participants specific questions to enrich their understanding. In this work, we introduce the pair-interview technique as a method that relies on two interviewers with specific and delineated roles, instead of one. Pair-interviewing focuses on the mechanics of conducting semi-structured interviews as a pair, and complements other existing visualization interview techniques. Based on a synthesis of the experiences and reflections of researchers in four diverse studies who used pair-interviewing, we outline recommendations for when and how to use pair-interviewing within visualization research studies.",
                        "uid": "v-short-1159",
                        "time_stamp": "2023-10-26T04:03:00Z",
                        "time_start": "2023-10-26T04:03:00Z",
                        "time_end": "2023-10-26T04:12:00Z",
                        "paper_type": "short",
                        "keywords": [
                            "Human-centered computing\u2014Visualization\u2014Empirical studies in visualization"
                        ],
                        "doi": "",
                        "fno": "",
                        "has_image": true,
                        "has_pdf": true,
                        "paper_award": "",
                        "image_caption": "This technique is not just for dinosaurs! We introduce the pair-interview technique for visualization researchers. During the talk we review the technique, the roles of the two interviewers, and the benefits we experienced across four separate research teams.",
                        "external_paper_link": "",
                        "youtube_ff_link": "https://youtu.be/Odi1U_gqb1A",
                        "youtube_ff_id": "Odi1U_gqb1A",
                        "bunny_ff_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-short/v-short-1159/v-short-1159_Preview.mp4?token=L4p9sp27R3ID--kDzF7TICfi8tCQNxIF8iBgA_HuHr4&expires=1706590800",
                        "bunny_ff_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-short/v-short-1159/v-short-1159_Preview.vtt?token=qTgS1nB2wgacR80mq46J3dEcrdSFRvbqW8eHMvUrjdg&expires=1706590800",
                        "youtube_prerecorded_link": "https://youtu.be/b5vW-BQZGlU",
                        "youtube_prerecorded_id": "b5vW-BQZGlU",
                        "bunny_prerecorded_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-short/v-short-1159/v-short-1159_Presentation.mp4?token=nsrNDLaW0FXH36j4E7dIpbf9_G63l0e_acLFT7g1U28&expires=1706590800",
                        "bunny_prerecorded_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-short/v-short-1159/v-short-1159_Presentation.vtt?token=ppql2HudkUKIi44S_FgpgAKuwkHV-4AfIo0h4eSuNIU&expires=1706590800"
                    }
                ]
            },
            {
                "title": "Applications / Design",
                "session_id": "short2",
                "event_prefix": "v-short",
                "track": "oneohfour",
                "session_image": "short2.png",
                "chair": [
                    "Panagiotis D. Ritsos"
                ],
                "time_start": "2023-10-25T23:45:00Z",
                "time_end": "2023-10-26T01:00:00Z",
                "discord_category": "",
                "discord_channel": "104",
                "discord_channel_id": "1161741240665124954",
                "discord_link": "https://discordapp.com/channels/1161400451619627068/1161741240665124954",
                "zoom_private_meeting": "",
                "zoom_private_password": "",
                "zoom_private_link": "",
                "zoom_broadcast_link": "",
                "ff_link": "https://youtu.be/db36rhSU4RQ",
                "time_slots": [
                    {
                        "slot_id": "v-short-1140",
                        "session_id": "short2",
                        "title": "Taken By Surprise? Evaluating how Bayesian Surprise & Suppression Influences Peoples' Takeaways in Map Visualizations",
                        "contributors": [
                            "Akim Ndlovu"
                        ],
                        "authors": [
                            "Akim Ndlovu",
                            "Hilson Shrestha",
                            "Lane Harrison"
                        ],
                        "abstract": "Choropleth maps have been studied and extended in many ways to counteract the many biases that can occur when using them. Two recent techniques, Surprise metrics and Value Suppressing Uncertainty Palettes (VSUPs), offer promising solutions but have yet to be tested empirically with users of visualizations. In this paper, we explore how well people can make use of these techniques in map exploration tasks. We report a crowdsourced experiment where n = 300 participants are assigned to one of Choropleth, Surprise (only), and VSUP conditions (depicting rates and Surprise in a suppressed palette). Results show clear differences in map analysis outcomes, e.g. with Surprise maps leading people to significantly higher areas of population, or VSUPs performing similar or better than Choropleths for rate selection. Qualitative analysis suggests that many participants may only consider a subset of the metrics presented to them during exploration and decision-making. We discuss how these results generally support the use of Surprise and VSUP techniques in practice, and opportunities for further technique development. The material for the study (data, study results and code) is publicly available on https://osf.io/exb95/.",
                        "uid": "v-short-1140",
                        "time_stamp": "2023-10-25T23:45:00Z",
                        "time_start": "2023-10-25T23:45:00Z",
                        "time_end": "2023-10-25T23:54:00Z",
                        "paper_type": "short",
                        "keywords": [
                            "Data Visualization\u2014Choropleth Maps\u2014Bayesian Surprise"
                        ],
                        "doi": "",
                        "fno": "",
                        "has_image": true,
                        "has_pdf": true,
                        "paper_award": "",
                        "image_caption": "How do Bayesian surprise metrics and suppression encodings influence peoples\u2019 takeaways in map visualizations? We conduct two experiments with Covid-19 and Poverty datasets, randomly assigning 300 participants to three map conditions. We collect data across three map takeaway tasks T1-Best, T1-Worst: Identify and T2: Explore. To mitigate biases for particular dataset  contexts discovered in pilot studies (e.g. vaccine skepticism) we reframe both datasets as a sales and marketing task. Metrics include participants\u2019 exploration metadata (quantitative) and takeaway comments (qualitative)",
                        "external_paper_link": "",
                        "youtube_ff_link": "https://youtu.be/m3E1KxV6v8E",
                        "youtube_ff_id": "m3E1KxV6v8E",
                        "bunny_ff_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-short/v-short-1140/v-short-1140_Preview.mp4?token=I-LbvGAoskHOkhR73EOma-_5ONcDn0Onkh7EIDMcLho&expires=1706590800",
                        "bunny_ff_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-short/v-short-1140/v-short-1140_Preview.vtt?token=7aqSLDQS0yY6Z2ZCoRK-Q5sCazHGHqB_e33avsUbfPs&expires=1706590800",
                        "youtube_prerecorded_link": "https://youtu.be/IQWkkuxymkQ",
                        "youtube_prerecorded_id": "IQWkkuxymkQ",
                        "bunny_prerecorded_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-short/v-short-1140/v-short-1140_Presentation.mp4?token=t7SBYDBLkTvsnC3CivcTu_4M-o-pIALaoLjjXMHGth4&expires=1706590800",
                        "bunny_prerecorded_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-short/v-short-1140/v-short-1140_Presentation.vtt?token=DYCshipEiHGQbeQEdutNio26IPCsjz5Y432kxh3-5tg&expires=1706590800"
                    },
                    {
                        "slot_id": "v-short-1071",
                        "session_id": "short2",
                        "title": "Towards Autocomplete Strategies for Visualization Construction",
                        "contributors": [
                            "Wei Wei"
                        ],
                        "authors": [
                            "Wei Wei",
                            "Samuel Huron",
                            "Yvonne Jansen"
                        ],
                        "abstract": "Constructive visualization uses physical data units - tokens -  to enable non-experts to create personalized visualizations engagingly. However, its physical nature limits efficiency and scalability. One potential solution to address this issue is autocomplete. By providing automated suggestions while still allowing for manual intervention, autocomplete can expedite visualization construction while maintaining expressivity. We conduct a speculative design study to examine how people would like to interact with a visualization authoring system that supports autocomplete. Our study identifies three types of autocomplete strategies and gains insights for designing future visualization authoring tools with autocomplete functionality. A free copy of this paper and all supplemental materials are available on our online repository: https://osf.io/nu4z3/?view_only=594baee54d114a99ab381886fb32a126.",
                        "uid": "v-short-1071",
                        "time_stamp": "2023-10-25T23:54:00Z",
                        "time_start": "2023-10-25T23:54:00Z",
                        "time_end": "2023-10-26T00:03:00Z",
                        "paper_type": "short",
                        "keywords": [
                            "Autocomplete, constructive visualization, visualization authoring, physicalization, automation, expressivity, design."
                        ],
                        "doi": "",
                        "fno": "",
                        "has_image": true,
                        "has_pdf": true,
                        "paper_award": "",
                        "image_caption": "The three visualization autocomplete strategies identified in our study. The dashed boxes in different colors represent autocomplete suggestions. Based on the token(s) that has been placed, NEXT-STEP provides suggestions for a single next visual mapping operation. GHOST provides situated partial or completed visualization recommendations. GALLERY provides a gallery of completed visual mapping options.",
                        "external_paper_link": "",
                        "youtube_ff_link": "https://youtu.be/UoZmbFoLeMA",
                        "youtube_ff_id": "UoZmbFoLeMA",
                        "bunny_ff_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-short/v-short-1071/v-short-1071_Preview.mp4?token=759mhXLeK5ffhl3CNyWYwqlfnSt9E5hbA1BhgDstqJc&expires=1706590800",
                        "bunny_ff_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-short/v-short-1071/v-short-1071_Preview.vtt?token=sILy7CTgNfaACSrJJ2YjY40LIq4uYgG_gewECMgKolk&expires=1706590800",
                        "youtube_prerecorded_link": "https://youtu.be/cxwSHXvzSSQ",
                        "youtube_prerecorded_id": "cxwSHXvzSSQ",
                        "bunny_prerecorded_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-short/v-short-1071/v-short-1071_Presentation.mp4?token=8yxib36taHZP0sZfPFEiNbnX9o1LywRINGRMBMNl4QE&expires=1706590800",
                        "bunny_prerecorded_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-short/v-short-1071/v-short-1071_Presentation.vtt?token=enxP4qcAXuP_U4e-_5F8_9fu_mrPbtkMZ3f8x5Ko-70&expires=1706590800"
                    },
                    {
                        "slot_id": "v-short-1116",
                        "session_id": "short2",
                        "title": "Indy Survey Tool: A Framework to Unearth Correlations in Survey Data",
                        "contributors": [
                            "Sara Di Bartolomeo"
                        ],
                        "authors": [
                            "Tarik Crnovrsanin",
                            "Sara Di Bartolomeo",
                            "Connor Wilson",
                            "Cody Dunne"
                        ],
                        "abstract": "Survey companion websites allow users to explore collected survey information more deeply, as well as update or add entries for papers. These sites can help information stay relevant past the original release date of the survey paper. However, creating and maintaining a website can be laborious and difficult, especially when authors might not be experienced with programming. We introduce Indy Survey Tool to help authors develop companion websites for survey papers across diverse fields of study. The tool\u2019s core aim is to identify correlations between categorizations of papers. To accomplish this, the tool offers multiple combined filters and correlation matrix visualizations that enable users to explore the data from diverse perspectives. The tool\u2019s visualizations, list of papers, and filters are harmoniously integrated and highly responsive, providing users with feedback based on their selections. Identifying correlations in survey papers is a pivotal aspect of research, as it can enable the recognition of common combinations of categorizations within the papers\u2014as well as highlight any omissions. The versatility of Indy Survey Tool enables researchers to delve into the correlations between categorizations in survey data, an essential aspect of research that can reveal gaps in the literature and highlight promising areas for future exploration. A preprint and supplemental material for the paper can be found at osf.io/tdhqn.",
                        "uid": "v-short-1116",
                        "time_stamp": "2023-10-26T00:03:00Z",
                        "time_start": "2023-10-26T00:03:00Z",
                        "time_end": "2023-10-26T00:12:00Z",
                        "paper_type": "short",
                        "keywords": [
                            "Software tool, systematic review, literature review, survey, co-occurrence visualization."
                        ],
                        "doi": "",
                        "fno": "",
                        "has_image": true,
                        "has_pdf": true,
                        "paper_award": "",
                        "image_caption": "An example of how the Indy Survey tool we present was used in recent survey on Immersive Analytics. The left panel lets users filter using a search bar, timeline, and topic selector. The top bar  provides information about the survey and how to add new entries. The center  shows a short summary of each included paper. The collapsible visualization panel on the right shows a correlation matrix for two selected dimensions. Interacting with the left and right panels filters the papers displayed in the center.  Upon selection of a paper, a detail view pops up with all of its information (not shown).",
                        "external_paper_link": "",
                        "youtube_ff_link": "https://youtu.be/U4g-8RTcFHs",
                        "youtube_ff_id": "U4g-8RTcFHs",
                        "bunny_ff_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-short/v-short-1116/v-short-1116_Preview.mp4?token=uEqs9vrIhDv3lSk_z9WYt7a8cbdTGnc7WRrdclCCxr4&expires=1706590800",
                        "bunny_ff_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-short/v-short-1116/v-short-1116_Preview.vtt?token=b9RdbsN427fM1StOlWn-rpbebDiGfQtUNuR8cHfOdtY&expires=1706590800",
                        "youtube_prerecorded_link": "https://youtu.be/c3fY5hZDlGM",
                        "youtube_prerecorded_id": "c3fY5hZDlGM",
                        "bunny_prerecorded_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-short/v-short-1116/v-short-1116_Presentation.mp4?token=y04gm0KJCWxD-fPchQDIaGxczrrJmf2XCcCGUtTd1nk&expires=1706590800",
                        "bunny_prerecorded_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-short/v-short-1116/v-short-1116_Presentation.vtt?token=8JJ3fno5E1g3oxwNjxwwiXgeszi_4fSBzuGzajaDxwc&expires=1706590800"
                    },
                    {
                        "slot_id": "v-short-1195",
                        "session_id": "short2",
                        "title": "Data in the Wind: Evaluating Multiple-Encoding Design for Particle Motion Visualizations",
                        "contributors": [
                            "Yiren Ding"
                        ],
                        "authors": [
                            "Yiren Ding",
                            "Lane Harrison"
                        ],
                        "abstract": "Motion is widely used in modern data visualizations, serving as a means for transitioning views and as a primary channel for conveying information. Particle flow maps have become a popular means for communicating the speed and direction of wind in engaging and informative ways. Yet there is little empirical design guidance supporting the multiple encodings these maps use, such as particle speed, particle density, and color saturation. In this paper, we investigate multiple encoding wind maps using a staircase methodology to estimate just-noticeable differences for a range of speed values across visualizations with or without motion encodings. Results suggest: 1. the multiple encodings designers use are not only aesthetically engaging\u2013 they also improve speed discriminability for the average participant. 2. The speed of particle motion should be controlled under a certain range for good information retrieval accuracy. These findings contribute empirical guidance for particle motion encoding design, and lay groundwork for future investigations as motion becomes more widely used in visualization practice.",
                        "uid": "v-short-1195",
                        "time_stamp": "2023-10-26T00:12:00Z",
                        "time_start": "2023-10-26T00:12:00Z",
                        "time_end": "2023-10-26T00:21:00Z",
                        "paper_type": "short",
                        "keywords": [
                            "Human-centered computing Visualization Visualization techniques Empirical study"
                        ],
                        "doi": "",
                        "fno": "",
                        "has_image": true,
                        "has_pdf": true,
                        "paper_award": "honorable",
                        "image_caption": "This study explores the use of motion in data visualizations, particularly in particle flow maps for representing wind speed and direction. This research employs a staircase methodology to model just-noticeable differences in perception in both motion-only and motion + static conditions. The results indicate that using motion encoding, as seen in popular wind flow maps, enhance not only visual appeal but also improves speed discrimination for the average viewer. These findings contribute empirical guidance for particle motion encoding design, and lay groundwork for future investigations as such motion encodings continue to be used in visualization practice.",
                        "external_paper_link": "",
                        "youtube_ff_link": "https://youtu.be/tyvIZNz3RXA",
                        "youtube_ff_id": "tyvIZNz3RXA",
                        "bunny_ff_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-short/v-short-1195/v-short-1195_Preview.mp4?token=KH8tNiOGUQmHYkrLs-iwvoNC3hABWrxFvB-dMacDus8&expires=1706590800",
                        "bunny_ff_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-short/v-short-1195/v-short-1195_Preview.vtt?token=nvdyVFaCKoYSWU86cXCpqsTeDF7Qc1agIHOJh-hoPMA&expires=1706590800",
                        "youtube_prerecorded_link": "https://youtu.be/iwFRiGB6EzY",
                        "youtube_prerecorded_id": "iwFRiGB6EzY",
                        "bunny_prerecorded_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-short/v-short-1195/v-short-1195_Presentation.mp4?token=Ktf4o8K8H4KLYqeaQ9tLuud4uynY85jSMFrRdcZosTE&expires=1706590800",
                        "bunny_prerecorded_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-short/v-short-1195/v-short-1195_Presentation.vtt?token=gbCddZrHGGHiRWXsJrTChjxyFtUI1V9uUitEEZb4oQY&expires=1706590800"
                    },
                    {
                        "slot_id": "v-short-1002",
                        "session_id": "short2",
                        "title": "Show me my Users: A Dashboard Design Visualizing User Interaction Logs with Interactive Visualization",
                        "contributors": [
                            "Mashael AlKadi"
                        ],
                        "authors": [
                            "Jinrui Wang",
                            "Mashael AlKadi",
                            "Benjamin Bach"
                        ],
                        "abstract": "This paper describes the design of a dashboard and analysis pipeline to monitor users of visualization tools in the wild. Our pipeline describes how to extract analysis KPIs from extensive log event data and a mix of user types. The resulting three-page dashboard displays live KPIs, helping analysts to understand users, detect exploratory behaviors, plan education interventions, and improve tool features. We propose this case study as a motivation to use the dashboard approach for a more `casual' monitoring of users and building carer mindsets for visualization tools.",
                        "uid": "v-short-1002",
                        "time_stamp": "2023-10-26T00:21:00Z",
                        "time_start": "2023-10-26T00:21:00Z",
                        "time_end": "2023-10-26T00:30:00Z",
                        "paper_type": "short",
                        "keywords": [
                            "Human-centered computing\u2014Visualization\u2014Visualization design and evaluation methods; Human-centered computing\u2014Visualization\u2014Visualization application domains\u2014Visual Analytics"
                        ],
                        "doi": "",
                        "fno": "",
                        "has_image": true,
                        "has_pdf": true,
                        "paper_award": "",
                        "image_caption": "(a) Dashboard Design Pipeline of Visualization Tool Logs: this diagram represents the process needed to create analytical dashboard to understand users\ufffd needs in visual exploration of networks. (b) Overview Page: this figure shows the first page of the dashboard\ufffds 3 pages (overview, visualizations, user) that provides analytics about the usage of the tool, the visualizations and the user exploration respectively.",
                        "external_paper_link": "",
                        "youtube_ff_link": "https://youtu.be/HsoRmS32Xm8",
                        "youtube_ff_id": "HsoRmS32Xm8",
                        "bunny_ff_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-short/v-short-1002/v-short-1002_Preview.mp4?token=W1QUcKc-4_ox-f_me7E-78yDiKNtwbS-pEPnD_V2cYM&expires=1706590800",
                        "bunny_ff_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-short/v-short-1002/v-short-1002_Preview.vtt?token=SNWnH09-M4tFg-3HREHp5Hr9wJMh3Jt2rq8i8sRk9t4&expires=1706590800",
                        "youtube_prerecorded_link": "https://youtu.be/cpDNKpGj9c4",
                        "youtube_prerecorded_id": "cpDNKpGj9c4",
                        "bunny_prerecorded_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-short/v-short-1002/v-short-1002_Presentation.mp4?token=MDiNlEHvshgB2IW955Kg39ea2GkdT6KUIy88D3RQ-hE&expires=1706590800",
                        "bunny_prerecorded_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-short/v-short-1002/v-short-1002_Presentation.vtt?token=8KUT0MnaDDkoRV24m-P_JQ7TQdXZTIi40mPcQH5BYoE&expires=1706590800"
                    },
                    {
                        "slot_id": "v-short-1010",
                        "session_id": "short2",
                        "title": "What Is the Difference Between a Mountain and a Molehill? Quantifying Semantic Labeling of Visual Features in Line Charts",
                        "contributors": [
                            "Dennis Bromley"
                        ],
                        "authors": [
                            "Dennis Bromley",
                            "Vidya Setlur"
                        ],
                        "abstract": "Relevant language describing visual features in charts can be useful for authoring captions and summaries about the charts to help with readers' takeaways. To better understand the interplay between concepts that describe visual features and the semantic relationships among those concepts (e.g., 'sharp increase' vs. 'gradual rise'), we conducted a crowdsourced study to collect labels and visual feature pairs for univariate line charts. Using this crowdsourced dataset of labeled visual signatures, this paper proposes a novel method for labeling visual chart features based on combining feature-word distributions with the visual features and the data domain of the charts. These feature-word-topic models identify word associations with similar yet subtle differences in semantics, such as 'flat,' 'plateau,' and 'stagnant,' and descriptors of the visual features, such as 'sharp increase,' 'slow climb,' and 'peak.' Our feature-word-topic model is computed using both a quantified semantics approach and a signal processing-inspired least-errors shape-similarity approach. We finally demonstrate the application of this dataset for annotating charts and generating textual data summaries.",
                        "uid": "v-short-1010",
                        "time_stamp": "2023-10-26T00:30:00Z",
                        "time_start": "2023-10-26T00:30:00Z",
                        "time_end": "2023-10-26T00:39:00Z",
                        "paper_type": "short",
                        "keywords": [
                            "Semantics, trends, annotation, text generation."
                        ],
                        "doi": "",
                        "fno": "",
                        "has_image": true,
                        "has_pdf": true,
                        "paper_award": "",
                        "image_caption": "The figure shows the average segment slope of the trend annotations. The maximum possible slope range available for the charts is -3 to +3 allowing us to empirically derive various inter-word relationships.",
                        "external_paper_link": "",
                        "youtube_ff_link": "https://youtu.be/lCfwJMuPyLg",
                        "youtube_ff_id": "lCfwJMuPyLg",
                        "bunny_ff_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-short/v-short-1010/v-short-1010_Preview.mp4?token=ZsSED6MFkWq6fWkiCjoBNL5VFt6GPunIHi6i6Vq08PA&expires=1706590800",
                        "bunny_ff_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-short/v-short-1010/v-short-1010_Preview.vtt?token=ZT8OKZmUgzfzDAVVQoELZ8XPgQEBbGv3Gj8wgtqUqmc&expires=1706590800",
                        "youtube_prerecorded_link": "https://youtu.be/w4ylo5_zOec",
                        "youtube_prerecorded_id": "w4ylo5_zOec",
                        "bunny_prerecorded_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-short/v-short-1010/v-short-1010_Presentation.mp4?token=2TSVStdGd9oNPheCuWmAuxrBwp-b8XJGyoophnaSEfg&expires=1706590800",
                        "bunny_prerecorded_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-short/v-short-1010/v-short-1010_Presentation.vtt?token=Gl09eA9oqBStxSmsAJPrjUB2zoEYaUPR4U23CsI9MNY&expires=1706590800"
                    },
                    {
                        "slot_id": "v-short-1018",
                        "session_id": "short2",
                        "title": "Draco 2: An Extensible Platform to Model Visualization Design",
                        "contributors": [
                            "P\u00e9ter Ferenc Gyarmati"
                        ],
                        "authors": [
                            "Junran Yang",
                            "P\u00e9ter Ferenc Gyarmati",
                            "Zehua Zeng",
                            "Dominik Moritz"
                        ],
                        "abstract": "Draco introduced a constraint-based framework to model visualization design in an extensible and testable form. It provides a way to abstract design guidelines from theoretical and empirical studies and applies the knowledge in automated design tools. However, Draco is challenging to use because there is limited tooling and documentation. In response, we present Draco 2, the successor with (1) a more flexible visualization specification format, (2) a comprehensive test suite and documentation, and (3) flexible and convenient APIs. We designed Draco 2 to be more extensible and easier to integrate into visualization systems. We demonstrate these advantages and believe that they make Draco 2 a platform for future research.",
                        "uid": "v-short-1018",
                        "time_stamp": "2023-10-26T00:39:00Z",
                        "time_start": "2023-10-26T00:39:00Z",
                        "time_end": "2023-10-26T00:48:00Z",
                        "paper_type": "short",
                        "keywords": [
                            "Visualization systems and tools"
                        ],
                        "doi": "",
                        "fno": "",
                        "has_image": true,
                        "has_pdf": true,
                        "paper_award": "honorable",
                        "image_caption": "Draco introduces a constraint-based framework to model visualization design guidelines in automated design tools. We present Draco 2 with an improved specification format, a comprehensive test suite, thorough documentation, and convenient APIs. Designed to be more extensible and easier to integrate into visualization systems, we demonstrate its distinct advantages. We believe these enhancements position Draco 2 as a platform for future research.",
                        "external_paper_link": "",
                        "youtube_ff_link": "https://youtu.be/jvydtuwnaQI",
                        "youtube_ff_id": "jvydtuwnaQI",
                        "bunny_ff_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-short/v-short-1018/v-short-1018_Preview.mp4?token=hsC7YwxwST0syoJUb8MX6kCSJfhWjxdVffm5go5rYpQ&expires=1706590800",
                        "bunny_ff_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-short/v-short-1018/v-short-1018_Preview.vtt?token=qnViK1eL2v88jxy845UpVm5PjcxdrMgDEjRjlbnLqEo&expires=1706590800",
                        "youtube_prerecorded_link": "https://youtu.be/RLVSbzd5C_c",
                        "youtube_prerecorded_id": "RLVSbzd5C_c",
                        "bunny_prerecorded_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-short/v-short-1018/v-short-1018_Presentation.mp4?token=W-FLEBBh-D2fD_sxZzjDsnCktp76LVli5Cb9Bkjilwc&expires=1706590800",
                        "bunny_prerecorded_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-short/v-short-1018/v-short-1018_Presentation.vtt?token=NQIPPpiS-ANO5Ex_37X9Hypi0L-qD2XomMEtwiZTbPE&expires=1706590800"
                    }
                ]
            },
            {
                "title": "Perception / Evaluation",
                "session_id": "short3",
                "event_prefix": "v-short",
                "track": "oneohfour",
                "session_image": "short3.png",
                "chair": [
                    "Lace M. Padilla"
                ],
                "time_start": "2023-10-24T22:00:00Z",
                "time_end": "2023-10-24T23:15:00Z",
                "discord_category": "",
                "discord_channel": "104",
                "discord_channel_id": "1161741240665124954",
                "discord_link": "https://discordapp.com/channels/1161400451619627068/1161741240665124954",
                "zoom_private_meeting": "",
                "zoom_private_password": "",
                "zoom_private_link": "",
                "zoom_broadcast_link": "",
                "ff_link": "https://youtu.be/6BQ_Y9bAzl0",
                "time_slots": [
                    {
                        "slot_id": "v-short-1142",
                        "session_id": "short3",
                        "title": "Topological Analysis and Approximate Identification of Leading Lines in Artworks Based on Discrete Morse Theory",
                        "contributors": [
                            "Fuminori Shibasaki"
                        ],
                        "authors": [
                            "Fuminori Shibasaki",
                            "Issei Fujishiro"
                        ],
                        "abstract": "Accomplished artists often incorporate leading lines into their compositions to guide the observer's attention. Although saliency maps are typically employed to locate attractive regions in still images, such scalar features do not express the trajectory of an observer's gaze. In this study, we propose a method for the visual analysis and approximate identification of leading lines based on maximum graphs, sparse subsets of Morse--Smale complexes, extracted from saliency maps. We provide empirical evidence substantiating the feasibility of our method through a comparison with actual observers' eye tracking results. Further, we investigate the limitations of our approach by employing it to analyze a variety of artworks with diverse styles.",
                        "uid": "v-short-1142",
                        "time_stamp": "2023-10-24T22:00:00Z",
                        "time_start": "2023-10-24T22:00:00Z",
                        "time_end": "2023-10-24T22:09:00Z",
                        "paper_type": "short",
                        "keywords": [
                            "Feature Detection, Extraction, Tracking & Transformation ; Other Application Areas ; Art & Graphic Design ; Data Analysis, Reasoning, Problem Solving, and Decision Making ; Perception & Cognition ; Personal Visualization, Personal Visual Analytics ; Application Motivated Visualization ; Data Abstractions & Types ; Human-Subjects Qualitative Studies ; Graph/Network and Tree Data ; Image and Video Data ; Scalar Field Data ; Computational Topology-based Techniques ; Saliency Map ; Leading Lines ; Eye Tracking"
                        ],
                        "doi": "",
                        "fno": "",
                        "has_image": true,
                        "has_pdf": true,
                        "paper_award": "",
                        "image_caption": "Still Life With Quince, Cabbage, Melon, and Cucumber (Juan S\u00e1nchez Cot\u00e1n, c. 1602) and its compositional analyses in terms of leading lines.",
                        "external_paper_link": "",
                        "youtube_ff_link": "https://youtu.be/kQXK8xvqLFY",
                        "youtube_ff_id": "kQXK8xvqLFY",
                        "bunny_ff_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-short/v-short-1142/v-short-1142_Preview.mp4?token=3HC0mSFtuOuR6aSoOJ5FEYz0r5z9kNEbILTIOasr0-U&expires=1706590800",
                        "bunny_ff_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-short/v-short-1142/v-short-1142_Preview.vtt?token=SLLXji9GMtB-TrPQ5p3lruhV2WbiDoHCaTdFlL8Klnk&expires=1706590800",
                        "youtube_prerecorded_link": "https://youtu.be/Qk1KccjUdU4",
                        "youtube_prerecorded_id": "Qk1KccjUdU4",
                        "bunny_prerecorded_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-short/v-short-1142/v-short-1142_Presentation.mp4?token=d3J26hW6f4InuzkDSvONU2lbDe26ra5ia3iYM7HW0j8&expires=1706590800",
                        "bunny_prerecorded_subtitles": ""
                    },
                    {
                        "slot_id": "v-short-1162",
                        "session_id": "short3",
                        "title": "Effects of data distribution and granularity on color semantics for colormap data visualizations",
                        "contributors": [
                            "Clementine Zimnicki"
                        ],
                        "authors": [
                            "Clementine Zimnicki",
                            "Chin Tseng",
                            "Danielle Albers Szafir",
                            "Karen Schloss"
                        ],
                        "abstract": "To create effective data visualizations, it helps to represent data using visual features in intuitive ways. When visualization designs match observer expectations, visualizations are easier to interpret. Prior work suggests that several factors influence such expectations. For example, the dark-is-more bias leads observers to infer that darker colors map to larger quantities, and the opaque-is-more bias leads them to infer that regions appearing more opaque (given the background color) map to larger quantities. Previous work suggested that the background color only plays a role if visualizations appear to vary in opacity. The present study challenges this claim. We hypothesized that the background color would modulate inferred mappings for colormaps that should not appear to vary in opacity (by previous measures) if the visualization appeared to have a \u201chole\u201d that revealed the background behind the map (hole hypothesis). We found that spatial aspects of the map contributed to inferred mappings, though the effects were inconsistent with the hole hypothesis. Our work raises new questions about how spatial distributions of data influence color semantics in colormap data visualizations.",
                        "uid": "v-short-1162",
                        "time_stamp": "2023-10-24T22:09:00Z",
                        "time_start": "2023-10-24T22:09:00Z",
                        "time_end": "2023-10-24T22:18:00Z",
                        "paper_type": "short",
                        "keywords": [
                            "Visual reasoning, information visualization, colormap data visualizations, color cognition"
                        ],
                        "doi": "",
                        "fno": "",
                        "has_image": true,
                        "has_pdf": true,
                        "paper_award": "",
                        "image_caption": "An image of 80 square colormap visualizations in 20 columns and 4 rows. They are grouped by the 10 color scales used to generate them; from left to right, ColorBrewer Red and Blue, Gray, Hot, Magma+, Mako+, Viridis, Plasma, Autumn, and Winter. Maps are also grouped by granularity (maps appear either coarse or smooth), background (maps are presented on a black or white background), and shift condition (the colors in the maps are either shifted to create large dark regions, or colors are uniformly distributed throughout the maps).",
                        "external_paper_link": "",
                        "youtube_ff_link": "https://youtu.be/amWMwbmqcTQ",
                        "youtube_ff_id": "amWMwbmqcTQ",
                        "bunny_ff_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-short/v-short-1162/v-short-1162_Preview.mp4?token=H0QyNxmK6rvCUn1c2RQP3YEsGnQWEu2q08J9-Lgmmu8&expires=1706590800",
                        "bunny_ff_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-short/v-short-1162/v-short-1162_Preview.vtt?token=q_i3hYIxa1FH3U55VFurvRBCy34cfBPEDoTLg5s7br0&expires=1706590800",
                        "youtube_prerecorded_link": "https://youtu.be/wpUUVzJfhjg",
                        "youtube_prerecorded_id": "wpUUVzJfhjg",
                        "bunny_prerecorded_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-short/v-short-1162/v-short-1162_Presentation.mp4?token=7tPEODfad7UETvr7sZV5QVUJ_ij221XTeUMBzgKE-GM&expires=1706590800",
                        "bunny_prerecorded_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-short/v-short-1162/v-short-1162_Presentation.vtt?token=gd8c4d75580nH0QejP8kLsju3-e-kT3XVfBlsipUP6g&expires=1706590800"
                    },
                    {
                        "slot_id": "v-short-1190",
                        "session_id": "short3",
                        "title": "Let's Get Vis-ical: Perceptual Accuracy in Visual & Tactile Encodings",
                        "contributors": [
                            "Zhongzheng Xu"
                        ],
                        "authors": [
                            "Zhongzheng Xu",
                            "Emily Wall",
                            "Kristin Williams"
                        ],
                        "abstract": "In this paper, we explore the effectiveness of tactile data encodings using swell paper in comparison to visual encodings displayed with SVGs for data perception tasks. By replicating and adapting Cleveland and McGill's graphical perception study for the tactile modality, we establish a novel tactile encoding hierarchy. In a study with 12 university students, we found that participants perceived visual encodings more accurately when comparing values, judging their ratios with lower cognitive load, and better self-evaluated performance than tactile encodings. However, tactile encodings differed from their visual counterparts in terms of how accurately values could be decoded from them. This suggests that data physicalizations will require different design guidance than that developed for visual encodings. By providing empirical evidence for the perceptual accuracy of tactile encodings, our work contributes to foundational research on forms of data representation that prioritize tactile perception such as tactile graphics.",
                        "uid": "v-short-1190",
                        "time_stamp": "2023-10-24T22:18:00Z",
                        "time_start": "2023-10-24T22:18:00Z",
                        "time_end": "2023-10-24T22:27:00Z",
                        "paper_type": "short",
                        "keywords": [
                            "Human-centered computing\u2014Visualization\u2014Visualization techniques; Human-centered computing\u2014Visualization\u2014Visualization design and evaluation methods"
                        ],
                        "doi": "",
                        "fno": "",
                        "has_image": false,
                        "has_pdf": true,
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "youtube_ff_link": "",
                        "youtube_ff_id": "",
                        "bunny_ff_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-short/v-short-1190/v-short-1190_Preview.mp4?token=BGnkSK3d5TgH53PipwWu9Dpfq5wlm2FEeHF1_9RWAk4&expires=1706590800",
                        "bunny_ff_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-short/v-short-1190/v-short-1190_Preview.vtt?token=HR2ENtHlVvnX1P5-6HMdE5iLK8PFXGNQXbVODLUcHjg&expires=1706590800",
                        "youtube_prerecorded_link": "",
                        "youtube_prerecorded_id": "",
                        "bunny_prerecorded_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-short/v-short-1190/v-short-1190_Presentation.mp4?token=6hkI9cHBan10FIiCSgD2UuREnHwyMdW4V1Zq_ziZbP4&expires=1706590800",
                        "bunny_prerecorded_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-short/v-short-1190/v-short-1190_Presentation.vtt?token=RvdQzyWsBoU3etU_hLeR2p5koDnGgH483NxVdVjU81o&expires=1706590800"
                    },
                    {
                        "slot_id": "v-short-1092",
                        "session_id": "short3",
                        "title": "MinMaxLTTB: Leveraging MinMax-Preselection to Scale LTTB",
                        "contributors": [
                            "Jeroen Van Der Donckt"
                        ],
                        "authors": [
                            "Jeroen Van Der Donckt",
                            "Jonas Van Der Donckt",
                            "Dr. Ir. Michael Rademaker",
                            "Sofie Van Hoecke"
                        ],
                        "abstract": "Visualization plays an important role in the analysis and exploration of time series data. To facilitate efficient visualization of large datasets, downsampling has emerged as a well-established approach. This work concentrates on LTTB (Largest-Triangle-Three-Buckets), a widely adopted downsampling algorithm for time series data point selection. Specifically, we introduce MinMaxLTTB, a two-step algorithm that significantly improves the scalability of LTTB. MinMaxLTTB consists of the following two steps: (i) the MinMax algorithm preselects a certain ratio of minimum and maximum data points, followed by (ii) applying the LTTB algorithm on only these preselected data points, effectively reducing LTTB\u2019s time complexity. The MinMax algorithm is computationally efficient and can be parallelized, enabling efficient data point preselection. Additionally, MinMax demonstrates competitive performance in terms of visual representation, making it also an effective data reduction method. Experimental results demonstrate that MinMaxLTTB outperforms LTTB by more than an order of magnitude in terms of computation time. Furthermore, preselecting a small multiple of the desired output size already yields similar visual representativeness compared to LTTB. In summary, MinMaxLTTB leverages the computational efficiency of MinMax to scale LTTB, without compromising on LTTB its favorable visualization properties. The code and experiments associated with this paper can be found at https://github.com/predict-idlab/MinMaxLTTB.",
                        "uid": "v-short-1092",
                        "time_stamp": "2023-10-24T22:27:00Z",
                        "time_start": "2023-10-24T22:27:00Z",
                        "time_end": "2023-10-24T22:36:00Z",
                        "paper_type": "short",
                        "keywords": [
                            "Time series, Line charts, Downsampling algorithms, MinMax, LTTB, Computational efficiency, Perception, Preselection, Evaluation"
                        ],
                        "doi": "",
                        "fno": "",
                        "has_image": false,
                        "has_pdf": true,
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "youtube_ff_link": "https://youtu.be/1CtPeN5XcK8",
                        "youtube_ff_id": "1CtPeN5XcK8",
                        "bunny_ff_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-short/v-short-1092/v-short-1092_Preview.mp4?token=7YsUVVz4MC5A7V5dR2uh7CENf3aGz9VHyrI1fAMXTpY&expires=1706590800",
                        "bunny_ff_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-short/v-short-1092/v-short-1092_Preview.vtt?token=YONlVZ92L8K84jeBAozA5CsYEo_2PTTTzCiGrpYcnhI&expires=1706590800",
                        "youtube_prerecorded_link": "https://youtu.be/jyJJK4Xl8OE",
                        "youtube_prerecorded_id": "jyJJK4Xl8OE",
                        "bunny_prerecorded_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-short/v-short-1092/v-short-1092_Presentation.mp4?token=45R4tCXENRdFbRsugPRCxmatSpvhU6b05CfN9OkTYE0&expires=1706590800",
                        "bunny_prerecorded_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-short/v-short-1092/v-short-1092_Presentation.vtt?token=j4GhVLKjMBi9RGMvjJwVkACy6ZnAVXRG85VLi6vEYRY&expires=1706590800"
                    },
                    {
                        "slot_id": "v-short-1066",
                        "session_id": "short3",
                        "title": "Do You Trust What You See? Toward A Multidimensional Measure of Trust in Visualization",
                        "contributors": [
                            "Saugat Pandey"
                        ],
                        "authors": [
                            "Saugat Pandey",
                            "Oen G McKinley",
                            "R. Jordan Crouser",
                            "Alvitta Ottley"
                        ],
                        "abstract": "Few concepts are as ubiquitous in computational fields as trust. However, in the case of information visualization, there are several unique and complex challenges, chief among them: defining and measuring trust. In this paper, we investigate the factors that influence trust in visualizations. We draw on the literature to identify five factors likely to affect trust: credibility, clarity, reliability, familiarity, and confidence. We then conduct two studies investigating these factors' relationship with visualization design features. In the first study, participants' credibility, understanding, and reliability ratings depended on the visualization design and its source. In the second study, we find these factors also align with subjective trust rankings. Our findings suggest that these five factors are important considerations for the design of trustworthy visualizations.",
                        "uid": "v-short-1066",
                        "time_stamp": "2023-10-24T22:36:00Z",
                        "time_start": "2023-10-24T22:36:00Z",
                        "time_end": "2023-10-24T22:45:00Z",
                        "paper_type": "short",
                        "keywords": [
                            "Human-centered computing, Trust, Visualization, Visualization design and evaluation methods"
                        ],
                        "doi": "",
                        "fno": "",
                        "has_image": true,
                        "has_pdf": true,
                        "paper_award": "",
                        "image_caption": "An illustrative of our experiments. Participants rated various visualizations on FAMILIARITY, CLARITY, CREDIBILITY, RELIABILITY, and CONFIDENCE, exploring their alignment with visual features and trust ratings.",
                        "external_paper_link": "",
                        "youtube_ff_link": "https://youtu.be/dxDhKdNvgzY",
                        "youtube_ff_id": "dxDhKdNvgzY",
                        "bunny_ff_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-short/v-short-1066/v-short-1066_Preview.mp4?token=i5-7QhaNdIHnr6jwQ116sn-pMUAzg4XaipQ0PRjKz0c&expires=1706590800",
                        "bunny_ff_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-short/v-short-1066/v-short-1066_Preview.vtt?token=ejin06Ejwz-Iyc5fPAOKJv5rxdY1X9_NgUnbLxLJKJ8&expires=1706590800",
                        "youtube_prerecorded_link": "https://youtu.be/bYGKEX-Ligg",
                        "youtube_prerecorded_id": "bYGKEX-Ligg",
                        "bunny_prerecorded_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-short/v-short-1066/v-short-1066_Presentation.mp4?token=hz3a8-gHd-7CgMD0W9pm8rigQAxqh-7xTLwt3VENFMs&expires=1706590800",
                        "bunny_prerecorded_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-short/v-short-1066/v-short-1066_Presentation.vtt?token=ON-Ef_-r0d80eZzlJUsoGp-2sA9HL2SNv3o1qDETHgI&expires=1706590800"
                    },
                    {
                        "slot_id": "v-short-1138",
                        "session_id": "short3",
                        "title": "reVISit: Supporting Scalable Evaluation of Interactive Visualizations",
                        "contributors": [
                            "Yiren Ding"
                        ],
                        "authors": [
                            "Yiren Ding",
                            "Jack Wilburn",
                            "Hilson Shrestha",
                            "Akim Ndlovu",
                            "Kiran Gadhave",
                            "Carolina Nobre",
                            "Alexander Lex",
                            "Lane Harrison"
                        ],
                        "abstract": "reVISit is an open-source software toolkit and framework for creating, deploying, and monitoring empirical visualization studies. Running a quality empirical study in visualization can be demanding and resource-intensive, requiring substantial time, cost, and technical expertise from the research team. These challenges are amplified as research norms trend towards more complex and rigorous study methodologies, alongside a growing need to evaluate more complex interactive visualizations. reVISit aims to ameliorate these challenges by introducing a domain-specific language for study set-up, and a series of software components, such as UI elements, behavior provenance, and an experiment monitoring and management interface. Together with interactive or static stimuli provided by the experimenter, these are compiled to a ready-to-deploy web-based experiment. We demonstrate reVISit's functionality by re-implementing two studies \u2013 a graphical perception task and a more complex, interactive study. reVISit is an open-source community project, available at https://revisit.dev/",
                        "uid": "v-short-1138",
                        "time_stamp": "2023-10-24T22:45:00Z",
                        "time_start": "2023-10-24T22:45:00Z",
                        "time_end": "2023-10-24T22:54:00Z",
                        "paper_type": "short",
                        "keywords": [
                            "Human-centered computing Software prototype Visualization systems and tools Empirical Study"
                        ],
                        "doi": "",
                        "fno": "",
                        "has_image": true,
                        "has_pdf": true,
                        "paper_award": "",
                        "image_caption": "reVISit is an open-source toolkit designed to simplify the process of conducting empirical visualization studies, which are often resource-intensive and technically challenging. It offers a domain-specific language for study setup and includes various software components like UI elements, behavior tracking, and experiment management tools. These components, along with interactive or static stimuli provided by researchers, are combined to create a deployable web-based experiment. reVISit streamlines the setup and monitoring of studies, making it easier to conduct both simple graphical perception tasks and more complex interactive studies, addressing the growing demands of modern visualization research.",
                        "external_paper_link": "",
                        "youtube_ff_link": "https://youtu.be/m_RKKAWWi5Q",
                        "youtube_ff_id": "m_RKKAWWi5Q",
                        "bunny_ff_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-short/v-short-1138/v-short-1138_Preview.mp4?token=O255rGjS3--y-1nXwJM0SlsrKEtqnCh6-0D-rXfk1YQ&expires=1706590800",
                        "bunny_ff_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-short/v-short-1138/v-short-1138_Preview.vtt?token=XLYxOqdVxEH0HVm3D1jFZbbUCKRWCXZXrTyuo6FhJs4&expires=1706590800",
                        "youtube_prerecorded_link": "https://youtu.be/2v7g_cr5oNQ",
                        "youtube_prerecorded_id": "2v7g_cr5oNQ",
                        "bunny_prerecorded_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-short/v-short-1138/v-short-1138_Presentation.mp4?token=dW_v1gtmpCB4YAgz4-eYHTs8zLHgPb37XoB22L8aYWA&expires=1706590800",
                        "bunny_prerecorded_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-short/v-short-1138/v-short-1138_Presentation.vtt?token=hc0oewye1Hs0x_gcYy7P2zisxAHo2U7XuqPxOfP2woE&expires=1706590800"
                    },
                    {
                        "slot_id": "v-short-1133",
                        "session_id": "short3",
                        "title": "Augmented Reality for Scholarly Publication of 3D Visualizations in Astronomy: An Empirical Evaluation",
                        "contributors": [
                            "Jane L. Adams"
                        ],
                        "authors": [
                            "Jane L. Adams",
                            "Laura South",
                            "Arzu \u00c7\u00f6ltekin",
                            "Alyssa Goodman",
                            "Michelle A. Borkin"
                        ],
                        "abstract": "We present a mixed methods user study evaluating augmented reality (AR) as a visualization technique for use in astronomy journal publications. This work is motivated by the highly spatial nature of scientific visualizations employed in astronomy, including spatial reasoning tasks for hypothesis generation and scientific communications. In this 52-person user study, we evaluate two AR approaches (one traditional tabletop projection and the other with a 'tangible' aid) as spatial 3D visualization techniques, as compared to a baseline 3D rendering on a phone. We identify a significant difference in mental and physical workload between the two AR conditions in men and women. Qualitatively, through thematic coding of interviews, we identify notable observed differences ranging from device-specific physical challenges, to subdomain-specific utility within astronomy. The confluence of quantitative and qualitative results suggest a tension between workload and engagement when comparing non-AR and AR technologies. We summarize these findings and contribute them for reference in data visualization research furthering novel scientific communications in astronomy journal publications.",
                        "uid": "v-short-1133",
                        "time_stamp": "2023-10-24T22:54:00Z",
                        "time_start": "2023-10-24T22:54:00Z",
                        "time_end": "2023-10-24T23:03:00Z",
                        "paper_type": "short",
                        "keywords": [
                            "Human-centered computing, Visualization, Empirical studies in visualization, Human Computer Interaction (HCI), Interaction paradigms, Mixed / augmented reality"
                        ],
                        "doi": "",
                        "fno": "",
                        "has_image": true,
                        "has_pdf": true,
                        "paper_award": "",
                        "image_caption": "Experimental design: Part 1, A survey on participants' expertise, experience with AR, and demographic information, Part 2, Two sets of three tasks in each the non-AR and one of the two AR conditions, along with NASA-TLX workload questionnaires, and Part 3 Open feedback from participants.",
                        "external_paper_link": "",
                        "youtube_ff_link": "https://youtu.be/RK-T57Rp-pE",
                        "youtube_ff_id": "RK-T57Rp-pE",
                        "bunny_ff_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-short/v-short-1133/v-short-1133_Preview.mp4?token=xIdOI9JdF-gS0YVXit17TVmbpjMdFZWKMaGXT9SqhOk&expires=1706590800",
                        "bunny_ff_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-short/v-short-1133/v-short-1133_Preview.vtt?token=REQqR_0wy8PVueeRD82P2_M3XSZWyJ2diLhSNO02k_M&expires=1706590800",
                        "youtube_prerecorded_link": "https://youtu.be/unmRB-k3F9Q",
                        "youtube_prerecorded_id": "unmRB-k3F9Q",
                        "bunny_prerecorded_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-short/v-short-1133/v-short-1133_Presentation.mp4?token=4_F4EvsbmxGaxRF1SLEBIyw5uloKisWn3wqECxlcAP4&expires=1706590800",
                        "bunny_prerecorded_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-short/v-short-1133/v-short-1133_Presentation.vtt?token=pV_9e5kNaz3SkhZcJiwkovsXfpe47gsxCdXbgjSxj0w&expires=1706590800"
                    },
                    {
                        "slot_id": "v-short-1137",
                        "session_id": "short3",
                        "title": "Comparing Morse Complexes Using Optimal Transport: An Experimental Study",
                        "contributors": [
                            "Mingzhe Li"
                        ],
                        "authors": [
                            "Mingzhe Li",
                            "Carson Storm",
                            "Austin Yang Li",
                            "Tom Needham",
                            "Bei Wang Phillips"
                        ],
                        "abstract": "Morse complexes and Morse-Smale complexes are topological descriptors popular in topology-based visualization. Comparing these complexes plays an important role in their applications in feature correspondences, feature tracking, symmetry detection, and uncertainty visualization. Leveraging recent advances in optimal transport, we apply a class of optimal transport distances to the comparative analysis of Morse complexes. Contrasting with existing comparative measures, such distances are easy and efficient to compute, and naturally provide structural matching between Morse complexes. We perform an experimental study involving scientific simulation datasets and discuss the effectiveness of these distances as comparative measures for Morse complexes. We also provide an initial guideline for choosing the optimal transport distances under various data assumptions.",
                        "uid": "v-short-1137",
                        "time_stamp": "2023-10-24T23:03:00Z",
                        "time_start": "2023-10-24T23:03:00Z",
                        "time_end": "2023-10-24T23:12:00Z",
                        "paper_type": "short",
                        "keywords": [
                            "Morse Complexes, topological data analysis, optimal transport, topology in visualization"
                        ],
                        "doi": "",
                        "fno": "",
                        "has_image": true,
                        "has_pdf": true,
                        "paper_award": "",
                        "image_caption": "The source and the target networks are generated from Morse graphs. The color correspondence indicates structural alignments between the source and the target using the partial Fused Gromov-Wasserstein distance. Noisy features that are ignored in the alignment are shown as hollow circles.",
                        "external_paper_link": "",
                        "youtube_ff_link": "https://youtu.be/l0ucjpJxnek",
                        "youtube_ff_id": "l0ucjpJxnek",
                        "bunny_ff_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-short/v-short-1137/v-short-1137_Preview.mp4?token=WK8F3KC7jmR7y0ut0wmARHnZqqIGOPRZPopwvE9JePY&expires=1706590800",
                        "bunny_ff_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-short/v-short-1137/v-short-1137_Preview.vtt?token=jjgyThxe0zTCWQr3Y0zLG7LmFgg9WZzXHkju-NFQk4Y&expires=1706590800",
                        "youtube_prerecorded_link": "https://youtu.be/V_CNZflM1vQ",
                        "youtube_prerecorded_id": "V_CNZflM1vQ",
                        "bunny_prerecorded_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-short/v-short-1137/v-short-1137_Presentation.mp4?token=yRIeDvYVmDKYVX_Dsc0Z033MYPwsfjVB8mz4yUmXViU&expires=1706590800",
                        "bunny_prerecorded_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-short/v-short-1137/v-short-1137_Presentation.vtt?token=sggen6dfHjWBkYzJelZXmIMxBwvVvlSo283Lctfkhl4&expires=1706590800"
                    }
                ]
            },
            {
                "title": "Scientific Visualization (Short)",
                "session_id": "short4",
                "event_prefix": "v-short",
                "track": "oneohfour",
                "session_image": "short4.png",
                "chair": [
                    "Paul Rosen"
                ],
                "time_start": "2023-10-25T03:00:00Z",
                "time_end": "2023-10-25T04:15:00Z",
                "discord_category": "",
                "discord_channel": "104",
                "discord_channel_id": "1161741240665124954",
                "discord_link": "https://discordapp.com/channels/1161400451619627068/1161741240665124954",
                "zoom_private_meeting": "",
                "zoom_private_password": "",
                "zoom_private_link": "",
                "zoom_broadcast_link": "",
                "ff_link": "https://youtu.be/Hyeo0DG3FqA",
                "time_slots": [
                    {
                        "slot_id": "v-short-1057",
                        "session_id": "short4",
                        "title": "Visualizing Query Traversals Over Bounding Volume Hierarchies Using Treemaps",
                        "contributors": [
                            "Abhishek Madan"
                        ],
                        "authors": [
                            "Abhishek Madan",
                            "Carolina Nobre"
                        ],
                        "abstract": "Bounding volume hierarchies (BVHs) are one of the most common spatial data structures in computer graphics. Visualizing ray intersections in these data structures is challenging due to the large number of queries in typical image rendering workloads, the spatial clutter induced by superimposing the tree in a 3D viewport, and the strong tendency of these queries to visit several tree leaves, all of which add a very high dimensionality to the data being visualized. We present a new technique for visualizing ray intersection traversals on BVHs over triangle meshes. Unlike previous approaches which display aggregate traversal costs using a heatmap over the rendered image, we display detailed traversal information about individual queries, using a 3D view of the mesh, a treemap of the BVH, and synchronized highlighting between the two views, along with a pixel grid to select a ray intersection query to view. We demonstrate how this technique elucidates traversal dynamics and tree construction properties, which makes it possible to easily spot algorithmic improvements in these two categories.",
                        "uid": "v-short-1057",
                        "time_stamp": "2023-10-25T03:00:00Z",
                        "time_start": "2023-10-25T03:00:00Z",
                        "time_end": "2023-10-25T03:09:00Z",
                        "paper_type": "short",
                        "keywords": [
                            "bounding volume hierarchies, treemaps"
                        ],
                        "doi": "",
                        "fno": "",
                        "has_image": true,
                        "has_pdf": true,
                        "paper_award": "",
                        "image_caption": "An annotated screenshot of our visualization, containing a 3D viewport, a zoomable treemap, and a pixel grid. All three views work together to show the structure of a bounding volume hierarchy (BVH) with coordination between the viewport and treemap, view statistics of ray intersection queries with the BVH, and view individual query traces and results.",
                        "external_paper_link": "",
                        "youtube_ff_link": "https://youtu.be/BOKui4Cnk58",
                        "youtube_ff_id": "BOKui4Cnk58",
                        "bunny_ff_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-short/v-short-1057/v-short-1057_Preview.mp4?token=WLiuLYxU_WiuT1iRRC4VsNH6vwH0tNDSx1MBBAlvnmo&expires=1706590800",
                        "bunny_ff_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-short/v-short-1057/v-short-1057_Preview.vtt?token=rC__EsrxmDAq5d_ueLtJA6IKS87NQZDszqxLDkvEYYg&expires=1706590800",
                        "youtube_prerecorded_link": "https://youtu.be/j5RSwvJbXik",
                        "youtube_prerecorded_id": "j5RSwvJbXik",
                        "bunny_prerecorded_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-short/v-short-1057/v-short-1057_Presentation.mp4?token=V4MlikpELHnc_xfiZD87DSAEdfRqZ-yzKmG8dYjSKrI&expires=1706590800",
                        "bunny_prerecorded_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-short/v-short-1057/v-short-1057_Presentation.vtt?token=YtOHYMy6ew0QlqQY91QmLFiWAdASyVbIaJG3hTafLXo&expires=1706590800"
                    },
                    {
                        "slot_id": "v-short-1089",
                        "session_id": "short4",
                        "title": "Visual Analysis of Large Multi-Field AMR Data on GPUs Using Interactive Volume Lines",
                        "contributors": [
                            "Stefan Zellmann"
                        ],
                        "authors": [
                            "Stefan Zellmann",
                            "Serkan Demirci",
                            "Ugur Gudukbay"
                        ],
                        "abstract": "To visually compare ensembles of volumes, dynamic volume lines (DVLs) represent each ensemble member as a 1D polyline. To compute these, the volume cells are sorted on a space-filling curve and scaled by the ensemble\u2019s local variation. The resulting 1D plot can augment or serve as an alternative to a 3D volume visualization free of visual clutter and occlusion. Interactively computing DVLs is challenging when the data is large, and the volume grid is not structured/regular, as is often the case with computational fluid dynamics simulations. We extend DVLs to support large-scale, multi-field adaptive mesh refinement (AMR) data that can be explored interactively. Our GPU-based system updates the DVL representation whenever the data or the alpha transfer function changes. We demonstrate and evaluate our interactive prototype using large AMR volumes from astrophysics simulations.",
                        "uid": "v-short-1089",
                        "time_stamp": "2023-10-25T03:09:00Z",
                        "time_start": "2023-10-25T03:09:00Z",
                        "time_end": "2023-10-25T03:18:00Z",
                        "paper_type": "short",
                        "keywords": [
                            "Human-centered computing\u2014Visualization\u2014Visualization application domains\u2014Visual analytics; Human-centered computing\u2014Visualization\u2014Visualization application domains\u2014Scientific visualization"
                        ],
                        "doi": "",
                        "fno": "",
                        "has_image": true,
                        "has_pdf": true,
                        "paper_award": "",
                        "image_caption": "Coupled interactive volume lines (IVL) and large volume visualization. At the top we show volume renderings of four different fields of an astrophysical data set; at the bottom, we show two different representations of IVLs as a way to visually explore large AMR volumes using techniques from visual analytics.",
                        "external_paper_link": "",
                        "youtube_ff_link": "https://youtu.be/0jBbDk5z8U4",
                        "youtube_ff_id": "0jBbDk5z8U4",
                        "bunny_ff_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-short/v-short-1089/v-short-1089_Preview.mp4?token=gTtG8oHAdqS8EiU1yY5_qBnKbttk3ghU-lzkRPyEAzQ&expires=1706590800",
                        "bunny_ff_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-short/v-short-1089/v-short-1089_Preview.vtt?token=yjInnXPpASRkrMGi_QFuwOaHufaYhfBVZU9Z6gVPSwk&expires=1706590800",
                        "youtube_prerecorded_link": "https://youtu.be/wGk9r_tjX98",
                        "youtube_prerecorded_id": "wGk9r_tjX98",
                        "bunny_prerecorded_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-short/v-short-1089/v-short-1089_Presentation.mp4?token=_WXlPAerQopOCwyYE4w3YVh6-_Fsvot3ujCKD-8WnjE&expires=1706590800",
                        "bunny_prerecorded_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-short/v-short-1089/v-short-1089_Presentation.vtt?token=feCvPh-CTccmmDi_3A1e0YtIwN-k1yjyzymHyclKGYM&expires=1706590800"
                    },
                    {
                        "slot_id": "v-short-1154",
                        "session_id": "short4",
                        "title": "Fast Fiber Line Extraction for 2D Bivariate Scalar Fields",
                        "contributors": [
                            "Felix Raith"
                        ],
                        "authors": [
                            "Felix Raith",
                            "Baldwin Nsonga",
                            "Gerik Scheuermann",
                            "Christian Heine"
                        ],
                        "abstract": "Extracting level sets from scalar data is a fundamental operation in visualization with many applications. Recently, the concept of level set extraction has been extended to bivariate scalar fields. Prior work on vector field equivalence, wherein an analyst marks a region in the domain and is shown other regions in the domain with similar vector values, pointed out the need to make this extraction operation fast, so that analysts can work interactively. To date, the fast extraction of level sets from bivariate scalar fields has not been researched as extensively as for the univariate case. In this paper, we present a novel algorithm that extracts fiber lines, i.e., the preimages of so called control polygons (FSCP), for bivariate 2D data by joint traversal of bounding volume hierarchies for both grid and FSCP elements. We performed an extensive evaluation, comparing our method to a two-dimensional adaptation of the method proposed by Klacansky et al., as well as to the naive approach for fiber line extraction. The evaluation incorporates a vast array of configurations in several datasets. We found that our method provides a speedup of several orders of magnitudes compared to the naive algorithm and requires two thirds of the computation time compared to Klacansky et al. adapted for 2D.",
                        "uid": "v-short-1154",
                        "time_stamp": "2023-10-25T03:18:00Z",
                        "time_start": "2023-10-25T03:18:00Z",
                        "time_end": "2023-10-25T03:27:00Z",
                        "paper_type": "short",
                        "keywords": [
                            "Visualization, scalar fields, bivariate data, fibers, preimage extraction algorithm, dual bounding volume hierarchy traversal"
                        ],
                        "doi": "",
                        "fno": "",
                        "has_image": false,
                        "has_pdf": true,
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "youtube_ff_link": "https://youtu.be/WfTz5SKGDt4",
                        "youtube_ff_id": "WfTz5SKGDt4",
                        "bunny_ff_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-short/v-short-1154/v-short-1154_Preview.mp4?token=NPN3qCsbiZ1-LyCFatfskEC3gMW6Vs1jytQlQyZXAJk&expires=1706590800",
                        "bunny_ff_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-short/v-short-1154/v-short-1154_Preview.vtt?token=ddSiZjM_khxDrZIW7OjSVxfw2kMdAUJXzgwJ8FiX9JA&expires=1706590800",
                        "youtube_prerecorded_link": "https://youtu.be/fVnIv4YFG5I",
                        "youtube_prerecorded_id": "fVnIv4YFG5I",
                        "bunny_prerecorded_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-short/v-short-1154/v-short-1154_Presentation.mp4?token=1UA6HuhYrEXjaOqg9zKkjw6LoGM14J3Tf7Pk-n954VY&expires=1706590800",
                        "bunny_prerecorded_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-short/v-short-1154/v-short-1154_Presentation.vtt?token=zvqAA-I2zF2L4L9ObZMJlkXutwofQoWEZeuF56KWXFo&expires=1706590800"
                    },
                    {
                        "slot_id": "v-short-1157",
                        "session_id": "short4",
                        "title": "GeneticFlow: Exploring Scholar Impact with Interactive Visualization",
                        "contributors": [
                            "Fengli Xiao"
                        ],
                        "authors": [
                            "Fengli Xiao",
                            "Lei Shi"
                        ],
                        "abstract": "Visualizing a scholar's scientific impact is important for many challenging tasks in academia such as tenure evaluation and award selection. Existing visualization and profiling approaches do not focus on the analysis of individual scholar's impact, or they are too abstract to provide detailed interpretation of high-impact scholars. This work builds over a new scholar-centric impact-oriented profiling method called GeneticFlow. We propose a visualization design of scholar's self-citation graphs using a time-dependent, hierarchical representation method. The graph visualization is augmented with color-coded topic information trained with cutting-edge deep learning techniques, and also temporal trend chart to illustrate the dynamics of topic/impact evolution. The visualization method is validated on a benchmark dataset established for the visualization field. Visualization results reveal key patterns of high-impact scholars and also demonstrate its capability to serve ordinary researchers for their impact visualization task.",
                        "uid": "v-short-1157",
                        "time_stamp": "2023-10-25T03:27:00Z",
                        "time_start": "2023-10-25T03:27:00Z",
                        "time_end": "2023-10-25T03:36:00Z",
                        "paper_type": "short",
                        "keywords": [
                            "Computing: Software, Networks, Security, Performance Engr., Distr. Systems, Databases ; Graph/Network and Tree Data"
                        ],
                        "doi": "",
                        "fno": "",
                        "has_image": true,
                        "has_pdf": true,
                        "paper_award": "",
                        "image_caption": "GeneticFlow visualization interface (Prof. Keim's graph): (a) system control panel; (b) scholar demographics; (c) graph statistics; (d) GF graph visualization; (e) topic distribution map; (f) author/paper/citation detail panel.",
                        "external_paper_link": "",
                        "youtube_ff_link": "https://youtu.be/QnO9jWurTgQ",
                        "youtube_ff_id": "QnO9jWurTgQ",
                        "bunny_ff_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-short/v-short-1157/v-short-1157_Preview.mp4?token=zuESmVFsbRcU_R7lixLYNovrrVOpPvoxPIEwGUgws3I&expires=1706590800",
                        "bunny_ff_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-short/v-short-1157/v-short-1157_Preview.vtt?token=-KncuC8SeAGAuujryzJBj1DBuTFoF9ji8EeNhN9KxeE&expires=1706590800",
                        "youtube_prerecorded_link": "https://youtu.be/QvBFksAvioU",
                        "youtube_prerecorded_id": "QvBFksAvioU",
                        "bunny_prerecorded_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-short/v-short-1157/v-short-1157_Presentation.mp4?token=_GlTVXBvTt_mzTB72bUieq3Q-P3j_VueGLwcaixhGwc&expires=1706590800",
                        "bunny_prerecorded_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-short/v-short-1157/v-short-1157_Presentation.vtt?token=wrieYW9izOFVii_zdvdYF3VXSAzJs2KjDMz_1ukinhA&expires=1706590800"
                    },
                    {
                        "slot_id": "v-short-1171",
                        "session_id": "short4",
                        "title": "Visualizing Similarity of Pathline Dynamics in 2D Flow Fields",
                        "contributors": [
                            "Baldwin Nsonga"
                        ],
                        "authors": [
                            "Baldwin Nsonga",
                            "Gerik Scheuermann"
                        ],
                        "abstract": "Even though the analysis of unsteady 2D flow fields is challenging, fluid mechanics experts generally have an intuition on where in the simulation domain specific features are expected. Using this intuition, showing similar regions enables the user to discover flow patterns within the simulation data. When focusing on similarity, a solid mathematical framework for a specific flow pattern is not required. We propose a technique that visualizes similar and dissimilar regions with respect to a region selected by the user. Using infinitesimal strain theory, we capture the strain and rotation progression and therefore the dynamics of fluid parcels along pathlines, which we encode as distributions. We then apply the Jensen\u2013Shannon divergence to compute the (dis)similarity between pathline dynamics originating in a user-defined flow region and the pathline dynamics of the flow field. We validate our method by applying it to two simulation datasets of two-dimensional unsteady flows. Our results show that our approach is suitable for analyzing the similarity of time-dependent flow fields.",
                        "uid": "v-short-1171",
                        "time_stamp": "2023-10-25T03:36:00Z",
                        "time_start": "2023-10-25T03:36:00Z",
                        "time_end": "2023-10-25T03:45:00Z",
                        "paper_type": "short",
                        "keywords": [
                            "Human-centered computing\u2014Visualization\u2014 Visualization application domains\u2014Scientific visualization"
                        ],
                        "doi": "",
                        "fno": "",
                        "has_image": true,
                        "has_pdf": true,
                        "paper_award": "",
                        "image_caption": "Even though the analysis of unsteady 2D flow fields is challenging, fluid mechanics experts generally have an intuition on where in the simulation domain specific features are expected. Using this intuition, showing similar regions enables the user to discover flow patterns within the simulation data.  We utilize infinitesimal strain theory and the Jensen-Shannon divergence to visualize similar and dissimilar regions with respect to a region selected by the user. We validate our method by applying it to two simulation datasets of two-dimensional unsteady flows.",
                        "external_paper_link": "",
                        "youtube_ff_link": "https://youtu.be/ZFRP8VzCJY0",
                        "youtube_ff_id": "ZFRP8VzCJY0",
                        "bunny_ff_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-short/v-short-1171/v-short-1171_Preview.mp4?token=joMdVUCBrjI0PpHJj-XYqm79ZWRl--UffJqVKGNntFY&expires=1706590800",
                        "bunny_ff_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-short/v-short-1171/v-short-1171_Preview.vtt?token=q7zBBxsOzqzUagv0sJDKhnV3tFnBWP0C5uJJh8FItZU&expires=1706590800",
                        "youtube_prerecorded_link": "https://youtu.be/B0UF5mp_afI",
                        "youtube_prerecorded_id": "B0UF5mp_afI",
                        "bunny_prerecorded_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-short/v-short-1171/v-short-1171_Presentation.mp4?token=ehQO6p304A6dyCm0kBi3_TL99uIxq8to232LYYfDzsU&expires=1706590800",
                        "bunny_prerecorded_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-short/v-short-1171/v-short-1171_Presentation.vtt?token=APixZS5yvXZtXUFdjjbIzINxuSdFksyxYgHAhZLZHCQ&expires=1706590800"
                    },
                    {
                        "slot_id": "v-short-1114",
                        "session_id": "short4",
                        "title": "Evaluation of cinematic volume rendering open-source and commercial solutions for the exploration of congenital heart data",
                        "contributors": [
                            "Oscar Camara"
                        ],
                        "authors": [
                            "Irum Baseer",
                            "Israel Valverde",
                            "Abdel H. Moustafa",
                            "Josep Blat",
                            "Oscar Camara"
                        ],
                        "abstract": "Detailed anatomical information is essential to optimize medical decisions for surgical and pre-operative planning in patients with congenital heart disease. The visualization techniques commonly used in clinical routine for the exploration of complex cardiac data are based on multi-planar reformations, maximum intensity projection, and volume rendering, which rely on basic lighting models prone to image distortion. On the other hand, cinematic rendering (CR), a three-dimensional visualization technique based on physically-based rendering methods, can create volumetric images with high fidelity. However, there are a lot of parameters involved in CR that affect the visualization results, thus being dependent on the user's experience and requiring detailed evaluation protocols to compare available solutions. In this study, we have analyzed the impact of the most relevant parameters in a CR pipeline developed in the open-source version of the MeVisLab framework for the visualization of the heart anatomy of three congenital patients and two adults from CT images. The resulting visualizations were compared to a commercial tool used in the clinics with a questionnaire filled in by clinical users, providing similar definitions of structures, depth perception, texture appearance, realism, and diagnostic ability.",
                        "uid": "v-short-1114",
                        "time_stamp": "2023-10-25T03:45:00Z",
                        "time_start": "2023-10-25T03:45:00Z",
                        "time_end": "2023-10-25T03:54:00Z",
                        "paper_type": "short",
                        "keywords": [
                            "Cinematic rendering, open-source, commercial tool, congenital heart data"
                        ],
                        "doi": "",
                        "fno": "",
                        "has_image": true,
                        "has_pdf": true,
                        "paper_award": "",
                        "image_caption": "Exploring Innovative Medical Visualization: We've ventured into the realm of medical imaging, investigating advanced techniques to improve decision-making in congenital heart disease cases. Traditional methods, which depend on basic lighting models, often lead to distortions in complex cardiac data. Enter cinematic rendering (CR), an impressive three-dimensional photo-realistic visualization method. Our study thoroughly examined the effects of CR on heart anatomy visualization, utilizing the open-source MeVisLab framework, and compared its performance to a commercial clinical tool.",
                        "external_paper_link": "",
                        "youtube_ff_link": "https://youtu.be/lo7LBkEeQHE",
                        "youtube_ff_id": "lo7LBkEeQHE",
                        "bunny_ff_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-short/v-short-1114/v-short-1114_Preview.mp4?token=MOc_HE_k-hQHyptKKdKdgwNsmTxc0sCrYCo2eIPmqH0&expires=1706590800",
                        "bunny_ff_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-short/v-short-1114/v-short-1114_Preview.vtt?token=5o3rwpCHZKiKKfJYcki7v5FY7z6U9wzQNGF2j2ritIg&expires=1706590800",
                        "youtube_prerecorded_link": "https://youtu.be/xWZqx7UutOs",
                        "youtube_prerecorded_id": "xWZqx7UutOs",
                        "bunny_prerecorded_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-short/v-short-1114/v-short-1114_Presentation.mp4?token=k20BHER2FN6pXFl8ukSHW79aNFB_1O9CGtgbE_oa82s&expires=1706590800",
                        "bunny_prerecorded_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-short/v-short-1114/v-short-1114_Presentation.vtt?token=Y_LFFaUy-2m9bnprEXl_rYXsY7YjAL-iqjP710cr1WQ&expires=1706590800"
                    },
                    {
                        "slot_id": "v-short-1182",
                        "session_id": "short4",
                        "title": "ExoplanetExplorer: Contextual Visualization of Exoplanet Systems",
                        "contributors": [
                            "Emma Broman"
                        ],
                        "authors": [
                            "Emma Broman",
                            "Jacqueline Faherty",
                            "Laura Kreidberg",
                            "Sebastian Zieba",
                            "Charles Hansen",
                            "Anders Ynnerman",
                            "Alexander Bock"
                        ],
                        "abstract": "An exoplanet is a planet outside of our solar system. Researchers study known exoplanets and gather data about them through observations and derived data. Ongoing efforts involve finding planets with an environment that supports life, which likely exists in what is known as the habitable zone around a star. Through a participatory design process, we developed a tool that enables the exploration of exoplanet attribute data and provides contextual visual information in a 3D spatial view that seamlessly presents an overview and a system view showing particular exoplanet systems.",
                        "uid": "v-short-1182",
                        "time_stamp": "2023-10-25T03:54:00Z",
                        "time_start": "2023-10-25T03:54:00Z",
                        "time_end": "2023-10-25T04:03:00Z",
                        "paper_type": "short",
                        "keywords": [
                            "Human-centered computing\u2014Visualization\u2014Visualization systems and tools; Human-centered computing\u2014Visualization\u2014Visualization application domains\u2014Scientific visualization"
                        ],
                        "doi": "",
                        "fno": "",
                        "has_image": true,
                        "has_pdf": true,
                        "paper_award": "",
                        "image_caption": "The overview mode of the ExoplanetExplorer, showing glyps for exoplanets positioned in their 3D spatial context, together with some menus.",
                        "external_paper_link": "",
                        "youtube_ff_link": "https://youtu.be/xZUzkzHqT60",
                        "youtube_ff_id": "xZUzkzHqT60",
                        "bunny_ff_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-short/v-short-1182/v-short-1182_Preview.mp4?token=ks7QYFg3C1hG1ESfxe1ul_4ZZGUhwo8TFw535L8TotA&expires=1706590800",
                        "bunny_ff_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-short/v-short-1182/v-short-1182_Preview.vtt?token=GyHA7QanMBy8rGL9DdX35nlZBqk_oSYVvnB2VRDijZY&expires=1706590800",
                        "youtube_prerecorded_link": "https://youtu.be/xMUhB5ncAaI",
                        "youtube_prerecorded_id": "xMUhB5ncAaI",
                        "bunny_prerecorded_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-short/v-short-1182/v-short-1182_Presentation.mp4?token=HygB6BqqrRHbpFmgyV7QjHwmsVyPWTMgbbxqvL9tZbA&expires=1706590800",
                        "bunny_prerecorded_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-short/v-short-1182/v-short-1182_Presentation.vtt?token=1O_GLizwkKJZKEVHjfzl9bBtovSOjWD5KCjaIXmUHK0&expires=1706590800"
                    },
                    {
                        "slot_id": "v-short-1053",
                        "session_id": "short4",
                        "title": "A Visualization System for Hexahedral Mesh Quality Study",
                        "contributors": [
                            "Adeel Zafar"
                        ],
                        "authors": [
                            "Lei Si",
                            "Guoning Chen"
                        ],
                        "abstract": "In this paper, we introduce a new 3D hex mesh visual analysis system that emphasizes poor-quality areas with an aggregated glyph, highlights overlapping elements, and provides detailed boundary error inspection in three forms. By supporting multi-level analysis through multiple views, our system effectively evaluates various mesh models and compares the performance of mesh generation and optimization algorithms for hexahedral meshes.",
                        "uid": "v-short-1053",
                        "time_stamp": "2023-10-25T04:03:00Z",
                        "time_start": "2023-10-25T04:03:00Z",
                        "time_end": "2023-10-25T04:12:00Z",
                        "paper_type": "short",
                        "keywords": [
                            "hex-mesh analysis, mesh quality visualization"
                        ],
                        "doi": "",
                        "fno": "",
                        "has_image": true,
                        "has_pdf": true,
                        "paper_award": "",
                        "image_caption": "Element quality visualization : element quality is visualized using glyphs, If glyphs overlap in the main view, they are aggregated. Each vertex glyph is displayed in the region view. Boundary error visualization : the distance between two boundaries is embedded into the UV domain. Users can access individual vertex quality details through an interactive bar chart.",
                        "external_paper_link": "",
                        "youtube_ff_link": "https://youtu.be/FMXqYe9mpLc",
                        "youtube_ff_id": "FMXqYe9mpLc",
                        "bunny_ff_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-short/v-short-1053/v-short-1053_Preview.mp4?token=DjKAC4HHAWSzXEMh2PJ-lrmAeN9k8Yx5DHUfZbQjbMg&expires=1706590800",
                        "bunny_ff_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-short/v-short-1053/v-short-1053_Preview.vtt?token=lsRg9LrzfIakJDQ-K2jgc__WbfvJEtqkusk3OJ0NkhA&expires=1706590800",
                        "youtube_prerecorded_link": "https://youtu.be/XPD2SLDBD-w",
                        "youtube_prerecorded_id": "XPD2SLDBD-w",
                        "bunny_prerecorded_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-short/v-short-1053/v-short-1053_Presentation.mp4?token=hoM03F_fEJWWBLlwX0nEY_MZfd7VBMxv0eAySnaCZf4&expires=1706590800",
                        "bunny_prerecorded_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-short/v-short-1053/v-short-1053_Presentation.vtt?token=aZ6kKXRLREg4YXlriuaM53Yd2e2fb4PcLyex8dKUsMw&expires=1706590800"
                    }
                ]
            },
            {
                "title": "Machine Learning / Language Models / Theory",
                "session_id": "short5",
                "event_prefix": "v-short",
                "track": "oneohfour",
                "session_image": "short5.png",
                "chair": [
                    "Chaoli Wang"
                ],
                "time_start": "2023-10-26T22:00:00Z",
                "time_end": "2023-10-26T23:15:00Z",
                "discord_category": "",
                "discord_channel": "104",
                "discord_channel_id": "1161741240665124954",
                "discord_link": "https://discordapp.com/channels/1161400451619627068/1161741240665124954",
                "zoom_private_meeting": "",
                "zoom_private_password": "",
                "zoom_private_link": "",
                "zoom_broadcast_link": "",
                "ff_link": "https://youtu.be/dYTrG7tKsAI",
                "time_slots": [
                    {
                        "slot_id": "v-short-1097",
                        "session_id": "short5",
                        "title": "Explain-and-Test: An Interactive Machine Learning Framework for Exploring Text Embeddings",
                        "contributors": [
                            "Shivam Raval"
                        ],
                        "authors": [
                            "Shivam Raval",
                            "Carolyn Ann Wang",
                            "Fernanda Viegas",
                            "Martin Wattenberg"
                        ],
                        "abstract": "TextCluster Explainer visualizes text embeddings, allowing users to lasso clusters of interest to get automated explanations of these clusters. The system provides two kinds of explanations: Contrastive PhraseClouds generated by an SVM model and Natural Language Explanations generated by a large language model (GPT-4). Users can test these automated explanations by entering manual labels to be dynamically embedded into the visualization, a feature we call Assessment by Re-projection. If the new text is mapped to the cluster, it validates the automated explanation. The figure shows a t-SNE projection where each point corresponds to the paper title from the Visualization Publications dataset along along with explanations and test inputs for two different clusters. The color encoding corresponds to labels predicted by clustering the projections.",
                        "uid": "v-short-1097",
                        "time_stamp": "2023-10-26T22:00:00Z",
                        "time_start": "2023-10-26T22:00:00Z",
                        "time_end": "2023-10-26T22:09:00Z",
                        "paper_type": "short",
                        "keywords": [
                            "Text Visualization, Dimensionality Reduction, Clustering\u2014Large Language Models\u2014Explanation\u2014"
                        ],
                        "doi": "",
                        "fno": "",
                        "has_image": true,
                        "has_pdf": true,
                        "paper_award": "",
                        "image_caption": "The Explain-and-test framework visualizes text embeddings, allowing users to lasso clusters of interest to get automated explanations of these clusters. The system provides two kinds of explanations: Contrastive PhraseClouds generated by an interpretable SVM model and Natural Language Explanations generated by a large language model (GPT-4). Users can test these automated explanations by entering manual labels to be dynamically embedded into the visualization, a feature we call Assessment by Re-projection. If the new text is mapped to the cluster, it validates the automated explanation. The figure shows a t-SNE projection where each point corresponds to the paper title from the Visualization Publications dataset along along with explanations and test inputs for two different clusters. The color encoding corresponds to labels predicted by clustering the projections",
                        "external_paper_link": "",
                        "youtube_ff_link": "https://youtu.be/nXoPKERLZI8",
                        "youtube_ff_id": "nXoPKERLZI8",
                        "bunny_ff_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-short/v-short-1097/v-short-1097_Preview.mp4?token=2S5HLmRT3obLlNz1SBP9GaAJe8_HYydlyZl9wdUz400&expires=1706590800",
                        "bunny_ff_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-short/v-short-1097/v-short-1097_Preview.vtt?token=rfuOo1kwqGJQ3JzhX218tXafXM7QK_YaLfvvd9of7fo&expires=1706590800",
                        "youtube_prerecorded_link": "https://youtu.be/p6-xK7qQiYQ",
                        "youtube_prerecorded_id": "p6-xK7qQiYQ",
                        "bunny_prerecorded_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-short/v-short-1097/v-short-1097_Presentation.mp4?token=eVuhtk6CQrvYQBTziW7GP_EdU2KfisFQCVvgQ7dem3o&expires=1706590800",
                        "bunny_prerecorded_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-short/v-short-1097/v-short-1097_Presentation.vtt?token=k3MwmwcRB6RLsfttluTdSM9HrKt_f630GHJBZUqT53I&expires=1706590800"
                    },
                    {
                        "slot_id": "v-short-1186",
                        "session_id": "short5",
                        "title": "Concept Lens: Visually Analyzing the Consistency of Semantic Manipulation in GANs",
                        "contributors": [
                            "Sangwon Jeong"
                        ],
                        "authors": [
                            "Sangwon Jeong",
                            "Mingwei Li",
                            "Matthew Berger",
                            "Shusen Liu"
                        ],
                        "abstract": "As applications of generative AI become mainstream, it is important to understand what generative models are capable of producing, and the extent to which one can predictably control their outputs. In this paper, we propose a visualization design, named Concept Lens, for jointly navigating the data distribution of a generative model, and concept manipulations supported by the model. Our work is focused on modern vision-based generative adversarial networks (GAN), and their learned latent spaces, wherein concept discovery has gained significant interest as a means of image manipulation. Concept Lens is designed to support users in understanding the diversity of a pro- vided set of concepts, the relationship between concepts, and the suitability of concepts to give semantic controls for image genera- tion. Key to our approach is the hierarchical grouping of concepts, generated images, and the associated joint exploration. We show how Concept Lens can reveal consistent semantic manipulations for editing images, while also serving as a diagnostic tool for studying the limitations and trade-offs of concept discovery methods.",
                        "uid": "v-short-1186",
                        "time_stamp": "2023-10-26T22:09:00Z",
                        "time_start": "2023-10-26T22:09:00Z",
                        "time_end": "2023-10-26T22:18:00Z",
                        "paper_type": "short",
                        "keywords": [
                            "Generative model, Explainable AI, Latent space, Clustering, Bivariate color"
                        ],
                        "doi": "",
                        "fno": "",
                        "has_image": true,
                        "has_pdf": true,
                        "paper_award": "",
                        "image_caption": "Concept Lens is an interactive diagnostic tool to let users understand the latent space of any generative models. Individual icicle plot reveals hierarchy of code and direction, respectively. Code and direction hierarchy is jointly represented in the bi-hierarchy view in the middle which allows users to study the latent space from both global and local perspective.",
                        "external_paper_link": "",
                        "youtube_ff_link": "https://youtu.be/eysuVKFEEtk",
                        "youtube_ff_id": "eysuVKFEEtk",
                        "bunny_ff_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-short/v-short-1186/v-short-1186_Preview.mp4?token=kjsqCRAKmQKcTdPZQ0LooT7lpqMfNUQJoC7bHA2IlDc&expires=1706590800",
                        "bunny_ff_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-short/v-short-1186/v-short-1186_Preview.vtt?token=sVWgZFYczrNb4bTxLeBn_uDSyF5BQ4rtLBoOTkw_RYw&expires=1706590800",
                        "youtube_prerecorded_link": "https://youtu.be/vGjovECYL2U",
                        "youtube_prerecorded_id": "vGjovECYL2U",
                        "bunny_prerecorded_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-short/v-short-1186/v-short-1186_Presentation.mp4?token=YO6N35fXgVlpE7MuddpifP0OXmsOwsBvPzBZhc0lAWk&expires=1706590800",
                        "bunny_prerecorded_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-short/v-short-1186/v-short-1186_Presentation.vtt?token=IE8ZIRhSm18tWMAU5kgUNGADNoVApyeblpFxq1Z-vDw&expires=1706590800"
                    },
                    {
                        "slot_id": "v-short-1065",
                        "session_id": "short5",
                        "title": "HAiVA: Hybrid AI-assisted Visual Analysis Framework to Study the Effects of Cloud Properties on Climate Patterns",
                        "contributors": [
                            "Subhashis Hazarika"
                        ],
                        "authors": [
                            "Subhashis Hazarika",
                            "Haruki Hirasawa",
                            "Sookyung Kim",
                            "Kalai Ramea",
                            "Salva R\u00fchling Cachay",
                            "Peetak Mitra",
                            "Dipti Hingmire",
                            "Hansi Singh",
                            "Phil Rasch"
                        ],
                        "abstract": "Clouds have a significant impact on the Earth's climate system. They play a vital role in modulating Earth\u2019s radiation budget and driving regional changes in temperature and precipitation. This makes clouds ideal for climate intervention techniques like Marine Cloud Brightening (MCB) which refers to modification in cloud reflectivity, thereby cooling the surrounding region. However, to avoid unintended effects of MCB, we need a better understanding of the complex cloud to climate response function. Designing and testing such interventions scenarios with conventional Earth System Models is computationally expensive. Therefore, we propose a hybrid AI-assisted visual analysis framework to drive such scientific studies and facilitate interactive what-if investigation of different MCB intervention scenarios to assess their intended and unintended impacts on climate patterns. We work with a team of climate scientists to develop a suite of hybrid AI models emulating cloud-climate response function and design a tightly coupled frontend interactive visual analysis system to perform different MCB intervention experiments.",
                        "uid": "v-short-1065",
                        "time_stamp": "2023-10-26T22:18:00Z",
                        "time_start": "2023-10-26T22:18:00Z",
                        "time_end": "2023-10-26T22:27:00Z",
                        "paper_type": "short",
                        "keywords": [
                            "climate, machine learning, visual analysis, interaction, climate intervention"
                        ],
                        "doi": "",
                        "fno": "",
                        "has_image": true,
                        "has_pdf": true,
                        "paper_award": "",
                        "image_caption": "HAiVA is a Hybrid AI-assisted Visual Analysis Framework to Study the Effects of Cloud Properties on Climate Patterns. It offers an interactive visualization system for climate scientists to perform rapid prototyping and exploration of climate intervention technique called Marine Cloud Brightening (MCB).  HAiVA facilitate testing different MCB intervention scenarios and evaluating their possible intended and unintended climate consequences.",
                        "external_paper_link": "",
                        "youtube_ff_link": "https://youtu.be/ZaXuPGJPh3E",
                        "youtube_ff_id": "ZaXuPGJPh3E",
                        "bunny_ff_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-short/v-short-1065/v-short-1065_Preview.mp4?token=7E12-0YRnGge3ntynudlfONDtbjHg08yrHplmh_EeBU&expires=1706590800",
                        "bunny_ff_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-short/v-short-1065/v-short-1065_Preview.vtt?token=HUMOWWit6YeWU-k1Y6Skbzo9pvs0AmKWJzs1DFSli3E&expires=1706590800",
                        "youtube_prerecorded_link": "https://youtu.be/HZhN-EGTZzE",
                        "youtube_prerecorded_id": "HZhN-EGTZzE",
                        "bunny_prerecorded_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-short/v-short-1065/v-short-1065_Presentation.mp4?token=evre508d_gYuq8vdkRSbY-VGvGqMUM4LWtr5U8FQzzw&expires=1706590800",
                        "bunny_prerecorded_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-short/v-short-1065/v-short-1065_Presentation.vtt?token=XoPlpshFtt1aYV-p0WQ1zOE_V-RbRFghdldlKa1ql7k&expires=1706590800"
                    },
                    {
                        "slot_id": "v-short-1108",
                        "session_id": "short5",
                        "title": "DataTales: Investigating the Use of Large Language Models for Authoring Data-Driven Articles",
                        "contributors": [
                            "Nicole Sultanum"
                        ],
                        "authors": [
                            "Nicole Sultanum",
                            "Arjun Srinivasan"
                        ],
                        "abstract": "Authoring data-driven articles is a complex process requiring authors to not only analyze data for insights but also craft a cohesive narrative that effectively communicates the insights. Text generation capabilities of contemporary large language models (LLMs) present an opportunity to assist the authoring of data-driven articles and expedite the writing process. In this work, we investigate the feasibility and perceived value of leveraging LLMs to support authors of data-driven articles. We designed a prototype system, DataTales, that leverages a LLM to generate textual narratives accompanying a given chart. Using DataTales as a design probe, we conducted a qualitative study with 11 professionals to evaluate the concept, from which we distilled affordances and opportunities to further integrate LLMs as valuable data-driven article authoring assistants.",
                        "uid": "v-short-1108",
                        "time_stamp": "2023-10-26T22:27:00Z",
                        "time_start": "2023-10-26T22:27:00Z",
                        "time_end": "2023-10-26T22:36:00Z",
                        "paper_type": "short",
                        "keywords": [
                            "large language models; data-driven articles; data-driven authoring; data stories"
                        ],
                        "doi": "",
                        "fno": "",
                        "has_image": true,
                        "has_pdf": true,
                        "paper_award": "",
                        "image_caption": "User Interface of DataTales on the background, and DataTales logo on the foreground. User interface features a stacked bar chart titled \"America's favorite & least favorite months of the year\", the contents of a data story generated for this chart, and a list of other generated stories. A mouse cursor hovers over a text passage, and the corresponding data elements mentioned in the passage are highlighted in the chart.",
                        "external_paper_link": "",
                        "youtube_ff_link": "https://youtu.be/bdeF9R7Ca0I",
                        "youtube_ff_id": "bdeF9R7Ca0I",
                        "bunny_ff_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-short/v-short-1108/v-short-1108_Preview.mp4?token=Q2ijfpU617fN9axVjlpXMWxXyd0Dd8mtBOaDKkcjHvg&expires=1706590800",
                        "bunny_ff_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-short/v-short-1108/v-short-1108_Preview.vtt?token=IFlDFN-4mLFlTrrJok49tBKyB_XXOxTMjXGFBAUXQvc&expires=1706590800",
                        "youtube_prerecorded_link": "https://youtu.be/KRKpifl4M_8",
                        "youtube_prerecorded_id": "KRKpifl4M_8",
                        "bunny_prerecorded_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-short/v-short-1108/v-short-1108_Presentation.mp4?token=QcObTspsqxD9DB00KtZL0nxSbRFsBz0I6BATr_Sdu88&expires=1706590800",
                        "bunny_prerecorded_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-short/v-short-1108/v-short-1108_Presentation.vtt?token=84Ek_99pjmcfXOsmDrvoBnCZ49fG3g3YUr2WEgQ7ypE&expires=1706590800"
                    },
                    {
                        "slot_id": "v-short-1123",
                        "session_id": "short5",
                        "title": "Visualizing Linguistic Diversity of Text Datasets Synthesized by Large Language Models",
                        "contributors": [
                            "Emily Reif"
                        ],
                        "authors": [
                            "Emily Reif",
                            "Minsuk Kahng",
                            "Savvas Petridis"
                        ],
                        "abstract": "Large language models (LLMs) can be used to generate smaller, more refined datasets via few-shot prompting for benchmarking, fine-tuning or other use cases. However, understanding and evaluating these datasets is difficult, and the failure modes of LLM-generated data are still not well understood. Specifically, the data can be repetitive in surprising ways, not only semantically but also syntactically and lexically. We present LinguisticLens, a novel interactive visualization tool for making sense of and analyzing syntactic diversity of LLM-generated datasets. LinguisticLens clusters text along syntactic, lexical, and semantic axes. It supports hierarchical visualization of a text dataset, allowing users to quickly scan for an overview and inspect individual examples. The live demo is available at https://shorturl.at/zHOUV.",
                        "uid": "v-short-1123",
                        "time_stamp": "2023-10-26T22:36:00Z",
                        "time_start": "2023-10-26T22:36:00Z",
                        "time_end": "2023-10-26T22:45:00Z",
                        "paper_type": "short",
                        "keywords": [
                            "Visualization\u2014Text\u2014MLStatsModel"
                        ],
                        "doi": "",
                        "fno": "",
                        "has_image": true,
                        "has_pdf": true,
                        "paper_award": "",
                        "image_caption": "LinguisticLens, a new visualization tool for making sense of text datasets synthesized by large language models (LLMs) and analyzing the diversity of examples.",
                        "external_paper_link": "",
                        "youtube_ff_link": "https://youtu.be/HypBI0K8-1Q",
                        "youtube_ff_id": "HypBI0K8-1Q",
                        "bunny_ff_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-short/v-short-1123/v-short-1123_Preview.mp4?token=FZ5tTTXEi0odCxCuSiSIKTQ5wp9ukBHRsqIKsnKbZ8U&expires=1706590800",
                        "bunny_ff_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-short/v-short-1123/v-short-1123_Preview.vtt?token=Gpa6u_Dj88yX4VLClyCj9M0azG6FHYwrCgcteKGqM9E&expires=1706590800",
                        "youtube_prerecorded_link": "https://youtu.be/9PZyZQrHmTQ",
                        "youtube_prerecorded_id": "9PZyZQrHmTQ",
                        "bunny_prerecorded_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-short/v-short-1123/v-short-1123_Presentation.mp4?token=X-PUYJ2WaFybrpmoB3tgrJXozcxwr5VZiowxhO6Avo0&expires=1706590800",
                        "bunny_prerecorded_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-short/v-short-1123/v-short-1123_Presentation.vtt?token=8AuInIuASf_ydVxk0PriSyPOI6X20ft1CMf4bH4mCFA&expires=1706590800"
                    },
                    {
                        "slot_id": "v-short-1001",
                        "session_id": "short5",
                        "title": "WUDA: Visualizing and Transforming Rotations in Real-Time with Quaternions and Smart Devices",
                        "contributors": [
                            "Slobodan Milanko"
                        ],
                        "authors": [
                            "slobodan milanko"
                        ],
                        "abstract": "The rising popularity of inertial sensing via smart devices is evident, finding use in multidisciplinary applications, such as gesture recognition and image stabilization. Amid the popularity of analyzing raw motion signals and Euler angles, we highlight quaternion rotations as a robust alternative for studying device orientation. Quaternions are a powerful mathematical tool for representing and affecting three-dimensional rotations. Their abstract nature, however, can make it difficult for researchers to visualize the data they provide. Mobile sensing rarely invents practical tools to experiment with this mathematical method. Since visualization effectively communicates data findings, we develop an open-source, real-time app, Wuda, that allows users to observe the orientation of smart devices via inertial sensing. Wuda helps users reduce dimensional complexity and intuitively study quaternion transformations. We demonstrate the practical nature of Wuda in the context of fitness tracking.",
                        "uid": "v-short-1001",
                        "time_stamp": "2023-10-26T22:45:00Z",
                        "time_start": "2023-10-26T22:45:00Z",
                        "time_end": "2023-10-26T22:54:00Z",
                        "paper_type": "short",
                        "keywords": [
                            "Mathematical Foundations & Numerical Methods; Machine Learning, Statistics, Modelling, and Simulation Applications"
                        ],
                        "doi": "",
                        "fno": "",
                        "has_image": true,
                        "has_pdf": true,
                        "paper_award": "",
                        "image_caption": "Wuda draws an interactive sphere visualization that embeds a Geodesic Icosahedron with labeled faces. It allows you to study smart device rotations, angles, and transformations in real-time.",
                        "external_paper_link": "",
                        "youtube_ff_link": "https://youtu.be/nopO0_yo6Qk",
                        "youtube_ff_id": "nopO0_yo6Qk",
                        "bunny_ff_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-short/v-short-1001/v-short-1001_Preview.mp4?token=7LUCrl6hC_DvF-3dZ7aANyoOHmwT6aTk9Bx2IvlDGSc&expires=1706590800",
                        "bunny_ff_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-short/v-short-1001/v-short-1001_Preview.vtt?token=ZAu7mHMYJFPFTSsplP5lf-7YXSR7Ht8jGItKCfEC8p0&expires=1706590800",
                        "youtube_prerecorded_link": "https://youtu.be/CgBWrbqxo1w",
                        "youtube_prerecorded_id": "CgBWrbqxo1w",
                        "bunny_prerecorded_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-short/v-short-1001/v-short-1001_Presentation.mp4?token=EW1OMmfHjE_V--fM1dcg9gCL2npwh2siKLz3Sb7GZio&expires=1706590800",
                        "bunny_prerecorded_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-short/v-short-1001/v-short-1001_Presentation.vtt?token=rLlTuEcsMWx15XH6sJmsj4O2Oa-Rsq-0ZaA_Q75RPD8&expires=1706590800"
                    },
                    {
                        "slot_id": "v-short-1006",
                        "session_id": "short5",
                        "title": "ScatterUQ: Interactive Uncertainty Visualizations for Multiclass Deep Learning Problems",
                        "contributors": [
                            "Harry Li"
                        ],
                        "authors": [
                            "Harry Li",
                            "Steven Jorgensen",
                            "John Holodnak",
                            "Allan Wollaber"
                        ],
                        "abstract": "Recently, uncertainty-aware deep learning methods for multiclass labeling problems have been developed that provide calibrated class prediction probabilities and out-of-distribution (OOD) indicators, letting machine learning (ML) consumers and engineers gauge a model's confidence in its predictions. However, this extra neural network prediction information is challenging to scalably convey visually for arbitrary data sources under multiple uncertainty contexts. To address these challenges, we present ScatterUQ, an interactive system that provides targeted visualizations to allow users to better understand model performance in context-driven uncertainty settings. ScatterUQ leverages recent advances in distance-aware neural networks, together with dimensionality reduction techniques, to construct robust, 2-D scatter plots explaining why a model predicts a test example to be (1) in-distribution and of a particular class, (2) in-distribution but unsure of the class, and (3) out-of-distribution. ML consumers and engineers can visually compare the salient features of test samples with training examples through the use of a ``hover callback'' to understand model uncertainty performance and decide follow up courses of action.   We demonstrate the effectiveness of ScatterUQ to explain model uncertainty for a multiclass image classification on a distance-aware neural network trained on Fashion-MNIST and tested on Fashion-MNIST (in distribution) and MNIST digits (out of distribution), as well as a deep learning model for a cyber dataset. We quantitatively evaluate dimensionality reduction techniques to optimize our contextually driven UQ visualizations. Our results indicate that the ScatterUQ system should scale to arbitrary, multiclass datasets. Our code is available at https://github.com/mit-ll-responsible-ai/equine-webapp.",
                        "uid": "v-short-1006",
                        "time_stamp": "2023-10-26T22:54:00Z",
                        "time_start": "2023-10-26T22:54:00Z",
                        "time_end": "2023-10-26T23:03:00Z",
                        "paper_type": "short",
                        "keywords": [
                            "Uncertainty quantification - Machine learning -Dimensionality reduction - Visualization - Explainable AI"
                        ],
                        "doi": "",
                        "fno": "",
                        "has_image": true,
                        "has_pdf": true,
                        "paper_award": "",
                        "image_caption": "ScatterUQ plot of an out of distribution MNIST test sample (left sidebar and gray dot), ten Fashion-MNIST training examplesfrom the closest class Sandal (blue dots), and the nearest Sandal training example (right sidebar). ScatterUQ uses dimensionality reduction to visualize neural network uncertainty to help end users make more informed decisions and to help machine learning engineers improve their models.",
                        "external_paper_link": "",
                        "youtube_ff_link": "https://youtu.be/nhZ_O5jhDk8",
                        "youtube_ff_id": "nhZ_O5jhDk8",
                        "bunny_ff_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-short/v-short-1006/v-short-1006_Preview.mp4?token=3yzVUOy7895MuUqE005qz5asexf4MYkXdGG9vPgqBg0&expires=1706590800",
                        "bunny_ff_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-short/v-short-1006/v-short-1006_Preview.vtt?token=VuKCUWf21nWXcIWu9_rl9sOfOXNoM1BEuE-EObSnbdU&expires=1706590800",
                        "youtube_prerecorded_link": "https://youtu.be/SEdSXHnKihc",
                        "youtube_prerecorded_id": "SEdSXHnKihc",
                        "bunny_prerecorded_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-short/v-short-1006/v-short-1006_Presentation.mp4?token=GNz9kA80OL2JYXWJNhyfPlPpCjWROnr_uTHuhFDnYQY&expires=1706590800",
                        "bunny_prerecorded_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-short/v-short-1006/v-short-1006_Presentation.vtt?token=8ciqHEEY7fSW7nQTT2nQVAPXzrO6K6_h1KpWRX0nF6E&expires=1706590800"
                    },
                    {
                        "slot_id": "v-short-1060",
                        "session_id": "short5",
                        "title": "Combining Degree of Interest Functions and Progressive Visualization",
                        "contributors": [
                            "Marius Hogr\u00e4fer"
                        ],
                        "authors": [
                            "Marius Hogr\u00e4fer",
                            "Dominik Moritz",
                            "Adam Perer",
                            "Hans-J\u00f6rg Schulz"
                        ],
                        "abstract": "When visualizing large datasets, an important goal is to emphasize data that is relevant to the task at hand. A common way of achieving this is to compute the relevance of the data using degree of interest (DOI) functions, which apply a scenario-specific metric to quantify the data items according to their relevance to the users and their tasks. These DOI values can then be used to adjust the visual encoding through mechanisms like focus+context or information hiding. For datasets too large to be visualized at once, an alternative approach is to visualize it progressively in chunks, allowing analysts to reason about partial results and concluding their analysis much earlier than had they waited for all data. Combining the advantages of both approaches to tailor the visualization seems synergistic, yet, in practice turns out to be challenging, as DOI functions require the context of all data to produce useful values, requiring lengthy computations that break analysts\u2019 flow in progressive visualization. In this paper, we propose an approach for uniting DOI functions with progressive visualization. We first introduce a new model for quantifying the user interest in analysis scenarios where the data is only partially available, by computing the interest for available data and predicting it for the rest. We then propose regression trees for implementing this approach in practice and evaluate it in benchmarks. With DOI values now available for progressive visualization as well, our approach opens the door for tailoring the visualization of large datasets to the analysis task at interactive update rates.",
                        "uid": "v-short-1060",
                        "time_stamp": "2023-10-26T23:03:00Z",
                        "time_start": "2023-10-26T23:03:00Z",
                        "time_end": "2023-10-26T23:12:00Z",
                        "paper_type": "short",
                        "keywords": [
                            "Progressive visualization, degree-of-interest functions"
                        ],
                        "doi": "",
                        "fno": "",
                        "has_image": true,
                        "has_pdf": true,
                        "paper_award": "",
                        "image_caption": "Our model that enables DOI functions on progressive visualization: Based on the current user interest, data that is deemed interesting gets retrieved from the database and integrated with the data already in the visualization, using the interest model to identify and update outdated interest values.",
                        "external_paper_link": "",
                        "youtube_ff_link": "https://youtu.be/JCNuMnDb4d4",
                        "youtube_ff_id": "JCNuMnDb4d4",
                        "bunny_ff_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-short/v-short-1060/v-short-1060_Preview.mp4?token=oYfmzqNgkQFM47iSPRfuTeBKz_tLPALfewiKZTudR9s&expires=1706590800",
                        "bunny_ff_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-short/v-short-1060/v-short-1060_Preview.vtt?token=Bldh9PC4qTuCh-RqoxeMkuYNdGjaRDWqTpH2t3ubs3U&expires=1706590800",
                        "youtube_prerecorded_link": "https://youtu.be/79h2iTLwJ2w",
                        "youtube_prerecorded_id": "79h2iTLwJ2w",
                        "bunny_prerecorded_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-short/v-short-1060/v-short-1060_Presentation.mp4?token=lZNjmNm7wUbwNp_XO1KJ4uoEgWScelS7-TYgpd68oso&expires=1706590800",
                        "bunny_prerecorded_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-short/v-short-1060/v-short-1060_Presentation.vtt?token=2GxPHVSvJblowlIqWZE-4TV2RYAGLMIL_qwl3kY4TDk&expires=1706590800"
                    }
                ]
            },
            {
                "title": "CoVID-19 / Bioinformatics / Visual Analytics",
                "session_id": "short6",
                "event_prefix": "v-short",
                "track": "oneohfour",
                "session_image": "short6.png",
                "chair": [
                    "Alfie Abdul-Rahman"
                ],
                "time_start": "2023-10-25T04:45:00Z",
                "time_end": "2023-10-25T06:00:00Z",
                "discord_category": "",
                "discord_channel": "104",
                "discord_channel_id": "1161741240665124954",
                "discord_link": "https://discordapp.com/channels/1161400451619627068/1161741240665124954",
                "zoom_private_meeting": "",
                "zoom_private_password": "",
                "zoom_private_link": "",
                "zoom_broadcast_link": "",
                "ff_link": "https://youtu.be/TpwSTq8WxrM",
                "time_slots": [
                    {
                        "slot_id": "v-short-1102",
                        "session_id": "short6",
                        "title": "The Role of Visualization in Genomics Data Analysis Workflows: The Interviews",
                        "contributors": [
                            "Sehi L'Yi"
                        ],
                        "authors": [
                            "Sehi L'Yi",
                            "Qianwen Wang",
                            "Nils Gehlenborg"
                        ],
                        "abstract": "The diversity of genome-mapped data and analysis tasks makes it challenging for a single visualization tool to fulfill all visualization needs. To design a visualization tool that supports various genomics workflows of users, it is critical to first gain insights into the diverse workflows and the limitations of existing genomics tools for supporting them. In this paper, we conducted semi-structured interviews (N=9) to understand the role of visualization in genomics data analysis workflows. Our main goals were to identify various genomics workflows, from data analysis to visual exploration and presentation, and to observe challenges that genomics analysts encounter in these workflows when using existing tools. Through the interviews, we found several unique characteristics of genomics workflows, such as the use of multiple visualization tools and many repetitive tasks, which can significantly affect the overall performance. Based on our findings, we discuss implications for designing effective visualization authoring tools that tightly support genomics workflows, such as supporting automation and reproducibility.",
                        "uid": "v-short-1102",
                        "time_stamp": "2023-10-25T04:45:00Z",
                        "time_start": "2023-10-25T04:45:00Z",
                        "time_end": "2023-10-25T04:54:00Z",
                        "paper_type": "short",
                        "keywords": [
                            "Large-Scale Data Techniques ; Life Sciences, Health, Medicine, Biology, Bioinformatics, Genomics ; Guidelines ; Process/Workflow Design ; Human-Subjects Qualitative Studies"
                        ],
                        "doi": "",
                        "fno": "",
                        "has_image": true,
                        "has_pdf": true,
                        "paper_award": "",
                        "image_caption": "The summary of workflows of all interview participants. These flow charts are drawn collaboratively with participants during the interviews, reflecting their everyday workflows. The names of the tools used in individual stages are labeled right below circular nodes.",
                        "external_paper_link": "",
                        "youtube_ff_link": "https://youtu.be/-AZey4x19N4",
                        "youtube_ff_id": "-AZey4x19N4",
                        "bunny_ff_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-short/v-short-1102/v-short-1102_Preview.mp4?token=tFdIcIsDrtwvOnXhA5iHXbX8ZNET_r5XEnVNZ1dYkOM&expires=1706590800",
                        "bunny_ff_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-short/v-short-1102/v-short-1102_Preview.vtt?token=4Z4_Tv4r4nJU45MxY4yvZVq4be7KkgETtOXTZq_41o8&expires=1706590800",
                        "youtube_prerecorded_link": "https://youtu.be/_oIsPQvZxMQ",
                        "youtube_prerecorded_id": "_oIsPQvZxMQ",
                        "bunny_prerecorded_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-short/v-short-1102/v-short-1102_Presentation.mp4?token=_2UL4xYzzHGPmICL7FYGrAwBokxxkviOhfNf-_jbAuI&expires=1706590800",
                        "bunny_prerecorded_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-short/v-short-1102/v-short-1102_Presentation.vtt?token=OpECw7AZuR80NBFialQcqq22m1jU_OuulnlRsZMATNI&expires=1706590800"
                    },
                    {
                        "slot_id": "v-short-1147",
                        "session_id": "short6",
                        "title": "Vis-SPLIT: Interactive Hierarchical Modeling for mRNA Expression Classification",
                        "contributors": [
                            "Braden Roper"
                        ],
                        "authors": [
                            "Braden Roper",
                            "James C. Mathews",
                            "Saad Nadeem",
                            "Ji Hwan Park"
                        ],
                        "abstract": "We propose an interactive visual analytics tool, Vis-SPLIT, for partitioning a population of individuals into groups with similar gene signatures. Vis-SPLIT allows users to interactively explore a dataset and exploit visual separations to build a classification model for specific cancers. The visualization components reveal gene expression and correlation to assist specific partitioning decisions, while also providing overviews for the decision model and clustered genetic signatures. We demonstrate the effectiveness of our framework through a case study and evaluate its usability with domain experts. Our results show that Vis-SPLIT can classify patients based on their genetic signatures to effectively gain insights into RNA sequencing data, as compared to an existing classification system.",
                        "uid": "v-short-1147",
                        "time_stamp": "2023-10-25T04:54:00Z",
                        "time_start": "2023-10-25T04:54:00Z",
                        "time_end": "2023-10-25T05:03:00Z",
                        "paper_type": "short",
                        "keywords": [
                            "Human-centered computing\u2014Visualization\u2014Visualization application domains\u2014Visual analytics"
                        ],
                        "doi": "",
                        "fno": "",
                        "has_image": true,
                        "has_pdf": true,
                        "paper_award": "",
                        "image_caption": "An overview of the Vis-SPLIT tool. (A) The Hierarchical Overview is an abstract view of the current clusters. (B) The Heatmap Overview displays the patterns for the expression values of genes in each cluster. (C) The Survival Analysis View visualizes survival curves for each cluster. (D) The PCA View allows users to view or split the selected node in the Hierarchical Overview, and includes (D.1) the Projection depicting individuals in 2D, placed based on genetic expression, (D.2-D.3) axis-aligned heatmaps displaying the expression values of genes, and (D.4) the Feature Loadings Plot showing current gene contributions.",
                        "external_paper_link": "",
                        "youtube_ff_link": "https://youtu.be/E6B61wJqbeQ",
                        "youtube_ff_id": "E6B61wJqbeQ",
                        "bunny_ff_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-short/v-short-1147/v-short-1147_Preview.mp4?token=p5bCvz5PO1nVHghfOKAIKTYBwjuP3MwDex-OHr63Ggo&expires=1706590800",
                        "bunny_ff_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-short/v-short-1147/v-short-1147_Preview.vtt?token=UqNuK9_4Ejdyw7CaWvEtEv9-Zadbo22d7kJIORTWhtQ&expires=1706590800",
                        "youtube_prerecorded_link": "https://youtu.be/JqQdV5oUhJA",
                        "youtube_prerecorded_id": "JqQdV5oUhJA",
                        "bunny_prerecorded_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-short/v-short-1147/v-short-1147_Presentation.mp4?token=V3P8XbIFrejUy6yzzikjoteDEG3s1yy4zerJmwEQWb4&expires=1706590800",
                        "bunny_prerecorded_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-short/v-short-1147/v-short-1147_Presentation.vtt?token=mviZ6ixy28IF6rwC17ajV1CqdnHwyfX8qPyfLa15HAs&expires=1706590800"
                    },
                    {
                        "slot_id": "v-short-1107",
                        "session_id": "short6",
                        "title": "Enabling Multimodal User Interactions for Genomics Visualization Creation",
                        "contributors": [
                            "Qianwen Wang"
                        ],
                        "authors": [
                            "Qianwen Wang",
                            "Xiao Liu",
                            "Man Qing Liang",
                            "Sehi L'Yi",
                            "Nils Gehlenborg"
                        ],
                        "abstract": "Visualization plays an important role in extracting insights from  complex and large-scale genomics data. Traditional graphical user interfaces (GUIs) offer limited flexibility for custom visualizations. Our prior work, Gosling, enables expressive visualization creation using a grammar-based approach, but beginners may face challenges in constructing complex visualizations. To address this, we explore multimodal interactions, including sketches, example images, and natural language inputs, to streamline visualization creation. Specifically, we customize two deep learning models (YOLO v7 and GPT3.5) to interpret user interactions and convert them into Gosling specifications. A workflow is proposed to progressively introduce and integrate multimodal interactions. We then present use cases demonstrating their effectiveness and identify challenges and opportunities for future research.",
                        "uid": "v-short-1107",
                        "time_stamp": "2023-10-25T05:03:00Z",
                        "time_start": "2023-10-25T05:03:00Z",
                        "time_end": "2023-10-25T05:12:00Z",
                        "paper_type": "short",
                        "keywords": [
                            "Human-centered computing\u2014Visualization\u2014Visualization systems and tools; Human-centered computing\u2014Interaction Design;"
                        ],
                        "doi": "",
                        "fno": "",
                        "has_image": true,
                        "has_pdf": true,
                        "paper_award": "",
                        "image_caption": "AutoGosling facilitates the creation of genomics visualizations by enabling multimodal interactions. Instead of directly constructing the grammar-based visualization specifications users express design intentions through sketches/examples images, a template GUI, and natural language commands, which are interpreted by AutoGosling and converted into interactive genomics visualizations. Interactions are progressively introduced to minimize information overload and reduce mode-switching.",
                        "external_paper_link": "",
                        "youtube_ff_link": "https://youtu.be/-Hsw6MJZqTA",
                        "youtube_ff_id": "-Hsw6MJZqTA",
                        "bunny_ff_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-short/v-short-1107/v-short-1107_Preview.mp4?token=jl-GSPD4VBacBIOEz8EkzGYGcyKlJk_V2tVUnPRnqqE&expires=1706590800",
                        "bunny_ff_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-short/v-short-1107/v-short-1107_Preview.vtt?token=Xql7Wag2jdq2aiIhfB5WFHCvicwJ2onrwo66-MnbqSk&expires=1706590800",
                        "youtube_prerecorded_link": "",
                        "youtube_prerecorded_id": "",
                        "bunny_prerecorded_link": "",
                        "bunny_prerecorded_subtitles": ""
                    },
                    {
                        "slot_id": "v-short-1067",
                        "session_id": "short6",
                        "title": "Simulating the Geometric Growth of the Marine Sponge Crella Incrustans",
                        "contributors": [
                            "Andrew Chalmers"
                        ],
                        "authors": [
                            "Josh O'Hagan",
                            "Andrew Chalmers",
                            "Taehyun James Rhee"
                        ],
                        "abstract": "Simulating marine sponge growth helps marine biologists analyze, measure, and predict the effects that the marine environment has on marine sponges, and vice versa. This paper describes a way to simulate and grow geometric models of the marine sponge Crella incrustans while considering environmental factors including fluid flow and nutrients. The simulation improves upon prior work by changing the skeletal architecture of the sponge in the growth model to better suit the structure of Crella incrustans. The change in skeletal architecture and other simulation parameters are then evaluated qualitatively against photos of a real-life Crella incrustans sponge. The results support the hypothesis that changing the skeletal architecture from radiate accretive to Halichondrid produces a sponge model which is closer in resemblance to Crella incrustans than the prior work.",
                        "uid": "v-short-1067",
                        "time_stamp": "2023-10-25T05:12:00Z",
                        "time_start": "2023-10-25T05:12:00Z",
                        "time_end": "2023-10-25T05:21:00Z",
                        "paper_type": "short",
                        "keywords": [
                            "Computing methodologies\u2014Computer graphics\u2014Shape modeling; Computing methodologies\u2014Modeling and simulation; Human-centered computing\u2014Visualization\u2014Scientific visualization"
                        ],
                        "doi": "",
                        "fno": "",
                        "has_image": true,
                        "has_pdf": true,
                        "paper_award": "",
                        "image_caption": "Overview of simulating the marine sponge Crella incrustans. Given the (a) skeletal architecture that resembles Crella  incrustans and the (b) simulation box for fluid and nutrients, we are able to (c) simulate sponge growth using the skeletal  architecture to guide the growth pattern. This results in a (d) 3D mesh of Crella incrustans.",
                        "external_paper_link": "",
                        "youtube_ff_link": "https://youtu.be/CVfMkR5820c",
                        "youtube_ff_id": "CVfMkR5820c",
                        "bunny_ff_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-short/v-short-1067/v-short-1067_Preview.mp4?token=KJPB1UTslo_bVj6BZsTsCrTICjIux-AZ2Yb1z7NkryY&expires=1706590800",
                        "bunny_ff_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-short/v-short-1067/v-short-1067_Preview.vtt?token=u_quubtP98kUYooDFtpAoE7sWffSuqzWO4afTIx25M4&expires=1706590800",
                        "youtube_prerecorded_link": "https://youtu.be/VZmpHePDRF8",
                        "youtube_prerecorded_id": "VZmpHePDRF8",
                        "bunny_prerecorded_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-short/v-short-1067/v-short-1067_Presentation.mp4?token=gLfCPx3omketvJGxbPlnKQUua5_mORj3suB_SeaKfrE&expires=1706590800",
                        "bunny_prerecorded_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-short/v-short-1067/v-short-1067_Presentation.vtt?token=a1Inw3x8b2ah-yrxJMO2RU3XZOqOJuqRyR1JGEVnHw8&expires=1706590800"
                    },
                    {
                        "slot_id": "v-short-1163",
                        "session_id": "short6",
                        "title": "How \"Applied\" is Fifteen Years of VAST conference?",
                        "contributors": [
                            "Lei Shi"
                        ],
                        "authors": [
                            "Lei Shi",
                            "Lei Xia",
                            "Zipeng Liu",
                            "Ye Sun",
                            "Huijie Guo",
                            "Klaus Mueller"
                        ],
                        "abstract": "Visual analytics (VA) science and technology emerge as a promising methodology in visualization and data science in the new century. Application-driven research continues to contribute significantly to the development of VA, as well as in a broader scope of VIS. However, existing studies on the trend and impact of VA/VIS application research stay at a commentary and subjective level, using methods such as panel discussions and expert interviews. On the contrary, this work presents a first study on VA application research using data-driven methodology with cutting-edge machine learning algorithms, achieving both objective and scalable goals. Experiment results demonstrate the validity of our method with high F1 scores up to 0.89 for the inference of VA application papers on both the expert-labeled benchmark dataset and two external validation data sources. Inference results on 15 years of VAST conference papers also narrate interesting patterns in VA application research's origin, trend, and constitution.",
                        "uid": "v-short-1163",
                        "time_stamp": "2023-10-25T05:21:00Z",
                        "time_start": "2023-10-25T05:21:00Z",
                        "time_end": "2023-10-25T05:30:00Z",
                        "paper_type": "short",
                        "keywords": [
                            "visual analytics, VAST, application"
                        ],
                        "doi": "",
                        "fno": "",
                        "has_image": true,
                        "has_pdf": true,
                        "paper_award": "",
                        "image_caption": "The dynamics of the number of VAST (non-)application papers and their penetration rates.",
                        "external_paper_link": "",
                        "youtube_ff_link": "https://youtu.be/E6z4idyZ7Lo",
                        "youtube_ff_id": "E6z4idyZ7Lo",
                        "bunny_ff_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-short/v-short-1163/v-short-1163_Preview.mp4?token=Jw2C7WW_17y1HWmnUtm4zLhNA4fG6gTEDG3xCzsHh-I&expires=1706590800",
                        "bunny_ff_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-short/v-short-1163/v-short-1163_Preview.vtt?token=yDuS2_BpfsgwgZfeoaUlAmp7LKNe77xxs1UYUTMqVVQ&expires=1706590800",
                        "youtube_prerecorded_link": "https://youtu.be/UV08ts2-3Ac",
                        "youtube_prerecorded_id": "UV08ts2-3Ac",
                        "bunny_prerecorded_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-short/v-short-1163/v-short-1163_Presentation.mp4?token=emQKzBKN6Z6v9iR-HrsPkZT7AkFOvB0NxVqWeVbcAN8&expires=1706590800",
                        "bunny_prerecorded_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-short/v-short-1163/v-short-1163_Presentation.vtt?token=onUmkoGpSu724bSEgpqAHWLhBiC1VVYfpXTEwyoVcYc&expires=1706590800"
                    },
                    {
                        "slot_id": "v-short-1165",
                        "session_id": "short6",
                        "title": "CLEVER: A Framework for Connecting Lived Experiences with Visualisation of Electronic Records",
                        "contributors": [
                            "Mai Elshehaly"
                        ],
                        "authors": [
                            "Mai Elshehaly",
                            "Lucy H Eddy",
                            "Mark Mon-Williams"
                        ],
                        "abstract": "The disconnect between insights generated from data and real-life practices of decision makers presents a number of open questions for visual analytics (VA). In public service planning, routine data are often perceived as unavailable, biased, incomplete and inconsistent across services. Decision makers often rely on qualitative data - sometimes collected through co-production - to understand the lived experience of communities before formulating a decision. We followed a subjectivist case study approach and immersed ourselves in ongoing co-production activities over the course of one year, to capture how VA can support the dialogue between population health decision-makers and the communities they serve. We present a framework for Connecting Lived Experiences with Visualisation of Electronic Records (CLEVER). The framework regards visualisation as a central component in a complex adaptive decision-making ecosystem and highlights the need to structure domain knowledge across decision contexts in Population Health Management (PHM) at clinical-, service- and district-levels. Our process for developing an initial framework comprised three steps: (i) we elicited decision-making tasks through a series of qualitative data collection activities; (ii) we developed a preliminary domain model to capture data views and a subjective view of the world through human stories; and (iii) we developed a series of visualisation prototypes to instantiate the framework and demonstrated them regularly to stakeholders. In future work, we will conduct \u2018deep dives\u2019 to systematically study the role of VA in individual stages of the framework.",
                        "uid": "v-short-1165",
                        "time_stamp": "2023-10-25T05:30:00Z",
                        "time_start": "2023-10-25T05:30:00Z",
                        "time_end": "2023-10-25T05:39:00Z",
                        "paper_type": "short",
                        "keywords": [
                            "Human-centered computing\u2014Visualization\u2014Visualization design and evaluation methods"
                        ],
                        "doi": "",
                        "fno": "",
                        "has_image": true,
                        "has_pdf": true,
                        "paper_award": "",
                        "image_caption": "Initial framework for Connecting Lived Experiences with Visualisation of Electronic Records (CLEVER), in Population Health Management (PHM). This initial framework is based on a subjectivist case study approach. The rows represent contexts of soft intelligence that support decision making, with increasing levels of agency and knowledge centralisation (bottom to top). The columns capture examples of soft intelligence that can emerge in these contexts at different stages of formulating decisions (left to right). Deep dives are required in each of the individual contexts to iterate the framework and inform design guidelines.",
                        "external_paper_link": "",
                        "youtube_ff_link": "https://youtu.be/mo7Lt-6yeWA",
                        "youtube_ff_id": "mo7Lt-6yeWA",
                        "bunny_ff_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-short/v-short-1165/v-short-1165_Preview.mp4?token=eTi_PyACa6gwQd2j9hNchryZDFnRgKQYhdnsnipSrig&expires=1706590800",
                        "bunny_ff_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-short/v-short-1165/v-short-1165_Preview.vtt?token=kUwfh71nWChmijC1piniV9b2pQcqHa_9x5tFLhtIifo&expires=1706590800",
                        "youtube_prerecorded_link": "https://youtu.be/X8eHVtaCCOA",
                        "youtube_prerecorded_id": "X8eHVtaCCOA",
                        "bunny_prerecorded_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-short/v-short-1165/v-short-1165_Presentation.mp4?token=6ffnkBPn72uJ2uH4x7pE9sjlkyidReymmXbgZKLiCIo&expires=1706590800",
                        "bunny_prerecorded_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-short/v-short-1165/v-short-1165_Presentation.vtt?token=eZWXtfD_PpCXFJFTdu_1EHMdu1rAPog84nalXQlsCaQ&expires=1706590800"
                    },
                    {
                        "slot_id": "v-short-1158",
                        "session_id": "short6",
                        "title": "Design of an Ecological Visual Analytics Interface for Operators of Time-Constant Processes",
                        "contributors": [
                            "Emmanuel Brorsson"
                        ],
                        "authors": [
                            "Elmira Zohrevandi",
                            "Emmanuel Brorsson",
                            "Andreas Darnell",
                            "Magnus B\u00e5ng",
                            "Jonas Lundberg",
                            "Anders Ynnerman"
                        ],
                        "abstract": "In industrial applications where the physical parameters are highly interconnected, keeping the process flow steady is a major concern for the operators. This is caused by the sensitivity of system to the process dynamics. As a result, a slight adjustment to a control parameter can significantly affect the efficiency of the system and thus impact the financial gain. Paper pulp production is an example of such a process, where operators continuously investigate the potential of changes in the process and predict the consequences of an adjustment before making a decision. Process parameter adjustments prescribed by simulated control models cannot be fully trusted as the external disturbances and the process inherent variabilities cannot be fully incorporated into the simulations. Therefore, to assess the viability of a strategy, operators often compare the situation with the historical records and trends during which the processes in the plant ran steadily. While previous research has mostly focused on developing advanced control models to simulate complex pulp production process, this work aims to support operators analytical reasoning by provision of effective data visualization. The contributions of our design study include a domain problem characterization and a linked-view visual encoding design, which aims to enhance operator's mental models independent of particular users or scenarios. Finally, by reflecting on the advantages of our choice of task abstraction technique, inherited from the ecological interface design framework [5], we reason for the generalizability of our approach to similar industrial applications.",
                        "uid": "v-short-1158",
                        "time_stamp": "2023-10-25T05:39:00Z",
                        "time_start": "2023-10-25T05:39:00Z",
                        "time_end": "2023-10-25T05:48:00Z",
                        "paper_type": "short",
                        "keywords": [
                            "Visual analytics interfaces, Design study, Focus+context techniques, Linked-view interfaces, Time-constant processes"
                        ],
                        "doi": "",
                        "fno": "",
                        "has_image": true,
                        "has_pdf": true,
                        "paper_award": "",
                        "image_caption": "On the proposed focus+context linked-view interface, the topological view enables interaction with an ML classifier which visualizes the historical data that are most similar to the current situation. The circle chart provides insight on model accuracy (mapped by color intensity) and performance (mapped on the radial axis). The situation view (i.e. the focus view) shows the key performance parameter profile through process cycle and its deviations from optimality.  The strategy view shows variation limits for relevant parameters obtained from the historical data. The control view shows the consequences of an adjustment predicted by the ML model for the selected parameter.",
                        "external_paper_link": "",
                        "youtube_ff_link": "https://youtu.be/I6ZAl5JmP9w",
                        "youtube_ff_id": "I6ZAl5JmP9w",
                        "bunny_ff_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-short/v-short-1158/v-short-1158_Preview.mp4?token=Zz-G_pPAFIEnnaN2KApRIjP0PMrECF61vfKzFix3d7U&expires=1706590800",
                        "bunny_ff_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-short/v-short-1158/v-short-1158_Preview.vtt?token=IjvQaWDq0_AIrC93PFTg42i4p5Ukej9JRfqdTSYeESY&expires=1706590800",
                        "youtube_prerecorded_link": "https://youtu.be/gwJGBgZD2xA",
                        "youtube_prerecorded_id": "gwJGBgZD2xA",
                        "bunny_prerecorded_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-short/v-short-1158/v-short-1158_Presentation.mp4?token=9bdDYafWp8sA9LwGH5X-joV_yLISuf9apE9-cQla06k&expires=1706590800",
                        "bunny_prerecorded_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-short/v-short-1158/v-short-1158_Presentation.vtt?token=IAjbiR108S_-qnR5jyYli9FM-LyJr8lbuHdu4bZD3jQ&expires=1706590800"
                    }
                ]
            }
        ]
    },
    "v-tvcg": {
        "event": "TVCG Invited Partnership Presentations",
        "long_name": "TVCG Invited Partnership Presentations",
        "event_type": "invited",
        "event_prefix": "v-tvcg",
        "event_description": "",
        "event_url": "",
        "organizers": [],
        "sessions": []
    },
    "v-cga": {
        "event": "CG&A Invited Partnership Presentations",
        "long_name": "CG&A Invited Partnership Presentations",
        "event_type": "invited",
        "event_prefix": "v-cga",
        "event_description": "",
        "event_url": "",
        "organizers": [],
        "sessions": [
            {
                "title": "Systems, Techniques, and Applications",
                "session_id": "cga1",
                "event_prefix": "v-cga",
                "track": "oneohfour",
                "session_image": "cga1.png",
                "chair": [
                    "Joseph Cottam"
                ],
                "time_start": "2023-10-24T23:45:00Z",
                "time_end": "2023-10-25T01:00:00Z",
                "discord_category": "",
                "discord_channel": "104",
                "discord_channel_id": "1161741240665124954",
                "discord_link": "https://discordapp.com/channels/1161400451619627068/1161741240665124954",
                "zoom_private_meeting": "",
                "zoom_private_password": "",
                "zoom_private_link": "",
                "zoom_broadcast_link": "",
                "ff_link": "https://youtu.be/DeRhgxPidCA",
                "time_slots": [
                    {
                        "slot_id": "v-cga-9769955",
                        "session_id": "cga1",
                        "title": "Giga Graph Cities: Their Buckets, Buildings, Waves, and Fragments",
                        "contributors": [
                            "James Abello"
                        ],
                        "authors": [
                            "James Abello",
                            "Haoyang Zhang",
                            "Daniel Nakhimovich",
                            "Chengguizi Han",
                            "Mridul Aanjaneya"
                        ],
                        "abstract": "",
                        "uid": "v-cga-9769955",
                        "time_stamp": "2023-10-24T23:45:00Z",
                        "time_start": "2023-10-24T23:45:00Z",
                        "time_end": "2023-10-24T23:57:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "doi": "",
                        "fno": "9769955",
                        "has_image": true,
                        "has_pdf": false,
                        "paper_award": "",
                        "image_caption": "Graph Cities allow visual exploration of billion-edge graphs. Challenges are the IO and the screen bottlenecks. Graphs get decomposed into edge layers called fixed points. This allows the disentangling of hairballs. Each fixed point is represented as a building, and all buildings are layout as a Graph City. The current implementation is scaled up to 1.8 billion edges. The rendering time is about 12 seconds. Please check out our paper Giga Graph Cities.",
                        "external_paper_link": "",
                        "youtube_ff_link": "https://youtu.be/3T8jzcMeKmg",
                        "youtube_ff_id": "3T8jzcMeKmg",
                        "bunny_ff_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-cga/v-cga-9769955/v-cga-9769955_Preview.mp4?token=G26APEkwKXe2KIhOBKgHyyh_QVbTlR5ezyy52-PZ_dw&expires=1706590800",
                        "bunny_ff_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-cga/v-cga-9769955/v-cga-9769955_Preview.vtt?token=jTK_v8xkQ9WqT0aube1Rn7bQnJRFEeID_-okulRETgg&expires=1706590800",
                        "youtube_prerecorded_link": "https://youtu.be/BIvAd71gxLA",
                        "youtube_prerecorded_id": "BIvAd71gxLA",
                        "bunny_prerecorded_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-cga/v-cga-9769955/v-cga-9769955_Presentation.mp4?token=U7gDYuV92mkvPLTsCMBH61qPN7YPpUJBWeBPB25jF4w&expires=1706590800",
                        "bunny_prerecorded_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-cga/v-cga-9769955/v-cga-9769955_Presentation.vtt?token=eU5UK6LLVyGndnMzA2AnvBBqLmnxAqHAjS_MiAL6mKs&expires=1706590800"
                    },
                    {
                        "slot_id": "v-cga-9787979",
                        "session_id": "cga1",
                        "title": "Narrative In Situ Visual Analysis for Large-Scale Ocean Eddy Evolution",
                        "contributors": [
                            "Xiaoyang Han"
                        ],
                        "authors": [
                            "Xiaoyang Han",
                            "Xiaomin Yu",
                            "Guan Li",
                            "Jun Liu",
                            "Ying Zhao",
                            "Guihua Shan"
                        ],
                        "abstract": "",
                        "uid": "v-cga-9787979",
                        "time_stamp": "2023-10-24T23:57:00Z",
                        "time_start": "2023-10-24T23:57:00Z",
                        "time_end": "2023-10-25T00:09:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "doi": "",
                        "fno": "9787979",
                        "has_image": true,
                        "has_pdf": false,
                        "paper_award": "",
                        "image_caption": "The global view of our two views, which is used for high-precision streamline interaction and regional eddy characteristics analysis of large ocean areas.",
                        "external_paper_link": "",
                        "youtube_ff_link": "https://youtu.be/8V8ffFatt3k",
                        "youtube_ff_id": "8V8ffFatt3k",
                        "bunny_ff_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-cga/v-cga-9787979/v-cga-9787979_Preview.mp4?token=QeznKLxoPqCoUDB278l0u1o8k-Laqcd2QTope8U4low&expires=1706590800",
                        "bunny_ff_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-cga/v-cga-9787979/v-cga-9787979_Preview.vtt?token=TqtOTzvL_9fBXDYAzZ10aPVAraWCszj7rewkMGPBE2A&expires=1706590800",
                        "youtube_prerecorded_link": "https://youtu.be/5OnBq6ZcTXw",
                        "youtube_prerecorded_id": "5OnBq6ZcTXw",
                        "bunny_prerecorded_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-cga/v-cga-9787979/v-cga-9787979_Presentation.mp4?token=PwLZK7iOG4ySGle25Gx5hdEOTqTrP0ra-vWSlUtLq24&expires=1706590800",
                        "bunny_prerecorded_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-cga/v-cga-9787979/v-cga-9787979_Presentation.vtt?token=gFl02kYzEf3gql7saGgl9E0Xx1bUx1UvJjuy1NFu19Y&expires=1706590800"
                    },
                    {
                        "slot_id": "v-cga-9830790",
                        "session_id": "cga1",
                        "title": "Technology Trends and Challenges for Large-Scale Scientific Visualization",
                        "contributors": [
                            "James Ahrens"
                        ],
                        "authors": [
                            "James Ahrens"
                        ],
                        "abstract": "",
                        "uid": "v-cga-9830790",
                        "time_stamp": "2023-10-25T00:09:00Z",
                        "time_start": "2023-10-25T00:09:00Z",
                        "time_end": "2023-10-25T00:21:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "doi": "",
                        "fno": "9830790",
                        "has_image": true,
                        "has_pdf": false,
                        "paper_award": "",
                        "image_caption": "A representation of the levels in the modern scientific process. A challenge for the visualization community is to explore the best visualization approaches for each of these levels as well as to be able to track, connect, and visualize relationships between levels. The levels increase in abstraction and complexity from the lowest level (data element) to the highest level (validation level).",
                        "external_paper_link": "",
                        "youtube_ff_link": "https://youtu.be/pXSlfBTW8z4",
                        "youtube_ff_id": "pXSlfBTW8z4",
                        "bunny_ff_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-cga/v-cga-9830790/v-cga-9830790_Preview.mp4?token=l6SvkBEcE7RyjNy2Kc4jscL3TNYuK67ExlZyGqmL2m8&expires=1706590800",
                        "bunny_ff_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-cga/v-cga-9830790/v-cga-9830790_Preview.vtt?token=LUFX14a6yx15CHDJkRBWW2MZw7LMGoPdia1D5bC2KSo&expires=1706590800",
                        "youtube_prerecorded_link": "https://youtu.be/LsNZqu9kops",
                        "youtube_prerecorded_id": "LsNZqu9kops",
                        "bunny_prerecorded_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-cga/v-cga-9830790/v-cga-9830790_Presentation.mp4?token=E5i3waM5ebcPib_bROtjOlJhElYbZJVFAKFl52egwD4&expires=1706590800",
                        "bunny_prerecorded_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-cga/v-cga-9830790/v-cga-9830790_Presentation.vtt?token=V2StFFRcD4jnrCwkZCeu44zXvNxNDhvjHKfz0Dd6mwM&expires=1706590800"
                    },
                    {
                        "slot_id": "v-cga-9861728",
                        "session_id": "cga1",
                        "title": "SUBPLEX: A Visual Analytics Approach to Understand Local Model Explanations at the Subpopulation Level",
                        "contributors": [
                            "Jun Yuan"
                        ],
                        "authors": [
                            "Jun Yuan",
                            "Gromit Yeuk-Yin Chan",
                            "Brian Barr",
                            "Kyle Overton",
                            "Kim Rees",
                            "Luis Gustavo Nonato",
                            "Enrico Bertini",
                            "Claudio Silva"
                        ],
                        "abstract": "",
                        "uid": "v-cga-9861728",
                        "time_stamp": "2023-10-25T00:21:00Z",
                        "time_start": "2023-10-25T00:21:00Z",
                        "time_end": "2023-10-25T00:33:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "doi": "",
                        "fno": "9861728",
                        "has_image": true,
                        "has_pdf": false,
                        "paper_award": "",
                        "image_caption": "SUBPLEX is a visual analytics tool in Jupyter notebook to assist data scientist to understand local model explanations at the subpopulation level. SUBPLEX contains five linked views: (a) code block, (b) cluster refinement view, (c) projection view, (d) subpopulation creation panel, (e) local explanation detail view.",
                        "external_paper_link": "",
                        "youtube_ff_link": "https://youtu.be/msAjrVejx6k",
                        "youtube_ff_id": "msAjrVejx6k",
                        "bunny_ff_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-cga/v-cga-9861728/v-cga-9861728_Preview.mp4?token=zsEwsAF7XzmCSLGIYU-LHoGaUClgqhVmmMc-ioW1qOo&expires=1706590800",
                        "bunny_ff_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-cga/v-cga-9861728/v-cga-9861728_Preview.vtt?token=DDSrpOVb5fIMuTGZhbdoalFWG_n8Pws5YIQYmO98_MQ&expires=1706590800",
                        "youtube_prerecorded_link": "https://youtu.be/RKgLgoAm20E",
                        "youtube_prerecorded_id": "RKgLgoAm20E",
                        "bunny_prerecorded_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-cga/v-cga-9861728/v-cga-9861728_Presentation.mp4?token=kZU9-HdpUH-Fwr8GjKRlX_huy9vNTETx50vYfY6tJlU&expires=1706590800",
                        "bunny_prerecorded_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-cga/v-cga-9861728/v-cga-9861728_Presentation.vtt?token=QL8tvubmM_B61HoeHll6sMgy8e9LiL8d0zRzyjvgpbc&expires=1706590800"
                    },
                    {
                        "slot_id": "v-cga-9992069",
                        "session_id": "cga1",
                        "title": "A Multiscale Geospatial Dataset and an Interactive Visualization Dashboard for Computational Epidemiology and Open Scientific Research",
                        "contributors": [
                            "Muhammad Usman"
                        ],
                        "authors": [
                            "Muhammad Usman",
                            "Honglu Zhou",
                            "Seonghyeon Moon",
                            "Xun Zhang",
                            "Petros Faloutsos",
                            "Mubbasir Kapadia"
                        ],
                        "abstract": "",
                        "uid": "v-cga-9992069",
                        "time_stamp": "2023-10-25T00:33:00Z",
                        "time_start": "2023-10-25T00:33:00Z",
                        "time_end": "2023-10-25T00:45:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "doi": "",
                        "fno": "9992069",
                        "has_image": true,
                        "has_pdf": false,
                        "paper_award": "",
                        "image_caption": "A Multi-Scale Geospatial Dataset and An Interactive Visualization Dashboard for Computational Epidemiology and Open Scientific Research",
                        "external_paper_link": "",
                        "youtube_ff_link": "https://youtu.be/okztZOD-h-M",
                        "youtube_ff_id": "okztZOD-h-M",
                        "bunny_ff_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-cga/v-cga-9992069/v-cga-9992069_Preview.mp4?token=JVkeC56QMFO_JXKnrw87bJI8ISbULbEgv4DFFxZUnyk&expires=1706590800",
                        "bunny_ff_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-cga/v-cga-9992069/v-cga-9992069_Preview.vtt?token=3lXsNHrExtbxW1PDr63s6kCPcuS9vVfqmhIn2nhe1pg&expires=1706590800",
                        "youtube_prerecorded_link": "",
                        "youtube_prerecorded_id": "",
                        "bunny_prerecorded_link": "",
                        "bunny_prerecorded_subtitles": ""
                    }
                ]
            },
            {
                "title": "Theory and Evaluation plus ISMAR/VR",
                "session_id": "cga2",
                "event_prefix": "v-cga",
                "track": "oneohthree",
                "session_image": "cga2.png",
                "chair": [
                    "Jian Chen"
                ],
                "time_start": "2023-10-26T03:00:00Z",
                "time_end": "2023-10-26T04:15:00Z",
                "discord_category": "",
                "discord_channel": "103",
                "discord_channel_id": "1161741183815524502",
                "discord_link": "https://discordapp.com/channels/1161400451619627068/1161741183815524502",
                "zoom_private_meeting": "",
                "zoom_private_password": "",
                "zoom_private_link": "",
                "zoom_broadcast_link": "",
                "ff_link": "https://youtu.be/87o09VpnURw",
                "time_slots": [
                    {
                        "slot_id": "v-cga-9830795",
                        "session_id": "cga2",
                        "title": "VisVisual: A Toolkit for Teaching and Learning Data Visualization",
                        "contributors": [
                            "Chaoli Wang"
                        ],
                        "authors": [
                            "Chaoli Wang"
                        ],
                        "abstract": "",
                        "uid": "v-cga-9830795",
                        "time_stamp": "2023-10-26T03:00:00Z",
                        "time_start": "2023-10-26T03:00:00Z",
                        "time_end": "2023-10-26T03:12:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "doi": "",
                        "fno": "9830795",
                        "has_image": true,
                        "has_pdf": false,
                        "paper_award": "",
                        "image_caption": "The VisVisual pedagogical toolkit (https://sites.nd.edu/chaoli-wang/visvisual) for teaching and learning essential visualization concepts, algorithms, and techniques. The toolkit was developed by Dr. Chaoli Wang's research team over ten years. It consists of four components: VolumeVisual, FlowVisual (including desktop and app versions), GraphVisual, and TreeVisual.",
                        "external_paper_link": "",
                        "youtube_ff_link": "https://youtu.be/tQlAdx5aYRA",
                        "youtube_ff_id": "tQlAdx5aYRA",
                        "bunny_ff_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-cga/v-cga-9830795/v-cga-9830795_Preview.mp4?token=ZzV8JUp99Iapo6xJP3EltqOJB7bFItWVuImVHmv7wp4&expires=1706590800",
                        "bunny_ff_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-cga/v-cga-9830795/v-cga-9830795_Preview.vtt?token=7IVRG8UwP1XLvdCuyfUY4L5M30-rQFHhLUBcwbPBZ6Y&expires=1706590800",
                        "youtube_prerecorded_link": "https://youtu.be/YqTuxJH8_vk",
                        "youtube_prerecorded_id": "YqTuxJH8_vk",
                        "bunny_prerecorded_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-cga/v-cga-9830795/v-cga-9830795_Presentation.mp4?token=e8OCn3RAM4ToBpQaMJYbXqNHgEgGtWx3NTrRZfrjGqA&expires=1706590800",
                        "bunny_prerecorded_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-cga/v-cga-9830795/v-cga-9830795_Presentation.vtt?token=Gw2xecAH0eIIOkT-frL1zKooyyxmitFnz2HCzrqiZ9Q&expires=1706590800"
                    },
                    {
                        "slot_id": "v-cga-9984051",
                        "session_id": "cga2",
                        "title": "Embracing Disciplinary Diversity in Visualization",
                        "contributors": [
                            "Sheelagh Carpendale"
                        ],
                        "authors": [
                            "Tatiana Losev",
                            "Justin Raynor",
                            "Sheelagh Carpendale",
                            "Melanie Tory"
                        ],
                        "abstract": "",
                        "uid": "v-cga-9984051",
                        "time_stamp": "2023-10-26T03:12:00Z",
                        "time_start": "2023-10-26T03:12:00Z",
                        "time_end": "2023-10-26T03:24:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "doi": "",
                        "fno": "9984051",
                        "has_image": true,
                        "has_pdf": false,
                        "paper_award": "",
                        "image_caption": "We outline six unique approaches to visualization research that represent a small sample of many that are possible. Different approaches produce different contribution types that can aid in enriching and strengthening the foundation of visualization research.",
                        "external_paper_link": "",
                        "youtube_ff_link": "https://youtu.be/K0hVDVdYepc",
                        "youtube_ff_id": "K0hVDVdYepc",
                        "bunny_ff_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-cga/v-cga-9984051/v-cga-9984051_Preview.mp4?token=wuk8W2qJ6HcAbG7xH17GbxAh76ANz4j6Xnca0ZDvkOs&expires=1706590800",
                        "bunny_ff_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-cga/v-cga-9984051/v-cga-9984051_Preview.vtt?token=ZwGE46alVg707emIrMZKrjjAqJw0pEoRvCrsj_quTuo&expires=1706590800",
                        "youtube_prerecorded_link": "https://youtu.be/5Ci1Iv_QKEI",
                        "youtube_prerecorded_id": "5Ci1Iv_QKEI",
                        "bunny_prerecorded_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-cga/v-cga-9984051/v-cga-9984051_Presentation.mp4?token=u1yFpcPOH1oXS61BTNVyI84x5RpmPyqGHZ4l7Fgx_FU&expires=1706590800",
                        "bunny_prerecorded_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-cga/v-cga-9984051/v-cga-9984051_Presentation.vtt?token=YwEVrxWwALypo4n6kB-XH4ESXownG3qY5f54p7vCj1k&expires=1706590800"
                    },
                    {
                        "slot_id": "v-cga-9708430",
                        "session_id": "cga2",
                        "title": "Lessons Learned From Quantitatively Exploring Visualization Rubric Utilization for Peer Feedback",
                        "contributors": [
                            "Sophie Engle"
                        ],
                        "authors": [
                            "Daniel J. Barajas",
                            "Xornam S. Apedoe",
                            "David G. Brizan",
                            "Alark P. Joshi",
                            "Sophie J. Engle"
                        ],
                        "abstract": "",
                        "uid": "v-cga-9708430",
                        "time_stamp": "2023-10-26T03:24:00Z",
                        "time_start": "2023-10-26T03:24:00Z",
                        "time_end": "2023-10-26T03:36:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "doi": "",
                        "fno": "9708430",
                        "has_image": true,
                        "has_pdf": false,
                        "paper_award": "",
                        "image_caption": "Sentiment (-1 to 1) of written feedback by numeric rating (1 to 5) for rubric responses",
                        "external_paper_link": "",
                        "youtube_ff_link": "https://youtu.be/NFgVFz45fYU",
                        "youtube_ff_id": "NFgVFz45fYU",
                        "bunny_ff_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-cga/v-cga-9708430/v-cga-9708430_Preview.mp4?token=PrFHhaubgsXGpENpu1fXSCy0ZYoGlZqkPB8TyKJyUGU&expires=1706590800",
                        "bunny_ff_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-cga/v-cga-9708430/v-cga-9708430_Preview.vtt?token=WXI0aPzMPfSPbgvCHStQEoeEl7o1pIA1qbBn27GDAZo&expires=1706590800",
                        "youtube_prerecorded_link": "https://youtu.be/V-2Fu0Gpldo",
                        "youtube_prerecorded_id": "V-2Fu0Gpldo",
                        "bunny_prerecorded_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-cga/v-cga-9708430/v-cga-9708430_Presentation.mp4?token=oaWxNvAlMz25ZUn1u6CZUEwCXU52L3jnRTycH3uGwew&expires=1706590800",
                        "bunny_prerecorded_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-cga/v-cga-9708430/v-cga-9708430_Presentation.vtt?token=hEuHJ0vqEYquGhhqmjavXxk5-Ew5KFkJyRGDAZPb2YY&expires=1706590800"
                    },
                    {
                        "slot_id": "v-cga-9656613",
                        "session_id": "cga2",
                        "title": "Finding Their Data Voice: Practices and Challenges of Dashboard Users",
                        "contributors": [
                            "Melanie Tory"
                        ],
                        "authors": [
                            "Melanie Tory",
                            "Lyn Bartram",
                            "Brittany Fiore-Gartland",
                            "Anamaria Crisan"
                        ],
                        "abstract": "",
                        "uid": "v-cga-9656613",
                        "time_stamp": "2023-10-26T03:36:00Z",
                        "time_start": "2023-10-26T03:36:00Z",
                        "time_end": "2023-10-26T03:48:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "doi": "",
                        "fno": "9656613",
                        "has_image": true,
                        "has_pdf": false,
                        "paper_award": "",
                        "image_caption": "Dashboards are many people's portal to data, but only the starting point of their data work. To share with others, dashboard users frequently dump data and screenshots out of dashboards and curate them into new artifacts.",
                        "external_paper_link": "",
                        "youtube_ff_link": "https://youtu.be/ArSjeUqNgfA",
                        "youtube_ff_id": "ArSjeUqNgfA",
                        "bunny_ff_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-cga/v-cga-9656613/v-cga-9656613_Preview.mp4?token=6aTulBe-gDLgCCSx9CMbETDVajO5rwDXvs1NWWtg30s&expires=1706590800",
                        "bunny_ff_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-cga/v-cga-9656613/v-cga-9656613_Preview.vtt?token=HXD7-W-UObW2MC4cf8bDAtlQ9QHNTVvb-ZmwffHLSCI&expires=1706590800",
                        "youtube_prerecorded_link": "https://youtu.be/ZLTGWWJfZP4",
                        "youtube_prerecorded_id": "ZLTGWWJfZP4",
                        "bunny_prerecorded_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-cga/v-cga-9656613/v-cga-9656613_Presentation.mp4?token=1Y2NkvLbQg_p9fGGdv3WEU7wKWDp6tk1eF796ctJ8T4&expires=1706590800",
                        "bunny_prerecorded_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-cga/v-cga-9656613/v-cga-9656613_Presentation.vtt?token=7vPMrJcGk0CHMKFY9Xx5lr4KQxbDqmEKw1b9pff7L5k&expires=1706590800"
                    },
                    {
                        "slot_id": "v-ismar-9995479",
                        "session_id": "cga2",
                        "title": "Augmented Scale Models: Presenting Multivariate Data Around Physical Scale Models in Augmented Reality",
                        "contributors": [
                            "Kadek Ananta Satriadi"
                        ],
                        "authors": [
                            "Kadek Ananta Satriadi",
                            "Andrew Cunningham",
                            "Bruce H. Thomas",
                            "Adam Drogemuller",
                            "Antoine Odi",
                            "Niki Patel",
                            "Cathlyn Aston",
                            "Ross T. Smith"
                        ],
                        "abstract": "",
                        "uid": "v-ismar-9995479",
                        "time_stamp": "2023-10-26T03:48:00Z",
                        "time_start": "2023-10-26T03:48:00Z",
                        "time_end": "2023-10-26T04:00:00Z",
                        "paper_type": "full",
                        "keywords": [],
                        "doi": "10.1109/ISMAR55827.2022.00019",
                        "fno": "9995479",
                        "has_image": true,
                        "has_pdf": false,
                        "paper_award": "",
                        "image_caption": "Illustration of one of the six augmented scale model techniques we evaluated in our study. This example depicts the technique with Dashboard Layout combined with \"On Scale Model\" View Arrangement.",
                        "external_paper_link": "",
                        "youtube_ff_link": "https://youtu.be/NOop_L5NJXw",
                        "youtube_ff_id": "NOop_L5NJXw",
                        "bunny_ff_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-ismar/v-ismar-9995479/v-ismar-9995479_Preview.mp4?token=uMzX2_7zoiZ6z5AS1et5fEmTt9lbPKrjU8jkElTKWSY&expires=1706590800",
                        "bunny_ff_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-ismar/v-ismar-9995479/v-ismar-9995479_Preview.vtt?token=pkKWXBJ27lenkUnxfl5V90FcbVWtGbW6qSKX4K2xrKs&expires=1706590800",
                        "youtube_prerecorded_link": "https://youtu.be/T9wOz20qurg",
                        "youtube_prerecorded_id": "T9wOz20qurg",
                        "bunny_prerecorded_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-ismar/v-ismar-9995479/v-ismar-9995479_Presentation.mp4?token=r3NRNov75nR7GCiLgMIDFGxHru7Pu79S1o4zma7cLy0&expires=1706590800",
                        "bunny_prerecorded_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-ismar/v-ismar-9995479/v-ismar-9995479_Presentation.vtt?token=Uc0RKxcY4iUczeAqgdF7GP1cKQMn56zJEJhI0YGU9z0&expires=1706590800"
                    },
                    {
                        "slot_id": "v-vr-10108427",
                        "session_id": "cga2",
                        "title": "Towards an Understanding of Distributed Asymmetric Collaborative Visualization on Problem-solving",
                        "contributors": [
                            "Wai Tong"
                        ],
                        "authors": [
                            "Wai Tong",
                            "Meng Xia",
                            "Kam Kwai Wong",
                            "Doug A. Bowman",
                            "Ting-Chuen Pong",
                            "Huamin Qu",
                            "Yalong Yang"
                        ],
                        "abstract": "",
                        "uid": "v-vr-10108427",
                        "time_stamp": "2023-10-26T04:00:00Z",
                        "time_start": "2023-10-26T04:00:00Z",
                        "time_end": "2023-10-26T04:12:00Z",
                        "paper_type": "full",
                        "keywords": [
                            "asymmetric collaborative visualization;virtual reality;data visualization;problem solving"
                        ],
                        "doi": "10.1109/VR55154.2023.00054",
                        "fno": "10108427",
                        "has_image": true,
                        "has_pdf": false,
                        "paper_award": "",
                        "image_caption": "We studied the trade-offs of collaborative visualization for problem-solving in an asymmetric environment. This figure shows how two collaborators perceive and interact with visualizations using two different devices: VR (left) and PC (right). Visualizations are in different dimensions to adapt to different devices (i.e., 3D in VR and 2D on PC) and can be blended together (as envisaged in the center) with tailored techniques to support collaboration awareness.",
                        "external_paper_link": "",
                        "youtube_ff_link": "https://youtu.be/gmR9hRfl3yM",
                        "youtube_ff_id": "gmR9hRfl3yM",
                        "bunny_ff_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-vr/v-vr-10108427/v-vr-10108427_Preview.mp4?token=0Uvc5kYTHCHt-py1oZ6OIdPwTNHLJqPTZ2-nvFcS10A&expires=1706590800",
                        "bunny_ff_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-vr/v-vr-10108427/v-vr-10108427_Preview.vtt?token=bA6eFcI5-sZuCU_oZSbY5AVNb_QeKsMKDtmYs6VdCMA&expires=1706590800",
                        "youtube_prerecorded_link": "https://youtu.be/gJ5XxmPX1NY",
                        "youtube_prerecorded_id": "gJ5XxmPX1NY",
                        "bunny_prerecorded_link": "https://ieeevis-uploads.b-cdn.net/vis23/v-vr/v-vr-10108427/v-vr-10108427_Presentation.mp4?token=FSSPdJfj_96HwePSKLgyktrNz7wgTmsu09t_hIdpFpQ&expires=1706590800",
                        "bunny_prerecorded_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/v-vr/v-vr-10108427/v-vr-10108427_Presentation.vtt?token=jaiYYyQ1kDRzWd49M75V5lITxACTq3lsWQi2sMJrDWU&expires=1706590800"
                    }
                ]
            }
        ]
    },
    "v-siggraph": {
        "event": "SIGGRAPH Invited Partnership Presentations",
        "long_name": "SIGGRAPH Invited Partnership Presentations",
        "event_type": "invited",
        "event_prefix": "v-siggraph",
        "event_description": "",
        "event_url": "",
        "organizers": [],
        "sessions": []
    },
    "v-vr": {
        "event": "VR Invited Partnership Presentations",
        "long_name": "VR Invited Partnership Presentations",
        "event_type": "invited",
        "event_prefix": "v-vr",
        "event_description": "",
        "event_url": "",
        "organizers": [],
        "sessions": []
    },
    "v-ismar": {
        "event": "ISMAR Invited Partnership Presentations",
        "long_name": "ISMAR Invited Partnership Presentations",
        "event_type": "invited",
        "event_prefix": "v-ismar",
        "event_description": "",
        "event_url": "",
        "organizers": [],
        "sessions": []
    },
    "v-panels": {
        "event": "VIS Panels",
        "long_name": "VIS Panels",
        "event_type": "panel",
        "event_prefix": "v-panels",
        "event_description": "",
        "event_url": "",
        "organizers": [],
        "sessions": [
            {
                "title": "Establishing and Thriving in an Academic Career",
                "session_id": "panel1",
                "event_prefix": "v-panels",
                "track": "oneohone",
                "session_image": "panel1.png",
                "chair": [],
                "time_start": "2023-10-25T04:45:00Z",
                "time_end": "2023-10-25T06:00:00Z",
                "discord_category": "",
                "discord_channel": "101-102",
                "discord_channel_id": "1161740777530069092",
                "discord_link": "https://discordapp.com/channels/1161400451619627068/1161740777530069092",
                "zoom_private_meeting": "",
                "zoom_private_password": "",
                "zoom_private_link": "",
                "zoom_broadcast_link": "",
                "ff_link": "",
                "time_slots": []
            },
            {
                "title": "What is a Visual Analytics contribution, and how is it changing?",
                "session_id": "panel2",
                "event_prefix": "v-panels",
                "track": "oneohone",
                "session_image": "panel2.png",
                "chair": [],
                "time_start": "2023-10-24T23:45:00Z",
                "time_end": "2023-10-25T01:00:00Z",
                "discord_category": "",
                "discord_channel": "101-102",
                "discord_channel_id": "1161740777530069092",
                "discord_link": "https://discordapp.com/channels/1161400451619627068/1161740777530069092",
                "zoom_private_meeting": "",
                "zoom_private_password": "",
                "zoom_private_link": "",
                "zoom_broadcast_link": "",
                "ff_link": "",
                "time_slots": []
            },
            {
                "title": "How should VIS4ML Redefine Itself in the Rapid Evolution of AI?",
                "session_id": "panel3",
                "event_prefix": "v-panels",
                "track": "oneohone",
                "session_image": "panel3.png",
                "chair": [],
                "time_start": "2023-10-26T04:45:00Z",
                "time_end": "2023-10-26T06:00:00Z",
                "discord_category": "",
                "discord_channel": "101-102",
                "discord_channel_id": "1161740777530069092",
                "discord_link": "https://discordapp.com/channels/1161400451619627068/1161740777530069092",
                "zoom_private_meeting": "",
                "zoom_private_password": "",
                "zoom_private_link": "",
                "zoom_broadcast_link": "",
                "ff_link": "",
                "time_slots": []
            }
        ]
    },
    "a-visap": {
        "event": "VIS Arts Program",
        "long_name": "VIS Arts Program",
        "event_type": "visap",
        "event_prefix": "a-visap",
        "event_description": "",
        "event_url": "",
        "organizers": [
            "Uta Hinrichs"
        ],
        "sessions": [
            {
                "title": "VISAP Session 1",
                "session_id": "visap1",
                "event_prefix": "a-visap",
                "track": "oneohthree",
                "session_image": "visap1.png",
                "chair": [],
                "time_start": "2023-10-24T23:45:00Z",
                "time_end": "2023-10-25T01:00:00Z",
                "discord_category": "",
                "discord_channel": "103",
                "discord_channel_id": "1161741183815524502",
                "discord_link": "https://discordapp.com/channels/1161400451619627068/1161741183815524502",
                "zoom_private_meeting": "",
                "zoom_private_password": "",
                "zoom_private_link": "",
                "zoom_broadcast_link": "",
                "ff_link": "",
                "time_slots": [
                    {
                        "slot_id": "a-visap-1062",
                        "session_id": "visap1",
                        "title": "The Heart",
                        "contributors": [
                            "Robert Walton"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "a-visap-1062",
                        "time_stamp": "2023-10-24T23:45:00Z",
                        "time_start": "2023-10-24T23:45:00Z",
                        "time_end": "2023-10-25T01:00:00Z",
                        "paper_type": "associated",
                        "keywords": [],
                        "doi": "",
                        "fno": "",
                        "has_image": false,
                        "has_pdf": true,
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "youtube_ff_link": "",
                        "youtube_ff_id": "",
                        "bunny_ff_link": "",
                        "bunny_ff_subtitles": "",
                        "youtube_prerecorded_link": "",
                        "youtube_prerecorded_id": "",
                        "bunny_prerecorded_link": "",
                        "bunny_prerecorded_subtitles": ""
                    },
                    {
                        "slot_id": "a-visap-1010",
                        "session_id": "visap1",
                        "title": "Waiting for the Wave in Metaverse",
                        "contributors": [
                            "Midori Yamazaki"
                        ],
                        "authors": [
                            "Midori Yamazaki"
                        ],
                        "abstract": "The artwork visualises the momentary shape of a wave in perfect shape, ideal for surfing, which retains its aesthetics forever, by mixing reality and virtual reality. By presenting it, the flexible cognitive abilities of the audience who sees it continue to work to actively select beautiful experiences and create a better future. The work reaffirms the supple strength of human cognitive abilities and expresses a sense of human existence that will remain unchanged forever, even in a future where reality is in chaos.",
                        "uid": "a-visap-1010",
                        "time_stamp": "2023-10-24T23:45:00Z",
                        "time_start": "2023-10-24T23:45:00Z",
                        "time_end": "2023-10-25T01:00:00Z",
                        "paper_type": "associated",
                        "keywords": [],
                        "doi": "",
                        "fno": "",
                        "has_image": true,
                        "has_pdf": true,
                        "paper_award": "",
                        "image_caption": "The artwork visualises the momentary shape of a wave in perfect shape, ideal for surfing, which retains its aesthetics forever, by mixing reality and virtual reality. By presenting it, the flexible cognitive abilities of the audience who sees it continue to work to actively select beautiful experiences and create a better future. The work reaffirms the supple strength of human cognitive abilities and expresses a sense of human existence that will remain unchanged forever, even in a future where reality is in chaos.",
                        "external_paper_link": "",
                        "youtube_ff_link": "",
                        "youtube_ff_id": "",
                        "bunny_ff_link": "https://ieeevis-uploads.b-cdn.net/vis23/a-visap/a-visap-1010/a-visap-1010_Preview.mp4?token=ha_cbu0MXnZrL4ZU2aeefUJMxA6MgBIFskutPg6BNq4&expires=1706590800",
                        "bunny_ff_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/a-visap/a-visap-1010/a-visap-1010_Preview.vtt?token=BbcCMw6-nV1urY1hMlXvUP4RXWhcpIzplM9FzVZGngs&expires=1706590800",
                        "youtube_prerecorded_link": "",
                        "youtube_prerecorded_id": "",
                        "bunny_prerecorded_link": "https://ieeevis-uploads.b-cdn.net/vis23/a-visap/a-visap-1010/a-visap-1010_Presentation.mp4?token=rVQuyfQx619-17ExtNyQPUfwRd-UEmS10hUlsMZZgsg&expires=1706590800",
                        "bunny_prerecorded_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/a-visap/a-visap-1010/a-visap-1010_Presentation.vtt?token=q5oXpBiiUQ30zj_-BSeO_-_lVGlfGX-Cv4DvxC_uBo4&expires=1706590800"
                    },
                    {
                        "slot_id": "a-visap-1018",
                        "session_id": "visap1",
                        "title": "Solar System",
                        "contributors": [
                            "Hyemi Song"
                        ],
                        "authors": [
                            "Hyemi Song"
                        ],
                        "abstract": "Solar System is an audio-visual live performance that marries data visualization and sonification. The installation system employs sidereal period data from the eight planets in our Solar System to generate a live soundtrack and visualization. The mission of this endeavor is to use audible and visual media to allow humans to uncover and cognize with the always surrounding, yet invisible, Solar System. Each planet's unique data patterns (sidereal periods) contribute to this exploration by being translated with audible and visible media. The media stimulate audiences' cognitive senses, enabling audiences to tangibly experience the aesthetic wonder of the cosmic world, which is deeply interconnected with the entirety of the Universe and humanity on Earth. The inspiration and message of the Solar System project resonate with this year's theme, Perpetual Presence. Throughout history, data from the Universe has been employed to uncover the existence of unseen worlds, the Cosmos. The discoveries gleaned from the process have influenced numerous sectors of our humanity, communicated through a multitude of languages and approaches. Scientists use numerical data and textual explanations to communicate information about the Cosmos. On the other hand, artists use sensory media, such as visuals and sounds, to narrate the story of the Cosmos. This art project aims to manifest the continuous and timeless presence of the Universe where it surrounds humanity, employing artistic transformation of universe data to illuminate its enduring existence.",
                        "uid": "a-visap-1018",
                        "time_stamp": "2023-10-24T23:45:00Z",
                        "time_start": "2023-10-24T23:45:00Z",
                        "time_end": "2023-10-25T01:00:00Z",
                        "paper_type": "associated",
                        "keywords": [],
                        "doi": "",
                        "fno": "",
                        "has_image": false,
                        "has_pdf": true,
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "youtube_ff_link": "",
                        "youtube_ff_id": "",
                        "bunny_ff_link": "",
                        "bunny_ff_subtitles": "",
                        "youtube_prerecorded_link": "",
                        "youtube_prerecorded_id": "",
                        "bunny_prerecorded_link": "",
                        "bunny_prerecorded_subtitles": ""
                    },
                    {
                        "slot_id": "a-visap-1081",
                        "session_id": "visap1",
                        "title": "Eco-Mending",
                        "contributors": [
                            "Racquel Fygenson"
                        ],
                        "authors": [
                            "Jane L. Adams",
                            "Racquel Fygenson"
                        ],
                        "abstract": "Eco-Mending juxtaposes old and new to tell a story of ecological regeneration. We find stories of human-mediated restoration in longitudinal data, and stitch modern data into imagery from the past, creating decorative wall and ceiling hangings. With perpetuality of data, comes the possibility of changing it. In this collection of art pieces, we pair past ecological destruction with present and future projections of data that highlight successful ecological reconstructions",
                        "uid": "a-visap-1081",
                        "time_stamp": "2023-10-24T23:45:00Z",
                        "time_start": "2023-10-24T23:45:00Z",
                        "time_end": "2023-10-25T01:00:00Z",
                        "paper_type": "associated",
                        "keywords": [],
                        "doi": "",
                        "fno": "",
                        "has_image": true,
                        "has_pdf": true,
                        "paper_award": "",
                        "image_caption": "Left: A close-up from data about soil remediation in Nigeria. Right: A photograph of the hanging sculpture recording data about the ozone layer.",
                        "external_paper_link": "",
                        "youtube_ff_link": "",
                        "youtube_ff_id": "",
                        "bunny_ff_link": "https://ieeevis-uploads.b-cdn.net/vis23/a-visap/a-visap-1081/a-visap-1081_Preview.mp4?token=iutZsUjr64YCPDmbrVWvqVJo0l5HQImnkJbBwEAo66s&expires=1706590800",
                        "bunny_ff_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/a-visap/a-visap-1081/a-visap-1081_Preview.vtt?token=Rh2vNwWFrIG89uo9JvqExuNrHEGPdvKzO8yen0VrGKo&expires=1706590800",
                        "youtube_prerecorded_link": "https://youtu.be/S8m0W3MNmag",
                        "youtube_prerecorded_id": "S8m0W3MNmag",
                        "bunny_prerecorded_link": "https://ieeevis-uploads.b-cdn.net/vis23/a-visap/a-visap-1081/a-visap-1081_Presentation.mp4?token=xW4ZyRql8rsDJWghNFxdu5EY2-e09TZAUch0cfYNdlU&expires=1706590800",
                        "bunny_prerecorded_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/a-visap/a-visap-1081/a-visap-1081_Presentation.vtt?token=jnKeh-rncqCo7bfdZmoi9AIPZH2Asl8IMkUZDC76TNo&expires=1706590800"
                    },
                    {
                        "slot_id": "a-visap-1056",
                        "session_id": "visap1",
                        "title": "Plastic Landscape - The Reversible World",
                        "contributors": [
                            "Yoon Chung Han"
                        ],
                        "authors": [
                            "Yoon Chung Han"
                        ],
                        "abstract": "\u201cPlastic Landscape - The Reversible World\u201d is an AI-generated 3D animated video design that shows the apocalyptic and surreal world surrounded by artificial plastic mixtures and objects in the ocean, urban city, Antarctica, and forest. Four different scenes are animated, with the camera panning slowly from left to right. Viewers can observe how the plastics are decomposed at a slower speed by looking at particle animations. Sound is created by the data of the decomposition of plastics. Different types of plastics and speed of decomposition determine the frequency, amplitude, and parameters of audio synthesis. This scene animation is inspired by Ilwalobongbyeong (a folding screen) behind the king\u2019s throne of the Joseon Dynasty. This animation depicts the twist of the landscape. Surreal objects/buildings in this animation made out of plastic look beautiful and mesmerizing at first glance. However, the viewers can notice that they are the decayed objects and destroyed nature impacted by human beings. This new multi-sensory artwork addresses the awareness of plastic pollution through the apocalyptic lens.",
                        "uid": "a-visap-1056",
                        "time_stamp": "2023-10-24T23:45:00Z",
                        "time_start": "2023-10-24T23:45:00Z",
                        "time_end": "2023-10-25T01:00:00Z",
                        "paper_type": "associated",
                        "keywords": [],
                        "doi": "",
                        "fno": "",
                        "has_image": false,
                        "has_pdf": true,
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "youtube_ff_link": "",
                        "youtube_ff_id": "",
                        "bunny_ff_link": "",
                        "bunny_ff_subtitles": "",
                        "youtube_prerecorded_link": "",
                        "youtube_prerecorded_id": "",
                        "bunny_prerecorded_link": "",
                        "bunny_prerecorded_subtitles": ""
                    },
                    {
                        "slot_id": "a-visap-1069",
                        "session_id": "visap1",
                        "title": "Parasitic signals: Multimodal Sonata for Real-time Interactive Simulation of the SARS-CoV-2 Virus",
                        "contributors": [
                            "Myungin Lee"
                        ],
                        "authors": [
                            "Myungin Lee",
                            "Sabina Hyoju Ahn",
                            "Yoojin Oh",
                            "JoAnn Kuchera-Morin"
                        ],
                        "abstract": "This project aims to transform the nano-scale of a striking biological phenomenon, the relationship between the SARS-CoV-2 virus and human molecules, into an interactive audiovisual simulation. In this work, Atomic Force Microscopy (AFM) touching and imaging a single molecule measures the interaction between the spike protein of SARS-CoV-2 and human cellular proteins and measures the dynamic of the spike protein. We create a comprehensive scientific model based on diverse datasets and theories presenting a real-time interactive complex system with efficient rendering and sonification using a single C++ platform. This project invites the audience into an immersive space where they can control the behavior of biomolecules, allowing them to intuitively perceive biological properties. This project is not only a demonstration of scientific data but also attempts to look at the interspecies relationship in parasitism which particularly deals with our current and post-pandemic life with coronavirus and how we might control our coexistence in a virtual space.",
                        "uid": "a-visap-1069",
                        "time_stamp": "2023-10-24T23:45:00Z",
                        "time_start": "2023-10-24T23:45:00Z",
                        "time_end": "2023-10-25T01:00:00Z",
                        "paper_type": "associated",
                        "keywords": [],
                        "doi": "",
                        "fno": "",
                        "has_image": true,
                        "has_pdf": true,
                        "paper_award": "",
                        "image_caption": "Installation of Parasitic Signals: Coexistence with the SARS-CoV-2 virus",
                        "external_paper_link": "",
                        "youtube_ff_link": "",
                        "youtube_ff_id": "",
                        "bunny_ff_link": "https://ieeevis-uploads.b-cdn.net/vis23/a-visap/a-visap-1069/a-visap-1069_Preview.mp4?token=DkxUwznZcTgMbbw9UJmiRvoEvsJDao6svFoQjAegbvk&expires=1706590800",
                        "bunny_ff_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/a-visap/a-visap-1069/a-visap-1069_Preview.vtt?token=gZJSD2OzyCZLqqOkCRbWeq0Cil0DwgtqJ1YUD4Da7Ys&expires=1706590800",
                        "youtube_prerecorded_link": "https://youtu.be/sAeBYKqlpIY",
                        "youtube_prerecorded_id": "sAeBYKqlpIY",
                        "bunny_prerecorded_link": "https://ieeevis-uploads.b-cdn.net/vis23/a-visap/a-visap-1069/a-visap-1069_Presentation.mp4?token=cC1JrB8epVJxwbgCAUfLRCoV_j7W63vcpvrpz5aDzxw&expires=1706590800",
                        "bunny_prerecorded_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/a-visap/a-visap-1069/a-visap-1069_Presentation.vtt?token=b4WHJYT4F7jtu8rBHkB1rZVGpeQINmr9T-O3JQnV5dE&expires=1706590800"
                    },
                    {
                        "slot_id": "a-visap-1029",
                        "session_id": "visap1",
                        "title": "Body Cosmos: An Immersive Experience Driven by Real-Time Bio-Data",
                        "contributors": [
                            "Rem RunGu Lin"
                        ],
                        "authors": [
                            "Rem RunGu Lin",
                            "Yongen Ke",
                            "Kang Zhang"
                        ],
                        "abstract": "This paper presents \"Body Cosmos\", an artwork that creates a symbiotic relationship between the human body and a simulated cosmic environment through volumetric rendering and particle system. Drawing from DICOM data to simulate the human body and nebulae, we create an interactive and dynamic virtual environment. The real-time bio-data of users, collected via heart rate sensors and EEG devices, is integrated into the visualization, fostering a personal engagement and unity within this 'cosmos.' Body Cosmos provokes curiosity and expands users' imagination, and deepens their understanding of life's macrocosm and microcosm. This exploratory project redefines traditional perceptions of the human body in relation to the universe, creating a unique lens to view selfhood, embodiment, and identity. As we look to the future, the system's evolution will include incorporation of more bio-data sensors, an investigation into its potential psychological and physiological benefits, and the development of social interactive features through multi-user capabilities.",
                        "uid": "a-visap-1029",
                        "time_stamp": "2023-10-24T23:45:00Z",
                        "time_start": "2023-10-24T23:45:00Z",
                        "time_end": "2023-10-25T01:00:00Z",
                        "paper_type": "associated",
                        "keywords": [],
                        "doi": "",
                        "fno": "",
                        "has_image": true,
                        "has_pdf": true,
                        "paper_award": "",
                        "image_caption": "Screenshot of Unreal Engine\u2019s real-time demo (\u00a9 Rungu Lin 2023)",
                        "external_paper_link": "",
                        "youtube_ff_link": "",
                        "youtube_ff_id": "",
                        "bunny_ff_link": "https://ieeevis-uploads.b-cdn.net/vis23/a-visap/a-visap-1029/a-visap-1029_Preview.mp4?token=aPYXA8zG2zE9drtnbryLWnT5kozZvUaQMToRBmysaAA&expires=1706590800",
                        "bunny_ff_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/a-visap/a-visap-1029/a-visap-1029_Preview.vtt?token=aS11gJ_XnNecukOxKHJiVPFZ27XV5GtZwVpAgeOEUho&expires=1706590800",
                        "youtube_prerecorded_link": "https://youtu.be/zxoVAVrkR8s",
                        "youtube_prerecorded_id": "zxoVAVrkR8s",
                        "bunny_prerecorded_link": "https://ieeevis-uploads.b-cdn.net/vis23/a-visap/a-visap-1029/a-visap-1029_Presentation.mp4?token=BZzDOW8KxT2r1NzVtFMRqqnx0ZZDCIJ15MGuFhrTROk&expires=1706590800",
                        "bunny_prerecorded_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/a-visap/a-visap-1029/a-visap-1029_Presentation.vtt?token=gX2HUY44ic1VDvTY_4aNRQJ6lpfzqT9Tij2rOZZImiU&expires=1706590800"
                    },
                    {
                        "slot_id": "a-visap-1084",
                        "session_id": "visap1",
                        "title": "Associative Forms for Encoding Multivariate Climate Data",
                        "contributors": [
                            "Francesca Samsel"
                        ],
                        "authors": [
                            "Francesca Samsel",
                            "Greg Abram",
                            "Catherine L. Bowma",
                            "Daniel F. Keefe"
                        ],
                        "abstract": "We are perpetually present in our environment, experiencing it with our senses. Scientific data describes the same environment quantitatively. Our goal is to use scientific and artistic methods to combine these environmental expressions and personal experience through the creation of glyphs visually abstracted from and associated with forms in nature in the representation of climate data. The use of these glyphs removes the distinctions between scientific data and sensory experience, to allow a fuller intuitive association between the two, creating an embodied experience and increasing awareness of the climate effects and changes all around us.",
                        "uid": "a-visap-1084",
                        "time_stamp": "2023-10-24T23:45:00Z",
                        "time_start": "2023-10-24T23:45:00Z",
                        "time_end": "2023-10-25T01:00:00Z",
                        "paper_type": "associated",
                        "keywords": [],
                        "doi": "",
                        "fno": "",
                        "has_image": false,
                        "has_pdf": true,
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "youtube_ff_link": "",
                        "youtube_ff_id": "",
                        "bunny_ff_link": "",
                        "bunny_ff_subtitles": "",
                        "youtube_prerecorded_link": "",
                        "youtube_prerecorded_id": "",
                        "bunny_prerecorded_link": "",
                        "bunny_prerecorded_subtitles": ""
                    }
                ]
            },
            {
                "title": "VISAP Session 2",
                "session_id": "visap2",
                "event_prefix": "a-visap",
                "track": "oneohthree",
                "session_image": "visap2.png",
                "chair": [],
                "time_start": "2023-10-25T23:45:00Z",
                "time_end": "2023-10-26T01:00:00Z",
                "discord_category": "",
                "discord_channel": "103",
                "discord_channel_id": "1161741183815524502",
                "discord_link": "https://discordapp.com/channels/1161400451619627068/1161741183815524502",
                "zoom_private_meeting": "",
                "zoom_private_password": "",
                "zoom_private_link": "",
                "zoom_broadcast_link": "",
                "ff_link": "",
                "time_slots": [
                    {
                        "slot_id": "a-visap-1026",
                        "session_id": "visap2",
                        "title": "Spotlight",
                        "contributors": [
                            "Charles Perin"
                        ],
                        "authors": [
                            "Kimiya Pahlevan",
                            "Charles Perin"
                        ],
                        "abstract": "Spotlight is an interactive book on the topic of government-imposed internet shutdowns. This data visualization project explores the effects of internet shutdowns and some of the common reasons they occur, as well as their relation to sensitive events such as protests and elections. The book is made interactive through the use of glow-in-the-dark paint in multiple interactive activities and visualizations, which requires the viewer to use a UV flashlight provided to them on the first page of the book to reveal additional information. For the purposes of this exhibition, the 10 pages of the book are individually placed on a wall in a linear fashion, each with their own UV flashlight. The viewers are able to interact with each page by going through a tunnel-shaped space created by placing a dark curtain from the ceiling in front of the wall. The book itself is placed outside, at the entrance of this tunnel, for display. The overall experience aims to replicate living through an internet shutdown by placing readers of the interactive book in a low-light environment and providing them with a flashlight in order to shine light on information and data that does not easily reach many people due to a lack of internet access.",
                        "uid": "a-visap-1026",
                        "time_stamp": "2023-10-25T23:45:00Z",
                        "time_start": "2023-10-25T23:45:00Z",
                        "time_end": "2023-10-26T01:00:00Z",
                        "paper_type": "associated",
                        "keywords": [],
                        "doi": "",
                        "fno": "",
                        "has_image": true,
                        "has_pdf": true,
                        "paper_award": "",
                        "image_caption": "Spotlight in the book format",
                        "external_paper_link": "",
                        "youtube_ff_link": "",
                        "youtube_ff_id": "",
                        "bunny_ff_link": "https://ieeevis-uploads.b-cdn.net/vis23/a-visap/a-visap-1026/a-visap-1026_Preview.mp4?token=TnSmD4XquVlTrTRkgkUCroNDZkxZAmB13SVY2bniJRc&expires=1706590800",
                        "bunny_ff_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/a-visap/a-visap-1026/a-visap-1026_Preview.vtt?token=O0yclK0-qbr3UI_p0mGXsow1xV8Ro2T5HiSEk_Qh5Xw&expires=1706590800",
                        "youtube_prerecorded_link": "https://youtu.be/qq54wsAQtac",
                        "youtube_prerecorded_id": "qq54wsAQtac",
                        "bunny_prerecorded_link": "https://ieeevis-uploads.b-cdn.net/vis23/a-visap/a-visap-1026/a-visap-1026_Presentation.mp4?token=MPQw0whiGXMMLs8QNZGbXXrWJQjhDlPip8uESkyBkyQ&expires=1706590800",
                        "bunny_prerecorded_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/a-visap/a-visap-1026/a-visap-1026_Presentation.vtt?token=Exubsapi4dpf1ePRkLI7YDIKCcN7FsufLHqpbvH0O-Q&expires=1706590800"
                    },
                    {
                        "slot_id": "a-visap-1034",
                        "session_id": "visap2",
                        "title": "Infinite Colours",
                        "contributors": [
                            "Xavier Ho"
                        ],
                        "authors": [
                            "Xavier Ho",
                            "Stephen James Krol"
                        ],
                        "abstract": "This generative work draws data from 2,499 queer independent games for about 12 seconds each. Each game adds a unique shape and colour onto the canvas, and plays a unique string of notes. Over 8 hours, the canvas will be filled with infinite colours to celebrate LGBTQIA+ independent videogames. History has always been queer. Through this generative visual and sound work, we aim to demonstrate the collective activism, movement, and creative expressions that queer folks are making to be visible, heard, and to say that we are here.  But queer movement does not happen over night; queer resistance is accumulative and built over generations of self-sacrifice and self-acceptance. The multitude intersectionality of the unruly times slowly bleeds colour into the world, blends motion into the landscape, and accumulatively becomes a canvas of ever-moving colourful light.",
                        "uid": "a-visap-1034",
                        "time_stamp": "2023-10-25T23:45:00Z",
                        "time_start": "2023-10-25T23:45:00Z",
                        "time_end": "2023-10-26T01:00:00Z",
                        "paper_type": "associated",
                        "keywords": [],
                        "doi": "",
                        "fno": "",
                        "has_image": true,
                        "has_pdf": true,
                        "paper_award": "",
                        "image_caption": "Infnite Colours This generative work draws data from 2,499 queer independent  games for about 12 seconds each. Each game adds a unique shape and colour onto  the canvas, and plays a unique string of notes. Over 8 hours, the canvas will be  filled with infinite colours to celebrate LGBTQIA+ independent videogames.",
                        "external_paper_link": "",
                        "youtube_ff_link": "",
                        "youtube_ff_id": "",
                        "bunny_ff_link": "https://ieeevis-uploads.b-cdn.net/vis23/a-visap/a-visap-1034/a-visap-1034_Preview.mp4?token=PLREXaMx9x1TmkNGHMCSeh0fGZtq0QHc-aA7rdpz-Qo&expires=1706590800",
                        "bunny_ff_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/a-visap/a-visap-1034/a-visap-1034_Preview.vtt?token=lrGUJy0G4uR4-tEq0LGMKDdvJRFv6j2Ndjk8T0gv7N4&expires=1706590800",
                        "youtube_prerecorded_link": "",
                        "youtube_prerecorded_id": "",
                        "bunny_prerecorded_link": "https://ieeevis-uploads.b-cdn.net/vis23/a-visap/a-visap-1034/a-visap-1034_Presentation.mp4?token=McJhZQHqX836CDZJUHYT16anGkN0egRTaS-p8IoguA4&expires=1706590800",
                        "bunny_prerecorded_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/a-visap/a-visap-1034/a-visap-1034_Presentation.vtt?token=w3qk5SZBDkJy68Z3r_TKSf3pVGA7c6eeYNt5JhR1bDo&expires=1706590800"
                    },
                    {
                        "slot_id": "a-visap-1074",
                        "session_id": "visap2",
                        "title": "The Island of Loneliness",
                        "contributors": [
                            "Junxiu Tang"
                        ],
                        "authors": [
                            "Junxiu Tang",
                            "Rui Sheng",
                            "Yifang Wang",
                            "Xinhuan Shu",
                            "Xiaojiao Chen",
                            "Tan Tang",
                            "Yingcai Wu"
                        ],
                        "abstract": "Loneliness and isolation are eternal emotions in human beings. Technological advancements create ample avenues, like social medias, for individuals to articulate themselves and record emotions. However, the sense of loneliness has never vanished, as their expressions are easily buried in the digital stream. We analyze tweets that express loneliness during holiday seasons but receive few responses. By superimposing digital charts on physical models, we visualize these lonely posts and generate the island of loneliness. We aim to reveal the complexities of human emotions in the digital age and reflect on the interconnections between technology, solitude, and social communication.",
                        "uid": "a-visap-1074",
                        "time_stamp": "2023-10-25T23:45:00Z",
                        "time_start": "2023-10-25T23:45:00Z",
                        "time_end": "2023-10-26T01:00:00Z",
                        "paper_type": "associated",
                        "keywords": [],
                        "doi": "",
                        "fno": "",
                        "has_image": true,
                        "has_pdf": true,
                        "paper_award": "",
                        "image_caption": "In today's interconnected world, everything is connected by the internet, but people still feel lonely. Through a data-driven approach, we physicalize people's perpetual loneliness into an island metaphor and try to reveal the overlooked voices within the internet data flow.",
                        "external_paper_link": "",
                        "youtube_ff_link": "",
                        "youtube_ff_id": "",
                        "bunny_ff_link": "https://ieeevis-uploads.b-cdn.net/vis23/a-visap/a-visap-1074/a-visap-1074_Preview.mp4?token=v9oOhcZ_YG_2QQNYVA1XJBgkOhNg2Fh0_4HPnH56joI&expires=1706590800",
                        "bunny_ff_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/a-visap/a-visap-1074/a-visap-1074_Preview.vtt?token=G2HOtiwmjksGL2Q_Vsk2w9nuHNVCZrJoW0dMbl__gtk&expires=1706590800",
                        "youtube_prerecorded_link": "https://youtu.be/gHBghCdqNtY",
                        "youtube_prerecorded_id": "gHBghCdqNtY",
                        "bunny_prerecorded_link": "https://ieeevis-uploads.b-cdn.net/vis23/a-visap/a-visap-1074/a-visap-1074_Presentation.mp4?token=TfdDYXdxGkWygKr28oUlgwQqWWPkXMISMaCn_vCBvqI&expires=1706590800",
                        "bunny_prerecorded_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/a-visap/a-visap-1074/a-visap-1074_Presentation.vtt?token=jMGb1ieSDZkyouBgEPrrHN4YpwFOt6psPGARn-da9Ro&expires=1706590800"
                    },
                    {
                        "slot_id": "a-visap-1083",
                        "session_id": "visap2",
                        "title": "Latent Prism",
                        "contributors": [
                            "Jane L. Adams"
                        ],
                        "authors": [
                            "Jane L. Adams"
                        ],
                        "abstract": "Latent Prism is a visually captivating and thought-provoking piece that incorporates artificial intelligence (AI) and data visualization. It presents a projection of an imagined environment, created through a generative adversarial network (GAN) trained on thousands of aerial photographs from royalty-free stock photo websites. The sculpture takes the form of a polished transparent lucite prism, within which layers of translucent mylar film are suspended. These films display frames extracted from an AI-generated video known as a \u201clatent walk,\u201d showcasing undulating ocean and forest landscapes captured from an aerial perspective. Frames are selected at regular intervals along the linear interpolation of images from the generative model, such that light from below is still able to permeate up to the viewer. The aggregated effect of these layered video frames results in an eerie visual sensation of peering down at a forested landscape that is being submerged in water. Surrounding the prism is a haphazardly piled 120ft. (36.5m)-long roll of credits, listing the names and photographers for every image used to train the model on a 2.5in (6.35cm) wide strip of drafting paper.",
                        "uid": "a-visap-1083",
                        "time_stamp": "2023-10-25T23:45:00Z",
                        "time_start": "2023-10-25T23:45:00Z",
                        "time_end": "2023-10-26T01:00:00Z",
                        "paper_type": "associated",
                        "keywords": [],
                        "doi": "",
                        "fno": "",
                        "has_image": true,
                        "has_pdf": true,
                        "paper_award": "",
                        "image_caption": "The latent prism is a lucite and mylar sculpture layering samples from an aerial photograph generative machine learning model. The sculpture is a rectangular stack of transparent blocks containing translucent layers of ocean and forest textures in undulating, indeterminate patterns. Coiled beneath the sculpture are 17,000 credits for each image used to train the model, printed onto a long scroll of architectural drafting paper.",
                        "external_paper_link": "",
                        "youtube_ff_link": "",
                        "youtube_ff_id": "",
                        "bunny_ff_link": "https://ieeevis-uploads.b-cdn.net/vis23/a-visap/a-visap-1083/a-visap-1083_Preview.mp4?token=s8cUI0ik-ZGG6yt_EnYlgbwN1EpC91TSmFNANjPFA2c&expires=1706590800",
                        "bunny_ff_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/a-visap/a-visap-1083/a-visap-1083_Preview.vtt?token=iZj_P0JyMwVZZ_wZvZCG1jzr8Fu4BxvlRR6pQYykMBQ&expires=1706590800",
                        "youtube_prerecorded_link": "https://youtu.be/Z5eCxaSvLd8",
                        "youtube_prerecorded_id": "Z5eCxaSvLd8",
                        "bunny_prerecorded_link": "https://ieeevis-uploads.b-cdn.net/vis23/a-visap/a-visap-1083/a-visap-1083_Presentation.mp4?token=KN064OaoEU01dYFdip0wcqr_oAx1zXmjw9nbHYWuKyI&expires=1706590800",
                        "bunny_prerecorded_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/a-visap/a-visap-1083/a-visap-1083_Presentation.vtt?token=qfwpybvqXR5FUmWwV_N9TvsWJ6U7ERNifLTj3F50pwE&expires=1706590800"
                    },
                    {
                        "slot_id": "a-visap-1040",
                        "session_id": "visap2",
                        "title": "The Vast Territory",
                        "contributors": [
                            "Ignacio P\u00e9rez-Messina"
                        ],
                        "authors": [
                            "Ignacio P\u00e9rez-Messina",
                            "Sim\u00f3n L\u00f3pez Trujillo"
                        ],
                        "abstract": "In the novel The Vast Territory (El vasto territorio. Alfaguara, 2021; Caja Negra, 2023), by Chilean author Sim\u00f3n L\u00f3pez Trujillo, a mycologist analyzes the way a certain fungus infects the mind of a forest worker called Pedro. In one moment, an abstract image, of countless white dots against a black background, in the form of waves or a mountain range, appears to explain the infection: the genetic origin of the language of Pedro, when the fungus starts speaking through him.  That image, included in the revised edition of the book, is also a depiction of the novel\u2019s genetic origin, as it was generated by a visualization of different drafts of the novel. In every literary reading, two texts are involved: the written or actual text, that we can smoothly read with our eyes, and a second text, imperceptible and invisible, made of all the deletions, editions, and additions of words involved in the process of writing the text. This project conceives the visualization of The Vast Territory as a visual novel on its own that explores the unconscious of the book: that black, secret space, where the words involved during the writing process emerge as ghostly presences. There, the data of the previous draft are manifested in the following one, as a latent presence in the actuality of the text we read. The work, where every white dot represents a word in the juxtaposed draft sequence, is a two-dimensional pixel-based text visualization. Its construction follows only two straightforward rules: (1) words are sequentially arranged in a horizontal line; (2) the vertical position of each word-dot is determined by its initial appearance within the entire sequence of drafts. The result is a \u201cdata-palimpsest\u201d where each draft leaves its imprint on the next through their cumulative determination of the spatial order. By using the order of first appearance as the guiding principle, the visualization emphasizes the inherited structure of each draft from its predecessors, akin to looking at the fossil record or geological strata, with the most ancient elements appearing at the greatest depth. The layer on top, where the words are shown, allows the viewer to inspect the content at each point in time, which is indicated by a thin vertical line or by the \u201cfolding\u201d of the canvas. It acts as a zoom-in, where the overlaid words are placed at their precise height in the visualization, or first-appearance depth.",
                        "uid": "a-visap-1040",
                        "time_stamp": "2023-10-25T23:45:00Z",
                        "time_start": "2023-10-25T23:45:00Z",
                        "time_end": "2023-10-26T01:00:00Z",
                        "paper_type": "associated",
                        "keywords": [],
                        "doi": "",
                        "fno": "",
                        "has_image": false,
                        "has_pdf": true,
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "youtube_ff_link": "",
                        "youtube_ff_id": "",
                        "bunny_ff_link": "",
                        "bunny_ff_subtitles": "",
                        "youtube_prerecorded_link": "",
                        "youtube_prerecorded_id": "",
                        "bunny_prerecorded_link": "",
                        "bunny_prerecorded_subtitles": ""
                    },
                    {
                        "slot_id": "a-visap-1060",
                        "session_id": "visap2",
                        "title": "Monster in a Snow Globe. Biographies as Data Physicalizations",
                        "contributors": [
                            "Florian Windhager"
                        ],
                        "authors": [
                            "Florian Windhager",
                            "Viola R\u00fchse",
                            "Smuc Michael"
                        ],
                        "abstract": "The data sculpture \"Monster in a Snowglobe\" offers a distant reading perspective on the life and work of the Austrian painter Herwig Zens (1943-2019). The 3D-printed artifact assembles major steps and movements of the artist's biography as an annotated trajectory in space and time. Its data selection draws both from biographical knowledge and from autobiographical entries in a unique diary, which the artist etched into a sequence of copper plates during his lifetime. A complete print of the diary\u2014also referred to as \u201cmonster\u201d due to its procedural and perceptual complexity\u2014is considered the longest etching in the world at 40 meters and was partially presented at the Museum of Art History in Vienna in Spring 2023. In this context, the data sculpture reframed the ephemeral complexity of a modern path of life as a material mimicry and instance of perpetual presence in an object-oriented exhibition environment.",
                        "uid": "a-visap-1060",
                        "time_stamp": "2023-10-24T23:45:00Z",
                        "time_start": "2023-10-24T23:45:00Z",
                        "time_end": "2023-10-25T01:00:00Z",
                        "paper_type": "associated",
                        "keywords": [],
                        "doi": "",
                        "fno": "",
                        "has_image": false,
                        "has_pdf": true,
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "youtube_ff_link": "",
                        "youtube_ff_id": "",
                        "bunny_ff_link": "",
                        "bunny_ff_subtitles": "",
                        "youtube_prerecorded_link": "",
                        "youtube_prerecorded_id": "",
                        "bunny_prerecorded_link": "",
                        "bunny_prerecorded_subtitles": ""
                    },
                    {
                        "slot_id": "a-visap-1066",
                        "session_id": "visap2",
                        "title": "Bitter Data: An Exploration into Data Edibilization of Negative Emotion",
                        "contributors": [
                            "Yufan Li"
                        ],
                        "authors": [
                            "Yufan Li",
                            "Yue Huang",
                            "Kang Zhang",
                            "varvara guljajeva"
                        ],
                        "abstract": "\u201cBitter Data\u201d transforms 100,000 distress postings from Chinese social media into a multi-sensory experience using data edibilization. We\u2019ve mapped distress data quantity to the bitterness and color of tea through data analysis and experimentation. Participants taste, smell, and observe 11 cups of tea, each embodying a year\u2019s distress data, in our workshop. Their facial expressions, recorded upon tasting, visually indicate emotional states. This project explores benefits and pragmatic solutions to challenges of data edibilization.",
                        "uid": "a-visap-1066",
                        "time_stamp": "2023-10-24T23:45:00Z",
                        "time_start": "2023-10-24T23:45:00Z",
                        "time_end": "2023-10-25T01:00:00Z",
                        "paper_type": "associated",
                        "keywords": [],
                        "doi": "",
                        "fno": "",
                        "has_image": false,
                        "has_pdf": true,
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "youtube_ff_link": "",
                        "youtube_ff_id": "",
                        "bunny_ff_link": "",
                        "bunny_ff_subtitles": "",
                        "youtube_prerecorded_link": "",
                        "youtube_prerecorded_id": "",
                        "bunny_prerecorded_link": "",
                        "bunny_prerecorded_subtitles": ""
                    },
                    {
                        "slot_id": "a-visap-1051",
                        "session_id": "visap2",
                        "title": "Reinterpreted Spaces, an AI Printmaking Collaboration",
                        "contributors": [
                            "Dr. Hannen E. Wolfe"
                        ],
                        "authors": [
                            "Hannen E. Wolfe",
                            "Charlotte Rogerson",
                            "Amanda Lilleston"
                        ],
                        "abstract": "We present an examination of organic spaces through print, book making, data, and machine learning. Artists created a book that explored the idea of organic and machine-made interpretations of a place that were generate using 3 different processes: a generative adversarial network, traditional printmaking and a camera. The artists found the results unexpected, discussing how the AI-generated image changed and complicated their understanding and constructed narrative about the original image and space. This caused them to think outside the box with over half of the students changing their print matrix and/or ink choices after seeing their AI-generated image. This supported the learning objective for students to collaborate with technology that is uninhibited by perspective or expectation, adapting and responding productively and creatively within a new framework. With the newfound accessibility to AI-generated images we encourage art teachers to explore how it fits into their curriculum.",
                        "uid": "a-visap-1051",
                        "time_stamp": "2023-10-24T23:45:00Z",
                        "time_start": "2023-10-24T23:45:00Z",
                        "time_end": "2023-10-25T01:00:00Z",
                        "paper_type": "associated",
                        "keywords": [],
                        "doi": "",
                        "fno": "",
                        "has_image": true,
                        "has_pdf": true,
                        "paper_award": "",
                        "image_caption": "The flag book is a sculptural object that can be read page by page or viewed in a full spread. It explores the idea of organic and machine-made interpretations of a place that were generated using 3 different processes: a generative adversarial network, traditional printmaking and a camera. This image shows a flag book from 4 different perspectives, opened like a book with 2 pages, spread open to reveal all nine pages, the back of a flag book showing the accordion folded spine, and the flag book from above.",
                        "external_paper_link": "",
                        "youtube_ff_link": "",
                        "youtube_ff_id": "",
                        "bunny_ff_link": "https://ieeevis-uploads.b-cdn.net/vis23/a-visap/a-visap-1051/a-visap-1051_Preview.mp4?token=qFEky2VplP0e9obbU4itM83UJJ50etMsdqvJOgyGYC4&expires=1706590800",
                        "bunny_ff_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/a-visap/a-visap-1051/a-visap-1051_Preview.vtt?token=9nDzuK2XQU6v4-rJNcciKxUOoekpmOOABXY3aDy4paE&expires=1706590800",
                        "youtube_prerecorded_link": "",
                        "youtube_prerecorded_id": "",
                        "bunny_prerecorded_link": "",
                        "bunny_prerecorded_subtitles": ""
                    }
                ]
            }
        ]
    },
    "v-spotlights": {
        "event": "Application Spotlights",
        "long_name": "Application Spotlights",
        "event_type": "application",
        "event_prefix": "v-spotlights",
        "event_description": "",
        "event_url": "",
        "organizers": [],
        "sessions": [
            {
                "title": "Visualization for spatial single-cell atlases",
                "session_id": "app1",
                "event_prefix": "v-spotlights",
                "track": "oneohone",
                "session_image": "app1.png",
                "chair": [],
                "time_start": "2023-10-25T23:45:00Z",
                "time_end": "2023-10-26T01:00:00Z",
                "discord_category": "",
                "discord_channel": "101-102",
                "discord_channel_id": "1161740777530069092",
                "discord_link": "https://discordapp.com/channels/1161400451619627068/1161740777530069092",
                "zoom_private_meeting": "",
                "zoom_private_password": "",
                "zoom_private_link": "",
                "zoom_broadcast_link": "",
                "ff_link": "",
                "time_slots": []
            }
        ]
    },
    "s-vds": {
        "event": "VDS: Visualization in Data Science Symposium",
        "long_name": "VDS: Visualization in Data Science Symposium",
        "event_type": "associated",
        "event_prefix": "s-vds",
        "event_description": "",
        "event_url": "",
        "organizers": [
            "Alvitta Ottley",
            "Anamaria Crisan",
            "Michael Behrisch",
            "Jorge Ono",
            "Shayan Monadjemi"
        ],
        "sessions": [
            {
                "title": "VDS: Visualization in Data Science Symposium",
                "session_id": "ae3",
                "event_prefix": "s-vds",
                "track": "oneohsix",
                "session_image": "ae3.png",
                "chair": [
                    "Alvitta Ottley",
                    "Anamaria Crisan",
                    "Michael Behrisch",
                    "Jorge Ono",
                    "Shayan Monadjemi"
                ],
                "time_start": "2023-10-23T03:00:00Z",
                "time_end": "2023-10-23T06:00:00Z",
                "discord_category": "",
                "discord_channel": "106",
                "discord_channel_id": "1161741373410644119",
                "discord_link": "https://discordapp.com/channels/1161400451619627068/1161741373410644119",
                "zoom_private_meeting": "",
                "zoom_private_password": "",
                "zoom_private_link": "",
                "zoom_broadcast_link": "",
                "ff_link": "",
                "time_slots": [
                    {
                        "slot_id": "s-vds-1002",
                        "session_id": "ae3",
                        "title": "HPCClusterScape: Increasing Transparency and Efficiency of Shared High-Performance Computing Clusters for Large-scale AI Models",
                        "contributors": [
                            "Heungseok Park"
                        ],
                        "authors": [
                            "Heungseok Park",
                            "Aeree Cho",
                            "Hyojun Jeon",
                            "Hayoung Lee",
                            "Youngil Yang",
                            "Sungjae Lee",
                            "Heungsub Lee",
                            "Jaegul Choo"
                        ],
                        "abstract": "The emergence of large-scale AI models, like GPT-4, has significantly impacted academia and industry, driving the demand for high-performance computing (HPC) to accelerate workloads. To address this, we present HPCClusterScape, a visualization system that enhances the efficiency and transparency of shared HPC clusters for large-scale AI models. HPCClusterScape provides a comprehensive overview of system-level (e.g., partitions, hosts, and workload status) and application-level (e.g., identification of experiments and researchers) information, allowing HPC operators and machine learning researchers to monitor resource utilization and identify issues through customizable violation rules. The system includes diagnostic tools to investigate workload imbalances and synchronization bottlenecks in large-scale distributed deep learning experiments. Deployed in industrial-scale HPC clusters, HPCClusterScape incorporates user feedback and meets specific requirements. This paper outlines the challenges and prerequisites for efficient HPC operation, introduces the interactive visualization system, and highlights its contributions in addressing pain points and optimizing resource utilization in shared HPC clusters.",
                        "uid": "s-vds-1002",
                        "time_stamp": "2023-10-23T03:00:00Z",
                        "time_start": "2023-10-23T03:00:00Z",
                        "time_end": "2023-10-23T06:00:00Z",
                        "paper_type": "associated",
                        "keywords": [],
                        "doi": "",
                        "fno": "",
                        "has_image": true,
                        "has_pdf": true,
                        "paper_award": "",
                        "image_caption": "HPCClusterScape offers real-time resource monitoring for HPC clusters. The (A) Cluster Overview displays GPU resources, allowing users to observe both the overall cluster and individual resources. Users can set (B) Violation Rules to track system metric statistics and detect anomalies over time. Clicking on specific workloads leads to the (C) Diagnostics View, enabling detailed node, GPU, and metric analysis for large-scale distributed training.",
                        "external_paper_link": "",
                        "youtube_ff_link": "",
                        "youtube_ff_id": "",
                        "bunny_ff_link": "https://ieeevis-uploads.b-cdn.net/vis23/s-vds/s-vds-1002/s-vds-1002_Preview.mp4?token=ICof0jgHpLv1gIhwh-TXnhdmWdXu6pD8RNfxgNNdzT0&expires=1706590800",
                        "bunny_ff_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/s-vds/s-vds-1002/s-vds-1002_Preview.vtt?token=Fblkza795cOc-vVqnHhWXDFwIMrZ_6ysgkY6dhLaXYY&expires=1706590800",
                        "youtube_prerecorded_link": "https://youtu.be/QN_jkguHyRg",
                        "youtube_prerecorded_id": "QN_jkguHyRg",
                        "bunny_prerecorded_link": "https://ieeevis-uploads.b-cdn.net/vis23/s-vds/s-vds-1002/s-vds-1002_Presentation.mp4?token=gJVI4JZeez4bXcbZ75Hk1xoVDhZpvMttYgrRuI3-Qns&expires=1706590800",
                        "bunny_prerecorded_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/s-vds/s-vds-1002/s-vds-1002_Presentation.vtt?token=D8Xp_EYSP6RiyNLskv3-ASqsJEVdsuUEzwZ8qA16IoA&expires=1706590800"
                    },
                    {
                        "slot_id": "s-vds-1014",
                        "session_id": "ae3",
                        "title": "NeighViz: Towards Better Understanding of Neighborhood Effects on Social Groups with Spatial Data",
                        "contributors": [
                            "Yue Yu"
                        ],
                        "authors": [
                            "Yue Yu",
                            "Yifang Wang",
                            "Qisen Yang",
                            "Di Weng",
                            "Yongjun Zhang",
                            "Xiaogang Wu",
                            "Yingcai Wu",
                            "Huamin Qu"
                        ],
                        "abstract": "Understanding how local environments influence individual behaviors, such as voting patterns or suicidal tendencies, is crucial in social science to reveal and reduce spatial disparities and promote social well-being. With the increasing availability of large-scale individual-level census data, new analytical opportunities arise for social scientists to explore human behaviors (e.g., political engagement) among social groups at a fine-grained level. However, traditional statistical methods mostly focus on global, aggregated spatial correlations, which are limited to understanding and comparing the impact of local environments (e.g., neighborhoods) on human behaviors among social groups.  In this study, we introduce a new analytical framework for analyzing multi-variate neighborhood effects between social groups. We then propose NeighViz, an interactive visual analytics system that helps social scientists explore, understand, and verify the influence of neighborhood effects on human behaviors. Finally, we use a case study to illustrate the effectiveness and usability of our system.",
                        "uid": "s-vds-1014",
                        "time_stamp": "2023-10-23T03:00:00Z",
                        "time_start": "2023-10-23T03:00:00Z",
                        "time_end": "2023-10-23T06:00:00Z",
                        "paper_type": "associated",
                        "keywords": [],
                        "doi": "",
                        "fno": "",
                        "has_image": true,
                        "has_pdf": true,
                        "paper_award": "",
                        "image_caption": "The impact of local environments on individual behaviors, such as voting patterns and suicidal tendencies, is important in social science to address spatial disparities and promote social well-being. Traditional statistical methods offer limited understanding of this impact with only global, aggregated spatial correlations. To address this, we introduce NeighViz, an interactive visual analytics system designed to help social scientists explore, understand, and verify the influence of neighborhood effects on human behaviors.",
                        "external_paper_link": "",
                        "youtube_ff_link": "",
                        "youtube_ff_id": "",
                        "bunny_ff_link": "https://ieeevis-uploads.b-cdn.net/vis23/s-vds/s-vds-1014/s-vds-1014_Preview.mp4?token=wTmOvCKv76EE2d6J0Hjmzb9q7HbCEmtGT8xb1Hjlqks&expires=1706590800",
                        "bunny_ff_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/s-vds/s-vds-1014/s-vds-1014_Preview.vtt?token=3kwUSDynvPC-1ZAZ14TcT9isPzVRpLRxvm04qdxQkGQ&expires=1706590800",
                        "youtube_prerecorded_link": "",
                        "youtube_prerecorded_id": "",
                        "bunny_prerecorded_link": "",
                        "bunny_prerecorded_subtitles": ""
                    },
                    {
                        "slot_id": "s-vds-1016",
                        "session_id": "ae3",
                        "title": "Aardvark: Comparative Visualization of Data Analysis Scripts",
                        "contributors": [
                            "Dr. Rebecca Faust"
                        ],
                        "authors": [
                            "Rebecca Faust",
                            "Carlos Scheidegger",
                            "Chris North"
                        ],
                        "abstract": "Debugging programs is famously one of the most challenging aspects of programming. Data analysis scripts present additional challenges as debugging tasks are often more exploratory, such as comparing results under different parameter settings. In fact, a common exploratory debugging process is to run, modify, and re-run a script to observe the effects of the change. Analyst\u2019s perform this process repeatedly as they explore different settings in their script. However, traditional debugging methods do not support direct comparison across script executions. To address this, we present Aardvark, a comparative trace-based debugging method for identifying and visualizing the differences between consecutive executions of analysis scripts. Aardvark traces two consecutive instances of a script, identifies the differences between them, and presents them through comparative visualizations. We present a prototype implementation in Python along with an extension to Jupyter notebooks and  demonstrate Aardvark through two usage scenarios on real world analysis scripts.",
                        "uid": "s-vds-1016",
                        "time_stamp": "2023-10-23T03:00:00Z",
                        "time_start": "2023-10-23T03:00:00Z",
                        "time_end": "2023-10-23T06:00:00Z",
                        "paper_type": "associated",
                        "keywords": [],
                        "doi": "",
                        "fno": "",
                        "has_image": true,
                        "has_pdf": true,
                        "paper_award": "",
                        "image_caption": "\u200b\u200bAn overview of Aardvark, a comparative visual debugging method for data analysis programs. Aardvark visualizes the differences between consecutive executions of an analysis script. The source view, (A), highlights the differences in the analysis source code. (B) shows the comparative generalized context tree to illustrate the differences in the execution structure. The original execution tree builds down from the center block, while the modified version builds up.\u00a0 (C) shows the comparative scatter plot that illustrates how the values differ across the consecutive executions.",
                        "external_paper_link": "",
                        "youtube_ff_link": "",
                        "youtube_ff_id": "",
                        "bunny_ff_link": "https://ieeevis-uploads.b-cdn.net/vis23/s-vds/s-vds-1016/s-vds-1016_Preview.mp4?token=IMBaufRG-xmTbGl62jfXFMujbYtT8-62y6YuTpfjtaM&expires=1706590800",
                        "bunny_ff_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/s-vds/s-vds-1016/s-vds-1016_Preview.vtt?token=mQTv0QCAyVh7HgU2jLo2roC_8jNBOHQzFCUSw6yZCnU&expires=1706590800",
                        "youtube_prerecorded_link": "",
                        "youtube_prerecorded_id": "",
                        "bunny_prerecorded_link": "https://ieeevis-uploads.b-cdn.net/vis23/s-vds/s-vds-1016/s-vds-1016_Presentation.mp4?token=b1PHI2tp7ptQ11ZYr2FCbKoeHwHh2UPEOZh6AvQYMRc&expires=1706590800",
                        "bunny_prerecorded_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/s-vds/s-vds-1016/s-vds-1016_Presentation.vtt?token=Yy3f7ZXTwkuYsljp_6bM8JniVMg_gzr5CECib0zQ_pw&expires=1706590800"
                    },
                    {
                        "slot_id": "s-vds-1017",
                        "session_id": "ae3",
                        "title": "A Declarative Specification for Authoring Metrics Dashboards",
                        "contributors": [
                            "Will Epperson"
                        ],
                        "authors": [
                            "Will Epperson",
                            "Kanit Wongsuphasawat",
                            "Allison Whilden",
                            "Fan Du",
                            "Justin Talbot"
                        ],
                        "abstract": "Despite their ubiquity, authoring dashboards for metrics reporting in modern data analysis tools remains a manual, time-consuming process. Rather than focusing on interesting combinations of their data, users have to spend time creating each chart in a dashboard one by one. This makes dashboard creation slow and tedious. We conducted a review of production metrics dashboards and found that many dashboards contain a common structure: breaking down one or more metrics by different dimensions. In response, we developed a high-level specification for describing dashboards as sections of metrics repeated across the same dimensions and a graphical interface, Quick Dashboard, for authoring dashboards based on this specification. We present several usage examples that demonstrate the flexibility of this specification to create various kinds of dashboards and support a data-first approach to dashboard authoring",
                        "uid": "s-vds-1017",
                        "time_stamp": "2023-10-23T03:00:00Z",
                        "time_start": "2023-10-23T03:00:00Z",
                        "time_end": "2023-10-23T06:00:00Z",
                        "paper_type": "associated",
                        "keywords": [],
                        "doi": "",
                        "fno": "",
                        "has_image": true,
                        "has_pdf": true,
                        "paper_award": "",
                        "image_caption": "Quick dashboarding presents a novel specification for dashboard authoring, comprised of sections of metrics combined with dimensions.",
                        "external_paper_link": "",
                        "youtube_ff_link": "",
                        "youtube_ff_id": "",
                        "bunny_ff_link": "https://ieeevis-uploads.b-cdn.net/vis23/s-vds/s-vds-1017/s-vds-1017_Preview.mp4?token=XJ7keJrB1CZHMphUA4SegluO0FDrl7Jq2oG0Q1e7Kdw&expires=1706590800",
                        "bunny_ff_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/s-vds/s-vds-1017/s-vds-1017_Preview.vtt?token=XzoHl07eUhYr5Y88-v4tCXeSWdazRL6rtLdtj-xMa34&expires=1706590800",
                        "youtube_prerecorded_link": "https://youtu.be/Q9jktmeRYJQ",
                        "youtube_prerecorded_id": "Q9jktmeRYJQ",
                        "bunny_prerecorded_link": "https://ieeevis-uploads.b-cdn.net/vis23/s-vds/s-vds-1017/s-vds-1017_Presentation.mp4?token=tycxDYPp4XFrZxDAnrAyDhkPsi2lBk8ZxtU2DnOfMdg&expires=1706590800",
                        "bunny_prerecorded_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/s-vds/s-vds-1017/s-vds-1017_Presentation.vtt?token=CFd6of6URqRobhxFzYdTPpBkUVqvaezRRUVvbBi3E4c&expires=1706590800"
                    },
                    {
                        "slot_id": "s-vds-1018",
                        "session_id": "ae3",
                        "title": "Visual Comparison of Text Sequences Generated by Large Language Models",
                        "contributors": [
                            "Mennatallah El-Assady"
                        ],
                        "authors": [
                            "Rita Sevastjanova",
                            "Simon Vogelbacher",
                            "Andreas Spitz",
                            "Daniel Keim",
                            "Mennatallah El-Assady"
                        ],
                        "abstract": "Causal language models have emerged as the leading technology for automating text generation tasks. Although these models tend to produce outputs that resemble human writing, they still suffer from quality issues (e.g., social biases). Researchers typically use automatic analysis methods to evaluate the model limitations, such as statistics on stereotypical words. Since different types of issues are embedded in the model parameters, the development of automated methods that capture all relevant aspects remains a challenge. To tackle this challenge, we propose a visual analytics approach that supports the exploratory analysis of text sequences generated by causal language models. Our approach enables users to specify starting prompts and effectively groups the resulting text sequences. To this end, we leverage a unified, ontology-driven embedding space, serving as a shared foundation for the thematic concepts present in the generated text sequences. Visual summaries provide insights into various levels of granularity within the generated data. Among others, we propose a novel comparison visualization that slices the embedding space and represents the differences between two prompt outputs in a radial layout. We demonstrate the effectiveness of our approach through case studies, showcasing its potential to reveal model biases and other quality issues.",
                        "uid": "s-vds-1018",
                        "time_stamp": "2023-10-23T03:00:00Z",
                        "time_start": "2023-10-23T03:00:00Z",
                        "time_end": "2023-10-23T06:00:00Z",
                        "paper_type": "associated",
                        "keywords": [],
                        "doi": "",
                        "fno": "",
                        "has_image": true,
                        "has_pdf": true,
                        "paper_award": "",
                        "image_caption": "We introduce a novel visual analytics approach supporting exploratory analysis of automatically generated text sequences and their comparison.  Our visualizations help to investigate stereotypes associated with different prompts, inspect model differences, and detect unexpected associations encoded in open source language models.",
                        "external_paper_link": "",
                        "youtube_ff_link": "",
                        "youtube_ff_id": "",
                        "bunny_ff_link": "https://ieeevis-uploads.b-cdn.net/vis23/s-vds/s-vds-1018/s-vds-1018_Preview.mp4?token=9EC_VqnLPL9ofVgqM-2zweM4ImRteqbFXP92LdPhU9E&expires=1706590800",
                        "bunny_ff_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/s-vds/s-vds-1018/s-vds-1018_Preview.vtt?token=uwE4ZgazlQMFh_H8Xnbio5ViBqcCj2oisN3tFNjoa4s&expires=1706590800",
                        "youtube_prerecorded_link": "",
                        "youtube_prerecorded_id": "",
                        "bunny_prerecorded_link": "https://ieeevis-uploads.b-cdn.net/vis23/s-vds/s-vds-1018/s-vds-1018_Presentation.mp4?token=nlj7IRTIAUJkmfAx_B4pMZcvXkq6XBCeqRjtx8OrP2A&expires=1706590800",
                        "bunny_prerecorded_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/s-vds/s-vds-1018/s-vds-1018_Presentation.vtt?token=MViyr5XKJ75fBRkUaUiKjSqn7MJZjazpzUSUvOlWc0M&expires=1706590800"
                    }
                ]
            }
        ]
    },
    "w-vis4dh": {
        "event": "VIS4DH: 8th Workshop on Visualization for the Digital Humanities",
        "long_name": "VIS4DH: 8th Workshop on Visualization for the Digital Humanities",
        "event_type": "workshop",
        "event_prefix": "w-vis4dh",
        "event_description": "",
        "event_url": "",
        "organizers": [
            "Alfie Abdul-Rahman",
            "Eric Alexander"
        ],
        "sessions": [
            {
                "title": "VIS4DH: 8th Workshop on Visualization for the Digital Humanities",
                "session_id": "w1",
                "event_prefix": "w-vis4dh",
                "track": "oneohfive",
                "session_image": "w1.png",
                "chair": [
                    "Alfie Abdul-Rahman",
                    "Eric Alexander"
                ],
                "time_start": "2023-10-23T03:00:00Z",
                "time_end": "2023-10-23T06:00:00Z",
                "discord_category": "",
                "discord_channel": "105",
                "discord_channel_id": "1161741309346861067",
                "discord_link": "https://discordapp.com/channels/1161400451619627068/1161741309346861067",
                "zoom_private_meeting": "",
                "zoom_private_password": "",
                "zoom_private_link": "",
                "zoom_broadcast_link": "",
                "ff_link": "",
                "time_slots": [
                    {
                        "slot_id": "w-vis4dh-1004",
                        "session_id": "w1",
                        "title": "A Stitch in Time: Using Data Embroidery to Tell Australian Convict Stories",
                        "contributors": [
                            "Dr. Monika M. Schwarz"
                        ],
                        "authors": [
                            "Monika M. Schwarz",
                            "Kim Marriott",
                            "Jon McCormack",
                            "Hamish Maxwell-Stewart"
                        ],
                        "abstract": "The Stitch in Time project extends a traditional browser-based visualisation, the LifeLines, by creating physical data embroideries from a subset of the data. The LifeLines visualise the individual life courses of over 13.600 nineteenth century Australian convict women, according to their paper trail left behind in the Colonial Archives. In this project we created 18 embroideries of 21 convict women, based on a sketch inspired by a specific bit of information in the woman's life, and then integrating the data of her lifeline into the embroidery. This way we use a novel approach to show convict women, of whom little imagery has survived. By exhibiting the embroideries in the Penitentiary Chapel in Hobart, Tasmania, we hope to arouse interest in the largely forgotten lives of this first coerced generation of European settlers. We use the unusual medium of embroidery on fabric because they both are tightly connected to the daily experiences of convict women. This way, we hope to create new access points to historical data by extending traditional data visualisation using this specific form of data physicalisation.",
                        "uid": "w-vis4dh-1004",
                        "time_stamp": "2023-10-23T03:00:00Z",
                        "time_start": "2023-10-23T03:00:00Z",
                        "time_end": "2023-10-23T06:00:00Z",
                        "paper_type": "workshop",
                        "keywords": [],
                        "doi": "",
                        "fno": "",
                        "has_image": true,
                        "has_pdf": true,
                        "paper_award": "",
                        "image_caption": "The Stitch in Time project extends a traditional timeline visualisation of Australian convict women\ufffds lives by creating physical data embroideries drawing from the available historical data of individual women. In this project we created 18 embroideries of 21 convict women, based on sketches inspired by specific pieces of information in each woman\ufffds life. We use the unusual medium of embroidery on fabric because it is tightly connected to the daily experiences of convict women. By extending traditional data visualisation using this specific form of data physicalisation we aim to create compelling access points to engage with individual Australian convict stories.",
                        "external_paper_link": "",
                        "youtube_ff_link": "",
                        "youtube_ff_id": "",
                        "bunny_ff_link": "",
                        "bunny_ff_subtitles": "",
                        "youtube_prerecorded_link": "",
                        "youtube_prerecorded_id": "",
                        "bunny_prerecorded_link": "",
                        "bunny_prerecorded_subtitles": ""
                    },
                    {
                        "slot_id": "w-vis4dh-1006",
                        "session_id": "w1",
                        "title": "Do Words Matter: Visualising Historical Policy and Media Narratives around Opportunity and Disadvantage in Australia",
                        "contributors": [
                            "Sarah Goodwin"
                        ],
                        "authors": [
                            "Sarah Goodwin",
                            "Simon D Angus",
                            "Lachlan O'Neill",
                            "Nancy Van Nieuwenhove",
                            "Ben Wu",
                            "Yingqi Zhang",
                            "Tim Dwyer"
                        ],
                        "abstract": "Despite the widely held belief that public discourse shapes and informs public policy, tracking and analysing the dynamics of public discourse over long time-frames remains a significant challenge. Myriad factors such as editorial policies, news sensationalism, election cycles, societal priorities, and political agendas can all impact the attention given, and treatment of, a range of important societal issues such as systematic disadvantage. Here, we introduce and describe `Discourse of the Past', an interactive visualisation created for both public touch-screen exhibition and online. The visualisation presents an AI-assisted analysis of hundreds of thousands of op-ed news articles and speeches from the major Australian mastheads and federal parliament respectively. By focusing on 23 population groups and 33 issues, we provide a rich, dynamic picture of how disadvantage is experienced in Australia and by whom. Users can discover a series of findings, such as: how News and Parliament have their own agenda and how each changes its focus over time; how some issues are more recurrent than others; how coverage and discourse intensity change relative to cycles and events; and how both discourses contribute to a better understanding of how disadvantage is lived in Australia.",
                        "uid": "w-vis4dh-1006",
                        "time_stamp": "2023-10-23T03:00:00Z",
                        "time_start": "2023-10-23T03:00:00Z",
                        "time_end": "2023-10-23T06:00:00Z",
                        "paper_type": "workshop",
                        "keywords": [],
                        "doi": "",
                        "fno": "",
                        "has_image": true,
                        "has_pdf": true,
                        "paper_award": "",
                        "image_caption": "Discourse of the Past: An exploratory visualisation tool with story narrative to highlight patterns of how Australian News coverage varies from that of the Parliament (Senate and House) when discussing disadvantaged groups or issues.",
                        "external_paper_link": "",
                        "youtube_ff_link": "",
                        "youtube_ff_id": "",
                        "bunny_ff_link": "",
                        "bunny_ff_subtitles": "",
                        "youtube_prerecorded_link": "",
                        "youtube_prerecorded_id": "",
                        "bunny_prerecorded_link": "",
                        "bunny_prerecorded_subtitles": ""
                    },
                    {
                        "slot_id": "w-vis4dh-1008",
                        "session_id": "w1",
                        "title": "A Workflow Approach to Visualization-Based Storytelling with Cultural Heritage Data",
                        "contributors": [
                            "Florian Windhager"
                        ],
                        "authors": [
                            "Johannes Liem",
                            "Jakob Kusnick",
                            "Samuel Beck",
                            "Florian Windhager",
                            "Eva Mayr"
                        ],
                        "abstract": "Stories are as old as human history\u2013-and a powerful means for the engaging communication of information, especially in combination with visualizations. The InTaVia project is built on this intersection and has developed a platform which supports the workflow of cultural heritage experts to create compelling visualization-based stories: From the search for relevant cultural objects and actors in a cultural knowledge graph, to the curation and visual analysis of the selected information, and to the creation of stories based on these data and visualizations, which can be shared with the interested public.",
                        "uid": "w-vis4dh-1008",
                        "time_stamp": "2023-10-23T03:00:00Z",
                        "time_start": "2023-10-23T03:00:00Z",
                        "time_end": "2023-10-23T06:00:00Z",
                        "paper_type": "workshop",
                        "keywords": [],
                        "doi": "",
                        "fno": "",
                        "has_image": true,
                        "has_pdf": true,
                        "paper_award": "",
                        "image_caption": "The InTaVia platform is designed to support diverse cultural heritage data practices through visualization-based interfaces. This includes activities like searching, creating, curating, analyzing, and communicating for a wide range of users. The platform follows an iterative workflow model, represented by blue arrows, with main modules such as the Data Curation Lab, Visual Analytics Studio, and Storytelling Suite covering each stage of development.",
                        "external_paper_link": "",
                        "youtube_ff_link": "",
                        "youtube_ff_id": "",
                        "bunny_ff_link": "https://ieeevis-uploads.b-cdn.net/vis23/w-vis4dh/w-vis4dh-1008/w-vis4dh-1008_Preview.mp4?token=JeL7bnH3f6wnh9_CcYJ3Wotb_Higmy32VVvbEwHNpC0&expires=1706590800",
                        "bunny_ff_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/w-vis4dh/w-vis4dh-1008/w-vis4dh-1008_Preview.vtt?token=F4rEPDh37b_zH65prvu67x-h-COKTOd1zC0oGdJUawk&expires=1706590800",
                        "youtube_prerecorded_link": "",
                        "youtube_prerecorded_id": "",
                        "bunny_prerecorded_link": "",
                        "bunny_prerecorded_subtitles": ""
                    }
                ]
            }
        ]
    },
    "w-topoinvis": {
        "event": "TopoInVis: Topological Data Analysis and Visualization",
        "long_name": "TopoInVis: Topological Data Analysis and Visualization",
        "event_type": "workshop",
        "event_prefix": "w-topoinvis",
        "event_description": "",
        "event_url": "",
        "organizers": [
            "Paul Rosen"
        ],
        "sessions": [
            {
                "title": "TopoInVis: Topological Data Analysis and Visualization",
                "session_id": "w3",
                "event_prefix": "w-topoinvis",
                "track": "oneohsix",
                "session_image": "w3.png",
                "chair": [
                    "Paul Rosen"
                ],
                "time_start": "2023-10-22T03:00:00Z",
                "time_end": "2023-10-22T06:00:00Z",
                "discord_category": "",
                "discord_channel": "106",
                "discord_channel_id": "1161741373410644119",
                "discord_link": "https://discordapp.com/channels/1161400451619627068/1161741373410644119",
                "zoom_private_meeting": "",
                "zoom_private_password": "",
                "zoom_private_link": "",
                "zoom_broadcast_link": "",
                "ff_link": "",
                "time_slots": [
                    {
                        "slot_id": "w-topoinvis-1006",
                        "session_id": "w3",
                        "title": "Homology-Preserving Multi-Scale Graph Skeletonization Using Mapper on Graphs",
                        "contributors": [
                            "Paul Rosen"
                        ],
                        "authors": [
                            "Paul Rosen",
                            "Mustafa Hajij",
                            "Bei Wang"
                        ],
                        "abstract": "Node-link diagrams are a popular method for representing graphs that capture relationships between individuals, businesses, proteins, and telecommunication endpoints. However, node-link diagrams may fail to convey insights regarding graph structures, even for moderately sized data of a few hundred nodes, due to visual clutter. We propose to apply the mapper construction---a popular tool in topological data analysis---to graph visualization, which provides a strong theoretical basis for summarizing the data while preserving their core structures. We develop a variation of the mapper construction targeting weighted, undirected graphs, called mapper on graphs, which generates homology-preserving skeletons of graphs. We further show how the adjustment of a single parameter enables multi-scale skeletonization of the input graph. We provide a software tool that enables interactive explorations of such skeletons and demonstrate the effectiveness of our method for synthetic and real-world data.",
                        "uid": "w-topoinvis-1006",
                        "time_stamp": "2023-10-22T03:00:00Z",
                        "time_start": "2023-10-22T03:00:00Z",
                        "time_end": "2023-10-22T06:00:00Z",
                        "paper_type": "workshop",
                        "keywords": [],
                        "doi": "",
                        "fno": "",
                        "has_image": true,
                        "has_pdf": true,
                        "paper_award": "",
                        "image_caption": "We present our technique, called Mapper on Graph, which adapts the mapper construction\u2014a popular tool from topological data analysis\u2014to the visualization of graphs. It provides multi-scale skeletonizations of the graph from diverse perspectives. By modifying a single input parameter, namely the number of cover elements, our approach provides a multi-scale skeletonization of the input graph that emphasizes the property of interest. In the examples presented here, the visible level-of-detail is reduced as the number of elements decreases while the homology of the graph\u2014in the form of components and tunnels\u2014remains well persevered.",
                        "external_paper_link": "",
                        "youtube_ff_link": "",
                        "youtube_ff_id": "",
                        "bunny_ff_link": "",
                        "bunny_ff_subtitles": "",
                        "youtube_prerecorded_link": "",
                        "youtube_prerecorded_id": "",
                        "bunny_prerecorded_link": "",
                        "bunny_prerecorded_subtitles": ""
                    },
                    {
                        "slot_id": "w-topoinvis-1007",
                        "session_id": "w3",
                        "title": "Planar Symmetry Detection and Quantification using the Extended Persistent Homology Transform",
                        "contributors": [
                            "Mr Nicholas A Bermingham"
                        ],
                        "authors": [
                            "Nicholas A Bermingham",
                            "Vanessa Robins",
                            "Katharine Turner"
                        ],
                        "abstract": "Symmetry is ubiquitous throughout nature and can often give great insights into the formation, structure and stability of objects studied by mathematicians, physicists, chemists and biologists. However, perfect symmetry occurs rarely so quantitative techniques must be developed to identify approximate symmetries. To facilitate the analysis of an independent variable on the symmetry of some object, we would like this quantity to be a smoothly varying real parameter rather than a boolean one. The extended persistent homology transform is a recently developed tool which can be used to define a distance between certain kinds of objects. Here, we describe  how the extended persistent homology transform can be used to visualise, detect and quantify certain kinds of symmetry and discuss the effectiveness and limitations of this method.",
                        "uid": "w-topoinvis-1007",
                        "time_stamp": "2023-10-22T03:00:00Z",
                        "time_start": "2023-10-22T03:00:00Z",
                        "time_end": "2023-10-22T06:00:00Z",
                        "paper_type": "workshop",
                        "keywords": [],
                        "doi": "",
                        "fno": "",
                        "has_image": false,
                        "has_pdf": true,
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "youtube_ff_link": "",
                        "youtube_ff_id": "",
                        "bunny_ff_link": "",
                        "bunny_ff_subtitles": "",
                        "youtube_prerecorded_link": "",
                        "youtube_prerecorded_id": "",
                        "bunny_prerecorded_link": "",
                        "bunny_prerecorded_subtitles": ""
                    },
                    {
                        "slot_id": "w-topoinvis-1008",
                        "session_id": "w3",
                        "title": "Visualizing Topological Importance: A Class-Driven Approach",
                        "contributors": [
                            "Brian Summa"
                        ],
                        "authors": [
                            "Yu Qin",
                            "Brittany Terese Fasy",
                            "Carola Wenk",
                            "Brian Summa"
                        ],
                        "abstract": "This paper presents the first approach to visualize the importance of topological features that define classes of data. Topological features, with their ability to abstract the fundamental structure of complex data, are an integral component of visualization and analysis pipelines. Although not all topological features present in data are of equal importance. To date, the default definition of feature importance is often assumed and fixed. This work shows how proven explainable deep learning approaches can be adapted for use in topological classification. In doing so, it provides the first technique that illuminates what topological structures are important in each dataset in regards to their class label. In particular, the approach uses a learned metric classifier with a density estimator of the points of a persistence diagram as input. This metric learns how to reweigh this density such that classification accuracy is high. By extracting this weight, an importance field on persistent point density can be created. This provides an intuitive representation of persistence point importance that can be used to drive new visualizations. This work provides two examples: Visualization on each diagram directly and, in the case of sublevel set filtrations on images, directly on the images themselves. This work highlights real-world examples of this approach visualizing the important topological features in graph, 3D shape, and medical image data.",
                        "uid": "w-topoinvis-1008",
                        "time_stamp": "2023-10-22T03:00:00Z",
                        "time_start": "2023-10-22T03:00:00Z",
                        "time_end": "2023-10-22T06:00:00Z",
                        "paper_type": "workshop",
                        "keywords": [],
                        "doi": "",
                        "fno": "",
                        "has_image": true,
                        "has_pdf": true,
                        "paper_award": "",
                        "image_caption": "(a) A common task in topological data analysis: extracting a persistence diagram of topological features. In this case, features are based on the sublevel set filtration of pathology images with class labels (Gleason grade) that define the progression of prostate cancer~\\cite{lawson2019persistent}. Knowing which features are important for each class is commonly an educated guess with the lifetime of a feature (persistence) often assumed to define importance. (b) Our approach, based on a learned metric classifier, takes as input the unweighted density of persistence points and reweighs this density based on what best defines a class. This allows us to build a field of importance for regions of a diagram. (c) This importance field can be used to create visualizations to illuminate which features drive a classification. For example, it can highlight what points are important directly in a diagram or, in the case of sublevel set filtrations, visualize the important structure directly in an image. Consider that a hallmark of prostate cancer is gland degeneration as the disease progresses.  Calcifications (red arrow) are only present in well-structured glands and are highlighted as important structures for Gleason 3, an earlier stage of the disease.",
                        "external_paper_link": "",
                        "youtube_ff_link": "",
                        "youtube_ff_id": "",
                        "bunny_ff_link": "",
                        "bunny_ff_subtitles": "",
                        "youtube_prerecorded_link": "",
                        "youtube_prerecorded_id": "",
                        "bunny_prerecorded_link": "",
                        "bunny_prerecorded_subtitles": ""
                    },
                    {
                        "slot_id": "w-topoinvis-1009",
                        "session_id": "w3",
                        "title": "Taming Horizontal Instability in Merge Trees: On the Computation of a Comprehensive Deformation-based Edit Distance",
                        "contributors": [
                            "Markus Anders"
                        ],
                        "authors": [
                            "Florian Wetzels",
                            "Markus Anders",
                            "Christoph Garth"
                        ],
                        "abstract": "Comparative analysis of scalar fields in scientific visualization often involves distance functions on topological abstractions. This paper focuses on the merge tree abstraction (representing the nesting of sub- or superlevel sets) and proposes the application of the unconstrained deformation-based edit distance. Previous approaches on merge trees often suffer from instability: small perturbations in the data can lead to large distances of the abstractions. While some existing methods can handle so-called vertical instability, the unconstrained deformation-based edit distance addresses both vertical and horizontal instabilities, also called saddle swaps. We establish the computational complexity as NP-complete, and provide an integer linear program formulation for computation. Experimental results on the TOSCA shape matching ensemble provide evidence for the stability of the proposed distance. We thereby showcase the potential of handling saddle swaps for comparison of scalar fields through merge trees.",
                        "uid": "w-topoinvis-1009",
                        "time_stamp": "2023-10-22T03:00:00Z",
                        "time_start": "2023-10-22T03:00:00Z",
                        "time_end": "2023-10-22T06:00:00Z",
                        "paper_type": "workshop",
                        "keywords": [],
                        "doi": "",
                        "fno": "",
                        "has_image": true,
                        "has_pdf": true,
                        "paper_award": "",
                        "image_caption": "Illustration of the improvements of unconstrained edit distances over constrained edit distances on the TOSCA ensemble: two embedded mappings of critical points are shown on the left, the same mappings in a typical merge tree layout on the right. In between, distance matrices for multiple members of the ensemble can be found.",
                        "external_paper_link": "",
                        "youtube_ff_link": "",
                        "youtube_ff_id": "",
                        "bunny_ff_link": "",
                        "bunny_ff_subtitles": "",
                        "youtube_prerecorded_link": "https://youtu.be/_CcQQJ0Y_vE",
                        "youtube_prerecorded_id": "_CcQQJ0Y_vE",
                        "bunny_prerecorded_link": "https://ieeevis-uploads.b-cdn.net/vis23/w-topoinvis/w-topoinvis-1009/w-topoinvis-1009_Presentation.mp4?token=sboylNvt4L4D1HAejV4-u9NX6VcsyEJseqZI-pg0UwY&expires=1706590800",
                        "bunny_prerecorded_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/w-topoinvis/w-topoinvis-1009/w-topoinvis-1009_Presentation.vtt?token=i3VdNa9BfkDvSnK1Zx-opkEiNEAjFQuB3xZIZNdtfZw&expires=1706590800"
                    },
                    {
                        "slot_id": "w-topoinvis-1010",
                        "session_id": "w3",
                        "title": "A Mathematical Foundation for the Spatial Uncertainty of Critical Points in Probabilistic Scalar Fields",
                        "contributors": [
                            "Dominik Vietinghoff"
                        ],
                        "authors": [
                            "Dominik Vietinghoff",
                            "Michael B\u00f6ttinger",
                            "Gerik Scheuermann",
                            "Christian Heine"
                        ],
                        "abstract": "Critical points mark locations in the domain where the level-set topology of a scalar function undergoes fundamental changes and thus indicate potentially interesting features in the data. Established methods exist to locate and relate such points in a deterministic setting, but it is less well understood how the concept of critical points can be extended to the analysis of uncertain data. Most methods for this task aim at finding likely locations of critical points or estimate the probability of their occurrence locally but do not indicate if critical points at potentially different locations in different realizations of a stochastic process are manifestations of the same feature, which is required to characterize the spatial uncertainty of critical points. Previous work on relating critical points across different realizations reported challenges for interpreting the resulting spatial distribution of critical points but did not investigate the causes. In this work, we provide a mathematical formulation of the problem of finding critical points with spatial uncertainty and computing their spatial distribution, which leads us to the notion of uncertain critical points. We analyze the theoretical properties of these structures and highlight connections to existing works for special classes of uncertain fields. We derive conditions under which well-interpretable results can be obtained and discuss the implications of those restrictions for the field of visualization. We demonstrate that the discussed limitations are not purely academic but also arise in real-world data.",
                        "uid": "w-topoinvis-1010",
                        "time_stamp": "2023-10-22T03:00:00Z",
                        "time_start": "2023-10-22T03:00:00Z",
                        "time_end": "2023-10-22T06:00:00Z",
                        "paper_type": "workshop",
                        "keywords": [],
                        "doi": "",
                        "fno": "",
                        "has_image": true,
                        "has_pdf": true,
                        "paper_award": "",
                        "image_caption": "Set of critical points of the function x^2/10 + a*sin(x) is decomposed into connected components by poles (black lines) and degenerate critical points (gray lines) where the critical type changes. We call these components uncertain critical points. If the randomness of the field is described by the random parameter 'a' following a certain distribution (left), we can compute the density at each point along the graph (color map on the right). Projection on the domain (bottom) yields the spatial distribution of critical points with spatial uncertainty. We analyze this in a more general setting in our paper.",
                        "external_paper_link": "",
                        "youtube_ff_link": "",
                        "youtube_ff_id": "",
                        "bunny_ff_link": "",
                        "bunny_ff_subtitles": "",
                        "youtube_prerecorded_link": "",
                        "youtube_prerecorded_id": "",
                        "bunny_prerecorded_link": "",
                        "bunny_prerecorded_subtitles": ""
                    },
                    {
                        "slot_id": "w-topoinvis-1012",
                        "session_id": "w3",
                        "title": "Probabilistic Gradient-Based Extrema Tracking",
                        "contributors": [
                            "Emma Nilsson"
                        ],
                        "authors": [
                            "Emma Nilsson",
                            "Jonas Lukasczyk",
                            "Talha Bin Masood",
                            "Christoph Garth",
                            "Ingrid Hotz"
                        ],
                        "abstract": "Feature tracking is a common task in visualization applications, where methods based on topological data analysis (TDA) have successfully been applied in the past for feature definition as well as tracking. In this work, we focus on tracking extrema of temporal scalar fields. A family of TDA approaches address this task by establishing one-to-one correspondences between extrema based on discrete gradient vector fields. More specifically, two extrema of subsequent time steps are matched if they fall into their respective ascending and descending manifolds. However, due to this one-to-one assignment, these approaches are prone to fail where, e.g., extrema are located in regions with low gradient magnitude, or are located close to boundaries of the manifolds. Therefore, we propose a probabilistic matching that captures a larger set of possible correspondences via neighborhood sampling, or by computing the overlap of the manifolds. We illustrate the usefulness of the approach with two application cases.",
                        "uid": "w-topoinvis-1012",
                        "time_stamp": "2023-10-22T03:00:00Z",
                        "time_start": "2023-10-22T03:00:00Z",
                        "time_end": "2023-10-22T06:00:00Z",
                        "paper_type": "workshop",
                        "keywords": [],
                        "doi": "",
                        "fno": "",
                        "has_image": true,
                        "has_pdf": true,
                        "paper_award": "",
                        "image_caption": "The image shows our result using gradient-based tracking on maxima of a valence electronic density distribution in a molecular dynamics simulation. The graph in (e) shows feature changes between time steps, while the top row (a-d) shows volume renderings of the density and the maxima. Each interesting feature is colored and highlighted based on the index of the maximum. In (f), we show closeups of a split event using the graph. On top, the line thickness encodes a binary matching and below, the line thickness encodes four probability categories based on our proposed maximum descending manifold overlap.",
                        "external_paper_link": "",
                        "youtube_ff_link": "",
                        "youtube_ff_id": "",
                        "bunny_ff_link": "https://ieeevis-uploads.b-cdn.net/vis23/w-topoinvis/w-topoinvis-1012/w-topoinvis-1012_Preview.mp4?token=AXA8gKqOAnsj9QoteydwFWYVQCXz55hCh-0Gh1NACzk&expires=1706590800",
                        "bunny_ff_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/w-topoinvis/w-topoinvis-1012/w-topoinvis-1012_Preview.vtt?token=UuuYT_MIT8yHjt7p8Byw_-HU9KGonPPRaihhaUzRCQo&expires=1706590800",
                        "youtube_prerecorded_link": "https://youtu.be/sxmb9FwzBkY",
                        "youtube_prerecorded_id": "sxmb9FwzBkY",
                        "bunny_prerecorded_link": "https://ieeevis-uploads.b-cdn.net/vis23/w-topoinvis/w-topoinvis-1012/w-topoinvis-1012_Presentation.mp4?token=UZZvYVFVuW5xmHYsR6Ce724MMFBRxaTI1iMcg1QyPJY&expires=1706590800",
                        "bunny_prerecorded_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/w-topoinvis/w-topoinvis-1012/w-topoinvis-1012_Presentation.vtt?token=uFJuBUQVb6uyLiE2HKjCVYTX9iL-zKY1FTKELHmXGHc&expires=1706590800"
                    },
                    {
                        "slot_id": "w-topoinvis-1014",
                        "session_id": "w3",
                        "title": "Sketching Merge Trees for Scientific Visualization",
                        "contributors": [
                            "Mingzhe Li"
                        ],
                        "authors": [
                            "Mingzhe Li",
                            "Sourabh Palande",
                            "Lin Yan",
                            "Bei Wang"
                        ],
                        "abstract": "Merge trees are a type of topological descriptors that record the connectivity among the sublevel sets of scalar fields. They are among the most widely used topological tools in visualization. In this paper, we are interested in sketching a set of merge trees using techniques from matrix sketching. That is, given a large set T of merge trees, we would like to find a much smaller set of basis trees S such that each tree in T can be approximately reconstructed from a linear combination of merge trees in S. A set of high-dimensional vectors can be approximated via matrix sketching techniques such as principal component analysis and column subset selection. However, until now, there has not been any work on sketching a set of merge trees. We develop a framework for sketching a set of merge trees that combines matrix sketching with tools from optimal transport. In particular, we vectorize a set of merge trees into high-dimensional vectors while preserving their structures and structural relations. We demonstrate the applications of our framework in sketching merge trees that arise from time-varying scientific simulations. Specifically, our framework obtains a set of basis trees as representatives that capture the \u201cmodes\u201d of physical phenomena for downstream analysis and visualization.",
                        "uid": "w-topoinvis-1014",
                        "time_stamp": "2023-10-22T03:00:00Z",
                        "time_start": "2023-10-22T03:00:00Z",
                        "time_end": "2023-10-22T06:00:00Z",
                        "paper_type": "workshop",
                        "keywords": [],
                        "doi": "",
                        "fno": "",
                        "has_image": true,
                        "has_pdf": true,
                        "paper_award": "",
                        "image_caption": "Left: the pipeline of our framework for apply matrix sketching techniques to a set of merge trees.   Right: results for the Rotating Gaussian dataset. Basis merge trees selected by our framework can represent the structure of all merge trees.",
                        "external_paper_link": "",
                        "youtube_ff_link": "",
                        "youtube_ff_id": "",
                        "bunny_ff_link": "https://ieeevis-uploads.b-cdn.net/vis23/w-topoinvis/w-topoinvis-1014/w-topoinvis-1014_Preview.mp4?token=qujIUsPWrjWQ_CfWNablUxvC3XiL5ioPTOC2CIyhjjA&expires=1706590800",
                        "bunny_ff_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/w-topoinvis/w-topoinvis-1014/w-topoinvis-1014_Preview.vtt?token=aKxlEuVb7pVDZ7XfVzJqrgM9GsstGkZXRTLT6mjV8Mk&expires=1706590800",
                        "youtube_prerecorded_link": "https://youtu.be/DswL2Wxt5OE",
                        "youtube_prerecorded_id": "DswL2Wxt5OE",
                        "bunny_prerecorded_link": "https://ieeevis-uploads.b-cdn.net/vis23/w-topoinvis/w-topoinvis-1014/w-topoinvis-1014_Presentation.mp4?token=Q481MaDqO2Bf0Dbg4KChII2HlQDjJGXrMh4ylIzZEn8&expires=1706590800",
                        "bunny_prerecorded_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/w-topoinvis/w-topoinvis-1014/w-topoinvis-1014_Presentation.vtt?token=qMPcahdBwbUvEPGd-h0ze0_Mbgy9hNvfm7rxWf7FFuU&expires=1706590800"
                    },
                    {
                        "slot_id": "w-topoinvis-1015",
                        "session_id": "w3",
                        "title": "Comparing Mapper Graphs of Artificial Neuron Activations",
                        "contributors": [
                            "Bei Wang"
                        ],
                        "authors": [
                            "Youjia Zhou",
                            "Helen Jenne",
                            "Davis Brown",
                            "Madelyn R Shapiro",
                            "Brett A Jefferson",
                            "Cliff Joslyn",
                            "Gregory Henselman-Petrusek",
                            "Brenda Praggastis",
                            "Emilie Purvine",
                            "Bei Wang"
                        ],
                        "abstract": "The mapper graph is a popular tool from topological data analysis that provides a graphical summary of point cloud data. It has been used to study data from cancer research, sports analytics, neurosciences, and machine learning. In particular, mapper graphs have been used recently to visualize the topology of high-dimensional artificial neural activations from convolutional neural networks and large language models. However, a key question that arises from using mapper graphs across applications is how to compare mapper graphs to study their structural differences. In this paper, we introduce a distance between mapper graphs using tools from optimal transport. We demonstrate the utility of such a distance by studying the topological changes of neural activations across convolutional layers in deep learning, as well as by capturing the loss of structural information for a multiscale mapper.",
                        "uid": "w-topoinvis-1015",
                        "time_stamp": "2023-10-22T03:00:00Z",
                        "time_start": "2023-10-22T03:00:00Z",
                        "time_end": "2023-10-22T06:00:00Z",
                        "paper_type": "workshop",
                        "keywords": [],
                        "doi": "",
                        "fno": "",
                        "has_image": true,
                        "has_pdf": true,
                        "paper_award": "",
                        "image_caption": "Mapper graphs have been used to visualize the topology of artificial neural activations from convolutional neural networks and large language models. A key question is how to compare mapper graphs to study their structural differences. We introduce a distance between mapper graphs using tools from optimal transport. We demonstrate the utility of such a distance by studying the topological changes of neural activations across convolutional layers in deep learning. Here we show a color transfer from the mapper graph nodes in two adjacent layers that highlights structural correspondences between the neuron activations.",
                        "external_paper_link": "",
                        "youtube_ff_link": "",
                        "youtube_ff_id": "",
                        "bunny_ff_link": "https://ieeevis-uploads.b-cdn.net/vis23/w-topoinvis/w-topoinvis-1015/w-topoinvis-1015_Preview.mp4?token=bzw9Rtok0x_1ETih4T1A0UmUKsz_GnPDMe-eVbaJ9_U&expires=1706590800",
                        "bunny_ff_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/w-topoinvis/w-topoinvis-1015/w-topoinvis-1015_Preview.vtt?token=bzp9CjtOwQgQwhsNoHT_lYb_xf355rM2DsD5C4HXiD0&expires=1706590800",
                        "youtube_prerecorded_link": "",
                        "youtube_prerecorded_id": "",
                        "bunny_prerecorded_link": "",
                        "bunny_prerecorded_subtitles": ""
                    },
                    {
                        "slot_id": "w-topoinvis-1016",
                        "session_id": "w3",
                        "title": "Multi-field Visualisation via Trait-induced Merge Trees",
                        "contributors": [
                            "Ingrid Hotz"
                        ],
                        "authors": [
                            "Jochen Jankowai",
                            "Talha Bin Masood",
                            "Ingrid Hotz"
                        ],
                        "abstract": "In this work, we propose trait-based merge trees a generalization of merge trees to feature level sets, targeting the analysis of tensor field or general multi-variate data. For this, we employ the notion of traits defined in attribute space as introduced in the feature level sets framework. The resulting distance field in attribute space induces a scalar field in the spatial domain that serves as input for topological data analysis. The leaves in the merge tree represent those areas in the input data that are closest to the defined trait and thus most closely resemble the defined feature. Hence, the merge tree yields a hierarchy of features that allows for querying the most relevant and persistent features. The presented method includes different query methods for the tree which enable the highlighting of different aspects. We demonstrate the cross-application capabilities of this approach with three case studies from different domains.",
                        "uid": "w-topoinvis-1016",
                        "time_stamp": "2023-10-22T03:00:00Z",
                        "time_start": "2023-10-22T03:00:00Z",
                        "time_end": "2023-10-22T06:00:00Z",
                        "paper_type": "workshop",
                        "keywords": [],
                        "doi": "",
                        "fno": "",
                        "has_image": false,
                        "has_pdf": true,
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "youtube_ff_link": "",
                        "youtube_ff_id": "",
                        "bunny_ff_link": "",
                        "bunny_ff_subtitles": "",
                        "youtube_prerecorded_link": "",
                        "youtube_prerecorded_id": "",
                        "bunny_prerecorded_link": "",
                        "bunny_prerecorded_subtitles": ""
                    },
                    {
                        "slot_id": "w-topoinvis-1018",
                        "session_id": "w3",
                        "title": "Combinatorial Exploration of Morse\u2013Smale Functions on the Sphere via Interactive Visualization",
                        "contributors": [
                            "Bei Wang"
                        ],
                        "authors": [
                            "Youjia Zhou",
                            "Janis Lazovskis",
                            "Michael J. Catanzaro",
                            "Matthew Zabka",
                            "Bei Wang"
                        ],
                        "abstract": "In this paper, we are interested in the characterization and classification of Morse\u2013Smale functions. To that end, we present MSF Designer, an interactive visualization tool that supports the combinatorial exploration of Morse\u2013Smale functions on the sphere. Our tool supports the design and visualization of a Morse\u2013Smale function in a simple way using fundamental moves, which are combinatorial operations introduced by Catanzaro et al. that modify the Morse\u2013Smale graph of the function. It also provides fine-grained control over the geometry and topology of its gradient vector field. The tool is designed to help mathematicians explore the complex configuration spaces of Morse\u2013Smale functions, as well as their associated gradient vector fields and Morse\u2013Smale complexes. Understanding these spaces will help mathematicians expand their applicability in topological data analysis and visualization. In particular, our tool helps topologists, geometers, and combinatorialists explore invariants in the classification of vector fields and characterize Morse functions in the persistent homology setting.",
                        "uid": "w-topoinvis-1018",
                        "time_stamp": "2023-10-22T03:00:00Z",
                        "time_start": "2023-10-22T03:00:00Z",
                        "time_end": "2023-10-22T06:00:00Z",
                        "paper_type": "workshop",
                        "keywords": [],
                        "doi": "",
                        "fno": "",
                        "has_image": true,
                        "has_pdf": true,
                        "paper_award": "",
                        "image_caption": "We present MSF Designer, a visualization tool that supports the combinatorial exploration of Morse-Smale functions on the sphere. The interface consists of (A) Function and flow visualization panel supports modifying the topology and geometry of the Morse-Smale graph of the function and visualizes the dynamics of its underlying gradient vector field; (B) Elementary moves panel provides a set of elementary moves as fundamental building blocks of a Morse-Smale function; (C) Function adjustment panel allows modifying the function values at singularities; (D) History panel provides undo and redo features; and (E) Barcode panel computes and displays barcodes to guide persistence simplification.",
                        "external_paper_link": "",
                        "youtube_ff_link": "",
                        "youtube_ff_id": "",
                        "bunny_ff_link": "https://ieeevis-uploads.b-cdn.net/vis23/w-topoinvis/w-topoinvis-1018/w-topoinvis-1018_Preview.mp4?token=5MAEECLAKHY6S3Ww9peieTU1liLL1Rs0rnqsepyBRhg&expires=1706590800",
                        "bunny_ff_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/w-topoinvis/w-topoinvis-1018/w-topoinvis-1018_Preview.vtt?token=8UIrkt1bTUq2QrkbIjr2PF8snQ8cE0t_WeM9v3q0Klk&expires=1706590800",
                        "youtube_prerecorded_link": "",
                        "youtube_prerecorded_id": "",
                        "bunny_prerecorded_link": "",
                        "bunny_prerecorded_subtitles": ""
                    }
                ]
            }
        ]
    },
    "w-vis4good": {
        "event": "Visualization for Social Good",
        "long_name": "Visualization for Social Good",
        "event_type": "workshop",
        "event_prefix": "w-vis4good",
        "event_description": "",
        "event_url": "",
        "organizers": [
            "Evanthia Dimara",
            "Uzma Haque Syeda",
            "Narges Mahyar",
            "Delvin Varghese",
            "Emily Wall"
        ],
        "sessions": [
            {
                "title": "Visualization for Social Good",
                "session_id": "w6",
                "event_prefix": "w-vis4good",
                "track": "oneohfive",
                "session_image": "w6.png",
                "chair": [
                    "Evanthia Dimara",
                    "Uzma Haque Syeda",
                    "Narges Mahyar",
                    "Delvin Varghese",
                    "Emily Wall"
                ],
                "time_start": "2023-10-22T22:00:00Z",
                "time_end": "2023-10-23T01:00:00Z",
                "discord_category": "",
                "discord_channel": "105",
                "discord_channel_id": "1161741309346861067",
                "discord_link": "https://discordapp.com/channels/1161400451619627068/1161741309346861067",
                "zoom_private_meeting": "",
                "zoom_private_password": "",
                "zoom_private_link": "",
                "zoom_broadcast_link": "",
                "ff_link": "",
                "time_slots": [
                    {
                        "slot_id": "w-vis4good-1892",
                        "session_id": "w6",
                        "title": "Supporting Mathematical Education with Interactive Visual Proofs",
                        "contributors": [
                            "Moritz Weckbecker"
                        ],
                        "authors": [
                            "Moritz Weckbecker, Solida Neziri, Georges Hattab"
                        ],
                        "abstract": "While visual proofs are already a valuable tool for conveying the logical ideas behind mathematical proofs, their potential for explaining and engaging can be enhanced by the addition of interactive elements. We present an open platform for interactive visual proofs that is freely available for students and teachers alike at https://visualproofs.github.io/, and which can be expanded by the community. Our platform has the potential to support mathematical education by providing students with a more interactive and engaging way to learn mathematical concepts and to show them directly the importance of proofs in mathematics. We aim to support equity in mathematical education by making our platform free, participatory and open to multiple languages. We encourage the community to contribute to its development and expansions through the GitHub repository available under CC BY NC 4.0 International License as well as through conducting future research on the effects of interactive visual proofs in education.",
                        "uid": "w-vis4good-1892",
                        "time_stamp": "2023-10-22T22:00:00Z",
                        "time_start": "2023-10-22T22:00:00Z",
                        "time_end": "2023-10-23T01:00:00Z",
                        "paper_type": "workshop",
                        "keywords": [],
                        "doi": "",
                        "fno": "",
                        "has_image": true,
                        "has_pdf": true,
                        "paper_award": "",
                        "image_caption": "Visual proofs are illustrations, which demonstrate the validity of a mathematical statement in a self-evident manner. Their potential for explaining and engaging can be enhanced by the addition of interactive elements. We present an open platform for interactive visual proofs that is freely available for students and teachers alike. We aim to support equity in mathematical education by creating the first platform of this kind which is free, participatory and multilingual. Check it out at visualproofs.github.io and find the paper and the rest of our work at visualization.group!",
                        "external_paper_link": "",
                        "youtube_ff_link": "",
                        "youtube_ff_id": "",
                        "bunny_ff_link": "https://ieeevis-uploads.b-cdn.net/vis23/w-vis4good/w-vis4good-1892/w-vis4good-1892_Preview.mp4?token=4yy0st3zzejCchHROclEwLxiLR4K7-3luglSjoZE2MM&expires=1706590800",
                        "bunny_ff_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/w-vis4good/w-vis4good-1892/w-vis4good-1892_Preview.vtt?token=eiGFMJ3vYvlMLzyfEsR2qJZBgaktwPFMOVSd3yrRxik&expires=1706590800",
                        "youtube_prerecorded_link": "https://youtu.be/6d5MksAX3NU",
                        "youtube_prerecorded_id": "6d5MksAX3NU",
                        "bunny_prerecorded_link": "https://ieeevis-uploads.b-cdn.net/vis23/w-vis4good/w-vis4good-1892/w-vis4good-1892_Presentation.mp4?token=D8WXmrOsXzlzDY93MpyD0S42LUmUl47zs8oMU45jmwg&expires=1706590800",
                        "bunny_prerecorded_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/w-vis4good/w-vis4good-1892/w-vis4good-1892_Presentation.vtt?token=9-l0mzEfn0QfBqCe35W6b6Vr1MUzaCF9qixmKGUj--0&expires=1706590800"
                    },
                    {
                        "slot_id": "w-vis4good-3038",
                        "session_id": "w6",
                        "title": "From Flowchart to Questionnaire: Increasing Access to Justice via Visualization",
                        "contributors": [
                            "Bei Wang"
                        ],
                        "authors": [
                            "Youjia Zhou, Arul Mishra, Himanshu Mishra, Bei Wang"
                        ],
                        "abstract": "One of the main barriers of access to justice is the lack of awareness of legal rights, procedures, and available resources. In this project, we develop F2Q (Flowchart to Questionnaire), an open-source tool- box for legal experts or staff members at legal clinics or help centers (oftentimes with no programming expertise) to easily design and automatically generate web-based interactive questionnaires. Such questionnaires help guide the clients of these help centers, frequently minorities and underserved populations without legal representation, through a series of questions that help them correctly categorize their legal problems and identify appropriate remedies or solutions. Using F2Q, the questionnaires can be easily modified in case of changes to the law and regulations. These questionnaires serve as a virtual assistant to provide initial guidance to find answers to legal questions and, if needed, to direct the clients to a legal expert at the help center for further assistance. Most importantly, this efficient process could prevent potential clients from giving up their quest for justice due to a lack of awareness or inability to find the appropriate legal aid: when people do not know what questions to ask and where to seek answers, they give up and live with the injustice. F2Q aims to break down, or at least weaken, such barriers.",
                        "uid": "w-vis4good-3038",
                        "time_stamp": "2023-10-22T22:00:00Z",
                        "time_start": "2023-10-22T22:00:00Z",
                        "time_end": "2023-10-23T01:00:00Z",
                        "paper_type": "workshop",
                        "keywords": [],
                        "doi": "",
                        "fno": "",
                        "has_image": true,
                        "has_pdf": true,
                        "paper_award": "",
                        "image_caption": "One of the main barriers of access to justice is the lack of awareness of legal rights, procedures, and available resources. In this project, we develop F2Q (Flowchart to Questionnaire), an open-source toolbox for legal experts or staff members at legal clinics or help centers (oftentimes with no programming expertise) to easily design and automatically generate web-based interactive questionnaires.  Such questionnaires help guide the clients of these help centers through a series of questions that help them correctly categorize their legal problems and identify appropriate remedies or solutions. We show in this figure the visual interface of the tool.",
                        "external_paper_link": "",
                        "youtube_ff_link": "",
                        "youtube_ff_id": "",
                        "bunny_ff_link": "https://ieeevis-uploads.b-cdn.net/vis23/w-vis4good/w-vis4good-3038/w-vis4good-3038_Preview.mp4?token=IAK4DhxyT2kfV95t9rivCtWRxEByhpaexQCRypRlois&expires=1706590800",
                        "bunny_ff_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/w-vis4good/w-vis4good-3038/w-vis4good-3038_Preview.vtt?token=xLqAyFKrxtAMXK69qC5VVLhNkLpoGe8jCTqxy4AVfx8&expires=1706590800",
                        "youtube_prerecorded_link": "",
                        "youtube_prerecorded_id": "",
                        "bunny_prerecorded_link": "",
                        "bunny_prerecorded_subtitles": ""
                    },
                    {
                        "slot_id": "w-vis4good-3347",
                        "session_id": "w6",
                        "title": "Bridging the Divide: Promoting Serendipitous Discovery of Opposing Viewpoints with Visual Analytics in Social Media",
                        "contributors": [
                            "Narges Mahyar"
                        ],
                        "authors": [
                            "Mahmood Jasim, Mumtaz Fatima, Sagarika Ramesh Sonni, Narges Mahyar"
                        ],
                        "abstract": "hile social media promises open access to information, prior works suggest that it also plays a role as a catalyst for the social divide, which is often attributed to a shift towards algorithmic content curation based on users' digital footprints. To combat this issue, methods that support serendipity has received attention in recent years that aim to provide information beyond a user's viewpoint or preference. However, the utility of systems that promote serendipity in raising awareness of opposing viewpoints remains underexplored, especially in the political context. To that end, we conducted a study where we asked 14 participants to explore tweets about two politically charged topics --- gun control and immigration --- using an interaction-driven visual analytics tool that visualizes users' exploration patterns and provides serendipitous suggestions from opposing viewpoints. We found that as participants explored the tweets, they gradually became aware of opposing viewpoints and identified information they did not consider before which helped them gain knowledge about arguments from all sides. We also found while people were keen to use technology that promotes serendipity to cover more topical information, they do not necessarily trust the information found on social media. We hope that our work will motivate future researchers to investigate serendipitous aspects in visualizations to promote a more holistic exploration of various viewpoints.",
                        "uid": "w-vis4good-3347",
                        "time_stamp": "2023-10-22T22:00:00Z",
                        "time_start": "2023-10-22T22:00:00Z",
                        "time_end": "2023-10-23T01:00:00Z",
                        "paper_type": "workshop",
                        "keywords": [],
                        "doi": "",
                        "fno": "",
                        "has_image": true,
                        "has_pdf": true,
                        "paper_award": "",
                        "image_caption": "Serendyze enables several features for social media post exploration including a search option to look for a specific word, a set of filters corresponding to representative pairs of keywords, filters for social media posts with For, Against, and Neutral alignments, three exploration metrics - Visit, Coverage, and Distribution, all social media posts, and suggested social media posts generated by the bias mitigation model that the readers may find interesting. This image was taken during P14\u2019s exploration of social media posts.",
                        "external_paper_link": "",
                        "youtube_ff_link": "",
                        "youtube_ff_id": "",
                        "bunny_ff_link": "",
                        "bunny_ff_subtitles": "",
                        "youtube_prerecorded_link": "",
                        "youtube_prerecorded_id": "",
                        "bunny_prerecorded_link": "",
                        "bunny_prerecorded_subtitles": ""
                    },
                    {
                        "slot_id": "w-vis4good-3888",
                        "session_id": "w6",
                        "title": "The Good Life: visualizing the complexity of supported living for people with disability",
                        "contributors": [
                            "Kate Sweetapple, Jacquie Lorber Kasunic"
                        ],
                        "authors": [
                            "Georgina Hibberd, Jacqueline Lorber Kasunic, Kate Sweetapple"
                        ],
                        "abstract": "This paper outlines the development of an interactive visualization tool, \u2018A Good Life,\u2019 which emerged out of a collaborative project between design researchers and practitioners from the University of Technology Sydney and Northcott, an Australian disability services organisation. Northcott provides supported accommodation services for people living in group homes (up to 6 people) with moderate to severe intellectual and physical disabilities requiring 24-hour support. Supported accommodation provides housing for marginalised and vulnerable people, often with limited resources. Working in this environment can be challenging but also rewarding. However, residents face even greater challenges because decisions made by others primarily determine their quality of life. These decision-makers can include family members or long-term support workers who have a deep understanding of the resident, allied health professionals who interact with the resident regularly but have a limited perspective, and government officials who lack a personal relationship with the resident but formulate policies that consequently have a profound effect on them. A significant issue for people with disability is the lack of visibility or understanding regarding how decisions affect their quality of life. To address this issue, the tool visualizes how decisions can restrict or enhance opportunities for people with disabilities. Additionally, it seeks to improve levels of communication by better expressing the will and preferences of the residents.",
                        "uid": "w-vis4good-3888",
                        "time_stamp": "2023-10-22T22:00:00Z",
                        "time_start": "2023-10-22T22:00:00Z",
                        "time_end": "2023-10-23T01:00:00Z",
                        "paper_type": "workshop",
                        "keywords": [],
                        "doi": "",
                        "fno": "",
                        "has_image": true,
                        "has_pdf": true,
                        "paper_award": "",
                        "image_caption": "The Good Life interactive visualisation tool",
                        "external_paper_link": "",
                        "youtube_ff_link": "",
                        "youtube_ff_id": "",
                        "bunny_ff_link": "",
                        "bunny_ff_subtitles": "",
                        "youtube_prerecorded_link": "",
                        "youtube_prerecorded_id": "",
                        "bunny_prerecorded_link": "",
                        "bunny_prerecorded_subtitles": ""
                    },
                    {
                        "slot_id": "w-vis4good-6432",
                        "session_id": "w6",
                        "title": "Empowering People with Intellectual and Developmental Disabilities through Cognitively Accessible Visualizations",
                        "contributors": [
                            "Keke Wu"
                        ],
                        "authors": [
                            "Keke Wu, Danielle Albers Szafir"
                        ],
                        "abstract": "Data has transformative potential to empower people with Intellectual and Developmental Disabilities (IDD). However, conventional data visualizations often rely on complex cognitive processes, and existing approaches for day-to-day analysis scenarios fail to consider neurodivergent capabilities, creating barriers for people with IDD to access data and leading to even further marginalization. We argue that visualizations could be an equalizer for people with IDD to participate in data-driven conversations. Drawing on preliminary research findings and our experiences working with people with IDD and their data, we introduce and expand on the concept of cognitively accessible visualizations, unpack its meaning and roles in increasing IDD individuals' access to data, and discuss two immediate research objectives. Specifically, we argue that cognitively accessible visualizations should support people with IDD in personal data storytelling for effective self-advocacy and self-expression; and balance novelty and familiarity in data design to accommodate cognitive diversity and promote inclusivity.",
                        "uid": "w-vis4good-6432",
                        "time_stamp": "2023-10-22T22:00:00Z",
                        "time_start": "2023-10-22T22:00:00Z",
                        "time_end": "2023-10-23T01:00:00Z",
                        "paper_type": "workshop",
                        "keywords": [],
                        "doi": "",
                        "fno": "",
                        "has_image": true,
                        "has_pdf": true,
                        "paper_award": "",
                        "image_caption": "The teaser figure illustrates two immediate objectives of cognitively accessible visualizations. Namely, designing for empathetic and therapeutic storytelling, and for multi-sensory and culturally relevant data experiences. The figure illustrates the idea in a creative manner: the four key words are laid out in two separate word clouds. In the middle, the three letters, V, I, S are artistically illustrated with light bulbs and a brain icon to represent cognitive diversity and creativity.",
                        "external_paper_link": "",
                        "youtube_ff_link": "",
                        "youtube_ff_id": "",
                        "bunny_ff_link": "",
                        "bunny_ff_subtitles": "",
                        "youtube_prerecorded_link": "",
                        "youtube_prerecorded_id": "",
                        "bunny_prerecorded_link": "",
                        "bunny_prerecorded_subtitles": ""
                    },
                    {
                        "slot_id": "w-vis4good-6918",
                        "session_id": "w6",
                        "title": "Beyond English: Centering Multilingualism in Data Visualization",
                        "contributors": [
                            "No\u00eblle Rakotondravony"
                        ],
                        "authors": [
                            "No\u00eblle Rakotondravony, Priya Dhawka, Melanie Bancilhon"
                        ],
                        "abstract": "Information visualization and natural language are intricately linked. However, the majority of research and relevant work in information and data visualization (and human-computer interaction) involve English-speaking populations as both researchers and participants, are published in English, and are presented predominantly at English-speaking venues. Although several solutions can be proposed such as translating English texts in visualization to other languages, there is little research that looks at the intersection of data visualization and different languages, and the implications that current visualization practices have on non-English speaking communities. In this position paper, we argue that linguistically diverse communities abound beyond the English-speaking world and offer a richness of experiences for the visualization research community to engage with. Through a case study of how two non-English languages interplay with data visualization reasoning in Madagascar, we describe how monolingualism in data visualization impacts the experiences of underrepresented populations and emphasize potential harm to these communities. Lastly, we raise several questions towards advocating for more inclusive visualization practices that center the diverse experiences of linguistically underrepresented populations.",
                        "uid": "w-vis4good-6918",
                        "time_stamp": "2023-10-22T22:00:00Z",
                        "time_start": "2023-10-22T22:00:00Z",
                        "time_end": "2023-10-23T01:00:00Z",
                        "paper_type": "workshop",
                        "keywords": [],
                        "doi": "",
                        "fno": "",
                        "has_image": true,
                        "has_pdf": true,
                        "paper_award": "",
                        "image_caption": "A picture with the paper title: \u201cBeyond English: Centering Multilingualism in data Visualization\u201d, and the illustration of diverse people dialoging.  The research questions are listed: How do experiences with data and visualization vary in non-English speaking contexts? Can we do data visualization in languages other than English? How to overcome challenges that come with that? Long-term collective thinking: What can be done? Where can we start?",
                        "external_paper_link": "",
                        "youtube_ff_link": "",
                        "youtube_ff_id": "",
                        "bunny_ff_link": "https://ieeevis-uploads.b-cdn.net/vis23/w-vis4good/w-vis4good-6918/w-vis4good-6918_Preview.mp4?token=TyfVOBtmDUa2IkXjCKUqtd9YJbfzZ5ASdoiThAtvmI0&expires=1706590800",
                        "bunny_ff_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/w-vis4good/w-vis4good-6918/w-vis4good-6918_Preview.vtt?token=7o50BCOVow8-NYpwpPDVfbasWaIdgMEEQM-n7mL1wik&expires=1706590800",
                        "youtube_prerecorded_link": "https://youtu.be/ZM8NJIqsnnU",
                        "youtube_prerecorded_id": "ZM8NJIqsnnU",
                        "bunny_prerecorded_link": "https://ieeevis-uploads.b-cdn.net/vis23/w-vis4good/w-vis4good-6918/w-vis4good-6918_Presentation.mp4?token=r3jsCsD_nU8FNuKzsuoki5CfIHDQvdQOGDMJc7-pcw4&expires=1706590800",
                        "bunny_prerecorded_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/w-vis4good/w-vis4good-6918/w-vis4good-6918_Presentation.vtt?token=apVFFMnJFhzSkXqkSfZ6Hnss9-0Pt7C6yUTtflFAMwg&expires=1706590800"
                    },
                    {
                        "slot_id": "w-vis4good-7520",
                        "session_id": "w6",
                        "title": "Open Questions about the Visualization of Sociodemographic Data",
                        "contributors": [
                            "Florent Cabric"
                        ],
                        "authors": [
                            "Florent Cabric, Margret Vilborg Bjarnadottir, Anne-Flore Cabouat, Petra Isenberg"
                        ],
                        "abstract": "This paper collects a set of open research questions on how to visualize sociodemographic data. Sociodemographic data is a common part of datasets related to people, including institutional censuses, health data systems, and human-resources files. This data is sensitive, and its collection, sharing, and analysis require careful consideration. For instance, the European Union, through the General Data Protection Regulation (GDPR), protects the collection and processing of any personal data, including sexual orientation, ethnicity, and religion. Data visualization of sociodemographic data can reinforce stereotypes, marginalize groups, and lead to biased decision-making. It is, therefore, critical that these visualizations are created based on good, equitable design principles. In this paper, we discuss and provide a set of open research questions around the visualization of sociodemographic data. Our work contributes to an ongoing reflection on representing data about people and highlights some important future research directions for the VIS community. A version of this paper and its figures are available online at osf.io/a2u9c.",
                        "uid": "w-vis4good-7520",
                        "time_stamp": "2023-10-22T22:00:00Z",
                        "time_start": "2023-10-22T22:00:00Z",
                        "time_end": "2023-10-23T01:00:00Z",
                        "paper_type": "workshop",
                        "keywords": [],
                        "doi": "",
                        "fno": "",
                        "has_image": true,
                        "has_pdf": true,
                        "paper_award": "",
                        "image_caption": "Visualizing sociodemographic data without causing harm presents many challenges. Among them, designers must balance the efficiency, inclusiveness, and simplicity of their visualization.",
                        "external_paper_link": "",
                        "youtube_ff_link": "",
                        "youtube_ff_id": "",
                        "bunny_ff_link": "https://ieeevis-uploads.b-cdn.net/vis23/w-vis4good/w-vis4good-7520/w-vis4good-7520_Preview.mp4?token=x2Y_tTNzHuhtt3HHwzHwwsU2o3Mcl4yFBgWoGu_OAdw&expires=1706590800",
                        "bunny_ff_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/w-vis4good/w-vis4good-7520/w-vis4good-7520_Preview.vtt?token=Lgjzqipl8yM-XhqCZQ2U8XizcQk5aRkp7kkCmjZA6ns&expires=1706590800",
                        "youtube_prerecorded_link": "https://youtu.be/dIhK4Sdb1ug",
                        "youtube_prerecorded_id": "dIhK4Sdb1ug",
                        "bunny_prerecorded_link": "https://ieeevis-uploads.b-cdn.net/vis23/w-vis4good/w-vis4good-7520/w-vis4good-7520_Presentation.mp4?token=OuFsGX-fOksIgT3rt8xupBD_Cj36gvRbyiT0FIV7X64&expires=1706590800",
                        "bunny_prerecorded_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/w-vis4good/w-vis4good-7520/w-vis4good-7520_Presentation.vtt?token=FA6Ckks3iC2A7YsIstUQbkb2Ij5vi3T0Tk9fvOqxoe4&expires=1706590800"
                    },
                    {
                        "slot_id": "w-vis4good-9528",
                        "session_id": "w6",
                        "title": "Visual Salience to Mitigate Gender Bias in Recommendation Letters",
                        "contributors": [
                            "Yanan Da"
                        ],
                        "authors": [
                            "Yanan Da, Mengyu Chen, Ben Altschuler, Yutong Bu, Emily Wall"
                        ],
                        "abstract": "Letters of recommendation (LOR) are an important and widely used evaluation criterion for hiring, university admissions, and many other domains. Prior work has identified that gender stereotypes can bias how recommenders describe female applicants compared to male applicants in contexts such as faculty positions and undergraduate research internships. For example, female applicants are more likely to be described as communal (e.g., affectionate, warm) while male applicants are more likely to be described as agentic (e.g., confident, intellectual). In this paper, we investigate the extent to which these differences in language affect readers' impression of applicant competitiveness and the efficacy of a mitigation strategy: visual highlighting.  Our findings suggest that simple changes in visual salience through highlighting language more commonly used to describe women can negatively affect readers' evaluation of candidates, while highlighting more language more commonly used to describe men can reduce the effects of the bias.",
                        "uid": "w-vis4good-9528",
                        "time_stamp": "2023-10-22T22:00:00Z",
                        "time_start": "2023-10-22T22:00:00Z",
                        "time_end": "2023-10-23T01:00:00Z",
                        "paper_type": "workshop",
                        "keywords": [],
                        "doi": "",
                        "fno": "",
                        "has_image": false,
                        "has_pdf": true,
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "youtube_ff_link": "",
                        "youtube_ff_id": "",
                        "bunny_ff_link": "",
                        "bunny_ff_subtitles": "",
                        "youtube_prerecorded_link": "",
                        "youtube_prerecorded_id": "",
                        "bunny_prerecorded_link": "",
                        "bunny_prerecorded_subtitles": ""
                    }
                ]
            }
        ]
    },
    "w-altvis": {
        "event": "alt.VIS 2023",
        "long_name": "alt.VIS 2023",
        "event_type": "workshop",
        "event_prefix": "w-altvis",
        "event_description": "",
        "event_url": "",
        "organizers": [
            "Andrew M McNutt",
            "Lonni Besan\u00e7on",
            "Derya Akbaba",
            "Sara Di Bartolomeo",
            "Victor Schetinger"
        ],
        "sessions": [
            {
                "title": "alt.VIS 2023",
                "session_id": "w8",
                "event_prefix": "w-altvis",
                "track": "oneten",
                "session_image": "w8.png",
                "chair": [
                    "Andrew M McNutt",
                    "Lonni Besan\u00e7on",
                    "Derya Akbaba",
                    "Sara Di Bartolomeo",
                    "Victor Schetinger"
                ],
                "time_start": "2023-10-23T03:00:00Z",
                "time_end": "2023-10-23T06:00:00Z",
                "discord_category": "",
                "discord_channel": "110",
                "discord_channel_id": "1161741529434566686",
                "discord_link": "https://discordapp.com/channels/1161400451619627068/1161741529434566686",
                "zoom_private_meeting": "",
                "zoom_private_password": "",
                "zoom_private_link": "",
                "zoom_broadcast_link": "",
                "ff_link": "",
                "time_slots": [
                    {
                        "slot_id": "w-altvis-4225",
                        "session_id": "w8",
                        "title": "On nonstandard visualization",
                        "contributors": [
                            "Dn. Alex Ravsky"
                        ],
                        "authors": [],
                        "abstract": "We discuss the nonstandard (non-Euclidean, four-dimensional, of variable dimension, and with two-edged space placement) visualizations, their neurophysiological and philosophical possibilities, and ways to realize them.",
                        "uid": "w-altvis-4225",
                        "time_stamp": "2023-10-23T03:00:00Z",
                        "time_start": "2023-10-23T03:00:00Z",
                        "time_end": "2023-10-23T06:00:00Z",
                        "paper_type": "workshop",
                        "keywords": [],
                        "doi": "",
                        "fno": "",
                        "has_image": true,
                        "has_pdf": true,
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "youtube_ff_link": "",
                        "youtube_ff_id": "",
                        "bunny_ff_link": "",
                        "bunny_ff_subtitles": "",
                        "youtube_prerecorded_link": "",
                        "youtube_prerecorded_id": "",
                        "bunny_prerecorded_link": "",
                        "bunny_prerecorded_subtitles": ""
                    },
                    {
                        "slot_id": "w-altvis-4650",
                        "session_id": "w8",
                        "title": "DevOps for DataVis: A Survey and Provocation for Teaching Deployment of Data Visualizations",
                        "contributors": [
                            "Jane L. Adams"
                        ],
                        "authors": [],
                        "abstract": "We present a provocation towards teaching development operations (\u201cDevOps\") and other infrastructure concepts in the course of collegiate data visualization instruction. We survey 65 syllabi from semester-long, college-level data visualization courses, with an eye toward languages and platforms used, as well as mentions of deployment related terms. Results convey significant variability in language and tooling used in curricula. We identify a distinct lack of discussions around \u2018DevOps for DataVis\u2019 scaffolding concepts such as version control, package management, server infrastructure, high-performance computing, and machine learning data pipelines. We acknowledge the challenges of adding supplemental information to already dense curricula, and the expectation that prior or concurrent classes should provide this computer science background. We propose a group community effort to create one free \u2018course\u2019 or \u2018wiki\u2019 as a living reference on the ways these broader DevOps concepts relate directly to data visualization specifically. A free copy of this paper and all supplemental materials are available at https://osf.io/bxaqz/",
                        "uid": "w-altvis-4650",
                        "time_stamp": "2023-10-23T03:00:00Z",
                        "time_start": "2023-10-23T03:00:00Z",
                        "time_end": "2023-10-23T06:00:00Z",
                        "paper_type": "workshop",
                        "keywords": [],
                        "doi": "",
                        "fno": "",
                        "has_image": false,
                        "has_pdf": true,
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "youtube_ff_link": "",
                        "youtube_ff_id": "",
                        "bunny_ff_link": "",
                        "bunny_ff_subtitles": "",
                        "youtube_prerecorded_link": "",
                        "youtube_prerecorded_id": "",
                        "bunny_prerecorded_link": "",
                        "bunny_prerecorded_subtitles": ""
                    },
                    {
                        "slot_id": "w-altvis-5076",
                        "session_id": "w8",
                        "title": "VisFutures",
                        "contributors": [
                            "Charles Perin"
                        ],
                        "authors": [],
                        "abstract": "Welcome to Vis Futures! ... where YOU have a say in how people use data in the Future! Vis Futures is a card-based sketching game where players think critically (and playfully) about the future of data and visualization. Players deal a set of cards that hint at a possible future, and a possible dataset in that future. Players then use those prompts to imagine and sketch new visualization designs and imagine ways that future people from a particular audience might encounter, interact with, or utilize data (quirks and all). At the end of each round, players share their visualizations, discuss, and vote on which scenarios and visualizations are the most creative! Our goal is to include more people (including visualization students, researchers, and practitioners, as well as clients and collaborators) in discussions of critical data issues that have implications for the future of data, visualization, and technology. This game encourages players to engage in future-forward design thinking, examining the increasingly complex implications of our relationships with data and technology, and considering how, where, and why visual representations of data might play a role. It can be pretty fun too! This submission consists of the instruction booklet that accompanies the game. The provided link contains as supplemental material the cards for the game as well as the optional creator pack. These will later be uploaded and available at the game's website provided in the instructions.",
                        "uid": "w-altvis-5076",
                        "time_stamp": "2023-10-23T03:00:00Z",
                        "time_start": "2023-10-23T03:00:00Z",
                        "time_end": "2023-10-23T06:00:00Z",
                        "paper_type": "workshop",
                        "keywords": [],
                        "doi": "",
                        "fno": "",
                        "has_image": true,
                        "has_pdf": true,
                        "paper_award": "",
                        "image_caption": "Vis Futures is a card-based sketching game where players think critically (and playfully) about the future of data and visualization.",
                        "external_paper_link": "",
                        "youtube_ff_link": "",
                        "youtube_ff_id": "",
                        "bunny_ff_link": "https://ieeevis-uploads.b-cdn.net/vis23/w-altvis/w-altvis-5076/w-altvis-5076_Preview.mp4?token=2sziiUSyoiwvx58vc8--de9hlKNR8Y5sHKz3SdlW2dU&expires=1706590800",
                        "bunny_ff_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/w-altvis/w-altvis-5076/w-altvis-5076_Preview.vtt?token=EmLA-ODa5cr0OsZOlkkQPfTzayuUXCeB-gLvYwvMVpY&expires=1706590800",
                        "youtube_prerecorded_link": "",
                        "youtube_prerecorded_id": "",
                        "bunny_prerecorded_link": "",
                        "bunny_prerecorded_subtitles": ""
                    },
                    {
                        "slot_id": "w-altvis-2463",
                        "session_id": "w8",
                        "title": "Data Embroidery with Black-and-White Textures",
                        "contributors": [
                            "Tingying He"
                        ],
                        "authors": [
                            "Tingying He",
                            "Petra Isenberg",
                            "Tobias Isenberg"
                        ],
                        "abstract": "We investigated data embroidery with black-and-white textures, identifying challenges in the use of textures for machine embroidery based on our own experience. Data embroidery, as a method of physically representing data, offers a unique way to integrate personal data into one's everyday fabric-based objects. Owing to their monochromatic characteristics, black-and-white textures promise to be easy to employ in machine embroidery. We experimented with different textured visualizations designed by experts and, in this paper, we detail our workflow and evaluate the performance and suitability of different textures. We then conducted a survey on vegetable preferences within a family and created a canvas bag as a case study, featuring the embroidered family data to show how embroidered data can be used in practice.",
                        "uid": "w-altvis-2463",
                        "time_stamp": "2023-10-23T03:00:00Z",
                        "time_start": "2023-10-23T03:00:00Z",
                        "time_end": "2023-10-23T06:00:00Z",
                        "paper_type": "workshop",
                        "keywords": [],
                        "doi": "",
                        "fno": "",
                        "has_image": true,
                        "has_pdf": true,
                        "paper_award": "",
                        "image_caption": "Two embroidered charts depict the performance of different textures in machine embroidery: the left chart for geometric textures, and the right chart for iconic textures.",
                        "external_paper_link": "",
                        "youtube_ff_link": "",
                        "youtube_ff_id": "",
                        "bunny_ff_link": "https://ieeevis-uploads.b-cdn.net/vis23/w-altvis/w-altvis-2463/w-altvis-2463_Preview.mp4?token=WQws-SBUZURtG5MkLLNjLLTaayuzG9pNARGEXRCu-Zs&expires=1706590800",
                        "bunny_ff_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/w-altvis/w-altvis-2463/w-altvis-2463_Preview.vtt?token=dhR_gGLPIZDA7LdVmGDHP_sQd8-tCG6Dz7R4XTIdOvM&expires=1706590800",
                        "youtube_prerecorded_link": "",
                        "youtube_prerecorded_id": "",
                        "bunny_prerecorded_link": "https://ieeevis-uploads.b-cdn.net/vis23/w-altvis/w-altvis-2463/w-altvis-2463_Presentation.mp4?token=GizeB1tolS8RGlqwODkmLGGbcVkh8a6ZEIJYo-ERC7g&expires=1706590800",
                        "bunny_prerecorded_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/w-altvis/w-altvis-2463/w-altvis-2463_Presentation.vtt?token=jJ3Jc49Xq5AkUzS2DegT_Y22ar4OJ3TRap2LihFcohw&expires=1706590800"
                    },
                    {
                        "slot_id": "w-altvis-4956",
                        "session_id": "w8",
                        "title": "n Walks in the Fictional Woods",
                        "contributors": [
                            "Victor Schetinger"
                        ],
                        "authors": [],
                        "abstract": "This paper presents a novel exploration of the interaction between generative AI models, visualization, and narrative generation processes, using OpenAI's GPT as a case study. We look at the question ``Where Does Generativeness Comes From'', which has a simple answer at the intersection of many domains. Drawing on Umberto Eco's ``Six Walks in the Fictional Woods'', we engender a speculative, transdisciplinary scientific narrative using ChatGPT in different roles: as an information repository, a ghost writer, a scientific coach, among others. The paper is written as a piling of plateaus where the titling of each (sub-)section, the ``teaser'' images, the headers, and a biblock of text are strata forming a narrative about narratives. To enrich our exposition, we present a visualization prototype to analyze storyboarded narratives, and extensive conversations with ChatGPT. Each link to a ChatGPT conversation is an experiment on writing where we try to use different plugins and techniques to investigate the topics that, ultimately form the content of this portable document file. Our visualization uses a dataset of stories with scene descriptions, textual descriptions of scenes (both generated by ChatGPT), and images (generated by Stable Diffusion using scene descriptions as prompts). We employ a simple graph-node diagram to try to make a ``forest of narratives'' visible, an example of a vis4gen application that can be used to analyze the output of Large Languange + Image Models.",
                        "uid": "w-altvis-4956",
                        "time_stamp": "2023-10-23T03:00:00Z",
                        "time_start": "2023-10-23T03:00:00Z",
                        "time_end": "2023-10-23T06:00:00Z",
                        "paper_type": "workshop",
                        "keywords": [],
                        "doi": "",
                        "fno": "",
                        "has_image": false,
                        "has_pdf": true,
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "youtube_ff_link": "",
                        "youtube_ff_id": "",
                        "bunny_ff_link": "",
                        "bunny_ff_subtitles": "",
                        "youtube_prerecorded_link": "",
                        "youtube_prerecorded_id": "",
                        "bunny_prerecorded_link": "",
                        "bunny_prerecorded_subtitles": ""
                    },
                    {
                        "slot_id": "w-altvis-9647",
                        "session_id": "w8",
                        "title": "LSDvis: Hallucinatory data visualisations in real world environments",
                        "contributors": [
                            "Benjamin Lee"
                        ],
                        "authors": [],
                        "abstract": "We propose the concept of \"LSDvis\": the (highly exaggerated) visual blending of situated visualisations and the real-world environment to produce data representations that resemble hallucinations. Such hallucinatory visualisations incorporate elements of the physical environment, twisting and morphing their appearance such that they become part of the visualisation itself. We demonstrate LSDvis in a \"proof of proof of concept\", where we use Stable Diffusion to modify images of real environments with abstract data visualisations as input. We conclude by discussing considerations of LSDvis. We hope that our work promotes visualisation designs which deprioritise saliency in favour of quirkiness and ambience.",
                        "uid": "w-altvis-9647",
                        "time_stamp": "2023-10-23T03:00:00Z",
                        "time_start": "2023-10-23T03:00:00Z",
                        "time_end": "2023-10-23T06:00:00Z",
                        "paper_type": "workshop",
                        "keywords": [],
                        "doi": "",
                        "fno": "",
                        "has_image": true,
                        "has_pdf": true,
                        "paper_award": "",
                        "image_caption": "Examples of LSDvis based on Australian landmarks. Left: A bar chart of popular visiting times blended onto the facade of the Federation Square building. Middle: An area chart of visiting vehicle counts added as a rock in The Twelve Apostles. Right: A pie chart of revenue percentages blended into the shells of the Sydney Opera House.",
                        "external_paper_link": "",
                        "youtube_ff_link": "",
                        "youtube_ff_id": "",
                        "bunny_ff_link": "",
                        "bunny_ff_subtitles": "",
                        "youtube_prerecorded_link": "",
                        "youtube_prerecorded_id": "",
                        "bunny_prerecorded_link": "",
                        "bunny_prerecorded_subtitles": ""
                    },
                    {
                        "slot_id": "w-altvis-1171",
                        "session_id": "w8",
                        "title": "Only YOU Can Make IEEE VIS Environmentally Sustainable",
                        "contributors": [
                            "Elsie Lee-Robbins"
                        ],
                        "authors": [
                            "Elsie Lee-Robbins",
                            "Andrew M McNutt"
                        ],
                        "abstract": "The IEEE VIS Conference (or VIS) hosts more than 1000 people annually. It brings together visualization researchers and practitioners from across the world to share new research and knowledge. Behind the scenes, a team of volunteers puts together the entire conference and makes sure it runs smoothly. Organizing involves logistics of the conference, ensuring that the attendees have an enjoyable time, allocating rooms to multiple concurrent tracks, and keeping the conference within budget. In recent years, the COVID-19 pandemic has abruptly disrupted plans, forcing organizers to switch to virtual, hybrid, and satellite formats. These alternatives offer many benefits: fewer costs (e.g., travel, venue, institutional), greater accessibility (who can physically travel, who can get visas, who can get child care), and a lower carbon footprint (as people do not need to fly to attend). As many conferences begin to revert to the pre-pandemic status quo of primarily in-person conferences, we suggest that it is an opportune moment to reflect on the benefits and drawbacks of lower-carbon conference formats. To learn more about the logistics of conference organizing, we talked to 6 senior executive-level VIS organizers. We review some of the many considerations that go into planning, particularly with regard to how they influence decisions about alternative formats. We aim to start a discussion about the sustainability of VIS -- including sustainability for finance, volunteers, and, central to this work, the environment -- for the next three years and the next three hundred years.",
                        "uid": "w-altvis-1171",
                        "time_stamp": "2023-10-23T03:00:00Z",
                        "time_start": "2023-10-23T03:00:00Z",
                        "time_end": "2023-10-23T06:00:00Z",
                        "paper_type": "workshop",
                        "keywords": [],
                        "doi": "",
                        "fno": "",
                        "has_image": true,
                        "has_pdf": true,
                        "paper_award": "",
                        "image_caption": "The IEEE VIS conference hosts more than a thousand participants, altogether flying thousands of miles, emitting thousands of kilograms of CO2 to attend. This figure represents that our tree(map)s are burning in wildfires, to show the direct connection that our IEEE VIS conference has on climate change.",
                        "external_paper_link": "",
                        "youtube_ff_link": "",
                        "youtube_ff_id": "",
                        "bunny_ff_link": "",
                        "bunny_ff_subtitles": "",
                        "youtube_prerecorded_link": "https://youtu.be/BF7cP3SIdeg",
                        "youtube_prerecorded_id": "BF7cP3SIdeg",
                        "bunny_prerecorded_link": "https://ieeevis-uploads.b-cdn.net/vis23/w-altvis/w-altvis-1171/w-altvis-1171_Presentation.mp4?token=5H_KT0k9HEpQAk-6em18f6G9v0v1nMM2d9-zTYLF7Yo&expires=1706590800",
                        "bunny_prerecorded_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/w-altvis/w-altvis-1171/w-altvis-1171_Presentation.vtt?token=A8OZy5nDI2BhN4vO7YbfqSEBVHySQtAmJtUgEZG8n0U&expires=1706590800"
                    },
                    {
                        "slot_id": "w-altvis-5620",
                        "session_id": "w8",
                        "title": "Visualizing the Weird and the Eerie",
                        "contributors": [
                            "Matthew Brehmer"
                        ],
                        "authors": [],
                        "abstract": "In this brief essay, I reflect on how Mark Fisher's definitions of the weird and the eerie could be applied in communicative data visualization. I ask how visualization designers might elicit these two impressions when a viewer is engaging with multimodal representations of data. I argue that there are situations in which viewers should feel uncertain or suspicious of unseen forces that account for the presence or absence of audiovisual patterns. Finally, I conclude that the ability to appreciate the weird and the eerie in data is particularly important at this moment in history, one marked by significant ecological and economic disruption.",
                        "uid": "w-altvis-5620",
                        "time_stamp": "2023-10-23T03:00:00Z",
                        "time_start": "2023-10-23T03:00:00Z",
                        "time_end": "2023-10-23T06:00:00Z",
                        "paper_type": "workshop",
                        "keywords": [],
                        "doi": "",
                        "fno": "",
                        "has_image": true,
                        "has_pdf": true,
                        "paper_award": "",
                        "image_caption": "A montage of imagery emblematic of the weird and the eerie; top row: a strange creature of the sea, something where there should be nothing, a tunnel; middle row: the grotesque, a flock of birds, the mysterious Nazca Lines; bottom row: crop circles, a beach, wildfire haze and the Seattle skyline; all images from Unsplash or Wikimedia - credits in paper.",
                        "external_paper_link": "",
                        "youtube_ff_link": "",
                        "youtube_ff_id": "",
                        "bunny_ff_link": "",
                        "bunny_ff_subtitles": "",
                        "youtube_prerecorded_link": "",
                        "youtube_prerecorded_id": "",
                        "bunny_prerecorded_link": "",
                        "bunny_prerecorded_subtitles": ""
                    },
                    {
                        "slot_id": "w-altvis-8303",
                        "session_id": "w8",
                        "title": "Humanity Influenced Visualization Design for Aerial Sensor-based Visualization of Environmental Factors",
                        "contributors": [
                            "Elina Esenbaeva, Dana Nursultanova, Sabina Ualibekova"
                        ],
                        "authors": [],
                        "abstract": "The motivating perspective of this work is that visualization is a human endeavor as natural as human life is itself. This has profound influence on the way visualization is approached as the focus shifts away from any data-centric, visualization technique- or system-based foundations to one of human-centric visual perception, information perception, information acquisition and learning.  This paper reports on part of a design of a visualization approach and system for deployments in wide-scope application areas of interest and which is guided by the Engineering Insightful Serviceable Visualization (EISV) model and is thus in the context of this human-centric perspective. The application areas are primarily loosely constrained environments, that are, environments for which available techniques such as computational modeling or fixed, location-based sensors are ill-suited. These environments have terrain, build or other similar features. An aerial drone-based sensor platform is proposed to sample environmental data in these environments. One of the included sensors on this platform is a LiDAR, a distance ranging sensor. The visual output of the LiDAR is primarily studied in this paper using the notions of iconicity and indexicality in the Peircean sense and guided by the EISV model. Several work-in-progress experiments that illustrate how the proposed system may respond are described.",
                        "uid": "w-altvis-8303",
                        "time_stamp": "2023-10-23T03:00:00Z",
                        "time_start": "2023-10-23T03:00:00Z",
                        "time_end": "2023-10-23T06:00:00Z",
                        "paper_type": "workshop",
                        "keywords": [],
                        "doi": "",
                        "fno": "",
                        "has_image": true,
                        "has_pdf": true,
                        "paper_award": "",
                        "image_caption": "The motivating perspective of this work is that visualization is human-centric.   This paper reports on the visualization approach, system and deployment, guided   by the Engineering Insightful Serviceable Visualization (EISV) model, and is thus   in the context of this human-centric perspective. An aerial drone-based sensor   platform is proposed to sample environmental data. One of the included sensors   on this platform is a LiDAR. The visual output of the LiDAR is primarily studied   in this paper using the notions of iconicity and indexicality in the Peircean   sense. Several work-in-progress experiments that illustrate how the proposed   system may respond are described.",
                        "external_paper_link": "",
                        "youtube_ff_link": "",
                        "youtube_ff_id": "",
                        "bunny_ff_link": "",
                        "bunny_ff_subtitles": "",
                        "youtube_prerecorded_link": "",
                        "youtube_prerecorded_id": "",
                        "bunny_prerecorded_link": "",
                        "bunny_prerecorded_subtitles": ""
                    }
                ]
            }
        ]
    },
    "a-vast-challenge": {
        "event": "VAST Challenge",
        "long_name": "VAST Challenge",
        "event_type": "associated",
        "event_prefix": "a-vast-challenge",
        "event_description": "",
        "event_url": "",
        "organizers": [
            "R. Jordan Crouser",
            "Jereme Haack"
        ],
        "sessions": [
            {
                "title": "VAST Challenge",
                "session_id": "c2",
                "event_prefix": "a-vast-challenge",
                "track": "oneohfour",
                "session_image": "c2.png",
                "chair": [
                    "R. Jordan Crouser",
                    "Jereme Haack"
                ],
                "time_start": "2023-10-22T22:00:00Z",
                "time_end": "2023-10-23T01:00:00Z",
                "discord_category": "",
                "discord_channel": "104",
                "discord_channel_id": "1161741240665124954",
                "discord_link": "https://discordapp.com/channels/1161400451619627068/1161741240665124954",
                "zoom_private_meeting": "",
                "zoom_private_password": "",
                "zoom_private_link": "",
                "zoom_broadcast_link": "",
                "ff_link": "",
                "time_slots": [
                    {
                        "slot_id": "a-vast-challenge-1003",
                        "session_id": "c2",
                        "title": "UKON-Frings-MC3",
                        "contributors": [
                            "Udo Schlegel"
                        ],
                        "authors": [
                            "Alexander Frings",
                            "Mathis Beck",
                            "Manuel Schmidt",
                            "Udo Schlegel",
                            "Daniel Keim"
                        ],
                        "abstract": "",
                        "uid": "a-vast-challenge-1003",
                        "time_stamp": "2023-10-22T22:00:00Z",
                        "time_start": "2023-10-22T22:00:00Z",
                        "time_end": "2023-10-23T01:00:00Z",
                        "paper_type": "associated",
                        "keywords": [],
                        "doi": "",
                        "fno": "",
                        "has_image": false,
                        "has_pdf": true,
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "youtube_ff_link": "",
                        "youtube_ff_id": "",
                        "bunny_ff_link": "",
                        "bunny_ff_subtitles": "",
                        "youtube_prerecorded_link": "",
                        "youtube_prerecorded_id": "",
                        "bunny_prerecorded_link": "",
                        "bunny_prerecorded_subtitles": ""
                    },
                    {
                        "slot_id": "a-vast-challenge-1004",
                        "session_id": "c2",
                        "title": "UKON-Grotzeck-MC3",
                        "contributors": [
                            "Udo Schlegel"
                        ],
                        "authors": [
                            "Henri Grotzeck",
                            "Maria-Viktoria Heinle",
                            "Annabella Mantskava",
                            "Elvin Medzhidov",
                            "Julian Storto",
                            "Udo Schlegel",
                            "Daniel Keim"
                        ],
                        "abstract": "",
                        "uid": "a-vast-challenge-1004",
                        "time_stamp": "2023-10-22T22:00:00Z",
                        "time_start": "2023-10-22T22:00:00Z",
                        "time_end": "2023-10-23T01:00:00Z",
                        "paper_type": "associated",
                        "keywords": [],
                        "doi": "",
                        "fno": "",
                        "has_image": false,
                        "has_pdf": true,
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "youtube_ff_link": "",
                        "youtube_ff_id": "",
                        "bunny_ff_link": "",
                        "bunny_ff_subtitles": "",
                        "youtube_prerecorded_link": "",
                        "youtube_prerecorded_id": "",
                        "bunny_prerecorded_link": "",
                        "bunny_prerecorded_subtitles": ""
                    },
                    {
                        "slot_id": "a-vast-challenge-1006",
                        "session_id": "c2",
                        "title": "FishLense",
                        "contributors": [
                            "Mr Robert H\u00f6nig"
                        ],
                        "authors": [
                            "Robert H\u00f6nig",
                            "Jiale Chen",
                            "Mennatallah El-Assady"
                        ],
                        "abstract": "We present FishLense, an interactive tool to visually explore large  knowledge graphs and identify interesting nodes and patterns. FishLense offers smooth transitions from small- to large-scale graph exploration and a ``click-it-all'' interface that lowers the learning curve. FishLense demonstrates these design patterns at the example of a large-scale fishing knowledge graph.",
                        "uid": "a-vast-challenge-1006",
                        "time_stamp": "2023-10-22T22:00:00Z",
                        "time_start": "2023-10-22T22:00:00Z",
                        "time_end": "2023-10-23T01:00:00Z",
                        "paper_type": "associated",
                        "keywords": [],
                        "doi": "",
                        "fno": "",
                        "has_image": true,
                        "has_pdf": true,
                        "paper_award": "",
                        "image_caption": "FishLense interactively visualizes large knowledge graphs. Its main innovation is a seamless integration of large-scale and small-scale graph exploration.",
                        "external_paper_link": "",
                        "youtube_ff_link": "",
                        "youtube_ff_id": "",
                        "bunny_ff_link": "",
                        "bunny_ff_subtitles": "",
                        "youtube_prerecorded_link": "",
                        "youtube_prerecorded_id": "",
                        "bunny_prerecorded_link": "",
                        "bunny_prerecorded_subtitles": ""
                    },
                    {
                        "slot_id": "a-vast-challenge-1015",
                        "session_id": "c2",
                        "title": "FishHook: A Visual Analytics System for Tracing Suspicious Entities in the Fisheries Domain using Knowledge Graphs",
                        "contributors": [
                            "Jingfu Wu"
                        ],
                        "authors": [
                            "Jingfu Wu",
                            "Diyun Lu",
                            "Lei Chen",
                            "Ningyi Peng",
                            "Fan Yang",
                            "Xinyu Tang",
                            "Yuxin Ma"
                        ],
                        "abstract": "Undertaking the visual exploration of a large knowledge graph in the domain of illegal fishery activities, FishHook offers an interactive visual analytics solution for scrutinizing individual entities via four distinct views: the Ego Net view, the Hierarchical Tree view, the Unity Net view, and the Parallel Coordinates view. The system incorporates an anomalous pattern detection mechanism, which is designed to address the challenges detailed in the 2023 IEEE VAST Challenge MC1. This integrated solution not only demonstrates robustness and scalability but also provides a powerful tool for comprehending large-scale knowledge graph datasets.",
                        "uid": "a-vast-challenge-1015",
                        "time_stamp": "2023-10-22T22:00:00Z",
                        "time_start": "2023-10-22T22:00:00Z",
                        "time_end": "2023-10-23T01:00:00Z",
                        "paper_type": "associated",
                        "keywords": [],
                        "doi": "",
                        "fno": "",
                        "has_image": true,
                        "has_pdf": true,
                        "paper_award": "",
                        "image_caption": "FishHook: A Visual Analytics System for Tracing Suspicious Entities in the Fisheries Domain Using Knowledge Graphs",
                        "external_paper_link": "",
                        "youtube_ff_link": "",
                        "youtube_ff_id": "",
                        "bunny_ff_link": "https://ieeevis-uploads.b-cdn.net/vis23/a-vast-challenge/a-vast-challenge-1015/a-vast-challenge-1015_Preview.mp4?token=rS7WttjWrgddFF1PVOuECBoRZ9dtA5EtquRZRTr_oCI&expires=1706590800",
                        "bunny_ff_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/a-vast-challenge/a-vast-challenge-1015/a-vast-challenge-1015_Preview.vtt?token=iBS8HqsA2wOKDSKKvibDjVnZ7hFuIEujOxxrt0TiPzQ&expires=1706590800",
                        "youtube_prerecorded_link": "",
                        "youtube_prerecorded_id": "",
                        "bunny_prerecorded_link": "",
                        "bunny_prerecorded_subtitles": ""
                    },
                    {
                        "slot_id": "a-vast-challenge-1017",
                        "session_id": "c2",
                        "title": "Purdue-Chen-MC3",
                        "contributors": [
                            "Hao Wang"
                        ],
                        "authors": [
                            "Ava Zhao",
                            "Zhanqi Su",
                            "William C Fei",
                            "Na Zhuo",
                            "Hao Wang",
                            "Tianzhou Yu",
                            "Zuotian Li",
                            "Zhenyu Cheryl Qian",
                            "Yingjie Victor Chen"
                        ],
                        "abstract": "To solve the VAST Challenge 2023 MC3, our team employed a large language model, ChatGPT, to explore the potential of AI-guided visual analytics for the detection of anomalies within a knowledge graph in the context of illegal fishing and marine trade. We employed a systematic and iterative approach, guided by GPT augmentation, that enabled problem understanding, data processing, solution explo- ration, code writing, and results analysis. By generating and analyz- ing various graphs, we identified anomalies related to revenue and product services. Further analyses unveiled potential illegal fishing activities and identified instances warranting additional investigation. Overall, our work highlights both the strengths and limitations of ChatGPT in aiding the visual analytics process and emphasizes the importance of human judgment in refining AI-generated outputs.",
                        "uid": "a-vast-challenge-1017",
                        "time_stamp": "2023-10-22T22:00:00Z",
                        "time_start": "2023-10-22T22:00:00Z",
                        "time_end": "2023-10-23T01:00:00Z",
                        "paper_type": "associated",
                        "keywords": [],
                        "doi": "",
                        "fno": "",
                        "has_image": true,
                        "has_pdf": true,
                        "paper_award": "",
                        "image_caption": "Self-structured mind map with data visualizations that generated via ChatGPT-assisted learning.",
                        "external_paper_link": "",
                        "youtube_ff_link": "",
                        "youtube_ff_id": "",
                        "bunny_ff_link": "https://ieeevis-uploads.b-cdn.net/vis23/a-vast-challenge/a-vast-challenge-1017/a-vast-challenge-1017_Preview.mp4?token=aBvF_-FjnxJTlaVjByY_LCtzfKbDbQAYSf5p7lnF95k&expires=1706590800",
                        "bunny_ff_subtitles": "",
                        "youtube_prerecorded_link": "",
                        "youtube_prerecorded_id": "",
                        "bunny_prerecorded_link": "https://ieeevis-uploads.b-cdn.net/vis23/a-vast-challenge/a-vast-challenge-1017/a-vast-challenge-1017_Presentation.mp4?token=V9JEGRJzEiEG-frK4-uU-zH_BPk9_Hj21uQNJwblT2Q&expires=1706590800",
                        "bunny_prerecorded_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/a-vast-challenge/a-vast-challenge-1017/a-vast-challenge-1017_Presentation.vtt?token=BN8y6zxVkOiJiurBIfVdg7iDUXFBuNvmGUTPYQl0FcI&expires=1706590800"
                    },
                    {
                        "slot_id": "a-vast-challenge-1023",
                        "session_id": "c2",
                        "title": "TTU-Phornsawan-C2",
                        "contributors": [
                            "Phornsawan Roemsri"
                        ],
                        "authors": [
                            "PHORNSAWAN ROEMSRI",
                            "Tommy Dang"
                        ],
                        "abstract": "Given an incomplete dataset, FishEye employs various tools, including artificial intelligence, to propose links that enhance the dataset. This paper addresses the challenge of identifying the most reliable tools for completing a knowledge graph. Additionally, the paper tackles the issue of identifying companies potentially involved in illegal, unreported, and unregulated (IUU) fishing. To aid in this endeavor, we introduce a web application designed to detect temporal patterns associated with companies that halt operations and reemerge with new identities. By aggregating data monthly and strategically displaying AI-generated bundle nodes using dropdown controls, our analysis uncovers suspicious patterns, particularly within the Chub Mackerel context. Notably, the Shark and Lichen bundles emerge as dependable graph-completion tools. The overlap between generated data and the knowledge graph in these two bundles underscores the reliability of predictions.",
                        "uid": "a-vast-challenge-1023",
                        "time_stamp": "2023-10-22T22:00:00Z",
                        "time_start": "2023-10-22T22:00:00Z",
                        "time_end": "2023-10-23T01:00:00Z",
                        "paper_type": "associated",
                        "keywords": [],
                        "doi": "",
                        "fno": "",
                        "has_image": false,
                        "has_pdf": true,
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "youtube_ff_link": "",
                        "youtube_ff_id": "",
                        "bunny_ff_link": "",
                        "bunny_ff_subtitles": "",
                        "youtube_prerecorded_link": "",
                        "youtube_prerecorded_id": "",
                        "bunny_prerecorded_link": "",
                        "bunny_prerecorded_subtitles": ""
                    }
                ]
            }
        ]
    },
    "t-colorvis": {
        "event": "Colorizing your Data Visualizations",
        "long_name": "Colorizing your Data Visualizations",
        "event_type": "tutorial",
        "event_prefix": "t-colorvis",
        "event_description": "",
        "event_url": "",
        "organizers": [
            "Theresa-Marie Rhyne"
        ],
        "sessions": [
            {
                "title": "Demystifying Color in Your Data Visualizations",
                "session_id": "t1",
                "event_prefix": "t-colorvis",
                "track": "oneeleven",
                "session_image": "t1.png",
                "chair": [
                    "Theresa-Marie Rhyne"
                ],
                "time_start": "2023-10-22T03:00:00Z",
                "time_end": "2023-10-22T06:00:00Z",
                "discord_category": "",
                "discord_channel": "111-112",
                "discord_channel_id": "1161741606651711498",
                "discord_link": "https://discordapp.com/channels/1161400451619627068/1161741606651711498",
                "zoom_private_meeting": "",
                "zoom_private_password": "",
                "zoom_private_link": "",
                "zoom_broadcast_link": "",
                "ff_link": "",
                "time_slots": []
            }
        ]
    },
    "w-vahc": {
        "event": "VAHC: 14th Workshop on Visual Analytics in Healthcare",
        "long_name": "VAHC: 14th Workshop on Visual Analytics in Healthcare",
        "event_type": "workshop",
        "event_prefix": "w-vahc",
        "event_description": "",
        "event_url": "",
        "organizers": [
            "J\u00fcrgen Bernard",
            "Annie T. Chen",
            "Danny T.Y. Wu"
        ],
        "sessions": [
            {
                "title": "VAHC: 14th Workshop on Visual Analytics in Healthcare",
                "session_id": "w4",
                "event_prefix": "w-vahc",
                "track": "oneohfour",
                "session_image": "w4.png",
                "chair": [
                    "J\u00fcrgen Bernard",
                    "Annie T. Chen",
                    "Danny T.Y. Wu"
                ],
                "time_start": "2023-10-22T03:00:00Z",
                "time_end": "2023-10-22T06:00:00Z",
                "discord_category": "",
                "discord_channel": "104",
                "discord_channel_id": "1161741240665124954",
                "discord_link": "https://discordapp.com/channels/1161400451619627068/1161741240665124954",
                "zoom_private_meeting": "",
                "zoom_private_password": "",
                "zoom_private_link": "",
                "zoom_broadcast_link": "",
                "ff_link": "",
                "time_slots": [
                    {
                        "slot_id": "w-vahc-9996",
                        "session_id": "w4",
                        "title": "Demo: Cohort Visualization and Analysis of Patients with Inflammatory Bowel Disease",
                        "contributors": [
                            "J\u00f6rn Kohlhammer"
                        ],
                        "authors": [
                            "David Sessler, Salmah Ahmad, J\u00f6rn Kohlhammer"
                        ],
                        "abstract": "This demo paper introduces the final version of a cohort analysis module for the support of treating patients with inflammatory bowel disease (IBD). It is not trivial to correctly diagnose the specific IBD in patients, and wrongly treated patients have to endure the disease effects for a long time, with large costs for the individuals and the healthcare systems. The goal of this work is complementing the examination of individual patients with interactive analyses of cohorts and populations with similar disease patterns to support learning from such similarities for future treatments. We report on additional data and functionality compared to 2021 and discuss an evaluation with eight IBD experts.",
                        "uid": "w-vahc-9996",
                        "time_stamp": "2023-10-22T03:00:00Z",
                        "time_start": "2023-10-22T03:00:00Z",
                        "time_end": "2023-10-22T06:00:00Z",
                        "paper_type": "workshop",
                        "keywords": [
                            "Human-centered computing\u2014Visualization\u2014Visual Analytics; Applied computing\u2014Life and medical sciences\u2014Health care information systems"
                        ],
                        "doi": "",
                        "fno": "",
                        "has_image": true,
                        "has_pdf": true,
                        "paper_award": "",
                        "image_caption": "This demo paper introduces the final version of a cohort analysis module for the support of treating patients with inflammatory bowel disease (IBD). It is not trivial to correctly diagnose the specific IBD in patients, and wrongly treated patients have to endure the disease effects for a long time, with large costs for the individuals and the healthcare systems. The goal of this work is complementing the examination of individual patients with interactive analyses of cohorts and populations with similar disease patterns to support learning from such similarities for future treatments.",
                        "external_paper_link": "",
                        "youtube_ff_link": "",
                        "youtube_ff_id": "",
                        "bunny_ff_link": "",
                        "bunny_ff_subtitles": "",
                        "youtube_prerecorded_link": "",
                        "youtube_prerecorded_id": "",
                        "bunny_prerecorded_link": "https://ieeevis-uploads.b-cdn.net/vis23/w-vahc/w-vahc-9996/w-vahc-9996_Presentation.mp4?token=3yj4Hf0N_XA1BgLnkqTKJxSrsv0mtC4GxP4-wN1HWu8&expires=1706590800",
                        "bunny_prerecorded_subtitles": ""
                    },
                    {
                        "slot_id": "w-vahc-8545",
                        "session_id": "w4",
                        "title": "Multi-Task Transformer Visualization to build Trust for Clinical Outcome Prediction",
                        "contributors": [
                            "Dario Antweiler"
                        ],
                        "authors": [
                            "Dario Antweiler, Florian Gallusser, Georg Fuchs"
                        ],
                        "abstract": "Clinical decision support systems based on machine learning are a rising application in healthcare. Early detection of deteriorating conditions provide the opportunity for medical intervention in hospital patients. Recent approaches increasingly rely on Large Language Models such as BERT, because patient data is often in the form of structured temporal data. These models are notoriously hard to interpret and therefore to trust, while precisely trust is an essential principle for technology in healthcare. We develop a visual analytics system to inspect, compare, and explain pre-trained transformer models for a given clinical outcome prediction task. The work is developed on the basis of a large hospital patient dataset and prediction tasks for acute kidney injury and heart failure. Discussion with healthcare professionals confirms that our system can lead to a faster decision process and improved modeling results.",
                        "uid": "w-vahc-8545",
                        "time_stamp": "2023-10-22T03:00:00Z",
                        "time_start": "2023-10-22T03:00:00Z",
                        "time_end": "2023-10-22T06:00:00Z",
                        "paper_type": "workshop",
                        "keywords": [
                            "Outcome prediction, Explainable AI and ML in healthcare, Clinical workflows, Patient safety"
                        ],
                        "doi": "",
                        "fno": "",
                        "has_image": true,
                        "has_pdf": true,
                        "paper_award": "",
                        "image_caption": "Proposed visual analytics system that fosters trust into clinical transformer models, consisting of multiple interactive views: (1) Trust in dataset with feature distribution plots and coordinated hierarchical medical code visualization and co-occurance diagrams, (2) trust in model architecture & training with architecture diagram and training loss graph, (3) trust in validation with precision-recall/ROC curves & baseline benchmarks, and (4) trust in prediction with Shapley-values to display feature importance for individual predictions.",
                        "external_paper_link": "",
                        "youtube_ff_link": "",
                        "youtube_ff_id": "",
                        "bunny_ff_link": "https://ieeevis-uploads.b-cdn.net/vis23/w-vahc/w-vahc-8545/w-vahc-8545_Preview.mp4?token=_i2XsKhiyhmabQX_MxaiwBCMfMTIV6wM-RQruSGvYfY&expires=1706590800",
                        "bunny_ff_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/w-vahc/w-vahc-8545/w-vahc-8545_Preview.vtt?token=vqqKmfrNRGOJX00iIKGaurAj8CcWOHifaPZaTSKPxZ8&expires=1706590800",
                        "youtube_prerecorded_link": "https://youtu.be/XcNXTHNVuPU",
                        "youtube_prerecorded_id": "XcNXTHNVuPU",
                        "bunny_prerecorded_link": "https://ieeevis-uploads.b-cdn.net/vis23/w-vahc/w-vahc-8545/w-vahc-8545_Presentation.mp4?token=gyyPc0CyfHlOZCpawQstTNUcxhsuj-MvJRQF_MdCmaI&expires=1706590800",
                        "bunny_prerecorded_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/w-vahc/w-vahc-8545/w-vahc-8545_Presentation.vtt?token=Y_x6XIpssMF8-Q7JZRyJ54vUx03DnTgGv1ZILcvsFY0&expires=1706590800"
                    },
                    {
                        "slot_id": "w-vahc-2982",
                        "session_id": "w4",
                        "title": "Towards medhub: A Self-Service Platform for Analysts and Physicians",
                        "contributors": [
                            "J\u00f6rn Kohlhammer"
                        ],
                        "authors": [
                            "Markus H\u00f6hn, Hendrik L\u00fccke-Tieke, Jan Burmeister, J\u00f6rn Kohlhammer"
                        ],
                        "abstract": "Combining clinical and omics data can improve both daily clinical routines and research to gain more insights into complex medical procedures. We present the results of our first phase in a multi-year collaboration with analysts and physicians aiming at improved inter-disciplinary biomarker identification. We also outline our user-centered approach along its challenges, describe the intermediate technical artifacts that serve as a basis for summative and formative evaluation for the second project phase. Finally, we sketch the road ahead and how we intend to combine visualization research with user-centered design through problem-based prioritization.",
                        "uid": "w-vahc-2982",
                        "time_stamp": "2023-10-22T03:00:00Z",
                        "time_start": "2023-10-22T03:00:00Z",
                        "time_end": "2023-10-22T06:00:00Z",
                        "paper_type": "workshop",
                        "keywords": [
                            "Human-centered computing\u2014Visualization\u2014Visual analytics; Information systems\u2014Information systems and applications\u2014Decision support systems; Applied computing\u2014Life and medical sciences"
                        ],
                        "doi": "",
                        "fno": "",
                        "has_image": true,
                        "has_pdf": true,
                        "paper_award": "",
                        "image_caption": "We are presenting the results of our first phase in a multi-year collaboration with analysts and physicians to improve biomarker identification by combining clinical and omics data.  We describe our user-centered visualization and the technical artifacts used as basis for evaluation in the second project phase.  The concept of our approach is based on the data-user-task triangle by Miksch et al.  Since there are two user groups with different usage scenarios, the analysts and physicians are obligated to communicate on a common basis.  This demand is also reflected in the task description.",
                        "external_paper_link": "",
                        "youtube_ff_link": "",
                        "youtube_ff_id": "",
                        "bunny_ff_link": "",
                        "bunny_ff_subtitles": "",
                        "youtube_prerecorded_link": "",
                        "youtube_prerecorded_id": "",
                        "bunny_prerecorded_link": "",
                        "bunny_prerecorded_subtitles": ""
                    },
                    {
                        "slot_id": "w-vahc-4703",
                        "session_id": "w4",
                        "title": "A Visual Analytics Approach to Exploring the Feature and Label Space Based on Semi-structured Electronic Medical Records",
                        "contributors": [
                            "Yang Ouyang"
                        ],
                        "authors": [
                            "He Wang, Yang Ouyang, Quan Li"
                        ],
                        "abstract": "Electronic health records (EHRs), serving as patient-centered repositories for medical data, offer the opportunity for researchers to uncover concealed patterns using machine learning (ML). However, in real-world medical settings, clinicians often face the task of selecting pertinent feature dimensions from a range of potential medical metrics and then deducing potential labels from vague diagnostic descriptions, prior to the modeling phase. This complexity presents challenges in obtaining reliable training/testing data and conducting thorough analysis. Consequently, these hurdles hinder the practical application of ML for automated modeling and comprehensible interpretation of influencing factors. To tackle these challenges, we introduce a visual analytics approach designed to navigate the feature and label space within EHRs, while also streamlining the modeling process through automated ML algorithms and techniques for improved interpretability.",
                        "uid": "w-vahc-4703",
                        "time_stamp": "2023-10-22T03:00:00Z",
                        "time_start": "2023-10-22T03:00:00Z",
                        "time_end": "2023-10-22T06:00:00Z",
                        "paper_type": "workshop",
                        "keywords": [
                            "Human-centered computing\u2014Visualization\u2014Visualization techniques"
                        ],
                        "doi": "",
                        "fno": "",
                        "has_image": true,
                        "has_pdf": true,
                        "paper_award": "",
                        "image_caption": "System Interface: (A) The interface panel showcases statistical information concerning the dataset and model. (B) Label Identification View aids clinicians in recognizing potential labels from unstructured diagnostic texts. (C) Feature Exploration View empowers clinicians to choose features by evaluating feature distribution and significance. (D) ML Modeling and Interpretation View reveals the distribution of features among diverse groups and the importance of features contributing to distinct categories.",
                        "external_paper_link": "",
                        "youtube_ff_link": "",
                        "youtube_ff_id": "",
                        "bunny_ff_link": "https://ieeevis-uploads.b-cdn.net/vis23/w-vahc/w-vahc-4703/w-vahc-4703_Preview.mp4?token=-xuwtjpcG7OKSBs1jSRtmbgkurd9C3dvlN7TSXbT4VQ&expires=1706590800",
                        "bunny_ff_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/w-vahc/w-vahc-4703/w-vahc-4703_Preview.vtt?token=VYAz8AdFNJj96jMi2y9SOHaPZPJN3cSqyv__65RtNIg&expires=1706590800",
                        "youtube_prerecorded_link": "",
                        "youtube_prerecorded_id": "",
                        "bunny_prerecorded_link": "",
                        "bunny_prerecorded_subtitles": ""
                    },
                    {
                        "slot_id": "w-vahc-6912",
                        "session_id": "w4",
                        "title": "Data Visualization for Mental Health Monitoring in Smart Home Environment: A Case Study",
                        "contributors": [
                            "Youngji Koh"
                        ],
                        "authors": [
                            "Youngji Koh, Chanhee Lee, Yunhee Ku, Uichin Lee"
                        ],
                        "abstract": "Mental health care and monitoring are important. Advancements in smart home sensing technology also make tracking people\u2019s activities easy in the home, enabling the monitoring of mental health more effectively. Some related works have demonstrated the possibilities of mental health monitoring using sensor data collected in smart homes.  However, there is a lack of prior research on how to effectively utilize smart home data visualization to help people understand how their everyday behaviors are related to their mental health status. This poster presents a case study on data visualization for mental health monitoring in a smart home environment. Our web-based application allows users to browse their self-reported mental health states and home activities and visualize the correlation between mental health states and home activities.",
                        "uid": "w-vahc-6912",
                        "time_stamp": "2023-10-22T03:00:00Z",
                        "time_start": "2023-10-22T03:00:00Z",
                        "time_end": "2023-10-22T06:00:00Z",
                        "paper_type": "workshop",
                        "keywords": [
                            "Mobile health, Patient-generated health data, Mental health monitoring"
                        ],
                        "doi": "",
                        "fno": "",
                        "has_image": true,
                        "has_pdf": true,
                        "paper_award": "",
                        "image_caption": "Correlation View of Data Visualization System",
                        "external_paper_link": "",
                        "youtube_ff_link": "",
                        "youtube_ff_id": "",
                        "bunny_ff_link": "",
                        "bunny_ff_subtitles": "",
                        "youtube_prerecorded_link": "",
                        "youtube_prerecorded_id": "",
                        "bunny_prerecorded_link": "",
                        "bunny_prerecorded_subtitles": ""
                    },
                    {
                        "slot_id": "w-vahc-3479",
                        "session_id": "w4",
                        "title": "Clinical Issues and Suggestions: Dashboard Visualization of the Trajectory of Patients with Malignant Hormone-Producing Tumors for Precision Medicine",
                        "contributors": [
                            "Masaki Uchihara"
                        ],
                        "authors": [
                            "Masaki Uchihara, Akiyo Tanabe, Hiroshi Kajio"
                        ],
                        "abstract": "Metastatic hormone-producing tumors have characteristics of both tumors and endocrine disorders with many time-series parameters. Therefore, making treatment decisions is often challenging. Data visualization methods have recently been developed to visualize time series, single or multiple pieces of information, and complex patient information. We focused on metastatic pheochromocytoma and paraganglioma and summarized the clinical needs and dashboard data visualization ideas for precision medicine.",
                        "uid": "w-vahc-3479",
                        "time_stamp": "2023-10-22T03:00:00Z",
                        "time_start": "2023-10-22T03:00:00Z",
                        "time_end": "2023-10-22T06:00:00Z",
                        "paper_type": "workshop",
                        "keywords": [
                            "Personalized medicine, Patient-generated health data, Physician-patient communication"
                        ],
                        "doi": "",
                        "fno": "",
                        "has_image": true,
                        "has_pdf": true,
                        "paper_award": "",
                        "image_caption": "Metastatic hormone-producing tumors have characteristics of both tumors and endocrine disorders with many time-series parameters. Therefore, making treatment decisions is often challenging. An important point in determining the therapeutic strategy for patients with these tumors is the cause of the symptoms and complications of tumor mass and hormone excess, which contribute to a decreased QOL. In addition, the speed of tumor progression is highly variable among patients and is considered the treatment choice. Therefore, it is necessary to visualize tumor size, hormone excess, QOL-related information, and treatments as time-series data.",
                        "external_paper_link": "",
                        "youtube_ff_link": "",
                        "youtube_ff_id": "",
                        "bunny_ff_link": "",
                        "bunny_ff_subtitles": "",
                        "youtube_prerecorded_link": "",
                        "youtube_prerecorded_id": "",
                        "bunny_prerecorded_link": "",
                        "bunny_prerecorded_subtitles": ""
                    },
                    {
                        "slot_id": "w-vahc-8888",
                        "session_id": "w4",
                        "title": "ExpLIMEable: A Visual Analytics Approach for Exploring LIME",
                        "contributors": [
                            "Furui Cheng"
                        ],
                        "authors": [
                            "Sonia Laguna, Julian Heidenreich, Jiugeng Sun, Nil\u00fcfer \u00c7etin, Ibrahim Al-Hazwani, Udo Schlegel, Furui Cheng, Mennatallah El-Assady"
                        ],
                        "abstract": "We introduce ExpLIMEable for enhancing the understanding of Local Interpretable Model-Agnostic Explanations (LIME), with a focus on medical image analysis. LIME is a popular and widely used method in explainable artificial intelligence (XAI) that provides locally faithful and interpretable post-hoc explanations for black box models. However, LIME explanations are not always robust due to variations in perturbation techniques and the selection of interpretable functions. The proposed visual analytics application aims to address these concerns by enabling the users to freely explore and compare the explanations generated by different LIME parameter instances. The application utilizes a convolutional neural network (CNN) for brain MRI tumor classification and allows users to customize post-hoc LIME parameters to gain insights into the model\u2019s decision-making process. The developed application assists machine learning developers in understanding the limitations of LIME and its sensitivity to different parameters, as well as the doctors in providing an explanation to machine learning models, enabling more informed decision-making, with the ultimate goal of improving its robustness and explanation quality.",
                        "uid": "w-vahc-8888",
                        "time_stamp": "2023-10-22T03:00:00Z",
                        "time_start": "2023-10-22T03:00:00Z",
                        "time_end": "2023-10-22T06:00:00Z",
                        "paper_type": "workshop",
                        "keywords": [
                            "Explainable AI Visualization LIME Healthcare"
                        ],
                        "doi": "",
                        "fno": "",
                        "has_image": true,
                        "has_pdf": true,
                        "paper_award": "",
                        "image_caption": "This figure includes the ExpLIMEable interactive workflow and a depiction of the four steps of the proposed pipeline in purple: 1. Image Selection, 2. Explanation, 3. Segmentation, 4. Reduction, and a, potentially endless, loop back to step 2. The algorithms used and methodology are highlighted in red, the user input in green, and the pipeline outputs in yellow. The machine learning expert is portrayed as the user of this pipeline. This explainability tool allows users to tailor and explore the explanation space generated post hoc by different LIME parameters to gain deeper insights into the model's decision-making process, its sensitivity, and limitations.",
                        "external_paper_link": "",
                        "youtube_ff_link": "",
                        "youtube_ff_id": "",
                        "bunny_ff_link": "",
                        "bunny_ff_subtitles": "",
                        "youtube_prerecorded_link": "",
                        "youtube_prerecorded_id": "",
                        "bunny_prerecorded_link": "",
                        "bunny_prerecorded_subtitles": ""
                    },
                    {
                        "slot_id": "w-vahc-6909",
                        "session_id": "w4",
                        "title": "The Iterative Design Process of an Explainable AI Application for Non-Invasive Diagnosis of CNS Tumors: A User-Centered Approach",
                        "contributors": [
                            "Eric Prince"
                        ],
                        "authors": [
                            "Eric Prince, Todd Hankinson, Carsten G\u00f6rg"
                        ],
                        "abstract": "Artificial Intelligence (AI) is well-suited to help support complex decision-making tasks within clinical medicine, including clinical imaging applications like radiographic differential diagnosis of central nervous system (CNS) tumors. So far, there have been numerous examples of theoretical AI solutions for this space, for example, large-scale corporate efforts like IBM\u2019s Watson AI. However, clinical implementation remains limited due to factors related to the alignment of this technology in the clinical setting. User-Centered Design (UCD) is a design philosophy that focuses on developing tailored solutions for specific users or user groups. In this study, we applied UCD to develop an explainable AI tool to support clinicians in our use case. Through four design iterations, starting from basic functionality and visualizations, we progressed to functional prototypes in a realistic testing environment. We discuss our motivation and approach for each iteration, along with key insights gained. This UCD process has advanced our conceptual idea from feasibility testing to interactive functional AI interfaces designed for specific clinical and cognitive tasks. It has also provided us with directions to develop further an AI system for the non-invasive diagnosis of CNS tumors.",
                        "uid": "w-vahc-6909",
                        "time_stamp": "2023-10-22T03:00:00Z",
                        "time_start": "2023-10-22T03:00:00Z",
                        "time_end": "2023-10-22T06:00:00Z",
                        "paper_type": "workshop",
                        "keywords": [
                            "Explainable AI and ML in healthcare, Specific treatment contexts, Clinical workflows"
                        ],
                        "doi": "",
                        "fno": "",
                        "has_image": false,
                        "has_pdf": true,
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "youtube_ff_link": "",
                        "youtube_ff_id": "",
                        "bunny_ff_link": "",
                        "bunny_ff_subtitles": "",
                        "youtube_prerecorded_link": "",
                        "youtube_prerecorded_id": "",
                        "bunny_prerecorded_link": "",
                        "bunny_prerecorded_subtitles": ""
                    },
                    {
                        "slot_id": "w-vahc-7983",
                        "session_id": "w4",
                        "title": "Scalable, interactive and hierarchical visualization of virus taxonomic data",
                        "contributors": [
                            "Sidharth Kumar"
                        ],
                        "authors": [
                            "Kashyap Balakavi, Rushitha Janga, Ahmedur Rahman Shovon, Don Dempsey, Elliot Lefkowitz, sidharth kumar"
                        ],
                        "abstract": "The International Committee on Taxonomy of Viruses (ICTV) maintains a database of virus taxonomy, including records from 1971 up to the most recent release in 2022. Recent years have seen an increasing number of viruses, recording 11,273 distinct species in the latest report. Exploring this increasingly high number of species requires a custom data visualization approach that is both scalable and interactive. Our paper presents a structure-preserving, collapsible, node-link layout-based hierarchical visualization called the Visual Taxonomy Browser, for the virus taxonomy dataset. Developed in collaboration with microbiologists, our visualization improves over the existing Taxonomy Browser3 which is a stacked bar-based approach that lacks scalability and does not preserve the underlying topological structure of the taxonomy. The Visual Taxonomy Browser has been deployed on the ICTV website and has been serving virologists from across the world.",
                        "uid": "w-vahc-7983",
                        "time_stamp": "2023-10-22T03:00:00Z",
                        "time_start": "2023-10-22T03:00:00Z",
                        "time_end": "2023-10-22T06:00:00Z",
                        "paper_type": "workshop",
                        "keywords": [
                            "Life Sciences, Health, Medicine, Biology, Bioinformatics, Genomics"
                        ],
                        "doi": "",
                        "fno": "",
                        "has_image": true,
                        "has_pdf": true,
                        "paper_award": "",
                        "image_caption": "The image shows a linked view visualization of virus taxonomy that maintains topology with interactive features such as a font size slider, a dropdown for switching years, zoom, and drag.",
                        "external_paper_link": "",
                        "youtube_ff_link": "",
                        "youtube_ff_id": "",
                        "bunny_ff_link": "",
                        "bunny_ff_subtitles": "",
                        "youtube_prerecorded_link": "",
                        "youtube_prerecorded_id": "",
                        "bunny_prerecorded_link": "",
                        "bunny_prerecorded_subtitles": ""
                    },
                    {
                        "slot_id": "w-vahc-6655",
                        "session_id": "w4",
                        "title": "MS Pattern Explorer: Interactive Visual Exploration of Temporal Activity Patterns",
                        "contributors": [
                            "Gabriela Morgenshtern"
                        ],
                        "authors": [
                            "Yves Rutishauer, Gabriela Morgenshtern, Christina Haag, Viktor von Wyl, J\u00fcrgen Bernard"
                        ],
                        "abstract": "Novel digital data sources have created new opportunities for digital health research, allowing for detailed insight into individuals\u2019 daily lives. In the case of multiple sclerosis (MS), a neurodegenerative condition presenting as a highly variable corpus of symptoms, sensing devices afford researchers novel methodology for non-invasive, in-the-wild tracking of individuals\u2019 physical activity. However, high-resolution sensors produce data of high dimensionality, complicating the contextualization of MS activity patterns. MS Pattern Explorer is a tool developed iteratively through close collaboration with clinicians, and facilitates the exploration and search for activity patterns in the context of explanatory metadata attributes, both within- and between persons with MS. Our validation includes feedback provided by health experts, along with two case studies conducted by a clinical collaborator. Results show that MS Pattern Explorer can effectively perform within- and between-patient analyses in novel ways, including pattern exploration, search, and contextualization.",
                        "uid": "w-vahc-6655",
                        "time_stamp": "2023-10-22T03:00:00Z",
                        "time_start": "2023-10-22T03:00:00Z",
                        "time_end": "2023-10-22T06:00:00Z",
                        "paper_type": "workshop",
                        "keywords": [
                            "Temporal Data, Task Abstractions & Application Domains, Application Motivated Visualization, Health Informatics, Mixed Initiative Human-Machine Analysis, Multiple Sclerosis"
                        ],
                        "doi": "",
                        "fno": "",
                        "has_image": false,
                        "has_pdf": true,
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "youtube_ff_link": "",
                        "youtube_ff_id": "",
                        "bunny_ff_link": "",
                        "bunny_ff_subtitles": "",
                        "youtube_prerecorded_link": "",
                        "youtube_prerecorded_id": "",
                        "bunny_prerecorded_link": "",
                        "bunny_prerecorded_subtitles": ""
                    },
                    {
                        "slot_id": "w-vahc-7145",
                        "session_id": "w4",
                        "title": "Designing the Australian Cancer Atlas: Visualising Geostatistical Model Uncertainty for Multiple Audiences",
                        "contributors": [
                            "Sarah Goodwin"
                        ],
                        "authors": [
                            "Sarah Goodwin, Thom Saunders, Joanne Aitken, Peter D Baade, Upeksha Chandrasiri, Di Cook, Susanna Cramb, Earl Duncan, Stephanie Kobakian, Jessie Roberts, Kerrie Mengersen"
                        ],
                        "abstract": "We present the visualisation design and development process for the Australian Cancer Atlas, which aims to provide small-area estimates of cancer incidence and survival in Australia to help identify and address geographical health disparities. An 18-month user-centered design study was conducted to visualise the small-area Bayesian model estimates, and in particular to enhance the communication of estimate uncertainty for multiple audiences: the general public, researchers, health practitioners and policy users. We document the methodology, design iterations, design decision-making, methods for reaching a wide audience and lessons learned for future atlases.",
                        "uid": "w-vahc-7145",
                        "time_stamp": "2023-10-22T03:00:00Z",
                        "time_start": "2023-10-22T03:00:00Z",
                        "time_end": "2023-10-22T06:00:00Z",
                        "paper_type": "workshop",
                        "keywords": [
                            "Human-centered computing\u2014Visualization\u2014Visualization design and evaluation methods"
                        ],
                        "doi": "",
                        "fno": "",
                        "has_image": false,
                        "has_pdf": true,
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "youtube_ff_link": "",
                        "youtube_ff_id": "",
                        "bunny_ff_link": "",
                        "bunny_ff_subtitles": "",
                        "youtube_prerecorded_link": "",
                        "youtube_prerecorded_id": "",
                        "bunny_prerecorded_link": "",
                        "bunny_prerecorded_subtitles": ""
                    }
                ]
            }
        ]
    },
    "w-Vis4PandEmRes": {
        "event": "Visualization for Pandemic and Emergency Responses Workshop (Vis4PandEmRes)",
        "long_name": "Visualization for Pandemic and Emergency Responses Workshop (Vis4PandEmRes)",
        "event_type": "workshop",
        "event_prefix": "w-Vis4PandEmRes",
        "event_description": "",
        "event_url": "",
        "organizers": [
            "Alfie Abdul-Rahman",
            "Kelly Gaither",
            "Wolfgang Jentner",
            "Tobias Schreck",
            "Min Chen",
            "David Ebert"
        ],
        "sessions": [
            {
                "title": "Visualization for Pandemic and Emergency Responses Workshop (Vis4PandEmRes)",
                "session_id": "w7",
                "event_prefix": "w-Vis4PandEmRes",
                "track": "oneohfour",
                "session_image": "w7.png",
                "chair": [
                    "Alfie Abdul-Rahman",
                    "Kelly Gaither",
                    "Wolfgang Jentner",
                    "Tobias Schreck",
                    "Min Chen",
                    "David Ebert"
                ],
                "time_start": "2023-10-21T22:00:00Z",
                "time_end": "2023-10-22T01:00:00Z",
                "discord_category": "",
                "discord_channel": "104",
                "discord_channel_id": "1161741240665124954",
                "discord_link": "https://discordapp.com/channels/1161400451619627068/1161741240665124954",
                "zoom_private_meeting": "",
                "zoom_private_password": "",
                "zoom_private_link": "",
                "zoom_broadcast_link": "",
                "ff_link": "",
                "time_slots": [
                    {
                        "slot_id": "w-Vis4PandEmRes-1013",
                        "session_id": "w7",
                        "title": "A Lens to Pandemic Stay at Home Attitudes",
                        "contributors": [
                            "G. Elisabeta Marai"
                        ],
                        "authors": [
                            "Andrew Wentzel",
                            "Lauren Levine",
                            "Vipul Dhariwal",
                            "Zahra Fatemi",
                            "Abari Bhattacharya",
                            "Barbara Di Eugenio",
                            "Andrew Rojecki",
                            "Elena Zheleva",
                            "G. Elisabeta Marai"
                        ],
                        "abstract": "We describe the design process and the challenges we met during a rapid multi-disciplinary pandemic project related to stay-at-home orders and social media moral frames. Unlike our typical design experience, we had to handle a steeper learning curve, emerging and continually changing datasets, as well as under-specified design requirements, persistent low visual literacy, and an extremely fast turnaround for new data ingestion, prototyping, testing and deployment. We describe the lessons learned through this experience.",
                        "uid": "w-Vis4PandEmRes-1013",
                        "time_stamp": "2023-10-21T22:00:00Z",
                        "time_start": "2023-10-21T22:00:00Z",
                        "time_end": "2023-10-22T01:00:00Z",
                        "paper_type": "workshop",
                        "keywords": [],
                        "doi": "",
                        "fno": "",
                        "has_image": false,
                        "has_pdf": true,
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "youtube_ff_link": "",
                        "youtube_ff_id": "",
                        "bunny_ff_link": "",
                        "bunny_ff_subtitles": "",
                        "youtube_prerecorded_link": "",
                        "youtube_prerecorded_id": "",
                        "bunny_prerecorded_link": "",
                        "bunny_prerecorded_subtitles": ""
                    },
                    {
                        "slot_id": "w-Vis4PandEmRes-1017",
                        "session_id": "w7",
                        "title": "ESID: Exploring the Design and Development of a Visual Analytics Tool for Epidemiological Emergencies",
                        "contributors": [
                            "Andreas Gerndt"
                        ],
                        "authors": [
                            "Pawandeep Kaur Betz",
                            "Julien Stoll",
                            "Valerie Grappendorf",
                            "Jonas Gilg",
                            "Moritz Zeumer",
                            "Margrit Klitz",
                            "Luca Spataro",
                            "Anne Klein",
                            "Lena Rothenh\u00e4user",
                            "Hartmut Bohnacker",
                            "Hans Kr\u00e4mer",
                            "Michael Meyer-Hermann",
                            "Sybille Somogyi",
                            "Andreas Gerndt",
                            "Martin J. K\u00fchn"
                        ],
                        "abstract": "Visual analytics tools can help illustrate the spread of infectious diseases and enable informed decisions on epidemiological and public health issues. To create visualisation tools that are intuitive, easy to use, and effective in communicating information, continued research and development focusing on user-centric and methodological design models is extremely important. As a contribution to this topic, this paper presents the design and development process of the visual analytics application ESID (Epidemiological Scenarios for Infectious Diseases ). ESID is a visual analytics tool aimed at projecting the future developments of infectious disease spread using reported and simulated data based on sound mathematical-epidemiological models. The development process involved a collaborative and participatory design approach with project partners from diverse scientific fields. The findings from these studies, along with the guidelines derived from them, played a pivotal role in shaping the visualisation tool.",
                        "uid": "w-Vis4PandEmRes-1017",
                        "time_stamp": "2023-10-21T22:00:00Z",
                        "time_start": "2023-10-21T22:00:00Z",
                        "time_end": "2023-10-22T01:00:00Z",
                        "paper_type": "workshop",
                        "keywords": [],
                        "doi": "",
                        "fno": "",
                        "has_image": false,
                        "has_pdf": true,
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "youtube_ff_link": "",
                        "youtube_ff_id": "",
                        "bunny_ff_link": "",
                        "bunny_ff_subtitles": "",
                        "youtube_prerecorded_link": "",
                        "youtube_prerecorded_id": "",
                        "bunny_prerecorded_link": "",
                        "bunny_prerecorded_subtitles": ""
                    },
                    {
                        "slot_id": "w-Vis4PandEmRes-1018",
                        "session_id": "w7",
                        "title": "Visual Analytics based Search-Analyze-Forecast Framework for Epidemiological Time-series Data",
                        "contributors": [
                            "Alfie Abdul-Rahman"
                        ],
                        "authors": [
                            "tuna gonen",
                            "Yiwen Xing",
                            "Cagatay Turkay",
                            "Alfie Abdul-Rahman",
                            "Radu Jianu",
                            "Hui Fang",
                            "Euan Freeman",
                            "Franck P Vidal",
                            "Min Chen"
                        ],
                        "abstract": "The COVID-19 pandemic has been a period where time-series of disease statistics, such as the number of cases or vaccinations, have been intensively used by public health professionals to estimate how their region compares to others and estimate what future could look like at home. Conventional visualizations are often limited in terms of advanced comparative features and in supporting forecasting systematically. This paper presents a visual analytics approach to support data-driven prediction based on a search-analyze-predict process comprising a multi-metric, multi-criteria time-series search method and a data-driven prediction technique. These are supported by a visualization framework for the comprehensive comparison of multiple time-series. We inform the design of our approach by getting iterative feedback from public health experts globally, and evaluate it both quantitatively and qualitatively.",
                        "uid": "w-Vis4PandEmRes-1018",
                        "time_stamp": "2023-10-21T22:00:00Z",
                        "time_start": "2023-10-21T22:00:00Z",
                        "time_end": "2023-10-22T01:00:00Z",
                        "paper_type": "workshop",
                        "keywords": [],
                        "doi": "",
                        "fno": "",
                        "has_image": false,
                        "has_pdf": true,
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "youtube_ff_link": "",
                        "youtube_ff_id": "",
                        "bunny_ff_link": "",
                        "bunny_ff_subtitles": "",
                        "youtube_prerecorded_link": "",
                        "youtube_prerecorded_id": "",
                        "bunny_prerecorded_link": "",
                        "bunny_prerecorded_subtitles": ""
                    }
                ]
            }
        ]
    },
    "w-viscomm": {
        "event": "Sixth Workshop on Visualization for Communication (VisComm)",
        "long_name": "Sixth Workshop on Visualization for Communication (VisComm)",
        "event_type": "workshop",
        "event_prefix": "w-viscomm",
        "event_description": "",
        "event_url": "",
        "organizers": [
            "Paul Parsons",
            "Jon Schwabish",
            "Alvitta Ottley",
            "Alice Feng"
        ],
        "sessions": [
            {
                "title": "Sixth Workshop on Visualization for Communication (VisComm)",
                "session_id": "w9",
                "event_prefix": "w-viscomm",
                "track": "oneohfive",
                "session_image": "w9.png",
                "chair": [
                    "Paul Parsons",
                    "Jon Schwabish",
                    "Alvitta Ottley",
                    "Alice Feng"
                ],
                "time_start": "2023-10-22T03:00:00Z",
                "time_end": "2023-10-22T06:00:00Z",
                "discord_category": "",
                "discord_channel": "105",
                "discord_channel_id": "1161741309346861067",
                "discord_link": "https://discordapp.com/channels/1161400451619627068/1161741309346861067",
                "zoom_private_meeting": "",
                "zoom_private_password": "",
                "zoom_private_link": "",
                "zoom_broadcast_link": "",
                "ff_link": "",
                "time_slots": [
                    {
                        "slot_id": "w-viscomm-1002",
                        "session_id": "w9",
                        "title": "Visual Communication of Aftershock Forecasts Based on User Needs: A Case Study of the US, Mexico and El Salvador",
                        "contributors": [
                            "Max Schneider"
                        ],
                        "authors": [
                            "Max Schneider, Anne Wein, Nicholas van der Elst, Sara K. McBride, Julia Becker, Raul R Castro, Manuel Diaz, Hector Gonzalez-Huizar, Jeanne Hardebeck, Andrew Michael, Luis Ernesto Mixco, Morgan Page, Jocelyn I Palomo"
                        ],
                        "abstract": "Aftershock forecasts can help reduce seismic risk by communicating how many aftershocks can be expected following a large earthquake, and how the expected number of aftershocks evolves over time and space. Prior work finds that graphical forecast products may better communicate this information than only text or numbers. To identify which visual products can serve numerous user groups, we held workshops with members of target professions, including emergency managers, engineers, critical infrastructure operators, science com- municators, and more. We conducted these workshops in the U.S., Mexico and El Salvador to understand which forecast products may be effective across countries. Many users reported needing maps of shaking hazard to support their work. We find a greater variation in user needs across professions than country, and that user needs also vary with time. We discuss the practical implications for effective visual communication of aftershock forecasts and natural hazards.",
                        "uid": "w-viscomm-1002",
                        "time_stamp": "2023-10-22T03:00:00Z",
                        "time_start": "2023-10-22T03:00:00Z",
                        "time_end": "2023-10-22T06:00:00Z",
                        "paper_type": "workshop",
                        "keywords": [
                            "Evaluation\u2014Human-Subjects Qualitative Studies Human Factors\u2014General Public Applications\u2014Physical & Environmental Sciences, Engineering, Mathematics"
                        ],
                        "doi": "",
                        "fno": "",
                        "has_image": true,
                        "has_pdf": true,
                        "paper_award": "",
                        "image_caption": "Example forecast map showing the probability of strong shaking due to aftershocks in the week following the 2010 M7.2 El Mayor-Cucapah earthquake in northern Mexico. Strong shaking is defined as a Modified Mercalli Intensity level of VI or higher.",
                        "external_paper_link": "",
                        "youtube_ff_link": "",
                        "youtube_ff_id": "",
                        "bunny_ff_link": "https://ieeevis-uploads.b-cdn.net/vis23/w-viscomm/w-viscomm-1002/w-viscomm-1002_Preview.mp4?token=YT734ths6VM3NjnV7_JlNhJ3PCGutCFBZEtRQ8wc18M&expires=1706590800",
                        "bunny_ff_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/w-viscomm/w-viscomm-1002/w-viscomm-1002_Preview.vtt?token=F0V0ztBONiLxMnbVBHKqMYW7ZuP7rt4lIJ4vXmzw5EQ&expires=1706590800",
                        "youtube_prerecorded_link": "https://youtu.be/lNaW64l4RTs",
                        "youtube_prerecorded_id": "lNaW64l4RTs",
                        "bunny_prerecorded_link": "https://ieeevis-uploads.b-cdn.net/vis23/w-viscomm/w-viscomm-1002/w-viscomm-1002_Presentation.mp4?token=Wo2RbC0RgmzQIMl9_0l8-Slnpa5w5Ztzrq-VcRbuoGM&expires=1706590800",
                        "bunny_prerecorded_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/w-viscomm/w-viscomm-1002/w-viscomm-1002_Presentation.vtt?token=U6D8lkx0cC7SfAlVrtrp5uUbaOwN2Gg1zWzDddq3RaY&expires=1706590800"
                    },
                    {
                        "slot_id": "w-viscomm-1007",
                        "session_id": "w9",
                        "title": "Towards an Online System to Generate Tailored Infographics: Supporting the Health Information Sharing Needs of Community-Based Organizations",
                        "contributors": [
                            "Dr. Adriana Arcia"
                        ],
                        "authors": [
                            "Adriana Arcia, Eri Noguchi, Suzanne Bakken"
                        ],
                        "abstract": "Infographics are an engaging way to share health information with the public, but their relevance and appeal can be improved if they can be tailored to the language, culture, and information needs of their target audience. Digital tools are needed to make such tailoring feasible at scale, and to meet the needs of the community-based organizations (CBOs) that are well-situated to share health information with the public. Here, we describe our progress toward the development of the TailorVis Toolbox, an online system that facilitates infographic tailoring at the level of the CBO and the individual viewer. Incorporated within this project was the participatory design of infographics related to COVID-19 testing and vaccination. The system will be extended to numerous health topics in the future.",
                        "uid": "w-viscomm-1007",
                        "time_stamp": "2023-10-22T03:00:00Z",
                        "time_start": "2023-10-22T03:00:00Z",
                        "time_end": "2023-10-22T06:00:00Z",
                        "paper_type": "workshop",
                        "keywords": [
                            "Human-centered computing\u2014Visualization\u2014Visualization application domains\u2014Information visualization; Applied computing\u2014Life and medical sciences\u2014Consumer health"
                        ],
                        "doi": "",
                        "fno": "",
                        "has_image": true,
                        "has_pdf": true,
                        "paper_award": "",
                        "image_caption": "Excerpts from the COVID-19 testing infographic showing the tailoring features of TailorVis Toolbox.",
                        "external_paper_link": "",
                        "youtube_ff_link": "",
                        "youtube_ff_id": "",
                        "bunny_ff_link": "https://ieeevis-uploads.b-cdn.net/vis23/w-viscomm/w-viscomm-1007/w-viscomm-1007_Preview.mp4?token=6x6kHRZ59wU5igqZdbjN7IeWSFtJ81wctsEE4s1IeSM&expires=1706590800",
                        "bunny_ff_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/w-viscomm/w-viscomm-1007/w-viscomm-1007_Preview.vtt?token=7qQp1BbYA6T4-MaxaDZxsbgt7YTFjnL_xwlZLBnw5Vc&expires=1706590800",
                        "youtube_prerecorded_link": "",
                        "youtube_prerecorded_id": "",
                        "bunny_prerecorded_link": "https://ieeevis-uploads.b-cdn.net/vis23/w-viscomm/w-viscomm-1007/w-viscomm-1007_Presentation.mp4?token=oFtrd36yFDsERCi9UAM7Y_XO-75y35bmwWv1izxnObs&expires=1706590800",
                        "bunny_prerecorded_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/w-viscomm/w-viscomm-1007/w-viscomm-1007_Presentation.vtt?token=CD6NSrqCdX-fx2u8CkZ1fsRAevaSLbqDSsuL75gB7o0&expires=1706590800"
                    },
                    {
                        "slot_id": "w-viscomm-1009",
                        "session_id": "w9",
                        "title": "Explicating Implicit Frames: A Key to Communicating Visualization Successfully",
                        "contributors": [
                            "Paul Parsons"
                        ],
                        "authors": [
                            "Prakash Chandra Shukla, Paul Parsons"
                        ],
                        "abstract": "As the field of visualization continues to expand, understanding how visualization designers think and the kinds of assumptions they hold becomes crucial for creating and communicating clear, accurate, and impactful visualizations. In this exploratory study, we analyzed Twitter threads discussing visualization, aiming to uncover the beliefs and assumptions held by visualization designers. Utilizing 8 Twitter threads rich in discussion as case studies, we reveal the implicit assumptions that designers harbor while creating visualizations. If unaddressed, these assumptions often lead to confusion or misinterpretation. This paper discusses several strategies that designers can leverage to increase awareness about the assumptions they make. Though this study has limitations, given its reliance on inferred findings from self-reported Twitter data, it offers valuable insights for visualization design practitioners and proposes directions for future research. By addressing the issue of implicit assumptions by visualization designers, we can enhance the effectiveness and clarity of visualization communication.",
                        "uid": "w-viscomm-1009",
                        "time_stamp": "2023-10-22T03:00:00Z",
                        "time_start": "2023-10-22T03:00:00Z",
                        "time_end": "2023-10-22T06:00:00Z",
                        "paper_type": "workshop",
                        "keywords": [
                            "Framing, Visualization Communication, Design Cognition, Design Practice"
                        ],
                        "doi": "",
                        "fno": "",
                        "has_image": false,
                        "has_pdf": true,
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "youtube_ff_link": "",
                        "youtube_ff_id": "",
                        "bunny_ff_link": "",
                        "bunny_ff_subtitles": "",
                        "youtube_prerecorded_link": "",
                        "youtube_prerecorded_id": "",
                        "bunny_prerecorded_link": "",
                        "bunny_prerecorded_subtitles": ""
                    },
                    {
                        "slot_id": "w-viscomm-1010",
                        "session_id": "w9",
                        "title": "Animating history: An energy Sankey movie, 1800\u20132019",
                        "contributors": [
                            "Nathan Matteson"
                        ],
                        "authors": [
                            "Robert Suits, Ben Kleeman, Nathan Matteson, Liz Moyer, Milson Munakami, Kalyan Reddivari"
                        ],
                        "abstract": "Concern over climate change is driving interest in changing the U.S. energy system, which is the result of two centuries of gradual development. Visualizing this evolution is vital to understanding this system, but visualizations in energy history do not fully capture its dynamics. In this project, we present an animated \u201cSankey diagram\u201d that shows the evolution of the U.S. energy system from 1800 to the present. Sankey diagrams are commonly used to compactly represent energy systems, showing the flow of various energy types from primary sources to end uses. However, Sankey diagrams are rarely animated, because of the computational and graphical challenge in developing a generalizable architecture for presenting information cleanly. The project we present therefore involves both archival historical research and development of new software tools.",
                        "uid": "w-viscomm-1010",
                        "time_stamp": "2023-10-22T03:00:00Z",
                        "time_start": "2023-10-22T03:00:00Z",
                        "time_end": "2023-10-22T06:00:00Z",
                        "paper_type": "workshop",
                        "keywords": [],
                        "doi": "",
                        "fno": "",
                        "has_image": false,
                        "has_pdf": true,
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "youtube_ff_link": "",
                        "youtube_ff_id": "",
                        "bunny_ff_link": "",
                        "bunny_ff_subtitles": "",
                        "youtube_prerecorded_link": "",
                        "youtube_prerecorded_id": "",
                        "bunny_prerecorded_link": "",
                        "bunny_prerecorded_subtitles": ""
                    },
                    {
                        "slot_id": "w-viscomm-1006",
                        "session_id": "w9",
                        "title": "Design of Visualization Onboarding Concepts for a 2D Scatterplot in a Biomedical Visual Analytics Tool",
                        "contributors": [
                            "Christina Stoiber"
                        ],
                        "authors": [
                            "Christina Stoiber, Daniela Moitzi, Holger Stitz, Dominic Girardi, Marc Streit, Wolfgang Aigner"
                        ],
                        "abstract": "Biomedical research is highly data-driven. Domain experts need to learn how to interpret complex data visualizations to gain insights. They often need help interpreting data visualizations as they are not part of their training. Integrating visualization onboarding concepts into visual analytics (VA) tools can support users in interpreting, reading, and extracting information from visual presentations. In this paper, we present the design of the onboarding concept for an interactive VA tool to analyze large scaled biological data, particularly high-throughput screening (HTS) data. We evaluated our onboarding design by conducting a cognitive walkthrough and interviews with thinking aloud. We also collected data on domain experts\u2019 visualization literacy. The results of the cognitive walkthrough showed that domain experts positively commented on the onboarding design and proposed adjusting smaller aspects. The interviews showed that domain experts are well-trained in interpreting basic visualizations (e.g., scatterplot, bar chart, line chart). However, they need support correctly interpreting the data visualized in the scatterplot, as they are new to them. Another important insight was fitting the onboarding messages into the domain\u2019s language.",
                        "uid": "w-viscomm-1006",
                        "time_stamp": "2023-10-22T03:00:00Z",
                        "time_start": "2023-10-22T03:00:00Z",
                        "time_end": "2023-10-22T06:00:00Z",
                        "paper_type": "workshop",
                        "keywords": [
                            "Human-centered computing\u2014Visualization\u2014 Visualization design and evaluation method;"
                        ],
                        "doi": "",
                        "fno": "",
                        "has_image": true,
                        "has_pdf": true,
                        "paper_award": "",
                        "image_caption": "{\\rtf1\\ansi\\ansicpg1252\\cocoartf2709 \\cocoatextscaling0\\cocoaplatform0{\\fonttbl\\f0\\fswiss\\fcharset0 ArialMT;\\f1\\froman\\fcharset0 Times-Roman;} {\\colortbl;\\red255\\green255\\blue255;\\red0\\green0\\blue0;} {\\*\\expandedcolortbl;;\\cssrgb\\c0\\c0\\c0;} \\paperw11900\\paperh16840\\margl1440\\margr1440\\vieww11520\\viewh8400\\viewkind0 \\deftab720 \\pard\\pardeftab720\\sa320\\partightenfactor0  \\f0\\fs29\\fsmilli14667 \\cf0 \\expnd0\\expndtw0\\kerning0 \\outl0\\strokewidth0 \\strokec2 Onboarding navigation concept. The navigation comprises two elements: the floating action button (1) revealing three onboarding stages: Reading, Interacting, and Analyzing, and (2) a step-by-step navigation widget that represents onboarding messages and arrow icons to navigate through them. \\f1\\fs24 \\ }",
                        "external_paper_link": "",
                        "youtube_ff_link": "",
                        "youtube_ff_id": "",
                        "bunny_ff_link": "",
                        "bunny_ff_subtitles": "",
                        "youtube_prerecorded_link": "",
                        "youtube_prerecorded_id": "",
                        "bunny_prerecorded_link": "",
                        "bunny_prerecorded_subtitles": ""
                    },
                    {
                        "slot_id": "w-viscomm-1005",
                        "session_id": "w9",
                        "title": "Exploring Annotation Strategies in Professional Visualizations: Insights from Prominent US News Portals",
                        "contributors": [
                            "Md Dilshadur Rahman"
                        ],
                        "authors": [
                            "Md Dilshadur Rahman, Ghulam Jilani Quadri, Paul Rosen"
                        ],
                        "abstract": "Annotations play a vital role in visualizations, providing valuable insights and focusing attention on critical visual elements.This study analyzes a curated corpus of 72 professionally designed static charts with annotations from prominent US news portals includingThe  New York Times, The Economists, The Wall Street Journal, and The  Washington Post. The analysis employed a qualitative approach involving identifying annotation types, assessing their frequency, exploring annotation combinations, categorizing text quantity, and examining the relationship between chart captions and annotations. The analysis reveals common patterns in annotation strategies used by professionals, including extensive use of annotations aligned with chart captions, targeted highlighting and descriptive text within charts, strategic utilization of multiple annotations as ensembles, and emphasis on article-related numerical values. These findings provide valuable guidance for improving annotation practices, tools, and methodologies, enhancing data comprehension and communication in visualizations.",
                        "uid": "w-viscomm-1005",
                        "time_stamp": "2023-10-22T03:00:00Z",
                        "time_start": "2023-10-22T03:00:00Z",
                        "time_end": "2023-10-22T06:00:00Z",
                        "paper_type": "workshop",
                        "keywords": [
                            "Annotations, Visualization design, Visualization techniques, Professional practices"
                        ],
                        "doi": "",
                        "fno": "",
                        "has_image": true,
                        "has_pdf": true,
                        "paper_award": "",
                        "image_caption": "Examples of annotated professional charts used in prominent US news portals, including (a) a waterfall chart (i.e., a variant of a bar chart) utilizing gray highlights, connectors, and text descriptions; (b) a bar chart with directional marks, value text, descriptions, and context-specific color highlighting; (c) a scatterplot featuring a trend line and text; (d) a line chart with context-specific highlighting and value text; (e) another line chart using text descriptions, connectors, enclosures, and context-specific highlighting; and (f) a scatterplot with context-specific highlighting, data point labels, and a text description.",
                        "external_paper_link": "",
                        "youtube_ff_link": "",
                        "youtube_ff_id": "",
                        "bunny_ff_link": "",
                        "bunny_ff_subtitles": "",
                        "youtube_prerecorded_link": "",
                        "youtube_prerecorded_id": "",
                        "bunny_prerecorded_link": "",
                        "bunny_prerecorded_subtitles": ""
                    }
                ]
            }
        ]
    },
    "w-eduvis": {
        "event": "EduVis: Workshop on Visualization Education, Literacy, and Activities",
        "long_name": "EduVis: Workshop on Visualization Education, Literacy, and Activities",
        "event_type": "workshop",
        "event_prefix": "w-eduvis",
        "event_description": "",
        "event_url": "",
        "organizers": [
            "Mandy Keck",
            "Samuel Huron",
            "Georgia Panagiotidou",
            "Christina Stoiber",
            "Fateme Rajabiyazdi",
            "Charles Perin",
            "Jonathan C Roberts",
            "Benjamin Bach"
        ],
        "sessions": [
            {
                "title": "EduVis: Workshop on Visualization Education, Literacy, and Activities",
                "session_id": "w11",
                "event_prefix": "w-eduvis",
                "track": "oneohnine",
                "session_image": "w11.png",
                "chair": [
                    "Mandy Keck",
                    "Samuel Huron",
                    "Georgia Panagiotidou",
                    "Christina Stoiber",
                    "Fateme Rajabiyazdi",
                    "Charles Perin",
                    "Jonathan C Roberts",
                    "Benjamin Bach"
                ],
                "time_start": "2023-10-22T22:00:00Z",
                "time_end": "2023-10-23T01:00:00Z",
                "discord_category": "",
                "discord_channel": "109",
                "discord_channel_id": "1161741434647482379",
                "discord_link": "https://discordapp.com/channels/1161400451619627068/1161741434647482379",
                "zoom_private_meeting": "",
                "zoom_private_password": "",
                "zoom_private_link": "",
                "zoom_broadcast_link": "",
                "ff_link": "",
                "time_slots": [
                    {
                        "slot_id": "w-eduvis-1001",
                        "session_id": "w11",
                        "title": "Reflections on Designing and Running Visualization Design and Programming Activities in Courses with Many Students",
                        "contributors": [
                            "S\u00f8ren Knudsen"
                        ],
                        "authors": [
                            "S\u00f8ren Knudsen",
                            "Mathilde Bech Bennetsen",
                            "Terese Kimmie H\u00f8j",
                            "Camilla Jensen",
                            "Rebecca Louise N\u00f8rskov J\u00f8rgensen",
                            "Christian S\u00f8e Loft"
                        ],
                        "abstract": "In this paper, we reflect on the educational challenges and research opportunities in running data visualization design activities in the context of large courses. With the increasing number and sizes of data visualization course, we need to better understand approaches to scaling our teaching efforts. We draw on experiences organizing and facilitating activities primarily based on one instance of a master's course given to about 130 students. We provide a detailed account of the course with particular focus on the purpose, structure, and outcome of six two-hour design activities. Based on this, we reflect on three aspects of the course: First, how the course scale led us to thoroughly plan, evaluate, and revise communication between students, teaching assistants, and lecturers. Second, how we designed learning scaffolds through the design activities, and the reflections we received from students on this matter. Finally, we reflect on the diversity of the students that followed the course, the visualization exercises we used, the projects they worked on, and when to key in on simple boring problems and data sets. Thus, our paper contributes with discussions about balancing topical diversity, scaling courses to many students, and problem-based learning.",
                        "uid": "w-eduvis-1001",
                        "time_stamp": "2023-10-22T22:00:00Z",
                        "time_start": "2023-10-22T22:00:00Z",
                        "time_end": "2023-10-23T01:00:00Z",
                        "paper_type": "workshop",
                        "keywords": [],
                        "doi": "",
                        "fno": "",
                        "has_image": false,
                        "has_pdf": true,
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "youtube_ff_link": "",
                        "youtube_ff_id": "",
                        "bunny_ff_link": "",
                        "bunny_ff_subtitles": "",
                        "youtube_prerecorded_link": "",
                        "youtube_prerecorded_id": "",
                        "bunny_prerecorded_link": "",
                        "bunny_prerecorded_subtitles": ""
                    },
                    {
                        "slot_id": "w-eduvis-1002",
                        "session_id": "w11",
                        "title": "Developing Technical Skillsets in Diverse Audiences",
                        "contributors": [
                            "Jonathan P. Leidig"
                        ],
                        "authors": [
                            "Jonathan P. Leidig"
                        ],
                        "abstract": "Designing courses that are taken by diverse student audiences is complicated by differing technical backgrounds and desired future roles. One proposed approach to course design bundles course assignments into parallel tracks for students to self-select and complete. An initial set of three tracks of computing laboratory-based assignments were developed and tested in the classroom. Each track option utilized different visualization software and libraries with a range of expected pre-requisite knowledge and technical abilities. Student and instructor reflections on the approach reinforced the suitability of a track-based course design to fulfill course objectives towards the education of diverse audiences.",
                        "uid": "w-eduvis-1002",
                        "time_stamp": "2023-10-22T22:00:00Z",
                        "time_start": "2023-10-22T22:00:00Z",
                        "time_end": "2023-10-23T01:00:00Z",
                        "paper_type": "workshop",
                        "keywords": [],
                        "doi": "",
                        "fno": "",
                        "has_image": true,
                        "has_pdf": true,
                        "paper_award": "",
                        "image_caption": "The paper discusses three tracks of laboratory assignments for visualization courses. Each track bundles multiple laboratory assignment covering one topical area. Students select and complete one track in the author's courses based on their own diverse backgrounds and personal interests.",
                        "external_paper_link": "",
                        "youtube_ff_link": "",
                        "youtube_ff_id": "",
                        "bunny_ff_link": "",
                        "bunny_ff_subtitles": "",
                        "youtube_prerecorded_link": "",
                        "youtube_prerecorded_id": "",
                        "bunny_prerecorded_link": "",
                        "bunny_prerecorded_subtitles": ""
                    },
                    {
                        "slot_id": "w-eduvis-1003",
                        "session_id": "w11",
                        "title": "Choose-your-own D3 labs for learning to adapt online code",
                        "contributors": [
                            "Maryam Hedayati"
                        ],
                        "authors": [
                            "Maryam Hedayati",
                            "Matthew Kay"
                        ],
                        "abstract": "D3 is a popular library for implementing data visualizations, and is often taught in data visualization classes. However, D3 can be difficult to learn, and it can be especially challenging to make use of online examples, which often require changes to work in standalone JavaScript. We have previously taught D3 using guided tutorials, but found that students struggled to apply what they had learned to other visualization types or contexts. To address this, we introduced a new assignment type: choose-your-own labs. In each lab, students implemented a visualization technique from that week\u2019s lecture. We provided a code sample and asked students to get the code sample working in the latest version of D3 as a standalone webpage, sometimes with a new dataset. This paper reflects on our experiences using this new assignment. Although students seemed to find the process of debugging real-world example D3 code to be tedious, they generally responded well to the assignment. We also observed that the quality and creativity of the final group projects in the class were improved from previous iterations of the course. We provide suggestions for educators who want to use a similar format in their courses, and provide our materials at https://osf.io/47vfy/?view_only=7e478a23e3e5414086569694279d38fe.",
                        "uid": "w-eduvis-1003",
                        "time_stamp": "2023-10-22T22:00:00Z",
                        "time_start": "2023-10-22T22:00:00Z",
                        "time_end": "2023-10-23T01:00:00Z",
                        "paper_type": "workshop",
                        "keywords": [],
                        "doi": "",
                        "fno": "",
                        "has_image": false,
                        "has_pdf": true,
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "youtube_ff_link": "",
                        "youtube_ff_id": "",
                        "bunny_ff_link": "",
                        "bunny_ff_subtitles": "",
                        "youtube_prerecorded_link": "",
                        "youtube_prerecorded_id": "",
                        "bunny_prerecorded_link": "",
                        "bunny_prerecorded_subtitles": ""
                    },
                    {
                        "slot_id": "w-eduvis-1006",
                        "session_id": "w11",
                        "title": "Educational Data Comics: What can Comics do for Education in Visualization?",
                        "contributors": [
                            "Magdalena Boucher"
                        ],
                        "authors": [
                            "Magdalena Boucher",
                            "Benjamin Bach",
                            "Christina Stoiber",
                            "Zezhong Wang",
                            "Wolfgang Aigner"
                        ],
                        "abstract": "This paper discusses the potential of comics for explaining concepts with and around data visualization. With the increasing spread of visualizations and the democratization of access to visualization tools, we see a growing need for easily approachable resources for learning visualization techniques, applications, design processes, etc. Comics are a promising medium for such explanation as they concisely combine graphical and textual content in a sequential manner and they provide fast visual access to specific parts of the explanations. Based on a first literature review and our extensive experience with the subject, we survey works at the respective intersections of comics, visualization and education: data comics, educational comics, and visualization education. We report on five potentials of comics to create and share educational material, to engage wide and potentially diverse audiences, and to support educational activities. For each potential we list, we describe open questions for future research. Our discussion aims to inform both the application of comics by educators and their extension and study by researchers.",
                        "uid": "w-eduvis-1006",
                        "time_stamp": "2023-10-22T22:00:00Z",
                        "time_start": "2023-10-22T22:00:00Z",
                        "time_end": "2023-10-23T01:00:00Z",
                        "paper_type": "workshop",
                        "keywords": [],
                        "doi": "",
                        "fno": "",
                        "has_image": true,
                        "has_pdf": true,
                        "paper_award": "",
                        "image_caption": "What can comics do for Education in Visualization? The image shows two simple comic figures sitting at a table, studying a comic and talking about it. In the second panel, both express happiness and seem to have understood something.",
                        "external_paper_link": "",
                        "youtube_ff_link": "",
                        "youtube_ff_id": "",
                        "bunny_ff_link": "",
                        "bunny_ff_subtitles": "",
                        "youtube_prerecorded_link": "",
                        "youtube_prerecorded_id": "",
                        "bunny_prerecorded_link": "",
                        "bunny_prerecorded_subtitles": ""
                    },
                    {
                        "slot_id": "w-eduvis-1007",
                        "session_id": "w11",
                        "title": "Design Actions for the Design of Visualization Onboarding Methods",
                        "contributors": [
                            "Christina Stoiber"
                        ],
                        "authors": [
                            "Christina Stoiber",
                            "Margit Pohl",
                            "Wolfgang Aigner"
                        ],
                        "abstract": "Integrating visualization onboarding methods into visual analytics tools presents challenges for designers and developers. These challenges include the varying complexity of visualization techniques, data types, and users\u2019 expertise levels. Selecting and integrating educational theories, ensuring the completeness and clarity of onboarding instructions, choosing the appropriate medium, and determining interaction techniques for exploration during onboarding are also problematic. However, there needs to be established design guidance specifically focused on visualization onboarding. Existing resources like VisGuides and design patterns offer some qualitative advice but lack scientific foundations. To address this gap, we propose nine design actions based on critical reflections of empirical studies, the development of a design space, and user studies. These design actions emphasize customizing onboarding experiences to address users\u2019 knowledge gaps, finding a balance between flexibility and structure, and incorporating concrete examples while considering potential limitations in knowledge transfer. Furthermore, we explore the connections, issues, and contradictions within the design actions. In conclusion, it is crucial to tailor onboarding experiences, balance flexibility and structure, and provide concrete examples while acknowledging knowledge transfer challenges.",
                        "uid": "w-eduvis-1007",
                        "time_stamp": "2023-10-22T22:00:00Z",
                        "time_start": "2023-10-22T22:00:00Z",
                        "time_end": "2023-10-23T01:00:00Z",
                        "paper_type": "workshop",
                        "keywords": [],
                        "doi": "",
                        "fno": "",
                        "has_image": true,
                        "has_pdf": true,
                        "paper_award": "",
                        "image_caption": "{\\rtf1\\ansi\\ansicpg1252\\cocoartf2709 \\cocoatextscaling0\\cocoaplatform0{\\fonttbl\\f0\\fswiss\\fcharset0 ArialMT;\\f1\\froman\\fcharset0 Times-Roman;} {\\colortbl;\\red255\\green255\\blue255;\\red0\\green0\\blue0;} {\\*\\expandedcolortbl;;\\cssrgb\\c0\\c0\\c0;} \\paperw11900\\paperh16840\\margl1440\\margr1440\\vieww11520\\viewh8400\\viewkind0 \\deftab720 \\pard\\pardeftab720\\sa320\\partightenfactor0  \\f0\\fs29\\fsmilli14667 \\cf0 \\expnd0\\expndtw0\\kerning0 \\outl0\\strokewidth0 \\strokec2 Overview of our proposed design actions categorized according to the guiding questions of our design space \\uc0\\u8232 The methodology we employ in our development of the design actions: \\u8232 First, we derive design guidelines and implications from empirical studies in visualization onboarding. Additionally, we revisit our study results and collect the lessons learned. As a final step, we develop design actions along the framework by De Bruijn and Spence [19], including: (1) description and title, the (2) effect of the design action in the context of onboarding, advantages, and trade-offs ((3) upside and (4) downside), (5) issues describing the application of the design action, and (6) references to cognitive theory are provided. Furthermore, we structure the design actions along the guiding ques- tions from our design space [71]: WHO is the user, and which knowl- edge gap does the user have? Which parts of a visualization need to be explained? How to phrase onboarding instructions? HOW, WHERE, and WHEN is visualization onboarding provided? \\f1\\fs24 \\ \\pard\\pardeftab720\\partightenfactor0 \\cf0 \\ }",
                        "external_paper_link": "",
                        "youtube_ff_link": "",
                        "youtube_ff_id": "",
                        "bunny_ff_link": "",
                        "bunny_ff_subtitles": "",
                        "youtube_prerecorded_link": "",
                        "youtube_prerecorded_id": "",
                        "bunny_prerecorded_link": "",
                        "bunny_prerecorded_subtitles": ""
                    },
                    {
                        "slot_id": "w-eduvis-1008",
                        "session_id": "w11",
                        "title": "dAn-oNo Learning Environment for Data Journalists Teaching Data Analytics Principles",
                        "contributors": [
                            "Christina Stoiber"
                        ],
                        "authors": [
                            "Christina Stoiber",
                            "\u0160tefan Emrich",
                            "Sonja Radkohl",
                            "Eva Goldgruber",
                            "Wolfgang Aigner"
                        ],
                        "abstract": "To derive narratives from data, several journalistic abilities are required, including the skill to discover and construct compelling stories (data storytelling), employ data-driven techniques to research and analyze information (data literacy), utilize visualization methods effectively (visualization literacy), and approaching data with a combination of creativity and critical thinking. Despite their expertise in journalism, journalists often encounter challenges in comprehending and utilizing novel visual representations or understanding data analysis methods. The main objective of the dAn-oNo learning environment is to guide journalists through the data analytics process by removing coding hurdles and allowing them to experiment with and understand the code. The learning environment utilizes a Jupyter notebook with Markdown sections and incorporates a step-by-step approach, covering various stages of the data analytics workflow, such as data importing, inspection, statistical analysis, and in-depth analysis. The learning environment also includes an automated profiler that translates warnings and information into human-understandable insights. The design and implementation of the dAn-oNo learning environment were informed by a literature review, user research, interviews with Austrian data journalists, a phase of exploring different technical possibilities for the learning environment, and rapid prototyping. The prototype is accessible here: https://github.com/stemrich/SEVA\\_DA-Onboarding-Tool",
                        "uid": "w-eduvis-1008",
                        "time_stamp": "2023-10-22T22:00:00Z",
                        "time_start": "2023-10-22T22:00:00Z",
                        "time_end": "2023-10-23T01:00:00Z",
                        "paper_type": "workshop",
                        "keywords": [],
                        "doi": "",
                        "fno": "",
                        "has_image": true,
                        "has_pdf": true,
                        "paper_award": "",
                        "image_caption": "{\\rtf1\\ansi\\ansicpg1252\\cocoartf2709 \\cocoatextscaling0\\cocoaplatform0{\\fonttbl\\f0\\fswiss\\fcharset0 ArialMT;\\f1\\froman\\fcharset0 Times-Roman;} {\\colortbl;\\red255\\green255\\blue255;\\red0\\green0\\blue0;} {\\*\\expandedcolortbl;;\\cssrgb\\c0\\c0\\c0;} \\paperw11900\\paperh16840\\margl1440\\margr1440\\vieww11520\\viewh8400\\viewkind0 \\deftab720 \\pard\\pardeftab720\\sa320\\partightenfactor0  \\f0\\fs29\\fsmilli14667 \\cf0 \\expnd0\\expndtw0\\kerning0 \\outl0\\strokewidth0 \\strokec2 Automated data set analysis using the Python library pandas_profiling delivering extensive insight, often overwhelming for DA novices \\f1\\fs24 \\  \\f0\\fs29\\fsmilli14667 Design Process of the dAn-oNo learning environment: (1) selection of an easy-to-understand data set & exploration and validation of technical possibilities for the implementation to overcome the hurdle of coding, support understanding of pitfalls and challenges of data analytics, easy access, and step-wise instructions and feedback. We meet the needs of the journalists by utilizing Jupyter and Markdown, binder, and GitHub, the learning environment. (2) The second step was the development of the structure and content, including the following steps: importing data to the notebook, inspecting the data set, statistical analysis, and in-depth analysis. (3) The final step was implementing the dAn-oNo learning environment, including developing the core component of translating the warnings and information delivered by the automated profiler into human-understandable information. \\f1\\fs24 \\ }",
                        "external_paper_link": "",
                        "youtube_ff_link": "",
                        "youtube_ff_id": "",
                        "bunny_ff_link": "",
                        "bunny_ff_subtitles": "",
                        "youtube_prerecorded_link": "",
                        "youtube_prerecorded_id": "",
                        "bunny_prerecorded_link": "",
                        "bunny_prerecorded_subtitles": ""
                    },
                    {
                        "slot_id": "w-eduvis-1011",
                        "session_id": "w11",
                        "title": "Beyond Generating Code: Evaluating GPT on a Data Visualization Course",
                        "contributors": [
                            "Zhutian Chen"
                        ],
                        "authors": [
                            "Zhutian Chen",
                            "Chenyang Zhang",
                            "Qianwen Wang",
                            "Jakob Troidl",
                            "Simon Alexander Warchol",
                            "Johanna Beyer",
                            "Nils Gehlenborg",
                            "Hanspeter Pfister"
                        ],
                        "abstract": "This paper presents an empirical evaluation of the performance of the Generative Pre-trained Transformer (GPT) model in Harvard\u2019s CS171 data visualization course. While previous studies have focused on GPT\u2019s ability to generate code for visualizations, this study goes beyond code generation to evaluate GPT\u2019s abilities in various visualization tasks, such as data interpretation, visualization design, visual data exploration, and insight communication. The evaluation utilized GPT-3.5 and GPT-4 through the APIs of OpenAI to complete assignments of CS171, and included a quantitative assessment based on the established course rubrics, a qualitative analysis informed by the feedback of three experienced graders, and an exploratory study of GPT\u2019s capabilities in completing border visualization tasks. Findings show that GPT-4 scored 80% on quizzes and homework, and Teaching Fellows could distinguish between GPT- and human-generated homework with 70% accuracy. The study also demonstrates GPT\u2019s potential in completing various visualization tasks, such as data cleanup, interaction with visualizations, and insight communication. The paper concludes by discussing the strengths and limitations of GPT in data visualization, potential avenues for incorporating GPT in broader visualization tasks, and the need to redesign visualization education.",
                        "uid": "w-eduvis-1011",
                        "time_stamp": "2023-10-22T22:00:00Z",
                        "time_start": "2023-10-22T22:00:00Z",
                        "time_end": "2023-10-23T01:00:00Z",
                        "paper_type": "workshop",
                        "keywords": [],
                        "doi": "",
                        "fno": "",
                        "has_image": true,
                        "has_pdf": true,
                        "paper_award": "",
                        "image_caption": "Our experiments show that GPT can a) clean and explore CSV datasets, b) read visualizations in SVG format, interact with visualizations through dispatching Javascript events, and c) create explanatory visualizations to present data insights.",
                        "external_paper_link": "",
                        "youtube_ff_link": "",
                        "youtube_ff_id": "",
                        "bunny_ff_link": "",
                        "bunny_ff_subtitles": "",
                        "youtube_prerecorded_link": "",
                        "youtube_prerecorded_id": "",
                        "bunny_prerecorded_link": "",
                        "bunny_prerecorded_subtitles": ""
                    },
                    {
                        "slot_id": "w-eduvis-1014",
                        "session_id": "w11",
                        "title": "Preparing Future Data Visualization Designers for Professional Practice",
                        "contributors": [
                            "Dr Paul Parsons"
                        ],
                        "authors": [
                            "Paul Parsons"
                        ],
                        "abstract": "As the professional field of data visualization grows, so does the importance of preparing students effectively for the demands of real-world practice. Computing education has historically sought to teach and evaluate abstract knowledge (e.g., theories, principles, guidelines, design patterns) and the application of such knowledge to given problems. However, situations faced in professional practice are often messy, dynamic, and uncertain, and do not lend themselves well to the clear and direct application of such knowledge. This leaves a gap between the knowledge learned in the classroom and what is required for skillful practice in professional settings. In this paper, I discuss some historical reasons for this dominant pedagogical perspective, some of the core features of professional practice that are not typically taught in classrooms, and ways in which data visualization design can be taught to be more resonant with the experience of professional practice.",
                        "uid": "w-eduvis-1014",
                        "time_stamp": "2023-10-22T22:00:00Z",
                        "time_start": "2023-10-22T22:00:00Z",
                        "time_end": "2023-10-23T01:00:00Z",
                        "paper_type": "workshop",
                        "keywords": [],
                        "doi": "",
                        "fno": "",
                        "has_image": false,
                        "has_pdf": true,
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "youtube_ff_link": "",
                        "youtube_ff_id": "",
                        "bunny_ff_link": "",
                        "bunny_ff_subtitles": "",
                        "youtube_prerecorded_link": "",
                        "youtube_prerecorded_id": "",
                        "bunny_prerecorded_link": "",
                        "bunny_prerecorded_subtitles": ""
                    },
                    {
                        "slot_id": "w-eduvis-1018",
                        "session_id": "w11",
                        "title": "Reflections on Teaching \u2018Data Exploration and Visualisation\u2019 in Multiple Modes",
                        "contributors": [
                            "Michael Niemann"
                        ],
                        "authors": [
                            "Michael Niemann",
                            "Sarah Goodwin",
                            "Kim Marriott"
                        ],
                        "abstract": "Whilst the visual arts is a very practical and tangible field, data visualisation is a combination of programming skills, data science and design theory. Each of these can be taught in a variety of ways, from traditional methods of lectures to technical programming classes or even asynchronous worksheets. The challenge is how to cover all of the required content within the constraints of the teaching period, environment, delivery method and ensuring the appropriate activities and assessment tasks. In this paper we reflect on the past 8 years of delivering the postgraduate unit \u2018Data Exploration and Visualisation\u2019 at Monash University. We present the four different ways that the same content has been taught to different cohorts, through a combination of on-campus lectures, large-format workshops, small group tutorials and online or hybrid tutorials and workshops, as well as various readings, videos and asynchronous activities. We explain the different external and internal requirements set for the teaching and describe the adaption of the teaching methods and assessments in order to meet these conditions. We reflect on our experiences of teaching multiple methods across different formats from very small to very large student cohorts.",
                        "uid": "w-eduvis-1018",
                        "time_stamp": "2023-10-22T22:00:00Z",
                        "time_start": "2023-10-22T22:00:00Z",
                        "time_end": "2023-10-23T01:00:00Z",
                        "paper_type": "workshop",
                        "keywords": [],
                        "doi": "",
                        "fno": "",
                        "has_image": true,
                        "has_pdf": true,
                        "paper_award": "",
                        "image_caption": "Data visualisation teaching at Monash University in multiple modes, represented by an image of points plotted on a vertical map, overlaid with a capital M on the left and the right.",
                        "external_paper_link": "",
                        "youtube_ff_link": "",
                        "youtube_ff_id": "",
                        "bunny_ff_link": "https://ieeevis-uploads.b-cdn.net/vis23/w-eduvis/w-eduvis-1018/w-eduvis-1018_Preview.mp4?token=Gczo6ICSQgvXQ5JsGIQSWjwkzy2H15AuK9uoGHKIg6M&expires=1706590800",
                        "bunny_ff_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/w-eduvis/w-eduvis-1018/w-eduvis-1018_Preview.vtt?token=GfEjxZi0bz6t82ztjbHIqNNWi1D0E4Fu6136lG6n2WU&expires=1706590800",
                        "youtube_prerecorded_link": "",
                        "youtube_prerecorded_id": "",
                        "bunny_prerecorded_link": "",
                        "bunny_prerecorded_subtitles": ""
                    }
                ]
            }
        ]
    },
    "a-ldav": {
        "event": "LDAV: 13th IEEE Symposium on Large Data Analysis and Visualization",
        "long_name": "LDAV: 13th IEEE Symposium on Large Data Analysis and Visualization",
        "event_type": "associated",
        "event_prefix": "a-ldav",
        "event_description": "",
        "event_url": "",
        "organizers": [
            "Peer-Timo Bremer",
            "Kristi Potter",
            "Steffen Frey",
            "Silvio Rizzi",
            "Gunther Weber"
        ],
        "sessions": [
            {
                "title": "LDAV: 13th IEEE Symposium on Large Data Analysis and Visualization",
                "session_id": "ae2",
                "event_prefix": "a-ldav",
                "track": "oneohsix",
                "session_image": "ae2.png",
                "chair": [
                    "Peer-Timo Bremer",
                    "Kristi Potter",
                    "Steffen Frey",
                    "Silvio Rizzi",
                    "Gunther Weber"
                ],
                "time_start": "2023-10-22T22:00:00Z",
                "time_end": "2023-10-23T01:00:00Z",
                "discord_category": "",
                "discord_channel": "106",
                "discord_channel_id": "1161741373410644119",
                "discord_link": "https://discordapp.com/channels/1161400451619627068/1161741373410644119",
                "zoom_private_meeting": "",
                "zoom_private_password": "",
                "zoom_private_link": "",
                "zoom_broadcast_link": "",
                "ff_link": "",
                "time_slots": [
                    {
                        "slot_id": "a-ldav-1002",
                        "session_id": "ae2",
                        "title": "Speculative Progressive Raycasting for Memory Constrained Isosurface Visualization of Massive Volumes",
                        "contributors": [
                            "Landon Dyken"
                        ],
                        "authors": [
                            "Will Usher",
                            "Landon Dyken",
                            "sidharth kumar"
                        ],
                        "abstract": "New web technologies have enabled the deployment of powerful GPU-based computational pipelines that run entirely in the web browser, opening a new frontier for accessible scientific visualization applications. However, these new capabilities do not address the memory constraints of lightweight end-user devices encountered when attempting to visualize the massive data sets produced by today\u2019s simulations and data acquisition systems. In this paper, we propose a novel implicit isosurface rendering algorithm for interactive visualization of massive volumes within a small memory footprint. We achieve this by progressively traversing a wavefront of rays through the volume and decompressing blocks of the data on-demand to perform implicit ray-isosurface intersections. The progressively rendered surface is displayed after each pass to improve interactivity. Furthermore, to accelerate rendering and increase GPU utilization, we introduce speculative ray-block intersection into our algorithm, where additional blocks are traversed and intersected speculatively along rays as other rays terminate to exploit additional parallelism in the workload. Our entire pipeline is run in parallel on the GPU to leverage the parallel computing power that is available even on lightweight end-user devices. We compare our algorithm to the state of the art in low-overhead isosurface extraction and demonstrate that it achieves 1.7\u00d7\u20135.7\u00d7 reductions in memory overhead and up to 8.4\u00d7 reductions in data decompressed.",
                        "uid": "a-ldav-1002",
                        "time_stamp": "2023-10-22T22:00:00Z",
                        "time_start": "2023-10-22T22:00:00Z",
                        "time_end": "2023-10-23T01:00:00Z",
                        "paper_type": "associated",
                        "keywords": [],
                        "doi": "",
                        "fno": "",
                        "has_image": true,
                        "has_pdf": true,
                        "paper_award": "",
                        "image_caption": "Interactive full-resolution isosurface visualization of the 2048x2048x1920 Richtmyer-Meshkov (R-M) data set in the browser. We propose a new GPU algorithm for implicit isosurface rendering that progressively traverses rays through the volume and decompresses data on-demand to minimize its memory footprint. We achieve up to 5.7x reductions in overall memory use and 8.4\u00d7 reductions in data decompressed compared to the state of the art in memory constrained isosurface extraction, without sacrificing interactivity. At 1280x720, the Richtmyer-Meshkov averages 264ms per-pass and 1.2s total on an RTX 3080.",
                        "external_paper_link": "",
                        "youtube_ff_link": "",
                        "youtube_ff_id": "",
                        "bunny_ff_link": "",
                        "bunny_ff_subtitles": "",
                        "youtube_prerecorded_link": "",
                        "youtube_prerecorded_id": "",
                        "bunny_prerecorded_link": "",
                        "bunny_prerecorded_subtitles": ""
                    },
                    {
                        "slot_id": "a-ldav-1003",
                        "session_id": "ae2",
                        "title": "Low-Cost Post Hoc Reconstruction of HPC Simulations at Full Resolution",
                        "contributors": [
                            "Ayman Yousef"
                        ],
                        "authors": [
                            "Ayman Yousef",
                            "Amanda Randles",
                            "Erik Draeger"
                        ],
                        "abstract": "High performance computing has played a pivotal and ongoing role in the field of computational fluid dynamics, enabling the simulation of increasingly larger-scale models. However, this rapid growth in model and simulation size has outpaced the capabilities of input/output (I/O) operations. Consequently, the conventional approach of saving and outputting data to persistent storage for analysis has become increasingly challenging, limiting the benefits of these advanced models. To address this challenge, we present a method for effectively handling massive-scale simulation data, ensuring its persistence at full spatial and temporal resolution for flexible post hoc analysis. We employ an in situ approach that captures interprocess communicated data, compressing cached data to a fraction of the overall simulation domain. We successfully reconstruct subdomains at full spatial and temporal resolution during post-processing through communication-free rerun using the cached halos. We detail the storage requirements of the new approach and demonstrate the substantial reductions in computational resources required to precisely recapitulate data within a local region of interest.",
                        "uid": "a-ldav-1003",
                        "time_stamp": "2023-10-22T22:00:00Z",
                        "time_start": "2023-10-22T22:00:00Z",
                        "time_end": "2023-10-23T01:00:00Z",
                        "paper_type": "associated",
                        "keywords": [],
                        "doi": "",
                        "fno": "",
                        "has_image": true,
                        "has_pdf": true,
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "youtube_ff_link": "",
                        "youtube_ff_id": "",
                        "bunny_ff_link": "",
                        "bunny_ff_subtitles": "",
                        "youtube_prerecorded_link": "",
                        "youtube_prerecorded_id": "",
                        "bunny_prerecorded_link": "",
                        "bunny_prerecorded_subtitles": ""
                    },
                    {
                        "slot_id": "a-ldav-1013",
                        "session_id": "ae2",
                        "title": "Sub-Linear Time Sampling Approach for Large-Scale Data Visualization Using Reinforcement Learning",
                        "contributors": [
                            "Ayan Biswas"
                        ],
                        "authors": [
                            "Ayan Biswas",
                            "Arindam Bhattacharya",
                            "Yi-Tang Chen",
                            "Han-Wei Shen"
                        ],
                        "abstract": "As the compute capabilities of the modern supercomputers continue to rise, domain scientists are able to run their simulations at very fine spatial and temporal resolutions. Compared to the compute speeds, the I/O bandwidth continues to lag by orders of magnitude. This necessitates that the analysis and data reduction are performed in situ while the simulation generated data is still at the supercomputer memory. Recently, intelligent data-driven sampling schemes have been proposed by visualization researchers. These sampling methods are scalable, in situ capable and able to identify the feature regions of the data. Although powerful, these sampling schemes need to traverse through the data sets at least twice, which can become problematic as the simulations start to touch the exascale capabilities. In this paper, we propose to use a reinforcement learning-based approach to devise a sub-linear time sampling algorithm. Using multi-arm bandits, we show that we can identify samples that are of similar or better quality compared to the existing methods while only touching a small fraction of the original data. We use multiple simulation data sets to show the efficacy of our proposed method.",
                        "uid": "a-ldav-1013",
                        "time_stamp": "2023-10-22T22:00:00Z",
                        "time_start": "2023-10-22T22:00:00Z",
                        "time_end": "2023-10-23T01:00:00Z",
                        "paper_type": "associated",
                        "keywords": [],
                        "doi": "",
                        "fno": "",
                        "has_image": true,
                        "has_pdf": true,
                        "paper_award": "",
                        "image_caption": "We propose a reinforcement learning-based intelligent sampling scheme using multi-arm bandits. Using this sublinear sampling strategy, we touch less than 30% of the original data and produce samples of similar quality as the existing sampling schemes that need to iterate the original data two times.",
                        "external_paper_link": "",
                        "youtube_ff_link": "",
                        "youtube_ff_id": "",
                        "bunny_ff_link": "https://ieeevis-uploads.b-cdn.net/vis23/a-ldav/a-ldav-1013/a-ldav-1013_Preview.mp4?token=4h_VlQ7M850UVVS5kreyl-LrYrNyrNjiMojRODezE3I&expires=1706590800",
                        "bunny_ff_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/a-ldav/a-ldav-1013/a-ldav-1013_Preview.vtt?token=Se5ED8JaBlBWa5oDTfBKwiabOMdclA-gGFPV_FFT9TE&expires=1706590800",
                        "youtube_prerecorded_link": "",
                        "youtube_prerecorded_id": "",
                        "bunny_prerecorded_link": "",
                        "bunny_prerecorded_subtitles": ""
                    },
                    {
                        "slot_id": "a-ldav-1014",
                        "session_id": "ae2",
                        "title": "A Distributed-Memory Parallel Approach for Volume Rendering with Shadows",
                        "contributors": [
                            "Manish Mathai"
                        ],
                        "authors": [
                            "Manish Mathai",
                            "Matthew C Larsen",
                            "Hank Childs"
                        ],
                        "abstract": "We present a parallel, distributed-memory technique that enhances traditional ray-casting volume rendering of large data sets to highlight the depth and perception of interesting volumetric features. The technique introduces a lighting system that accounts for global shadows across distributed MPI nodes while using shared-memory parallelism within each node to compute shading information efficiently. The first stage of the approach involves estimating energy attenuation from a point light source through the global volume, using a reduced spatial resolution representation of the volume, with minimal global communication between nodes. It is then used in the second stage during volume rendering to shade sample points captured during ray-casting, generating a high-quality image. In this work, we study the technique's performance across varying spatial resolutions of the estimated light attenuation using synthetic and real-world volumetric data sets on distributed systems.",
                        "uid": "a-ldav-1014",
                        "time_stamp": "2023-10-22T22:00:00Z",
                        "time_start": "2023-10-22T22:00:00Z",
                        "time_end": "2023-10-23T01:00:00Z",
                        "paper_type": "associated",
                        "keywords": [],
                        "doi": "",
                        "fno": "",
                        "has_image": true,
                        "has_pdf": true,
                        "paper_award": "",
                        "image_caption": "Volume renderings of four data sets using shadows. From left to right, the data sets are Perlin Noise, Rayleigh-Taylor Instability, Richtmyer-Meshhkov Instability, and Rotating Stratified Turbulence.",
                        "external_paper_link": "",
                        "youtube_ff_link": "",
                        "youtube_ff_id": "",
                        "bunny_ff_link": "",
                        "bunny_ff_subtitles": "",
                        "youtube_prerecorded_link": "",
                        "youtube_prerecorded_id": "",
                        "bunny_prerecorded_link": "",
                        "bunny_prerecorded_subtitles": ""
                    },
                    {
                        "slot_id": "a-ldav-1015",
                        "session_id": "ae2",
                        "title": "Towards Adaptive Refinement for Multivariate Functional Approximation of Scientific Data",
                        "contributors": [
                            "Tom Peterka"
                        ],
                        "authors": [
                            "Tom Peterka",
                            "David Lenz",
                            "Iulian Grindeanu Iulian Grindeanu",
                            "Vijay S. Mahadevan"
                        ],
                        "abstract": "We investigate a data model for adaptive refinement in multivariate functional approximation of scientific data, based on a mesh of varying-resolution tensor products, and offering reduced size compared with a single tensor product representation. The mesh of tensor products adjoins in irregular fashion to tessellate the entire domain, with high-degree continuity across tensor product boundaries. The result is that regions of refinement, with additional knots and control points, are localized to individual tensor products rather than extending throughout the entire domain. The model attains similar accuracy with fewer total control points than a single tensor product model, at the cost of added computational complexity to manage the continuity and accuracy across tensor products. We describe our high-dimensional data organization and demonstrate how to approximate scientific datasets using our data model. Size and speed are compared between a single tensor product and our representation. Initial results demonstrate correct functionality and modest reduction in the number of control points required to attain comparable accuracy as a single tensor, with increased computation time as expected. Our initial findings indicate two avenues for future research: additional tuning of the adaptive refinement algorithm to reduce size further, and accelerating computation through parallelism.",
                        "uid": "a-ldav-1015",
                        "time_stamp": "2023-10-22T22:00:00Z",
                        "time_start": "2023-10-22T22:00:00Z",
                        "time_end": "2023-10-23T01:00:00Z",
                        "paper_type": "associated",
                        "keywords": [],
                        "doi": "",
                        "fno": "",
                        "has_image": true,
                        "has_pdf": true,
                        "paper_award": "",
                        "image_caption": "Left: adding one control point to a single tensor product generates additional redundant control points in order to maintain the definition of a tensor product. Right: in this paper we propose a mesh of different resolution tensor products that limits the number of extra control points required.",
                        "external_paper_link": "",
                        "youtube_ff_link": "",
                        "youtube_ff_id": "",
                        "bunny_ff_link": "",
                        "bunny_ff_subtitles": "",
                        "youtube_prerecorded_link": "",
                        "youtube_prerecorded_id": "",
                        "bunny_prerecorded_link": "",
                        "bunny_prerecorded_subtitles": ""
                    }
                ]
            }
        ]
    },
    "a-biomedchallenge": {
        "event": "Bio+MedVis Challenges",
        "long_name": "Bio+MedVis Challenges",
        "event_type": "associated",
        "event_prefix": "a-biomedchallenge",
        "event_description": "",
        "event_url": "",
        "organizers": [
            "Barbora Kozlikova",
            "Daniel J\u00f6nsson",
            "Renata Raidou",
            "Sean O\u2019Donoghue"
        ],
        "sessions": [
            {
                "title": "Bio+MedVis Challenges",
                "session_id": "c1",
                "event_prefix": "a-biomedchallenge",
                "track": "oneeleven",
                "session_image": "c1.png",
                "chair": [
                    "Barbora Kozlikova",
                    "Daniel J\u00f6nsson",
                    "Renata Raidou",
                    "Sean O\u2019Donoghue"
                ],
                "time_start": "2023-10-22T22:00:00Z",
                "time_end": "2023-10-23T01:00:00Z",
                "discord_category": "",
                "discord_channel": "111-112",
                "discord_channel_id": "1161741606651711498",
                "discord_link": "https://discordapp.com/channels/1161400451619627068/1161741606651711498",
                "zoom_private_meeting": "",
                "zoom_private_password": "",
                "zoom_private_link": "",
                "zoom_broadcast_link": "",
                "ff_link": "",
                "time_slots": [
                    {
                        "slot_id": "a-biomedchallenge-7741",
                        "session_id": "c1",
                        "title": "CytoCave: An Interactive Visualization Tool for Exploring Protein and Drug Interaction Networks",
                        "contributors": [
                            "Morris Chukhman"
                        ],
                        "authors": [],
                        "abstract": "CytoCave is an immersive interactive online visualization platform based on the NeuroCave connectome visualization application, but modified and optimized for general -omics network visual- ization. CytoCave leverages existing features of NeuroCave, in- cluding the side-by-side viewports designed for freeform explo- ration of node-link diagrams under various transformations or co- ordinate system embeddings, as well its immersive capabili- ties. However, CytoCave introduces a range of new features that facilitate the exploration of multi-omics networks using di- mensionality reduction projections of high-dimensional biological data with cluster membership highlighting, and includes scalabil- ity upgrades to support visualizing and interacting with large scale proteomics and multi-omics datasets. The CytoCave project source code, which includes the web application JavaScript and the data processing pipeline Jupyter notebook, is available on GitHub at https://github.com/iMammal/CytoCave. In this paper, we describe how to use CytoCave to facilitate the following proteomics and drug discovery tasks. CytoCave enables users: (Task 1) to select a protein and analyze drugs that target them, or, similarly, (Task 2) to select a drug and visualize the proteins that are strongly associated with the drug response. Additionally, CytoCave re-designs the traditional 2D force-directed layout visu- alization of protein interaction networks, providing an interactive interface that allows users to pan, rotate, and zoom in and out of the network, and to switch layouts and color coding templates from a variety of pre-calculated options, with optional pop-up annotations on mouse hovers and edge display / edge hiding on mouse clicks.",
                        "uid": "a-biomedchallenge-7741",
                        "time_stamp": "2023-10-22T22:00:00Z",
                        "time_start": "2023-10-22T22:00:00Z",
                        "time_end": "2023-10-23T01:00:00Z",
                        "paper_type": "associated",
                        "keywords": [],
                        "doi": "",
                        "fno": "",
                        "has_image": false,
                        "has_pdf": true,
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "youtube_ff_link": "",
                        "youtube_ff_id": "",
                        "bunny_ff_link": "",
                        "bunny_ff_subtitles": "",
                        "youtube_prerecorded_link": "",
                        "youtube_prerecorded_id": "",
                        "bunny_prerecorded_link": "",
                        "bunny_prerecorded_subtitles": ""
                    },
                    {
                        "slot_id": "a-biomedchallenge-5794",
                        "session_id": "c1",
                        "title": "PepProEx - Peptide and Protein Exploration Framework",
                        "contributors": [
                            "Vikash Prasad"
                        ],
                        "authors": [
                            "Vikash Prasad",
                            "Ji Hwan Park"
                        ],
                        "abstract": "We present a visual analytics framework, PepProEx, for unraveling peptide and protein interactions. In the framework, users can understand holistic trends, discover patterns, and get insights from the peptide/protein intensity data. Our framework provides multiple linked views to navigate intricate tissue/cancer-related dynamics. We enable researchers to shift effortlessly from an overview of the input data to granular statistical insights.",
                        "uid": "a-biomedchallenge-5794",
                        "time_stamp": "2023-10-22T22:00:00Z",
                        "time_start": "2023-10-22T22:00:00Z",
                        "time_end": "2023-10-23T01:00:00Z",
                        "paper_type": "associated",
                        "keywords": [],
                        "doi": "",
                        "fno": "",
                        "has_image": false,
                        "has_pdf": true,
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "youtube_ff_link": "",
                        "youtube_ff_id": "",
                        "bunny_ff_link": "",
                        "bunny_ff_subtitles": "",
                        "youtube_prerecorded_link": "",
                        "youtube_prerecorded_id": "",
                        "bunny_prerecorded_link": "",
                        "bunny_prerecorded_subtitles": ""
                    },
                    {
                        "slot_id": "a-biomedchallenge-3267",
                        "session_id": "c1",
                        "title": "ProtEGOnist \u2013 Exploration of protein-protein interactions using ego-graph networks",
                        "contributors": [
                            "Michael Krone"
                        ],
                        "authors": [
                            "Nicolas Brich",
                            "Theresa Harbig",
                            "Mathias Witte Paz",
                            "Simon Tim Hackl",
                            "Caroline Jachmann",
                            "Marco Sch\u00e4fer",
                            "Michael Krone",
                            "Kay Nieselt"
                        ],
                        "abstract": "The complexity of protein-protein interaction (PPI) networks often leads to visual clutter and limited interpretability. To overcome these problems, we present ProtEGOnist, a novel, interactive visualization approach designed to explore PPI networks with a focus on drug-protein associations. ProtEGOnist addresses the challenges by introducing the concept of ego-graphs to represent local PPI neighborhoods around proteins of interest. These ego-graphs are aggregated into an ego-graph network, where edges between ego-graphs encoded their similarity using the Jaccard index. Our proposed visualization design offers an overview of drug-associated proteins, radar charts to compare protein functions, and detailed ego-graph subnetworks for interactive exploration. Our aim was to reduce visual complexity while enabling detailed exploration, facilitating the discovery of meaningful patterns in PPI networks. A web-based prototype of ProtEGOnist is available for interactive use.",
                        "uid": "a-biomedchallenge-3267",
                        "time_stamp": "2023-10-22T22:00:00Z",
                        "time_start": "2023-10-22T22:00:00Z",
                        "time_end": "2023-10-23T01:00:00Z",
                        "paper_type": "associated",
                        "keywords": [],
                        "doi": "",
                        "fno": "",
                        "has_image": true,
                        "has_pdf": true,
                        "paper_award": "",
                        "image_caption": "ProtEGOnist focuses on comparing the local protein-protein interaction neighborhoods - called ego-graphs - of proteins of interest. It consists of three views: (1) a network of ego-graphs of the proteins of interest, (2) an ego-graph subnetwork of ego-graphs selected from the overview that provides more details on the ego-graphs and their overlap when decollapsing them, and (3) a radar chart showing the similarity between a selected ego-graph and its neighbor ego-graphs, which are grouped by the function of the central protein using the BRITE hierarchy. All views are linked and provide details on demand.",
                        "external_paper_link": "",
                        "youtube_ff_link": "",
                        "youtube_ff_id": "",
                        "bunny_ff_link": "https://ieeevis-uploads.b-cdn.net/vis23/a-biomedchallenge/a-biomedchallenge-3267/a-biomedchallenge-3267_Preview.mp4?token=ZSnuEdx1fz3wLOvQsy_cE1oPb6NyiX1QONULIxAhUTc&expires=1706590800",
                        "bunny_ff_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/a-biomedchallenge/a-biomedchallenge-3267/a-biomedchallenge-3267_Preview.vtt?token=8Co8-3sBYQAvmDTGgl5yjxJEXgv_iUw6ZziZOrlyd8Q&expires=1706590800",
                        "youtube_prerecorded_link": "",
                        "youtube_prerecorded_id": "",
                        "bunny_prerecorded_link": "",
                        "bunny_prerecorded_subtitles": ""
                    }
                ]
            }
        ]
    },
    "a-scivis-contest": {
        "event": "SciVis Contest",
        "long_name": "SciVis Contest",
        "event_type": "associated",
        "event_prefix": "a-scivis-contest",
        "event_description": "",
        "event_url": "",
        "organizers": [
            "Tim Gerrits",
            "Divya Banesh"
        ],
        "sessions": [
            {
                "title": "SciVis Contest",
                "session_id": "c3",
                "event_prefix": "a-scivis-contest",
                "track": "oneeleven",
                "session_image": "c3.png",
                "chair": [
                    "Tim Gerrits",
                    "Divya Banesh"
                ],
                "time_start": "2023-10-23T03:00:00Z",
                "time_end": "2023-10-23T06:00:00Z",
                "discord_category": "",
                "discord_channel": "111-112",
                "discord_channel_id": "1161741606651711498",
                "discord_link": "https://discordapp.com/channels/1161400451619627068/1161741606651711498",
                "zoom_private_meeting": "",
                "zoom_private_password": "",
                "zoom_private_link": "",
                "zoom_broadcast_link": "",
                "ff_link": "",
                "time_slots": [
                    {
                        "slot_id": "a-scivis-contest-1001",
                        "session_id": "c3",
                        "title": "Immersive Exploration of Brain Simulation Data",
                        "contributors": [
                            "Hogr\u00e4fer, Marius"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "a-scivis-contest-1001",
                        "time_stamp": "2023-10-23T03:00:00Z",
                        "time_start": "2023-10-23T03:00:00Z",
                        "time_end": "2023-10-23T06:00:00Z",
                        "paper_type": "associated",
                        "keywords": [],
                        "doi": "",
                        "fno": "",
                        "has_image": false,
                        "has_pdf": true,
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "youtube_ff_link": "",
                        "youtube_ff_id": "",
                        "bunny_ff_link": "",
                        "bunny_ff_subtitles": "",
                        "youtube_prerecorded_link": "",
                        "youtube_prerecorded_id": "",
                        "bunny_prerecorded_link": "",
                        "bunny_prerecorded_subtitles": ""
                    },
                    {
                        "slot_id": "a-scivis-contest-1005",
                        "session_id": "c3",
                        "title": "PlastiVis: An Interactive Visualization Tool for Synaptic Networks",
                        "contributors": [
                            "Dang, Tommy"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "a-scivis-contest-1005",
                        "time_stamp": "2023-10-23T03:00:00Z",
                        "time_start": "2023-10-23T03:00:00Z",
                        "time_end": "2023-10-23T06:00:00Z",
                        "paper_type": "associated",
                        "keywords": [],
                        "doi": "",
                        "fno": "",
                        "has_image": false,
                        "has_pdf": true,
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "youtube_ff_link": "",
                        "youtube_ff_id": "",
                        "bunny_ff_link": "",
                        "bunny_ff_subtitles": "",
                        "youtube_prerecorded_link": "",
                        "youtube_prerecorded_id": "",
                        "bunny_prerecorded_link": "",
                        "bunny_prerecorded_subtitles": ""
                    },
                    {
                        "slot_id": "a-scivis-contest-1007",
                        "session_id": "c3",
                        "title": "NeuroViz: Visual Analytics of Neural Behavior in Temporal Plasticity Changes",
                        "contributors": [
                            "Han, Xiaoyang"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "a-scivis-contest-1007",
                        "time_stamp": "2023-10-23T03:00:00Z",
                        "time_start": "2023-10-23T03:00:00Z",
                        "time_end": "2023-10-23T06:00:00Z",
                        "paper_type": "associated",
                        "keywords": [],
                        "doi": "",
                        "fno": "",
                        "has_image": false,
                        "has_pdf": true,
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "youtube_ff_link": "",
                        "youtube_ff_id": "",
                        "bunny_ff_link": "",
                        "bunny_ff_subtitles": "",
                        "youtube_prerecorded_link": "",
                        "youtube_prerecorded_id": "",
                        "bunny_prerecorded_link": "",
                        "bunny_prerecorded_subtitles": ""
                    },
                    {
                        "slot_id": "a-scivis-contest-1010",
                        "session_id": "c3",
                        "title": "An Interactive Visualization for Neuronal Network Simulations of Plasticity Changes in the Human Brain",
                        "contributors": [
                            "Lawonn, Kai"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "a-scivis-contest-1010",
                        "time_stamp": "2023-10-23T03:00:00Z",
                        "time_start": "2023-10-23T03:00:00Z",
                        "time_end": "2023-10-23T06:00:00Z",
                        "paper_type": "associated",
                        "keywords": [],
                        "doi": "",
                        "fno": "",
                        "has_image": true,
                        "has_pdf": true,
                        "paper_award": "",
                        "image_caption": "A visualization of neuron activity. On the left we can see can see in yellow the connections from and to a selected neuron. The colored spots from dark blue to light yellow indicate the electrical potential at each neuron from -30 to 70 mV. We can observe a wave of neurons firing propagating from left to right. This is a capture from our interactive real-time application.",
                        "external_paper_link": "",
                        "youtube_ff_link": "",
                        "youtube_ff_id": "",
                        "bunny_ff_link": "https://ieeevis-uploads.b-cdn.net/vis23/a-scivis-contest/a-scivis-contest-1010/a-scivis-contest-1010_Preview.mp4?token=FKFkrRcSxSFb3qLJiteffdsFOBIqaooYq6uq0NkCHQE&expires=1706590800",
                        "bunny_ff_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/a-scivis-contest/a-scivis-contest-1010/a-scivis-contest-1010_Preview.vtt?token=cxaMLVrifajIvZBhWWD1hoS3sLu81A_wPcsp1YUDCGw&expires=1706590800",
                        "youtube_prerecorded_link": "https://youtu.be/3sWPkbodmmA",
                        "youtube_prerecorded_id": "3sWPkbodmmA",
                        "bunny_prerecorded_link": "https://ieeevis-uploads.b-cdn.net/vis23/a-scivis-contest/a-scivis-contest-1010/a-scivis-contest-1010_Presentation.mp4?token=L096JIr3j1N8t7zKGy3QrgtxAg2Q224j1LUYCFOC8mk&expires=1706590800",
                        "bunny_prerecorded_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/a-scivis-contest/a-scivis-contest-1010/a-scivis-contest-1010_Presentation.vtt?token=PcZphUTgX2n7ClLxb1fuq84DfLD0uFXwyxs4rUqtsbo&expires=1706590800"
                    },
                    {
                        "slot_id": "a-scivis-contest-1011",
                        "session_id": "c3",
                        "title": "Exploring Synaptic Plasticity with NeuroCavePlus",
                        "contributors": [
                            "Chukhman, Morris"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "a-scivis-contest-1011",
                        "time_stamp": "2023-10-23T03:00:00Z",
                        "time_start": "2023-10-23T03:00:00Z",
                        "time_end": "2023-10-23T06:00:00Z",
                        "paper_type": "associated",
                        "keywords": [],
                        "doi": "",
                        "fno": "",
                        "has_image": true,
                        "has_pdf": true,
                        "paper_award": "",
                        "image_caption": "Explore synaptic plasticity with NeuroCavePlus side-by-side views can be used to compare sequential or distant simulation steps coordinate system and color codes can be set to track the evolution of engrams to achieve new steady state.",
                        "external_paper_link": "",
                        "youtube_ff_link": "",
                        "youtube_ff_id": "",
                        "bunny_ff_link": "https://ieeevis-uploads.b-cdn.net/vis23/a-scivis-contest/a-scivis-contest-1011/a-scivis-contest-1011_Preview.mp4?token=GotMb8o8c8vUptDfDYaKDt7o6Hom-5HGF87b7IZkUM8&expires=1706590800",
                        "bunny_ff_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/a-scivis-contest/a-scivis-contest-1011/a-scivis-contest-1011_Preview.vtt?token=V0kPR88c4y_Xuy-_d4uCOeFAK3g4OI3E_tkiI6mQcZk&expires=1706590800",
                        "youtube_prerecorded_link": "",
                        "youtube_prerecorded_id": "",
                        "bunny_prerecorded_link": "https://ieeevis-uploads.b-cdn.net/vis23/a-scivis-contest/a-scivis-contest-1011/a-scivis-contest-1011_Presentation.mp4?token=c9RnP2B8wtTdC-kon0rIcspTCjY2zW3TUCBjOKnoE4w&expires=1706590800",
                        "bunny_prerecorded_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/a-scivis-contest/a-scivis-contest-1011/a-scivis-contest-1011_Presentation.vtt?token=TZJ24d-EMC1lmcPqift2n0_YDNS8zyaUS7KrPbEnFeg&expires=1706590800"
                    },
                    {
                        "slot_id": "a-scivis-contest-1012",
                        "session_id": "c3",
                        "title": "VisAnywhere: Developing Multi-platform Scientific Visualization Applications",
                        "contributors": [
                            "Marrinan, Thomas"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "a-scivis-contest-1012",
                        "time_stamp": "2023-10-23T03:00:00Z",
                        "time_start": "2023-10-23T03:00:00Z",
                        "time_end": "2023-10-23T06:00:00Z",
                        "paper_type": "associated",
                        "keywords": [],
                        "doi": "",
                        "fno": "",
                        "has_image": true,
                        "has_pdf": true,
                        "paper_award": "",
                        "image_caption": "Overview of the brain plasticity 3D visualization application. The application shows neurons (spheres) and connections (tubes), and it  contains a user interface for controlling the application.",
                        "external_paper_link": "",
                        "youtube_ff_link": "",
                        "youtube_ff_id": "",
                        "bunny_ff_link": "https://ieeevis-uploads.b-cdn.net/vis23/a-scivis-contest/a-scivis-contest-1012/a-scivis-contest-1012_Preview.mp4?token=2Yn6zH9vIyRcpmpuGdu0YIGTS50ARuCA_ukWjkF4L3k&expires=1706590800",
                        "bunny_ff_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/a-scivis-contest/a-scivis-contest-1012/a-scivis-contest-1012_Preview.vtt?token=sgMAmdzZoYWCeXFZ18uPklq2ABNWwjaGQ5J3Z1JnARo&expires=1706590800",
                        "youtube_prerecorded_link": "",
                        "youtube_prerecorded_id": "",
                        "bunny_prerecorded_link": "",
                        "bunny_prerecorded_subtitles": ""
                    }
                ]
            }
        ]
    },
    "t-analysis": {
        "event": "Visualization Analysis and Design",
        "long_name": "Visualization Analysis and Design",
        "event_type": "tutorial",
        "event_prefix": "t-analysis",
        "event_description": "",
        "event_url": "",
        "organizers": [
            "Tamara Munzner"
        ],
        "sessions": [
            {
                "title": "Visualization Analysis and Design",
                "session_id": "t2",
                "event_prefix": "t-analysis",
                "track": "oneohone",
                "session_image": "t2.png",
                "chair": [
                    "Tamara Munzner"
                ],
                "time_start": "2023-10-21T22:00:00Z",
                "time_end": "2023-10-22T01:00:00Z",
                "discord_category": "",
                "discord_channel": "101-102",
                "discord_channel_id": "1161740777530069092",
                "discord_link": "https://discordapp.com/channels/1161400451619627068/1161740777530069092",
                "zoom_private_meeting": "",
                "zoom_private_password": "",
                "zoom_private_link": "",
                "zoom_broadcast_link": "",
                "ff_link": "",
                "time_slots": []
            }
        ]
    },
    "t-ttk": {
        "event": "A Hands-on TTK Tutorial for Absolute Beginners",
        "long_name": "A Hands-on TTK Tutorial for Absolute Beginners",
        "event_type": "tutorial",
        "event_prefix": "t-ttk",
        "event_description": "",
        "event_url": "",
        "organizers": [
            "Julien Tierny"
        ],
        "sessions": [
            {
                "title": "A Hands-on TTK Tutorial for Absolute Beginners",
                "session_id": "t5",
                "event_prefix": "t-ttk",
                "track": "oneohsix",
                "session_image": "t5.png",
                "chair": [
                    "Julien Tierny"
                ],
                "time_start": "2023-10-21T22:00:00Z",
                "time_end": "2023-10-22T01:00:00Z",
                "discord_category": "",
                "discord_channel": "106",
                "discord_channel_id": "1161741373410644119",
                "discord_link": "https://discordapp.com/channels/1161400451619627068/1161741373410644119",
                "zoom_private_meeting": "",
                "zoom_private_password": "",
                "zoom_private_link": "",
                "zoom_broadcast_link": "",
                "ff_link": "",
                "time_slots": []
            }
        ]
    },
    "w-nlviz": {
        "event": "NLVIZ Workshop: Exploring Research Opportunities for Natural Language, Text, and Data Visualization",
        "long_name": "NLVIZ Workshop: Exploring Research Opportunities for Natural Language, Text, and Data Visualization",
        "event_type": "workshop",
        "event_prefix": "w-nlviz",
        "event_description": "",
        "event_url": "",
        "organizers": [
            "Vidya Setlur",
            "Arjun Srinivasan"
        ],
        "sessions": [
            {
                "title": "NLVIZ Workshop: Exploring Research Opportunities for Natural Language, Text, and Data Visualization",
                "session_id": "w5",
                "event_prefix": "w-nlviz",
                "track": "oneten",
                "session_image": "w5.png",
                "chair": [
                    "Vidya Setlur",
                    "Arjun Srinivasan"
                ],
                "time_start": "2023-10-22T03:00:00Z",
                "time_end": "2023-10-22T06:00:00Z",
                "discord_category": "",
                "discord_channel": "110",
                "discord_channel_id": "1161741529434566686",
                "discord_link": "https://discordapp.com/channels/1161400451619627068/1161741529434566686",
                "zoom_private_meeting": "",
                "zoom_private_password": "",
                "zoom_private_link": "",
                "zoom_broadcast_link": "",
                "ff_link": "",
                "time_slots": [
                    {
                        "slot_id": "w-nlviz-1011",
                        "session_id": "w5",
                        "title": "WEC-Explainer: A Descriptive Framework for Exploring Word Embedding Contextualization",
                        "contributors": [
                            "Mennatallah El-Assady"
                        ],
                        "authors": [
                            "Rita Sevastjanova",
                            "Mennatallah El-Assady"
                        ],
                        "abstract": "Contextual word embeddings -- high-dimensional vectors that encode the semantic similarity between words -- are proven to be effective for diverse natural language processing applications. These embeddings originate in large language models and are updated throughout the model's architecture (i.e., the model's layers). Given their intricacy, the explanation of embedding characteristics and limitations -- their contextualization -- has emerged as a widely investigated research subject. To provide an overview of the existing explanation methods and motivate researchers to design new approaches, we present a descriptive framework that connects data, features, tasks, and users involved in the word embedding explanation process. We use the framework as theoretical groundwork and implement a data processing pipeline that we use to solve three different tasks related to word embedding contextualization. These tasks enable answering questions about the encoded context properties in the embedding vectors, captured semantic concepts and their similarity, and masked-prediction meaningfulness and their relation to embedding characteristics. We show that divergent research questions can be analyzed by combining different data curation methods with a similar set of features.",
                        "uid": "w-nlviz-1011",
                        "time_stamp": "2023-10-22T03:00:00Z",
                        "time_start": "2023-10-22T03:00:00Z",
                        "time_end": "2023-10-22T06:00:00Z",
                        "paper_type": "workshop",
                        "keywords": [],
                        "doi": "",
                        "fno": "",
                        "has_image": true,
                        "has_pdf": true,
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "youtube_ff_link": "",
                        "youtube_ff_id": "",
                        "bunny_ff_link": "https://ieeevis-uploads.b-cdn.net/vis23/w-nlviz/w-nlviz-1011/w-nlviz-1011_Preview.mp4?token=kbrRLs_za4QVpZs3p9gvlqrs-gpP108OGaSzAllUbG4&expires=1706590800",
                        "bunny_ff_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/w-nlviz/w-nlviz-1011/w-nlviz-1011_Preview.vtt?token=ef9fk4i22sg9mqZx9IH7JlApnw7R6w-eNcR058YgBw0&expires=1706590800",
                        "youtube_prerecorded_link": "",
                        "youtube_prerecorded_id": "",
                        "bunny_prerecorded_link": "",
                        "bunny_prerecorded_subtitles": ""
                    },
                    {
                        "slot_id": "w-nlviz-1014",
                        "session_id": "w5",
                        "title": "From Natural Language to Data Visualization (NL2VIS) with Large Language Model and Pattern Matching",
                        "contributors": [
                            "Dr. Zana Vosough"
                        ],
                        "authors": [
                            "Zana Vosough",
                            "Sameer Merchant",
                            "Rajesh Bhagwat"
                        ],
                        "abstract": "In the rapidly evolving domain of artificial intelligence, Large Language Models (LLMs) such as ChatGPT have made remarkable advancements, revolutionizing how users consume and generate content. The current research on LLM-based generative tools has primarily been concentrated on the generation of text, codes, or images. This paper presents an investigation into the application of LLMs for automating the creation of data visualizations from natural language queries. The proposed approach leverages LLMs for generating data queries, coupling this capability with a pattern matching strategy aimed at creating visualizations ( eventually auto-generated dashboards) based on the extracted data. Furthermore, we introduce a novel domain-specific data visualization language (DVL). This DVL can be effortlessly translated into executable code compatible with various data visualization libraries. This exploration into the confluence of visualization, NLP, and human-computer interaction, is substantiated by our work in a practical industrial context, enriching a live product. Our findings aspire to contribute to the ongoing dialogue of how NLP techniques and interactive visualizations can be harmonized to bolster data-driven communication and analytical discourse.",
                        "uid": "w-nlviz-1014",
                        "time_stamp": "2023-10-22T03:00:00Z",
                        "time_start": "2023-10-22T03:00:00Z",
                        "time_end": "2023-10-22T06:00:00Z",
                        "paper_type": "workshop",
                        "keywords": [],
                        "doi": "",
                        "fno": "",
                        "has_image": false,
                        "has_pdf": true,
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "youtube_ff_link": "",
                        "youtube_ff_id": "",
                        "bunny_ff_link": "",
                        "bunny_ff_subtitles": "",
                        "youtube_prerecorded_link": "",
                        "youtube_prerecorded_id": "",
                        "bunny_prerecorded_link": "",
                        "bunny_prerecorded_subtitles": ""
                    },
                    {
                        "slot_id": "w-nlviz-1010",
                        "session_id": "w5",
                        "title": "Unveiling Insights: Surfacing Fine-Grained Discourse Acts in Short Free-Form Public Input Text through Visual Analytics",
                        "contributors": [
                            "Narges Mahyar"
                        ],
                        "authors": [
                            "Mahmood Jasim",
                            "Mohit Iyyer",
                            "Narges Mahyar"
                        ],
                        "abstract": "Exploratory analysis of public-generated short free-form online text often plays a critical role in facilitating decision-making across various domains. Particularly, in the civic domain, analyzing and understanding the thoughts, opinions, and comments shared by the public on social media or online engagement platforms on various civic issues are crucial for tracking consensus and making informed policy decisions. However, the public inputs are often short, redundant, unstructured, and full of nuances and ambiguity with a lack of clear boundary between positive and negative stances, which demands significant time and effort to analyze. Coupled with a lack of guidelines to visualize short free-form text, designing visualizations to show meaningful insights from public input while preserving the context and semantic values remains a challenging task. In this work, we explored discourse acts as a fine-grained categorization to characterize public input beyond positive or negative stances. Furthermore, we applied McDonald's greedy approximation of global inference to prioritize the relevance and negate the redundancy present in the text. We integrated these approaches with an interactive prototype called Matryona that employs visualization techniques to provide a contextual summary of unstructured public input and enables multilayered exploration from different angles by controlling the information content. Our initial evaluation of the prototype suggests Matryona's potential for reducing ambiguity and eliciting nuances present in the short free-form text by using discourse acts and accelerating the analysis process by ranking text comments.",
                        "uid": "w-nlviz-1010",
                        "time_stamp": "2023-10-22T03:00:00Z",
                        "time_start": "2023-10-22T03:00:00Z",
                        "time_end": "2023-10-22T06:00:00Z",
                        "paper_type": "workshop",
                        "keywords": [],
                        "doi": "",
                        "fno": "",
                        "has_image": true,
                        "has_pdf": true,
                        "paper_award": "",
                        "image_caption": "Matryona's detail view provides a dropdown to select discussions. The bar on the left represents the topic percentage contribution of the extracted topic towards the discussion. The detail view also present the topic key phrases and keywords. Next, there is a set of circles representing the discourse act distribution for each topic. There is a slider bar to control the number of ranked text comments to display. Finally, the comments associated with the filters are shown.",
                        "external_paper_link": "",
                        "youtube_ff_link": "",
                        "youtube_ff_id": "",
                        "bunny_ff_link": "",
                        "bunny_ff_subtitles": "",
                        "youtube_prerecorded_link": "",
                        "youtube_prerecorded_id": "",
                        "bunny_prerecorded_link": "",
                        "bunny_prerecorded_subtitles": ""
                    },
                    {
                        "slot_id": "w-nlviz-1018",
                        "session_id": "w5",
                        "title": "An Explainable AI Approach to Large Language Model Assisted Causal Model Auditing and Development",
                        "contributors": [
                            "Klaus Mueller"
                        ],
                        "authors": [
                            "Yanming Zhang",
                            "Brette Fitzgibbon",
                            "Dino Garofolo",
                            "Akshith Kota",
                            "Eric Papenhausen",
                            "Klaus Mueller"
                        ],
                        "abstract": "Causal networks are widely used in many fields, including epidemiology, social science, medicine, and engineering, to model the complex relationships between variables. While it can be convenient to algorithmically infer these models directly from observational data, the resulting networks are often plagued with erroneous edges. Auditing and correcting these networks may require domain expertise frequently unavailable to the analyst. We propose the use of large language models such as ChatGPT as an auditor for causal networks. Our method presents ChatGPT witha causal network, one edge at a time, to produce insights about edge directionality, possible confounders, and mediating variables. We ask ChatGPT to reflect on various aspects of each causal link and we then produce visualizations that summarize these viewpoints for the human analyst to direct the edge, gather more data, or test further hypotheses. We envision a system where large language models, automated causal inference, and the human analyst and domain expert work hand in hand as a team to derive holistic and comprehensive causal models for any given case scenario. This paper presents first results obtained with an emerging prototype.",
                        "uid": "w-nlviz-1018",
                        "time_stamp": "2023-10-22T03:00:00Z",
                        "time_start": "2023-10-22T03:00:00Z",
                        "time_end": "2023-10-22T06:00:00Z",
                        "paper_type": "workshop",
                        "keywords": [],
                        "doi": "",
                        "fno": "",
                        "has_image": true,
                        "has_pdf": true,
                        "paper_award": "",
                        "image_caption": "ChatGPT has rich knowledge about the real world. We propose to use it as an auditor for causal networks. Our method presents ChatGPT with a causal network and prompts it for insights about edge directionality, confounders, and mediating variables, resulting in a more accurate and detailed causal model. To summarize the large text produced by ChatGPT we designed several visualizations: The Causal Debate Chart, the Causal Environment Chart, and the Confounder/Mediator Chart.",
                        "external_paper_link": "",
                        "youtube_ff_link": "",
                        "youtube_ff_id": "",
                        "bunny_ff_link": "https://ieeevis-uploads.b-cdn.net/vis23/w-nlviz/w-nlviz-1018/w-nlviz-1018_Preview.mp4?token=DXvLy_v6CToylKULSDeXlQHkd9I1k1aqOc65ngpDakw&expires=1706590800",
                        "bunny_ff_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/w-nlviz/w-nlviz-1018/w-nlviz-1018_Preview.vtt?token=NKGmqr43K-r59XBkW1VmoVmLprxYGVjA8JSxr4_HmkI&expires=1706590800",
                        "youtube_prerecorded_link": "",
                        "youtube_prerecorded_id": "",
                        "bunny_prerecorded_link": "",
                        "bunny_prerecorded_subtitles": ""
                    },
                    {
                        "slot_id": "w-nlviz-1017",
                        "session_id": "w5",
                        "title": "Discourse Lines: Visualising Current Policy and Media Storylines of Opportunity and Disadvantage with Narrative Exploration Maps",
                        "contributors": [
                            "Simon Angus, Ying Yang"
                        ],
                        "authors": [
                            "Simon D Angus",
                            "Satya Borgohain",
                            "Songhai Fan",
                            "Sarah Goodwin",
                            "Adrian Kristanto",
                            "Lachlan O'Neill",
                            "Helen C. Purchase",
                            "Nancy Van Nieuwenhove",
                            "Ying Yang",
                            "Yingqi Zhang",
                            "Tim Dwyer"
                        ],
                        "abstract": "Topics of disadvantage are often discussed in the media. The discourse of disadvantage is multidimensional and has many intersecting elements, with some issues more common than others (e.g. violence, addiction), and some tending to co-occur, like human rights, criminal justice, and health to name just a few common themes. Here, we introduce and describe \u201cDiscourse Lines\u201d, an online interactive visualisation to discover which co-occurring disadvantage issues are being discussed in the media, and which ones are left out and obscured. The visualisation presents an AI-assisted analysis of news articles on topics of discourse. Our multi-scale architecture metro map visualisation invites users to drill down from a topics overview landing map to topic-specific metro maps until individual news articles. This dynamic platform allows users to see how discourse on topics of disadvantage unfolds and how news conflates or separates various issues over time.",
                        "uid": "w-nlviz-1017",
                        "time_stamp": "2023-10-22T03:00:00Z",
                        "time_start": "2023-10-22T03:00:00Z",
                        "time_end": "2023-10-22T06:00:00Z",
                        "paper_type": "workshop",
                        "keywords": [],
                        "doi": "",
                        "fno": "",
                        "has_image": true,
                        "has_pdf": true,
                        "paper_award": "",
                        "image_caption": "We introduce and describe \"Discourse Lines\", an online interactive visualisation to discover co-occurring topic groups in the Australian media, akin to the metro map metaphor. The visualisation presents an AI-assisted analysis that has enabled the efficient processing of very large bodies of text about the information on disadvantages. Our multi-scale architecture metro map visualisation invites users to drill down from a topics overview landing map to topic-specific metro maps until individual news articles. This dynamic platform allows users to see how discourse on topics of disadvantage unfolds and how news conflates or separates various issues over time.",
                        "external_paper_link": "",
                        "youtube_ff_link": "",
                        "youtube_ff_id": "",
                        "bunny_ff_link": "",
                        "bunny_ff_subtitles": "",
                        "youtube_prerecorded_link": "",
                        "youtube_prerecorded_id": "",
                        "bunny_prerecorded_link": "",
                        "bunny_prerecorded_subtitles": ""
                    },
                    {
                        "slot_id": "w-nlviz-1013",
                        "session_id": "w5",
                        "title": "Visualizing LLM Text Style Transfer: visually dissecting how to talk like a pirate",
                        "contributors": [
                            "Richard Brath"
                        ],
                        "authors": [
                            "Richard Brath",
                            "Adam James Bradley",
                            "David Jonker"
                        ],
                        "abstract": "Text Style Transfer (TST) retains semantic content while modifying stylistic features. Exploratory visualization of LLM-generated TST via semantically aligned text visualization reveals advanced stylistic techniques such as use of metaphors. LLM style inquiry can be used to articulate advanced stylistic devices such as interjections, idioms and rhetorical devices and visually depicted as multivariate style heatmaps.",
                        "uid": "w-nlviz-1013",
                        "time_stamp": "2023-10-22T03:00:00Z",
                        "time_start": "2023-10-22T03:00:00Z",
                        "time_end": "2023-10-22T06:00:00Z",
                        "paper_type": "workshop",
                        "keywords": [],
                        "doi": "",
                        "fno": "",
                        "has_image": true,
                        "has_pdf": true,
                        "paper_award": "",
                        "image_caption": "Text Style Transfer (TST) retains semantic content while modifying stylistic features from e.g. a pirate to a detective. We semantically align transfered text which reveals techniques such as  metaphors. We do LLM style inquiry to articulate retorical devices as multivariate style heatmaps.",
                        "external_paper_link": "",
                        "youtube_ff_link": "",
                        "youtube_ff_id": "",
                        "bunny_ff_link": "https://ieeevis-uploads.b-cdn.net/vis23/w-nlviz/w-nlviz-1013/w-nlviz-1013_Preview.mp4?token=r92Pcz6MnzmiUympelA9j5PeQT0248JCZUvc2uD6ckU&expires=1706590800",
                        "bunny_ff_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/w-nlviz/w-nlviz-1013/w-nlviz-1013_Preview.vtt?token=e9qBwwqg6fg_R8CZCt-IaYArkM2Ylj7co9lfr_06B9E&expires=1706590800",
                        "youtube_prerecorded_link": "",
                        "youtube_prerecorded_id": "",
                        "bunny_prerecorded_link": "",
                        "bunny_prerecorded_subtitles": ""
                    },
                    {
                        "slot_id": "w-nlviz-1012",
                        "session_id": "w5",
                        "title": "Visualizing textual distributions of repeated LLM responses to characterize LLM knowledge",
                        "contributors": [
                            "Richard Brath"
                        ],
                        "authors": [
                            "Richard Brath",
                            "Adam James Bradley",
                            "David Jonker"
                        ],
                        "abstract": "The breadth and depth of knowledge learned by Large Language Models (LLMs) can be assessed through repetitive prompting and visual analysis of commonality across the responses. We show levels of LLM verbatim completions of prompt text through aligned responses, mind-maps of knowledge across several areas in general topics, and an association graph of topics generated directly from recursive prompting of the LLM.",
                        "uid": "w-nlviz-1012",
                        "time_stamp": "2023-10-22T03:00:00Z",
                        "time_start": "2023-10-22T03:00:00Z",
                        "time_end": "2023-10-22T06:00:00Z",
                        "paper_type": "workshop",
                        "keywords": [],
                        "doi": "",
                        "fno": "",
                        "has_image": true,
                        "has_pdf": true,
                        "paper_award": "",
                        "image_caption": "Issuing the same prompt to an LLM many times generates similar responses that approximate distributions. These distributions can be visualized with proportionally encoded correct words to depict exact matches; or processed into mind-maps to show LLM knowledge breadth.",
                        "external_paper_link": "",
                        "youtube_ff_link": "",
                        "youtube_ff_id": "",
                        "bunny_ff_link": "https://ieeevis-uploads.b-cdn.net/vis23/w-nlviz/w-nlviz-1012/w-nlviz-1012_Preview.mp4?token=cS-Q10AMzmllpya4y8SatAX_f1Ps6sZFqZerzJgiVxk&expires=1706590800",
                        "bunny_ff_subtitles": "",
                        "youtube_prerecorded_link": "",
                        "youtube_prerecorded_id": "",
                        "bunny_prerecorded_link": "",
                        "bunny_prerecorded_subtitles": ""
                    },
                    {
                        "slot_id": "w-nlviz-1009",
                        "session_id": "w5",
                        "title": "A Vega-Lite Dataset and Natural Language Generation Pipeline with Large Language Models",
                        "contributors": [
                            "Hyung-Kwon Ko"
                        ],
                        "authors": [
                            "Hyung-Kwon Ko",
                            "Hyeon Jeon",
                            "Gwanmo Park",
                            "Dae Hyun Kim",
                            "Nam Wook Kim",
                            "Juho Kim",
                            "Jinwook Seo"
                        ],
                        "abstract": "There is a growing trend of utilizing Visualization-oriented Natural Language Interfaces (V-NLIs) to author charts. However, researchers consistently highlight the lack of high-quality chart and natural language datasets, which impedes the development of more sophisticated and data-driven systems using V-NLIs. In this study, we present a meticulously curated collection of human-generated 1,981 Vega-Lite specifications, derived from real-world data, and use Large Language Models (LLMs) for generating natural language queries for chart generation tasks. Unlike previous datasets that relied on relatively simple and homogeneous templates, our Vega-Lite dataset contains more complex and diverse (i.e., varying interactions, multiple plots/views, and different chart types). Using this dataset, we demonstrate generating natural language queries for chart generation, and how the results can be different when different input types are used (e.g., Vega-Lite, Image, both Vega-Lite and Image).",
                        "uid": "w-nlviz-1009",
                        "time_stamp": "2023-10-22T03:00:00Z",
                        "time_start": "2023-10-22T03:00:00Z",
                        "time_end": "2023-10-22T06:00:00Z",
                        "paper_type": "workshop",
                        "keywords": [],
                        "doi": "",
                        "fno": "",
                        "has_image": true,
                        "has_pdf": true,
                        "paper_award": "",
                        "image_caption": "This is sample charts of Vega-Lite specifications we present in our work. This charts includes multiple chart types such as map, heatmap, distribution, bar, line and so on. They have multiple interaction techniques like selection, panning, zooming, and brushing. They also include composite views so that many plots are connected with interaction techniques.",
                        "external_paper_link": "",
                        "youtube_ff_link": "",
                        "youtube_ff_id": "",
                        "bunny_ff_link": "https://ieeevis-uploads.b-cdn.net/vis23/w-nlviz/w-nlviz-1009/w-nlviz-1009_Preview.mp4?token=9UKAfOMMnhXI1qS01PC6eUjZXYPrqUnTBpSkxOlkJWs&expires=1706590800",
                        "bunny_ff_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/w-nlviz/w-nlviz-1009/w-nlviz-1009_Preview.vtt?token=pjew3AfKunHc3n8HNtAgRdjexidMzJ3asvnGm-QzeDo&expires=1706590800",
                        "youtube_prerecorded_link": "https://youtu.be/ipnFwTRtOow",
                        "youtube_prerecorded_id": "ipnFwTRtOow",
                        "bunny_prerecorded_link": "https://ieeevis-uploads.b-cdn.net/vis23/w-nlviz/w-nlviz-1009/w-nlviz-1009_Presentation.mp4?token=WipgOddAAoBY2HEt9tuSA8iIgukj7YAdopB5bXMGtAU&expires=1706590800",
                        "bunny_prerecorded_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/w-nlviz/w-nlviz-1009/w-nlviz-1009_Presentation.vtt?token=3YXhetOCzRFtxNcD9ncQKTOxP4DmuNXOWUf45UbfWQo&expires=1706590800"
                    }
                ]
            }
        ]
    },
    "w-energyvis": {
        "event": "EnergyVis 2023: 3rd Workshop on Energy Data Visualization",
        "long_name": "EnergyVis 2023: 3rd Workshop on Energy Data Visualization",
        "event_type": "workshop",
        "event_prefix": "w-energyvis",
        "event_description": "",
        "event_url": "",
        "organizers": [
            "Kenny Gruchalla",
            "Arnaud Prouzeau",
            "Lyn Bartram",
            "Sarah Goodwin"
        ],
        "sessions": [
            {
                "title": "EnergyVis 2023: 3rd Workshop on Energy Data Visualization",
                "session_id": "w10",
                "event_prefix": "w-energyvis",
                "track": "oneohthree",
                "session_image": "w10.png",
                "chair": [
                    "Kenny Gruchalla",
                    "Arnaud Prouzeau",
                    "Lyn Bartram",
                    "Sarah Goodwin"
                ],
                "time_start": "2023-10-21T22:00:00Z",
                "time_end": "2023-10-22T01:00:00Z",
                "discord_category": "",
                "discord_channel": "103",
                "discord_channel_id": "1161741183815524502",
                "discord_link": "https://discordapp.com/channels/1161400451619627068/1161741183815524502",
                "zoom_private_meeting": "",
                "zoom_private_password": "",
                "zoom_private_link": "",
                "zoom_broadcast_link": "",
                "ff_link": "",
                "time_slots": [
                    {
                        "slot_id": "w-energyvis-1001",
                        "session_id": "w10",
                        "title": "Alternatives to Contour Visualizations for Power Systems Data",
                        "contributors": [
                            "Kenny Gruchalla"
                        ],
                        "authors": [
                            "Isaiah Lyons-Galante",
                            "Morteza Karimzadeh",
                            "Sam Molnar",
                            "Graham Johnson",
                            "Kenny Gruchalla"
                        ],
                        "abstract": "Electrical grids are geographical and topological structures whose voltage states are challenging to represent accurately and efficiently for visual analysis. The current common practice is to use colored contour maps, yet these can misrepresent the data. We examine the suitability of four alternative visualization methods for depicting voltage data in a geographically dense distribution system\u2014Voronoi polygons, H3 tessellations, S2 tessellations, and a network-weighted contour map. We find that Voronoi tessellations and network-weighted contour maps more accurately represent the statistical distribution of the data than regular contour maps.",
                        "uid": "w-energyvis-1001",
                        "time_stamp": "2023-10-21T22:00:00Z",
                        "time_start": "2023-10-21T22:00:00Z",
                        "time_end": "2023-10-22T01:00:00Z",
                        "paper_type": "workshop",
                        "keywords": [],
                        "doi": "",
                        "fno": "",
                        "has_image": true,
                        "has_pdf": true,
                        "paper_award": "",
                        "image_caption": "A synthetic electrical grid covers a neighborhood in Oakland, CA. This network includes over 24,000 junctions, or buses, each with a given voltage at a single snapshot in time. Here, we surrounded each bus with a Voronoi polygon. The electrical state of the network is represented by coloring each polygon by the bus voltage. A blue-white-red diverging color scale shows deviations from the expected voltage. Giving each bus its own polygon increases the fidelity of the visualization to the real data distribution by avoiding averaging. We explore this and other alternatives to contour visualizations for power systems data.",
                        "external_paper_link": "",
                        "youtube_ff_link": "",
                        "youtube_ff_id": "",
                        "bunny_ff_link": "",
                        "bunny_ff_subtitles": "",
                        "youtube_prerecorded_link": "",
                        "youtube_prerecorded_id": "",
                        "bunny_prerecorded_link": "",
                        "bunny_prerecorded_subtitles": ""
                    },
                    {
                        "slot_id": "w-energyvis-1002",
                        "session_id": "w10",
                        "title": "Topological Guided Detection of Extreme Wind Phenomena: Implications for Wind Energy",
                        "contributors": [
                            "Brian Summa"
                        ],
                        "authors": [
                            "Yu Qin",
                            "Graham Johnson",
                            "Brian Summa"
                        ],
                        "abstract": "Extreme wind phenomena play a crucial role in the efficient operation of wind farms for renewable energy generation. However, existing detection methods are computationally expensive, limited to specific coordinate. In real-world scenarios, understanding the occurrence of these phenomena over a large area is essential. Therefore, there is a significant demand for a fast and accurate approach to forecast such events. In this paper, we propose a novel method for detecting wind phenomena using topological analysis, leveraging the gradient of wind speed or critical points in a topological framework. By extracting topological features from the wind speed profile within a defined region, we employ topological distance to identify extreme wind phenomena. Our results demonstrate the effectiveness of utilizing topological features derived from regional wind speed profiles. We validate our approach using high-resolution simulations with the Weather Research and Forecasting model (WRF) over a month in the US East Coast.",
                        "uid": "w-energyvis-1002",
                        "time_stamp": "2023-10-21T22:00:00Z",
                        "time_start": "2023-10-21T22:00:00Z",
                        "time_end": "2023-10-22T01:00:00Z",
                        "paper_type": "workshop",
                        "keywords": [],
                        "doi": "",
                        "fno": "",
                        "has_image": true,
                        "has_pdf": true,
                        "paper_award": "",
                        "image_caption": "Extreme wind phenomena play a crucial role in the efficient operation of wind farms for renewable energy generation. However, existing detection methods are computationally expensive, limited to specific coordinate. In real-world scenarios, understanding the occurrence of these phenomena over a large area is essential. Therefore, there is a significant demand for a fast and accurate approach to forecast such events. In this paper, we propose a novel method for detecting wind phenomena using topological analysis, leveraging the gradient of wind speed or critical points in a topological framework. By extracting topological features from the wind speed profile within a defined region, we employ topological distance to identify extreme wind phenomena. Our results demonstrate the effectiveness of utilizing topological features derived from regional wind speed profiles. We validate our approach using high-resolution simulations with the Weather Research and Forecasting model (WRF) over a month in the US East Coast.",
                        "external_paper_link": "",
                        "youtube_ff_link": "",
                        "youtube_ff_id": "",
                        "bunny_ff_link": "",
                        "bunny_ff_subtitles": "",
                        "youtube_prerecorded_link": "",
                        "youtube_prerecorded_id": "",
                        "bunny_prerecorded_link": "",
                        "bunny_prerecorded_subtitles": ""
                    },
                    {
                        "slot_id": "w-energyvis-1003",
                        "session_id": "w10",
                        "title": "An Interactive, Scenario-Based Visualization Dashboard for Model-to-Model Comparison",
                        "contributors": [
                            "Erica Attard"
                        ],
                        "authors": [
                            "Erica Attard",
                            "Madeleine McPherson"
                        ],
                        "abstract": "Energy models can have a large impact on policy options and decarbonization pathways, however, they are currently stunted by obstacles including the need for transparency, accessibility of model inputs and outputs, increased stakeholder engagement, and open-source tools. The Energy Modeling Hub has responded to this need with the development of an integrated modeling platform consisting of (1) a Canadian database for energy systems, (2) a suite of energy system models, and (3) a visualization dashboard. This paper focuses on the requirements and development of a visualization dashboard used to facilitate scenario comparison, multi-model comparison, and the co-development of scenarios. The design consists of a series of tabs supporting side-by-side visualizations with a multitude of interactive features including a hover tool, toolbar, data filters, buttons, and an interactive legend. The dashboard has been used in presentations to Ministers in the British Colombia Government and received positive feedback for transparently displaying results and supplemental information as well as allowing users to interact with the data in a way that can drive their own decisions. The visualization dashboard serves as a solid foundation to facilitate scenario-based and multi-model comparisons and close the communication gap between energy modelers and stakeholders. Future work includes expanding the visualization and interactive elements implemented, specifically for input data visualization.",
                        "uid": "w-energyvis-1003",
                        "time_stamp": "2023-10-21T22:00:00Z",
                        "time_start": "2023-10-21T22:00:00Z",
                        "time_end": "2023-10-22T01:00:00Z",
                        "paper_type": "workshop",
                        "keywords": [],
                        "doi": "",
                        "fno": "",
                        "has_image": true,
                        "has_pdf": true,
                        "paper_award": "",
                        "image_caption": "The Visualization Dashboard showing the generation capacity for two models and the same scenario.",
                        "external_paper_link": "",
                        "youtube_ff_link": "",
                        "youtube_ff_id": "",
                        "bunny_ff_link": "",
                        "bunny_ff_subtitles": "",
                        "youtube_prerecorded_link": "",
                        "youtube_prerecorded_id": "",
                        "bunny_prerecorded_link": "",
                        "bunny_prerecorded_subtitles": ""
                    },
                    {
                        "slot_id": "w-energyvis-1005",
                        "session_id": "w10",
                        "title": "Virtual Reality for Enhancing Engagement with Net Zero Transitions",
                        "contributors": [
                            "Amal Alshardy"
                        ],
                        "authors": [
                            "Amal Alshardy",
                            "Sarah Goodwin",
                            "Andres Santos-Torres",
                            "Ariel Liebman"
                        ],
                        "abstract": "Immersive experiences can increase engagement and improve data understanding. We explore the use of virtual reality to visualise the complexities of electric networks and facilitate a greater understand- ing of net zero initiatives. We propose that an abstract metaphorical immersive experience can provide an intuitive overview of the complex components of energy networks and how they behave over time in response to fluctuating supply and demand in order to achieve stability and sustainability. This work aims to enable a variety of stakeholders to better understand and associate the complexities in the systems and be aware of the connections over time and space. Using the Monash University electric network system as a case study, we explore the network dynamics and fluctuations over time. The virtual reality experience allows users to view campus electricity assets and local and remote energy sources, as well as witness the dynamics of the network. Thus, the complexities of the energy system and its impact on sustainability can be more readily understood.",
                        "uid": "w-energyvis-1005",
                        "time_stamp": "2023-10-21T22:00:00Z",
                        "time_start": "2023-10-21T22:00:00Z",
                        "time_end": "2023-10-22T01:00:00Z",
                        "paper_type": "workshop",
                        "keywords": [],
                        "doi": "",
                        "fno": "",
                        "has_image": true,
                        "has_pdf": true,
                        "paper_award": "",
                        "image_caption": "Immersive VR environment showing an overview of Monash Clayton electric network dynamics: (A) Batteries supply the Clayton campus with stored energy at night. (B) With the wind turbines slowed in early morning, the connecting edge between the Murra Warra Wind Farm and the campus becomes thinner. (C) The campus relies on solar and wind power for daytime consumption. (D) At certain times the campus relies on grid generation, the simulation shows coal powered generation. A thin edge means the campus energy is off the grid.",
                        "external_paper_link": "",
                        "youtube_ff_link": "",
                        "youtube_ff_id": "",
                        "bunny_ff_link": "",
                        "bunny_ff_subtitles": "",
                        "youtube_prerecorded_link": "",
                        "youtube_prerecorded_id": "",
                        "bunny_prerecorded_link": "",
                        "bunny_prerecorded_subtitles": ""
                    },
                    {
                        "slot_id": "w-energyvis-1007",
                        "session_id": "w10",
                        "title": "Visualization of the Oscillatory Dynamics of an Island Power System",
                        "contributors": [
                            "Kenny Gruchalla"
                        ],
                        "authors": [
                            "Sam Molnar",
                            "Kenny Gruchalla",
                            "Shuan Dong",
                            "Jin Tan"
                        ],
                        "abstract": "In this work, we discuss the design of visualizations for understand- ing the complex oscillatory dynamics of an island power system with renewable generation sources after the loss of a large oil power plant. As more renewable generation sources are added to power systems, the oscillatory dynamics will change, which requires new visualization techniques to determine causes and strategies to avoid unwanted behaviors in the future. Our approach integrates geo- graphic views, time-series plots, and novel oscillatory-trajectory curves, providing unique insights into the interdependent oscillatory behaviors of multiple state variables and generators over time. By enabling multi-node and multivariate comparisons over time, users can qualitatively determine drivers of oscillations and differences in generator dynamics, which is not possible with other commonly used visualization techniques.",
                        "uid": "w-energyvis-1007",
                        "time_stamp": "2023-10-21T22:00:00Z",
                        "time_start": "2023-10-21T22:00:00Z",
                        "time_end": "2023-10-22T01:00:00Z",
                        "paper_type": "workshop",
                        "keywords": [],
                        "doi": "",
                        "fno": "",
                        "has_image": true,
                        "has_pdf": true,
                        "paper_award": "",
                        "image_caption": "A sequence of snapshots of our oscillatory trajectory curves for the real power and frequency of generators connected to an island power sytem. In addition to showing the relationship between to timeseries variables, we also overlay the network structure to provide structural context.",
                        "external_paper_link": "",
                        "youtube_ff_link": "",
                        "youtube_ff_id": "",
                        "bunny_ff_link": "",
                        "bunny_ff_subtitles": "",
                        "youtube_prerecorded_link": "",
                        "youtube_prerecorded_id": "",
                        "bunny_prerecorded_link": "",
                        "bunny_prerecorded_subtitles": ""
                    },
                    {
                        "slot_id": "w-energyvis-1008",
                        "session_id": "w10",
                        "title": "Exploring the Benefits of Geography on Power Network Diagrams",
                        "contributors": [
                            "Sarah Goodwin"
                        ],
                        "authors": [
                            "Merry Hoang",
                            "Sarah Goodwin",
                            "Michael Wybrow",
                            "Ying Yang"
                        ],
                        "abstract": "This paper introduces an interactive visualisation that combines a spatial element to the single line diagram (SLD). SLDs are conceptual maps of the power network used by power engineers to understand the connectivity between assets of the network, study power flow, and maintain grid stability and security. Enabling users with varying degrees of electrical knowledge to understand the geographical aspect of the SLD was the key design goal. Developed through an iterative process, the visualisation intuitively transitions from an SLD view to a map view. Evaluation of the visualisation during, and following development revealed that the prototype was well-liked and that having a spatial element to the SLD was useful in understanding the geographical relationships in the power network. The evaluation also helped identify stakeholders with an interest in the hybrid view and showed the prototype\u2019s potential utility in communicating technical data to electrical non-technical users.",
                        "uid": "w-energyvis-1008",
                        "time_stamp": "2023-10-21T22:00:00Z",
                        "time_start": "2023-10-21T22:00:00Z",
                        "time_end": "2023-10-22T01:00:00Z",
                        "paper_type": "workshop",
                        "keywords": [],
                        "doi": "",
                        "fno": "",
                        "has_image": true,
                        "has_pdf": true,
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "youtube_ff_link": "",
                        "youtube_ff_id": "",
                        "bunny_ff_link": "",
                        "bunny_ff_subtitles": "",
                        "youtube_prerecorded_link": "",
                        "youtube_prerecorded_id": "",
                        "bunny_prerecorded_link": "",
                        "bunny_prerecorded_subtitles": ""
                    }
                ]
            }
        ]
    },
    "w-cityvis": {
        "event": "5th Workshop on Urban Data Visualization (CityVis) - Focus: The Role of Data Governance",
        "long_name": "5th Workshop on Urban Data Visualization (CityVis) - Focus: The Role of Data Governance",
        "event_type": "workshop",
        "event_prefix": "w-cityvis",
        "event_description": "",
        "event_url": "",
        "organizers": [
            "Jessica Bou Nassar",
            "Lyn Bartram",
            "Sebastian Meier",
            "Darren Sharp",
            "Leonard Higi",
            "Sarah Goodwin"
        ],
        "sessions": [
            {
                "title": "5th Workshop on Urban Data Visualization (CityVis) - Focus: The Role of Data Governance",
                "session_id": "w12",
                "event_prefix": "w-cityvis",
                "track": "oneohthree",
                "session_image": "w12.png",
                "chair": [
                    "Jessica Bou Nassar",
                    "Lyn Bartram",
                    "Sebastian Meier",
                    "Darren Sharp",
                    "Leonard Higi",
                    "Sarah Goodwin"
                ],
                "time_start": "2023-10-22T03:00:00Z",
                "time_end": "2023-10-22T06:00:00Z",
                "discord_category": "",
                "discord_channel": "103",
                "discord_channel_id": "1161741183815524502",
                "discord_link": "https://discordapp.com/channels/1161400451619627068/1161741183815524502",
                "zoom_private_meeting": "",
                "zoom_private_password": "",
                "zoom_private_link": "",
                "zoom_broadcast_link": "",
                "ff_link": "",
                "time_slots": [
                    {
                        "slot_id": "w-cityvis-1001",
                        "session_id": "w12",
                        "title": "Enhancing Collaboration in Urban Data Governance: A Measurement Framework for Applied Data Visualization",
                        "contributors": [
                            "Chien-Yu Lin"
                        ],
                        "authors": [
                            "Chien-Yu Lin"
                        ],
                        "abstract": "The integration of data science, machine learning, and artificial intelligence in urban studies and design promises transformative impacts on cities. While acknowledging that urban complexities transcend data, the concepts of datafication and dataism emphasize the potential to sample, model, and predict urban phenomena through data. This study explores the synergy of collaboration and data visualization in urban data governance. An analytical framework, rooted in the multidimensional collaboration model and guided by theories, elucidates dimensions like Governance, Administration, Autonomy, Mutuality, Norms, and Equality. A combination of qualitative and quantitative research complements the framework, generating indicators to assess the impact of data visualization on data governance. This study contributes to structuring the framework to examine the symbiotic relationship between data visualization, collaboration, and decision-making, propelling transformative urban data governance.",
                        "uid": "w-cityvis-1001",
                        "time_stamp": "2023-10-22T03:00:00Z",
                        "time_start": "2023-10-22T03:00:00Z",
                        "time_end": "2023-10-22T06:00:00Z",
                        "paper_type": "workshop",
                        "keywords": [],
                        "doi": "",
                        "fno": "",
                        "has_image": true,
                        "has_pdf": true,
                        "paper_award": "",
                        "image_caption": "We all recognize the crucial role of collaboration in urban data. Our study's framework is built upon insights from existing literature. However, our objective goes beyond presenting methodology. Through an extensive literature review, we have meticulously defined six key dimensions: Governance, Administration, Autonomy, Mutuality, Norm, and Equality. Our goal is to emphasize the importance of collaborative research in the urban data profession. We are excited to share this framework with you, as it will allow us to apply indicators to assess the impact of these dimensions on data governance.",
                        "external_paper_link": "",
                        "youtube_ff_link": "",
                        "youtube_ff_id": "",
                        "bunny_ff_link": "https://ieeevis-uploads.b-cdn.net/vis23/w-cityvis/w-cityvis-1001/w-cityvis-1001_Preview.mp4?token=GtmHgNdyKlpT7XP1Xj_51qAQCS4dUTVbOVvZ7QoXFKg&expires=1706590800",
                        "bunny_ff_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/w-cityvis/w-cityvis-1001/w-cityvis-1001_Preview.vtt?token=Vyw0p88ZX89yI5-Ap7P9jqhpQXmiMmZ8Ibr-2mGvvrg&expires=1706590800",
                        "youtube_prerecorded_link": "https://youtu.be/p84Bx1HNFUI",
                        "youtube_prerecorded_id": "p84Bx1HNFUI",
                        "bunny_prerecorded_link": "https://ieeevis-uploads.b-cdn.net/vis23/w-cityvis/w-cityvis-1001/w-cityvis-1001_Presentation.mp4?token=S4RMl4a4p_Xn38jeQop-WnZRpDWsOOQG1fCDadcY6eo&expires=1706590800",
                        "bunny_prerecorded_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/w-cityvis/w-cityvis-1001/w-cityvis-1001_Presentation.vtt?token=uI4a-OmlNh5Too-GLdTRCqg4pqcSEspf-jqG6odYyVY&expires=1706590800"
                    },
                    {
                        "slot_id": "w-cityvis-1017",
                        "session_id": "w12",
                        "title": "The Use of Causal Loop Diagrams to Explore People-Centred Data Governance in Australian Cities",
                        "contributors": [
                            "Ms. Jessica Bou Nassar"
                        ],
                        "authors": [],
                        "abstract": "In this study, we explore, through co-creation, how Australian cities might incorporate a people-centred data governance ecosystem. We use CLDs (1) during interviews to elicit perspectives from various stakeholders in cities regarding the current state of data governance and (2) during workshops for solution ideation. In this work we focus on the multiplicity of perspectives regarding DG in Australian cities. We aim to co-create a conceptual model of the status quo of data governance and explore solutions to achieve people-centred DG. An interactive visualisation of the model will showcase the diversity of perspectives, delineate the power dynamics underpinning data governance in Australian cities in an accessible manner, and portray people-centred solutions.",
                        "uid": "w-cityvis-1017",
                        "time_stamp": "2023-10-22T03:00:00Z",
                        "time_start": "2023-10-22T03:00:00Z",
                        "time_end": "2023-10-22T06:00:00Z",
                        "paper_type": "workshop",
                        "keywords": [],
                        "doi": "",
                        "fno": "",
                        "has_image": false,
                        "has_pdf": true,
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "youtube_ff_link": "",
                        "youtube_ff_id": "",
                        "bunny_ff_link": "",
                        "bunny_ff_subtitles": "",
                        "youtube_prerecorded_link": "",
                        "youtube_prerecorded_id": "",
                        "bunny_prerecorded_link": "",
                        "bunny_prerecorded_subtitles": ""
                    },
                    {
                        "slot_id": "w-cityvis-1013",
                        "session_id": "w12",
                        "title": "Visualizing Scalar Effects of Urban Data Aggregation",
                        "contributors": [
                            "Jonathan Nelson"
                        ],
                        "authors": [
                            "Jonathan K Nelson"
                        ],
                        "abstract": "The process of geospatial data aggregation provides a means for abstracting the complexity of urban systems to not just better understand them, but also protect the privacy of the individuals within them. However, level of aggregation and the arbitrary sizes, shapes, and arrangements of areal units may lead to statistical and visual bias that affects the reliability and validity of findings derived from the analysis of areally aggregated urban data. This bias and resulting analytical uncertainty \u2013 known as the Modifiable Areal Unit Problem (MAUP) \u2013 has implications for public policy implementation and allocation of critical resources in both urban and rural areas. Despite a wealth of geographic research on MAUP and development of advanced statistical approaches to quantifying its effects, many of these insights and techniques remain largely inaccessible and subsequently unadopted by GIS professionals working on city planning applications. This paper introduces a simple vector-to-raster choropleth mapping workflow that enables a broad range of urban analysts to visually assess the scalar effects of the modifiable areal unit problem.",
                        "uid": "w-cityvis-1013",
                        "time_stamp": "2023-10-22T03:00:00Z",
                        "time_start": "2023-10-22T03:00:00Z",
                        "time_end": "2023-10-22T06:00:00Z",
                        "paper_type": "workshop",
                        "keywords": [],
                        "doi": "",
                        "fno": "",
                        "has_image": true,
                        "has_pdf": true,
                        "paper_award": "",
                        "image_caption": "Creating choropleth maps of area deprivation indices for Wisconsin State (USA) at the census block group (left), tract (middle), and county (right) levels of aggregation as an initial step in a workflow for visualizing the scalar effects of urban data aggregation.",
                        "external_paper_link": "",
                        "youtube_ff_link": "",
                        "youtube_ff_id": "",
                        "bunny_ff_link": "",
                        "bunny_ff_subtitles": "",
                        "youtube_prerecorded_link": "",
                        "youtube_prerecorded_id": "",
                        "bunny_prerecorded_link": "",
                        "bunny_prerecorded_subtitles": ""
                    },
                    {
                        "slot_id": "w-cityvis-1014",
                        "session_id": "w12",
                        "title": "How Far Can Public Transport Take You?",
                        "contributors": [
                            "Markus Trainer"
                        ],
                        "authors": [
                            "Markus Trainer",
                            "Christina Humer",
                            "Patrick Adelberger",
                            "Marc Streit"
                        ],
                        "abstract": "Sustainable mobility is crucial in our current era. Our proposed interactive web application provides a user-friendly way to evaluate public transport networks and analyze how well connected a user defined location is. The current implementation comprises data from all Austrian public transport systems but can be extended with data from any provider. We made the code available on github: github.com/jku-vds-lab/publictransport. The tool can be tested in the deployed version: publictransport.jku-vds-lab.at.",
                        "uid": "w-cityvis-1014",
                        "time_stamp": "2023-10-22T03:00:00Z",
                        "time_start": "2023-10-22T03:00:00Z",
                        "time_end": "2023-10-22T06:00:00Z",
                        "paper_type": "workshop",
                        "keywords": [],
                        "doi": "",
                        "fno": "",
                        "has_image": true,
                        "has_pdf": true,
                        "paper_award": "",
                        "image_caption": "A proposed web application that allows users to determine how well connected a location in Austria is through public transport.",
                        "external_paper_link": "",
                        "youtube_ff_link": "",
                        "youtube_ff_id": "",
                        "bunny_ff_link": "https://ieeevis-uploads.b-cdn.net/vis23/w-cityvis/w-cityvis-1014/w-cityvis-1014_Preview.mp4?token=ywEpoCGwJ99M3KcYO7tQDWn5j4KHvPVNg_wbaxeS2NA&expires=1706590800",
                        "bunny_ff_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/w-cityvis/w-cityvis-1014/w-cityvis-1014_Preview.vtt?token=IckZcp4LCXiJMjGAnKNa36MOmZGl9Zp0oIB8AAGbqdA&expires=1706590800",
                        "youtube_prerecorded_link": "https://youtu.be/WNteZ-vY6fs",
                        "youtube_prerecorded_id": "WNteZ-vY6fs",
                        "bunny_prerecorded_link": "https://ieeevis-uploads.b-cdn.net/vis23/w-cityvis/w-cityvis-1014/w-cityvis-1014_Presentation.mp4?token=sxproHeuGonZkLzQT347R7R4MYFkfLqKZdrFoJc52Ac&expires=1706590800",
                        "bunny_prerecorded_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/w-cityvis/w-cityvis-1014/w-cityvis-1014_Presentation.vtt?token=yWK6-_JV4K1wPEbY9qy-T6BMYxAQ2ZYUiqw25jX6Njc&expires=1706590800"
                    }
                ]
            }
        ]
    },
    "w-mercado": {
        "event": "MERCADO: Multimodal Experiences for Remote Communication Around Data Online",
        "long_name": "MERCADO: Multimodal Experiences for Remote Communication Around Data Online",
        "event_type": "workshop",
        "event_prefix": "w-mercado",
        "event_description": "",
        "event_url": "",
        "organizers": [
            "Matthew Brehmer",
            "Maxime Cordeil",
            "Christophe Hurter",
            "Takayuki Itoh"
        ],
        "sessions": [
            {
                "title": "MERCADO: Multimodal Experiences for Remote Communication Around Data Online",
                "session_id": "w13",
                "event_prefix": "w-mercado",
                "track": "oneohfive",
                "session_image": "w13.png",
                "chair": [
                    "Matthew Brehmer",
                    "Maxime Cordeil",
                    "Christophe Hurter",
                    "Takayuki Itoh"
                ],
                "time_start": "2023-10-21T22:00:00Z",
                "time_end": "2023-10-22T01:00:00Z",
                "discord_category": "",
                "discord_channel": "105",
                "discord_channel_id": "1161741309346861067",
                "discord_link": "https://discordapp.com/channels/1161400451619627068/1161741309346861067",
                "zoom_private_meeting": "",
                "zoom_private_password": "",
                "zoom_private_link": "",
                "zoom_broadcast_link": "",
                "ff_link": "",
                "time_slots": [
                    {
                        "slot_id": "w-mercado-8665",
                        "session_id": "w13",
                        "title": "Talking to Data Visualizations: Opportunities and Challenges",
                        "contributors": [
                            "Gabriela Molina Le\u00f3n"
                        ],
                        "authors": [
                            "Gabriela Molina Le\u00f3n, Petra Isenberg, Andreas Breiter"
                        ],
                        "abstract": "Speech is one of...",
                        "uid": "w-mercado-8665",
                        "time_stamp": "2023-10-21T22:00:00Z",
                        "time_start": "2023-10-21T22:00:00Z",
                        "time_end": "2023-10-22T01:00:00Z",
                        "paper_type": "workshop",
                        "keywords": [],
                        "doi": "",
                        "fno": "",
                        "has_image": true,
                        "has_pdf": true,
                        "paper_award": "",
                        "image_caption": "Collaborative scenario example: Two collaborators interacting with data visualizations via speech.",
                        "external_paper_link": "",
                        "youtube_ff_link": "",
                        "youtube_ff_id": "",
                        "bunny_ff_link": "https://ieeevis-uploads.b-cdn.net/vis23/w-mercado/w-mercado-8665/w-mercado-8665_Preview.mp4?token=0-fRl4NGbnO7QlrWKa2a6EhhGeg2yuXtdZNF0Wvv5Yk&expires=1706590800",
                        "bunny_ff_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/w-mercado/w-mercado-8665/w-mercado-8665_Preview.vtt?token=XeVt10Hvl8w5xlRtQTwZFLEb_0saCH3FCIkLXcHkNxs&expires=1706590800",
                        "youtube_prerecorded_link": "",
                        "youtube_prerecorded_id": "",
                        "bunny_prerecorded_link": "",
                        "bunny_prerecorded_subtitles": ""
                    },
                    {
                        "slot_id": "w-mercado-8642",
                        "session_id": "w13",
                        "title": "Combining Voice and Gesture for Presenting Data to Remote Audiences",
                        "contributors": [
                            "Arjun Srinivasan"
                        ],
                        "authors": [
                            "Arjun Srinivasan, Matthew Brehmer"
                        ],
                        "abstract": "We consider the combination of voice commands with touchless bimanual gestures performed during presentations about data delivered via teleconference applications. Our demonstration extends recent work that considers the latter interaction modality in a presentation environment where charts can be composited over live webcam video, charts that dynamically respond to the presenter\u2019s operational (i.e., functional and deictic) hand gestures. In complementing these gestures with voice commands, new functionality is unlocked: the ability to precisely filter, sort, and highlight subsets in the data. While these abilities provide presenters with more flexibility in terms of presentation linearity and the capacity for responding to audience questions, imperative voice commands can come across to audiences as stilted or unnatural, and may be distracting.",
                        "uid": "w-mercado-8642",
                        "time_stamp": "2023-10-21T22:00:00Z",
                        "time_start": "2023-10-21T22:00:00Z",
                        "time_end": "2023-10-22T01:00:00Z",
                        "paper_type": "workshop",
                        "keywords": [],
                        "doi": "",
                        "fno": "",
                        "has_image": true,
                        "has_pdf": true,
                        "paper_award": "",
                        "image_caption": "Six moments in a remote presentation about American post-secondary institutions and their admission statistics, in which the presenter appears behind a semi-transparent unit chart composited in the foreground. The presenter ephemerally selects and highlights categories and items in the chart via pointing. Meanwhile, utterances forming part of the presenter's spoken monologue trigger the filtering, sorting, and aggregating of the data. See the supplemental video to watch the 3-minute presentation.",
                        "external_paper_link": "",
                        "youtube_ff_link": "",
                        "youtube_ff_id": "",
                        "bunny_ff_link": "",
                        "bunny_ff_subtitles": "",
                        "youtube_prerecorded_link": "",
                        "youtube_prerecorded_id": "",
                        "bunny_prerecorded_link": "",
                        "bunny_prerecorded_subtitles": ""
                    },
                    {
                        "slot_id": "w-mercado-5242",
                        "session_id": "w13",
                        "title": "Hanstreamer: an Open-source Webcam-based Live Data Presentation System",
                        "contributors": [
                            "Maxime Cordeil"
                        ],
                        "authors": [
                            "Adrian Kristanto, Maxime Cordeil, Benjamin Tag, Nathalie Riche, Tim Dwyer"
                        ],
                        "abstract": "We present Hanstreamer, a free and open-source system for webcam-based data presentation. The system performs real-time gesture recognition on the user\u2019s webcam video stream to provide interactive data visuals. Apart from standard chart and map visuals, Hanstreamer is the first such video data presentation system to support network visualisation and interactive DimpVis-style time-series data exploration. The system is ready for use with popular online meeting software such as Zoom and Microsoft Teams.",
                        "uid": "w-mercado-5242",
                        "time_stamp": "2023-10-21T22:00:00Z",
                        "time_start": "2023-10-21T22:00:00Z",
                        "time_end": "2023-10-22T01:00:00Z",
                        "paper_type": "workshop",
                        "keywords": [],
                        "doi": "",
                        "fno": "",
                        "has_image": false,
                        "has_pdf": true,
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "youtube_ff_link": "",
                        "youtube_ff_id": "",
                        "bunny_ff_link": "",
                        "bunny_ff_subtitles": "",
                        "youtube_prerecorded_link": "",
                        "youtube_prerecorded_id": "",
                        "bunny_prerecorded_link": "",
                        "bunny_prerecorded_subtitles": ""
                    },
                    {
                        "slot_id": "w-mercado-1322",
                        "session_id": "w13",
                        "title": "CommunityClick-Virtual: Multi-Modal Interactions for Enhancing Participation in Virtual Meetings",
                        "contributors": [
                            "Narges Mahyar"
                        ],
                        "authors": [
                            "Mahmood Jasim, Ali Sarvghad, Narges Mahyar"
                        ],
                        "abstract": "Government officials often rely on public engagements to gauge people's perspectives on civic issues and gather feedback to make informed policy decisions. Traditional public engagement methods are often face-to-face, such as town halls, public forums, and workshops. However, during the COVID-19 pandemic, these approaches were rendered ineffective due to health risks and the engagement process saw a shift towards virtual meetings. While accessible to a broader audience, virtual public meetings introduced challenges around limited time and opportunity for attendees to share feedback. Furthermore, attendees were often required to identify themselves, potentially discouraging reticent attendees from speaking up and risking confrontations with other attendees. To mitigate this issue, we designed and developed CommunityClick-Virtual, a multi-modal companion web application that allows virtual meeting participants to provide feedback on meeting discussions silently and anonymously using six customizable options or through chat messages without the need to speak up. The organizers have access to all attendee feedback channels where they can use synchronized coordinated visualizations to gather a more holistic understanding of people's perspectives. The field deployments of CommunityClick-Virtual demonstrated its efficacy in increasing participation and enabling organizers to identify insights that could help them make more informed decisions.",
                        "uid": "w-mercado-1322",
                        "time_stamp": "2023-10-21T22:00:00Z",
                        "time_start": "2023-10-21T22:00:00Z",
                        "time_end": "2023-10-22T01:00:00Z",
                        "paper_type": "workshop",
                        "keywords": [],
                        "doi": "",
                        "fno": "",
                        "has_image": true,
                        "has_pdf": true,
                        "paper_award": "",
                        "image_caption": "A snapshot of CommunityClick-Virtual\u2019s real-time component provides organizers with quick statistics, a set of real-time visualizations to track attendee responses, including a bar chart that shows the number of reactions for each feedback category alongside the unique number of attendees who generated those reactions, accumulated and interval bar and line charts to track how attendees have been providing feedback to  the meeting discussion. Finally, the organizers have access to CMS controls on the attendee visualization and the chat option.",
                        "external_paper_link": "",
                        "youtube_ff_link": "",
                        "youtube_ff_id": "",
                        "bunny_ff_link": "",
                        "bunny_ff_subtitles": "",
                        "youtube_prerecorded_link": "",
                        "youtube_prerecorded_id": "",
                        "bunny_prerecorded_link": "",
                        "bunny_prerecorded_subtitles": ""
                    },
                    {
                        "slot_id": "w-mercado-8593",
                        "session_id": "w13",
                        "title": "Asymmetric Immersive Presentation System for Financial Data Visualization",
                        "contributors": [
                            "David Saffo"
                        ],
                        "authors": [
                            "Matt Gottsacker, Mengyu Chen, David Saffo, Feiyu Lu, Blair MacIntyre"
                        ],
                        "abstract": "This paper presents the current design of a work-in-progress system for giving engaging, immersive presentations based on financial data in an immersive environment. The system consists of a presenter controlling the presentation flow through a tablet interface and an audience experiencing the presentation content through an augmented reality (AR) head-worn display (HWD). We discuss the user scenario motivating our system design, its associated design considerations, the high level features of the system being built, and how a system such as this can be extended to other presentation contexts.",
                        "uid": "w-mercado-8593",
                        "time_stamp": "2023-10-21T22:00:00Z",
                        "time_start": "2023-10-21T22:00:00Z",
                        "time_end": "2023-10-22T01:00:00Z",
                        "paper_type": "workshop",
                        "keywords": [],
                        "doi": "",
                        "fno": "",
                        "has_image": false,
                        "has_pdf": true,
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "youtube_ff_link": "",
                        "youtube_ff_id": "",
                        "bunny_ff_link": "",
                        "bunny_ff_subtitles": "",
                        "youtube_prerecorded_link": "",
                        "youtube_prerecorded_id": "",
                        "bunny_prerecorded_link": "",
                        "bunny_prerecorded_subtitles": ""
                    },
                    {
                        "slot_id": "w-mercado-2570",
                        "session_id": "w13",
                        "title": "Echoes in the Gallery: A Collaborative Immersive Analytics System for Analyzing Audience Reactions in Virtual Reality Exhibitions",
                        "contributors": [
                            "Linping Yuan"
                        ],
                        "authors": [
                            "Linping Yuan, Wai Tong, Kentaro Takahira, Zikai Wen, Yalong Yang, Huamin Qu"
                        ],
                        "abstract": "Virtual reality (VR) has enriched the exhibition experience, yet curators and artists often face challenges in curation due to disagreements in the spatial arrangement of exhibits. Although VR applications permit audiences to express their reactions, these multimodal and semantically rich data remain underutilized in spatial arrangement deliberations. Based on insights from semi-structured interviews with curators and artists, we outline design requirements to enhance collaboration in analyzing visitors' reactions, focusing on traffic flow analysis, angle-induced reactions, what-if analysis, and cross-view annotations. To meet these requirements, we propose a novel collaborative immersive analytic system that visualizes audience flow and individual reactions within VR spaces, facilitating more effective communication and decision-making between artists and curators. Future research will explore advanced visual design techniques to address scalability issues and in-place spatial comparison. We will also finish the development and conduct usability studies.",
                        "uid": "w-mercado-2570",
                        "time_stamp": "2023-10-21T22:00:00Z",
                        "time_start": "2023-10-21T22:00:00Z",
                        "time_end": "2023-10-22T01:00:00Z",
                        "paper_type": "workshop",
                        "keywords": [],
                        "doi": "",
                        "fno": "",
                        "has_image": false,
                        "has_pdf": true,
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "youtube_ff_link": "",
                        "youtube_ff_id": "",
                        "bunny_ff_link": "",
                        "bunny_ff_subtitles": "",
                        "youtube_prerecorded_link": "",
                        "youtube_prerecorded_id": "",
                        "bunny_prerecorded_link": "",
                        "bunny_prerecorded_subtitles": ""
                    }
                ]
            }
        ]
    },
    "w-visxvision": {
        "event": "VisxVision: Workshop on Novel Directions in Vision Science and Visualization Research",
        "long_name": "VisxVision: Workshop on Novel Directions in Vision Science and Visualization Research",
        "event_type": "workshop",
        "event_prefix": "w-visxvision",
        "event_description": "",
        "event_url": "",
        "organizers": [
            "Ghulam Jilani Quadri",
            "Clementine Zimnicki",
            "Racquel Fygenson",
            "Madeline Awad",
            "Ouxun Jiang"
        ],
        "sessions": [
            {
                "title": "VisxVision: Workshop on Novel Directions in Vision Science and Visualization Research",
                "session_id": "w14",
                "event_prefix": "w-visxvision",
                "track": "oneohnine",
                "session_image": "w14.png",
                "chair": [
                    "Ghulam Jilani Quadri",
                    "Clementine Zimnicki",
                    "Racquel Fygenson",
                    "Madeline Awad",
                    "Ouxun Jiang"
                ],
                "time_start": "2023-10-23T03:00:00Z",
                "time_end": "2023-10-23T06:00:00Z",
                "discord_category": "",
                "discord_channel": "109",
                "discord_channel_id": "1161741434647482379",
                "discord_link": "https://discordapp.com/channels/1161400451619627068/1161741434647482379",
                "zoom_private_meeting": "",
                "zoom_private_password": "",
                "zoom_private_link": "",
                "zoom_broadcast_link": "",
                "ff_link": "",
                "time_slots": [
                    {
                        "slot_id": "w-visxvision-1015",
                        "session_id": "w14",
                        "title": "Adjusting Point Size to Facilitate More Accurate Correlation Perception in Scatterplots",
                        "contributors": [
                            "Mr Gabriel Strain"
                        ],
                        "authors": [
                            "Gabriel Strain, Andrew James Stewart, Paul A Warren, Caroline Jay"
                        ],
                        "abstract": "Viewers consistently underestimate correlation in positively correlated scatterplots. We use a novel data point size manipulation to correct for this bias. In a high-powered and fully reproducible study, we demonstrate that decreasing the size of a point on a scatterplot as a function of its distance from the regression line is able to correct for a systematic perceptual bias long present in the literature. We recommend the implementation of our technique when designing scatterplots that aim to communicate positive correlations.",
                        "uid": "w-visxvision-1015",
                        "time_stamp": "2023-10-23T03:00:00Z",
                        "time_start": "2023-10-23T03:00:00Z",
                        "time_end": "2023-10-23T06:00:00Z",
                        "paper_type": "workshop",
                        "keywords": [],
                        "doi": "",
                        "fno": "",
                        "has_image": true,
                        "has_pdf": true,
                        "paper_award": "",
                        "image_caption": "Examples of the experimental stimuli used, with an r value of 0.6. When 150 participants were asked to rate correlation in scatterplots, they were most accurate when the non-linear decay condition was used across a range of 45 r values.",
                        "external_paper_link": "",
                        "youtube_ff_link": "",
                        "youtube_ff_id": "",
                        "bunny_ff_link": "",
                        "bunny_ff_subtitles": "",
                        "youtube_prerecorded_link": "",
                        "youtube_prerecorded_id": "",
                        "bunny_prerecorded_link": "",
                        "bunny_prerecorded_subtitles": ""
                    },
                    {
                        "slot_id": "w-visxvision-1020",
                        "session_id": "w14",
                        "title": "Can AI Mitigate Human Perceptual Biases?  A Pilot Study",
                        "contributors": [
                            "Jian Chen"
                        ],
                        "authors": [],
                        "abstract": "",
                        "uid": "w-visxvision-1020",
                        "time_stamp": "2023-10-23T03:00:00Z",
                        "time_start": "2023-10-23T03:00:00Z",
                        "time_end": "2023-10-23T06:00:00Z",
                        "paper_type": "workshop",
                        "keywords": [],
                        "doi": "",
                        "fno": "",
                        "has_image": false,
                        "has_pdf": false,
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "youtube_ff_link": "",
                        "youtube_ff_id": "",
                        "bunny_ff_link": "",
                        "bunny_ff_subtitles": "",
                        "youtube_prerecorded_link": "",
                        "youtube_prerecorded_id": "",
                        "bunny_prerecorded_link": "",
                        "bunny_prerecorded_subtitles": ""
                    }
                ]
            }
        ]
    },
    "w-visxprov": {
        "event": "(Vis + Prov) x Domain: Workshop on Visualization and Provenance Across Domains",
        "long_name": "(Vis + Prov) x Domain: Workshop on Visualization and Provenance Across Domains",
        "event_type": "workshop",
        "event_prefix": "w-visxprov",
        "event_description": "",
        "event_url": "",
        "organizers": [
            "Kai Xu",
            "Michelle Dowling",
            "John Wenskovitch",
            "Yilin Xia",
            "Jeremy E Block"
        ],
        "sessions": [
            {
                "title": "(Vis + Prov) x Domain: Workshop on Visualization and Provenance Across Domains",
                "session_id": "w15",
                "event_prefix": "w-visxprov",
                "track": "oneten",
                "session_image": "w15.png",
                "chair": [
                    "Kai Xu",
                    "Michelle Dowling",
                    "John Wenskovitch",
                    "Yilin Xia",
                    "Jeremy E Block"
                ],
                "time_start": "2023-10-22T22:00:00Z",
                "time_end": "2023-10-23T01:00:00Z",
                "discord_category": "",
                "discord_channel": "110",
                "discord_channel_id": "1161741529434566686",
                "discord_link": "https://discordapp.com/channels/1161400451619627068/1161741529434566686",
                "zoom_private_meeting": "",
                "zoom_private_password": "",
                "zoom_private_link": "",
                "zoom_broadcast_link": "",
                "ff_link": "",
                "time_slots": [
                    {
                        "slot_id": "w-visxprov-1003",
                        "session_id": "w15",
                        "title": "Modeling the Dashboard Provenance",
                        "contributors": [
                            "Johne Marcus Jarske"
                        ],
                        "authors": [
                            "Johne Marcus Jarske",
                            "Jorge Rady de Almeida J\u00fanior",
                            "Lucia Vilela Leite Filgueiras",
                            "Leandro Manuel Reis Velloso",
                            "T\u00e2nia Let\u00edcia Let\u00edcia Santos dos"
                        ],
                        "abstract": "Organizations of all kinds, whether public or private, profit-driven or non-profit, and across various industries and sectors, rely on dashboards for effective data visualization. However, the reliability and efficacy of these dashboards rely on the quality of the visual and data they present. Studies show that less than a quarter of dashboards provide information about their sources, which is just one of the expected metadata when provenance is seriously considered. Provenance is a record that describes people, organizations, entities, and activities that had a role in the production, influence, or delivery of a piece of data or an object. This paper aims to provide a provenance representation model, that entitles standardization, modeling, generation, capture, and visualization, specifically designed for dashboards and its visual and data components. The proposed model will offer a comprehensive set of essential provenance metadata that enables users to evaluate the quality, consistency, and reliability of the information presented on dashboards. This will allow a clear and precise understanding of the context in which a specific dashboard was developed, ultimately leading to better decision-making.",
                        "uid": "w-visxprov-1003",
                        "time_stamp": "2023-10-22T22:00:00Z",
                        "time_start": "2023-10-22T22:00:00Z",
                        "time_end": "2023-10-23T01:00:00Z",
                        "paper_type": "workshop",
                        "keywords": [],
                        "doi": "",
                        "fno": "",
                        "has_image": true,
                        "has_pdf": true,
                        "paper_award": "",
                        "image_caption": "Provenance Dashboard Model to improve reliability and effectiveness of a dashboard.  Organizations across various sectors rely on dashboards for effective data visualization. Nevertheless, the reliability and effectiveness of these dashboards hinge on the quality of the visual elements and data they present. But how can we ensure the reliability and effectiveness of a dashboard?  By integrating provenance into dashboards, users can significantly improve their decision-making processes by gaining a more comprehensive understanding of the dashboard's context and reliability. Provenance-driven dashboards empower users to explore a richer narrative of the data and visualizations, unveiling their meticulously curated history.",
                        "external_paper_link": "",
                        "youtube_ff_link": "",
                        "youtube_ff_id": "",
                        "bunny_ff_link": "https://ieeevis-uploads.b-cdn.net/vis23/w-visxprov/w-visxprov-1003/w-visxprov-1003_Preview.mp4?token=dFeR9flyvuthspaf7fEcggdDk8i8dXcmiyshGs-LG3Y&expires=1706590800",
                        "bunny_ff_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/w-visxprov/w-visxprov-1003/w-visxprov-1003_Preview.vtt?token=v5taZxUqI984q_O559yF27x22URnBRXqiSU1HbfrJFI&expires=1706590800",
                        "youtube_prerecorded_link": "https://youtu.be/ldi6mIu5GcM",
                        "youtube_prerecorded_id": "ldi6mIu5GcM",
                        "bunny_prerecorded_link": "https://ieeevis-uploads.b-cdn.net/vis23/w-visxprov/w-visxprov-1003/w-visxprov-1003_Presentation.mp4?token=Ds-uBoLLKbCpXz5o-1SdjUgVV3gFt8D6PmOTJjnNTYs&expires=1706590800",
                        "bunny_prerecorded_subtitles": "https://ieeevis-uploads.b-cdn.net/vis23/w-visxprov/w-visxprov-1003/w-visxprov-1003_Presentation.vtt?token=jJI9L3i69DfOIC56DeCVAFHfl7OEVZfZ3IMMn8D1rFg&expires=1706590800"
                    },
                    {
                        "slot_id": "w-visxprov-1002",
                        "session_id": "w15",
                        "title": "Visualising category recoding and numeric redistributions",
                        "contributors": [
                            "Cynthia A Huang"
                        ],
                        "authors": [
                            "Cynthia A Huang"
                        ],
                        "abstract": "This paper proposes graphical representations of data and rationale provenance in workflows that convert both category labels and associated numeric data between distinct but semantically related taxonomies. We motivate the graphical representations with a new task abstraction, the cross-taxonomy transformation, and associated graph-based information structure, the crossmap. The task abstraction supports the separation of category recoding and numeric redistribution decisions from the specifics of data manipulation in ex-post data harmonisation. The crossmap structure is illustrated using an example conversion of numeric statistics from a country-specific taxonomy to an international classification standard. We discuss the opportunities and challenges of using visualisation to audit and communicate cross-taxonomy transformations and present candidate graphical representations.",
                        "uid": "w-visxprov-1002",
                        "time_stamp": "2023-10-22T22:00:00Z",
                        "time_start": "2023-10-22T22:00:00Z",
                        "time_end": "2023-10-23T01:00:00Z",
                        "paper_type": "workshop",
                        "keywords": [],
                        "doi": "",
                        "fno": "",
                        "has_image": true,
                        "has_pdf": true,
                        "paper_award": "",
                        "image_caption": "A crossmap for converting values observed using Australian occupation categories (ANZSCO22) into observations under the International Standard Classification of Occupations (ISCO8). Crossmaps are directed multipartite graph structures for capturing details of cross-taxonomy transformation. This crossmap is visualised using a two-layer bigraph layout, with ANZSCO22 codes forming the first source layer, and ISCO8 codes the second target layer. The weights on the links indicate what share of ANZSCO22 observed values are redistributed to corresponding ISCO8 codes.",
                        "external_paper_link": "",
                        "youtube_ff_link": "",
                        "youtube_ff_id": "",
                        "bunny_ff_link": "",
                        "bunny_ff_subtitles": "",
                        "youtube_prerecorded_link": "",
                        "youtube_prerecorded_id": "",
                        "bunny_prerecorded_link": "",
                        "bunny_prerecorded_subtitles": ""
                    }
                ]
            }
        ]
    },
    "a-visinpractice": {
        "event": "VisInPractice",
        "long_name": "VisInPractice",
        "event_type": "associated",
        "event_prefix": "a-visinpractice",
        "event_description": "",
        "event_url": "",
        "organizers": [
            "Ayan Biswas"
        ],
        "sessions": [
            {
                "title": "VisInPractice",
                "session_id": "ae1",
                "event_prefix": "a-visinpractice",
                "track": "oneohone",
                "session_image": "ae1.png",
                "chair": [
                    "Alexander Bock",
                    "Ayan Biswas"
                ],
                "time_start": "2023-10-22T22:00:00Z",
                "time_end": "2023-10-23T01:00:00Z",
                "discord_category": "",
                "discord_channel": "101-102",
                "discord_channel_id": "1161740777530069092",
                "discord_link": "https://discordapp.com/channels/1161400451619627068/1161740777530069092",
                "zoom_private_meeting": "",
                "zoom_private_password": "",
                "zoom_private_link": "",
                "zoom_broadcast_link": "",
                "ff_link": "",
                "time_slots": []
            }
        ]
    },
    "a-vizsec": {
        "event": "VizSec",
        "long_name": "VizSec",
        "event_type": "associated",
        "event_prefix": "a-vizsec",
        "event_description": "",
        "event_url": "",
        "organizers": [
            "Lyndsey Franklin",
            "Xumeng Wang",
            "Aritra Dasgupta",
            "Adrian Komandina",
            "Kuhu Gupta"
        ],
        "sessions": [
            {
                "title": "VizSec",
                "session_id": "ae4",
                "event_prefix": "a-vizsec",
                "track": "oneohone",
                "session_image": "ae4.png",
                "chair": [
                    "Lyndsey Franklin",
                    "Xumeng Wang",
                    "Aritra Dasgupta",
                    "Adrian Komandina",
                    "Kuhu Gupta"
                ],
                "time_start": "2023-10-22T03:00:00Z",
                "time_end": "2023-10-22T06:00:00Z",
                "discord_category": "",
                "discord_channel": "101-102",
                "discord_channel_id": "1161740777530069092",
                "discord_link": "https://discordapp.com/channels/1161400451619627068/1161740777530069092",
                "zoom_private_meeting": "",
                "zoom_private_password": "",
                "zoom_private_link": "",
                "zoom_broadcast_link": "",
                "ff_link": "",
                "time_slots": [
                    {
                        "slot_id": "a-vizsec-1350",
                        "session_id": "ae4",
                        "title": "Vis-SAGA: Visual Analytics for Situational Awareness of Grid Anomalies",
                        "contributors": [
                            "Graham Johnson"
                        ],
                        "authors": [
                            "Graham Johnson",
                            "Kenny Gruchalla",
                            "Michael Ingram",
                            "Nalinrat Guba",
                            "Robert Cruickshank",
                            "Scott Caruso"
                        ],
                        "abstract": "We describe supporting near real-time situational awareness of the electric distribution system by visualizing novel data from voltage sensors deployed on existing broadband cable television network equipment. Our scalable web-based visual analytics platform supports interactive geospatial exploration, time-series analysis, and summarization of grid behavior during potentially anomalous events. The broadband cable television sensor network provides observability of the electrical distribution system at a higher local spatial resolution than is typically available to most utilities, revealing the operational state of the network and aiding in the detection of abnormal behaviors or deviations from expected patterns, particularly across electric utility service areas. We outline the design and development of interactive geospatial and time-series visualization components and the scalable data services that supply metadata, historical, and real-time streams of sensor data across the network. We present our platform during periods of extreme weather, demonstrating its ability to assist in detecting patterns of operation that affect power availability, quality, resiliency, and service restoration.",
                        "uid": "a-vizsec-1350",
                        "time_stamp": "2023-10-22T03:00:00Z",
                        "time_start": "2023-10-22T03:00:00Z",
                        "time_end": "2023-10-22T06:00:00Z",
                        "paper_type": "associated",
                        "keywords": [],
                        "doi": "",
                        "fno": "",
                        "has_image": false,
                        "has_pdf": true,
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "youtube_ff_link": "",
                        "youtube_ff_id": "",
                        "bunny_ff_link": "",
                        "bunny_ff_subtitles": "",
                        "youtube_prerecorded_link": "",
                        "youtube_prerecorded_id": "",
                        "bunny_prerecorded_link": "",
                        "bunny_prerecorded_subtitles": ""
                    },
                    {
                        "slot_id": "a-vizsec-5220",
                        "session_id": "ae4",
                        "title": "FuzzPlanner: Visually Assisting the Design of Firmware Fuzzing Campaigns",
                        "contributors": [
                            "Alessio Izzillo"
                        ],
                        "authors": [
                            "Emilio Coppa",
                            "Alessio Izzillo",
                            "Riccardo Lazzeretti",
                            "Simone Lenti"
                        ],
                        "abstract": "Embedded devices are pivotal in many aspects to our everyday life, acting as key elements within our critical infrastructures, e-health sector, and the IoT ecosystem. These devices ship with custom software, dubbed firmware, whose development may not have followed strict security-by-design guidelines and for which no detailed documentation may be available. Given their critical role, testing their software before deploying them is crucial. Software fuzzing is a popular software testing technique that has shown to be quite effective in the last decade. However, the firmware may contain thousands of subcomponents with unexpected interplays. Moreover, operators may have a tight time budget to perform a security evaluation, requiring focused fuzzing on the most critical subcomponents. Also, considering the lack of accurate documentation for a device, it is quite hard for a security operator to understand what to fuzz and how to fuzz a specific device firmware. In this paper, we present FuzzPlanner, a visual analytics solution that enables security operators during the design of a fuzzing campaign over a device firmware. FuzzPlanner helps the operator identify the best candidates for fuzzing using several innovative visual aids. Our contributions include introducing FuzzPlanner, exploring diverse analytical tools to pinpoint critical binaries, and showing its efficacy with two real-world firmware image scenarios.",
                        "uid": "a-vizsec-5220",
                        "time_stamp": "2023-10-22T03:00:00Z",
                        "time_start": "2023-10-22T03:00:00Z",
                        "time_end": "2023-10-22T06:00:00Z",
                        "paper_type": "associated",
                        "keywords": [],
                        "doi": "",
                        "fno": "",
                        "has_image": true,
                        "has_pdf": true,
                        "paper_award": "",
                        "image_caption": "FuzzPlanner is a visual tool that assists security operators in designing fuzzing campaigns for device firmware. It employs dynamic analysis to monitor inter-binary data interactions and process interactions, collecting information about firmware binary components. Operators utilize this information to prioritize their testing efforts, as they often need to conduct security assessments within tight timeframes, such as a week. This underscores the importance of efficient fuzzing campaign design due to the time-consuming nature of fuzzing.",
                        "external_paper_link": "",
                        "youtube_ff_link": "",
                        "youtube_ff_id": "",
                        "bunny_ff_link": "",
                        "bunny_ff_subtitles": "",
                        "youtube_prerecorded_link": "",
                        "youtube_prerecorded_id": "",
                        "bunny_prerecorded_link": "",
                        "bunny_prerecorded_subtitles": ""
                    },
                    {
                        "slot_id": "a-vizsec-7061",
                        "session_id": "ae4",
                        "title": "Exploring the Representation of Cyber-Risk Data Through Sketching",
                        "contributors": [
                            "Mr Thomas Miller"
                        ],
                        "authors": [
                            "Thomas Miller",
                            "Miriam Sturdee",
                            "Daniel Prince"
                        ],
                        "abstract": "Dealing with complex information regarding cyber security risks is increasingly important as attacks rise in frequency. Visualisation techniques are used to support decision-making and insight. However, the use of visualisations across different stakeholders and cyber security risk data is not well explored. This work presents an exploratory study in which participants use sketching to represent cyber-risk data. We critically discuss the method and our results demonstrate the usefulness of the method to identify new, diverse visualisation approaches, as well as the richness of stakeholder visualisation conceptualisation.",
                        "uid": "a-vizsec-7061",
                        "time_stamp": "2023-10-22T03:00:00Z",
                        "time_start": "2023-10-22T03:00:00Z",
                        "time_end": "2023-10-22T06:00:00Z",
                        "paper_type": "associated",
                        "keywords": [],
                        "doi": "",
                        "fno": "",
                        "has_image": true,
                        "has_pdf": true,
                        "paper_award": "",
                        "image_caption": "A data representation continuum that represents data from numerical to abstract (left to right). Light-blue dots represent the categories populated by the originally develop continuum. Red  dots represent visualisation techniques identified in industry. Green dots represent participants\u2019 first sketches. Dark-blue dots represent  participants\u2019 second sketches",
                        "external_paper_link": "",
                        "youtube_ff_link": "",
                        "youtube_ff_id": "",
                        "bunny_ff_link": "",
                        "bunny_ff_subtitles": "",
                        "youtube_prerecorded_link": "",
                        "youtube_prerecorded_id": "",
                        "bunny_prerecorded_link": "",
                        "bunny_prerecorded_subtitles": ""
                    },
                    {
                        "slot_id": "a-vizsec-8322",
                        "session_id": "ae4",
                        "title": "PassViz: An Interactive Visualisation System for Analysing Leaked Passwords",
                        "contributors": [
                            "Mr Samuel Charles Parker"
                        ],
                        "authors": [
                            "Samuel Charles Parker",
                            "Haiyue Yuan",
                            "Shujun Li"
                        ],
                        "abstract": "Passwords remain the most widely used form of user authentication, despite advancements in other methods. However, their limitations, such as susceptibility to attacks, especially weak passwords defined by human users, are well-documented. The existence of weak human-defined passwords has led to repeated password leaks from websites, many of which are of large scale. While such password leaks are unfortunate security incidents, they provide security researchers and practitioners with good opportunities to learn valuable insights from such leaked passwords, in order to identify ways to improve password policies and other security controls on passwords. Researchers have proposed different data visualisation techniques to help analyse leaked passwords. However, many approaches rely solely on frequency analysis, with limited exploration of distance-based graphs. This paper reports PassViz, a novel method that combines the edit distance with the t-SNE (t-distributed stochastic neighbour embedding) dimensionality reduction algorithm [19] for visualising and analysing leaked passwords in a 2-D space. We implemented PassViz as an easy-to-use command-line tool for visualising large-scale password databases, and also as a graphical user interface (GUI) to support interactive visual analytics of small password databases. Using the \u201c000webhost\u201d leaked database as an example, we show how PassViz can be used to visually analyse different aspects of leaked passwords and to facilitate the discovery of previously unknown password patterns. Overall, our approach empowers researchers and practitioners to gain valuable insights and improve password security through effective data visualisation and analysis.",
                        "uid": "a-vizsec-8322",
                        "time_stamp": "2023-10-22T03:00:00Z",
                        "time_start": "2023-10-22T03:00:00Z",
                        "time_end": "2023-10-22T06:00:00Z",
                        "paper_type": "associated",
                        "keywords": [],
                        "doi": "",
                        "fno": "",
                        "has_image": true,
                        "has_pdf": true,
                        "paper_award": "",
                        "image_caption": "Examples of visualisation of different clusters for 000webhost leaked password database",
                        "external_paper_link": "",
                        "youtube_ff_link": "",
                        "youtube_ff_id": "",
                        "bunny_ff_link": "",
                        "bunny_ff_subtitles": "",
                        "youtube_prerecorded_link": "",
                        "youtube_prerecorded_id": "",
                        "bunny_prerecorded_link": "",
                        "bunny_prerecorded_subtitles": ""
                    },
                    {
                        "slot_id": "a-vizsec-8611",
                        "session_id": "ae4",
                        "title": "Visualizing Comparisons of Bill of Materials",
                        "contributors": [
                            "Rebecca Jones"
                        ],
                        "authors": [
                            "Rebecca Jones",
                            "Lucas Tate"
                        ],
                        "abstract": "The complexity of distributed manufacturing and software development coupled with the increasing prevalence of cyber and supply chain attacks necessitates a greater understanding of the hardware and software components that comprise equipment in critical infrastructure. When a vulnerability in a single software library can have disastrous consequences, being able to identify where that library may exist in equipment or software becomes a prerequisite for protecting the overall infrastructure. This need has sparked a large effort around the development and incorporation of bill-of-materials(BOM) into security, asset management, and procurement practices to aid in mitigating, and responding to future attacks. While much of the current research is devoted to creating BOMs, it is equally important to develop methods for comparing them to answer questions, such as: How has my software changed? Are two pieces of equipment equivalent? Does this piece of equipment that just arrived match my historical information? In this work, we demonstrate how BOMs can be represented by graph structures.  We then describe how these structures can be fed into a graph comparison algorithm to produce a novel interactive visualization that allows us to not only identify differences in BOMs but show exactly where they are in the product.",
                        "uid": "a-vizsec-8611",
                        "time_stamp": "2023-10-22T03:00:00Z",
                        "time_start": "2023-10-22T03:00:00Z",
                        "time_end": "2023-10-22T06:00:00Z",
                        "paper_type": "associated",
                        "keywords": [],
                        "doi": "",
                        "fno": "",
                        "has_image": false,
                        "has_pdf": true,
                        "paper_award": "",
                        "image_caption": "",
                        "external_paper_link": "",
                        "youtube_ff_link": "",
                        "youtube_ff_id": "",
                        "bunny_ff_link": "",
                        "bunny_ff_subtitles": "",
                        "youtube_prerecorded_link": "",
                        "youtube_prerecorded_id": "",
                        "bunny_prerecorded_link": "",
                        "bunny_prerecorded_subtitles": ""
                    }
                ]
            }
        ]
    },
    "t-taurus": {
        "event": "TAURUS: a unified framework for creating graph layouts",
        "long_name": "TAURUS: a unified framework for creating graph layouts",
        "event_type": "tutorial",
        "event_prefix": "t-taurus",
        "event_description": "",
        "event_url": "",
        "organizers": [
            "Yunhai Wang"
        ],
        "sessions": [
            {
                "title": "TAURUS: a unified framework for creating graph layouts",
                "session_id": "t3",
                "event_prefix": "t-taurus",
                "track": "oneohone",
                "session_image": "t3.png",
                "chair": [
                    "Yunhai Wang"
                ],
                "time_start": "2023-10-23T03:00:00Z",
                "time_end": "2023-10-23T06:00:00Z",
                "discord_category": "",
                "discord_channel": "101-102",
                "discord_channel_id": "1161740777530069092",
                "discord_link": "https://discordapp.com/channels/1161400451619627068/1161740777530069092",
                "zoom_private_meeting": "",
                "zoom_private_password": "",
                "zoom_private_link": "",
                "zoom_broadcast_link": "",
                "ff_link": "",
                "time_slots": []
            }
        ]
    },
    "t-network": {
        "event": "Mining Useful Information Via Complex Network Visualization",
        "long_name": "Mining Useful Information Via Complex Network Visualization",
        "event_type": "tutorial",
        "event_prefix": "t-network",
        "event_description": "",
        "event_url": "",
        "organizers": [
            "Sonali Agarwal",
            "Garima Jindal"
        ],
        "sessions": [
            {
                "title": "Mining Useful Information Via Complex Network Visualization",
                "session_id": "t4",
                "event_prefix": "t-network",
                "track": "oneohthree",
                "session_image": "t4.png",
                "chair": [
                    "Sonali Agarwal",
                    "Garima Jindal"
                ],
                "time_start": "2023-10-22T22:00:00Z",
                "time_end": "2023-10-23T01:00:00Z",
                "discord_category": "",
                "discord_channel": "103",
                "discord_channel_id": "1161741183815524502",
                "discord_link": "https://discordapp.com/channels/1161400451619627068/1161741183815524502",
                "zoom_private_meeting": "",
                "zoom_private_password": "",
                "zoom_private_link": "",
                "zoom_broadcast_link": "",
                "ff_link": "",
                "time_slots": []
            }
        ]
    },
    "t-empirical": {
        "event": "Transparent Practices for Quantitative Empirical Research",
        "long_name": "Transparent Practices for Quantitative Empirical Research",
        "event_type": "tutorial",
        "event_prefix": "t-empirical",
        "event_description": "",
        "event_url": "",
        "organizers": [
            "Abhraneel Sarma"
        ],
        "sessions": [
            {
                "title": "Transparent Practices for Quantitative Empirical Research",
                "session_id": "t6",
                "event_prefix": "t-empirical",
                "track": "oneohthree",
                "session_image": "t6.png",
                "chair": [
                    "Abhraneel Sarma"
                ],
                "time_start": "2023-10-23T03:00:00Z",
                "time_end": "2023-10-23T06:00:00Z",
                "discord_category": "",
                "discord_channel": "103",
                "discord_channel_id": "1161741183815524502",
                "discord_link": "https://discordapp.com/channels/1161400451619627068/1161741183815524502",
                "zoom_private_meeting": "",
                "zoom_private_password": "",
                "zoom_private_link": "",
                "zoom_broadcast_link": "",
                "ff_link": "",
                "time_slots": []
            }
        ]
    },
    "t-design": {
        "event": "Design Sprints for Visualization",
        "long_name": "Design Sprints for Visualization",
        "event_type": "tutorial",
        "event_prefix": "t-design",
        "event_description": "",
        "event_url": "",
        "organizers": [
            "Carolina Nobre"
        ],
        "sessions": [
            {
                "title": "Design Sprints for Visualization",
                "session_id": "t7",
                "event_prefix": "t-design",
                "track": "oneohfour",
                "session_image": "t7.png",
                "chair": [
                    "Carolina Nobre"
                ],
                "time_start": "2023-10-23T03:00:00Z",
                "time_end": "2023-10-23T06:00:00Z",
                "discord_category": "",
                "discord_channel": "104",
                "discord_channel_id": "1161741240665124954",
                "discord_link": "https://discordapp.com/channels/1161400451619627068/1161741240665124954",
                "zoom_private_meeting": "",
                "zoom_private_password": "",
                "zoom_private_link": "",
                "zoom_broadcast_link": "",
                "ff_link": "",
                "time_slots": []
            }
        ]
    },
    "t-nlp4vis": {
        "event": "NLP4Vis: Natural Language Processing for Information Visualization",
        "long_name": "NLP4Vis: Natural Language Processing for Information Visualization",
        "event_type": "tutorial",
        "event_prefix": "t-nlp4vis",
        "event_description": "",
        "event_url": "",
        "organizers": [
            "Enamul Hoque"
        ],
        "sessions": [
            {
                "title": "NLP4Vis: Natural Language Processing for Information Visualization",
                "session_id": "t8",
                "event_prefix": "t-nlp4vis",
                "track": "oneten",
                "session_image": "t8.png",
                "chair": [
                    "Enamul Hoque"
                ],
                "time_start": "2023-10-21T22:00:00Z",
                "time_end": "2023-10-22T01:00:00Z",
                "discord_category": "",
                "discord_channel": "110",
                "discord_channel_id": "1161741529434566686",
                "discord_link": "https://discordapp.com/channels/1161400451619627068/1161741529434566686",
                "zoom_private_meeting": "",
                "zoom_private_password": "",
                "zoom_private_link": "",
                "zoom_broadcast_link": "",
                "ff_link": "",
                "time_slots": []
            }
        ]
    },
    "w-visxai": {
        "event": "VISxAI",
        "long_name": "VISxAI",
        "event_type": "workshop",
        "event_prefix": "w-visxai",
        "event_description": "",
        "event_url": "",
        "organizers": [
            "Alex B\u00e4uerle",
            "Angie Boggust",
            "Fred Hohman",
            "Ian Johnson",
            "Zijie Jay Wang"
        ],
        "sessions": []
    }
}