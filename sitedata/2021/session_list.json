{
    "a-vds": {
        "event": "VDS",
        "long_name": "VDS",
        "event_type": "Associated Event",
        "event_description": "",
        "event_url": "",
        "sessions": [
            {
                "title": "Opening",
                "session_id": "a-vds-1",
                "track": "room1",
                "schedule_image": "a-vds-1.png",
                "chair": [
                    "Liang Gou",
                    "Junming Shao"
                ],
                "organizers": [
                    "Liang Guo"
                ],
                "time_start": "2021-10-24T13:00:00Z",
                "time_end": "2021-10-24T14:10:00Z",
                "discord_category": "",
                "discord_channel": "",
                "discord_channel_id": 0,
                "youtube_url": null,
                "zoom_meeting": "",
                "zoom_password": "",
                "ff_link": "",
                "ff_playlist": "",
                "time_slots": [
                    {
                        "type": "live",
                        "title": "Opening",
                        "contributors": [
                            "Adam Pier",
                            "Torsten Moeller"
                        ],
                        "abstract": null,
                        "time_start": "2021-10-24T13:00:00Z",
                        "time_end": "2021-10-24T14:30:00Z"
                    },
                    {
                        "type": "live",
                        "title": "Opening Keynote",
                        "contributors": [
                            "Pei Jian"
                        ],
                        "abstract": null,
                        "time_start": "2021-10-24T13:00:00Z",
                        "time_end": "2021-10-24T14:10:00Z"
                    }
                ]
            },
            {
                "title": "Papers and Closing",
                "session_id": "a-vds-2",
                "track": "room1",
                "schedule_image": "a-vds-2.png",
                "chair": [
                    "Alvitta Ottley",
                    "Liang Gou"
                ],
                "organizers": [
                    "Liang Guo"
                ],
                "time_start": "2021-10-24T14:25:00Z",
                "time_end": "2021-10-24T17:00:00Z",
                "discord_category": "",
                "discord_channel": "",
                "discord_channel_id": 0,
                "youtube_url": null,
                "zoom_meeting": "",
                "zoom_password": "",
                "ff_link": "",
                "ff_playlist": "",
                "time_slots": [
                    {
                        "type": "recorded",
                        "title": "CACTUS: Detecting and Resolving Conflicts in Objective Functions",
                        "contributors": [
                            "Subhajit Das"
                        ],
                        "abstract": "Machine learning (ML) models are constructed by expert ML practitioners using various coding languages, in which they tune and select model hyperparameters and learning algorithms for a given problem domain. In multi-objective optimization, conflicting objectives and constraints is a major area of concern. In such problems, several competing objectives are seen for which no single optimal solution is found that satisfies all desired objectives simultaneously. In the past, visual analytic (VA) systems have allowed users to interactively construct objective functions for a classifier. In this paper, we extend this line of work by prototyping a technique to visualize multi-objective objective functions either defined in a Jupyter notebook or defined using an interactive visual interface to help users to detect and resolve conflicting objectives. Visualization of the objective function enlightens potentially conflicting objectives that obstructs selecting correct solution(s) for the desired ML task or goal. We also present an enumeration of potential conflicts in objective specification in multi-objective objective functions for classifier selection. Furthermore, we demonstrate our approach in a VA system that helps users in specifying meaningful objective functions to a classifier by detecting and resolving conflicting objectives.",
                        "time_start": "2021-10-24T14:25:00Z",
                        "time_end": "2021-10-24T14:40:00Z",
                        "uid": "a-vds-11008",
                        "youtube_video_id": "fQNGfM71JzA"
                    },
                    {
                        "type": "recorded",
                        "title": "Graph Comparison for Causal Discovery",
                        "contributors": [
                            "Joseph Cottam"
                        ],
                        "abstract": "Reasoning about cause and effect is one of the frontiers for modern machine learning. Many causality techniques reason over a ``causal graph'' provided as input to the problem. When a causal graph cannot be produced from human expertise, ``causal discovery'' algorithms can be used to generate one from data. Unfortunately, causal discovery algorithms vary wildly in their results due to unrealistic data and modeling assumptions, so the results still need to be manually validated and adjusted. This paper presents a graph comparison tool designed to help analysts curate causal discovery results. This tool facilitates feedback loops whereby an analyst compares proposed graphs from multiple algorithms (or ensembles) and then uses insights from the comparison to refine parameters and inputs to the algorithms. We illustrate different types of comparisons and show how the interplay of causal discovery and graph comparison improves causal discovery.",
                        "time_start": "2021-10-24T14:40:00Z",
                        "time_end": "2021-10-24T14:55:00Z",
                        "uid": "a-vds-21006",
                        "youtube_video_id": "KXBNNNci23Y"
                    },
                    {
                        "type": "recorded",
                        "title": "Lodestar: Supporting Independent Learning and Rapid Experimentation Through Data-Driven Analysis Recommendations",
                        "contributors": [
                            "Deepthi Raghunandan"
                        ],
                        "abstract": "Keeping abreast of current trends, technologies, and best practices in visualization and data analysis is becoming increasingly difficult, especially for fledgling data scientists. In this paper, we propose Lodestar, an interactive computational notebook that allows users to quickly explore and construct new data science workflows by selecting from a list of automated analysis recommendations. We derive our recommendations from directed graphs of known analysis states, with two input sources: one manually curated from online data science tutorials, and another extracted through semi-automatic analysis of a corpus of over 6,000 Jupyter notebooks. We evaluate Lodestar in a formative study guiding our next set of improvements to the tool. The evaluation suggests that users find Lodestar useful for rapidly creating data science workflows.",
                        "time_start": "2021-10-24T14:55:00Z",
                        "time_end": "2021-10-24T15:10:00Z",
                        "uid": "a-vds-31005",
                        "youtube_video_id": "eAghILb6TaM"
                    },
                    {
                        "type": "recorded",
                        "title": "Natto: Rapid Visual Iteration of Analytic Data Models with Intelligent Assistance",
                        "contributors": [
                            "Anamaria Crisan"
                        ],
                        "abstract": "Data analysts need to routinely transform data into a form conducive for deeper investigation. While there exists a myriad of tools to support this task on tabular data, few tools exist to support analysts with more complex data types. In this study, we investigate how analysts process and transform large sets of XML data to create an analytic data model useful to further their analysis. We conduct a set of formative interviews with four experts that have diverse yet specialized knowledge of a common dataset. From these interviews, we derive a set of goals, tasks, and design requirements for transforming XML data into an analytic data model. We implement Natto as a proof-of-concept prototype that actualizes these design requirements into a set of visual and interaction design choices. We demonstrate the utility of the system through the presentation of analysis scenarios using real-world data. Our research contributes novel insights into the unique challenges of transforming data that is both hierarchical and internally linked. Further, it extends the knowledge of the visualization community in the areas of data preparation and wrangling.",
                        "time_start": "2021-10-24T15:10:00Z",
                        "time_end": "2021-10-24T15:25:00Z",
                        "uid": "a-vds-41007",
                        "youtube_video_id": "OUfxB9M7l2Y"
                    },
                    {
                        "type": "live",
                        "title": "Break",
                        "contributors": "",
                        "abstract": null,
                        "time_start": "2021-10-24T15:25:00Z",
                        "time_end": "2021-10-24T15:40:00Z"
                    },
                    {
                        "type": "live",
                        "title": "Closing Keynote",
                        "contributors": [
                            "Polo Chau"
                        ],
                        "abstract": null,
                        "time_start": "2021-10-24T15:40:00Z",
                        "time_end": "2021-10-24T16:40:00Z"
                    },
                    {
                        "type": "live",
                        "title": "Closing",
                        "contributors": [
                            "Liang Gou"
                        ],
                        "abstract": null,
                        "time_start": "2021-10-24T16:40:00Z",
                        "time_end": "2021-10-24T17:00:00Z"
                    }
                ]
            }
        ]
    },
    "a-vahc": {
        "event": "VAHC",
        "long_name": "VAHC",
        "event_type": "Associated Event",
        "event_description": "",
        "event_url": "",
        "sessions": [
            {
                "title": "Introduction",
                "session_id": "a-vahc-1",
                "track": "room2",
                "schedule_image": "a-vahc-1.png",
                "chair": [
                    "Annie T. Chen",
                    "Danny T.Y. Wu",
                    "J\u00fcrgen Bernard"
                ],
                "organizers": [
                    "J\u00fcrgen Bernard",
                    "Annie T. Chen",
                    "Danny T.Y. Wu"
                ],
                "time_start": "2021-10-24T13:00:00Z",
                "time_end": "2021-10-24T14:30:00Z",
                "discord_category": "",
                "discord_channel": "",
                "discord_channel_id": 0,
                "youtube_url": null,
                "zoom_meeting": "",
                "zoom_password": "",
                "ff_link": "",
                "ff_playlist": "",
                "time_slots": [
                    {
                        "type": "live",
                        "title": "Opening Ceremony",
                        "contributors": [
                            "J\u00fcrgen Bernard",
                            "Annie T. Chen",
                            "Danny T.Y. Wu"
                        ],
                        "abstract": null,
                        "time_start": "2021-10-24T13:00:00Z",
                        "time_end": "2021-10-24T13:15:00Z"
                    },
                    {
                        "type": "live",
                        "title": "Keynote",
                        "contributors": [
                            "Keynote Speaker missing"
                        ],
                        "abstract": null,
                        "time_start": "2021-10-24T13:15:00Z",
                        "time_end": "2021-10-24T14:05:00Z"
                    },
                    {
                        "type": "live",
                        "title": "Best Paper Award",
                        "contributors": [
                            "Paper Chairs"
                        ],
                        "abstract": null,
                        "time_start": "2021-10-24T14:05:00Z",
                        "time_end": "2021-10-24T14:10:00Z"
                    },
                    {
                        "type": "recorded",
                        "title": "Best Paper: A Multi-scale Visual Analytics Approach for Exploring Biomedical Knowledge",
                        "contributors": [
                            "Rosa Romero-G\u00f3mez"
                        ],
                        "abstract": "This paper describes an ongoing multi-scale visual analytics approach for exploring and analyzing biomedical knowledge at scale. We utilize global and local views, hierarchical and flow-based graph layouts, multi-faceted search, neighborhood recommendations, and document visualizations to help researchers interactively explore, query, and analyze biological graphs against the backdrop of biomedical knowledge. The generality of our approach - insofar as it re-quires only knowledge graphs linked to documents - means it can support a range of therapeutic use cases across different domains, from disease propagation to drug discovery. Early interactions with domain experts support our approach for use cases with graphs with over 40,000 nodes and 350,000 edges.",
                        "time_start": "2021-10-24T14:10:00Z",
                        "time_end": "2021-10-24T14:30:00Z",
                        "uid": "a-vahc-1007",
                        "youtube_video_id": "qd7I5chqopk"
                    }
                ]
            },
            {
                "title": "Clinical and Medical Decision Making",
                "session_id": "a-vahc-2",
                "track": "room2",
                "schedule_image": "a-vahc-2.png",
                "chair": [
                    "MISSING"
                ],
                "organizers": [
                    "J\u00fcrgen Bernard",
                    "Annie T. Chen",
                    "Danny T.Y. Wu"
                ],
                "time_start": "2021-10-24T15:00:00Z",
                "time_end": "2021-10-24T16:20:00Z",
                "discord_category": "",
                "discord_channel": "",
                "discord_channel_id": 0,
                "youtube_url": null,
                "zoom_meeting": "",
                "zoom_password": "",
                "ff_link": "",
                "ff_playlist": "",
                "time_slots": [
                    {
                        "type": "recorded",
                        "title": "Towards a Comprehensive \u2026",
                        "contributors": [
                            "Salmah Ahmad"
                        ],
                        "abstract": "This paper reports on a joint project with medical experts on inflammatory bowel disease (IBD). Patients suffering from IBD, e.g. Crohn\u2019s disease or ulcerative colitis, do not have a reduced life expectancy and disease progressions easily span several decades. We designed a visualization to highlight information that is vital for comparing patients and progressions, especially with respect to the treatments administered over the years. Medical experts can interactively determine the amount of information displayed and can synchronize the progressions to the beginning of certain treatments and medications. While the visualization was designed in close collaboration with IBD experts, we additionally evaluated our approach with 35 participants to ensure good usability and accessibility. The paper also highlights the future work on similarity definition and additional visual features in this on-going project.",
                        "time_start": "2021-10-24T15:00:00Z",
                        "time_end": "2021-10-24T15:20:00Z",
                        "uid": "a-vahc-1003",
                        "youtube_video_id": "QqYFjUAqTcY"
                    },
                    {
                        "type": "recorded",
                        "title": "Phoenix Virtual Heart: A Hybrid VR-Desktop Visualization System for Cardiac Surgery Planning and Education",
                        "contributors": [
                            "Jinbin Huang"
                        ],
                        "abstract": "Physicians diagnosing and treating complex, structural congenital heart disease (CHD), i.e., heart defects present at birth, often rely on visualization software that scrolls through a volume stack of two-dimensional (2D) medical images. Due to limited display dimensions, conventional desktop-based applications have difficulties facilitating physicians converting 2D images to 3D intelligence. Recently, 3D printing of anatomical models has emerged as a technique to analyze CHD, but current workflows are tedious. To this end, we introduce and describe our ongoing work developing the Phoenix Virtual Heart (PVH), a hybrid VR-desktop software to aid in CHD surgical planning and family consultation. PVH is currently being integrated into a 3D printing workflow at a children's hospital as a way to increase physician efficiency and confidence, allowing physicians to analyze virtual anatomical models for surgical planning and family consultation.",
                        "time_start": "2021-10-24T15:20:00Z",
                        "time_end": "2021-10-24T15:40:00Z",
                        "uid": "a-vahc-1009",
                        "youtube_video_id": "98UBRbqlBlo"
                    },
                    {
                        "type": "recorded",
                        "title": "Communicating Performance of Regression Models Using Visualization in Pharmacovigilance",
                        "contributors": [
                            "Ashley Suh"
                        ],
                        "abstract": "We describe the iterative design process that led to PVH, discuss how it fits into a 3D printing workflow, and present formative feedback from clinicians that are beginning to use the application.",
                        "time_start": "2021-10-24T15:40:00Z",
                        "time_end": "2021-10-24T16:00:00Z",
                        "uid": "a-vahc-1012",
                        "youtube_video_id": "-gPzpmELbCM"
                    },
                    {
                        "type": "recorded",
                        "title": "Interactive Cohort Analysis and Hypothesis Discovery by Exploring Temporal Patterns in Population-Level Health Records",
                        "contributors": [
                            "Tianyi Zhang"
                        ],
                        "abstract": "It is challenging to visualize temporal patterns in electronic health records (EHRs) due to the high volume and high dimensionality of EHRs. In this paper, we conduct a formative study with three clinical researchers to understand their needs of exploring temporal patterns in EHRs. Based on those insights, we develop a new visualization interface that renders medical event trajectories in a holistic timeline view and guides users towards interesting patterns using an information scent based method. We demonstrate how a clinical researcher can use our tool to discover interesting sub-cohorts with unique disease progression and treatment trajectories in a case study.",
                        "time_start": "2021-10-24T16:00:00Z",
                        "time_end": "2021-10-24T16:20:00Z",
                        "uid": "a-vahc-1010",
                        "youtube_video_id": "tFwpS9Q4IKQ"
                    }
                ]
            },
            {
                "title": "COVID-19 and Public Health Dashboards",
                "session_id": "a-vahc-3",
                "track": "room2",
                "schedule_image": "a-vahc-3.png",
                "chair": [
                    "MISSING"
                ],
                "organizers": [
                    "J\u00fcrgen Bernard",
                    "Annie T. Chen",
                    "Danny T.Y. Wu"
                ],
                "time_start": "2021-10-24T17:00:00Z",
                "time_end": "2021-10-24T18:20:00Z",
                "discord_category": "",
                "discord_channel": "",
                "discord_channel_id": 0,
                "youtube_url": null,
                "zoom_meeting": "",
                "zoom_password": "",
                "ff_link": "",
                "ff_playlist": "",
                "time_slots": [
                    {
                        "type": "recorded",
                        "title": "Enabling Longitudinal Exploratory Analysis of Clinical COVID Data",
                        "contributors": [
                            "David Gotz"
                        ],
                        "abstract": "As the COVID-19 pandemic continues to impact the world, data is being gathered and analyzed to better understand the disease. Recognizing the potential for visual analytics technologies to support exploratory analysis and hypothesis generation from longitudinal clinical data, a team of collaborators worked to apply existing event sequence visual analytics technologies to a longitudinal clinical data from a cohort of 998 patients with high rates of COVID-19 infection. This paper describes the initial steps toward this goal, including: (1) the data transformation and processing work required to prepare the data for visual analysis, (2) initial findings and observations, and (3) qualitative feedback and lessons learned which highlight key features as well as limitations to address in future work",
                        "time_start": "2021-10-24T17:00:00Z",
                        "time_end": "2021-10-24T17:20:00Z",
                        "uid": "a-vahc-1014",
                        "youtube_video_id": "HbPyRT-pgl0"
                    },
                    {
                        "type": "recorded",
                        "title": "Visual Analytics for Decision-Makers and Public Audiences within the United States National COVID-19 Response",
                        "contributors": [
                            "Elisha Peterson"
                        ],
                        "abstract": "The COVID-19 pandemic launched a worldwide effort to collect, process, and communicate public health data at unprecedented scales, and a host of visualization capabilities have been launched and maintained to meet the need for presenting data in ways that the general public can understand. This paper presents a selection of visualizations developed in support of the United States National COVID-19 Response, describes the unique set of constraints and challenges of operational visualization in this context, and reflects on ways the visualization community might be able to support public health operations moving forward.",
                        "time_start": "2021-10-24T17:20:00Z",
                        "time_end": "2021-10-24T17:40:00Z",
                        "uid": "a-vahc-1022",
                        "youtube_video_id": "Pv9FN5Mmb34"
                    },
                    {
                        "type": "recorded",
                        "title": "COVID-19 EnsembleVis: Visual Analysis of County-level Ensemble Forecast Models",
                        "contributors": [
                            "Sanjana Srabanti"
                        ],
                        "abstract": "The spread of the SARS-CoV-2 virus and its contagious disease COVID-19 has impacted countries to an extent not seen since the 1918 flu pandemic. In the absence of an effective vaccine and as cases surge worldwide, governments were forced to adopt measures to inhibit the spread of the disease. To reduce its impact and to guide policy planning and resource allocation, researchers have been developing models to forecast the infectious disease. Ensemble models, by aggregating forecasts from multiple individual models, have been shown to be a useful forecasting method. However, these models can still provide less-than-adequate forecasts at higher spatial resolutions. In this paper, we built COVID-19 EnsembleVis, a web-based interactive visual interface that allows the assessment of the errors of ensembles and individual models by enabling users to effortlessly navigate through and compare the outputs of models considering their space and time dimensions. COVID-19 EnsembleVis enables a more detailed understanding of uncertainty and the range of forecasts generated by individual models.",
                        "time_start": "2021-10-24T17:40:00Z",
                        "time_end": "2021-10-24T18:00:00Z",
                        "uid": "a-vahc-1020",
                        "youtube_video_id": "2wSmgitgTL8"
                    },
                    {
                        "type": "recorded",
                        "title": "Communicating Area-level Social Determinants of Health Information: The Ohio Children\u2019s Opportunity Index Dashboards",
                        "contributors": [
                            "Dr Naleef Fareed"
                        ],
                        "abstract": "Social determinants of health (SDoH) can be measured at the geographic level to convey information about neighborhood deprivation. The Ohio Children\u2019s Opportunity Index (OCOI) is a multi-dimensional area-level opportunity index comprised of eight health dimensions. Our research team has documented the design, development, and use cases of dashboard solutions to visualize OCOI. The OCOI is a multi-dimensional index spanning the following eight domains or dimensions: family stability, infant health, children\u2019s health, access, education, housing, environment, and criminal justice. Information on these eight domains is derived from the American Community Survey and other administrative datasets maintained by the state of Ohio. Our team used the Tableau Desktop visualization software and applied a user-centered design approach to developing the two OCOI dashboards\u2014 main OCOI dashboard and OCOI-race dashboard. We also performed convergence analysis to visualize the census tracts, where different health indicators simultaneously exist at their worst levels. The OCOI dashboards have multiple, interactive components: a choropleth map of Ohio displaying OCOI scores for a specific census tract, graphs presenting OCOI or domain scores to compare relative positions for tracts, and a sortable table to visualize scores for specific county and census tracts. Stakeholders provided iterative feedback on dashboard design in regard to functionality, content, and aesthetics. A case study using the two dashboards for convergence analysis revealed census tracts in neighborhoods with low infant health scores and a high proportion of minority population. The OCOI dashboards could assist end-users in making decisions that tailor health care delivery and policy decision-making regarding children\u2019s health particularly in areas where multiple health indicators exist at their worst levels.",
                        "time_start": "2021-10-24T18:00:00Z",
                        "time_end": "2021-10-24T18:20:00Z",
                        "uid": "a-vahc-1019",
                        "youtube_video_id": "HGcOvc7kJdU"
                    }
                ]
            },
            {
                "title": "Panel",
                "session_id": "a-vahc-4",
                "track": "room2",
                "schedule_image": "a-vahc-4.png",
                "chair": [
                    "MISSING"
                ],
                "organizers": [
                    "J\u00fcrgen Bernard",
                    "Annie T. Chen",
                    "Danny T.Y. Wu"
                ],
                "time_start": "2021-10-24T19:00:00Z",
                "time_end": "2021-10-24T20:30:00Z",
                "discord_category": "",
                "discord_channel": "",
                "discord_channel_id": 0,
                "youtube_url": null,
                "zoom_meeting": "",
                "zoom_password": "",
                "ff_link": "",
                "ff_playlist": "",
                "time_slots": [
                    {
                        "type": "live",
                        "title": "Panel",
                        "contributors": [
                            "MISSING"
                        ],
                        "abstract": null,
                        "time_start": "2021-10-24T19:00:00Z",
                        "time_end": "2021-10-24T20:30:00Z"
                    }
                ]
            }
        ]
    },
    "a-vis4dh": {
        "event": "VIS4DH",
        "long_name": "VIS4DH",
        "event_type": "Associated Event",
        "event_description": "",
        "event_url": "",
        "sessions": [
            {
                "title": "Opening and Keynote",
                "session_id": "a-vis4dh-1",
                "track": "room3",
                "schedule_image": "a-vis4dh-1.png",
                "chair": [
                    "Florian Windhager",
                    "Houda Lamqaddamm"
                ],
                "organizers": [
                    "Houda Lamqaddamm",
                    "Florian Windhager"
                ],
                "time_start": "2021-10-24T13:00:00Z",
                "time_end": "2021-10-24T14:30:00Z",
                "discord_category": "",
                "discord_channel": "",
                "discord_channel_id": 0,
                "youtube_url": null,
                "zoom_meeting": "",
                "zoom_password": "",
                "ff_link": "",
                "ff_playlist": "",
                "time_slots": [
                    {
                        "type": "live",
                        "title": "Opening",
                        "contributors": [
                            "Houda Lamqaddamm",
                            "Florian Windhager"
                        ],
                        "abstract": null,
                        "time_start": "2021-10-24T13:00:00Z",
                        "time_end": "2021-10-24T13:10:00Z"
                    },
                    {
                        "type": "live",
                        "title": "VIS4DH Interactive Retrospective (Where Are We Now?)",
                        "contributors": [
                            "Houda Lamqaddamm",
                            "Florian Windhager"
                        ],
                        "abstract": null,
                        "time_start": "2021-10-24T13:10:00Z",
                        "time_end": "2021-10-24T13:30:00Z"
                    },
                    {
                        "type": "live",
                        "title": "Keynote by Yanni Loukissas",
                        "contributors": [
                            "Yanni Loukissas"
                        ],
                        "abstract": null,
                        "time_start": "2021-10-24T13:30:00Z",
                        "time_end": "2021-10-24T14:30:00Z"
                    }
                ]
            },
            {
                "title": "Papers & Provocations I",
                "session_id": "a-vis4dh-2",
                "track": "room3",
                "schedule_image": "a-vis4dh-2.png",
                "chair": [
                    "Uta Hinrichs"
                ],
                "organizers": [
                    "Houda Lamqaddamm",
                    "Florian Windhager"
                ],
                "time_start": "2021-10-24T15:00:00Z",
                "time_end": "2021-10-24T16:30:00Z",
                "discord_category": "",
                "discord_channel": "",
                "discord_channel_id": 0,
                "youtube_url": null,
                "zoom_meeting": "",
                "zoom_password": "",
                "ff_link": "",
                "ff_playlist": "",
                "time_slots": [
                    {
                        "type": "live",
                        "title": "Provocation 1",
                        "contributors": [
                            "Richard Brath"
                        ],
                        "abstract": null,
                        "time_start": "2021-10-24T15:00:00Z",
                        "time_end": "2021-10-24T15:10:00Z"
                    },
                    {
                        "type": "recorded",
                        "title": "Paper 1: Small Data and Process in Data Visualization: The Radical Translations Case Study",
                        "contributors": [
                            "Arianna Ciula",
                            "Miguel Vieira",
                            "Ginestra Ferraro",
                            "Tiffany Ong",
                            "Sanja Perovic",
                            "Rosa Mucignat",
                            "Niccol\u00f2 Valmori",
                            "Brecht Deseure",
                            "Erica Joy Mannucci"
                        ],
                        "abstract": null,
                        "time_start": "2021-10-24T15:10:00Z",
                        "time_end": "2021-10-24T15:30:00Z",
                        "uid": "a-vis4dh-1007",
                        "youtube_video_id": "y2VPCUug8YA"
                    },
                    {
                        "type": "live",
                        "title": "Provocation 2",
                        "contributors": [
                            "Rodolfo Almeida"
                        ],
                        "abstract": null,
                        "time_start": "2021-10-24T15:30:00Z",
                        "time_end": "2021-10-24T15:40:00Z"
                    },
                    {
                        "type": "recorded",
                        "title": "Paper 2: Uncertainty-aware Topic Modeling Visualization",
                        "contributors": [
                            "Lars Linsen"
                        ],
                        "abstract": null,
                        "time_start": "2021-10-24T15:40:00Z",
                        "time_end": "2021-10-24T16:00:00Z",
                        "uid": "a-vis4dh-1014",
                        "youtube_video_id": "-d0yMUQ3zfA"
                    },
                    {
                        "type": "live",
                        "title": "Provocation 3",
                        "contributors": [
                            "Paul Heinicker"
                        ],
                        "abstract": null,
                        "time_start": "2021-10-24T16:00:00Z",
                        "time_end": "2021-10-24T16:10:00Z"
                    },
                    {
                        "type": "recorded",
                        "title": "Paper 3: Visualizing Wonderland for many more Literature Visualization Techniques",
                        "contributors": [
                            "Richard Brath"
                        ],
                        "abstract": null,
                        "time_start": "2021-10-24T16:10:00Z",
                        "time_end": "2021-10-24T16:30:00Z",
                        "uid": "a-vis4dh-1006",
                        "youtube_video_id": "ebT_vKzBs8g"
                    }
                ]
            },
            {
                "title": "Lab Talks & Capstone",
                "session_id": "a-vis4dh-3",
                "track": "room3",
                "schedule_image": "a-vis4dh-3.png",
                "chair": [
                    "Eric Alexander",
                    "Mark-Jan Bludau",
                    "Bridget Moynihan"
                ],
                "organizers": [
                    "Houda Lamqaddamm",
                    "Florian Windhager"
                ],
                "time_start": "2021-10-24T17:00:00Z",
                "time_end": "2021-10-24T18:30:00Z",
                "discord_category": "",
                "discord_channel": "",
                "discord_channel_id": 0,
                "youtube_url": null,
                "zoom_meeting": "",
                "zoom_password": "",
                "ff_link": "",
                "ff_playlist": "",
                "time_slots": [
                    {
                        "type": "live",
                        "title": "Lab Talks Intro",
                        "contributors": [
                            "Mark-Jan Bludau"
                        ],
                        "abstract": null,
                        "time_start": "2021-10-24T17:00:00Z",
                        "time_end": "2021-10-24T17:05:00Z"
                    },
                    {
                        "type": "live",
                        "title": "Lab Talk #1: Canberra Design Lab",
                        "contributors": [
                            "Ben Ennis Butler"
                        ],
                        "abstract": null,
                        "time_start": "2021-10-24T17:06:00Z",
                        "time_end": "2021-10-24T17:09:00Z"
                    },
                    {
                        "type": "live",
                        "title": "Lab Talk #2: LaSTIG",
                        "contributors": [
                            "Mathieu Br\u00e9dif"
                        ],
                        "abstract": null,
                        "time_start": "2021-10-24T17:10:00Z",
                        "time_end": "2021-10-24T17:13:00Z"
                    },
                    {
                        "type": "live",
                        "title": "Lab Talk #3: Luxembourg Centre for Contemporary and Digital History (C2DH)",
                        "contributors": [
                            "Marten D\u00fcring"
                        ],
                        "abstract": null,
                        "time_start": "2021-10-24T17:14:00Z",
                        "time_end": "2021-10-24T17:17:00Z"
                    },
                    {
                        "type": "live",
                        "title": "Lab Talk #4: LingVis",
                        "contributors": [
                            "Mennatallah El-Assady"
                        ],
                        "abstract": null,
                        "time_start": "2021-10-24T17:18:00Z",
                        "time_end": "2021-10-24T17:21:00Z"
                    },
                    {
                        "type": "live",
                        "title": "Lab Talk #5: Digital Matters",
                        "contributors": [
                            "Rebekah Cummings",
                            "David Roh"
                        ],
                        "abstract": null,
                        "time_start": "2021-10-24T17:22:00Z",
                        "time_end": "2021-10-24T17:25:00Z"
                    },
                    {
                        "type": "live",
                        "title": "DH-Capstone by Romi Ron Morrison",
                        "contributors": [
                            "Romi Ron Morrison"
                        ],
                        "abstract": null,
                        "time_start": "2021-10-24T17:30:00Z",
                        "time_end": "2021-10-24T18:30:00Z"
                    }
                ]
            },
            {
                "title": "Papers & Provocations II, Town Hall",
                "session_id": "a-vis4dh-4",
                "track": "room3",
                "schedule_image": "a-vis4dh-4.png",
                "chair": [
                    "Houda Lamqaddamm",
                    "Florian Windhager",
                    "Chris Weaver",
                    "Stefania Forlini"
                ],
                "organizers": [
                    "Houda Lamqaddamm",
                    "Florian Windhager"
                ],
                "time_start": "2021-10-24T19:00:00Z",
                "time_end": "2021-10-24T20:30:00Z",
                "discord_category": "",
                "discord_channel": "",
                "discord_channel_id": 0,
                "youtube_url": null,
                "zoom_meeting": "",
                "zoom_password": "",
                "ff_link": "",
                "ff_playlist": "",
                "time_slots": [
                    {
                        "type": "live",
                        "title": "Provocation 4",
                        "contributors": [
                            "Sara Akhlaq"
                        ],
                        "abstract": null,
                        "time_start": "2021-10-24T19:00:00Z",
                        "time_end": "2021-10-24T19:10:00Z"
                    },
                    {
                        "type": "recorded",
                        "title": "Paper 4: The Virtual Rosetta A Tool for Exploring Historical Drawings in VR",
                        "contributors": [
                            "Anessa Petteruti"
                        ],
                        "abstract": null,
                        "time_start": "2021-10-24T19:10:00Z",
                        "time_end": "2021-10-24T19:30:00Z",
                        "uid": "a-vis4dh-1016",
                        "youtube_video_id": "JKq5of82bgs"
                    },
                    {
                        "type": "live",
                        "title": "Provocation 5",
                        "contributors": [
                            "Eugene Park"
                        ],
                        "abstract": null,
                        "time_start": "2021-10-24T19:30:00Z",
                        "time_end": "2021-10-24T19:40:00Z"
                    },
                    {
                        "type": "recorded",
                        "title": "Paper 5: Reflexivity in Issues of Scale and Representation in a Digital Humanities Project",
                        "contributors": [
                            "Annie Chen"
                        ],
                        "abstract": null,
                        "time_start": "2021-10-24T19:40:00Z",
                        "time_end": "2021-10-24T20:00:00Z",
                        "uid": "a-vis4dh-1008",
                        "youtube_video_id": "LbNTKl0dnb0"
                    },
                    {
                        "type": "live",
                        "title": "Closing and Sharing Link for Town Hall",
                        "contributors": [
                            "Houda Lamqaddamm",
                            "Florian Windhager"
                        ],
                        "abstract": null,
                        "time_start": "2021-10-24T20:00:00Z",
                        "time_end": "2021-10-24T20:05:00Z"
                    },
                    {
                        "type": "live",
                        "title": "Town Hall (Where Are We Going?)",
                        "contributors": [
                            "Houda Lamqaddamm",
                            "Florian Windhager"
                        ],
                        "abstract": null,
                        "time_start": "2021-10-24T20:05:00Z",
                        "time_end": "2021-10-24T20:30:00Z"
                    }
                ]
            }
        ]
    },
    "w-nlviz": {
        "event": "Exploring Opportunities and Challenges for Natural Language Techniques to Support Visual Analysis Tasks",
        "long_name": "Exploring Opportunities and Challenges for Natural Language Techniques to Support Visual Analysis Tasks",
        "event_type": "Workshop",
        "event_description": "",
        "event_url": "",
        "sessions": [
            {
                "title": "Exploring Opportunities and Challenges for Natural Language Techniques to Support Visual Analysis Tasks",
                "session_id": "w-nlviz",
                "track": "room4",
                "schedule_image": "w-nlviz.png",
                "chair": [
                    "Arjun Srinivasan",
                    "Vidya Setlur"
                ],
                "organizers": [
                    "Vidya Setlur",
                    "Arjun Srinivasan"
                ],
                "time_start": "2021-10-24T13:00:00Z",
                "time_end": "2021-10-24T16:30:00Z",
                "discord_category": "",
                "discord_channel": "",
                "discord_channel_id": 0,
                "youtube_url": null,
                "zoom_meeting": "",
                "zoom_password": "",
                "ff_link": "",
                "ff_playlist": "",
                "time_slots": [
                    {
                        "type": "live",
                        "title": "Exploring Opportunities and Challenges for Natural Language Techniques to Support Visual Analysis Tasks",
                        "contributors": [
                            "Vidya Setlur",
                            "Arjun Srinivasan"
                        ],
                        "abstract": null,
                        "time_start": "2021-10-24T13:00:00Z",
                        "time_end": "2021-10-24T14:30:00Z"
                    },
                    {
                        "type": "live",
                        "title": "Exploring Opportunities and Challenges for Natural Language Techniques to Support Visual Analysis Tasks",
                        "contributors": [
                            "Vidya Setlur",
                            "Arjun Srinivasan"
                        ],
                        "abstract": null,
                        "time_start": "2021-10-24T15:00:00Z",
                        "time_end": "2021-10-24T16:30:00Z"
                    }
                ]
            }
        ]
    },
    "w-trex": {
        "event": "TREX: Workshop on TRust and EXpertise in Visualization",
        "long_name": "TREX: Workshop on TRust and EXpertise in Visualization",
        "event_type": "Workshop",
        "event_description": "",
        "event_url": "",
        "sessions": [
            {
                "title": "TREX: Workshop on TRust and EXpertise in Visualization",
                "session_id": "w-trex",
                "track": "room5",
                "schedule_image": "w-trex.png",
                "chair": [],
                "organizers": [
                    "Mahsan Nourani",
                    "Eric Ragan",
                    "Emily Wall",
                    "Alireza Karduni"
                ],
                "time_start": "2021-10-24T13:00:00Z",
                "time_end": "2021-10-24T16:30:00Z",
                "discord_category": "",
                "discord_channel": "",
                "discord_channel_id": 0,
                "youtube_url": null,
                "zoom_meeting": "",
                "zoom_password": "",
                "ff_link": "",
                "ff_playlist": "",
                "time_slots": [
                    {
                        "type": null,
                        "title": null,
                        "contributors": "",
                        "abstract": null,
                        "time_start": "2021-10-24T13:00:00Z",
                        "time_end": "2021-10-24T14:30:00Z"
                    },
                    {
                        "type": null,
                        "title": null,
                        "contributors": "",
                        "abstract": null,
                        "time_start": "2021-10-24T15:00:00Z",
                        "time_end": "2021-10-24T16:30:00Z"
                    }
                ]
            }
        ]
    },
    "t-color": {
        "event": "Color Matters in Visualization",
        "long_name": "Color Matters in Visualization",
        "event_type": "Tutorial",
        "event_description": "",
        "event_url": "",
        "sessions": [
            {
                "title": "Color Matters in Visualization",
                "session_id": "t-color",
                "track": "room6",
                "schedule_image": "t-color.png",
                "chair": [
                    "Theresa-Marie Rhyne"
                ],
                "organizers": [
                    "Theresa-Marie Rhyne"
                ],
                "time_start": "2021-10-24T13:00:00Z",
                "time_end": "2021-10-24T16:30:00Z",
                "discord_category": "",
                "discord_channel": "",
                "discord_channel_id": 0,
                "youtube_url": null,
                "zoom_meeting": "",
                "zoom_password": "",
                "ff_link": "",
                "ff_playlist": "",
                "time_slots": [
                    {
                        "type": "recorded",
                        "title": "Color Fundamentals for Visualization",
                        "contributors": [
                            "Theresa-Marie Rhyne"
                        ],
                        "abstract": null,
                        "time_start": "2021-10-24T13:00:00Z",
                        "time_end": "2021-10-24T14:30:00Z"
                    },
                    {
                        "type": "recorded",
                        "title": "Color Tools & Case Studies for Viz",
                        "contributors": [
                            "Theresa-Marie Rhyne"
                        ],
                        "abstract": null,
                        "time_start": "2021-10-24T15:00:00Z",
                        "time_end": "2021-10-24T16:30:00Z"
                    }
                ]
            }
        ]
    },
    "t-ttk": {
        "event": "Topological Analysis of Ensemble Scalar Data with TTK",
        "long_name": "Topological Analysis of Ensemble Scalar Data with TTK",
        "event_type": "Tutorial",
        "event_description": "",
        "event_url": "",
        "sessions": [
            {
                "title": "Topological Analysis of Ensemble Scalar Data with TTK",
                "session_id": "t-ttk",
                "track": "room7",
                "schedule_image": "t-ttk.png",
                "chair": [
                    "Josh Levine",
                    "Julien Tierny"
                ],
                "organizers": [
                    "Christoph Garth",
                    "Charles Gueunet",
                    "Pierre Guillou",
                    "Lutz Hofmann",
                    "Joshua A Levine",
                    "Jonas Lukasczyk",
                    "Julien Tierny",
                    "Jules Vidal",
                    "Bei Wang",
                    "Florian Wetzels"
                ],
                "time_start": "2021-10-24T13:00:00Z",
                "time_end": "2021-10-24T16:30:00Z",
                "discord_category": "",
                "discord_channel": "",
                "discord_channel_id": 0,
                "youtube_url": null,
                "zoom_meeting": "",
                "zoom_password": "",
                "ff_link": "",
                "ff_playlist": "",
                "time_slots": [
                    {
                        "type": "recorded",
                        "title": "General Introduction",
                        "contributors": [
                            "Julien Tierny"
                        ],
                        "abstract": null,
                        "time_start": "2021-10-24T13:00:00Z",
                        "time_end": "2021-10-24T13:05:00Z"
                    },
                    {
                        "type": "recorded",
                        "title": "Introduction to topological methods for data analysis",
                        "contributors": [
                            "Bei Wang"
                        ],
                        "abstract": null,
                        "time_start": "2021-10-24T13:05:00Z",
                        "time_end": "2021-10-24T13:30:00Z",
                        "youtube_video_id": "louyNgsvAKo"
                    },
                    {
                        "type": "recorded",
                        "title": "Quick introduction to ParaView's user interface",
                        "contributors": [
                            "Charles Gueunet"
                        ],
                        "abstract": null,
                        "time_start": "2021-10-24T13:30:00Z",
                        "time_end": "2021-10-24T13:55:00Z",
                        "youtube_video_id": "DNt26-0cYBI"
                    },
                    {
                        "type": "gathertown",
                        "title": "Break",
                        "contributors": "",
                        "abstract": null,
                        "time_start": "2021-10-24T13:55:00Z",
                        "time_end": "2021-10-24T14:10:00Z"
                    },
                    {
                        "type": "recorded",
                        "title": "Running TTK with Docker",
                        "contributors": [
                            "Christoph Garth"
                        ],
                        "abstract": null,
                        "time_start": "2021-10-24T14:10:00Z",
                        "time_end": "2021-10-24T14:20:00Z"
                    },
                    {
                        "type": "recorded",
                        "title": "A tour of TTK",
                        "contributors": [
                            "Josh Levine"
                        ],
                        "abstract": null,
                        "time_start": "2021-10-24T14:20:00Z",
                        "time_end": "2021-10-24T14:50:00Z"
                    },
                    {
                        "type": "recorded",
                        "title": "TTK Python support",
                        "contributors": [
                            "Lutz Hofmann"
                        ],
                        "abstract": null,
                        "time_start": "2021-10-24T14:50:00Z",
                        "time_end": "2021-10-24T15:00:00Z",
                        "youtube_video_id": "kk9O3iRuTII"
                    },
                    {
                        "type": "recorded",
                        "title": "TTK Cinema support",
                        "contributors": [
                            "Jonas Lukasczyk"
                        ],
                        "abstract": null,
                        "time_start": "2021-10-24T15:00:00Z",
                        "time_end": "2021-10-24T15:10:00Z",
                        "youtube_video_id": "FXPFE4SthJM"
                    },
                    {
                        "type": "gathertown",
                        "title": "Break",
                        "contributors": "",
                        "abstract": null,
                        "time_start": "2021-10-24T15:10:00Z",
                        "time_end": "2021-10-24T15:25:00Z"
                    },
                    {
                        "type": "recorded",
                        "title": "Contour tree alignment",
                        "contributors": [
                            "Florian Wetzels"
                        ],
                        "abstract": null,
                        "time_start": "2021-10-24T15:25:00Z",
                        "time_end": "2021-10-24T15:40:00Z",
                        "youtube_video_id": "6mQdu6tAyA8"
                    },
                    {
                        "type": "recorded",
                        "title": "Mandatory critical points",
                        "contributors": [
                            "Julien Tierny"
                        ],
                        "abstract": null,
                        "time_start": "2021-10-24T15:40:00Z",
                        "time_end": "2021-10-24T15:55:00Z",
                        "youtube_video_id": "kQNgDWfhq8U"
                    },
                    {
                        "type": "recorded",
                        "title": "Distances, barycenters and clusters of Persistence diagrams",
                        "contributors": [
                            "Jules Vidal"
                        ],
                        "abstract": null,
                        "time_start": "2021-10-24T15:55:00Z",
                        "time_end": "2021-10-24T16:10:00Z",
                        "youtube_video_id": "URgWn31MNi4"
                    },
                    {
                        "type": "recorded",
                        "title": "Ensemble summarization with linked planar views",
                        "contributors": [
                            "Pierre Guillou"
                        ],
                        "abstract": null,
                        "time_start": "2021-10-24T16:10:00Z",
                        "time_end": "2021-10-24T16:25:00Z"
                    },
                    {
                        "type": "live",
                        "title": "Concluding remarks",
                        "contributors": [
                            "Julien Tierny"
                        ],
                        "abstract": null,
                        "time_start": "2021-10-24T16:25:00Z",
                        "time_end": "2021-10-24T16:30:00Z"
                    }
                ]
            }
        ]
    },
    "t-analysisdesign": {
        "event": "Visualization Analysis and Design",
        "long_name": "Visualization Analysis and Design",
        "event_type": "Tutorial",
        "event_description": "",
        "event_url": "",
        "sessions": [
            {
                "title": "Visualization Analysis and Design",
                "session_id": "t-analysisdesign",
                "track": "room8",
                "schedule_image": "t-analysisdesign.png",
                "chair": [
                    "Tamara Munzner"
                ],
                "organizers": [
                    "Tamara Munzner"
                ],
                "time_start": "2021-10-24T13:00:00Z",
                "time_end": "2021-10-24T20:30:00Z",
                "discord_category": "",
                "discord_channel": "",
                "discord_channel_id": 0,
                "youtube_url": null,
                "zoom_meeting": "",
                "zoom_password": "",
                "ff_link": "",
                "ff_playlist": "",
                "time_slots": [
                    {
                        "type": "recorded",
                        "title": "Introduction",
                        "contributors": [
                            "Tamara Munzner"
                        ],
                        "abstract": null,
                        "time_start": "2021-10-24T13:00:00Z",
                        "time_end": "2021-10-24T13:16:00Z",
                        "youtube_video_id": "ciLo00EAH-o"
                    },
                    {
                        "type": "live",
                        "title": "Q&A",
                        "contributors": [
                            "Tamara Munzner"
                        ],
                        "abstract": null,
                        "time_start": "2021-10-24T13:16:00Z",
                        "time_end": "2021-10-24T13:18:00Z"
                    },
                    {
                        "type": "recorded",
                        "title": "Nested Model",
                        "contributors": [
                            "Tamara Munzner"
                        ],
                        "abstract": null,
                        "time_start": "2021-10-24T13:18:00Z",
                        "time_end": "2021-10-24T13:27:00Z",
                        "youtube_video_id": "IVHcOD_WPpE"
                    },
                    {
                        "type": "recorded",
                        "title": "What: Data Abstraction",
                        "contributors": [
                            "Tamara Munzner"
                        ],
                        "abstract": null,
                        "time_start": "2021-10-24T13:27:00Z",
                        "time_end": "2021-10-24T13:54:00Z",
                        "youtube_video_id": "yEGopsHtae0"
                    },
                    {
                        "type": "recorded",
                        "title": "Why: Task Abstraction",
                        "contributors": [
                            "Tamara Munzner"
                        ],
                        "abstract": null,
                        "time_start": "2021-10-24T13:54:00Z",
                        "time_end": "2021-10-24T14:08:00Z",
                        "youtube_video_id": "bwA5U43L92Y"
                    },
                    {
                        "type": "live",
                        "title": "Q&A",
                        "contributors": [
                            "Tamara Munzner"
                        ],
                        "abstract": null,
                        "time_start": "2021-10-24T14:08:00Z",
                        "time_end": "2021-10-24T14:12:00Z"
                    },
                    {
                        "type": "recorded",
                        "title": "Marks and Channels",
                        "contributors": [
                            "Tamara Munzner"
                        ],
                        "abstract": null,
                        "time_start": "2021-10-24T14:12:00Z",
                        "time_end": "2021-10-24T14:25:00Z",
                        "youtube_video_id": "CG_GCAc2WtQ"
                    },
                    {
                        "type": "live",
                        "title": "Q&A",
                        "contributors": [
                            "Tamara Munzner"
                        ],
                        "abstract": null,
                        "time_start": "2021-10-24T14:25:00Z",
                        "time_end": "2021-10-24T14:30:00Z"
                    },
                    {
                        "type": "recorded",
                        "title": "Tables 1",
                        "contributors": [
                            "Tamara Munzner"
                        ],
                        "abstract": null,
                        "time_start": "2021-10-24T15:00:00Z",
                        "time_end": "2021-10-24T15:32:00Z",
                        "youtube_video_id": "sBWXKhMsnyQ"
                    },
                    {
                        "type": "recorded",
                        "title": "Tables 2",
                        "contributors": [
                            "Tamara Munzner"
                        ],
                        "abstract": null,
                        "time_start": "2021-10-24T15:32:00Z",
                        "time_end": "2021-10-24T15:53:00Z",
                        "youtube_video_id": "td_yRx1jeJE"
                    },
                    {
                        "type": "live",
                        "title": "Q&A",
                        "contributors": [
                            "Tamara Munzner"
                        ],
                        "abstract": null,
                        "time_start": "2021-10-24T15:53:00Z",
                        "time_end": "2021-10-24T15:56:00Z"
                    },
                    {
                        "type": "recorded",
                        "title": "Networks",
                        "contributors": [
                            "Tamara Munzner"
                        ],
                        "abstract": null,
                        "time_start": "2021-10-24T15:56:00Z",
                        "time_end": "2021-10-24T16:27:00Z",
                        "youtube_video_id": "Qbwypom_j5Q"
                    },
                    {
                        "type": "live",
                        "title": "Q&A",
                        "contributors": [
                            "Tamara Munzner"
                        ],
                        "abstract": null,
                        "time_start": "2021-10-24T16:27:00Z",
                        "time_end": "2021-10-24T16:30:00Z"
                    },
                    {
                        "type": "recorded",
                        "title": "Geographic Maps",
                        "contributors": [
                            "Tamara Munzner"
                        ],
                        "abstract": null,
                        "time_start": "2021-10-24T17:30:00Z",
                        "time_end": "2021-10-24T17:45:00Z",
                        "youtube_video_id": "rY8A4Bq5NMo"
                    },
                    {
                        "type": "recorded",
                        "title": "Spatial Fields",
                        "contributors": [
                            "Tamara Munzner"
                        ],
                        "abstract": null,
                        "time_start": "2021-10-24T17:45:00Z",
                        "time_end": "2021-10-24T17:58:00Z",
                        "youtube_video_id": "BsvMCQxDaLY"
                    },
                    {
                        "type": "live",
                        "title": "Q&A",
                        "contributors": [
                            "Tamara Munzner"
                        ],
                        "abstract": null,
                        "time_start": "2021-10-24T17:58:00Z",
                        "time_end": "2021-10-24T18:00:00Z"
                    },
                    {
                        "type": "recorded",
                        "title": "Color 1",
                        "contributors": [
                            "Tamara Munzner"
                        ],
                        "abstract": null,
                        "time_start": "2021-10-24T18:00:00Z",
                        "time_end": "2021-10-24T18:19:00Z",
                        "youtube_video_id": "-pxVAunumNA"
                    },
                    {
                        "type": "recorded",
                        "title": "Color 2",
                        "contributors": [
                            "Tamara Munzner"
                        ],
                        "abstract": null,
                        "time_start": "2021-10-24T18:19:00Z",
                        "time_end": "2021-10-24T18:25:00Z",
                        "youtube_video_id": "CwxI0mSZ12o"
                    },
                    {
                        "type": "live",
                        "title": "Q&A",
                        "contributors": [
                            "Tamara Munzner"
                        ],
                        "abstract": null,
                        "time_start": "2021-10-24T18:25:00Z",
                        "time_end": "2021-10-24T18:30:00Z"
                    },
                    {
                        "type": "recorded",
                        "title": "Interactive Views",
                        "contributors": [
                            "Tamara Munzner"
                        ],
                        "abstract": null,
                        "time_start": "2021-10-24T19:00:00Z",
                        "time_end": "2021-10-24T19:18:00Z",
                        "youtube_video_id": "SQpG8DsFjP8"
                    },
                    {
                        "type": "recorded",
                        "title": "Multiple Views",
                        "contributors": [
                            "Tamara Munzner"
                        ],
                        "abstract": null,
                        "time_start": "2021-10-24T19:18:00Z",
                        "time_end": "2021-10-24T19:48:00Z",
                        "youtube_video_id": "sVJxzHtKaDU"
                    },
                    {
                        "type": "live",
                        "title": "Q&A",
                        "contributors": [
                            "Tamara Munzner"
                        ],
                        "abstract": null,
                        "time_start": "2021-10-24T19:48:00Z",
                        "time_end": "2021-10-24T19:52:00Z"
                    },
                    {
                        "type": "recorded",
                        "title": "Aggregation",
                        "contributors": [
                            "Tamara Munzner"
                        ],
                        "abstract": null,
                        "time_start": "2021-10-24T19:52:00Z",
                        "time_end": "2021-10-24T20:17:00Z",
                        "youtube_video_id": "9mMG0MgBZr0"
                    },
                    {
                        "type": "recorded",
                        "title": "Wrapup",
                        "contributors": [
                            "Tamara Munzner"
                        ],
                        "abstract": null,
                        "time_start": "2021-10-24T20:17:00Z",
                        "time_end": "2021-10-24T20:19:00Z",
                        "youtube_video_id": "FJXibGUJ6d0"
                    },
                    {
                        "type": "live",
                        "title": "Q&A",
                        "contributors": [
                            "Tamara Munzner"
                        ],
                        "abstract": null,
                        "time_start": "2021-10-24T20:19:00Z",
                        "time_end": "2021-10-24T20:30:00Z"
                    }
                ]
            }
        ]
    },
    "a-vastchallenge": {
        "event": "VAST Challenge",
        "long_name": "VAST Challenge",
        "event_type": "Associated Event",
        "event_description": "",
        "event_url": "",
        "sessions": [
            {
                "title": "VAST Challenge",
                "session_id": "a-vastchallenge",
                "track": "room1",
                "schedule_image": "a-vastchallenge.png",
                "chair": [
                    "Kris Cook",
                    "R. Jordan Crouser"
                ],
                "organizers": [
                    "R. Jordan Crouser",
                    "Kris Cook"
                ],
                "time_start": "2021-10-24T17:00:00Z",
                "time_end": "2021-10-24T20:30:00Z",
                "discord_category": "",
                "discord_channel": "",
                "discord_channel_id": 0,
                "youtube_url": null,
                "zoom_meeting": "",
                "zoom_password": "",
                "ff_link": "",
                "ff_playlist": "",
                "time_slots": [
                    {
                        "type": "live",
                        "title": "Intro and Welcom: Overview of MC1",
                        "contributors": [
                            "R. Jordan Crouser",
                            "Kris Cook"
                        ],
                        "abstract": null,
                        "time_start": "2021-10-24T17:00:00Z",
                        "time_end": "2021-10-24T17:20:00Z"
                    },
                    {
                        "type": "recorded",
                        "title": "MC1 Award Presentation 1",
                        "contributors": [
                            "MISSING"
                        ],
                        "abstract": null,
                        "time_start": "2021-10-24T17:20:00Z",
                        "time_end": "2021-10-24T17:40:00Z",
                        "uid": "a-vastchallenge-1010",
                        "youtube_video_id": "gPP1QwnsGzM"
                    },
                    {
                        "type": "recorded",
                        "title": "MC1 Award Presentation 2",
                        "contributors": [
                            "MISSING"
                        ],
                        "abstract": null,
                        "time_start": "2021-10-24T17:40:00Z",
                        "time_end": "2021-10-24T18:00:00Z",
                        "uid": "a-vastchallenge-1001",
                        "youtube_video_id": "yBUt_tF2xM8"
                    },
                    {
                        "type": "live",
                        "title": "Intro to MC2",
                        "contributors": [
                            "R. Jordan Crouser",
                            "Kris Cook"
                        ],
                        "abstract": null,
                        "time_start": "2021-10-24T18:00:00Z",
                        "time_end": "2021-10-24T18:10:00Z"
                    },
                    {
                        "type": "recorded",
                        "title": "MC2 Award Presentation 1",
                        "contributors": [
                            "MISSING"
                        ],
                        "abstract": null,
                        "time_start": "2021-10-24T18:10:00Z",
                        "time_end": "2021-10-24T18:30:00Z",
                        "uid": "a-vastchallenge-1012"
                    },
                    {
                        "type": "recorded",
                        "title": "MC2 Award Presentation 2",
                        "contributors": [
                            "MISSING"
                        ],
                        "abstract": null,
                        "time_start": "2021-10-24T19:00:00Z",
                        "time_end": "2021-10-24T19:20:00Z",
                        "uid": "a-vastchallenge-1008"
                    },
                    {
                        "type": "live",
                        "title": "MC3 Introduction",
                        "contributors": [
                            "R. Jordan Crouser",
                            "Kris Cook"
                        ],
                        "abstract": null,
                        "time_start": "2021-10-24T19:20:00Z",
                        "time_end": "2021-10-24T19:30:00Z"
                    },
                    {
                        "type": "recorded",
                        "title": "MC3 Award Presentation 1",
                        "contributors": [
                            "MISSING"
                        ],
                        "abstract": null,
                        "time_start": "2021-10-24T19:30:00Z",
                        "time_end": "2021-10-24T19:50:00Z",
                        "uid": "a-vastchallenge-1011",
                        "youtube_video_id": "O2kd8KukzJk"
                    },
                    {
                        "type": "recorded",
                        "title": "MC3 Award Presentation 2",
                        "contributors": [
                            "MISSING"
                        ],
                        "abstract": null,
                        "time_start": "2021-10-24T19:50:00Z",
                        "time_end": "2021-10-24T20:10:00Z",
                        "uid": "a-vastchallenge-1019"
                    },
                    {
                        "type": "recorded",
                        "title": "MC3 HM Presentation",
                        "contributors": [
                            "MISSING"
                        ],
                        "abstract": null,
                        "time_start": "2021-10-24T20:10:00Z",
                        "time_end": "2021-10-24T20:25:00Z",
                        "uid": "a-vastchallenge-1015"
                    },
                    {
                        "type": "live",
                        "title": "Closing",
                        "contributors": [
                            "R. Jordan Crouser",
                            "Kris Cook"
                        ],
                        "abstract": null,
                        "time_start": "2021-10-24T20:25:00Z",
                        "time_end": "2021-10-24T20:30:00Z"
                    }
                ]
            }
        ]
    },
    "w-altvis": {
        "event": "alt.VIS",
        "long_name": "alt.VIS",
        "event_type": "Workshop",
        "event_description": "",
        "event_url": "",
        "sessions": [
            {
                "title": "alt.VIS",
                "session_id": "w-altvis",
                "track": "room4",
                "schedule_image": "w-altvis.png",
                "chair": [
                    "Michael Correll",
                    "Charles Perin",
                    "Paul Rosen",
                    "Jane Adams"
                ],
                "organizers": [
                    "Jane L. Adams",
                    "Lonni Besan\u00e7on",
                    "Michael Correll",
                    "R. Jordan Crouser",
                    "Charles Perin",
                    "Paul Rosen"
                ],
                "time_start": "2021-10-24T17:00:00Z",
                "time_end": "2021-10-24T20:30:00Z",
                "discord_category": "",
                "discord_channel": "",
                "discord_channel_id": 0,
                "youtube_url": null,
                "zoom_meeting": "",
                "zoom_password": "",
                "ff_link": "",
                "ff_playlist": "",
                "time_slots": [
                    {
                        "type": "live",
                        "title": "Opening",
                        "contributors": [
                            "Jane L. Adams",
                            "Lonni Besan\u00e7on",
                            "Michael Correll",
                            "R. Jordan Crouser",
                            "Charles Perin",
                            "Paul Rosen"
                        ],
                        "abstract": null,
                        "time_start": "2021-10-24T17:00:00Z",
                        "time_end": "2021-10-24T17:10:00Z"
                    },
                    {
                        "type": "live",
                        "title": "Towards a Theory of Bullshit Visualization",
                        "contributors": [
                            "Michael Correll"
                        ],
                        "abstract": null,
                        "time_start": "2021-10-24T17:10:00Z",
                        "time_end": "2021-10-24T17:20:00Z"
                    },
                    {
                        "type": "live",
                        "title": "Visualization for Villainy",
                        "contributors": [
                            "Andrew M McNutt"
                        ],
                        "abstract": null,
                        "time_start": "2021-10-24T17:20:00Z",
                        "time_end": "2021-10-24T17:30:00Z"
                    },
                    {
                        "type": "live",
                        "title": "Manifesto for Putting Chartjunk in the Trash 2021!",
                        "contributors": [
                            "Derya Akbaba"
                        ],
                        "abstract": null,
                        "time_start": "2021-10-24T17:30:00Z",
                        "time_end": "2021-10-24T17:40:00Z"
                    },
                    {
                        "type": "live",
                        "title": "Series Q&A: Alt.ercations",
                        "contributors": [
                            "Michael Correll",
                            "Andrew M McNutt",
                            "Derya Akbaba"
                        ],
                        "abstract": null,
                        "time_start": "2021-10-24T17:40:00Z",
                        "time_end": "2021-10-24T17:50:00Z"
                    },
                    {
                        "type": "live",
                        "title": "'Is IEEE VIS *that* good?'' On key factors in the initial assessment of manuscript and venue quality",
                        "contributors": [
                            "Nicholas S Spyrison"
                        ],
                        "abstract": null,
                        "time_start": "2021-10-24T17:50:00Z",
                        "time_end": "2021-10-24T18:00:00Z"
                    },
                    {
                        "type": "live",
                        "title": "Visualizations as Data Input",
                        "contributors": [
                            "Samuel Huron"
                        ],
                        "abstract": null,
                        "time_start": "2021-10-24T18:00:00Z",
                        "time_end": "2021-10-24T18:10:00Z"
                    },
                    {
                        "type": "live",
                        "title": "Publishing Visualization Studies as Registered Reports: Expected Benefits and Researchers\u2019 Attitudes",
                        "contributors": [
                            "Lonni Besan\u00e7on"
                        ],
                        "abstract": null,
                        "time_start": "2021-10-24T18:10:00Z",
                        "time_end": "2021-10-24T18:20:00Z"
                    },
                    {
                        "type": "live",
                        "title": "Series Q&A: Alt.itude",
                        "contributors": [
                            "Nicholas S Spyrison",
                            "Samuel Huron",
                            "Lonni Besan\u00e7on"
                        ],
                        "abstract": null,
                        "time_start": "2021-10-24T18:10:00Z",
                        "time_end": "2021-10-24T18:30:00Z"
                    },
                    {
                        "type": "live",
                        "title": "Illegible Semantics: Exploring the Design Space of Metal Logos",
                        "contributors": [
                            "Gerrit Rijken"
                        ],
                        "abstract": null,
                        "time_start": "2021-10-24T19:00:00Z",
                        "time_end": "2021-10-24T19:10:00Z"
                    },
                    {
                        "type": "live",
                        "title": "Touching Art - A Method for Visualizing Tactile Experience",
                        "contributors": [
                            "Bernice Rogowitz"
                        ],
                        "abstract": null,
                        "time_start": "2021-10-24T19:10:00Z",
                        "time_end": "2021-10-24T19:20:00Z"
                    },
                    {
                        "type": "live",
                        "title": "Xenakis: Data Cities and Sound",
                        "contributors": [
                            "Victor Schetinger"
                        ],
                        "abstract": null,
                        "time_start": "2021-10-24T19:20:00Z",
                        "time_end": "2021-10-24T19:30:00Z"
                    },
                    {
                        "type": "live",
                        "title": "Data Knitualization: An Exploration of Knitting as a Visualization Medium",
                        "contributors": [
                            "Noeska Natasja Smit"
                        ],
                        "abstract": null,
                        "time_start": "2021-10-24T19:30:00Z",
                        "time_end": "2021-10-24T19:40:00Z"
                    },
                    {
                        "type": "live",
                        "title": "Series Q&A: Ex.alt.ations",
                        "contributors": [
                            "Gerrit Rijken",
                            "Bernice Rogowitz",
                            "Victor Schetinger",
                            "Noeska Natasja Smit"
                        ],
                        "abstract": null,
                        "time_start": "2021-10-24T19:40:00Z",
                        "time_end": "2021-10-24T19:50:00Z"
                    },
                    {
                        "type": "live",
                        "title": "Discussion: The Future of alt.VIS",
                        "contributors": [
                            "Jane L. Adams",
                            "Lonni Besan\u00e7on"
                        ],
                        "abstract": null,
                        "time_start": "2021-10-24T19:50:00Z",
                        "time_end": "2021-10-24T20:30:00Z"
                    }
                ]
            }
        ]
    },
    "w-mlui": {
        "event": "MLUI 2021: Machine Learning from User Interaction for Visualization and Analytics",
        "long_name": "MLUI 2021: Machine Learning from User Interaction for Visualization and Analytics",
        "event_type": "Workshop",
        "event_description": "",
        "event_url": "",
        "sessions": [
            {
                "title": "MLUI 2021: Machine Learning from User Interaction for Visualization and Analytics",
                "session_id": "w-mlui",
                "track": "room5",
                "schedule_image": "w-mlui.png",
                "chair": [],
                "organizers": [
                    "John Wenskovitch",
                    "Michelle Dowling",
                    "Eli T Brown",
                    "Ab Mosca",
                    "Conny Walchshofer",
                    "Marc Streit",
                    "Kai Xu"
                ],
                "time_start": "2021-10-24T17:00:00Z",
                "time_end": "2021-10-24T20:30:00Z",
                "discord_category": "",
                "discord_channel": "",
                "discord_channel_id": 0,
                "youtube_url": null,
                "zoom_meeting": "",
                "zoom_password": "",
                "ff_link": "",
                "ff_playlist": "",
                "time_slots": [
                    {
                        "type": null,
                        "title": null,
                        "contributors": "",
                        "abstract": null,
                        "time_start": "2021-10-24T17:00:00Z",
                        "time_end": "2021-10-24T18:30:00Z"
                    },
                    {
                        "type": null,
                        "title": null,
                        "contributors": "",
                        "abstract": null,
                        "time_start": "2021-10-24T19:00:00Z",
                        "time_end": "2021-10-24T20:30:00Z"
                    }
                ]
            }
        ]
    },
    "w-hdi": {
        "event": "Human-Data Interaction",
        "long_name": "Human-Data Interaction",
        "event_type": "Workshop",
        "event_description": "",
        "event_url": "",
        "sessions": [
            {
                "title": "Human-Data Interaction",
                "session_id": "w-hdi",
                "track": "room6",
                "schedule_image": "w-hdi.png",
                "chair": [
                    "Eun Kyoung Choe",
                    "Bongshin Lee",
                    "Lyn Bartram",
                    "Melanie Tory",
                    "Sheelagh Carpendale"
                ],
                "organizers": [
                    "Lyn Bartram",
                    "Sheelagh Carpendale",
                    "Eun Kyoung Choe",
                    "Bongshin Lee",
                    "Melanie Tory"
                ],
                "time_start": "2021-10-24T17:00:00Z",
                "time_end": "2021-10-24T20:30:00Z",
                "discord_category": "",
                "discord_channel": "",
                "discord_channel_id": 0,
                "youtube_url": null,
                "zoom_meeting": null,
                "zoom_password": null,
                "ff_link": "",
                "ff_playlist": "",
                "time_slots": [
                    {
                        "type": "live",
                        "title": "Opening",
                        "contributors": [
                            "Lyn Bartram",
                            "Sheelagh Carpendale",
                            "Eun Kyoung Choe",
                            "Bongshin Lee",
                            "Melanie Tory"
                        ],
                        "abstract": null,
                        "time_start": "2021-10-24T17:00:00Z",
                        "time_end": "2021-10-24T17:10:00Z"
                    },
                    {
                        "type": "live",
                        "title": "Panel Discussion",
                        "contributors": [
                            "Lyn Bartram",
                            "Sheelagh Carpendale",
                            "Eun Kyoung Choe",
                            "Bongshin Lee",
                            "Melanie Tory"
                        ],
                        "abstract": null,
                        "time_start": "2021-10-24T17:10:00Z",
                        "time_end": "2021-10-24T18:00:00Z"
                    },
                    {
                        "type": "live",
                        "title": "Lightning Talks",
                        "contributors": [
                            "Lyn Bartram",
                            "Sheelagh Carpendale",
                            "Eun Kyoung Choe",
                            "Bongshin Lee",
                            "Melanie Tory"
                        ],
                        "abstract": null,
                        "time_start": "2021-10-24T18:00:00Z",
                        "time_end": "2021-10-24T18:20:00Z"
                    },
                    {
                        "type": "live",
                        "title": "Finalize Theme Building",
                        "contributors": [
                            "Lyn Bartram",
                            "Sheelagh Carpendale",
                            "Eun Kyoung Choe",
                            "Bongshin Lee",
                            "Melanie Tory"
                        ],
                        "abstract": null,
                        "time_start": "2021-10-24T18:20:00Z",
                        "time_end": "2021-10-24T18:30:00Z"
                    },
                    {
                        "type": "live",
                        "title": "Breakout Session",
                        "contributors": [
                            "Lyn Bartram",
                            "Sheelagh Carpendale",
                            "Eun Kyoung Choe",
                            "Bongshin Lee",
                            "Melanie Tory"
                        ],
                        "abstract": null,
                        "time_start": "2021-10-24T19:00:00Z",
                        "time_end": "2021-10-24T19:50:00Z"
                    },
                    {
                        "type": "live",
                        "title": "Report Back",
                        "contributors": [
                            "Lyn Bartram",
                            "Sheelagh Carpendale",
                            "Eun Kyoung Choe",
                            "Bongshin Lee",
                            "Melanie Tory"
                        ],
                        "abstract": null,
                        "time_start": "2021-10-24T19:50:00Z",
                        "time_end": "2021-10-24T20:20:00Z"
                    },
                    {
                        "type": "live",
                        "title": "Closing",
                        "contributors": [
                            "Lyn Bartram",
                            "Sheelagh Carpendale",
                            "Eun Kyoung Choe",
                            "Bongshin Lee",
                            "Melanie Tory"
                        ],
                        "abstract": null,
                        "time_start": "2021-10-24T20:20:00Z",
                        "time_end": "2021-10-24T20:30:00Z"
                    }
                ]
            }
        ]
    },
    "w-viscomm": {
        "event": "Fourth Workshop on Visualization for Communication (VisComm)",
        "long_name": "Fourth Workshop on Visualization for Communication (VisComm)",
        "event_type": "Workshop",
        "event_description": "",
        "event_url": "",
        "sessions": [
            {
                "title": "Fourth Workshop on Visualization for Communication (VisComm)",
                "session_id": "w-viscomm",
                "track": "room7",
                "schedule_image": "w-viscomm.png",
                "chair": [],
                "organizers": [
                    "Barbara Millet",
                    "Jonathan Schwabish",
                    "Adriana Arcia",
                    "Alvitta Ottley"
                ],
                "time_start": "2021-10-24T17:00:00Z",
                "time_end": "2021-10-24T20:30:00Z",
                "discord_category": "",
                "discord_channel": "",
                "discord_channel_id": 0,
                "youtube_url": null,
                "zoom_meeting": "",
                "zoom_password": "",
                "ff_link": "",
                "ff_playlist": "",
                "time_slots": [
                    {
                        "type": null,
                        "title": null,
                        "contributors": "",
                        "abstract": null,
                        "time_start": "2021-10-24T17:00:00Z",
                        "time_end": "2021-10-24T18:30:00Z"
                    },
                    {
                        "type": null,
                        "title": null,
                        "contributors": "",
                        "abstract": null,
                        "time_start": "2021-10-24T19:00:00Z",
                        "time_end": "2021-10-24T20:30:00Z"
                    }
                ]
            }
        ]
    },
    "a-ldav": {
        "event": "LDAV",
        "long_name": "LDAV",
        "event_type": "Associated Event",
        "event_description": "",
        "event_url": "",
        "sessions": [
            {
                "title": "Keynote and Best Paper",
                "session_id": "a-ldav-1",
                "track": "room1",
                "schedule_image": "a-ldav-1.png",
                "chair": [
                    "Julien Tierny"
                ],
                "organizers": [
                    "Kristi Potter",
                    "Julien Tierny",
                    "Chaoli Wang"
                ],
                "time_start": "2021-10-25T13:00:00Z",
                "time_end": "2021-10-25T14:30:00Z",
                "discord_category": "",
                "discord_channel": "",
                "discord_channel_id": 0,
                "youtube_url": null,
                "zoom_meeting": "",
                "zoom_password": "",
                "ff_link": "",
                "ff_playlist": "",
                "time_slots": [
                    {
                        "type": "live",
                        "title": "Opening Presentation",
                        "contributors": [
                            "Julien Tierny"
                        ],
                        "abstract": null,
                        "time_start": "2021-10-25T13:00:00Z",
                        "time_end": "2021-10-25T13:10:00Z"
                    },
                    {
                        "type": "live",
                        "title": "Keynote by Alex Telea, Utrecht University",
                        "contributors": [
                            "Alex Telea"
                        ],
                        "abstract": null,
                        "time_start": "2021-10-25T13:10:00Z",
                        "time_end": "2021-10-25T14:00:00Z"
                    },
                    {
                        "type": "recorded",
                        "title": "Data-Aware Predictive Scheduling for Distributed-Memory Ray Tracing",
                        "contributors": [
                            "Hyungman Park"
                        ],
                        "abstract": null,
                        "time_start": "2021-10-25T14:00:00Z",
                        "time_end": "2021-10-25T14:30:00Z",
                        "uid": "a-ldav-1002",
                        "youtube_video_id": "dcnePG3JEyI"
                    }
                ]
            },
            {
                "title": "Algorithms",
                "session_id": "a-ldav-2",
                "track": "room1",
                "schedule_image": "a-ldav-2.png",
                "chair": [
                    "Filip Sadlo"
                ],
                "organizers": [
                    "Kristi Potter",
                    "Julien Tierny",
                    "Chaoli Wang"
                ],
                "time_start": "2021-10-25T15:00:00Z",
                "time_end": "2021-10-25T16:30:00Z",
                "discord_category": "",
                "discord_channel": "",
                "discord_channel_id": 0,
                "youtube_url": null,
                "zoom_meeting": "",
                "zoom_password": "",
                "ff_link": "",
                "ff_playlist": "",
                "time_slots": [
                    {
                        "type": "recorded",
                        "title": "Fast Approximation of Persistence Diagrams with Guarantees",
                        "contributors": [
                            "Jules Vidal"
                        ],
                        "abstract": null,
                        "time_start": "2021-10-25T15:00:00Z",
                        "time_end": "2021-10-25T16:30:00Z",
                        "uid": "a-ldav-1000",
                        "youtube_video_id": "QrgjeZ-pJVQ"
                    },
                    {
                        "type": "recorded",
                        "title": "IExchange: Asynchronous Communication and Termination Detection for Iterative Algorithms",
                        "contributors": [
                            "Tom Peterka"
                        ],
                        "abstract": null,
                        "time_start": "2021-10-25T15:00:00Z",
                        "time_end": "2021-10-25T16:30:00Z",
                        "uid": "a-ldav-1003",
                        "youtube_video_id": "SEhRBQRJneY"
                    },
                    {
                        "type": "recorded",
                        "title": "Trigger Happy: Assessing the Viability of Trigger-Based In Situ Analysis",
                        "contributors": [
                            "Matthew Larsen"
                        ],
                        "abstract": null,
                        "time_start": "2021-10-25T15:00:00Z",
                        "time_end": "2021-10-25T16:30:00Z",
                        "uid": "a-ldav-1009",
                        "youtube_video_id": "9fz8_3rWgiM"
                    },
                    {
                        "type": "recorded",
                        "title": "High-quality and Low-memory-footprint Progressive Decoding of Large-scale Particle Data",
                        "contributors": [
                            "Duong Hoang"
                        ],
                        "abstract": null,
                        "time_start": "2021-10-25T15:00:00Z",
                        "time_end": "2021-10-25T16:30:00Z",
                        "uid": "a-ldav-1015",
                        "youtube_video_id": "tuPUwyG_YBA"
                    }
                ]
            },
            {
                "title": "Render/Display",
                "session_id": "a-ldav-3",
                "track": "room1",
                "schedule_image": "a-ldav-3.png",
                "chair": [
                    "Kelly Gaither"
                ],
                "organizers": [
                    "Kristi Potter",
                    "Julien Tierny",
                    "Chaoli Wang"
                ],
                "time_start": "2021-10-25T17:00:00Z",
                "time_end": "2021-10-25T18:30:00Z",
                "discord_category": "",
                "discord_channel": "",
                "discord_channel_id": 0,
                "youtube_url": null,
                "zoom_meeting": "",
                "zoom_password": "",
                "ff_link": "",
                "ff_playlist": "",
                "time_slots": [
                    {
                        "type": "recorded",
                        "title": "GPU-based Image Compression for Efficient Compositing in Distributed Rendering Applications",
                        "contributors": [
                            "Riley D. Lipinski"
                        ],
                        "abstract": null,
                        "time_start": "2021-10-25T17:00:00Z",
                        "time_end": "2021-10-25T18:30:00Z",
                        "uid": "a-ldav-1007",
                        "youtube_video_id": "MAm8Wn2LjCA"
                    },
                    {
                        "type": "recorded",
                        "title": "Amortised Encoding for Large High-Resolution Displays",
                        "contributors": [
                            "Florian Frie\u00df"
                        ],
                        "abstract": null,
                        "time_start": "2021-10-25T17:00:00Z",
                        "time_end": "2021-10-25T18:30:00Z",
                        "uid": "a-ldav-1019",
                        "youtube_video_id": "VOChOmbSk0k"
                    },
                    {
                        "type": "recorded",
                        "title": "Portable and Composable Flow Graphs for In Situ Analytics",
                        "contributors": [
                            "Sergei Shudler"
                        ],
                        "abstract": null,
                        "time_start": "2021-10-25T17:00:00Z",
                        "time_end": "2021-10-25T18:30:00Z",
                        "uid": "a-ldav-1021",
                        "youtube_video_id": "4fiRHoHs2dg"
                    },
                    {
                        "type": "recorded",
                        "title": "An Entropy-Based Approach for Identifying User-Preferred Camera Positions",
                        "contributors": [
                            "Nicole J. Marsaglia"
                        ],
                        "abstract": null,
                        "time_start": "2021-10-25T17:00:00Z",
                        "time_end": "2021-10-25T18:30:00Z",
                        "uid": "a-ldav-1022",
                        "youtube_video_id": "Kpg3RM6rB8w"
                    }
                ]
            },
            {
                "title": "Early Career Research Talks, Best Paper Awards and Closing Remarks",
                "session_id": "a-ldav-4",
                "track": "room1",
                "schedule_image": "a-ldav-4.png",
                "chair": [
                    "Chaoli Wang",
                    "Kristi Potter",
                    "Johanna Beyer"
                ],
                "organizers": [
                    "Kristi Potter",
                    "Julien Tierny",
                    "Chaoli Wang"
                ],
                "time_start": "2021-10-25T19:00:00Z",
                "time_end": "2021-10-25T20:30:00Z",
                "discord_category": "",
                "discord_channel": "",
                "discord_channel_id": 0,
                "youtube_url": null,
                "zoom_meeting": "",
                "zoom_password": "",
                "ff_link": "",
                "ff_playlist": "",
                "time_slots": [
                    {
                        "type": "live",
                        "title": null,
                        "contributors": [
                            "James Kress"
                        ],
                        "abstract": null,
                        "time_start": "2021-10-25T19:00:00Z",
                        "time_end": "2021-10-25T19:13:00Z"
                    },
                    {
                        "type": "live",
                        "title": null,
                        "contributors": [
                            "Jun Han"
                        ],
                        "abstract": null,
                        "time_start": "2021-10-25T19:13:00Z",
                        "time_end": "2021-10-25T19:26:00Z"
                    },
                    {
                        "type": "live",
                        "title": null,
                        "contributors": [
                            "Sam Molnar"
                        ],
                        "abstract": null,
                        "time_start": "2021-10-25T19:26:00Z",
                        "time_end": "2021-10-25T19:39:00Z"
                    },
                    {
                        "type": "live",
                        "title": null,
                        "contributors": [
                            "Takanori Fujiwara"
                        ],
                        "abstract": null,
                        "time_start": "2021-10-25T19:39:00Z",
                        "time_end": "2021-10-25T19:52:00Z"
                    },
                    {
                        "type": "live",
                        "title": null,
                        "contributors": [
                            "Victor Mateevitsi"
                        ],
                        "abstract": null,
                        "time_start": "2021-10-25T19:52:00Z",
                        "time_end": "2021-10-25T20:05:00Z"
                    },
                    {
                        "type": "live",
                        "title": null,
                        "contributors": [
                            "Sudhanshu Sane"
                        ],
                        "abstract": null,
                        "time_start": "2021-10-25T19:05:00Z",
                        "time_end": "2021-10-25T20:18:00Z"
                    },
                    {
                        "type": "live",
                        "title": null,
                        "contributors": [
                            "Event Chairs"
                        ],
                        "abstract": null,
                        "time_start": "2021-10-25T20:20:00Z",
                        "time_end": "2021-10-25T20:30:00Z"
                    }
                ]
            }
        ]
    },
    "a-visinpractice": {
        "event": "VisInPractice",
        "long_name": "VisInPractice",
        "event_type": "Associated Event",
        "event_description": "",
        "event_url": "",
        "sessions": [
            {
                "title": "Panel on Moving between Academia and Industry",
                "session_id": "a-visinpractice-1",
                "track": "room2",
                "schedule_image": "a-visinpractice-1.png",
                "chair": [
                    "Zhicheng Leo Liu"
                ],
                "organizers": [
                    "Matthew Brehmer",
                    "Matt Larsen",
                    "Zhicheng (Leo) Liu",
                    "Sean McKenna"
                ],
                "time_start": "2021-10-25T13:00:00Z",
                "time_end": "2021-10-25T14:30:00Z",
                "discord_category": "",
                "discord_channel": "",
                "discord_channel_id": 0,
                "youtube_url": null,
                "zoom_meeting": "",
                "zoom_password": "",
                "ff_link": "",
                "ff_playlist": "",
                "time_slots": [
                    {
                        "type": "live",
                        "title": "Panel Intro",
                        "contributors": [
                            "Zhicheng Leo Liu"
                        ],
                        "abstract": null,
                        "time_start": "2021-10-25T13:00:00Z",
                        "time_end": "2021-10-25T13:05:00Z"
                    },
                    {
                        "type": "live",
                        "title": "Panel",
                        "contributors": [
                            "Hank Childs",
                            "Miguel Encarna\u00e7\u00e3o",
                            "Melanie Tory",
                            "Jian Zhao"
                        ],
                        "abstract": "Many of us attending IEEE VIS have spent our entire careers in either academia or industry, and sometimes wonder about the grass being greener on the other side. In this panel we bring together four panelists who have experiences in both academia and industry to discuss various topics related to moving between the two sectors. Why does one move back and forth? In what ways are academia and industry similar or different? What are the differences between research labs and product teams in the industry? What affects one\u2019s movability from one place to another?\u00a0 The panelists include Hank Childs (University of Oregon), Miguel Encarna\u00e7\u00e3o (Regions Financial Corporation and University of Iowa), Melanie Tory (Northeastern University) and Jian Zhao (University of Waterloo). ",
                        "time_start": "2021-10-25T13:05:00Z",
                        "time_end": "2021-10-25T14:30:00Z"
                    }
                ]
            },
            {
                "title": "Panel on the Tools of the Trade",
                "session_id": "a-visinpractice-2",
                "track": "room2",
                "schedule_image": "a-visinpractice-2.png",
                "chair": [
                    "Sean McKenna"
                ],
                "organizers": [
                    "Matthew Brehmer",
                    "Matt Larsen",
                    "Zhicheng (Leo) Liu",
                    "Sean McKenna"
                ],
                "time_start": "2021-10-25T15:00:00Z",
                "time_end": "2021-10-25T16:30:00Z",
                "discord_category": "",
                "discord_channel": "",
                "discord_channel_id": 0,
                "youtube_url": null,
                "zoom_meeting": "",
                "zoom_password": "",
                "ff_link": "",
                "ff_playlist": "",
                "time_slots": [
                    {
                        "type": "live",
                        "title": "Panel Intro",
                        "contributors": [
                            "Sean McKenna"
                        ],
                        "abstract": null,
                        "time_start": "2021-10-25T15:00:00Z",
                        "time_end": "2021-10-25T15:05:00Z"
                    },
                    {
                        "type": "live",
                        "title": "Panel",
                        "contributors": [
                            "Zan Armstrong",
                            "Stephanie Kirmer",
                            "Nicolas Kruchten",
                            "Krist Wongsuphasawat"
                        ],
                        "abstract": "Today, data visualization practitioners are flooded with a barrage of different tools at their disposal, from libraries (D3) to applications (Tableau, Power BI) to entire platforms (Observable, Jupyter). How do we manage it all? Come explore and discuss this space with industry practitioners and experts that consume with these tools and build new tools on a daily basis. What limitations and trade-offs are there when selecting between different tools for different audiences? How can we promote stronger engagement between researchers and practitioners as we build these tools? What upcoming challenges will we face with the next generation of data vis tools? \n        \nThe panelists include: Zan Armstrong (Observable), Stephanie Kirmer (Saturn Cloud), Nicolas Kruchten (Plotly), and Krist Wongsuphasawat (Airbnb).",
                        "time_start": "2021-10-25T15:05:00Z",
                        "time_end": "2021-10-25T16:30:00Z"
                    }
                ]
            },
            {
                "title": "Panel on Practitioner Collaborations",
                "session_id": "a-visinpractice-3",
                "track": "room2",
                "schedule_image": "a-visinpractice-3.png",
                "chair": [
                    "Matt Larsen"
                ],
                "organizers": [
                    "Matthew Brehmer",
                    "Matt Larsen",
                    "Zhicheng (Leo) Liu",
                    "Sean McKenna"
                ],
                "time_start": "2021-10-25T17:00:00Z",
                "time_end": "2021-10-25T18:30:00Z",
                "discord_category": "",
                "discord_channel": "",
                "discord_channel_id": 0,
                "youtube_url": null,
                "zoom_meeting": "",
                "zoom_password": "",
                "ff_link": "",
                "ff_playlist": "",
                "time_slots": [
                    {
                        "type": "live",
                        "title": "Panel Intro",
                        "contributors": [
                            "Matt Larsen"
                        ],
                        "abstract": null,
                        "time_start": "2021-10-25T17:00:00Z",
                        "time_end": "2021-10-25T17:05:00Z"
                    },
                    {
                        "type": "live",
                        "title": "Panel",
                        "contributors": [
                            "Cyrus Harrison",
                            "Janine Bennett",
                            "Axel Huebl",
                            "Renata Raidou",
                            "Steve Morley"
                        ],
                        "abstract": null,
                        "time_start": "2021-10-25T17:05:00Z",
                        "time_end": "2021-10-25T18:30:00Z"
                    }
                ]
            },
            {
                "title": "Panel on Writing About Visualization",
                "session_id": "a-visinpractice-4",
                "track": "room2",
                "schedule_image": "a-visinpractice-4.png",
                "chair": [
                    "Matthew Brehmer"
                ],
                "organizers": [
                    "Matthew Brehmer",
                    "Matt Larsen",
                    "Zhicheng (Leo) Liu",
                    "Sean McKenna"
                ],
                "time_start": "2021-10-25T19:00:00Z",
                "time_end": "2021-10-25T20:30:00Z",
                "discord_category": "",
                "discord_channel": "",
                "discord_channel_id": 0,
                "youtube_url": null,
                "zoom_meeting": "",
                "zoom_password": "",
                "ff_link": "",
                "ff_playlist": "",
                "time_slots": [
                    {
                        "type": "recorded",
                        "title": "Panel Intro",
                        "contributors": [
                            "Mary Aviles",
                            "Tiziana Alocci",
                            "Piero Zagami",
                            "Christian Miles",
                            "Danielle Szafir"
                        ],
                        "abstract": "While IEEE VIS attendees are accustomed to reading and writing academic papers about data visualization, they may be less familiar with non-academic writing dedicated to the subject, writing that is intended primarily for audiences of visualization practitioners. This panel brings together the editors of four unique publications, each with a unique format and set of perspectives on visualization. The panelists include Mary Aviles (editor of Nightingale, the trade journal of the Data Visualization Society), Tiziana Alocci & Piero Zagami (co-founders of of Market Cafe Magazine), Christian Miles (editor of the Source / Target graph newsletter), and Danielle Szafir (editor of Multiple Views: Visualization Research Explained). Our discussion of publication origin stories and lessons learned will inspire VIS attendees to consider alternative publications, audiences, and formats for communicating their work.",
                        "time_start": "2021-10-25T19:00:00Z",
                        "time_end": "2021-10-25T19:10:00Z",
                        "youtube_video_id": "n6dP2gc8fcA"
                    },
                    {
                        "type": "live",
                        "title": "Panel",
                        "contributors": [
                            "Mary Aviles",
                            "Tiziana Alocci",
                            "Piero Zagami",
                            "Christian Miles",
                            "Danielle Szafir"
                        ],
                        "abstract": "While IEEE VIS attendees are accustomed to reading and writing academic papers about data visualization, they may be less familiar with non-academic writing dedicated to the subject, writing that is intended primarily for audiences of visualization practitioners. This panel brings together the editors of four unique publications, each with a unique format and set of perspectives on visualization. The panelists include Mary Aviles (editor of Nightingale, the trade journal of the Data Visualization Society), Tiziana Alocci & Piero Zagami (co-founders of of Market Cafe Magazine), Christian Miles (editor of the Source / Target graph newsletter), and Danielle Szafir (editor of Multiple Views: Visualization Research Explained). Our discussion of publication origin stories and lessons learned will inspire VIS attendees to consider alternative publications, audiences, and formats for communicating their work.",
                        "time_start": "2021-10-25T19:10:00Z",
                        "time_end": "2021-10-25T20:30:00Z"
                    }
                ]
            }
        ]
    },
    "a-biovischallenge": {
        "event": "BioVis Challenge",
        "long_name": "BioVis Challenge",
        "event_type": "Associated Event",
        "event_description": "",
        "event_url": "",
        "sessions": [
            {
                "title": "BioVis Challenge",
                "session_id": "a-biovischallenge",
                "track": "room3",
                "schedule_image": "a-biovischallenge.png",
                "chair": [
                    "Zeynep H Gumus",
                    "Thomas H\u00f6llt"
                ],
                "organizers": [
                    "Zeynep H Gumus",
                    "Thomas H\u00f6llt"
                ],
                "time_start": "2021-10-25T13:00:00Z",
                "time_end": "2021-10-25T16:30:00Z",
                "discord_category": "",
                "discord_channel": "",
                "discord_channel_id": 0,
                "youtube_url": null,
                "zoom_meeting": "",
                "zoom_password": "",
                "ff_link": "",
                "ff_playlist": "",
                "time_slots": [
                    {
                        "type": "live",
                        "title": "Welcome by the Organizers",
                        "contributors": [
                            "Zeynep H Gumus",
                            "Thomas H\u00f6llt"
                        ],
                        "abstract": null,
                        "time_start": "2021-10-25T13:00:00Z",
                        "time_end": "2021-10-25T13:05:00Z"
                    },
                    {
                        "type": "live",
                        "title": "Integrating methods to annotate, register, segment, cluster, QC, and correlate high-dimensional spatial data from whole tumor tissues",
                        "contributors": [
                            "Sacha Gnjatic"
                        ],
                        "abstract": null,
                        "time_start": "2021-10-25T13:05:00Z",
                        "time_end": "2021-10-25T13:35:00Z"
                    },
                    {
                        "type": "live",
                        "title": "From Visual Analytics to Interactive Data Stories for Large Multiplexed Images of Cancer",
                        "contributors": [
                            "Robert Krueger"
                        ],
                        "abstract": null,
                        "time_start": "2021-10-25T13:35:00Z",
                        "time_end": "2021-10-25T14:05:00Z"
                    },
                    {
                        "type": "recorded",
                        "title": "High-dimensional spatial-omics data, a blessing and a curse",
                        "contributors": [
                            "Alma Anderson"
                        ],
                        "abstract": null,
                        "time_start": "2021-10-25T14:05:00Z",
                        "time_end": "2021-10-25T14:30:00Z"
                    },
                    {
                        "type": "live",
                        "title": "Challenge Introduction",
                        "contributors": [
                            "Noel F.C.C. De Miranda"
                        ],
                        "abstract": null,
                        "time_start": "2021-10-25T15:00:00Z",
                        "time_end": "2021-10-25T15:30:00Z"
                    },
                    {
                        "type": "recorded",
                        "title": "Challenge Talk: Guo-Cheng Yuan",
                        "contributors": [
                            "Guo-Cheng Yuan"
                        ],
                        "abstract": null,
                        "time_start": "2021-10-25T15:30:00Z",
                        "time_end": "2021-10-25T15:50:00Z",
                        "youtube_video_id": "JPKXWi72ywg"
                    },
                    {
                        "type": "live",
                        "title": "Spatial Omics Visualizations: Lessons Learned from Networks and Maps",
                        "contributors": [
                            "Alexander Lex"
                        ],
                        "abstract": null,
                        "time_start": "2021-10-25T16:00:00Z",
                        "time_end": "2021-10-25T16:30:00Z"
                    }
                ]
            }
        ]
    },
    "w-visactivities": {
        "event": "2nd IEEE VIS Workshop on Data Vis Activities to Facilitate Learning, Reflecting, Discussing and Designing",
        "long_name": "2nd IEEE VIS Workshop on Data Vis Activities to Facilitate Learning, Reflecting, Discussing and Designing",
        "event_type": "Workshop",
        "event_description": "",
        "event_url": "",
        "sessions": [
            {
                "title": "2nd IEEE VIS Workshop on Data Vis Activities to Facilitate Learning, Reflecting, Discussing and Designing",
                "session_id": "w-visactivities",
                "track": "room4",
                "schedule_image": "w-visactivities.png",
                "chair": [],
                "organizers": [
                    "Samuel Huron",
                    "Benjamin Bach",
                    "Georgia Panagiotidou",
                    "Mandy Keck",
                    "Jonathan C. Roberts",
                    "Sheelagh Carpendale"
                ],
                "time_start": "2021-10-25T13:00:00Z",
                "time_end": "2021-10-25T20:30:00Z",
                "discord_category": "",
                "discord_channel": "",
                "discord_channel_id": 0,
                "youtube_url": null,
                "zoom_meeting": "",
                "zoom_password": "",
                "ff_link": "",
                "ff_playlist": "",
                "time_slots": [
                    {
                        "type": null,
                        "title": null,
                        "contributors": "",
                        "abstract": null,
                        "time_start": "2021-10-25T13:00:00Z",
                        "time_end": "2021-10-25T14:30:00Z"
                    },
                    {
                        "type": null,
                        "title": null,
                        "contributors": "",
                        "abstract": null,
                        "time_start": "2021-10-25T15:00:00Z",
                        "time_end": "2021-10-25T16:30:00Z"
                    },
                    {
                        "type": null,
                        "title": null,
                        "contributors": "",
                        "abstract": null,
                        "time_start": "2021-10-25T17:00:00Z",
                        "time_end": "2021-10-25T18:30:00Z"
                    },
                    {
                        "type": null,
                        "title": null,
                        "contributors": "",
                        "abstract": null,
                        "time_start": "2021-10-25T19:00:00Z",
                        "time_end": "2021-10-25T20:30:00Z"
                    }
                ]
            }
        ]
    },
    "w-visxvision": {
        "event": "VisXVision: Workshop on Novel Directions in Vision Science and Visualization Research",
        "long_name": "VisXVision: Workshop on Novel Directions in Vision Science and Visualization Research",
        "event_type": "Workshop",
        "event_description": "",
        "event_url": "",
        "sessions": [
            {
                "title": "VisXVision: Workshop on Novel Directions in Vision Science and Visualization Research",
                "session_id": "w-visxvision",
                "track": "room5",
                "schedule_image": "w-visxvision.png",
                "chair": [
                    "Danielle Szafir",
                    "Christie Nothelfer",
                    "Madison Elliott",
                    "Cindy Xiong"
                ],
                "organizers": [
                    "Cindy Xiong",
                    "Christine Nothelfer",
                    "Madison Elliott",
                    "Danielle Albers Szafir"
                ],
                "time_start": "2021-10-25T13:00:00Z",
                "time_end": "2021-10-25T16:30:00Z",
                "discord_category": "",
                "discord_channel": "",
                "discord_channel_id": 0,
                "youtube_url": null,
                "zoom_meeting": null,
                "zoom_password": null,
                "ff_link": "",
                "ff_playlist": "",
                "time_slots": [
                    {
                        "type": "live",
                        "title": "Opening Remarks",
                        "contributors": [
                            "Madison Elliott"
                        ],
                        "abstract": null,
                        "time_start": "2021-10-25T13:00:00Z",
                        "time_end": "2021-10-25T13:05:00Z"
                    },
                    {
                        "type": "live",
                        "title": "Effects of direct color-concept associations on interpretations of colormap information visualizations",
                        "contributors": [
                            "Melissa Schoeniein"
                        ],
                        "abstract": "Colormap information visualizations use gradations of color to communicate patterns of data (e.g., weather maps and neuroimaging scans). Observers have expectations about how visual properties of colormaps will map onto properties of data, which influence their interpretations. For example, the dark-is-more bias leads observers to infer darker colors map to larger quantities. Previous studies on this bias tested visualizations representing ambiguous or novel domains to avoid effects of direct color-concept associations. Here, we investigated whether direct associations interact with the dark-is-more bias to influence expectations about the meanings of colors in colormaps. Participants saw colormaps representing fictitious data about amounts of shade or sunshine across geographical regions. Concept (shade/sunshine) varied between-subjects. All colormaps varied in lightness; one side was lighter and the other was darker. The task was to indicate which side had more shade/sunshine. In half the colormaps, the darker color was far more associated with shade, and less associated with sunshine, than the lighter color (high association difference). Thus, the dark-is-more bias and direct associations aligned for shade and conflicted for sunshine. In the other half, the light and dark colors were both weakly associated with shade and sunshine (low association difference). Thus, there was little alignment/conflict between direct associations and the dark-is-more bias. For colormaps with high association difference, participants inferred that darker colors mapped to more shade and lighter colors mapped to more sunshine, indicating that strong direct associations can override the dark-is-more bias. However, for colormaps with low association difference, participants inferred that darker colors mapped to larger amounts of both shade and sunshine, consistent with the dark-is-more bias. This latter result may be surprising, given that sunshine is inherently light. These results emphasize the importance of considering direct and relational associations to anticipate expectations about the meanings of colors in information visualizations.",
                        "time_start": "2021-10-25T13:05:00Z",
                        "time_end": "2021-10-25T13:15:00Z"
                    },
                    {
                        "type": "live",
                        "title": "Show or Tell? Visual and Verbal Representations Bias Position Recall",
                        "contributors": [
                            "Cristina Ceja"
                        ],
                        "abstract": "When we view visualizations, we not only have a visual representation of the data, but also a verbal one. Recent work has shown that these visual representations of data can be biased, such that the position of a line in a chart will be consistently underestimated. But are the verbal representations of position encodings also biased in the same manner, or is this a purely visual bias that can be mitigated with verbal context? We explored the bias in position reproductions for simple uniform lines for both visual and verbal representations. We find that the direction of the bias changed depending on the response modality, with visual reproductions showing a position underestimation while verbal responses showed overestimation. This finding indicates that, even for simple line charts, biases are still present for both visual and verbal representations, although the directionality of this bias depends on the modality.",
                        "time_start": "2021-10-25T13:15:00Z",
                        "time_end": "2021-10-25T13:25:00Z"
                    },
                    {
                        "type": "live",
                        "title": "Different Visualizations of Machine Learning Outputs Influence the Speed and Accuracy of User Evaluations",
                        "contributors": [
                            "Laura Matzen"
                        ],
                        "abstract": "In this study, participants completed a visual search task with the assistance of mock machine learning (ML) outputs that indicated the location of targets in images. In a baseline condition, the participants searched for targets without the assistance of any ML indicators. In five other conditions, the ML outputs were presented to the users in different ways (bounding box, heat map, histogram, color coding, and text). In each visualization condition, the ML outputs could be correct (hits or correct rejections) or incorrect (misses or false alarms). When a target was present in the image and correctly identified by the ML output, participants responded faster and more accurately than they did in the baseline condition, regardless of how the output was visualized. Their speed and accuracy were best in the conditions in which the ML outputs were superimposed onto the image itself, rather than being presented adjacent to the image. However, when the superimposed ML outputs were incorrect, the participants performed significantly worse than they did in the baseline condition. Incorrect indicators that were adjacent to the image were less detrimental to the participants\u2019 performance.",
                        "time_start": "2021-10-25T13:25:00Z",
                        "time_end": "2021-10-25T13:35:00Z"
                    },
                    {
                        "type": "live",
                        "title": "Illusory Area Perception in Bubble Plots",
                        "contributors": [
                            "Evan Anderson"
                        ],
                        "abstract": "Perception of area has been shown to be imprecise and susceptible to systematic biases. We assess the impact of numerosity (i.e., the number of points) on the perception of cumulative area, using bubble plots as a case study. We identify conditions where display equal in cumulative area, but different in numerosity, are perceived as having different cumulative areas. We also identify conditions where displays unequal in cumulative area, but different in numerosity, are perceived as being equal. These findings suggest that perception of area can be distorted by aspects of the plot other than true mathematical area.",
                        "time_start": "2021-10-25T13:35:00Z",
                        "time_end": "2021-10-25T13:45:00Z"
                    },
                    {
                        "type": "live",
                        "title": "Visualization Using Field of Light Displays: Opportunities and New Questions",
                        "contributors": [
                            "Matthew Hamilton"
                        ],
                        "abstract": "Visualization techniques are typically evaluated on conventional 2D displays. Current immersive displasys technology such as head-mounted displays (HMDs) and existing (and future) field of light (FoLD) 3D displays can present visual information with additional perceptual cues not found in existing 2D displays. We review immersive technology and existing studies which suggest that additional perceptual cues (e.g. stereoscopy and focal cues) from 3D displays can enhance visualization task performance. This suggests potential new studies which measure the effect of additional perceptual cues and their influence on effectiveness of visualization methods. It is our hypothesis that effective visualization techniques in the 2D setting may exhibit less optimal behavior when viewed on FoLDs and newer, more optimal methods will need to be developed and evaluated. We consider the problem of visualizing data which contains interesting structures across multiple scales. We hypothesize that the additional perceptual cues of FoLDs can enhance perception of shape structures across multiple scales and we suggest several possible approaches for studying this.",
                        "time_start": "2021-10-25T13:45:00Z",
                        "time_end": "2021-10-25T13:55:00Z"
                    },
                    {
                        "type": "live",
                        "title": "How Not to Study a Visualization",
                        "contributors": [
                            "Ron Rensink"
                        ],
                        "abstract": "Twelve problems are described which can potentially hobble the study of how a given visualization works. Examples are given of each problem, along with suggestions as to how to fix it. The hope is that if these suggestions are followed, researchers may improve their chances of carrying out investigations that have a lasting impact on the field.",
                        "time_start": "2021-10-25T13:55:00Z",
                        "time_end": "2021-10-25T14:05:00Z"
                    },
                    {
                        "type": "live",
                        "title": "Paper Talk Panel",
                        "contributors": [
                            "Madison Elliott"
                        ],
                        "abstract": null,
                        "time_start": "2021-10-25T14:05:00Z",
                        "time_end": "2021-10-25T14:15:00Z"
                    },
                    {
                        "type": "live",
                        "title": "Towards a science of visual health risk communication",
                        "contributors": [
                            "Todd Horowitz"
                        ],
                        "abstract": "Communicating quantitative information about risk is central to cancer control, and across public health and medicine. Persuading people to adopt healthier behaviors, helping patients choose between courses of treatment with complex outcomes, deciding whether or not to sign up for a clinical trial; all of these scenarios involve accurately communicating probabilistic information about risk to laypeople. Even highly educated people have difficulty understanding probabilistic health information when communicated verbally. The solution has been to shift to graphic representations of probability. Unfortunately, the science of health graphics is atheoretical and largely ignores developments in basic vision science, leading to suboptimal outcomes and lack of replicability. Conversely, work on quantitative perception in vision science is rarely conducted in a health context. Visualization science has been primarily focused on serving physicians. It is time for a science of visual risk communication aimed at improving the visual communication of health risks and probabilities to patients and the public at large, across all educational and socioeconomic groups. This effort requires contributions from medicine, psychophysics, visualization, cognitive psychology, and affective science, among other disciplines.",
                        "time_start": "2021-10-25T14:15:00Z",
                        "time_end": "2021-10-25T14:30:00Z"
                    },
                    {
                        "type": "live",
                        "title": "Welcome Back",
                        "contributors": [
                            "Cindy Xiong"
                        ],
                        "abstract": null,
                        "time_start": "2021-10-25T15:00:00Z",
                        "time_end": "2021-10-25T15:01:00Z"
                    },
                    {
                        "type": "live",
                        "title": "Dilemma of visual working memory: visual working memory gets distorted when used for perceptual comparisons",
                        "contributors": [
                            "Kei Fukuda"
                        ],
                        "abstract": "How do we look for a friend on a crowded street? Many theories propose that we actively represent a memory representation of the friend in our visual working memory (VWM) and compare it with the individuals you see on the street. Although past studies have successfully characterized the capacity and quality of VWM, the consequence of its usage in perceptual comparisons has been largely unknown. In this talk, I will demonstrate that VWM representations get distorted when we use them for perceptual comparisons with new visual inputs, especially when the inputs are subjectively similar to the VWM representations. Furthermore, I will show that this similarity-induced memory bias (SIMB) occurs for both simple (e.g., color, shape) and complex stimuli (e.g., real-world objects, faces) that are perceptually encoded and retrieved from long-term memory. Given the observed versatility of the SIMB, its implication for other memory distortion phenomena (e.g., distractor-induced distortion, misinformation effect) will be discussed.",
                        "time_start": "2021-10-25T15:01:00Z",
                        "time_end": "2021-10-25T15:16:00Z"
                    },
                    {
                        "type": "live",
                        "title": "Attentional and Perceptual Biases of Climate Change",
                        "contributors": [
                            "Jiaying Zhao"
                        ],
                        "abstract": "Climate change is the most significant global challenge facing humanity, but many people still remain skeptical and refuse to take actions despite the unequivocal scientific evidence. In this talk, I discuss a number of attentional and perceptual biases that contribute to the public polarization on climate change. These biases include differences in attentional and visual processing driven by political orientation, exaggerated perceptions of out-group and in-group norms, and underestimations of greenhouse gas emissions of common objects and actions. I further propose communication approaches such as visualization and framing to mitigate some of these biases, with the broader goal of minimizing polarizing views and promoting public actions to address climate change.",
                        "time_start": "2021-10-25T15:16:00Z",
                        "time_end": "2021-10-25T15:31:00Z"
                    },
                    {
                        "type": "live",
                        "title": "Invited Speaker Panel",
                        "contributors": [
                            "Cindy Xiong"
                        ],
                        "abstract": null,
                        "time_start": "2021-10-25T15:31:00Z",
                        "time_end": "2021-10-25T15:55:00Z"
                    },
                    {
                        "type": "live",
                        "title": "Workshopping Introduction",
                        "contributors": [
                            "Danielle Szafir"
                        ],
                        "abstract": null,
                        "time_start": "2021-10-25T15:55:00Z",
                        "time_end": "2021-10-25T16:00:00Z"
                    },
                    {
                        "type": "live",
                        "title": "Workshopping Session",
                        "contributors": [
                            "Danielle Szafir"
                        ],
                        "abstract": null,
                        "time_start": "2021-10-25T16:00:00Z",
                        "time_end": "2021-10-25T16:25:00Z"
                    },
                    {
                        "type": "live",
                        "title": "Closing Remarks/Recruiting",
                        "contributors": [
                            "Madison Elliot"
                        ],
                        "abstract": null,
                        "time_start": "2021-10-25T16:25:00Z",
                        "time_end": "2021-10-25T16:30:00Z"
                    }
                ]
            }
        ]
    },
    "w-vis4good": {
        "event": "Visualization for Social Good",
        "long_name": "Visualization for Social Good",
        "event_type": "Workshop",
        "event_description": "",
        "event_url": "",
        "sessions": [
            {
                "title": "Visualization for Social Good",
                "session_id": "w-vis4good",
                "track": "room6",
                "schedule_image": "w-vis4good.png",
                "chair": [],
                "organizers": [
                    "Leilani Battle",
                    "Michelle Borkin",
                    "Michael Correll",
                    "Lane Harrison",
                    "Evan Peck"
                ],
                "time_start": "2021-10-25T13:00:00Z",
                "time_end": "2021-10-25T16:30:00Z",
                "discord_category": "",
                "discord_channel": "",
                "discord_channel_id": 0,
                "youtube_url": null,
                "zoom_meeting": "",
                "zoom_password": "",
                "ff_link": "",
                "ff_playlist": "",
                "time_slots": [
                    {
                        "type": null,
                        "title": null,
                        "contributors": "",
                        "abstract": null,
                        "time_start": "2021-10-25T13:00:00Z",
                        "time_end": "2021-10-25T14:30:00Z"
                    },
                    {
                        "type": null,
                        "title": null,
                        "contributors": "",
                        "abstract": null,
                        "time_start": "2021-10-25T15:00:00Z",
                        "time_end": "2021-10-25T16:30:00Z"
                    }
                ]
            }
        ]
    },
    "t-usereval": {
        "event": "User-Centred Evaluation in Visualization",
        "long_name": "User-Centred Evaluation in Visualization",
        "event_type": "Tutorial",
        "event_description": "",
        "event_url": "",
        "sessions": [
            {
                "title": "User-Centred Evaluation in Visualization",
                "session_id": "t-usereval",
                "track": "room7",
                "schedule_image": "t-usereval.png",
                "chair": [
                    "Camilla Forsell",
                    "Niklas R\u00f6nnberg",
                    "Matt Cooper"
                ],
                "organizers": [
                    "Matt Cooper",
                    "Camilla Forsell",
                    "Niklas R\u00f6nnberg"
                ],
                "time_start": "2021-10-25T13:00:00Z",
                "time_end": "2021-10-25T16:30:00Z",
                "discord_category": "",
                "discord_channel": "",
                "discord_channel_id": 0,
                "youtube_url": null,
                "zoom_meeting": "",
                "zoom_password": "",
                "ff_link": "",
                "ff_playlist": "",
                "time_slots": [
                    {
                        "type": "live",
                        "title": "Introduction",
                        "contributors": [
                            "Matt Cooper"
                        ],
                        "abstract": null,
                        "time_start": "2021-10-25T13:00:00Z",
                        "time_end": "2021-10-25T13:10:00Z"
                    },
                    {
                        "type": "live",
                        "title": "Usability-Centred Evaluation and Important Concepts",
                        "contributors": [
                            "Camilla Forsell"
                        ],
                        "abstract": null,
                        "time_start": "2021-10-25T13:10:00Z",
                        "time_end": "2021-10-25T13:40:00Z"
                    },
                    {
                        "type": "live",
                        "title": "The Main Phases of an Evaluation Study",
                        "contributors": [
                            "Niklas R\u00f6nnberg"
                        ],
                        "abstract": null,
                        "time_start": "2021-10-25T13:40:00Z",
                        "time_end": "2021-10-25T14:30:00Z"
                    },
                    {
                        "type": null,
                        "title": "Reporting",
                        "contributors": [
                            "Matt Cooper"
                        ],
                        "abstract": null,
                        "time_start": "2021-10-25T15:00:00Z",
                        "time_end": "2021-10-25T15:30:00Z"
                    },
                    {
                        "type": null,
                        "title": "Common Problems",
                        "contributors": [
                            "Matt Cooper"
                        ],
                        "abstract": null,
                        "time_start": "2021-10-25T15:30:00Z",
                        "time_end": "2021-10-25T15:50:00Z"
                    },
                    {
                        "type": null,
                        "title": "Reviewing",
                        "contributors": [
                            "Matt Cooper"
                        ],
                        "abstract": null,
                        "time_start": "2021-10-25T15:50:00Z",
                        "time_end": "2021-10-25T16:10:00Z"
                    },
                    {
                        "type": null,
                        "title": "Closure",
                        "contributors": [
                            "Matt Cooper",
                            "Camilla Forsell",
                            "Niklas R\u00f6nnberg"
                        ],
                        "abstract": null,
                        "time_start": "2021-10-25T16:10:00Z",
                        "time_end": "2021-10-25T16:30:00Z"
                    }
                ]
            }
        ]
    },
    "t-observable": {
        "event": "Observable: Quick and effective visualization prototyping with reactive notebooks",
        "long_name": "Observable: Quick and effective visualization prototyping with reactive notebooks",
        "event_type": "Tutorial",
        "event_description": "",
        "event_url": "",
        "sessions": [
            {
                "title": "Observable: Quick and effective visualization prototyping with reactive notebooks",
                "session_id": "t-observable",
                "track": "room8",
                "schedule_image": "t-observable.png",
                "chair": [
                    "John A. Guerra-Gomez"
                ],
                "organizers": [
                    "John A. Guerra-Gomez"
                ],
                "time_start": "2021-10-25T13:00:00Z",
                "time_end": "2021-10-25T20:30:00Z",
                "discord_category": "",
                "discord_channel": "",
                "discord_channel_id": 0,
                "youtube_url": null,
                "zoom_meeting": "",
                "zoom_password": "",
                "ff_link": "",
                "ff_playlist": "",
                "time_slots": [
                    {
                        "type": "live",
                        "title": "Introduction",
                        "contributors": [
                            "John A. Guerra-Gomez"
                        ],
                        "abstract": "Why observable is maybe best option for creating and sharing data visualizations on 2021.",
                        "time_start": "2021-10-25T13:00:00Z",
                        "time_end": "2021-10-25T13:50:00Z"
                    },
                    {
                        "type": "live",
                        "title": "Basic Concepts, Reactive Notebooks, Importing",
                        "contributors": [
                            "John A. Guerra-Gomez"
                        ],
                        "abstract": "What are reactive notebooks, and how they are different from Jupyter notebooks. We'll also cover interactive notebooks using standard HTML inputs How to load JavaScript libraries and speed up visualization creation by importing other modules.",
                        "time_start": "2021-10-25T14:00:00Z",
                        "time_end": "2021-10-25T14:50:00Z"
                    },
                    {
                        "type": "live",
                        "title": "Data processing with Arquero.js, Vega-Lite Javascript API",
                        "contributors": [
                            "John A. Guerra-Gomez"
                        ],
                        "abstract": "https://uwdata.github.io/arquero/. How to use transform your data to make it usable with your intended visualizations. https://vega.github.io/vega-lite-api/. Now that you have your data on Observable, how to take the Marks and Channels visualization theory into practice with Vega-Lite API.",
                        "time_start": "2021-10-25T15:00:00Z",
                        "time_end": "2021-10-25T15:50:00Z"
                    },
                    {
                        "type": "live",
                        "title": "Advanced Vega-Lite Javascript API",
                        "contributors": [
                            "John A. Guerra-Gomez"
                        ],
                        "abstract": "Advanced Vega-Lite, how to use the interactive charts in vega-lite and use them to interact with other cells in your notebook. For when you want to create more custom made or more advanced visualizations or reuse examples from the D3 community.",
                        "time_start": "2021-10-25T16:00:00Z",
                        "time_end": "2021-10-25T16:50:00Z"
                    },
                    {
                        "type": "live",
                        "title": "D3 in Observable",
                        "contributors": [
                            "John A. Guerra-Gomez"
                        ],
                        "abstract": "What are the different options for displaying and embedding your visualizations outside Observable. https://observablehq.com/@observablehq/introducing-observable-plot. The latest grammar based visualization library from Observable itself.",
                        "time_start": "2021-10-25T17:00:00Z",
                        "time_end": "2021-10-25T17:50:00Z"
                    },
                    {
                        "type": "live",
                        "title": "Exporting / Embedding, Observable Plot",
                        "contributors": [
                            "John A. Guerra-Gomez"
                        ],
                        "abstract": "Observable has been used for teaching information visualization at UC Berkeley (Co designed between the Author and Andy Reagan) and at University of Washington (by Mike Freeman). This could be a section where instructors that have used the tool for teaching share their pros and cons.",
                        "time_start": "2021-10-25T18:00:00Z",
                        "time_end": "2021-10-25T18:50:00Z"
                    },
                    {
                        "type": "live",
                        "title": "Teaching with Observable",
                        "contributors": [
                            "John A. Guerra-Gomez"
                        ],
                        "abstract": null,
                        "time_start": "2021-10-25T19:00:00Z",
                        "time_end": "2021-10-25T20:30:00Z"
                    }
                ]
            }
        ]
    },
    "a-visxai": {
        "event": "VISxAI",
        "long_name": "VISxAI",
        "event_type": "Associated Event",
        "event_description": "",
        "event_url": "",
        "sessions": [
            {
                "title": "VISxAI",
                "session_id": "a-visxai",
                "track": "room3",
                "schedule_image": "a-visxai.png",
                "chair": [],
                "organizers": [
                    "Adam Perer",
                    "Fred Hohman",
                    "Hendrik Strobelt",
                    "Mennatallah El-Assady"
                ],
                "time_start": "2021-10-25T17:00:00Z",
                "time_end": "2021-10-25T20:30:00Z",
                "discord_category": "",
                "discord_channel": "",
                "discord_channel_id": 0,
                "youtube_url": null,
                "zoom_meeting": "",
                "zoom_password": "",
                "ff_link": "",
                "ff_playlist": "",
                "time_slots": [
                    {
                        "type": null,
                        "title": null,
                        "contributors": "",
                        "abstract": null,
                        "time_start": "2021-10-25T17:00:00Z",
                        "time_end": "2021-10-25T18:30:00Z"
                    },
                    {
                        "type": null,
                        "title": null,
                        "contributors": "",
                        "abstract": null,
                        "time_start": "2021-10-25T19:00:00Z",
                        "time_end": "2021-10-25T20:30:00Z"
                    }
                ]
            }
        ]
    },
    "w-ava": {
        "event": "Workshop on AudioVisual Analytics \u2013 Towards a Research Agenda for Integrating Sonification and Visualization",
        "long_name": "Workshop on AudioVisual Analytics \u2013 Towards a Research Agenda for Integrating Sonification and Visualization",
        "event_type": "Workshop",
        "event_description": "",
        "event_url": "",
        "sessions": [
            {
                "title": "Workshop on AudioVisual Analytics \u2013 Towards a Research Agenda for Integrating Sonification and Visualization",
                "session_id": "w-ava",
                "track": "room5",
                "schedule_image": "w-ava.png",
                "chair": [
                    "Bruce N. Walker",
                    "Kajetan Enge",
                    "Wolfgang Aigner",
                    "Michael Iber",
                    "Niklas Elmqvist",
                    "Alexander Rind",
                    "Robert H\u00f6ldrich",
                    "Niklas R\u00f6nnberg"
                ],
                "organizers": [
                    "Wolfgang Aigner",
                    "Kajetan Enge",
                    "Michael Iber",
                    "Alexander Rind",
                    "Niklas Elmqvist",
                    "Robert H\u00f6ldrich",
                    "Niklas R\u00f6nnberg",
                    "Bruce N. Walker"
                ],
                "time_start": "2021-10-25T17:00:00Z",
                "time_end": "2021-10-25T20:30:00Z",
                "discord_category": "",
                "discord_channel": "",
                "discord_channel_id": 0,
                "youtube_url": null,
                "zoom_meeting": "",
                "zoom_password": "",
                "ff_link": "",
                "ff_playlist": "",
                "time_slots": [
                    {
                        "type": "live",
                        "title": "Workshop on AudioVisual Analytics \u2013 Towards a Research Agenda for Integrating Sonification and Visualization",
                        "contributors": [
                            "Wolfgang Aigner",
                            "Kajetan Enge",
                            "Michael Iber",
                            "Alexander Rind",
                            "Niklas Elmqvist",
                            "Robert H\u00f6ldrich",
                            "Niklas R\u00f6nnberg",
                            "Bruce N. Walker"
                        ],
                        "abstract": null,
                        "time_start": "2021-10-25T17:00:00Z",
                        "time_end": "2021-10-25T18:30:00Z"
                    },
                    {
                        "type": "live",
                        "title": "Workshop on AudioVisual Analytics \u2013 Towards a Research Agenda for Integrating Sonification and Visualization",
                        "contributors": [
                            "Wolfgang Aigner",
                            "Kajetan Enge",
                            "Michael Iber",
                            "Alexander Rind",
                            "Niklas Elmqvist",
                            "Robert H\u00f6ldrich",
                            "Niklas R\u00f6nnberg",
                            "Bruce N. Walker"
                        ],
                        "abstract": null,
                        "time_start": "2021-10-25T19:00:00Z",
                        "time_end": "2021-10-25T20:30:00Z"
                    }
                ]
            }
        ]
    },
    "t-reviews": {
        "event": "IEEE VIS Full Paper Review Model and Process: Becoming a (Better) Program Committee Member",
        "long_name": "IEEE VIS Full Paper Review Model and Process: Becoming a (Better) Program Committee Member",
        "event_type": "Tutorial",
        "event_description": "",
        "event_url": "",
        "sessions": [
            {
                "title": "IEEE VIS Full Paper Review Model and Process: Becoming a (Better) Program Committee Member",
                "session_id": "t-reviews",
                "track": "room6",
                "schedule_image": "t-reviews.png",
                "chair": [
                    "Petra Isenberg",
                    "Bongshin Lee",
                    "Anastasia Bezerianos"
                ],
                "organizers": [
                    "Bongshin Lee",
                    "Petra Isenberg",
                    "Anastasia Bezerianos"
                ],
                "time_start": "2021-10-25T17:00:00Z",
                "time_end": "2021-10-25T20:30:00Z",
                "discord_category": "",
                "discord_channel": "",
                "discord_channel_id": 0,
                "youtube_url": null,
                "zoom_meeting": null,
                "zoom_password": null,
                "ff_link": "",
                "ff_playlist": "",
                "time_slots": [
                    {
                        "type": "live",
                        "title": "Part 1",
                        "contributors": [
                            "Bongshin Lee",
                            "Petra Isenberg",
                            "Anastasia Bezerianos"
                        ],
                        "abstract": null,
                        "time_start": "2021-10-25T17:00:00Z",
                        "time_end": "2021-10-25T18:30:00Z"
                    },
                    {
                        "type": "live",
                        "title": "Part 2",
                        "contributors": [
                            "Bongshin Lee",
                            "Petra Isenberg",
                            "Anastasia Bezerianos"
                        ],
                        "abstract": null,
                        "time_start": "2021-10-25T19:00:00Z",
                        "time_end": "2021-10-25T20:30:00Z"
                    }
                ]
            }
        ]
    },
    "t-riemannian": {
        "event": "Riemannian Geometry for Scientific Visualization",
        "long_name": "Riemannian Geometry for Scientific Visualization",
        "event_type": "Tutorial",
        "event_description": "",
        "event_url": "",
        "sessions": [
            {
                "title": "Riemannian Geometry for Scientific Visualization",
                "session_id": "t-riemannian",
                "track": "room7",
                "schedule_image": "t-riemannian.png",
                "chair": [
                    "Markus Hadwiger"
                ],
                "organizers": [
                    "Markus Hadwiger",
                    "Thomas Theussl",
                    "Peter Rautek"
                ],
                "time_start": "2021-10-25T17:00:00Z",
                "time_end": "2021-10-25T20:30:00Z",
                "discord_category": "",
                "discord_channel": "",
                "discord_channel_id": 0,
                "youtube_url": null,
                "zoom_meeting": "",
                "zoom_password": "",
                "ff_link": "",
                "ff_playlist": "",
                "time_slots": [
                    {
                        "type": "recorded MISSING coming oct 8",
                        "title": "Intro and overview of concepts",
                        "contributors": [
                            "Markus Hadwiger"
                        ],
                        "abstract": null,
                        "time_start": "2021-10-25T17:00:00Z",
                        "time_end": "2021-10-25T17:12:00Z"
                    },
                    {
                        "type": "recorded MISSING coming oct 8",
                        "title": "Manifolds, coordinate charts, vector fields",
                        "contributors": [
                            "Markus Hadwiger"
                        ],
                        "abstract": null,
                        "time_start": "2021-10-25T17:12:00Z",
                        "time_end": "2021-10-25T17:35:00Z"
                    },
                    {
                        "type": "recorded MISSING coming oct 8",
                        "title": "Tensor fields and differential forms",
                        "contributors": [
                            "Peter Rautek"
                        ],
                        "abstract": null,
                        "time_start": "2021-10-25T17:35:00Z",
                        "time_end": "2021-10-25T18:00:00Z"
                    },
                    {
                        "type": "recorded MISSING coming oct 8",
                        "title": "Riemannian metrics and connections",
                        "contributors": [
                            "Thomas Theussl"
                        ],
                        "abstract": null,
                        "time_start": "2021-10-25T18:00:00Z",
                        "time_end": "2021-10-25T18:25:00Z"
                    },
                    {
                        "type": "live",
                        "title": "Q&A",
                        "contributors": [
                            "Markus Hadwiger",
                            "Peter Rautek",
                            "Thomas Theussl"
                        ],
                        "abstract": null,
                        "time_start": "2021-10-25T18:25:00Z",
                        "time_end": "2021-10-25T18:30:00Z"
                    },
                    {
                        "type": "recorded MISSING coming oct 8",
                        "title": "Smooth maps between manifolds, isometries",
                        "contributors": [
                            "Markus Hadwiger"
                        ],
                        "abstract": null,
                        "time_start": "2021-10-25T19:00:00Z",
                        "time_end": "2021-10-25T19:25:00Z"
                    },
                    {
                        "type": "recorded MISSING coming oct 8",
                        "title": "Covariant derivatives and Lie derivatives",
                        "contributors": [
                            "Peter Rautek"
                        ],
                        "abstract": null,
                        "time_start": "2021-10-25T19:25:00Z",
                        "time_end": "2021-10-25T19:50:00Z"
                    },
                    {
                        "type": "recorded MISSING coming oct 8",
                        "title": "Lie groups and Lie algebras",
                        "contributors": [
                            "Thomas Theussl"
                        ],
                        "abstract": null,
                        "time_start": "2021-10-25T19:50:00Z",
                        "time_end": "2021-10-25T20:15:00Z"
                    },
                    {
                        "type": "live",
                        "title": "Q&A",
                        "contributors": [
                            "Markus Hadwiger",
                            "Peter Rautek",
                            "Thomas Theussl"
                        ],
                        "abstract": null,
                        "time_start": "2021-10-25T20:15:00Z",
                        "time_end": "2021-10-25T20:30:00Z"
                    }
                ]
            }
        ]
    },
    "v-opening": {
        "event": "VIS",
        "long_name": "VIS",
        "event_type": "VIS Opening",
        "event_description": "",
        "event_url": "",
        "sessions": [
            {
                "title": "VIS Opening",
                "session_id": "v-opening-1",
                "track": "room1",
                "schedule_image": "v-opening-1.png",
                "chair": [
                    "Terry Yoo",
                    "Catherine Plaisant",
                    "Tamara Munzner",
                    "Daniel Weiskopf",
                    "Brian Summa",
                    "Luis Gustavo Nonato"
                ],
                "organizers": [],
                "time_start": "2021-10-26T12:45:00Z",
                "time_end": "2021-10-26T14:30:00Z",
                "discord_category": "",
                "discord_channel": "",
                "discord_channel_id": 0,
                "youtube_url": null,
                "zoom_meeting": "",
                "zoom_password": "",
                "ff_link": "",
                "ff_playlist": "",
                "time_slots": [
                    {
                        "type": "live",
                        "title": "VIS Welcome",
                        "contributors": [
                            "Brian Summa",
                            "Luis Gustavo Nonato"
                        ],
                        "abstract": null,
                        "time_start": "2021-10-26T12:45:00Z",
                        "time_end": "2021-10-26T13:00:00Z"
                    },
                    {
                        "type": "live",
                        "title": "VGTC Awards",
                        "contributors": "",
                        "abstract": null,
                        "time_start": "2021-10-26T13:00:00Z",
                        "time_end": "2021-10-26T13:45:00Z"
                    },
                    {
                        "type": "live",
                        "title": "Test of Time Awards",
                        "contributors": [
                            "Daniel Weiskopf",
                            "Catherine Plaisant",
                            "Tamara Munznser ",
                            " Terry Yoo"
                        ],
                        "abstract": null,
                        "time_start": "2021-10-26T13:45:00Z",
                        "time_end": "2021-10-26T14:30:00Z"
                    }
                ]
            }
        ]
    },
    "v-full": {
        "event": "VIS Full Papers",
        "long_name": "VIS Full Papers",
        "event_type": "Paper Presentations",
        "event_description": "",
        "event_url": "",
        "sessions": [
            {
                "title": "Best Papers",
                "session_id": "v-full-full1",
                "track": "room1",
                "schedule_image": "v-full-full1.png",
                "chair": [
                    "Silvia Miksch",
                    "Bongshin Lee",
                    "Anders Ynnerman"
                ],
                "organizers": [],
                "time_start": "2021-10-26T15:00:00Z",
                "time_end": "2021-10-26T16:15:00Z",
                "discord_category": "",
                "discord_channel": "",
                "discord_channel_id": 0,
                "youtube_url": null,
                "zoom_meeting": "",
                "zoom_password": "",
                "ff_link": "",
                "ff_playlist": "",
                "time_slots": [
                    {
                        "type": "recorded",
                        "title": "Simultaneous Matrix Orderings for Graph Collections",
                        "contributors": [
                            "Nathan van Beusekom",
                            "Wouter Meulemans",
                            "Bettina Speckmann"
                        ],
                        "abstract": "Undirected graphs are frequently used to model phenomena that deal with interacting objects, such as social networks, brain activity and communication networks. The topology of an undirected graph G can be captured by an adjacency matrix; this matrix in turn can be visualized directly to give insight into the graph structure. Which visual patterns appear in such a matrix visualization crucially depends on the ordering of its rows and columns. Formally defining the quality of an ordering and then automatically computing a high-quality ordering are both challenging problems; however, effective heuristics exist and are used in practice.\n  \n  Often, graphs do not exist in isolation but as part of a collection of graphs on the same set of vertices, for example, brain scans over time or of different people. To visualize such graph collections, we need a single ordering that works well for all matrices simultaneously. The current state-of-the-art solves this problem by taking a (weighted) union over all graphs and applying existing heuristics. However, this union leads to a loss of information, specifically in those parts of the graphs which are different. We propose a collection-aware approach to avoid this loss of information and apply it to two popular heuristic methods: leaf order and barycenter.\n  \n  The de-facto standard computational quality metrics for matrix ordering capture only block-diagonal patterns (cliques). Instead, we propose to use Moran's I, a spatial auto-correlation metric, which captures the full range of established patterns. Moran's I refines previously proposed stress measures. Furthermore, the popular leaf order method heuristically optimizes a similar measure which further supports the use of Moran's I in this context. An ordering that maximizes Moran's I can be computed via solutions to the Traveling Salesperson Problem (TSP); approximate orderings can be computed more efficiently, using any of the approximation algorithms for metric TSP.\n  \n  We evaluated our methods for simultaneous orderings on real-world datasets using Moran's I as the quality metric. Our results show that our collection-aware approach matches or improves performance compared to the union approach, depending on the similarity of the graphs in the collection. Specifically, our Moran's I-based collection-aware leaf order implementation consistently outperforms other implementations. Our collection-aware implementations carry no significant additional computational costs.",
                        "time_start": "2021-10-26T15:00:00Z",
                        "time_end": "2021-10-26T15:15:00Z",
                        "uid": "v-full-1346",
                        "youtube_video_id": "xbmXg0xDPFs"
                    },
                    {
                        "type": "recorded",
                        "title": "IRVINE: Using Interactive Clustering and Labeling to Analyze Correlation Patterns: A Design Study from the Manufacturing of Electrical Engines",
                        "contributors": [
                            "Joscha Eirich",
                            "Jakob Bonart",
                            "Dominik J\u00e4ckle",
                            "Michael Sedlmair",
                            "Ute Schmid",
                            "Kai Fischbach",
                            "Tobias Schreck",
                            "J\u00fcrgen Bernard"
                        ],
                        "abstract": "In this design study, we present IRVINE, a Visual Analytics (VA) system, which facilitates the analysis of acoustic data to detect and understand previously unknown errors in the manufacturing of electrical engines. In serial manufacturing processes, signatures from acoustic data provide valuable information on how the relationship between multiple produced engines serves to detect and understand previously unknown errors. To analyze such signatures, IRVINE leverages interactive clustering and data labeling techniques, allowing users to analyze clusters of engines with similar signatures, drill down to groups of engines, and select an engine of interest. Furthermore, IRVINE allows to assign labels to engines and clusters and annotate the cause of an error in the acoustic raw measurement of an engine. Since labels and annotations represent valuable knowledge, they are conserved in a knowledge database to be available for other stakeholders. We contribute a design study, where we developed IRVINE in four main iterations with engineers from a company in the automotive sector. To validate IRVINE, we conducted a field study with six domain experts. Our results suggest a high usability and usefulness of IRVINE as part of the improvement of a real-world manufacturing process. Specifically, with IRVINE domain experts were able to label and annotate produced electrical engines more than 30% faster.",
                        "time_start": "2021-10-26T15:15:00Z",
                        "time_end": "2021-10-26T15:30:00Z",
                        "uid": "v-full-1187",
                        "youtube_video_id": "E_u27Xltbt0"
                    },
                    {
                        "type": "recorded",
                        "title": "Perception! Immersion! Empowerment! Superpowers as Inspiration for Visualization",
                        "contributors": [
                            "Wesley Willett",
                            "Bon Adriel Aseniero",
                            "Sheelagh Carpendale",
                            "Pierre Dragicevic",
                            "Yvonne Jansen",
                            "Lora Oehlberg",
                            "Petra Isenberg"
                        ],
                        "abstract": "We explore how the lens of fictional superpowers can help characterize how visualizations empower people and provide inspiration for new visualization systems. Researchers and practitioners often tout visualizations\u2019 ability to \u201cmake the invisible visible\u201dand to \u201cenhance cognitive abilities.\u201d Meanwhile superhero comics and other modern fiction often depict characters with similarly fantastic abilities that allow them to see and interpret the world in ways that transcend traditional human perception. We investigate the intersection of these domains, and show how the language of superpowers can be used to characterize existing visualization systems and suggest opportunities for new and empowering ones. We introduce two frameworks: The first characterizes seven underlying mechanics that form the basis for a variety of visual superpowers portrayed in fiction. The second identifies seven ways in which visualization tools and interfaces can instill a sense of empowerment in the people who use them. Building on these observations, we illustrate a diverse set of \u201cvisualization superpowers\u201d and highlight opportunities for the visualization community to create new system sand interactions that empower new experiences with data",
                        "time_start": "2021-10-26T15:30:00Z",
                        "time_end": "2021-10-26T15:45:00Z",
                        "uid": "v-full-1160",
                        "youtube_video_id": "_k51PyDj5Ag"
                    },
                    {
                        "type": "recorded",
                        "title": "Feature Curves and Surfaces of 3D Asymmetric Tensor Fields",
                        "contributors": [
                            "Shih-Hsuan Hung",
                            "Yue Zhang",
                            "Harry Yeh",
                            "Eugene Zhang"
                        ],
                        "abstract": "3D asymmetric tensor fields have found many applications in science and engineering domains, such as fluid dynamics and solid mechanics. 3D asymmetric tensors can have complex eigenvalues, which makes their analysis and visualization more challenging than 3D symmetric tensors. Existing research in tensor field visualization focuses on 2D asymmetric tensor fields and 3D symmetric tensor fields. In this paper, we address the analysis and visualization of 3D asymmetric tensor fields. We introduce six topological surfaces and one topological curve, which lead to an eigenvalue space based on the tensor mode that we define. In addition, we identify several non-topological feature surfaces that are nonetheless physically important. Included in our analysis are the realizations that triple degenerate tensors are structurally stable and form curves, unlike the case for 3D symmetric tensors fields. Furthermore, there are two different ways of measuring the relative strengths of rotation and angular deformation in the tensor fields, unlike the case for 2D asymmetric tensor fields. We extract these feature surfaces using the A-patches algorithm. However, since three of our feature surfaces are quadratic, we develop a method to extract quadratic surfaces at any given accuracy. To facilitate the analysis of eigenvector fields, we visualize a hyperstreamline as a tree stem with the other two eigenvectors represented as thorns in the real domain or the dual-eigenvectors as leaves in the complex domain. To demonstrate the effectiveness of our analysis and visualization, we apply our approach to datasets from solid mechanics and fluid dynamics.",
                        "time_start": "2021-10-26T15:45:00Z",
                        "time_end": "2021-10-26T16:00:00Z",
                        "uid": "v-full-1426",
                        "youtube_video_id": "LZ5MjWtcGp8"
                    },
                    {
                        "type": "recorded",
                        "title": "Jurassic Mark: Inattentional Blindness for a Datasaurus Reveals that Visualizations are Explored, not Seen",
                        "contributors": [
                            "Tal Boger",
                            "Steven Most",
                            "Steven Franconeri"
                        ],
                        "abstract": "Graphs effectively communicate data because they capitalize on the visual system\u2019s ability to rapidly extract patterns. Yet, this pattern extraction does not occur in a single glance. Instead, research on visual attention suggests that the visual system iteratively applies a sequence of filtering operations on an image, extracting patterns from subsets of visual information over time, and selectively inhibiting other information at each of these moments. To demonstrate that this powerful series of filtering operations also occurs during the perception of visualized data, we designed a task where participants made judgments from one class of marks on a scatterplot, presumably incentivizing them to relatively ignore other classes of marks. Participants consistently missed a conspicuous dinosaur in the ignored collection of marks (93% for a 1s presentation, and 61% for 2.5s), but not in a control condition where the judgment task was removed (25% for a 1s presentation, and 11% for 2.5s), suggesting that data visualizations are not \"seen\" in a single glance, and instead require an active process of exploration.",
                        "time_start": "2021-10-26T16:00:00Z",
                        "time_end": "2021-10-26T16:15:00Z",
                        "uid": "v-short-1184",
                        "youtube_video_id": "3x3uzspI9wU"
                    }
                ]
            },
            {
                "title": "Interaction",
                "session_id": "v-full-full19",
                "track": "room2",
                "schedule_image": "v-full-full19.png",
                "chair": [
                    "Christian Tominski"
                ],
                "organizers": [],
                "time_start": "2021-10-27T13:00:00Z",
                "time_end": "2021-10-27T14:30:00Z",
                "discord_category": "",
                "discord_channel": "",
                "discord_channel_id": 0,
                "youtube_url": null,
                "zoom_meeting": "",
                "zoom_password": "",
                "ff_link": "",
                "ff_playlist": "",
                "time_slots": [
                    {
                        "type": "recorded",
                        "title": "Task-Based Effectiveness of Interactive Contiguous Area Cartograms",
                        "contributors": [
                            "Ian Duncan",
                            "Shi Tingsheng",
                            "Simon Perrault",
                            "Michael Gastner"
                        ],
                        "abstract": "Cartograms are map-based data visualizations in which the area of each map region is proportional to an associated numeric data value (e.g., population or gross domestic product). A cartogram is called contiguous if it conforms to this area principle while also keeping neighboring regions connected. Because of their distorted appearance, contiguous cartograms have been criticized as difficult to read. Some authors have suggested that cartograms may be more legible if they are accompanied by interactive features (e.g., animations, linked brushing, or infotips). We conducted an experiment to evaluate this claim. Participants had to perform visual analysis tasks with interactive and noninteractive contiguous cartograms. The task types covered various aspects of cartogram readability, ranging from elementary lookup tasks to synoptic tasks (i.e., tasks in which participants had to summarize high-level differences between two cartograms). Elementary tasks were carried out equally well with and without interactivity. Synoptic tasks, by contrast, were more difficult without interactive features. With access to interactivity, however, most participants answered even synoptic questions correctly. In a subsequent survey, participants rated the interactive features as \u201ceasy to use\u201d and \u201chelpful.\u201d Our study suggests that interactivity has the potential to make contiguous cartograms accessible even for those readers who are unfamiliar with interactive computer graphics or do not have a prior affinity to working with maps. Among the interactive features, animations had the strongest positive effect, so we recommend them as a minimum of interactivity when contiguous cartograms are displayed on a computer screen.",
                        "time_start": "2021-10-27T13:00:00Z",
                        "time_end": "2021-10-27T13:15:00Z",
                        "uid": "v-tvcg-9275378",
                        "youtube_video_id": "wYvr8yv0mR8"
                    },
                    {
                        "type": "recorded",
                        "title": "Nebula: A Coordinating Grammar of Graphics",
                        "contributors": [
                            "Ran Chen",
                            "Xinhuan Shu",
                            "Jiahui Chen",
                            "Di Weng",
                            "Junxiu Tang",
                            "Siwei Fu",
                            "Yingcai Wu"
                        ],
                        "abstract": "In multiple coordinated views (MCVs), visualizations across views update their content in response to users interactions in other views. Interactive systems provide direct manipulation to create coordination between views, but are restricted to limited types of predefined templates. By contrast, textual specification languages enable flexible coordination but expose technical burden. To bridge the gap, we contribute Nebula, a grammar based on natural language for coordinating visualizations in MCVs. The grammar design is informed by a novel framework based on a systematic review of 176 coordinations from existing theories and applications, which describes coordination by demonstration, i.e., how coordination is performed by users. With the framework, Nebula specification formalizes coordination as a composition of user- and coordination-triggered interactions in origin and destination views, respectively, along with potential data transformation between the interactions. We evaluate Nebula by demonstrating its expressiveness with a gallery of diverse examples and analyzing its usability on cognitive dimensions.",
                        "time_start": "2021-10-27T13:15:00Z",
                        "time_end": "2021-10-27T13:30:00Z",
                        "uid": "v-tvcg-9417674",
                        "youtube_video_id": "2sX4HxTbkkY"
                    },
                    {
                        "type": "recorded",
                        "title": "Rotate or Wrap? Interactive Visualisations of Cyclical Data on Cylindrical or Toroidal Topologies",
                        "contributors": [
                            "Kun-Ting Chen",
                            "Tim Dwyer",
                            "Benjamin Bach",
                            "Kim Marriott"
                        ],
                        "abstract": "In this paper, we report on a study of visual representations for cyclical data and the effect of interactively wrapping a bar chart \u2018around its boundaries\u2019. Compared to linear bar chart, polar (or radial) visualisations have the advantage that cyclical data can be presented continuously without mentally bridging the visual \u2018cut\u2019 across the left-and-right boundaries. To investigate this hypothesis and to assess the effect the cut has on analysis performance, this paper presents results from a crowdsourced, controlled experiment with 72 participants comparing new continuous panning technique to linear bar charts (interactive wrapping). Our results show that bar charts with interactive wrapping lead to less errors compared to standard bar charts or polar charts. Inspired by these results, we generalise the concept of interactive wrapping to other visualisations for cyclical or relational data. We describe a design space based on the concept of one-dimensional wrapping and two-dimensional wrapping, linked to two common 3D topologies; cylinder and torus that can be used to metaphorically explain one- and two-dimensional wrapping. This design space suggests that interactive wrapping is widely applicable to many different data types.",
                        "time_start": "2021-10-27T13:30:00Z",
                        "time_end": "2021-10-27T13:45:00Z",
                        "uid": "v-full-1155",
                        "youtube_video_id": "yxXFtCef44E"
                    },
                    {
                        "type": "recorded",
                        "title": "DIEL: Interactive Visualization Beyond the Here and Now",
                        "contributors": [
                            "Yifan Wu",
                            "Remco Chang",
                            "Joseph Hellerstein",
                            "Arvind Satyanarayan",
                            "Eugene Wu"
                        ],
                        "abstract": "Interactive visualization design and research have primarily focused on local data and synchronous events. However, for more complex use cases\u2014e.g., remote database access and streaming data sources\u2014developers must grapple with distributed data and asynchronous events. Currently, constructing these use cases is difficult and time-consuming; developers are forced to operationally program low-level details like asynchronous database querying and reactive event handling. This approach is in stark contrast to modern methods for browser-based interactive visualization, which feature high-level declarative specifications. In response, we present DIEL, a declarative framework that supports asynchronous events over distributed data. Like many declarative visualization languages, DIEL developers need only specify what data they want, rather than procedural steps for how to assemble it; uniquely, DIEL models asynchronous events (e.g., user interactions or server responses) as streams of data that are captured in event logs. To specify the state of a user interface at any time, developers author declarative queries over the data and event logs; DIEL compiles and optimizes a corresponding dataflow graph, and synthesizes necessary low-level distributed systems details. We demonstrate DIEL\u2019s performance and expressivity through ex-ample interactive visualizations that make diverse use of remote data and coordination of asynchronous events. We further evaluate DIEL\u2019s usability using the Cognitive Dimensions of Notations framework, revealing wins such as ease of change, and compromises such as premature commitments.",
                        "time_start": "2021-10-27T13:45:00Z",
                        "time_end": "2021-10-27T14:00:00Z",
                        "uid": "v-full-1275",
                        "youtube_video_id": "RQuwFU-MmJA"
                    },
                    {
                        "type": "recorded",
                        "title": "VizSnippets: Compressing Visualization Bundles Into Representative Previews for Browsing Visualization Collections",
                        "contributors": [
                            "Michael Oppermann",
                            "Tamara Munzner"
                        ],
                        "abstract": "Visualization collections, accessed by platforms such as Tableau Online or Power BI, are used by millions of people to share and access diverse analytical knowledge in the form of interactive visualization bundles. Result snippets, compact previews of these bundles, are presented to users to help them identify relevant content when browsing collections. Our engagement with Tableau product teams and review of existing snippet designs on five platforms showed us that current practices fail to help people judge the relevance of bundles because they include only the title and one image. Users frequently need to undertake the time-consuming endeavour of opening a bundle within its visualization system to examine its many views and dashboards. In response, we contribute the first systematic approach to visualization snippet design. We propose a framework for snippet design that addresses eight key challenges that we identify. We present a computational pipeline to compress the visual and textual content of bundles into representative previews that is adaptive to a provided pixel budget and provides high information density with multiple images and carefully chosen keywords. We also reflect on the method of visual inspection through random sampling to gain confidence in model and parameter choices.",
                        "time_start": "2021-10-27T14:00:00Z",
                        "time_end": "2021-10-27T14:15:00Z",
                        "uid": "v-full-1204",
                        "youtube_video_id": "cQFM-aUmitc"
                    },
                    {
                        "type": "recorded",
                        "title": "Interactive Dimensionality Reduction for Comparative Analysis",
                        "contributors": [
                            "Takanori Fujiwara",
                            "Xinhai Wei",
                            "Jian Zhao",
                            "Kwan-Liu Ma"
                        ],
                        "abstract": "Finding the similarities and differences between two or more groups of datasets is a fundamental analysis task. For high-dimensional data, dimensionality reduction (DR) methods are often used to find the characteristics of each group. However, existing DR methods provide limited capability and flexibility for such comparative analysis as each method is designed only for a narrow analysis target, such as identifying factors that most differentiate groups. In this work, we introduce an interactive DR framework where we integrate our new DR method, called ULCA (unified linear comparative analysis), with an interactive visual interface. ULCA unifies two DR schemes, discriminant analysis and contrastive learning, to support various comparative analysis tasks. To provide flexibility for comparative analysis, we develop an optimization algorithm that enables analysts to interactively refine ULCA results. Additionally, we provide an interactive visualization interface to examine ULCA results with a rich set of analysis libraries. We evaluate ULCA and the optimization algorithm to show their efficiency as well as present multiple case studies using real-world datasets to demonstrate the usefulness of our framework.",
                        "time_start": "2021-10-27T14:15:00Z",
                        "time_end": "2021-10-27T14:30:00Z",
                        "uid": "v-full-1570",
                        "youtube_video_id": "1xCy-uop8f0"
                    }
                ]
            },
            {
                "title": "Explainable AI and Machine Learning",
                "session_id": "v-full-full20",
                "track": "room1",
                "schedule_image": "v-full-full20.png",
                "chair": [
                    "Hendrik Strobelt"
                ],
                "organizers": [],
                "time_start": "2021-10-27T13:00:00Z",
                "time_end": "2021-10-27T14:30:00Z",
                "discord_category": "",
                "discord_channel": "",
                "discord_channel_id": 0,
                "youtube_url": null,
                "zoom_meeting": "",
                "zoom_password": "",
                "ff_link": "",
                "ff_playlist": "",
                "time_slots": [
                    {
                        "type": "recorded",
                        "title": "AffectiveTDA: Using Topological Data Analysis for Improved Explainability in Affective Computing",
                        "contributors": [
                            "Hamza Elhamdadi",
                            "Shaun Canavan",
                            "Paul Rosen"
                        ],
                        "abstract": "We present an approach utilizing Topological Data Analysis to study the structure of face poses used in affective computing, i.e., the process of recognizing human emotion. The approach uses a conditional comparison of different emotions, both respective and irrespective of time, with multiple topological distance metrics, dimension reduction techniques, and face subsections (e.g., eyes, nose, mouth, etc.). The results confirm that our topology-based approach captures known patterns, distinctions between emotions, and distinctions between individuals, which is an important step towards more robust and explainable emotion recognition by machines.",
                        "time_start": "2021-10-27T13:00:00Z",
                        "time_end": "2021-10-27T13:15:00Z",
                        "uid": "v-full-1486",
                        "youtube_video_id": "buqJx0w5PiQ"
                    },
                    {
                        "type": "recorded",
                        "title": "Human-in-the-loop Extraction of Interpretable Concepts in Deep Learning Models",
                        "contributors": [
                            "Zhenge Zhao",
                            "Panpan Xu",
                            "Carlos Scheidegger",
                            "Liu Ren"
                        ],
                        "abstract": "The interpretation of deep neural networks (DNNs) has become a key topic as more people apply them to solve various problems and making critical decisions. Recently, concept-based explanation has become a popular approach for post-hoc interpretation of DNNs. Instead of focusing on a single data sample to obtain local interpretation such as saliency maps, concept-based explanation provides a global interpretation of model predictions by analyzing how visual concepts affects model decision. For example, how the presence of shadow affects an object detection model. However, identifying human-friendly visual concepts that affect model decisions is a challenging task that can not be easily addressed with automatic approaches. In this paper, we present a novel human-in-the-loop visual analytics framework to generate user-defined concepts for model interpretation and diagnostics. The core of our approach is the use of active learning, where we integrate human knowledge and feedback to train a concept extractor in each stage. We crop or segment the original images into small image patches, extract the latent presentations from the hidden layer of the task model, select image patches sharing a common concept, and train a shallow net on top of the latent representation to collect image patches containing the visual concept. We combine these processes into an interactive system, ConceptExtract. Through two case studies, we show how our approach helps analyze model behavior and extract human-friendly concepts for different machine learning tasks and datasets and how to use these concepts to understand the predictions, compare model performance and make suggestions for model refinement. Quantitative experiments show that our active learning approach can accurately extract meaningful visual concepts. More importantly, by identifying visual concepts that negatively affect model performance, we develop the corresponding data augmentation strategy that consistently improves model performance.",
                        "time_start": "2021-10-27T13:15:00Z",
                        "time_end": "2021-10-27T13:30:00Z",
                        "uid": "v-full-1621",
                        "youtube_video_id": "JoMZfFXC9pc"
                    },
                    {
                        "type": "recorded",
                        "title": "Towards Visual Explainable Active Learning for Zero-Shot Classification",
                        "contributors": [
                            "Shichao Jia",
                            "zeyu li",
                            "Nuo Chen",
                            "Jiawan Zhang"
                        ],
                        "abstract": "Zero-shot classification is a promising paradigm to solve an applicable problem when the training classes and test classes are disjoint. Achieving this usually needs experts to externalize their domain knowledge by manually specifying a class-attribute matrix to define which classes have which attributes. Designing a suitable class-attribute matrix is the key to the subsequent procedure, but this design process is tedious and trial-and-error with no guidance. This paper proposes a visual explainable active learning approach with its design and implementation called semantic navigator to solve the above problems. This approach promotes human-AI teaming with four actions (ask, explain, recommend, respond) in each interaction loop. The machine asks contrastive questions to guide humans in the thinking process of attributes. A novel visualization called semantic map explains the current status of the machine. Therefore analysts can better understand why the machine misclassifies objects. Moreover, the machine recommends the labels of classes for each attribute to ease the labeling burden. Finally, humans can steer the model by modifying the labels interactively, and the machine adjusts its recommendations. The visual explainable active learning approach improves humans' efficiency of building zero-shot classification models interactively, compared with the method without guidance. We justify our results with user studies using the standard benchmarks for zero-shot classification.",
                        "time_start": "2021-10-27T13:30:00Z",
                        "time_end": "2021-10-27T13:45:00Z",
                        "uid": "v-full-1123",
                        "youtube_video_id": "a8Xa8jAQKi8"
                    },
                    {
                        "type": "recorded",
                        "title": "M^2Lens: Visualizing and Explaining Multimodal Models for Sentiment Analysis",
                        "contributors": [
                            "Xingbo Wang",
                            "Jianben He",
                            "Zhihua Jin",
                            "Muqiao Yang",
                            "Yong Wang",
                            "Huamin Qu"
                        ],
                        "abstract": "Multimodal sentiment analysis aims to recognize people\u2019s attitudes from\n  multiple communication channels such as verbal content (i.e., text), voice, and facial expressions. It has become a vibrant and important research topic in natural language processing. Much research focuses on modeling the complex intra- and inter-modal interactions between different communication channels.\n  However, current multimodal models with strong performance are often deep-learning-based techniques and work like black boxes.\n  It is not clear how models utilize multimodal information for sentiment predictions.\n  Despite recent advances in techniques for enhancing the explainability of machine learning models, they often target unimodal scenarios (e.g., images, sentences),\n  and little research has been done on explaining multimodal models.\n  In this paper, we present an interactive visual analytics system, M2Lens, to visualize and explain multimodal models for sentiment analysis. M2Lens provides explanations on intra- and inter-modal interactions at the global, subset, and local levels. Specifically, it summarizes the influence of three typical interaction types (i.e., dominance, complement, and conflict) on the model predictions. \n  Moreover, M2Lens identifies frequent and influential multimodal features and supports the multi-faceted exploration of model behaviors from language, acoustic, and visual modalities.\n  Through two case studies and expert interviews, we demonstrate our system can help users gain deep insights into the multimodal models for sentiment analysis.",
                        "time_start": "2021-10-27T13:45:00Z",
                        "time_end": "2021-10-27T14:00:00Z",
                        "uid": "v-full-1112",
                        "youtube_video_id": "YGLtpi6pGlI"
                    },
                    {
                        "type": "recorded",
                        "title": "NeuroCartography: Scalable Automatic Visual Summarization of Concepts in Deep Neural Networks",
                        "contributors": [
                            "Haekyu Park",
                            "Nilaksh Das",
                            "Rahul Duggal",
                            "Austin Wright",
                            "Omar Shaikh",
                            "Fred Hohman",
                            "Duen Horng Chau"
                        ],
                        "abstract": "Existing research on making sense of deep neural networks often focuses on neuron-level interpretation, which may not adequately capture the bigger picture of how concepts are collectively encoded by multiple neurons. We present NeuroCartography, an interactive system that scalably summarizes and visualizes concepts learned by neural networks. It automatically discovers and groups neurons that detect the same concepts, and describes how such neuron groups interact to form higher-level concepts and the subsequent predictions. NeuroCartography introduces two scalable summarization techniques: (1) neuron clustering groups neurons based on the semantic similarity of the concepts detected by neurons (e.g., neurons detecting \u201cdog faces\u201d of different breeds are grouped); and (2) neuron embedding encodes the associations between related concepts based on how often they co-occur (e.g., neurons detecting \u201cdog face\u201d and \u201cdog tail\u201d are placed closer in the embedding space). Key to our scalable techniques is the ability to efficiently compute all neuron pairs\u2019 relationships, in time linear to the number of neurons instead of quadratic time. NeuroCartography scales to large data, such as the ImageNet dataset with 1.2M images. The system\u2019s tightly coordinated views integrate the scalable techniques to visualize the concepts and their relationships, projecting the concept associations to a 2D space in Neuron Projection View, and summarizing neuron clusters and their relationships in Graph View. Through a large-scale human evaluation, we demonstrate that our technique discovers neuron groups that represent coherent, human-meaningful concepts. And through usage scenarios, we describe how our approaches enable interesting and surprising discoveries, such as concept cascades of related and isolated concepts. The NeuroCartography visualization runs in modern browsers and is open-sourced.",
                        "time_start": "2021-10-27T14:00:00Z",
                        "time_end": "2021-10-27T14:15:00Z",
                        "uid": "v-full-1194",
                        "youtube_video_id": "_mxQMvfNLEU"
                    },
                    {
                        "type": "recorded",
                        "title": "Visual Analytics for RNN-Based Deep Reinforcement Learning",
                        "contributors": [
                            "Junpeng Wang",
                            "Wei Zhang",
                            "Hao Yang",
                            "Chin-Chia Yeh",
                            "Liang Wang"
                        ],
                        "abstract": "Deep reinforcement learning (DRL) targets to train an autonomous agent to interact with a pre-defined environment and strives to achieve specific goals through deep neural networks (DNN). Recurrent neural network (RNN) based DRL has demonstrated superior performance, as RNNs can effectively capture the temporal evolution of the environment and respond with proper agent actions. However, apart from the outstanding performance, little is known about how RNNs understand the environment internally and what has been memorized over time. Revealing these details is extremely important for deep learning experts to understand and improve DRLs, which in contrast, is also challenging due to the complicated data transformations inside these models. In this paper, we propose Deep Reinforcement Learning Interactive Visual Explorer (DRLIVE), a visual analytics system to effectively explore, interpret, and diagnose RNN-based DRLs. Focused on DRL agents trained for different Atari games, DRLIVE targets to accomplish three tasks: game episode exploration, RNN hidden/cell state examination, and interactive model perturbation. Using the system, one can flexibly explore a DRL agent through interactive visualizations, discover interpretable RNN cells by prioritizing RNN hidden/cell states with a set of metrics, and further diagnose the DRL model by interactively perturbing its inputs. Through concrete studies with multiple deep learning experts, we validated the efficacy of DRLIVE.",
                        "time_start": "2021-10-27T14:15:00Z",
                        "time_end": "2021-10-27T14:30:00Z",
                        "uid": "v-tvcg-9420254",
                        "youtube_video_id": "ybUaUcIPjBs"
                    }
                ]
            },
            {
                "title": "Graphical Perception and Coloring",
                "session_id": "v-full-full18",
                "track": "room4",
                "schedule_image": "v-full-full18.png",
                "chair": [
                    "Danielle Albers Szafir"
                ],
                "organizers": [],
                "time_start": "2021-10-27T13:00:00Z",
                "time_end": "2021-10-27T14:30:00Z",
                "discord_category": "",
                "discord_channel": "",
                "discord_channel_id": 0,
                "youtube_url": null,
                "zoom_meeting": "",
                "zoom_password": "",
                "ff_link": "",
                "ff_playlist": "",
                "time_slots": [
                    {
                        "type": "recorded",
                        "title": "Context Matters: A Theory of Semantic Discriminability for Perceptual Encoding Systems",
                        "contributors": [
                            "Kushin Mukherjee",
                            "Brian Yin",
                            "Brianne Sherman",
                            "Laurent Lessard",
                            "Karen Schloss"
                        ],
                        "abstract": "People\u2019s associations between colors and concepts influence their ability to interpret the meanings of colors in information visualizations. Previous work has suggested such effects are limited to concepts that have strong, specific associations with colors. However, although a concept may not be strongly associated with any colors, its mapping can be disambiguated in the context of other concepts in an encoding system. We articulate this view in Semantic Discriminability Theory, a general framework for understanding conditions determining when people can infer meaning from perceptual features. Semantic discriminability is the degree to which observers can infer a unique mapping between visual features and concepts. Semantic Discriminability Theory posits that the capacity for semantic discriminability for a set of concepts is constrained by the difference between the feature-concept association distributions across the concepts in the set. We define formal properties of this theory, and test its implications in two experiments. The results show that the capacity to produce semantically discriminable colors for sets of concepts was indeed constrained by the statistical distance between color-concept association distributions (Experiment 1). Moreover, people could interpret meanings of colors in bar graphs insofar as the colors were semantically discriminable, even for concepts previously deemed \u201cnon-colorable\u201d (Experiment 2). The results suggest that colors are more robust for visual communication than previously thought.",
                        "time_start": "2021-10-27T13:00:00Z",
                        "time_end": "2021-10-27T13:15:00Z",
                        "uid": "v-full-1314",
                        "youtube_video_id": "84EfC2j4Uwk"
                    },
                    {
                        "type": "recorded",
                        "title": "Conceptual Metaphor and Graphical Convention Influence the Interpretation of Line Graphs",
                        "contributors": [
                            "Greg Woodin",
                            "Bodo Winter",
                            "Lace Padilla"
                        ],
                        "abstract": "Many metaphors in language reflect conceptual metaphors that structure thought. In line with metaphorical expressions such as \u2018high number\u2019, experiments show that people associate larger numbers with upward space. Consistent with this metaphor, high numbers are conventionally depicted in high positions on the y-axis of line graphs. People also associate good and bad (emotional valence) with upward and downward locations, in line with metaphorical expressions such as \u2018uplifting\u2019 and \u2018down in the dumps\u2019. Graphs depicting good quantities (e.g., vacation days) are consistent with graphical convention and the valence metaphor, because \u2018more\u2019 of the good quantity is represented by higher y-axis positions. In contrast, graphs depicting bad quantities (e.g., murders) are consistent with graphical convention, but not the valence metaphor, because more of the bad quantity is represented by higher (rather than lower) y-axis positions. We conducted two experiments (N = 300 per experiment) where participants answered questions about line graphs depicting good and bad quantities. For some graphs, we inverted the conventional axis ordering of numbers. Line graphs that aligned (vs misaligned) with valence metaphors (up = good) were easier to interpret, but this beneficial effect did not outweigh the adverse effect of inverting the axis numbering. Line graphs depicting good (vs bad) quantities were easier to interpret, as were graphs that depicted quantity using the x-axis (vs y-axis). Our results suggest that conceptual metaphors matter for the interpretation of line graphs. However, designers of line graphs are warned against subverting graphical convention to align with conceptual metaphors.",
                        "time_start": "2021-10-27T13:15:00Z",
                        "time_end": "2021-10-27T13:30:00Z",
                        "uid": "v-tvcg-9451590",
                        "youtube_video_id": "yDz4tKgrGcY"
                    },
                    {
                        "type": "recorded",
                        "title": "Rethinking the Ranks of Visual Channels",
                        "contributors": [
                            "Caitlyn McColeman",
                            "Fumeng Yang",
                            "Timothy F. Brady",
                            "Steven Franconeri"
                        ],
                        "abstract": "Data can be visually represented using visual channels like position, length or luminance. An existing ranking of these visual channels is based on how accurately participants could report the ratio between two depicted values. There is an assumption that this ranking should hold for different tasks and for different numbers of marks. However, there is surprisingly little existing work that tests this assumption, especially given that visually computing ratios is relatively unimportant in real-world visualizations, compared to seeing, remembering, and comparing trends and motifs, across displays that almost universally depict more than two values. To simulate the information extracted from a glance at a visualization, we instead asked participants to immediately reproduce a set of values from memory after they were shown the visualization. These values could be shown in a bar graph (position (bar)), line graph (position (line)), heat map (luminance), bubble chart (area), misaligned bar graph (length), or \u2018wind map\u2019 (angle). \n  With a Bayesian multilevel modeling approach, we observed how the relevant rank positions of visual channels shift across different numbers of marks (2, 4 or 8) and for bias, precision, and error measures. The ranking did not hold, even for reproductions of only 2 marks, and the new ranking was highly inconsistent for reproductions of different numbers of marks. Other factors besides channel choice had an order of magnitude more influence on performance, such as the number of values in the series (e.g., more marks led to larger errors), or the value of each mark (e.g., small values were systematically overestimated). \n  Every visual channel was worse for displays with 8 marks than 4, consistent with established limits on visual memory.\n  These results point to the need for a body of empirical studies that move beyond two-value ratio judgments as a baseline for ranking the quality of a visual channel, including testing new tasks (detection of trends or motifs), timescales (immediate computation, or later comparison), and the number of values (from a handful, to thousands).",
                        "time_start": "2021-10-27T13:30:00Z",
                        "time_end": "2021-10-27T13:45:00Z",
                        "uid": "v-full-1398",
                        "youtube_video_id": "FQMhTa_Whr0"
                    },
                    {
                        "type": "recorded",
                        "title": "Affective Congruence in Visualization Design: Influences on Reading Categorical Maps",
                        "contributors": [
                            "Cary Anderson",
                            "Anthony Robinson"
                        ],
                        "abstract": "Recent work in data visualization has demonstrated that small, perceptually-distinct color palettes\u2014such as those used in categorical mapping\u2014can connote significant affective qualities. Data that are mapped or otherwise visualized are also often emotive in nature, either inherently (e.g., climate change, disease mortality rates), or by design, such as can be found in visual storytelling. However, little is known about how the affective qualities of color interact with those of data context in visualization design. This paper describes the results of a crowdsourced study on the influence of affectively congruent versus incongruent color schemes on categorical map-reading response. We report both objective (pattern detection; area comparison) and subjective (affective quality; appropriateness; preference) measures of map-reader response. Our results suggest that affectively congruent colors amplify perceptions of the affective qualities of maps with emotive topics, affective incongruence may cause confusion, and that affective congruence is particularly influential in maps of positive-leaning data topics. Finally, we offer preliminary design recommendations for balancing color congruence with other design factors, and for synthesizing color and affective context in thematic map design.",
                        "time_start": "2021-10-27T13:45:00Z",
                        "time_end": "2021-10-27T14:00:00Z",
                        "uid": "v-tvcg-9318559",
                        "youtube_video_id": "yXp20dH_fXs"
                    },
                    {
                        "type": "recorded",
                        "title": "Modeling Just Noticeable Differences in Charts",
                        "contributors": [
                            "Min Lu",
                            "Joel Lanir",
                            "Chufeng Wang",
                            "Yucong Yao",
                            "Wen Zhang",
                            "Oliver Deussen",
                            "Hui Huang"
                        ],
                        "abstract": "One of the fundamental tasks in visualization is to compare two or more visual elements. However, it is often difficult to visually differentiate graphical elements encoding a small difference in value, such as the heights of similar bars in bar chart or angles of similar sections in pie chart. Perceptual laws can be used in order to model when and how we perceive this difference. In this work, we model the perception of Just Noticeable Differences (JNDs), the minimum difference in visual attributes that allow faithfully comparing similar elements, in charts. Specifically, we explore the relation between JNDs and two major visual variables: the intensity of visual elements and the distance between them, and study it in three charts: bar chart, pie chart and bubble chart. Through an empirical study, we identify main effects on JND for distance in bar charts, intensity in pie charts, and both distance and intensity in bubble charts. By fitting a linear mixed effects model, we model JND and find that JND grows as the exponential function of variables. We highlight several usage scenarios that make use of the JND modeling in which elements below the fitted JND are detected and enhanced with secondary visual cues for better discrimination.",
                        "time_start": "2021-10-27T14:00:00Z",
                        "time_end": "2021-10-27T14:15:00Z",
                        "uid": "v-full-1633",
                        "youtube_video_id": "f57dQK3-yrs"
                    },
                    {
                        "type": "recorded",
                        "title": "Augmenting Parallel Coordinates Plots with Color-coded Stacked Histograms",
                        "contributors": [
                            "Jinwook Bok",
                            "Bohyoung Kim",
                            "Jinwook Seo"
                        ],
                        "abstract": "We introduce Parallel Histogram Plot (PHP), a technique that overcomes the innate limitations of parallel coordinates plot (PCP) by attaching stacked-bar histograms with discrete color schemes to PCP. The color-coded histograms enable users to see an overview of the whole data without cluttering or scalability issues. Each rectangle in the PHP histograms is color coded according to the data ranking by a selected attribute. This color-coding scheme allows users to visually examine relationships between attributes, even between those that are displayed far apart, without repositioning or reordering axes. We adopt the Visual Information Seeking Mantra so that the polylines of the original PCP can be used to show details of a small number of selected items when the cluttering problem subsides. We also design interactions, such as a focus+context technique, to help users investigate small regions of interest in a space-efficient manner. We provide a real-world example in which PHP is effectively utilized compared with other visualizations, and we perform a controlled user study to evaluate the performance of PHP in helping users estimate the correlation between attributes. The results demonstrate that the performance of PHP was consistent in the estimation of correlations between two attributes regardless of the distance between them.",
                        "time_start": "2021-10-27T14:15:00Z",
                        "time_end": "2021-10-27T14:30:00Z",
                        "uid": "v-tvcg-9262081",
                        "youtube_video_id": "QCLXKrvCnPI"
                    }
                ]
            },
            {
                "title": "Surfaces and Volumes",
                "session_id": "v-full-full7",
                "track": "room3",
                "schedule_image": "v-full-full7.png",
                "chair": [
                    "Koji Koyamada"
                ],
                "organizers": [],
                "time_start": "2021-10-27T13:00:00Z",
                "time_end": "2021-10-27T14:30:00Z",
                "discord_category": "",
                "discord_channel": "",
                "discord_channel_id": 0,
                "youtube_url": null,
                "zoom_meeting": "",
                "zoom_password": "",
                "ff_link": "",
                "ff_playlist": "",
                "time_slots": [
                    {
                        "type": "recorded",
                        "title": "Real-Time Denoising of Volumetric Path Tracing for Direct Volume Rendering",
                        "contributors": [
                            "Jose Iglesias-Guitian",
                            "Prajita Mane",
                            "Bochang Moon"
                        ],
                        "abstract": "Direct Volume Rendering (DVR) using Volumetric Path Tracing (VPT) is a scientific visualization technique that simulates light transport with objects' matter using physically-based lighting models. Monte Carlo (MC) path tracing is often used with surface models, yet its application for volumetric models is difficult due to the complexity of integrating MC light-paths in volumetric media with none or smooth material boundaries. Moreover, auxiliary geometry-buffers (G-buffers) produced for volumes are typically very noisy, failing to guide image denoisers relying on that information to preserve image details. This makes existing real-time denoisers, which take noise-free G-buffers as their input, less effective when denoising VPT images. We propose the necessary modifications to an image-based denoiser previously used when rendering surface models, and demonstrate effective denoising of VPT images. In particular, our denoising exploits temporal coherence between frames, without relying on noise-free G-buffers, which has been a common assumption of existing denoisers for surface-models. Our technique preserves high-frequency details through a weighted recursive least squares that handles heterogeneous noise for volumetric models. We show for various real data sets that our method improves the visual fidelity and temporal stability of VPT during classic DVR operations such as camera movements, modifications of the light sources, and editions to the volume transfer function.",
                        "time_start": "2021-10-27T13:00:00Z",
                        "time_end": "2021-10-27T13:15:00Z",
                        "uid": "v-tvcg-9258424",
                        "youtube_video_id": "QHCQ4M5gFWw"
                    },
                    {
                        "type": "recorded",
                        "title": "Learning Adaptive Sampling and Reconstruction for Volume Visualization",
                        "contributors": [
                            "Sebastian Weiss",
                            "Mustafa Isik",
                            "Justus Thies",
                            "R\u00fcdiger Westermann"
                        ],
                        "abstract": "A central challenge in data visualization is to understand which data samples are required to generate an image of a data set in which the relevant information is encoded. In this work, we make a first step towards answering the question of whether an artificial neural network can predict where to sample the data with higher or lower density, by learning of correspondences between the data, the sampling patterns and the generated images. We introduce a novel neural rendering pipeline, which is trained end-to-end to generate a sparse adaptive sampling structure from a given low-resolution input image, and reconstructs a high-resolution image from the sparse set of samples. For the first time, to the best of our knowledge, we demonstrate that the selection of structures that are relevant for the final visual representation can be jointly learned together with the reconstruction of this representation from these structures. Therefore, we introduce differentiable sampling and reconstruction stages, which can leverage back-propagation based on supervised losses solely on the final image. We shed light on the adaptive sampling patterns generated by the network pipeline and analyze its use for volume visualization including isosurface and direct volume rendering.",
                        "time_start": "2021-10-27T13:15:00Z",
                        "time_end": "2021-10-27T13:30:00Z",
                        "uid": "v-tvcg-9264699",
                        "youtube_video_id": "q5nF9VRPnR0"
                    },
                    {
                        "type": "recorded",
                        "title": "STNet: An End-to-End Generative Framework for Synthesizing Spatiotemporal Super-Resolution Volumes",
                        "contributors": [
                            "Jun Han",
                            "Hao Zheng",
                            "Danny Chen",
                            "Chaoli Wang"
                        ],
                        "abstract": "We present STNet, an end-to-end generative framework that synthesizes spatiotemporal super-resolution volumes with high fidelity for time-varying data. STNet includes two modules: a generator and a spatiotemporal discriminator. The input to the generator is two low-resolution volumes at both ends, and the output is the intermediate and the two-ending spatiotemporal super- resolution volumes. The spatiotemporal discriminator, leveraging convolutional long short-term memory, accepts a spatiotemporal super-resolution sequence as input and predicts a conditional score for each volume based on its spatial (the volume itself) and temporal (the previous volumes) information. We propose an unsupervised pre-training stage using cycle loss to improve the generalization of STNet. Once trained, STNet can generate spatiotemporal super-resolution volumes from low-resolution ones, offering scientists an option to save data storage (i.e., sparsely sampling the simulation output in both spatial and temporal dimensions). We compare STNet with the baseline bicubic+linear interpolation, two deep learning solutions (SSR+TSR, STD), and a state-of-the-art tensor compression solution (TTHRESH) to show the effectiveness of STNet.",
                        "time_start": "2021-10-27T13:30:00Z",
                        "time_end": "2021-10-27T13:45:00Z",
                        "uid": "v-full-1098",
                        "youtube_video_id": "P_0wjXR3Vs8"
                    },
                    {
                        "type": "recorded",
                        "title": "Interactive Exploration of Physically-Observable Objective Vortices in Unsteady 2D Flow",
                        "contributors": [
                            "Xingdi Zhang",
                            "Markus Hadwiger",
                            "Thomas Theussl",
                            "Peter Rautek"
                        ],
                        "abstract": "State-of-the-art computation and visualization of vortices in unsteady fluid flow employ objective vortex criteria, which makes them independent of reference frames or observers. However, objectivity by itself, although crucial, is not sufficient to guarantee that one can identify physically-realizable observers that would perceive or detect the same vortices. Moreover, a significant challenge is that a single reference frame is often not sufficient to accurately observe multiple vortices that follow different motions. This paper presents a novel framework for the exploration and use of an interactively-chosen set of observers, of the resulting relative velocity fields, and of objective vortex structures. We show that our approach facilitates the objective detection and visualization of vortices relative to well-adapted reference frame motions, while at the same time guaranteeing that these observers are in fact physically realizable. In order to represent and manipulate observers efficiently, we make use of the low-dimensional vector space structure of the Lie algebra of physically-realizable observer motions. We illustrate that our framework facilitates the efficient choice and guided exploration of objective vortices in unsteady 2D flow, on planar as well as on spherical domains, using well-adapted reference frames.",
                        "time_start": "2021-10-27T13:45:00Z",
                        "time_end": "2021-10-27T14:00:00Z",
                        "uid": "v-full-1582",
                        "youtube_video_id": "g6pNtQBWIt0"
                    },
                    {
                        "type": "recorded",
                        "title": "3D Virtual Pancreatography",
                        "contributors": [
                            "Shreeraj Jadhav",
                            "Konstantin Dmitriev",
                            "Joseph Marino",
                            "Matthew Barish",
                            "Arie Kaufman"
                        ],
                        "abstract": "We present 3D virtual pancreatography (VP), a novel visualization procedure and application for non-invasive diagnosis and classification of pancreatic lesions, the precursors of pancreatic cancer. Currently, non-invasive screening of patients is performed through visual inspection of 2D axis-aligned CT images, though the relevant features are often not clearly visible nor automatically detected. VP is an end-to-end visual diagnosis system that includes: a machine learning based automatic segmentation of the pancreatic gland and the lesions, a semi-automatic approach to extract the primary pancreatic duct, a machine learning based automatic classification of lesions into four prominent types, and specialized 3D and 2D exploratory visualizations of the pancreas, lesions and surrounding anatomy. We combine volume rendering with pancreas- and lesion-centric visualizations and measurements for effective diagnosis. We designed VP through close collaboration and feedback from expert radiologists, and evaluated it on multiple real-world CT datasets with various pancreatic lesions and case studies examined by the expert radiologists.",
                        "time_start": "2021-10-27T14:00:00Z",
                        "time_end": "2021-10-27T14:15:00Z",
                        "uid": "v-tvcg-9184129",
                        "youtube_video_id": "DPpFh2JyxNk"
                    },
                    {
                        "type": "recorded",
                        "title": "Visual Analysis of Multi-Parameter Distributions across Ensembles of 3D Fields",
                        "contributors": [
                            "Alexander Kumpf",
                            "Josef Stumpfegger",
                            "Patrick H\u00e4rtl",
                            "R\u00fcdiger Westermann"
                        ],
                        "abstract": "For an ensemble of 3D multi-parameter fields, we present a visual analytics workflow to analyse whether and which parts of a selected multi-parameter distribution is present in all ensemble members. Supported by a parallel coordinate plot, a multi-parameter brush is applied to all ensemble members to select data points with similar multi-parameter distribution. By a combination of spatial sub-division and a covariance analysis of partitioned sub-sets of data points, a tight partition in multi-parameter space with reduced number of selected data points is obtained. To assess the representativeness of the selected multi-parameter distribution across the ensemble, we propose a novel extension of violin plots that can show multiple parameter distributions simultaneously. We investigate the visual design that effectively conveys (dis-)similarities in multi-parameter distributions, and demonstrate that users can quickly comprehend parameter-specific differences regarding distribution shape and representativeness from a side-by-side view of these plots. In a 3D spatial view, users can analyse and compare the spatial distribution of selected data points in different ensemble members via interval-based isosurface raycasting. In two real-world application cases we show how our approach is used to analyse the multi-parameter distributions across an ensemble of 3D fields.",
                        "time_start": "2021-10-27T14:15:00Z",
                        "time_end": "2021-10-27T14:30:00Z",
                        "uid": "v-tvcg-9362264",
                        "youtube_video_id": "qVe8iDHa8W4"
                    }
                ]
            },
            {
                "title": "Perspectives and Reflections",
                "session_id": "v-full-full13",
                "track": "room1",
                "schedule_image": "v-full-full13.png",
                "chair": [
                    "Kwan-Liu Ma"
                ],
                "organizers": [],
                "time_start": "2021-10-27T15:00:00Z",
                "time_end": "2021-10-27T16:30:00Z",
                "discord_category": "",
                "discord_channel": "",
                "discord_channel_id": 0,
                "youtube_url": null,
                "zoom_meeting": "",
                "zoom_password": "",
                "ff_link": "",
                "ff_playlist": "",
                "time_slots": [
                    {
                        "type": "recorded",
                        "title": "Seek for Success: A Visualization Approach for Understanding the Dynamics of Academic Careers",
                        "contributors": [
                            "Yifang Wang",
                            "Tai-Quan Peng",
                            "Huihua Lu",
                            "Haoren Wang",
                            "Xiao Xie",
                            "Huamin Qu",
                            "Yingcai Wu"
                        ],
                        "abstract": "How to achieve academic career success has been a long-standing research question in social science research. With the growing availability of large-scale well-documented academic profiles and career trajectories, scholarly interest in career success has been reinvigorated, which has emerged to be an active research domain called the Science of Science (i.e., SciSci). In this study, we adopt an innovative dynamic perspective to examine how individual and social factors will influence career success over time. We propose ACSeeker, an interactive visual analytics approach to explore the potential factors of success and how the influence of multiple factors changes at different stages of academic careers. We first applied a Multi-factor Impact Analysis framework to estimate the effect of different factors on academic career success over time. We then developed a visual analytics system to understand the dynamic effects interactively. A novel timeline is designed to reveal and compare the factor impacts based on the whole population. A customized career line showing the individual career development is provided to allow a detailed inspection. To validate the effectiveness and usability of ACSeeker, we report two case studies and interviews with a social scientist and general researchers.",
                        "time_start": "2021-10-27T15:00:00Z",
                        "time_end": "2021-10-27T15:15:00Z",
                        "uid": "v-full-1226",
                        "youtube_video_id": "9uwQNwOtT4g"
                    },
                    {
                        "type": "recorded",
                        "title": "VitaLITy: Promoting Serendipitous Discovery of Academic Literature with Transformers & Visual Analytics",
                        "contributors": [
                            "Arpit Narechania",
                            "Alireza Karduni",
                            "Ryan Wesslen",
                            "Emily Wall"
                        ],
                        "abstract": "There are a few prominent practices for conducting reviews of academic literature, including searching for specific keywords on Google Scholar or checking citations from some initial seed paper(s). These approaches serve a critical purpose for academic literature reviews, yet there remain challenges in identifying relevant literature when similar work may utilize different terminology (e.g., mixed-initiative visual analytics papers may not use the same terminology as papers on model-steering, yet the two topics are relevant to one another). In this paper, we introduce a system, VITALITY, intended to complement existing practices. In particular, VITALITY promotes serendipitous discovery of relevant literature using transformer language models, allowing users to find semantically similar papers in a word embedding space given (1) a list of input paper(s) or (2) a working abstract. VITALITY visualizes this document-level embedding space in an interactive 2-D scatterplot using dimension reduction. VITALITY also summarizes meta information about the document corpus or search query, including keywords and co authors, and allows users to save and export papers for use in a literature review. We present qualitative findings from an evaluation of VITALITY, suggesting it can be a promising complementary technique for conducting academic literature reviews. Furthermore, we contribute data from 38 popular data visualization publication venues in VITALITY, and we provide scrapers for the open-source community to continue to grow the list of supported venues.",
                        "time_start": "2021-10-27T15:15:00Z",
                        "time_end": "2021-10-27T15:30:00Z",
                        "uid": "v-full-1149",
                        "youtube_video_id": "WhByKz9jUM8"
                    },
                    {
                        "type": "recorded",
                        "title": "VIS30K: A Collection of Figures and Tables From IEEE Visualization Conference Publications",
                        "contributors": [
                            "Jian Chen",
                            "Meng Ling",
                            "Rui Li",
                            "Petra Isenberg",
                            "Tobias Isenberg",
                            "Michael Sedlmair",
                            "Torsten M\u00f6ller",
                            "Robert S. Laramee",
                            "Han-Wei Shen",
                            "Katharina W\u00fcsche",
                            "Qiru Wang"
                        ],
                        "abstract": "We present the VIS30K dataset, a collection of 29,689 images that represents 30 years of figures and tables from each track of the IEEE Visualization conference series (Vis, SciVis, InfoVis, VAST). VIS30K\u2019s comprehensive coverage of the scientific literature in visualization not only reflects the progress of the field but also enables researchers to study the evolution of the state-of-the-art and to find relevant work based on graphical content. We describe the dataset and our semi-automatic collection process, which couples convolutional neural networks (CNN) with curation. Extracting figures and tables semi-automatically allows us to verify that no images are overlooked or extracted erroneously. To improve quality further, we engaged in a peer-search process for high-quality figures from early IEEE Visualization papers. With the resulting data, we also contribute VISImageNavigator (VIN,visimagenavigator.github.io), a web-based tool that facilitates searching and exploring VIS30K by author names, paper keywords, title and abstract, and years.",
                        "time_start": "2021-10-27T15:30:00Z",
                        "time_end": "2021-10-27T15:45:00Z",
                        "uid": "v-tvcg-9337213"
                    },
                    {
                        "type": "recorded",
                        "title": "Gender in 30 Years of IEEE Visualization",
                        "contributors": [
                            "Natkamon Tovanich",
                            "Pierre Dragicevic",
                            "Petra Isenberg"
                        ],
                        "abstract": "We present an exploratory analysis of gender representation among the authors, committee members, and award winners at the IEEE Visualization (VIS) conference over the last 30 years. Our goal is to provide descriptive data on which diversity discussions and efforts in the community can build. We look in particular at the gender of VIS authors as a proxy for the community at large. We consider measures of overall gender representation among authors, differences in careers, positions in author lists, and collaborations. We found that the proportion of female authors has increased from 9% in the first five years to 22% in the last five years of the conference. Over the years, we found the same representation of women in program committees and slightly more women in organizing committees. Women are less likely to appear in the last author position, but more in the middle positions. In terms of collaboration patterns, female authors tend to collaborate more than expected with other women in the community. All non-gender related data is available on https://osf.io/ydfj4/ and the gender-author matching can be accessed through https://nyu.databrary.org/volume/1301.",
                        "time_start": "2021-10-27T15:45:00Z",
                        "time_end": "2021-10-27T16:00:00Z",
                        "uid": "v-full-1212",
                        "youtube_video_id": "pqkywtSTZz4"
                    },
                    {
                        "type": "recorded",
                        "title": "E-ffective: A Visual Analytic System for Exploring the Emotion and Effectiveness of Inspirational Speeches",
                        "contributors": [
                            "Kevin Maher",
                            "Zeyuan Huang",
                            "Jiancheng Song",
                            "Xiaoming Deng",
                            "Yu-Kun Lai",
                            "Cuixia Ma",
                            "Hao Wang",
                            "Yong-Jin Liu",
                            "Hongan Wang"
                        ],
                        "abstract": "What makes speeches effective has long been a subject for debate, and until today there is broad controversy among public speaking experts about what factors make a speech effective as well as the roles of these factors in speeches. Moreover, there is a lack of quantitative analysis methods to help understand effective speaking strategies. In this paper, we propose E-ffective, a visual analytic system allowing speaking experts and novices to analyze both the role of speech factors and their contribution in effective speeches. From interviews with domain experts and investigating existing literature, we figured out important factors to consider in inspirational speeches. We obtained the generated factors from multi-modal data that were then related to effectiveness data. Our system supports rapid understanding of critical factors in inspirational speeches, including the influence of emotions by means of novel visualization methods and interaction. Two novel visualizations include E-spiral (that shows the emotional shifts in speeches in a visually compact way) and E-script (that connects speech content with key speech delivery information). In our evaluation we studied the influence of our system on experts' domain knowledge about speech factors. We further studied the usability of the system by speaking novices and experts on assisting analysis of inspirational speech effectiveness.",
                        "time_start": "2021-10-27T16:00:00Z",
                        "time_end": "2021-10-27T16:15:00Z",
                        "uid": "v-full-1107",
                        "youtube_video_id": "ldcI-H2VmBI"
                    },
                    {
                        "type": "recorded",
                        "title": "Explanatory Journeys: Visualising to Understand and Explain Administrative Justice Paths of Redress",
                        "contributors": [
                            "Jonathan Roberts",
                            "Peter Butcher",
                            "Ann Sherlock",
                            "Sarah Nason"
                        ],
                        "abstract": "Administrative justice concerns the relationships between individuals and the state. It includes redress and complaints on decisions of a child\u2019s education, social care, licensing, planning, environment, housing and homelessness. However, if someone has a complaint or an issue, it is challenging for people to understand different possible redress paths and explore what path is suitable for their situation. Explanatory visualisation has the potential to display these paths of redress in a clear way, such that people can see, understand and explore their options. The visualisation challenge is further complicated because information is spread across many documents, laws, guidance and policies and requires judicial interpretation. Consequently, there is not a single database of paths of redress. In this work we present how we have co-designed a system to visualise administrative justice paths of redress. Simultaneously, we classify, collate and organise the underpinning data, from expert workshops, heuristic evaluation and expert critical reflection. We make four contributions: (i) an application design study of the explanatory visualisation tool (Artemus), (ii) coordinated and co-design approach to aggregating the data, (iii) two in-depth case studies in housing and education demonstrating explanatory paths of redress in administrative law, and (iv) reflections on the expert co-design process and expert data gathering and explanatory visualisation for administrative justice and law.",
                        "time_start": "2021-10-27T16:15:00Z",
                        "time_end": "2021-10-27T16:30:00Z",
                        "uid": "v-full-1316",
                        "youtube_video_id": "l5yT4PkWwE8"
                    }
                ]
            },
            {
                "title": "Decision Making",
                "session_id": "v-full-full27",
                "track": "room2",
                "schedule_image": "v-full-full27.png",
                "chair": [
                    "Daniel Archambault"
                ],
                "organizers": [],
                "time_start": "2021-10-27T15:00:00Z",
                "time_end": "2021-10-27T16:30:00Z",
                "discord_category": "",
                "discord_channel": "",
                "discord_channel_id": 0,
                "youtube_url": null,
                "zoom_meeting": "",
                "zoom_password": "",
                "ff_link": "",
                "ff_playlist": "",
                "time_slots": [
                    {
                        "type": "recorded",
                        "title": "The Unmet Data Visualization Needs of Decision Makers within Organizations",
                        "contributors": [
                            "Evanthia Dimara",
                            "Harry Zhang",
                            "Melanie Tory",
                            "Steven Franconeri"
                        ],
                        "abstract": "When an organization chooses one course of action over alternatives, this task typically falls on a decision maker with relevant knowledge, experience, and understanding of context. Decision makers rely on data analysis, which is either delegated to analysts, or done on their own. Often the decision maker combines data, likely uncertain or incomplete, with non-formalized knowledge within a multi-objective problem space, weighing the recommendations of analysts within broader contexts and goals. As most past research in visual analytics has focused on understanding the needs and challenges of data analysts, less is known about the tasks and challenges of organizational decision makers, and how visualization support tools might help. Here we characterize the decision maker as a domain expert, review relevant literature in management theories, and report the results of an empirical survey and interviews with people who make organizational decisions. We identify challenges and opportunities for novel visualization tools, including trade-off overviews, scenario-based analysis, interrogation tools, flexible data input and collaboration support. Our findings stress the need to expand visualization design beyond data analysis into tools for information management.",
                        "time_start": "2021-10-27T15:00:00Z",
                        "time_end": "2021-10-27T15:15:00Z",
                        "uid": "v-tvcg-9408391",
                        "youtube_video_id": "3C9ZpGVCXvk"
                    },
                    {
                        "type": "recorded",
                        "title": "Knowledge Rocks: Adding Knowledge Assistance to Visualization Systems",
                        "contributors": [
                            "Anna-Pia Lohfink",
                            "Simon Duque Anton",
                            "Heike Leitte",
                            "Christoph Garth"
                        ],
                        "abstract": "We present Knowledge Rocks, an implementation strategy and guideline for augmenting visualization systems to knowledge-assisted visualization systems, as defined by the KAVA model. Visualization systems become more and more sophisticated. Hence, it is increasingly important to support users with an integrated knowledge base in making constructive choices and drawing the right conclusions. We support the effective reactivation of visualization software resources by augmenting them with knowledge-assistance. To provide a general and yet supportive implementation strategy, we propose an implementation process that bases on an application-agnostic architecture. This architecture is derived from existing knowledge-assisted visualization systems and the KAVA model. Its centerpiece is an ontology that is able to automatically analyze and classify input data, linked to a database to store classified instances. We discuss design decisions and advantages of the KR framework and illustrate its broad area of application in diverse integration possibilities of this architecture into an existing visualization system. In addition, we provide a detailed case study by augmenting an it-security system with knowledge-assistance facilities.",
                        "time_start": "2021-10-27T15:15:00Z",
                        "time_end": "2021-10-27T15:30:00Z",
                        "uid": "v-full-1308",
                        "youtube_video_id": "kOPahy46lKM"
                    },
                    {
                        "type": "recorded",
                        "title": "A Critical Reflection on Visualization Research: Where Do Decision Making Tasks Hide?",
                        "contributors": [
                            "Evanthia Dimara",
                            "John Stasko"
                        ],
                        "abstract": "It has been widely suggested that a key goal of visualization systems is to assist decision making, but is this true? We conduct a critical investigation on whether the activity of decision making is indeed central to the visualization domain. By approaching decision making as a user task, we explore the degree to which decision tasks are evident in visualization research and user studies. Our analysis suggests that decision tasks are not commonly found in current visualization task taxonomies and that the visualization field has yet to leverage guidance from decision theory domains on how to study such tasks. We further found that the majority of visualizations addressing decision making were not evaluated based on their ability to assist decision tasks. Finally, to help expand the impact of visual analytics in organizational as well as casual decision making activities, we initiate a research agenda on how decision making assistance could be elevated throughout visualization research.",
                        "time_start": "2021-10-27T15:30:00Z",
                        "time_end": "2021-10-27T15:45:00Z",
                        "uid": "v-full-1341",
                        "youtube_video_id": "2LknNe-ONig"
                    },
                    {
                        "type": "recorded",
                        "title": "From Jam Session to Concert Hall: Synchronous Communication and Collaboration Around Data in Organizations",
                        "contributors": [
                            "Matthew Brehmer",
                            "Robert Kosara"
                        ],
                        "abstract": "Prior research on communicating with visualization has focused on public presentation and asynchronous individual consumption, such as in the domain of journalism. The visualization research community knows comparatively little about synchronous and multimodal communication around data within organizations, from team meetings to executive briefings. We conducted two qualitative interview studies with individuals who prepare and deliver presentations about data to audiences in organizations. In contrast to prior work, we did not limit our interviews to those who self-identify as data analysts or data scientists. Both studies examined aspects of speaking about data with visual aids such as charts, dashboards, and tables. One study was a retrospective examination of current practices and difficulties, from which we identified three scenarios involving presentations of data. We describe these scenarios using an analogy to musical performance: small collaborative team meetings are akin to jam session, while more structured presentations can range from semi-improvisational performances among peers to formal recitals given to executives or customers. In our second study, we grounded the discussion around three design probes, each examining a different aspect of presenting data: the progressive reveal of visualization to direct attention and advance a narrative, visualization presentation controls that are hidden from the audience\u2019s view, and the coordination of a presenter\u2019s video with interactive visualization. Our distillation of interviewees\u2019 responses surfaced twelve themes, from ways of authoring presentations to creating accessible and engaging audience experiences.",
                        "time_start": "2021-10-27T15:45:00Z",
                        "time_end": "2021-10-27T16:00:00Z",
                        "uid": "v-full-1065",
                        "youtube_video_id": "974JeiZ2Wuc"
                    },
                    {
                        "type": "recorded",
                        "title": "Causal Support: Modeling Causal Inferences with Visualizations",
                        "contributors": [
                            "Alex Kale",
                            "Yifan Wu",
                            "Jessica Hullman"
                        ],
                        "abstract": "Analysts often make visual causal inferences about possible data-generating models. However, visual analytics (VA) software tends to leave these models implicit in the mind of the analyst, which casts doubt on the statistical validity of informal visual \"insights\". We formally evaluate the quality of causal inferences from visualizations by adopting causal support---a Bayesian cognition model that learns the probability of alternative causal explanations given some data---as a normative benchmark for causal inferences. We contribute two experiments assessing how well crowdworkers can detect (1) a treatment effect and (2) a confounding relationship. We find that chart users\u2019 causal inferences tend to be insensitive to sample size such that they deviate from our normative benchmark. While interactively cross-filtering data in visualizations can improve sensitivity, on average users do not perform reliably better with common visualizations than they do with textual contingency tables. These experiments demonstrate the utility of causal support as an evaluation framework for inferences in VA and point to opportunities to make analysts' mental models more explicit in VA software.",
                        "time_start": "2021-10-27T16:00:00Z",
                        "time_end": "2021-10-27T16:15:00Z",
                        "uid": "v-full-1417",
                        "youtube_video_id": "5ZXLkqMudLU"
                    },
                    {
                        "type": "recorded",
                        "title": "Sibyl: Understanding and Addressing the Usability Challenges of Machine Learning In High-Stakes Decision Making",
                        "contributors": [
                            "Alexandra Zytek",
                            "Dongyu Liu",
                            "Rhema Vaithianathan",
                            "Kalyan Veeramachaneni"
                        ],
                        "abstract": "Machine learning (ML) is being applied to a diverse and ever-growing set of domains. In many cases, domain experts --- who often have no expertise in ML or data science --- are asked to use ML predictions to make high-stakes decisions. Multiple ML usability challenges can appear as result, such as lack of user trust in the model, inability to reconcile human-ML disagreement, and ethical concerns about oversimplification of complex problems to a single algorithm output. In this paper, we investigate the ML usability challenges that present in the domain of child welfare screening through a series of collaborations with child welfare screeners. Following the iterative design process between the ML scientists, visualization researchers, and domain experts (child screeners), we first identified four key ML challenges and honed in on one promising explainable ML technique to address them (local factor contributions). Then we designed, implemented, and evaluated our visual analytics tool, Sibyl, to increase the interpretability and interactivity of local factor contributions. The effectiveness of our tool is demonstrated by two formal user studies with 12 non-expert participants and 13 expert participants respectively. Valuable feedback is collected, from which we also composed a list of design implications as a useful guideline for researchers that aim to develop an interpretable and interactive visualization tool for ML prediction models deployed for child welfare screeners and other similar domain experts.",
                        "time_start": "2021-10-27T16:15:00Z",
                        "time_end": "2021-10-27T16:30:00Z",
                        "uid": "v-full-1492",
                        "youtube_video_id": "rSXsxE2FCMM"
                    }
                ]
            },
            {
                "title": "Multi-View and Visual Environments",
                "session_id": "v-full-full2",
                "track": "room3",
                "schedule_image": "v-full-full2.png",
                "chair": [
                    "Christophe Hurter"
                ],
                "organizers": [],
                "time_start": "2021-10-27T15:00:00Z",
                "time_end": "2021-10-27T16:30:00Z",
                "discord_category": "",
                "discord_channel": "",
                "discord_channel_id": 0,
                "youtube_url": null,
                "zoom_meeting": "",
                "zoom_password": "",
                "ff_link": "",
                "ff_playlist": "",
                "time_slots": [
                    {
                        "type": "recorded",
                        "title": "Semantic Snapping for Guided Multi-View Visualization Design",
                        "contributors": [
                            "Yngve S. Kristiansen",
                            "Laura Garrison",
                            "Stefan Bruckner"
                        ],
                        "abstract": "Visual information displays are typically composed of multiple visualizations that are used to facilitate an understanding of the underlying data. A common example are dashboards, which are frequently used in domains such as finance, process monitoring and business intelligence. However, users may not be aware of existing guidelines and lack expert design knowledge when composing such multi-view visualizations. In this paper, we present semantic snapping, an approach to help non-expert users design effective multi-view visualizations from sets of pre-existing views. When a particular view is placed on a canvas, it is \"aligned'' with the remaining views--not with respect to its geometric layout, but based on aspects of the visual encoding itself, such as how data dimensions are mapped to channels. Our method uses an on-the-fly procedure to detect and suggest resolutions for conflicting, misleading, or ambiguous designs, as well as to provide suggestions for alternative presentations. With this approach, users can be guided to avoid common pitfalls encountered when composing visualizations. Our provided examples and case studies demonstrate the usefulness and validity of our approach.",
                        "time_start": "2021-10-27T15:00:00Z",
                        "time_end": "2021-10-27T15:15:00Z",
                        "uid": "v-full-1349",
                        "youtube_video_id": "0wf7fuElPp8"
                    },
                    {
                        "type": "recorded",
                        "title": "SightBi: Exploring Cross-View Data Relationships with Biclusters",
                        "contributors": [
                            "Maoyuan Sun",
                            "Abdul Rahman Shaikh",
                            "Hamed Alhoori",
                            "Jian Zhao"
                        ],
                        "abstract": "Multiple-view visualization (MV) has been heavily used in visual analysis tools for sensemaking of data in various domains (e.g., bioinformatics, cybersecurity and text analytics). One common task of visual analysis with multiple views is to relate data across different views. For example, to identify threats, an intelligence analyst needs to link people from a social network graph with locations on a crime-map, and then search and read relevant documents. Currently, exploring cross-view data relationships heavily relies on view-coordination techniques (e.g., brushing and linking). They may require significant user effort on many trial-and-error attempts, such as repetitiously selecting elements in one view, observing and following elements highlighted in other views. To address this, we present SightBi, a visual analytics approach for supporting cross-view data relationship explorations. We discuss the design rationale of SightBi in detail, with identified user tasks regarding the usage of cross-view data relationships. SightBi formalize cross-view data relationships as biclusters and compute them from a dataset. SightBi uses a bi-context design that highlights creating stand-alone relationship-views. This helps to preserve existing views and serves as an overview of cross-view data relationships to guide user explorations. Moreover, SightBi allows users to interactively manage the layout of multiple views by using newly created relationship-views. With a usage scenario, we demonstrate the usefulness of SightBi for sensemaking of cross-view data relationships.",
                        "time_start": "2021-10-27T15:15:00Z",
                        "time_end": "2021-10-27T15:30:00Z",
                        "uid": "v-full-1293",
                        "youtube_video_id": "ypku1nBry6o"
                    },
                    {
                        "type": "recorded",
                        "title": "matExplorer: Visual Exploration on Predicting of Ionic Conductivity for Solid-State Electrolytes",
                        "contributors": [
                            "Jiansu Pu",
                            "Hui Shao",
                            "Boyang Gao",
                            "Zhengguo Zhu",
                            "Yanlin Zhu",
                            "Yunbo Rao",
                            "Yong Xiang"
                        ],
                        "abstract": "Lithium ion batteries (LIBs) are widely used as important energy sources for mobile phones, electric vehicles, and drones. Experts have attempted to replace liquid electrolytes with solid electrolytes that have wider electrochemical window and higher stability due to the potential safety risks, such as electrolyte leakage, flammable solvents, poor thermal stability, and many side reactions caused by liquid electrolytes. However, finding suitable alternative materials using traditional approaches is very difficult due to the incredibly high cost in searching. Machine learning (ML)-based methods are currently introduced and used for material prediction. However, learning tools designed for domain experts to conduct intuitive performance comparison and analysis of ML models are rare. In this case, we propose an interactive visualization system for experts to select suitable ML models and understand and explore the predication results comprehensively. Our system uses a multifaceted visualization scheme designed to support analysis from various perspectives, such as feature distribution, data similarity, model performance, and result presentation. Case studies with actual lab experiments have been conducted by the experts, and the final results confirmed the effectiveness and helpfulness of our system.",
                        "time_start": "2021-10-27T15:30:00Z",
                        "time_end": "2021-10-27T15:45:00Z",
                        "uid": "v-full-1462",
                        "youtube_video_id": "UQMrSD55sv4"
                    },
                    {
                        "type": "recorded",
                        "title": "TimeTubesX: A Query-Driven Visual Exploration of Observable, Photometric, and Polarimetric Behaviors of Blazars",
                        "contributors": [
                            "Naoko Sawada",
                            "Makoto Uemura",
                            "Johanna Beyer",
                            "Hanspeter Pfister",
                            "Issei Fujishiro"
                        ],
                        "abstract": "Blazars are celestial bodies of high interest to astronomers. In particular, through the analysis of photometric and polarimetric observations of blazars, astronomers aim to understand the physics of the blazar's relativistic jet. However, it is challenging to recognize correlations and time variations of the observed polarization, intensity, and color of the emitted light. In our prior study, we proposed TimeTubes to visualize a blazar dataset as a 3D volumetric tube. In this paper, we build primarily on the TimeTubes representation of blazar datasets to present a new visual analytics environment named TimeTubesX, into which we have integrated sophisticated feature and pattern detection techniques for effective location of observable and recurring time variation patterns in long-term, multi-dimensional datasets. Automatic feature extraction detects time intervals corresponding to well-known blazar behaviors. Dynamic visual querying allows users to search long-term observations for time intervals similar to a time interval of interest (query-by-example) or a sketch of temporal patterns (query-by-sketch). Users are also allowed to build up another visual query guided by the time interval of interest found in the previous process and refine the results. We demonstrate how TimeTubesX has been used successfully by domain experts for the detailed analysis of blazar datasets and report on the results.",
                        "time_start": "2021-10-27T15:45:00Z",
                        "time_end": "2021-10-27T16:00:00Z",
                        "uid": "v-tvcg-9200781",
                        "youtube_video_id": "KeeazynGw50"
                    },
                    {
                        "type": "recorded",
                        "title": "Towards replacing physical testing of granular materials with a Topology-based Model",
                        "contributors": [
                            "Aniketh Venkat",
                            "Attila Gyulassy",
                            "Graham Kosiba",
                            "Amitesh Maiti",
                            "Henry Reinstein",
                            "Richard Gee",
                            "Peer-Timo Bremer",
                            "Valerio Pascucci"
                        ],
                        "abstract": "In the study of packed granular materials, the performance of a sample (e.g., the detonation of a high-energy explosive) often correlates to measurements of a fluid flowing through it. The \u201ceffective surface area,\u201d the surface area accessible to the airflow, is typically measured using a permeametry apparatus that relates the flow conductance to the permeable surface area via the Carman-Kozeny equation. This equation allows calculating the flow rate of a fluid flowing through the granules packed in the sample for a given pressure drop. However, Carman-Kozeny makes inherent assumptions about tunnel shapes and flow paths that may not accurately hold in situations where the particles possess a wide distribution in shapes, sizes, and aspect ratios, as is true with many powdered systems of technological and commercial interest. To address this challenge, we replicate these measurements virtually on micro-CT images of the powdered material, introducing a new Pore Network Model (PNM) based on the skeleton of the Morse-Smale complex. Pores are identified as basins of the complex, their incidence encodes adjacency, and the conductivity of the capillary between them computed from the cross-section at their interface. We build and solve a resistive network to compute an approximate laminar fluid flow through the pore structure. We provide two means of estimating flow-permeable surface area: (i) by direct computation of conductivity, and (ii) by identifying dead-ends in the flow coupled with isosurface extraction and the application of the Carman-Kozeny equation, with the aim of establishing consistency over a range of particle shapes, sizes, porosity levels, and void distribution patterns.",
                        "time_start": "2021-10-27T16:00:00Z",
                        "time_end": "2021-10-27T16:15:00Z",
                        "uid": "v-full-1681",
                        "youtube_video_id": "ApcPl2cZQEA"
                    },
                    {
                        "type": "recorded",
                        "title": "Tracking Internal Frames of Reference for Consistent Molecular Distribution Functions",
                        "contributors": [
                            "Robin Sk\u00e5nberg",
                            "Martin Falk",
                            "Mathieu Linares",
                            "Anders Ynnerman",
                            "Ingrid Hotz"
                        ],
                        "abstract": "In molecular analysis, Spatial Distribution Functions (SDF) are fundamental instruments in answering questions related to spatial occurrences and relations of atomic structures over time. Given a molecular trajectory, SDFs can, for example, reveal the occurrence of water in relation to particular structures and hence provide clues of hydrophobic and hydrophilic regions. For the computation of meaningful distribution functions, the definition of molecular reference structures is essential. Therefore we introduce the concept of an internal frame of reference (IFR) for labeled point sets that represent selected molecular structures, and we propose an algorithm for tracking the IFR over time and space using a variant of Kabsch\u2019s algorithm. This approach lets us generate a consistent space for the aggregation of the SDF for molecular trajectories and molecular ensembles. We demonstrate the usefulness of the technique by applying it to temporal molecular trajectories as well as ensemble datasets. The examples include different docking scenarios with DNA, insulin, and aspirin.",
                        "time_start": "2021-10-27T16:15:00Z",
                        "time_end": "2021-10-27T16:30:00Z",
                        "uid": "v-tvcg-9324971",
                        "youtube_video_id": "EQW5jdrDl9g"
                    }
                ]
            },
            {
                "title": "Recommendation and Automation",
                "session_id": "v-full-full5",
                "track": "room1",
                "schedule_image": "v-full-full5.png",
                "chair": [
                    "Torsten M\u00f6ller"
                ],
                "organizers": [],
                "time_start": "2021-10-27T17:00:00Z",
                "time_end": "2021-10-27T18:30:00Z",
                "discord_category": "",
                "discord_channel": "",
                "discord_channel_id": 0,
                "youtube_url": null,
                "zoom_meeting": "",
                "zoom_password": "",
                "ff_link": "",
                "ff_playlist": "",
                "time_slots": [
                    {
                        "type": "recorded",
                        "title": "InfoColorizer: Interactive Recommendation of Color Palettes for Infographics",
                        "contributors": [
                            "Lin-Ping Yuan",
                            "Ziqi Zhou",
                            "Jian Zhao",
                            "Yiqiu Guo",
                            "Fan Du",
                            "Huamin Qu"
                        ],
                        "abstract": "When designing infographics, general users usually struggle with getting desired color palettes using existing infographic authoring tools, which sometimes sacrifice customizability, require design expertise, or neglect the influence of elements\u2019 spatial arrangement. We propose a data-driven method that provides flexibility by considering users\u2019 preferences, lowers the expertise barrier via automation, and tailors suggested palettes to the spatial layout of elements. We build a recommendation engine by utilizing deep learning techniques to characterize good color design practices from data, and further develop InfoColorizer, a tool that allows users to obtain color palettes for their infographics in an interactive and dynamic manner. To validate our method, we conducted a comprehensive four-part evaluation, including case studies, a controlled user study, a survey study, and an interview study. The results indicate that InfoColorizer can provide compelling palette recommendations with adequate flexibility, allowing users to effectively obtain high-quality color design for input infographics with low effort.",
                        "time_start": "2021-10-27T17:00:00Z",
                        "time_end": "2021-10-27T17:15:00Z",
                        "uid": "v-tvcg-9444798",
                        "youtube_video_id": "z23Zq5kZruE"
                    },
                    {
                        "type": "recorded",
                        "title": "A Mixed-Initiative Approach to Reusing Infographic Charts",
                        "contributors": [
                            "Weiwei Cui",
                            "Jinpeng Wang",
                            "He Huang",
                            "Yun Wang",
                            "Chin-Yew Lin",
                            "Haidong Zhang",
                            "Dongmei Zhang"
                        ],
                        "abstract": "Infographic bar charts have been widely adopted for communicating numerical information because of their attractiveness and memorability. However, these infographics are often created manually with general tools, such as PowerPoint and Adobe Illustrator, and merely composed of primitive visual elements, such as text blocks and shapes. With the absence of chart models, updating or reusing these infographics requires tedious and error-prone manual edits. In this paper, we propose a mixed-initiative approach to mitigate this pain point. On one hand, machines are adopted to perform precise and trivial operations, such as mapping numerical values to shape attributes and aligning shapes. On the other hand, we rely on humans to perform subjective and creative tasks, such as changing embellishments or approving the edits made by machines. We encapsulate our technique in a PowerPoint add-in prototype and demonstrate the effectiveness by applying our technique on a diverse set of infographic bar chart examples.",
                        "time_start": "2021-10-27T17:15:00Z",
                        "time_end": "2021-10-27T17:30:00Z",
                        "uid": "v-full-1637",
                        "youtube_video_id": "aCO3XhOMS80"
                    },
                    {
                        "type": "recorded",
                        "title": "Kori: Interactive Synthesis of Text and Charts in Data Documents",
                        "contributors": [
                            "Shahid Latif",
                            "Zheng Zhou",
                            "Yoon Kim",
                            "Fabian Beck",
                            "Nam Wook Kim"
                        ],
                        "abstract": "Charts go hand in hand with text to communicate complex data and are widely adopted in news articles, online blogs, and academic papers. They provide graphical summaries of the data, while text explains the message and context. However, synthesizing information across text and charts is difficult; it requires readers to frequently shift their attention. We investigated ways to support the tight coupling of text and charts in data documents. To understand their interplay, we analyzed the design space of such references through news articles and scientific papers. Informed by the analysis, we developed a mixed-initiative interface enabling users to construct interactive references between text and charts. It leverages natural language processing to automatically suggest references as well as allows users to manually construct other references effortlessly. A user study complemented with algorithmic evaluation of the system suggests that the interface provides an effective way to compose interactive data documents.",
                        "time_start": "2021-10-27T17:30:00Z",
                        "time_end": "2021-10-27T17:45:00Z",
                        "uid": "v-full-1008",
                        "youtube_video_id": "V7sGGWicuSA"
                    },
                    {
                        "type": "recorded",
                        "title": "Deconstructing Categorization in Visualization Recommendation: A Taxonomy and Comparative Study",
                        "contributors": [
                            "Doris Lee",
                            "Vidya Setlur",
                            "Melanie Tory",
                            "Karrie Karahalios",
                            "Aditya Parameswaran"
                        ],
                        "abstract": "Visualization recommendation (VisRec) systems provide users with suggestions for potentially interesting and useful next steps during exploratory data analysis. These recommendations are typically organized into categories based on their analytical actions, i.e., operations employed to transition from the current exploration state to a recommended visualization. However, despite the emergence of a plethora of VisRec systems in recent work, the utility of the categories employed by these systems in analytical workflows has not been systematically investigated. Our paper explores the efficacy of recommendation categories by formalizing a taxonomy of common categories and developing a system, Frontier, that implements these categories. Using Frontier, we evaluate workflow strategies adopted by users and how categories influence those strategies. Participants found recommendations that add attributes to enhance the current visualization and recommendations that filter to sub-populations to be comparatively most useful during data exploration. Our findings pave the way for next-generation VisRec systems that are adaptive and personalized via carefully chosen, effective recommendation categories.",
                        "time_start": "2021-10-27T17:45:00Z",
                        "time_end": "2021-10-27T18:00:00Z",
                        "uid": "v-tvcg-9444894",
                        "youtube_video_id": "O38Kceclb1Q"
                    },
                    {
                        "type": "recorded",
                        "title": "KG4Vis: A Knowledge Graph-Based Approach for Visualization Recommendation",
                        "contributors": [
                            "Haotian Li",
                            "Yong Wang",
                            "Songheng Zhang",
                            "Yangqiu Song",
                            "Huamin Qu"
                        ],
                        "abstract": "Visualization recommendation or automatic visualization generation can significantly lower the barriers for general users to rapidly create effective data visualizations, especially for those users without a background in data visualizations. However, existing rule-based approaches require tedious manual specifications of visualization rules by visualization experts. Other machine learning-based approaches often work like black-box and are difficult to understand why a specific visualization is recommended, limiting the wider adoption of these approaches. This paper fills the gap by presenting KG4Vis, a knowledge graph (KG)-based approach for visualization recommendation. It does not require manual specifications of visualization rules and can also guarantee good explainability. Specifically, we propose a framework for building knowledge graphs, consisting of three types of entities (i.e., data features, data columns and visualization design choices) and the relations between them, to model the mapping rules between data and effective visualizations. A TransE-based embedding technique is employed to learn the embeddings of both entities and relations of the knowledge graph from existing dataset-visualization pairs. Such embeddings intrinsically model the desirable visualization rules. Then, given a new dataset, effective visualizations can be inferred from the knowledge graph with semantically meaningful rules. We conducted extensive evaluations to assess the proposed approach, including quantitative comparisons, case studies and expert interviews. The results demonstrate the effectiveness of our approach.",
                        "time_start": "2021-10-27T18:00:00Z",
                        "time_end": "2021-10-27T18:15:00Z",
                        "uid": "v-full-1103",
                        "youtube_video_id": "kqgIdt41TWg"
                    },
                    {
                        "type": "recorded",
                        "title": "VizLinter: A Linter and Fixer Framework for Data Visualization",
                        "contributors": [
                            "Qing Chen",
                            "Fuling Sun",
                            "Xinyue Xu",
                            "Zui Chen",
                            "Jiazhe Wang",
                            "Nan Cao"
                        ],
                        "abstract": "Despite the rising popularity of automated visualization tools, existing systems tend to provide direct results which do not always fit the input data or meet visualization requirements. Therefore, additional specification adjustments are still required in real-world use cases. However, manual adjustments are difficult since most users do not necessarily possess adequate skills or visualization knowledge. Even experienced users might create imperfect visualizations that involve chart construction errors. We present a framework, VizFixer, to help users detect flaws and rectify already-built but defective visualizations. The framework consists of two components, (1) a visualization linter, which applies well-recognized principles to inspect the legitimacy of rendered visualizations, and (2) a visualization fixer, which automatically corrects the detected violations according to the linter. We implement the framework into an online editor prototype based on Vega-Lite specifications. To further evaluate the system, we conduct an in-lab user study. The results prove its effectiveness and efficiency in identifying and fixing errors for data visualizations.",
                        "time_start": "2021-10-27T18:15:00Z",
                        "time_end": "2021-10-27T18:30:00Z",
                        "uid": "v-full-1286",
                        "youtube_video_id": "zEUPVtBYjLc"
                    }
                ]
            },
            {
                "title": "Immersive Environments, Personal Vis, and Dashboards",
                "session_id": "v-full-full3",
                "track": "room2",
                "schedule_image": "v-full-full3.png",
                "chair": [
                    "Yalong Yang"
                ],
                "organizers": [],
                "time_start": "2021-10-27T17:00:00Z",
                "time_end": "2021-10-27T18:30:00Z",
                "discord_category": "",
                "discord_channel": "",
                "discord_channel_id": 0,
                "youtube_url": null,
                "zoom_meeting": "",
                "zoom_password": "",
                "ff_link": "",
                "ff_playlist": "",
                "time_slots": [
                    {
                        "type": "recorded",
                        "title": "Propagating Visual Designs to Numerous Plots and Dashboards",
                        "contributors": [
                            "Saiful Khan",
                            "Phong Nguyen",
                            "Alfie Abdul-Rahman",
                            "Benjamin Bach",
                            "Min Chen",
                            "Euan Freeman",
                            "Cagatay Turkay"
                        ],
                        "abstract": "In the process of developing an infrastructure for providing visualization and visual analytics (VIS) tools to epidemiologists and modeling scientists, we encountered a technical challenge for applying a number of visual designs to numerous datasets rapidly and reliably with limited development resources. In this paper, we present a technical solution to address this challenge. Operationally, we separate the tasks of data management, visual designs, and plots and dashboard deployment in order to streamline the development workflow. Technically, we utilize: an ontology to bring datasets, visual designs, and deployable plots and dashboards under the same management framework; multi-criteria search and ranking algorithms for discovering potential datasets that match a visual design; and a purposely-design user interface for propagating each visual design to appropriate datasets (often in tens and hundreds) and quality-assuring the propagation before the deployment. This technical solution has been used in the development of the RAMPVIS infrastructure for supporting a consortium of epidemiologists and modeling scientists through visualization.",
                        "time_start": "2021-10-27T17:00:00Z",
                        "time_end": "2021-10-27T17:15:00Z",
                        "uid": "v-full-1119",
                        "youtube_video_id": "w2FoWyMrAYM"
                    },
                    {
                        "type": "recorded",
                        "title": "Exploring the Personal Informatics Analysis Gap: \"There's a Lot of Bacon\"",
                        "contributors": [
                            "Jimmy Moore",
                            "Pascal Goffin",
                            "Jason Wiese",
                            "Miriah Meyer"
                        ],
                        "abstract": "Personal informatics research supports people in tracking personal data for the purposes of self-reflection and gaining self-knowledge. This field, however, has predominantly focused on the data collection and insight-generation elements of self-tracking, with less attention paid to flexible data analysis. As a result, this inattention has lead to inflexible analytic pipelines that do not reflect or support the diverse ways people want to engage their data. This paper contributes a review of personal informatics and visualization research literature to expose a gap in our knowledge for designing flexible tools that assist people with engaging and analyzing personal data in personal contexts, which we call the personal informatics analysis gap. We explore this gap through a multi-stage longitudinal study on how asthmatics engage personal air quality data, and we report how participants: are motivated by broad and diverse goals; exhibited patterns in the way they explored their data; engaged with their data in playful ways; discovered new insights through serendipitous exploration; and were reluctant to use analysis tools on their own. These results present new opportunities for visual analysis research and suggest the need for fundamental shifts in how and what we design for supporting analysis of personal data.",
                        "time_start": "2021-10-27T17:15:00Z",
                        "time_end": "2021-10-27T17:30:00Z",
                        "uid": "v-full-1022",
                        "youtube_video_id": "Qk20ayrFhxw"
                    },
                    {
                        "type": "recorded",
                        "title": "What's the Situation with Situated Visualization? A Survey and Perspectives on Situatedness",
                        "contributors": [
                            "Nathalie Bressa",
                            "Henrik Korsgaard",
                            "Aur\u00e9lien Tabard",
                            "Steven Houben",
                            "Jo Vermeulen"
                        ],
                        "abstract": "Situated visualization is an emerging concept within information visualization, in which data is visualized in situ, where it is relevant to people. The concept has gained interest from multiple research communities, including information visualization, human-computer interaction and augmented reality. This has led to a range of explorations and applications of the concept, however, this early work has focused on the operational aspect of situatedness leading to inconsistent adoption of the concept and terminology. \n  First, we contribute a literature survey in which we analyze 40 papers that explicitly use the term \"situated visualization\" to provide an overview of the research area, how it defines situated visualization, common application areas and technology used, as well as type of data and type of visualizations. Our survey shows that research on situated visualization has focused on technology-centric approaches that forefront a spatial understanding of situatedness. \n  Secondly, we contribute five perspectives on situatedness (space, time, place, activity, and community) that expand on the prevalent notion of situatedness in the corpus. We draw from six case studies and prior theoretical developments in HCI. Each perspective develops a generative way of looking at and working with situatedness in design and research. We outline future directions, including considering technology, material and aesthetics, leveraging the perspectives for design, and methods for stronger engagement with target audiences. We conclude with opportunities to consolidate situated visualization research.",
                        "time_start": "2021-10-27T17:30:00Z",
                        "time_end": "2021-10-27T17:45:00Z",
                        "uid": "v-full-1106",
                        "youtube_video_id": "oBSUEt3jwBI"
                    },
                    {
                        "type": "recorded",
                        "title": "TIVEE: Visual Exploration and Explanation of Badminton Tactics in Immersive Visualizations",
                        "contributors": [
                            "Xiangtong Chu",
                            "Xiao Xie",
                            "Shuainan Ye",
                            "Haolin Lu",
                            "Hongguang Xiao",
                            "Zeqing Yuan",
                            "Zhutian Chen",
                            "Hui Zhang",
                            "Yingcai Wu"
                        ],
                        "abstract": "Tactic analysis is a major issue in badminton as the effective usage of tactics is the key to win. The tactic in badminton is defined as a sequence of consecutive strokes. Most existing methods use statistical models to find sequential patterns of strokes and apply 2D visualizations such as glyphs and statistical charts to explore and analyze the discovered patterns. However, in badminton, spatial information like the shuttle trajectory, which is inherently 3D, is the core of a tactic. The lack of sufficient spatial awareness in 2D visualizations largely limited the tactic analysis of badminton. In this work, we collaborate with domain experts to study the tactic analysis of badminton in a 3D environment and propose an immersive visual analytics system, TIVEE, to assist users in exploring and explaining badminton tactics from multi-levels. Users can first explore various tactics from the third-person perspective using an unfolded visual presentation of stroke sequences. By selecting a tactic of interest, users can turn to the first-person perspective to perceive the detailed kinematic characteristics and explain its effects on the game result. The effectiveness and usefulness of TIVEE are demonstrated by case studies and an expert interview.",
                        "time_start": "2021-10-27T17:45:00Z",
                        "time_end": "2021-10-27T18:00:00Z",
                        "uid": "v-full-1038",
                        "youtube_video_id": "nZzM4cWW5Mc"
                    },
                    {
                        "type": "recorded",
                        "title": "Touch and Beyond: Comparing Physical and Virtual Reality Visualizations",
                        "contributors": [
                            "Kurtis Danyluk",
                            "Teoman Ulusoy",
                            "Wei Wei",
                            "Wesley Willett"
                        ],
                        "abstract": "We compare physical and virtual reality (VR) versions of simple data visualizations. We also explore how the addition of virtual annotation and filtering tools affects how viewers solve basic data analysis tasks. We report on two studies, inspired by previous examinations of data physicalizations. The first study examined differences in how viewers interact with physical hand-scale, virtual hand-scale, and virtual table-scale visualizations and the impact that the different forms had on viewer's problem solving behavior. A second study examined how interactive annotation and filtering tools might sup-port new modes of use that transcend the limitations of physical representations. Our results highlight challenges associated with virtual reality representations and hint at the potential of interactive annotation and filtering tools in VR visualizations.",
                        "time_start": "2021-10-27T18:00:00Z",
                        "time_end": "2021-10-27T18:15:00Z",
                        "uid": "v-tvcg-9193986",
                        "youtube_video_id": "z1-rG-q28g8"
                    },
                    {
                        "type": "recorded",
                        "title": "An Automated Approach to Reasoning About Task-Oriented Visualization Insights in Responsive Visualization",
                        "contributors": [
                            "Hyeok Kim",
                            "Ryan Rossi",
                            "Abhraneel Sarma",
                            "Dominik Moritz",
                            "Jessica Hullman"
                        ],
                        "abstract": "Authors often transform a large screen visualization for smaller displays through rescaling, aggregation and other techniques when creating visualizations for both desktop and mobile devices (i.e., responsive visualization). However, transformations can alter relationships or patterns implied by the large screen view, requiring authors to reason carefully about what information to preserve while adjusting their design for the smaller display. We propose an automated approach to approximating the loss of support for task-oriented visualization insights (identification, comparison, and trend) in responsive transformation of a source visualization. We operationalize identification, comparison, and trend loss as objective functions calculated by comparing properties of the rendered source visualization to each realized target (small screen) visualization. To evaluate the utility of our approach, we train machine learning models on human ranked small screen alternative visualizations across a set of source visualizations. We find that our approach achieves an accuracy of 84% (random forest model) in ranking visualizations. We demonstrate this approach in a prototype responsive visualization recommender that enumerates responsive transformations using Answer Set Programming and evaluates the preservation of task-oriented insights using our loss measures. We discuss implications of our approach for the development of automated and semi-automated responsive visualization recommendation.",
                        "time_start": "2021-10-27T18:15:00Z",
                        "time_end": "2021-10-27T18:30:00Z",
                        "uid": "v-full-1219",
                        "youtube_video_id": "WaMnB0Pp0U0"
                    }
                ]
            },
            {
                "title": "Multi-Dimensional Data",
                "session_id": "v-full-full14",
                "track": "room3",
                "schedule_image": "v-full-full14.png",
                "chair": [
                    "Michael Sedlmair"
                ],
                "organizers": [],
                "time_start": "2021-10-27T17:00:00Z",
                "time_end": "2021-10-27T18:30:00Z",
                "discord_category": "",
                "discord_channel": "",
                "discord_channel_id": 0,
                "youtube_url": null,
                "zoom_meeting": "",
                "zoom_password": "",
                "ff_link": "",
                "ff_playlist": "",
                "time_slots": [
                    {
                        "type": "recorded",
                        "title": "Imma Sort by Two or More Attributes With Interpretable Monotonic Multi-Attribute Sorting",
                        "contributors": [
                            "Yan Lyu",
                            "Fan Gao",
                            "I-Shuen Wu",
                            "Brian Lim"
                        ],
                        "abstract": "Many choice problems often involve multiple attributes which are mentally challenging, because only one attribute is neatly sorted while others could be randomly arranged. We hypothesize that perceiving approximately monotonic trends across multiple attributes is key to the overall interpretability of sorted results, because users can easily predict the attribute values of the next items. We extend a ranking principal curve model to tune monotonic trends in attributes and present Imma Sort to sort items by multiple attributes simultaneously by trading-off the monotonicity in the primary sorted attribute to increase the human predictability for other attributes. We characterize how it performs for varying attribute correlations, attribute preferences, list lengths and number of attributes. We further extend Imma Sort with ImmaAnchor and ImmaCenter to improve the learnability and efficiency to search sorted items with conflicting attributes. We demonstrate usage scenarios for two applications and evaluate its learnability, usability, interpretability and user performance in prediction and search tasks. We find that Imma Sort improves the interpretability and satisfaction of sorting by \u2265 2 attributes. We discuss why, when, where, and how to deploy Imma Sort for real-world applications.",
                        "time_start": "2021-10-27T17:00:00Z",
                        "time_end": "2021-10-27T17:15:00Z",
                        "uid": "v-tvcg-9288641",
                        "youtube_video_id": "ob1OqwSD5p0"
                    },
                    {
                        "type": "recorded",
                        "title": "Integrated Dual Analysis of Quantitative and Qualitative High-Dimensional Data",
                        "contributors": [
                            "Juliane M\u00fcller",
                            "Laura Garrison",
                            "Philipp Ulbrich",
                            "Stefanie Schreiber",
                            "Stefan Bruckner",
                            "Helwig Hauser",
                            "Steffen Oeltze-Jafra"
                        ],
                        "abstract": "The Dual Analysis framework is a powerful enabling technology for the exploration of high dimensional quantitative data by treating data dimensions as first-class objects that can be explored in tandem with data values. In this work, we extend the Dual Analysis framework through the joint treatment of quantitative (numerical) and qualitative (categorical) dimensions. Computing common measures for all dimensions allows us to visualize both quantitative and qualitative dimensions in the same view. This enables a natural joint treatment of mixed data during interactive visual exploration and analysis. Several measures of variation for nominal qualitative data can also be applied to ordinal qualitative and quantitative data. For example, instead of measuring variability from a mean or median, other measures assess inter-data variation or average variation from a mode. In this work, we demonstrate how these measures can be integrated into the Dual Analysis framework to explore and generate hypotheses about high-dimensional mixed data. A medical case study using clinical routine data of patients suffering from Cerebral Small Vessel Disease (CSVD), conducted with a senior neurologist and a medical student, shows that a joint Dual Analysis approach for quantitative and qualitative data can rapidly lead to new insights based on which new hypotheses may be generated.",
                        "time_start": "2021-10-27T17:15:00Z",
                        "time_end": "2021-10-27T17:30:00Z",
                        "uid": "v-tvcg-9346003"
                    },
                    {
                        "type": "recorded",
                        "title": "Revisiting Dimensionality Reduction Approaches for Visual Cluster Analysis: An Empirical Study",
                        "contributors": [
                            "Jiazhi Xia",
                            "Yuchen Zhang",
                            "Jie Song",
                            "Yang Chen",
                            "Yunhai Wang",
                            "Shixia Liu"
                        ],
                        "abstract": "Dimensionality Reduction (DR) techniques can generate 2D projections and enable visual exploration of cluster structures of high-dimensional datasets. However, different DR techniques would yield various patterns, which significantly affect the performance of visual cluster analysis tasks. We present the results of a user study that investigates the influence of different DR techniques on visual cluster analysis. Our study focuses on the most concerned property types, namely the linearity and locality, and evaluates twelve representative DR techniques that cover the concerned properties. Four controlled experiments were conducted to evaluate how the DR techniques facilitate the tasks of 1) identifying clusters, 2) associating cluster members, 3) comparing distances among clusters, and 4) comparing cluster densities, respectively. We also evaluated users\u2019 subjective preference of the DR techniques regarding the quality of projected clusters. The results show that: 1) Non-linear and Local techniques are preferred in identifying clusters and associating cluster members; 2) Linear techniques perform better than non-linear techniques in comparing cluster densities; 3) UMAP (Uniform Manifold Approximation and Projection) and t-SNE (t-Distributed Stochastic Neighbor Embedding) perform the best in identifying clusters and associating cluster members; 4) NMF(Nonnegative Matrix Factorization) has competitive performance in comparing distances among clusters; 5) t-SNLE(t-Distributed Stochastic Neighbor Linear Embedding) has competitive performance in comparing cluster densities.",
                        "time_start": "2021-10-27T17:30:00Z",
                        "time_end": "2021-10-27T17:45:00Z",
                        "uid": "v-full-1532",
                        "youtube_video_id": "0W5Trbcb9sM"
                    },
                    {
                        "type": "recorded",
                        "title": "DimLift: Interactive Hierarchical Data Exploration through Dimensional Bundling",
                        "contributors": [
                            "Laura Garrison",
                            "Juliane M\u00fcller",
                            "Stefanie Schreiber",
                            "Steffen Oeltze-Jafra",
                            "Helwig Hauser",
                            "Stefan Bruckner"
                        ],
                        "abstract": "The identification of interesting patterns and relationships is essential to exploratory data analysis. This becomes increasingly difficult in high dimensional datasets. While dimensionality reduction techniques can be utilized to reduce the analysis space, these may unintentionally bury key dimensions within a larger grouping and obfuscate meaningful patterns. With this work we introduce DimLift, a novel visual analysis method for creating and interacting with dimensional bundles. Generated through an iterative dimensionality reduction or user-driven approach, dimensional bundles are expressive groups of dimensions that contribute similarly to the variance of a dataset. Interactive exploration and reconstruction methods via a layered parallel coordinates plot allow users to lift interesting and subtle relationships to the surface, even in complex scenarios of missing and mixed data types. We exemplify the power of this technique in an expert case study on clinical cohort data alongside two additional case examples from nutrition and ecology.",
                        "time_start": "2021-10-27T17:45:00Z",
                        "time_end": "2021-10-27T18:00:00Z",
                        "uid": "v-tvcg-9349198",
                        "youtube_video_id": "MdptI_kkes8"
                    },
                    {
                        "type": "recorded",
                        "title": "Attribute-based Explanations of Non-Linear Embeddings of High-Dimensional Data",
                        "contributors": [
                            "Jan-Tobias Sohns",
                            "Michaela Schmitt",
                            "Fabian Jirasek",
                            "Hans Hasse",
                            "Heike Leitte"
                        ],
                        "abstract": "Embeddings of high-dimensional data are widely used to explore data, to verify analysis results, and to communicate information. Their explanation, in particular with respect to the input attributes, is often difficult. With linear projects like PCA the axes can still be annotated meaningfully. With non-linear projections this is no longer possible and alternative strategies such as attribute-based color coding are required. In this paper, we review existing augmentation techniques and discuss their limitations. We present the Non-Linear Embeddings Surveyor (NoLiES) that combines a novel augmentation strategy for projected data (rangesets) with interactive analysis in a small multiples setting. Rangesets use a set-based visualization approach for binned attribute values that enable the user to quickly observe structure and detect outliers. We detail the link between algebraic topology and rangesets and demonstrate the utility of NoLiES in case studies with various challenges (complex attribute value distribution, many attributes, many data points) and a real-world application to understand latent features of matrix completion in thermodynamics.",
                        "time_start": "2021-10-27T18:00:00Z",
                        "time_end": "2021-10-27T18:15:00Z",
                        "uid": "v-full-1440",
                        "youtube_video_id": "hnRBSaXC0QU"
                    },
                    {
                        "type": "recorded",
                        "title": "Measuring and Explaining the Inter-Cluster Reliability of Multidimensional Projections",
                        "contributors": [
                            "Hyeon Jeon",
                            "Hyung-Kwon Ko",
                            "Jaemin Jo",
                            "Youngtaek Kim",
                            "Jinwook Seo"
                        ],
                        "abstract": "We propose Steadiness and Cohesiveness, two novel metrics to measure the inter-cluster reliability of multidimensional projection (MDP), specifically how well the inter-cluster structures are preserved between the original high-dimensional space and the low-dimensional projection space. Measuring inter-cluster reliability is crucial as it directly affects how well inter-cluster tasks (e.g., identifying cluster relationships in the original space from a projected view) can be conducted; however, despite the importance of inter-cluster tasks, we found that previous metrics, such as Trustworthiness and Continuity, fail to measure inter-cluster reliability. Our metrics consider two aspects of the inter-cluster reliability: Steadiness measures the extent to which clusters in the projected space form clusters in the original space, and Cohesiveness measures the opposite. They extract random clusters with arbitrary shapes and positions in one space and evaluate how much the clusters are stretched or dispersed in the other space. Furthermore, our metrics can quantify pointwise distortions, allowing for the visualization of inter-cluster reliability in a projection, which we call a reliability map. Through quantitative experiments, we verify that our metrics precisely capture the distortions that harm inter-cluster reliability while previous metrics have difficulty capturing the distortions. A case study also demonstrates that our metrics and the reliability map 1) support users in selecting the proper projection techniques or hyperparameters and 2) prevent misinterpretation while performing inter-cluster tasks, thus allow an adequate identification of inter-cluster structure.",
                        "time_start": "2021-10-27T18:15:00Z",
                        "time_end": "2021-10-27T18:30:00Z",
                        "uid": "v-full-1020",
                        "youtube_video_id": "haQ5uwI5lmw"
                    }
                ]
            },
            {
                "title": "Sports, Commerce, and Social Media",
                "session_id": "v-full-full21",
                "track": "room1",
                "schedule_image": "v-full-full21.png",
                "chair": [
                    "Romain Vuillemot"
                ],
                "organizers": [],
                "time_start": "2021-10-28T13:00:00Z",
                "time_end": "2021-10-28T14:30:00Z",
                "discord_category": "",
                "discord_channel": "",
                "discord_channel_id": 0,
                "youtube_url": null,
                "zoom_meeting": "",
                "zoom_password": "",
                "ff_link": "",
                "ff_playlist": "",
                "time_slots": [
                    {
                        "type": "recorded",
                        "title": "Augmenting Sports Videos with VisCommentator",
                        "contributors": [
                            "Zhutian Chen",
                            "Shuainan Ye",
                            "Xiangtong Chu",
                            "Haijun Xia",
                            "Hui Zhang",
                            "Huamin Qu",
                            "Yingcai Wu"
                        ],
                        "abstract": "Visualizing data in sports videos is gaining traction in sports analytics, given its ability to communicate insights and explicate player strategies engagingly. However, augmenting sports videos with such data visualizations is challenging, especially for sports analysts, as it requires considerable expertise in video editing. To ease the creation process, we present a design space that characterizes augmented sports videos at an element-level (what the constituents are) and clip-level (how those constituents are organized). We do so by systematically reviewing 233 examples of augmented sports videos collected from TV channels, teams, and leagues. The design space guides selection of data insights and visualizations for various purposes. Informed by the design space and close collaboration with domain experts, we design VisCommentator, a fast prototyping tool, to eases the creation of augmented table tennis videos by leveraging machine learning-based data extractors and design space-based visualization recommendations. With VisCommentator, sports analysts can create an augmented video by selecting the data to visualize instead of manually drawing the graphical marks. Our system can be generalized to other racket sports (e.g., tennis, badminton) once the underlying datasets and models are available. A user study with seven domain experts shows high satisfaction with our system, confirms that the participants can reproduce augmented sports videos in a short period, and provides insightful implications into future improvements and opportunities.",
                        "time_start": "2021-10-28T13:00:00Z",
                        "time_end": "2021-10-28T13:15:00Z",
                        "uid": "v-full-1578",
                        "youtube_video_id": "qUH-XCqcefo"
                    },
                    {
                        "type": "recorded",
                        "title": "TacticFlow: Visual Analytics of Ever-Changing Tactics in Racket Sports",
                        "contributors": [
                            "Jiang Wu",
                            "Dongyu Liu",
                            "Ziyang Guo",
                            "Qingyang Xu",
                            "Yingcai Wu"
                        ],
                        "abstract": "Event sequence mining is often used to summarize patterns from hundreds of sequences but faces special challenges when handling racket sports data. In racket sports (e.g., tennis and badminton), a player hitting the ball is considered a multivariate event consisting of multiple attributes (e.g., hit technique and ball position). A rally (i.e., a series of consecutive hits beginning with one player serving the ball and ending with one player winning a point) thereby can be viewed as a multivariate event sequence. Mining frequent patterns and depicting how patterns change over time is instructive and meaningful to players who want to learn more short-term competitive strategies (i.e., tactics) that encompass multiple hits. However, players in racket sports usually change their tactics rapidly according to the opponent\u2019s reaction, resulting in ever-changing tactic progression. In this work, we introduce a tailored visualization system built on a novel multivariate sequence pattern mining algorithm to facilitate explorative identification and analysis of various tactics and tactic progression. The algorithm can mine multiple non-overlapping multivariate patterns from hundreds of sequences effectively. Based on the mined results, we propose a glyph-based Sankey diagram to visualize the ever-changing tactic progression and support interactive data exploration. Through two case studies with four domain experts in tennis and badminton, we demonstrate that our system can effectively obtain insights about tactic progression in most racket sports. We further discuss the strengths and the limitations of our system based on domain experts\u2019 feedback.",
                        "time_start": "2021-10-28T13:15:00Z",
                        "time_end": "2021-10-28T13:30:00Z",
                        "uid": "v-full-1135",
                        "youtube_video_id": "HA640FRJGxA"
                    },
                    {
                        "type": "recorded",
                        "title": "VideoModerator: A Risk-aware Framework for Multimodal Video Moderation in E-Commerce",
                        "contributors": [
                            "Tan Tang",
                            "Yanhong Wu",
                            "Lingyun Yu",
                            "Yuhong Li",
                            "Yingcai Wu"
                        ],
                        "abstract": "Video moderation, which refers to remove deviant or explicit content from e-commerce livestreams, has become prevalent owing to social and engaging features. However, this task is tedious and time consuming due to the difficulties associated with watching and reviewing multimodal video content, including video frames and audio clips. To ensure effective video moderation, we propose VideoModerator, a risk-aware framework that seamlessly integrates human knowledge with machine insights. This framework incorporates a set of advanced machine learning models to extract the risk-aware features from multimodal video content and discover potentially deviant videos. Moreover, this framework introduces an interactive visualization interface with three views, namely, a video view, a frame view, and an audio view. In the video view, we adopt a segmented timeline and highlight high-risk periods that may contain deviant information. In the frame view, we present a novel visual summarization method that combines risk-aware features and video context to enable quick video navigation. In the audio view, we employ a storyline-based design to provide a multi-faceted overview which can be used to explore audio content. Furthermore, we report the usage of VideoModerator through a case scenario and conduct experiments and a controlled user study to validate its effectiveness.",
                        "time_start": "2021-10-28T13:30:00Z",
                        "time_end": "2021-10-28T13:45:00Z",
                        "uid": "v-full-1218",
                        "youtube_video_id": "hJJswpDpOdM"
                    },
                    {
                        "type": "recorded",
                        "title": "A Visualization Approach for Monitoring Order Processing in E-Commerce Warehouse",
                        "contributors": [
                            "Junxiu Tang",
                            "Yuhua Zhou",
                            "Tan Tang",
                            "Di Weng",
                            "Boyang Xie",
                            "Lingyun Yu",
                            "Huaqiang Zhang",
                            "Yingcai Wu"
                        ],
                        "abstract": "The efficiency of warehouses is vital to e-commerce. Fast order processing at the warehouses ensures timely deliveries and improves customer satisfaction. However, monitoring, analyzing, and manipulating order processing in the warehouses in real time are challenging for traditional methods due to the sheer volume of incoming orders, the fuzzy definition of delayed order patterns, and the complex decision-making of order handling priorities. In this paper, we adopt a data-driven approach and propose OrderMonitor, a visual analytics system that assists warehouse managers in analyzing and improving order processing efficiency in real time based on streaming warehouse event data. Specifically, the order processing pipeline is visualized with a novel pipeline design based on the sedimentation metaphor to facilitate real-time order monitoring and suggest potentially abnormal orders. We also design a novel visualization that depicts order timelines based on the Gantt charts and Marey's graphs. Such a visualization helps the managers gain insights into the performance of order processing and find major blockers for delayed orders. Furthermore, an evaluating view is provided to assist users in inspecting order details and assigning priorities to improve the processing performance. The effectiveness of OrderMonitor is evaluated with two case studies on a real-world warehouse dataset.",
                        "time_start": "2021-10-28T13:45:00Z",
                        "time_end": "2021-10-28T14:00:00Z",
                        "uid": "v-full-1421",
                        "youtube_video_id": "Xg6QrmL3KTo"
                    },
                    {
                        "type": "recorded",
                        "title": "MiningVis: Visual Analytics of the Bitcoin Mining Economy",
                        "contributors": [
                            "Natkamon Tovanich",
                            "Nicolas Souli\u00e9",
                            "Nicolas Heulot",
                            "Petra Isenberg"
                        ],
                        "abstract": "We present a visual analytics tool, MiningVis, to explore the long-term historical evolution and dynamics of the Bitcoin mining ecosystem. Bitcoin is a cryptocurrency that attracts much attention but remains difficult to understand. Particularly important to the success, stability, and security of Bitcoin is a component of the system called \"mining.'' Miners are responsible for validating transactions and are incentivized to participate by the promise of a monetary reward. Mining pools have emerged as collectives of miners that ensure a more stable and predictable income. MiningVis aims to help analysts understand the evolution and dynamics of the Bitcoin mining ecosystem, including mining market statistics, multi-measure mining pool rankings, and pool hopping behavior. Each of these features can be compared to external data concerning pool characteristics and Bitcoin news. In order to assess the value of MiningVis, we conducted online interviews and insight-based user studies with Bitcoin miners. We describe research questions tackled and insights made by our participants and illustrate practical implications for visual analytics systems for Bitcoin mining.",
                        "time_start": "2021-10-28T14:00:00Z",
                        "time_end": "2021-10-28T14:15:00Z",
                        "uid": "v-full-1211",
                        "youtube_video_id": "cmEJX05wU3I"
                    },
                    {
                        "type": "recorded",
                        "title": "Real-Time Visual Analysis of High-Volume Social Media Posts",
                        "contributors": [
                            "Johannes Knittel",
                            "Steffen Koch",
                            "Tan Tang",
                            "Wei Chen",
                            "Yingcai Wu",
                            "Shixia Liu",
                            "Thomas Ertl"
                        ],
                        "abstract": "Breaking news and first-hand reports often trend on social media platforms before traditional news outlets cover them. The real-time analysis of posts on such platforms can reveal valuable and timely insights for journalists, politicians, business analysts, and first responders, but the high number and diversity of new posts pose a challenge. In this work, we present an interactive system that enables the visual analysis of streaming social media data on a large scale in real-time. We propose an efficient and explainable dynamic clustering algorithm that powers a continuously updated visualization of the current thematic landscape as well as detailed visual summaries of specific topics of interest. Our parallel clustering strategy provides an adaptive stream with a digestible but diverse selection of recent posts related to relevant topics. We also integrate familiar visual metaphors that are highly interlinked for enabling both explorative and more focused monitoring tasks. Analysts can gradually increase the resolution to dive deeper into particular topics. In contrast to previous work, our system also works with non-geolocated posts and avoids extensive preprocessing such as detecting events. We evaluated our dynamic clustering algorithm and discuss several use cases that show the utility of our system.",
                        "time_start": "2021-10-28T14:15:00Z",
                        "time_end": "2021-10-28T14:30:00Z",
                        "uid": "v-full-1607",
                        "youtube_video_id": "y8rA9mvohl8"
                    }
                ]
            },
            {
                "title": "Studies and Evaluation Methodology",
                "session_id": "v-full-full17",
                "track": "room2",
                "schedule_image": "v-full-full17.png",
                "chair": [
                    "Cindy Xiong"
                ],
                "organizers": [],
                "time_start": "2021-10-28T13:00:00Z",
                "time_end": "2021-10-28T14:30:00Z",
                "discord_category": "",
                "discord_channel": "",
                "discord_channel_id": 0,
                "youtube_url": null,
                "zoom_meeting": "",
                "zoom_password": "",
                "ff_link": "",
                "ff_playlist": "",
                "time_slots": [
                    {
                        "type": "recorded",
                        "title": "CoUX: Collaborative Visual Analysis of Think-Aloud Usability Test Videos for Digital Interfaces",
                        "contributors": [
                            "Ehsan Jahangirzadeh Soure",
                            "Emily Kuang",
                            "Mingming Fan",
                            "Jian Zhao"
                        ],
                        "abstract": "Reviewing a think-aloud video is both time-consuming and demanding as it requires UX (user experience) professionals to attend to many behavioral signals of the user in the video. Moreover, challenges arise when multiple UX professionals need to collaborate to reduce bias and errors. We propose a collaborative visual analytics tool, CoUX, to facilitate UX evaluators collectively reviewing think-aloud usability test videos of digital interfaces. CoUX seamlessly supports usability problem identification, annotation, and discussion in an integrated environment. To ease the discovery of usability problems, CoUX visualizes a set of problem-indicators based on acoustic, textual, and visual features extracted from the video and audio of a think-aloud session with machine learning.CoUX further enables collaboration amongst UX evaluators for logging, commenting, and consolidating the discovered problems with a chatbox-like user interface. We designed CoUX based on a formative study with two UX experts and insights derived from the literature. We conducted a user study with six pairs of UX practitioners on collaborative think-aloud video analysis tasks. The results indicate that CoUX is useful and effective in facilitating both problem identification and collaborative teamwork. We provide insights into how different features of CoUX were used to support both independent analysis and collaboration. Furthermore, our work highlights opportunities to improve collaborative usability test video analysis.",
                        "time_start": "2021-10-28T13:00:00Z",
                        "time_end": "2021-10-28T13:15:00Z",
                        "uid": "v-full-1209",
                        "youtube_video_id": "Kig_fDLBO-E"
                    },
                    {
                        "type": "recorded",
                        "title": "Professional Differences: A Comparative Study of Visualization Task Performance and Spatial Ability Across Disciplines",
                        "contributors": [
                            "Kyle Hall",
                            "Anthony Kouroupis",
                            "Anastasia Bezerianos",
                            "Danielle Albers Szafir",
                            "Christopher Collins"
                        ],
                        "abstract": "Problem-driven visualization work is rooted in deeply understanding the data, actors, processes, and workflows of a target domain. However, an individual's personality traits and cognitive abilities may also influence visualization use. Diverse user needs and abilities raise natural questions for specificity in visualization design: Could individuals from different domains exhibit performance differences when using visualizations? Are any systematic variations related to their cognitive abilities? This study bridges domain-specific perspectives on visualization design with those provided by cognition and perception. We measure variations in visualization task performance across chemistry, computer science, and education, and relate these differences to variations in spatial ability. We conducted an online study with over 60 domain experts consisting of tasks related to pie charts, isocontour plots, and 3D scatterplots, and grounded by a well-documented spatial ability test. Task performance (correctness) varied with profession across more complex visualizations (isocontour plots and scatterplots), but not pie charts, a comparatively common visualization. We found that correctness correlates with spatial ability, and the professions differ in terms of spatial ability. These results indicate that domains differ not only in the specifics of their data and tasks, but also in terms of how effectively their constituent members engage with visualizations and their cognitive traits. Analyzing participants' confidence and strategy comments suggests that focusing on performance neglects important nuances, such as differing approaches to engage with even common visualizations and potential skill transference. Our findings offer a fresh perspective on discipline-specific visualization with specific recommendations to help guide visualization design that celebrates the uniqueness of the disciplines and individuals we seek to serve.",
                        "time_start": "2021-10-28T13:15:00Z",
                        "time_end": "2021-10-28T13:30:00Z",
                        "uid": "v-full-1452",
                        "youtube_video_id": "6sX4x3SgcRc"
                    },
                    {
                        "type": "recorded",
                        "title": "Rainbow Dash: Intuitiveness, Interpretability and Memorability of the Rainbow Color Scheme in Visualization",
                        "contributors": [
                            "Izabela Golebiowska",
                            "Arzu \u00c7\u00f6ltekin"
                        ],
                        "abstract": "After demonstrating that rainbow colors are still commonly used in scientific publications, we comparatively evaluate the rainbow and sequential color schemes on choropleth and isarithmic maps in an empirical user study with 544 participants to examine if a) people intuitively associate order for the colors in these schemes, b) they can successfully conduct perceptual and semantic map reading and recall tasks with quantitative data where order may have implicit or explicit importance. We find that there is little to no agreement in ordering of rainbow colors while sequential colors are indeed intuitively ordered by the participants with a strong dark is more bias. Sequential colors facilitate most quantitative map reading tasks better than the rainbow colors, whereas rainbow colors competitively facilitate extracting specific values from a map, and may support hue recall better than sequential. We thus contribute to dark- vs. light is more bias debate, and demonstrate why and when rainbow colors may impair performance, and add further nuance to our understanding of this highly popular, yet highly criticized color scheme.",
                        "time_start": "2021-10-28T13:30:00Z",
                        "time_end": "2021-10-28T13:45:00Z",
                        "uid": "v-tvcg-9249052",
                        "youtube_video_id": "YPOZl9Ejyzo"
                    },
                    {
                        "type": "recorded",
                        "title": "Understanding Data Visualization Design Practice",
                        "contributors": [
                            "Paul Parsons"
                        ],
                        "abstract": "Professional roles for data visualization designers are growing in popularity, and interest in relationships between the academic research and professional practice communities is gaining traction. However, despite the potential for knowledge sharing between these communities, we have little understanding of the ways in which practitioners design in real-world, professional settings. Inquiry in numerous design disciplines indicates that practitioners approach complex situations in ways that are fundamentally different from those of researchers. In this work, I take a practice-led approach to understanding design practice on its own terms. Twenty data visualization practitioners were interviewed and asked about their design process, including the steps they take, how they make decisions, and the methods they use. Findings suggest that practitioners do not follow highly systematic processes, but instead rely on situated forms of knowing and acting in which they draw from precedent and use methods and principles that are determined appropriate in the moment. These findings have implications for how visualization researchers understand and engage with practitioners, and how educators approach the training of future data visualization designers.",
                        "time_start": "2021-10-28T13:45:00Z",
                        "time_end": "2021-10-28T14:00:00Z",
                        "uid": "v-full-1683",
                        "youtube_video_id": "fpyr9x7PO_U"
                    },
                    {
                        "type": "recorded",
                        "title": "Learning Objectives, Insights, and Assessments: How Specification Formats Impact Design",
                        "contributors": [
                            "Elsie Lee",
                            "Shiqing He",
                            "Eytan Adar"
                        ],
                        "abstract": "Despite the ubiquity of communicative visualizations, specifying communicative intent during design is ad hoc. Whether we are selecting from a set of visualizations, commissioning someone to produce them, or creating them ourselves, an effective way of specifying intent can help guide this process. Ideally, we would have a concise and shared specification language. In previous work, we have argued that communicative intents can be viewed as a learning/assessment problem (i.e., what should the reader learn and what test should they do well on). Learning-based specification formats are linked (e.g., assessments are derived from objectives) but some may more effectively specify communicative intent. Through a large-scale experiment, we studied three specification types: learning objectives, insights, and assessments. Participants, guided by one of these specifications, rated their preferences for a set of visualization designs. Then, we evaluated the set of visualization designs to assess which specification led participants to prefer the most effective visualizations. We find that while all specification types have benefits over no-specification, each format has its own advantages. Our results show that learning objective-based specifications helped participants the most in visualization selection. We also identify situations in which specifications may be insufficient and assessments are vital.",
                        "time_start": "2021-10-28T14:00:00Z",
                        "time_end": "2021-10-28T14:15:00Z",
                        "uid": "v-full-1334",
                        "youtube_video_id": "_Majf3HA7MU"
                    },
                    {
                        "type": "recorded",
                        "title": "Untidy Data: The Unreasonable Effectiveness of Tables",
                        "contributors": [
                            "Lyn Bartram",
                            "Michael Correll",
                            "Melanie Tory"
                        ],
                        "abstract": "Working with data in table form is usually considered a preparatory and tedious step in the sensemaking pipeline; a way of getting the data ready for more sophisticated visualization and analytical tools. But for many people, spreadsheets \u2014 the quintessential table tool \u2014 remain a critical part of their information ecosystem, allowing them to interact with their data in ways that are hidden or abstracted in more complex tools. This is particularly true for data workers [61], people who work with data as part of their job but do not identify as professional analysts or data scientists. We report on a qualitative study of how these workers interact with and reason about their data. Our findings show that data tables serve a broader purpose beyond data cleanup at the initial stage of a linear analytic flow: users want to see and \u201cget their hands on\u201d the underlying data throughout the analytics process, reshaping and augmenting it to support sensemaking. They reorganize, mark up, layer on levels of detail, and spawn alternatives within the context of the base data. These direct interactions and human-readable table representations form a rich and cognitively important part of building understanding of what the data mean and what they can do with it. We argue that interactive tables are an important visualization idiom in their own right; that the direct data interaction they afford offers a fertile design space for visual analytics; and that sense making can be enriched by more flexible human-data interaction than is currently supported in visual analytics tools.",
                        "time_start": "2021-10-28T14:15:00Z",
                        "time_end": "2021-10-28T14:30:00Z",
                        "uid": "v-full-1224",
                        "youtube_video_id": "Ps8aUXT67us"
                    }
                ]
            },
            {
                "title": "Accessible Visualization and Natural Language",
                "session_id": "v-full-full26",
                "track": "room3",
                "schedule_image": "v-full-full26.png",
                "chair": [
                    "Melanie Tory"
                ],
                "organizers": [],
                "time_start": "2021-10-28T13:00:00Z",
                "time_end": "2021-10-28T14:30:00Z",
                "discord_category": "",
                "discord_channel": "",
                "discord_channel_id": 0,
                "youtube_url": null,
                "zoom_meeting": "",
                "zoom_password": "",
                "ff_link": "",
                "ff_playlist": "",
                "time_slots": [
                    {
                        "type": "recorded",
                        "title": "Accessible Visualization via Natural Language Descriptions: A Four-Level Model of Semantic Content",
                        "contributors": [
                            "Alan Lundgard",
                            "Arvind Satyanarayan"
                        ],
                        "abstract": "Natural language descriptions sometimes accompany visualizations to better communicate and contextualize their insights, and to improve their accessibility for readers with disabilities. However, it is difficult to evaluate the usefulness of these descriptions, and how effectively they improve access to meaningful information, because we have little understanding of the semantic content they convey, and how different readers receive this content. In response, we introduce a conceptual model for the semantic content conveyed by natural language descriptions of visualizations. Developed through a grounded theory analysis of 2,147 sentences, our model spans four levels of semantic content: enumerating visualization construction properties (e.g., marks and encodings); reporting statistical concepts and relations (e.g., extrema and correlations); identifying perceptual and cognitive phenomena (e.g., complex trends and patterns); and elucidating domain-specific insights (e.g., social and political context). To demonstrate how our model can be applied to evaluate the effectiveness of visualization descriptions, we conduct a mixed-methods evaluation with 30 blind and 90 sighted readers, and find that these reader groups differ significantly on which semantic content they rank as most useful. Together, our model and findings suggest that access to meaningful information is strongly reader-specific, and that research in automatic visualization captioning should orient toward descriptions that more richly communicate overall trends and statistics, sensitive to reader preferences. Our work further opens a space of research on natural language as a data interface coequal with visualization.",
                        "time_start": "2021-10-28T13:00:00Z",
                        "time_end": "2021-10-28T13:15:00Z",
                        "uid": "v-full-1401",
                        "youtube_video_id": "xowu8EOa6NQ"
                    },
                    {
                        "type": "recorded",
                        "title": "Towards Understanding Sensory Substitution for Accessible Visualization: An Interview Study",
                        "contributors": [
                            "Pramod Chundury",
                            "Biswaksen Patnaik",
                            "Yasmin Reyazuddin",
                            "Christine Tang",
                            "Jonathan Lazar",
                            "Niklas Elmqvist"
                        ],
                        "abstract": "For all its potential in supporting data analysis, particularly in exploratory situations, visualization also creates barriers: accessibility for blind and visually impaired individuals. Regardless of how effective a visualization is, providing equal access for blind users requires a paradigm shift for the visualization research community. To enact such a shift, it is not sufficient to treat visualization accessibility as merely another technical problem to overcome. Instead, supporting the millions of blind and visually impaired users around the world who have equally valid needs for data analysis as sighted individuals requires a respectful, equitable, and holistic approach that includes all users from the onset. In this paper, we draw on accessibility research methodologies to make inroads towards such an approach. We first identify the people who have specific insight into how blind people perceive the world: orientation and mobility (O&M) experts, who are instructors that teach blind individuals how to navigate the physical world using non-visual senses. We interview 10 O&M experts\u2014all of them blind\u2014to understand how best to use sensory substitution other than the visual sense for conveying spatial layouts. Finally, we investigate our qualitative findings using thematic analysis. While blind people in general tend to use both sound and touch to understand their surroundings, we focused on auditory affordances and how they can be used to make data visualizations accessible\u2014using sonification and auralization. However, our experts recommended supporting a combination of senses\u2014sound and touch\u2014to make charts accessible as blind individuals may be more familiar with exploring tactile charts. We report results on both sound and touch affordances, and conclude by discussing implications for accessible visualization for blind individuals.",
                        "time_start": "2021-10-28T13:15:00Z",
                        "time_end": "2021-10-28T13:30:00Z",
                        "uid": "v-full-1145",
                        "youtube_video_id": "_ToqOGmN4c8"
                    },
                    {
                        "type": "recorded",
                        "title": "Communicating Visualizations without Visuals: Investigation of Visualization Alternative Text for People with Visual Impairments",
                        "contributors": [
                            "Crescentia Jung",
                            "Shubham Mehta",
                            "Atharva Kulkarni",
                            "Yuhang Zhao",
                            "Yea-Seul Kim"
                        ],
                        "abstract": "Alternative text is critical in communicating graphics to people who are blind or have low vision. Especially for graphics that contain rich information, such as visualizations, poorly written or an absence of alternative texts can worsen the information access inequality for people with visual impairments. In this work, we consolidate existing guidelines and survey current practices to inspect to what extent current practices and recommendations are aligned. Then, to gain more insight into what people want in visualization alternative texts, we interviewed 22 people with visual impairments regarding their experience with visualizations and their information needs in alternative texts. The study findings suggest that participants actively try to construct an image of visualizations in their head while listening to alternative texts and wish to carry out visualization tasks (e.g., retrieve specific values) as sighted viewers would. The study also provides ample support for the need to reference the underlying data instead of visual elements to reduce users\u2019 cognitive burden. Informed by the study, we provide a set of recommendations to compose an informative alternative text.",
                        "time_start": "2021-10-28T13:30:00Z",
                        "time_end": "2021-10-28T13:45:00Z",
                        "uid": "v-full-1500",
                        "youtube_video_id": "MsWuHvtXvPg"
                    },
                    {
                        "type": "recorded",
                        "title": "Natural Language to Visualization by Neural Machine Translation",
                        "contributors": [
                            "Yuyu Luo",
                            "Nan Tang",
                            "Guoliang Li",
                            "Jiawei Tang",
                            "Chengliang Chai",
                            "Xuedi Qin"
                        ],
                        "abstract": "Supporting the translation from natural language (NL) to visualization (NL2VIS) can simplify the creation of data visualizations, because if successful, anyone can generate visualizations by their natural language. The state-of-the-art NL2VIS approaches (e.g., NL4DV and FlowSense) are based on semantic parsers and heuristic algorithms, which are not end-to-end and are not designed for supporting (possibly) complex data transformations. Deep neural network powered neural machine translation models have made great strides in many machine translation tasks, which suggests that they might be viable for NL2VIS as well. In this paper, we present ncNet, a Transformer-based sequence-to-sequence model for supporting NL2VIS, with several novel visualization-aware optimizations, including using attention-forcing to optimize the learning process, and visualization-aware rendering to produce better visualization result. To enhance the capability of machine to comprehend natural language queries, ncNet is also designed to take an optional chart template (e.g., a pie chart or a scatter plot) as input, where the chart template will be served as a constraint to limit what could be visualized. We conducted both quantitative evaluation and user study, showing that neural machine translation techniques are easy-to-use and achieve good accuracy that is comparable with the state-of-the-art NL2SQL result.",
                        "time_start": "2021-10-28T13:45:00Z",
                        "time_end": "2021-10-28T14:00:00Z",
                        "uid": "v-full-1419",
                        "youtube_video_id": "Q93fmf1JsrA"
                    },
                    {
                        "type": "recorded",
                        "title": "GenNI: Human-AI Collaboration for Data-Backed Text Generation",
                        "contributors": [
                            "Hendrik Strobelt",
                            "Jambay Kinley",
                            "Robert Kr\u00fcger",
                            "Johanna Beyer",
                            "Hanspeter Pfister",
                            "Alexander Rush"
                        ],
                        "abstract": "Table2Text systems generate textual output based on structured data utilizing machine learning. These systems are essential for fluent natural language interfaces in tools such as virtual assistants; however, left to generate freely these ML systems often produce misleading or unexpected outputs. GenNI (Generation Negotiation Interface) is an interactive visual system for high-level human-AI collaboration in producing descriptive text. The tool utilizes a deep learning model designed with explicit control states. These controls allow users to globally constrain model generations, without sacrificing the representation power of the deep learning models. The visual interface makes it possible for users to interact with AI systems following a Refine-Forecast paradigm to ensure that the generation system acts in a manner human users find suitable. We report multiple use cases on two experiments that improve over uncontrolled generation approaches, while at the same time providing fine-grained control.",
                        "time_start": "2021-10-28T14:00:00Z",
                        "time_end": "2021-10-28T14:15:00Z",
                        "uid": "v-full-1490",
                        "youtube_video_id": "RE-_AA8Oj-s"
                    },
                    {
                        "type": "recorded",
                        "title": "Words of Estimative Correlation: Studying Verbalizations of Scatterplots",
                        "contributors": [
                            "Rafael Henkin",
                            "Cagatay Turkay"
                        ],
                        "abstract": "Natural language and visualization are being increasingly deployed together for supporting data analysis in different ways, from multimodal interaction to enriched data summaries and insights. Yet, researchers still lack systematic knowledge on how viewers verbalize their interpretations of visualizations, and how they interpret verbalizations of visualizations in such contexts. We describe two studies aimed at identifying characteristics of data and charts that are relevant in such tasks. The first study asks participants to verbalize what they see in scatterplots that depict various levels of orrelations. The second study then asks participants to choose visualizations that match a given verbal description of correlation. We extract key concepts from responses, organize them in a taxonomy and analyze the categorized responses. We observe that participants use a wide range of vocabulary across all scatterplots, but particular concepts are preferred for higher levels of correlation. A comparison between the studies reveals the ambiguity of some of the concepts. We discuss how the results could inform the design of multimodal representations aligned with the data and analytical tasks, and present a research roadmap to deepen the understanding about visualizations and natural language.",
                        "time_start": "2021-10-28T14:15:00Z",
                        "time_end": "2021-10-28T14:30:00Z",
                        "uid": "v-tvcg-9195155",
                        "youtube_video_id": "Gb6r0Qw3kRw"
                    }
                ]
            },
            {
                "title": "Flow, Topology, and Fields",
                "session_id": "v-full-full8",
                "track": "room4",
                "schedule_image": "v-full-full8.png",
                "chair": [
                    "Gerik Scheuermann"
                ],
                "organizers": [],
                "time_start": "2021-10-28T13:00:00Z",
                "time_end": "2021-10-28T14:30:00Z",
                "discord_category": "",
                "discord_channel": "",
                "discord_channel_id": 0,
                "youtube_url": null,
                "zoom_meeting": "",
                "zoom_password": "",
                "ff_link": "",
                "ff_playlist": "",
                "time_slots": [
                    {
                        "type": "recorded",
                        "title": "FTK: A Simplicial Spacetime Meshing Framework for Robust and Scalable Feature Tracking",
                        "contributors": [
                            "Hanqi Guo",
                            "David Lenz",
                            "Jiayi Xu",
                            "Xin Liang",
                            "Wenbin He",
                            "Iulian Grindeanu",
                            "Han-Wei Shen",
                            "Tom Peterka",
                            "Todd Munson",
                            "Ian Foster"
                        ],
                        "abstract": "We present the Feature Tracking Kit (FTK), a framework that simplifies, scales, and delivers various feature-tracking algorithms for scientific data. The key of FTK is our simplicial spacetime meshing scheme that generalizes both regular and unstructured spatial meshes to spacetime while tessellating spacetime mesh elements into simplices. The benefits of using simplicial spacetime meshes include (1) reducing ambiguity cases for feature extraction and tracking, (2) simplifying the handling of degeneracies using symbolic perturbations, and (3) enabling scalable and parallel processing. The use of simplicial spacetime meshing simplifies and improves the implementation of several feature-tracking algorithms for critical points, quantum vortices, and isosurfaces. As a software framework, FTK provides end users with VTK/ParaView filters, Python bindings, a command line interface, and programming interfaces for feature-tracking applications. We demonstrate use cases as well as scalability studies through both synthetic data and scientific applications including tokamak, fluid dynamics, and superconductivity simulations. We also conduct end-to-end performance studies on the Summit supercomputer. FTK is open sourced under the MIT license: https://github.com/hguo/ftk.",
                        "time_start": "2021-10-28T13:00:00Z",
                        "time_end": "2021-10-28T13:15:00Z",
                        "uid": "v-tvcg-9405464",
                        "youtube_video_id": "UzlrmgOAI-4"
                    },
                    {
                        "type": "recorded",
                        "title": "A Progressive Approach to Scalar Field Topology",
                        "contributors": [
                            "Jules Vidal",
                            "Pierre Guillou",
                            "Julien Tierny"
                        ],
                        "abstract": "This paper introduces progressive algorithms for the topological analysis of scalar data. Our approach is based on a hierarchical representation of the input data and the fast identification of topologically invariant vertices, which are vertices that have no impact on the topological description of the data and for which we show that no computation is required as they are introduced in the hierarchy. This enables the definition of efficient coarse-to-fine topological algorithms, which leverage fast update mechanisms for ordinary vertices and avoid computation for the topologically invariant ones. We demonstrate our approach with two examples of topological algorithms (critical point extraction and persistence diagram computation), which generate interpretable outputs upon interruption requests and which progressively refine them otherwise. Experiments on real-life datasets illustrate that our progressive strategy, in addition to the continuous visual feedback it provides, even improves run time performance with regard to non-progressive algorithms and we describe further accelerations with shared-memory parallelism. We illustrate the utility of our approach in batch-mode and interactive setups, where it respectively enables the control of the execution time of complete topological pipelines as well as previews of the topological features found in a dataset, with progressive updates delivered within interactive times.",
                        "time_start": "2021-10-28T13:15:00Z",
                        "time_end": "2021-10-28T13:30:00Z",
                        "uid": "v-tvcg-9359504",
                        "youtube_video_id": "ZpawX_VkxRQ"
                    },
                    {
                        "type": "recorded",
                        "title": "Unordered Task-Parallel Augmented Merge Tree Construction",
                        "contributors": [
                            "Kilian Werner",
                            "Christoph Garth"
                        ],
                        "abstract": "Contemporary scientific data sets require fast and scalable topological analysis to enable visualization, simplification and interaction. Within this field, parallel merge tree construction has seen abundant recent contributions, with a trend of decentralized, task-parallel or SMP-oriented algorithms dominating in terms of total runtime. However, none of these recent approaches computed complete merge trees on distributed systems, leaving this field to traditional divide and conquer approaches. This paper introduces a scalable, parallel and distributed algorithm for merge tree construction outperforming the previously fastest distributed solution by a factor of around three. This is achieved by a task-parallel identification of individual merge tree arcs by growing regions around critical points in the data, without any need for ordered progression or global data structures, based on a novel insight introducing a sufficient local boundary for region growth.",
                        "time_start": "2021-10-28T13:30:00Z",
                        "time_end": "2021-10-28T13:45:00Z",
                        "uid": "v-tvcg-9420248",
                        "youtube_video_id": "Bkic1KfZyNc"
                    },
                    {
                        "type": "recorded",
                        "title": "Wasserstein Distances, Geodesics and Barycenters of Merge Trees",
                        "contributors": [
                            "Mathieu Pont",
                            "Jules Vidal",
                            "Julie Delon",
                            "Julien Tierny"
                        ],
                        "abstract": "This paper presents a unified computational framework for the estimation of distances, geodesics and barycenters of merge trees. We extend recent work on the edit distance and introduce a new metric, called the Wasserstein distance between merge trees, which is purposely designed to enable efficient computations of geodesics and barycenters. Specifically, our new distance is strictly equivalent to the L 2 -Wasserstein distance between extremum persistence diagrams, but it is restricted to a smaller solution space, namely, the space of rooted partial isomorphisms between branch decomposition trees. This enables a simple extension of existing optimization frameworks for geodesics and barycenters from persistence diagrams to merge trees. We introduce a task-based algorithm which can be generically applied to distance, geodesic, barycenter or cluster computation. The task-based nature of our approach enables further accelerations with shared-memory parallelism. Extensive experiments on public ensembles and SciVis contest benchmarks demonstrate the efficiency of our approach \u2013 with barycenter computations in the orders of minutes for the largest examples \u2013 as well as its qualitative ability to generate representative barycenter merge trees, visually summarizing the features of interest found in the ensemble. We show the utility of our contributions with dedicated visualization applications: feature tracking, temporal reduction and ensemble clustering. We provide a lightweight C++ implementation that can be used to reproduce our results.",
                        "time_start": "2021-10-28T13:45:00Z",
                        "time_end": "2021-10-28T14:00:00Z",
                        "uid": "v-full-1163",
                        "youtube_video_id": "M5fUzDpm3kY"
                    },
                    {
                        "type": "recorded",
                        "title": "Optimization and Augmentation for Data Parallel Contour Trees",
                        "contributors": [
                            "Hamish Carr",
                            "Oliver R\u00fcbel",
                            "Gunther Weber",
                            "James Ahrens"
                        ],
                        "abstract": "Contour trees are used for topological data analysis in scientific visualization. While originally computed with serial algorithms, recent work has introduced a vector-parallel algorithm. However, this algorithm is relatively slow for fully augmented contour trees which are needed for many practical data analysis tasks. We therefore introduce a representation called the hyperstructure that enables efficient searches through the contour tree and use it to construct a fully augmented contour tree in data parallel, with performance on average 6 times faster than the state-of-the-art parallel algorithm in the TTK topological toolkit.",
                        "time_start": "2021-10-28T14:00:00Z",
                        "time_end": "2021-10-28T14:15:00Z",
                        "uid": "v-tvcg-9372897",
                        "youtube_video_id": "PRQqgvf8QpQ"
                    },
                    {
                        "type": "recorded",
                        "title": "A Domain-Oblivious Approach for Learning Concise Representations of Filtered Topological Spaces for Clustering",
                        "contributors": [
                            "Yu Qin",
                            "Brittany Terese Fasy",
                            "Carola Wenk",
                            "Brian Summa"
                        ],
                        "abstract": "Persistence diagrams have been widely used to quantify the underlying features of filtered topological spaces in data visualization. In many applications, computing distances between diagrams is essential; however, computing these distances has been challenging due to the computational cost. In this paper, we propose a persistence diagram hashing framework that learns a binary code representation of persistence diagrams, which allows for fast computation of distances. This framework is built upon a generative adversarial network (GAN) with a diagram distance loss function to steer the learning process. Instead of using standard representations, we hash diagrams into binary codes, which have natural advantages in large-scale tasks. The training of this model is domain-oblivious in that it can be computed purely from synthetic, randomly created diagrams. As a consequence, our proposed method is directly applicable to various datasets without the need for retraining the model. These binary codes, when compared using fast Hamming distance, better maintain topological similarity properties between datasets than other vectorized representations. To evaluate this method, we apply our framework to the problem of diagram clustering and we compare the quality and performance of our approach to the state-of-the-art. In addition, we show the scalability of our approach on a dataset with 10k persistence diagrams, which is not possible with current techniques. Moreover, our experimental results demonstrate that our method is significantly faster with the potential of less memory usage, while retaining comparable or better quality comparisons.",
                        "time_start": "2021-10-28T14:15:00Z",
                        "time_end": "2021-10-28T14:30:00Z",
                        "uid": "v-full-1011",
                        "youtube_video_id": "Bk6v_P-dnkM"
                    }
                ]
            },
            {
                "title": "Grammar and Learning",
                "session_id": "v-full-full4",
                "track": "room1",
                "schedule_image": "v-full-full4.png",
                "chair": [
                    "Dominik Moritz"
                ],
                "organizers": [],
                "time_start": "2021-10-28T15:00:00Z",
                "time_end": "2021-10-28T16:30:00Z",
                "discord_category": "",
                "discord_channel": "",
                "discord_channel_id": 0,
                "youtube_url": null,
                "zoom_meeting": "",
                "zoom_password": "",
                "ff_link": "",
                "ff_playlist": "",
                "time_slots": [
                    {
                        "type": "recorded",
                        "title": "ChartSeer: Interactive Steering Exploratory Visual Analysis with Machine Intelligence",
                        "contributors": [
                            "Jian Zhao",
                            "Mingming Fan",
                            "Mi Feng"
                        ],
                        "abstract": "During exploratory visual analysis (EVA), analysts need to continually determine which subsequent activities to perform, such as which data variables to explore or how to present data variables visually. Due to the vast combinations of data variables and visual encodings that are possible, it is often challenging to make such decisions. Further, while performing local explorations, analysts often fail to attend to the holistic picture that is emerging from their analysis, leading them to improperly steer their EVA. These issues become even more impactful in the real world analysis scenarios where EVA occurs in multiple asynchronous sessions that could be completed by one or more analysts. To address these challenges, this work proposes ChartSeer, a system that uses machine intelligence to enable analysts to visually monitor the current state of an EVA and effectively identify future activities to perform. ChartSeer utilizes deep learning techniques to characterize analyst-created data charts to generate visual summaries and recommend appropriate charts for further exploration based on user interactions. A case study was first conducted to demonstrate the usage of ChartSeer in practice, followed by a controlled study to compare ChartSeer\u2019s performance with a baseline during EVA tasks. The results demonstrated that ChartSeer enables analysts to adequately understand current EVA status and advance their analysis by creating charts with increased coverage and visual encoding diversity.",
                        "time_start": "2021-10-28T15:00:00Z",
                        "time_end": "2021-10-28T15:15:00Z",
                        "uid": "v-tvcg-9174891",
                        "youtube_video_id": "UcJLVrFO_Fw"
                    },
                    {
                        "type": "recorded",
                        "title": "Net2Vis - A Visual Grammar for Automatically Generating Publication-Tailored CNN Architecture Visualizations",
                        "contributors": [
                            "Alex B\u00e4uerle",
                            "Christian van Onzenoodt",
                            "Timo Ropinski"
                        ],
                        "abstract": "To convey neural network architectures in publications, appropriate visualizations are of great importance. While most current deep learning papers contain such visualizations, these are usually handcrafted just before publication, which results in a lack of a common visual grammar, significant time investment, errors, and ambiguities. Current automatic network visualization tools focus on debugging the network itself and are not ideal for generating publication visualizations. Therefore, we present an approach to automate this process by translating network architectures specified in Keras into visualizations that can directly be embedded into any publication. To do so, we propose a visual grammar for convolutional neural networks (CNNs), which has been derived from an analysis of such figures extracted from all ICCV and CVPR papers published between 2013 and 2019. The proposed grammar incorporates visual encoding, network layout, layer aggregation, and legend generation. We have further realized our approach in an online system available to the community, which we have evaluated through expert feedback, and a quantitative study. It not only reduces the time needed to generate network visualizations for publications, but also enables a unified and unambiguous visualization design.",
                        "time_start": "2021-10-28T15:15:00Z",
                        "time_end": "2021-10-28T15:30:00Z",
                        "uid": "v-tvcg-9350177",
                        "youtube_video_id": "opdm9kja07M"
                    },
                    {
                        "type": "recorded",
                        "title": "Gosling: A Grammar-based Toolkit for Scalable and Interactive Genomics Data Visualization",
                        "contributors": [
                            "Sehi L'Yi",
                            "Qianwen Wang",
                            "Fritz Lekschas",
                            "Nils Gehlenborg"
                        ],
                        "abstract": "The combination of diverse data types and analysis tasks in genomics has resulted in the development of a wide range of visualization techniques and tools. However, most existing tools are tailored to a specific problem or data type and offer limited customization, making it challenging to optimize visualizations for new analysis tasks or datasets. To address this challenge, we designed Gosling\u2014a grammar for interactive and scalable genomics data visualization. Gosling balances expressiveness for comprehensive multi-scale genomics data visualizations with accessibility for domain scientists. Our accompanying JavaScript toolkit called Gosling.js provides scalable and interactive rendering. Gosling.js is built on top of an existing platform for web-based genomics data visualization to further simplify the visualization of common genomics data formats. We demonstrate the expressiveness of the grammar through a variety of real-world examples. Furthermore, we show how Gosling supports the design of novel genomics visualizations. An online editor and examples of Gosling.js, its source code, and documentation are available at https://gosling.js.org.",
                        "time_start": "2021-10-28T15:30:00Z",
                        "time_end": "2021-10-28T15:45:00Z",
                        "uid": "v-full-1274",
                        "youtube_video_id": "HR7iwVelxdg"
                    },
                    {
                        "type": "recorded",
                        "title": "THALIS: Human-Machine Analysis of Longitudinal Symptoms in Cancer Therapy",
                        "contributors": [
                            "Carla Floricel",
                            "Md Nafiul Nipu",
                            "Mikayla Biggs",
                            "Andrew Wentzel",
                            "Guadalupe Canahuate",
                            "Lisanne van Dijk",
                            "Abdallah Mohamed",
                            "Clifton David Fuller",
                            "G. Elisabeta Marai"
                        ],
                        "abstract": "Although cancer patients survive years after oncologic therapy, they are plagued with long-lasting or permanent residual symptoms, whose severity, rate of development, and resolution after treatment vary largely between survivors. The analysis and interpretation of symptoms is complicated by their partial co-occurrence, variability across populations and across time, and, in the case of cancers that use radiation, by further symptom dependency on the prescribed therapy. We describe an integrated environment for visual analysis and knowledge discovery from cancer therapy symptom data, developed in close collaboration with oncology experts. Our visual environment leverages unsupervised machine learning methodology over cohorts of patients, and, in conjunction with custom visual encodings and interactions, provides context for new patients based on patients with similar diagnostic features and symptom evolution. We evaluate this environment on data collected from a cohort of head and neck cancer patients. Feedback from our clinician collaborators indicates that this environment supports knowledge discovery beyond the limits of machines or humans alone, and that it serves as a valuable tool in both the clinic and symptom research.",
                        "time_start": "2021-10-28T15:45:00Z",
                        "time_end": "2021-10-28T16:00:00Z",
                        "uid": "v-full-1504",
                        "youtube_video_id": "4g7vN1BbigY"
                    },
                    {
                        "type": "recorded",
                        "title": "MultiVision: Designing Analytical Dashboards with Deep Learning Based Recommendation",
                        "contributors": [
                            "Aoyu Wu",
                            "Yun Wang",
                            "Mengyu Zhou",
                            "Xinyi He",
                            "Haidong Zhang",
                            "Huamin Qu",
                            "Dongmei Zhang"
                        ],
                        "abstract": "We contribute a deep-learning-based method that assists in designing analytical dashboards for analyzing a data table.\n  Given a data table, data workers usually need to experience a tedious and time-consuming process to select meaningful combinations of data columns for creating charts. This process is further complicated by the needs of creating dashboards composed of multiple views that unveil different perspectives of data. Existing automated approaches for recommending multiple-view visualizations mainly build on manually crafted design rules, producing sub-optimal or irrelevant suggestions. To address this gap, we present a deep learning approach for selecting data columns and recommending multiple charts. More importantly, we integrate the deep learning models into a mixed-initiative system. Our model could make recommendations given optional user-input selections of data columns. The model, in turn, learns from provenance data of authoring logs in an offline manner. We compare our deep learning model with existing methods for visualization recommendation and conduct a user study to evaluate the usefulness of the system.",
                        "time_start": "2021-10-28T16:00:00Z",
                        "time_end": "2021-10-28T16:15:00Z",
                        "uid": "v-full-1035",
                        "youtube_video_id": "AIravg7CZ0Q"
                    },
                    {
                        "type": "recorded",
                        "title": "Interactive Graph Construction for Graph-Based Semi-Supervised Learning",
                        "contributors": [
                            "Changjian Chen",
                            "Zhaowei Wang",
                            "Jing Wu",
                            "Xiting Wang",
                            "Lan-Zhe Guo",
                            "Yu-Feng Li",
                            "Shixia Liu"
                        ],
                        "abstract": "Semi-supervised learning (SSL) provides a way to improve the performance of prediction models (e.g., classifier) via the usage of unlabeled samples. An effective and widely used method is to construct a graph that describes the relationship between labeled and unlabeled samples. Practical experience indicates that graph quality significantly affects the model performance. In this paper, we present a visual analysis method that interactively constructs a high-quality graph for better model performance. In particular, we propose an interactive graph construction method based on the large margin principle. We have developed a river visualization and a hybrid visualization that combines a scatterplot, a node-link diagram, and a bar chart, to convey the label propagation of graph-based SSL. Based on the understanding of the propagation, a user can select regions of interest to inspect and modify the graph. We conducted two case studies to showcase how our method facilitates the exploitation of labeled and unlabeled samples for improving model performance.",
                        "time_start": "2021-10-28T16:15:00Z",
                        "time_end": "2021-10-28T16:30:00Z",
                        "uid": "v-tvcg-9444198",
                        "youtube_video_id": "oTjDUQKPzyI"
                    }
                ]
            },
            {
                "title": "Autonomous Driving, Urban, and Spatiotemporal Data",
                "session_id": "v-full-full25",
                "track": "room2",
                "schedule_image": "v-full-full25.png",
                "chair": [
                    "Catagay Turkay"
                ],
                "organizers": [],
                "time_start": "2021-10-28T15:00:00Z",
                "time_end": "2021-10-28T16:30:00Z",
                "discord_category": "",
                "discord_channel": "",
                "discord_channel_id": 0,
                "youtube_url": null,
                "zoom_meeting": "",
                "zoom_password": "",
                "ff_link": "",
                "ff_playlist": "",
                "time_slots": [
                    {
                        "type": "recorded",
                        "title": "Geo-Context Aware Study of Vision-Based Autonomous Driving Models and Spatial Video Data",
                        "contributors": [
                            "Suphanut Jamonnak",
                            "Ye Zhao",
                            "Xinyi Huang",
                            "Md Amiruzzaman"
                        ],
                        "abstract": "Vision-based deep learning (DL) methods have achieved success in learning autonomous driving models from large scale crowd-sourced video datasets. They are trained to predict instantaneous driving behaviors from video data captured by on-vehicle cameras. In this paper, we develop a geo-context aware visualization system for the study of Autonomous Driving Model (ADM) predictions together with large scale ADM video data. The visual study is seamlessly integrated with the geographical environment by combining DL model performance with geospatial visualization techniques. Model performance measures can be studied together with a set of geo-spatial attributes over map views. Users can also discover and compare prediction behaviors of multiple DL models in both city-wide and street-level analysis, together with road images and video contents. Therefore, the system provides a new visual exploration platform for DL model designers in autonomous driving. Use cases and domain expert evaluation show the utility and effectiveness of the visualization system.",
                        "time_start": "2021-10-28T15:00:00Z",
                        "time_end": "2021-10-28T15:15:00Z",
                        "uid": "v-full-1524",
                        "youtube_video_id": "G5gNiw6Sjck"
                    },
                    {
                        "type": "recorded",
                        "title": "Visual Evaluation for Autonomous Driving",
                        "contributors": [
                            "Yijie Hou",
                            "Chengshun Wang",
                            "Junhong Wang",
                            "Xiangyang Xue",
                            "Xiaolong Zhang",
                            "Jun Zhu",
                            "Dongliang Wang",
                            "Siming Chen"
                        ],
                        "abstract": "Autonomous driving technologies often use state-of-the-art artificial intelligence algorithms to understand the relationship between the vehicle and the external environment, to predict the changes of the environment, and then to plan and control the behaviors of the vehicle accordingly. The complexity of such technologies makes it challenging to evaluate the performance of autonomous driving systems and to find ways to improve them. The current approaches to evaluating such autonomous driving systems largely use a single score to indicate the overall performance of a system, but domain experts have difficulties in understanding how individual components or algorithms in an autonomous driving system may contribute to the score. To address this problem, we collaborate with domain experts on autonomous driving algorithms, and propose a visual evaluation method for autonomous driving. Our method considers the data generated in all components during the whole process of autonomous driving, including perception results, planning routes, prediction of obstacles, various controlling parameters, and evaluation of comfort. We develop a visual analytics workflow to integrate an evaluation mathematical model with adjustable parameters, support the evaluation of the system from the level of the overall performance to the level of detailed measures of individual components, and to show both evaluation scores and their contributing factors. Our implemented visual analytics system provides an overview evaluation score at the beginning and shows the animation of the dynamic change of the scores at each period. Experts can interactively explore the specific component at different time periods and identify related factors. With our method, domain experts not only learn about the performance of an autonomous driving system, but also identify and access the problematic parts of each component. Our visual evaluation system can be applied to the autonomous driving simulation system and used for various evaluation cases. The results of using our system in some simulation cases and the feedback from involved domain experts confirm the usefulness and efficiency of our method in helping people gain in-depth insight into autonomous driving systems.",
                        "time_start": "2021-10-28T15:15:00Z",
                        "time_end": "2021-10-28T15:30:00Z",
                        "uid": "v-full-1480",
                        "youtube_video_id": "BCXDGSMAxoM"
                    },
                    {
                        "type": "recorded",
                        "title": "Where Can We Help? A Visual Analytics Approach to Diagnosing and Improving Semantic Segmentation of Movable Objects",
                        "contributors": [
                            "Wenbin He",
                            "Lincan Zou",
                            "Shekar Arvind Kumar",
                            "Liang Gou",
                            "Liu Ren"
                        ],
                        "abstract": "Semantic segmentation is a critical component in autonomous driving and has to be thoroughly evaluated due to safety concerns. Deep neural network (DNN) based semantic segmentation models are widely used in autonomous driving. However, it is challenging to evaluate DNN-based models due to their black-box-like nature, and it is even more difficult to assess model performance for crucial objects, such as lost cargos and pedestrians, in autonomous driving applications. In this work, we propose VASS, a Visual Analytics approach to diagnosing and improving the accuracy and robustness of Semantic Segmentation models, especially for critical objects moving in various driving scenes. The key component of our approach is a context-aware spatial representation learning that extracts important spatial information of objects, such as position, size, and aspect ratio, with respect to given scene contexts. Based on this spatial representation, we first use it to create visual summarization to analyze models\u2019 performance. We then use it to guide the generation of adversarial examples to evaluate models\u2019 spatial robustness and obtain actionable insights. We demonstrate the effectiveness of VASS via two case studies of lost cargo detection and pedestrian detection in autonomous driving. For both cases, we show quantitative evaluation on the improvement of models\u2019 performance with actionable insights obtained from VASS.",
                        "time_start": "2021-10-28T15:30:00Z",
                        "time_end": "2021-10-28T15:45:00Z",
                        "uid": "v-full-1517",
                        "youtube_video_id": "cqWH1xDz-rE"
                    },
                    {
                        "type": "recorded",
                        "title": "Compass: Towards Better Causal Analysis of Urban Time Series",
                        "contributors": [
                            "Zikun Deng",
                            "Di Weng",
                            "Xiao Xie",
                            "Jie Bao",
                            "Yu Zheng",
                            "Mingliang Xu",
                            "Wei Chen",
                            "Yingcai Wu"
                        ],
                        "abstract": "The spatial time series generated by city sensors allow us to observe urban phenomena like environmental pollution and traffic congestion at an unprecedented scale. However, recovering causal relations from these observations to explain the sources of urban phenomena remains a challenging task because these causal relations tend to be time-varying and demand proper time series partitioning for effective analyses. The prior approaches extract one causal graph given long-time observations, which cannot be directly applied to capturing, interpreting, and validating dynamic urban causality. This paper presents Compass, a novel visual analytics approach for in-depth analyses of the dynamic causality in urban time series. To develop Compass, we identify and address three challenges: detecting urban causality, interpreting dynamic causal relations, and unveiling suspicious causal relations. First, multiple causal graphs over time among urban time series are obtained with a causal detection framework extended from the Granger causality test. Then, a dynamic causal graph visualization is designed to reveal the time-varying causal relations across these causal graphs and facilitate the exploration of the graphs along the time. Finally, a tailored multi-dimensional visualization is developed to support the identification of spurious causal relations, thereby improving the reliability of causal analyses. The effectiveness of Compass is evaluated with two case studies conducted on the real-world urban datasets, including the air pollution and traffic speed datasets, and positive feedback was received from domain experts.",
                        "time_start": "2021-10-28T15:45:00Z",
                        "time_end": "2021-10-28T16:00:00Z",
                        "uid": "v-full-1181",
                        "youtube_video_id": "t64WxezpMrY"
                    },
                    {
                        "type": "recorded",
                        "title": "Visual Cascade Analytics of Large-scale Spatiotemporal Data",
                        "contributors": [
                            "Zikun Deng",
                            "Di Weng",
                            "Yuxuan Liang",
                            "Jie Bao",
                            "Yu Zheng",
                            "Tobias Schreck",
                            "Mingliang Xu",
                            "Yingcai Wu"
                        ],
                        "abstract": "Many spatiotemporal events can be viewed as contagions. These events implicitly propagate across space and time by following cascading patterns, expanding their influence, and generating event cascades that involve multiple locations. Analyzing such cascading processes presents valuable implications in various urban applications, such as traffic planning and pollution diagnostics. Motivated by the limited capability of the existing approaches in mining and interpreting cascading patterns, we propose a visual analytics system called VisCas. VisCas combines an inference model with interactive visualizations and empowers analysts to infer and interpret the latent cascading patterns in the spatiotemporal context. To develop VisCas, we address three major challenges, 1) generalized pattern inference, 2) implicit influence visualization, and 3) multifaceted cascade analysis. For the first challenge, we adapt the state-of-the-art cascading network inference technique to general urban scenarios, where cascading patterns can be reliably inferred from large-scale spatiotemporal data. For the second and third challenges, we assemble a set of effective visualizations to support location navigation, influence inspection, and cascading exploration, and facilitate the in-depth cascade analysis. We design a novel influence view based on a three-fold optimization strategy for analyzing the implicit influences of the inferred patterns. We demonstrate the capability and effectiveness of VisCas with two case studies conducted on real-world traffic congestion and air pollution datasets with domain experts.",
                        "time_start": "2021-10-28T16:00:00Z",
                        "time_end": "2021-10-28T16:15:00Z",
                        "uid": "v-tvcg-9397369",
                        "youtube_video_id": "CbPp2xvQkVQ"
                    },
                    {
                        "type": "recorded",
                        "title": "DDLVis: Real-time Visual Query of Spatiotemporal Data Distribution via Density Dictionary Learning",
                        "contributors": [
                            "Chenhui Li",
                            "George Baciu",
                            "Yunzhe WANG",
                            "Junjie Chen",
                            "Changbo Wang"
                        ],
                        "abstract": "Visual query of spatiotemporal data is becoming an increasingly important function in visual analytics applications. Various works have been presented for querying large spatiotemporal data in real time. However, the real-time query of spatiotemporal data distribution is still an open challenge. As spatiotemporal data become larger, methods of aggregation, storage and querying become critical. We propose a new visual query system that creates a low-memory storage component and provides real-time visual interactions of spatiotemporal data. We first present a peak-based kernel density estimation method to produce the data distribution for the spatiotemporal data. Then a novel density dictionary learning approach is proposed to compress temporal density maps and accelerate the query calculation. Moreover, various intuitive query interactions are presented to interactively gain patterns. The experimental results obtained on three datasets demonstrate that the presented system offers an effective query for visual analytics of spatiotemporal data.",
                        "time_start": "2021-10-28T16:15:00Z",
                        "time_end": "2021-10-28T16:30:00Z",
                        "uid": "v-full-1055",
                        "youtube_video_id": "V-7tq-ER8j4"
                    }
                ]
            },
            {
                "title": "Scalability and Rendering",
                "session_id": "v-full-full15",
                "track": "room3",
                "schedule_image": "v-full-full15.png",
                "chair": [
                    "Hanqi Guo"
                ],
                "organizers": [],
                "time_start": "2021-10-28T15:00:00Z",
                "time_end": "2021-10-28T16:30:00Z",
                "discord_category": "",
                "discord_channel": "",
                "discord_channel_id": 0,
                "youtube_url": null,
                "zoom_meeting": "",
                "zoom_password": "",
                "ff_link": "",
                "ff_playlist": "",
                "time_slots": [
                    {
                        "type": "recorded",
                        "title": "Interactive Focus+Context Rendering for Hexahedral Mesh Inspection",
                        "contributors": [
                            "Christoph Neuhauser",
                            "Junpeng Wang",
                            "R\u00fcdiger Westermann"
                        ],
                        "abstract": "The visual inspection of a hexahedral mesh with respect to element quality is difficult due to clutter and occlusions that are produced when rendering all element faces or their edges simultaneously. Current approaches overcome this problem by using focus on specific elements that are then rendered opaque, and carving away all elements occluding their view. In this work, we make use of advanced GPU shader functionality to generate a focus+context rendering that highlights the elements in a selected region and simultaneously conveys the global mesh structure and deformation field. To achieve this, we propose a gradual transition from edge-based focus rendering to volumetric context rendering, by combining fragment shader-based edge and face rendering with per-pixel fragment lists. A fragment shader smoothly transitions between wireframe and face-based rendering, including focus-dependent rendering style and depth-dependent edge thickness and halos, and per-pixel fragment lists are used to blend fragments in correct visibility order. To maintain the global mesh structure in the context regions, we propose a new method to construct a sheet-based level-of-detail hierarchy and smoothly blend it with volumetric information. The user guides the exploration process by moving a lens-like hotspot. Since all operations are performed on the GPU, interactive frame rates are achieved even for large meshes.",
                        "time_start": "2021-10-28T15:00:00Z",
                        "time_end": "2021-10-28T15:15:00Z",
                        "uid": "v-tvcg-9409710",
                        "youtube_video_id": "R8LFnExQUJI"
                    },
                    {
                        "type": "recorded",
                        "title": "Differentiable Direct Volume Rendering",
                        "contributors": [
                            "Sebastian Weiss",
                            "R\u00fcdiger Westermann"
                        ],
                        "abstract": "We present a differentiable volume rendering solution that provides differentiability of all continuous parameters of the volume rendering process. This differentiable renderer is used to steer the parameters towards a setting with an optimal solution of a problem-specific objective function. We have tailored the approach to volume rendering by enforcing a constant memory footprint via analytic inversion of the blending functions. This makes it independent of the number of sampling steps through the volume and facilitates the consideration of small-scale changes. The approach forms the basis for automatic optimizations regarding external parameters of the rendering process and the volumetric density field itself. We demonstrate its use for automatic viewpoint selection using differentiable entropy as objective, and for optimizing a transfer function from rendered images of a given volume. Optimization of per-voxel densities is addressed in two different ways: First, we mimic inverse tomography and optimize a 3D density field from images using an absorption model. This simplification enables comparisons with algebraic reconstruction techniques and state-of-the-art differentiable path tracers. Second, we introduce a novel approach for tomographic reconstruction from images using an emission-absorption model with post-shading via an arbitrary transfer function.",
                        "time_start": "2021-10-28T15:15:00Z",
                        "time_end": "2021-10-28T15:30:00Z",
                        "uid": "v-full-1136",
                        "youtube_video_id": "NLGiIs2Ala0"
                    },
                    {
                        "type": "recorded",
                        "title": "Accelerating Unstructured Mesh Point Location with RT Cores",
                        "contributors": [
                            "Nate Morrical",
                            "Ingo Wald",
                            "Will Usher",
                            "Valerio Pascucci"
                        ],
                        "abstract": "We present a technique that leverages ray tracing hardware available in recent Nvidia RTX GPUs to solve a problem other than classical ray tracing. Specifically, we demonstrate how to use these units to accelerate the point location of general unstructured elements consisting of both planar and bilinear faces. This unstructured mesh point location problem has previously been challenging to accelerate on GPU architectures; yet, the performance of these queries is crucial to many unstructured volume rendering and compute applications. Starting with a CUDA reference method, we describe and evaluate three approaches that reformulate these point queries to incrementally map algorithmic complexity to these new hardware ray tracing units. Each variant replaces the simpler problem of point queries with a more complex one of ray queries. Initial variants exploit ray tracing cores for accelerated BVH traversal, and subsequent variants use ray-triangle intersections and per-face metadata to detect point-in-element intersections. Although these later variants are more algorithmically complex, they are significantly faster than the reference method thanks to hardware acceleration. Using our approach, we improve the performance of an unstructured volume renderer by up to 4\u00d7 for tetrahedral meshes and up to 15\u00d7 for general bilinear element meshes, matching, or out-performing state-of-the-art solutions while simultaneously improving on robustness and ease-of-implementation.",
                        "time_start": "2021-10-28T15:30:00Z",
                        "time_end": "2021-10-28T15:45:00Z",
                        "uid": "v-tvcg-9286513",
                        "youtube_video_id": "xyh4oSlc-oM"
                    },
                    {
                        "type": "recorded",
                        "title": "Probabilistic Occlusion Culling using Confidence Maps for High-Quality Rendering of Large Particle Data",
                        "contributors": [
                            "Mohamed Ibrahim",
                            "Peter Rautek",
                            "Guido Reina",
                            "Marco Agus",
                            "Markus Hadwiger"
                        ],
                        "abstract": "Achieving high rendering quality in the visualization of large particle data, for example from large-scale molecular dynamics simulations, requires a significant amount of sub-pixel super-sampling, due to very high numbers of particles per pixel. Although it is impossible to super-sample all particles of large-scale data at interactive rates, efficient occlusion culling can decouple the overall data size from a high effective sampling rate of visible particles. However, while the latter is essential for domain scientists to be able to see important data features, performing occlusion culling by sampling or sorting the data is usually slow or error-prone due to visibility estimates of insufficient quality. We present a novel probabilistic culling architecture for super-sampled high-quality rendering of large particle data. Occlusion is dynamically determined at the sub-pixel level, without explicit visibility sorting or data simplification. We introduce confidence maps to probabilistically estimate confidence in the visibility data gathered so far. This enables progressive, confidence-based culling, helping to avoid wrong visibility decisions. In this way, we determine particle visibility with high accuracy, although only a small part of the data set is sampled. This enables extensive super-sampling of (partially) visible particles for high rendering quality, at a fraction of the cost of sampling all particles. For real-time performance with millions of particles, we exploit novel features of recent GPU architectures to group particles into two hierarchy levels, combining fine-grained culling with high frame rates.",
                        "time_start": "2021-10-28T15:45:00Z",
                        "time_end": "2021-10-28T16:00:00Z",
                        "uid": "v-full-1586",
                        "youtube_video_id": "ruzqXhWVfrM"
                    },
                    {
                        "type": "recorded",
                        "title": "A Memory Efficient Encoding for Ray Tracing Large Unstructured Data",
                        "contributors": [
                            "Ingo Wald",
                            "Nate Morrical",
                            "Stefan Zellmann"
                        ],
                        "abstract": "In theory, efficient and high-quality rendering of unstructured data should greatly benefit from modern GPUs, but in practice, GPUs are often limited by the large amount of memory that large meshes require for element representation and for sample reconstruction acceleration structures. We describe a memory-optimized encoding for large unstructured meshes that efficiently encodes both the unstructured mesh and corresponding sample reconstruction acceleration structure, while still allowing for fast random-access sampling as required for rendering. We demonstrate that for large data our encoding is more efficient than OSPRay\u2019s and allows for rendering even the 2.9 billion element Mars Lander on a single off-the-shelf GPU--and the largest 6.3 billion version on a pair of such GPUs.",
                        "time_start": "2021-10-28T16:00:00Z",
                        "time_end": "2021-10-28T16:15:00Z",
                        "uid": "v-full-1420",
                        "youtube_video_id": "b6NTLgAib1A"
                    },
                    {
                        "type": "recorded",
                        "title": "Pyramid-based Scatterplots Sampling for Progressive and Streaming Data Visualization",
                        "contributors": [
                            "Xin Chen",
                            "Jian Zhang",
                            "Chi-Wing Fu",
                            "Jean-Daniel Fekete",
                            "Yunhai Wang"
                        ],
                        "abstract": "We present a pyramid-based scatterplot sampling technique to avoid overplotting and enable progressive and streaming\n  visualization of large data. Our technique is based on a multiresolution pyramid-based decomposition of the underlying density map\n  and makes use of the density values in the pyramid to guide the sampling at each scale for preserving the relative data densities\n  and outliers. We show that our technique is competitive in quality with state-of-the-art methods and runs faster by about an order of\n  magnitude. Also, we have adapted it to deliver progressive and streaming data visualization by processing the data in chunks and\n  updating the scatterplot areas with visible changes in the density map. A quantitative evaluation shows that our approach generates\n  stable and faithful progressive samples that are comparable to the state-of-the-art method in preserving relative densities and superior\n  to it in keeping outliers and stability when switching frames. We present two case studies that demonstrate the effectiveness of our\n  approach for exploring large data.",
                        "time_start": "2021-10-28T16:15:00Z",
                        "time_end": "2021-10-28T16:30:00Z",
                        "uid": "v-full-1336",
                        "youtube_video_id": "GWbXjjOfnpY"
                    }
                ]
            },
            {
                "title": "Time Series and Events",
                "session_id": "v-full-full22",
                "track": "room1",
                "schedule_image": "v-full-full22.png",
                "chair": [
                    "Wolfgang Aigner"
                ],
                "organizers": [],
                "time_start": "2021-10-28T17:00:00Z",
                "time_end": "2021-10-28T18:30:00Z",
                "discord_category": "",
                "discord_channel": "",
                "discord_channel_id": 0,
                "youtube_url": null,
                "zoom_meeting": "",
                "zoom_password": "",
                "ff_link": "",
                "ff_playlist": "",
                "time_slots": [
                    {
                        "type": "recorded",
                        "title": "KD-Box: Line-segment-based KD-tree for Interactive Exploration of Large-scale Time-Series Data",
                        "contributors": [
                            "Yue Zhao",
                            "Jian Zhang",
                            "Chi-Wing Fu",
                            "Mingliang Xu",
                            "Dominik Moritz",
                            "Yunhai Wang"
                        ],
                        "abstract": "Time-series data\u2014usually presented in the form of lines\u2014plays an important role in many domains such as finance, meteorology, health, and urban informatics. Yet, little has been done to support interactive exploration of large-scale time-series data, which requires a clutter-free visual representation with low-latency interactions. In this paper, we contribute a novel line-segment-based KD-tree method to enable interactive analysis of many time series. Our method enables not only fast queries over time series in selected regions of interest but also a line splatting method for efficient computation of the density field and selection of representative lines. Further, we develop KD-Box, an interactive system that provides rich interactions, e.g., timebox, attribute filtering, and coordinated multiple views. We demonstrate the effectiveness of KD-Box in supporting efficient line query and density field computation through a quantitative comparison and show its usefulness for interactive visual analysis on several real-world datasets.",
                        "time_start": "2021-10-28T17:00:00Z",
                        "time_end": "2021-10-28T17:15:00Z",
                        "uid": "v-full-1433",
                        "youtube_video_id": "hKJ7ihBYH4Q"
                    },
                    {
                        "type": "recorded",
                        "title": "Sequen-C: A Multilevel Overview of Temporal Event Sequences",
                        "contributors": [
                            "Jessica Magallanes",
                            "Tony Stone",
                            "Paul Morris",
                            "Suzanne Mason",
                            "Steven Wood",
                            "Maria-Cruz Villa-Uriol"
                        ],
                        "abstract": "Building a visual overview of temporal event sequences with an optimal level-of-detail (i.e. simplified but informative) is an ongoing challenge - expecting the user to zoom into every important aspect of the overview can lead to missing insights. We propose a technique to build and explore a multilevel overview of event sequences, from coarse to fine vertical or horizontal level-of-detail, using hierarchical aggregation and a novel cluster data representation Align-Score-Simplify. By default, the overview shows an optimal number of sequence clusters obtained through the average silhouette width metric \u2013 then users are able to explore alternative optimal sequence clusterings. The vertical level-of-detail of the overview changes along with the number of clusters, whilst the horizontal level-of-detail refers to the level of summarization applied to each cluster representation. The proposed technique has been implemented into a visualization system called Sequence Cluster Explorer (Sequen-C) that allows multilevel and detail-on-demand exploration through three coordinated views, and the inspection of data attributes at cluster, unique sequence, and individual sequence level. We present two case studies using real-world datasets in the healthcare domain: CUREd and MIMIC-III, which demonstrate how the technique can aid users in exploring and defining a set of distinct pathways that best summarize the dataset, while also being able of identifying deviating pathways and exploring data attributes for selected patterns.",
                        "time_start": "2021-10-28T17:15:00Z",
                        "time_end": "2021-10-28T17:30:00Z",
                        "uid": "v-full-1116",
                        "youtube_video_id": "MIBxUq7eZ0M"
                    },
                    {
                        "type": "recorded",
                        "title": "EVis: Visually Analyzing Environmentally Driven Events",
                        "contributors": [
                            "Tinghao Feng",
                            "Jing Yang",
                            "Martha-Cary Eppes",
                            "Zhaocong Yang",
                            "Faye Moser"
                        ],
                        "abstract": "Earth scientists are increasingly employing time series data with multiple dimensions and high temporal resolution to study the impacts of climate and environmental changes on Earth\u2019s atmosphere, biosphere, hydrosphere, and lithosphere. However, the large number of variables and varying time scales of antecedent conditions contributing to natural phenomena hinder scientists from completing more than the most basic analyses. In this paper, we present EVis (Environmental Visualization), a new visual analytics prototype to help scientists analyze and explore recurring environmental events (e.g. rock fracture, landslides, heat waves, floods) and their relationships with high dimensional time series of continuous numeric environmental variables, such as ambient temperature and precipitation. EVis provides coordinated scatterplots, heatmaps, histograms, and RadViz for foundational analyses. These features allow users to interactively examine relationships between events and one, two, three, or more environmental variables. EVis also provides a novel visual analytics approach to allowing users to discover temporally lagging relationships related to antecedent conditions between events and multiple variables, a critical task in Earth sciences. In particular, this latter approach projects multivariate time series onto trajectories in a 2D space using RadViz, and clusters the trajectories for temporal pattern discovery. Our case studies with rock cracking data and interviews with domain experts from a range of sub-disciplines within Earth sciences illustrate the extensive applicability and usefulness of EVis.",
                        "time_start": "2021-10-28T17:30:00Z",
                        "time_end": "2021-10-28T17:45:00Z",
                        "uid": "v-full-1287",
                        "youtube_video_id": "YXJeaAoYF4M"
                    },
                    {
                        "type": "recorded",
                        "title": "Interactive Visual Exploration of Longitudinal Historical Career Mobility Data",
                        "contributors": [
                            "Yifang Wang",
                            "Hongye Liang",
                            "Xinhuan Shu",
                            "Jiachen Wang",
                            "Ke Xu",
                            "Zikun Deng",
                            "Cameron Campbell",
                            "Bijia Chen",
                            "Yingcai Wu",
                            "Huamin Qu"
                        ],
                        "abstract": "The increased availability of quantitative historical datasets has provided new research opportunities for multiple disciplines in social science. In this paper, we work closely with the constructors of a new dataset, CGED-Q (China Government Employee Database-Qing), that records the career trajectories of over 340,000 government officials in the Qing bureaucracy in China from 1760 to 1912. We use these data to study career mobility from a historical perspective and understand social mobility and inequality. However, existing statistical approaches are inadequate for analyzing career mobility in this historical dataset with its fine-grained attributes and long time span, since they are mostly hypothesis-driven and require substantial effort. We propose CareerLens, an interactive visual analytics system for assisting experts in exploring, understanding, and reasoning from historical career data. With CareerLens, experts examine mobility patterns in three levels-of-detail, namely, the macro-level providing a summary of overall mobility, the meso-level extracting latent group mobility patterns, and the micro-level revealing social relationships of individuals. We demonstrate the effectiveness and usability of CareerLens through two case studies and receive encouraging feedback from follow-up interviews with domain experts",
                        "time_start": "2021-10-28T17:45:00Z",
                        "time_end": "2021-10-28T18:00:00Z",
                        "uid": "v-tvcg-9382844",
                        "youtube_video_id": "ArOIzF8zrog"
                    },
                    {
                        "type": "recorded",
                        "title": "HisVA: a Visual Analytics System for Learning History",
                        "contributors": [
                            "Dongyun Han",
                            "Gorakh Parsad",
                            "Hwiyeon Kim",
                            "Jaekyom Shim",
                            "Oh-Sang Kwon",
                            "Kyung Son",
                            "Jooyoung Lee",
                            "Isaac Cho",
                            "Sungahn Ko"
                        ],
                        "abstract": "Studying history involves many difficult tasks. Examples include searching for proper data in a large event space, understanding stories of historical events by time and space, and finding relationships among events that may not be apparent. Instructors who extensively use well-organized and well-argued materials (e.g., textbooks and online resources) can lead students to a narrow perspective in understanding history and prevent spontaneous investigation of historical events, with the students asking their own questions. In this work, we proposed HisVA, a visual analytics system that allows the efficient exploration of historical events from Wikipedia using three views: event, map, and resource. HisVA provides an effective event exploration space, where users can investigate relationships among historical events by reviewing and linking them in terms of space and time. To evaluate our system, we present two usage scenarios, a user study with a qualitative analysis of user exploration strategies, and in-class deployment results",
                        "time_start": "2021-10-28T18:00:00Z",
                        "time_end": "2021-10-28T18:15:00Z",
                        "uid": "v-tvcg-9447222",
                        "youtube_video_id": "BLZTWWHwam0"
                    },
                    {
                        "type": "recorded",
                        "title": "SSR-TVD: Spatial Super-Resolution for Time-Varying Data Analysis and Visualization",
                        "contributors": [
                            "Jun Han",
                            "Chaoli Wang"
                        ],
                        "abstract": "We present SSR-TVD, a novel deep learning framework that produces coherent spatial super-resolution (SSR) of time-varying data (TVD) using adversarial learning. In scientific visualization, SSR-TVD is the first work that applies the generative adversarial network (GAN) to generate high-resolution volumes for three-dimensional time-varying data sets. The design of SSR-TVD includes a generator and two discriminators (spatial and temporal discriminators). The generator takes a low-resolution volume as input and outputs a synthesized high-resolution volume. To capture spatial and temporal coherence in the volume sequence, the two discriminators take the synthesized high-resolution volume(s) as input and produce a score indicating the realness of the volume(s). Our method can work in the in situ visualization setting by downscaling volumetric data from selected time steps as the simulation runs and upscaling downsampled volumes to their original resolution during postprocessing. To demonstrate the effectiveness of SSR-TVD, we show quantitative and qualitative results with several time-varying data sets of different characteristics and compare our method against volume upscaling using bicubic interpolation and a solution solely based on CNN.",
                        "time_start": "2021-10-28T18:15:00Z",
                        "time_end": "2021-10-28T18:30:00Z",
                        "uid": "v-tvcg-9229162",
                        "youtube_video_id": "qwUrR4Dzwag"
                    }
                ]
            },
            {
                "title": "Mitigating Bias",
                "session_id": "v-full-full24",
                "track": "room3",
                "schedule_image": "v-full-full24.png",
                "chair": [
                    "Evanthia Dimara"
                ],
                "organizers": [],
                "time_start": "2021-10-28T17:00:00Z",
                "time_end": "2021-10-28T18:30:00Z",
                "discord_category": "",
                "discord_channel": "",
                "discord_channel_id": 0,
                "youtube_url": null,
                "zoom_meeting": "",
                "zoom_password": "",
                "ff_link": "",
                "ff_playlist": "",
                "time_slots": [
                    {
                        "type": "recorded",
                        "title": "Left, Right, and Gender: Exploring Interaction Traces to Mitigate Human Biases",
                        "contributors": [
                            "Emily Wall",
                            "Arpit Narechania",
                            "Adam Coscia",
                            "Jamal Paden",
                            "Alex Endert"
                        ],
                        "abstract": "Human biases impact the way people analyze data and make decisions. Recent work has shown that some visualization designs can better support cognitive processes and mitigate cognitive biases (i.e., errors that occur due to the use of mental \u201cshortcuts\u201d). In this work, we explore how visualizing a user\u2019s interaction history (i.e., which data points a user has interacted with) can be used to mitigate potential biases that drive decision making by promoting conscious reflection of one\u2019s analysis process. Given an interactive scatterplot-based visualization tool, we showed interaction history in real-time while exploring data (e.g., by coloring points in the scatterplot that the user has interacted with), and in a summative format after a decision has been made (e.g., by comparing the distribution of user interactions to the underlying distribution of the data). We conducted a series of in-lab experiments and a crowdsourced experiment to evaluate the effectiveness of interaction history interventions toward mitigating bias. We contextualized this work in a political scenario in which participants were instructed to choose a committee of 10 fictitious politicians to review a recent bill passed in the U.S. state of Georgia banning abortion after 6 weeks, where e.g. gender bias or political party bias may drive one\u2019s analysis process. We demonstrate the generalizability of this approach by evaluating a second decision making scenario related to movies. Our results are inconclusive for the effectiveness of interaction history (henceforth referred to as interaction traces) toward mitigating biased decision making. However, we find some mixed support that interaction traces, particularly in a summative format, can increase awareness of potential unconscious biases.",
                        "time_start": "2021-10-28T17:00:00Z",
                        "time_end": "2021-10-28T17:15:00Z",
                        "uid": "v-full-1148",
                        "youtube_video_id": "BScEMdLnskI"
                    },
                    {
                        "type": "recorded",
                        "title": "VisQA: X-raying Vision and Language Reasoning in Transformers",
                        "contributors": [
                            "Theo Jaunet",
                            "Corentin Kervadec",
                            "Romain Vuillemot",
                            "Grigory Antipov",
                            "Moez Baccouche",
                            "Christian Wolf"
                        ],
                        "abstract": "Visual Question Answering systems target answering open-ended textual questions given input images. They are a testbed for learning high-level reasoning with a primary use in HCI, for instance assistance for the visually impaired. Recent research has shown that state-of-the-art models tend to produce answers exploiting biases and shortcuts in the training data, and sometimes do not even look at the input image, instead of performing the required reasoning steps. We present VisQA, a visual analytics tool that explores this question of reasoning vs. bias exploitation. It exposes the key element of state-of-the-art neural models --- attention maps in transformers. Our working hypothesis is that reasoning steps leading to model predictions are observable from attention distributions, which are particularly useful for visualization. The design process of VisQA was motivated by well-known bias examples from the fields of deep learning and vision-language reasoning and evaluated in two ways. First, as a result of a collaboration of three fields, machine learning, vision and language reasoning, and data analytics, the work lead to a better understanding of bias exploitation of neural models for VQA, which eventually resulted in an impact on its design and training through the proposition of a method for the transfer of reasoning patterns from an oracle model. Second, we also report on the design of VisQA, and a goal-oriented evaluation of VisQA targeting the analysis of a model decision process from multiple experts, providing evidence that it makes the inner workings of models accessible to users.",
                        "time_start": "2021-10-28T17:15:00Z",
                        "time_end": "2021-10-28T17:30:00Z",
                        "uid": "v-full-1583",
                        "youtube_video_id": "EZoPzXpc8O4"
                    },
                    {
                        "type": "recorded",
                        "title": "The Weighted Average Illusion: Biases in Perceived Mean Position in Scatterplots",
                        "contributors": [
                            "Matt-Heun Hong",
                            "Jessica Witt",
                            "Danielle Albers Szafir"
                        ],
                        "abstract": "Scatterplots can encode a third dimension by using additional channels like size or color (e.g. bubble charts). We explore a potential misinterpretation of trivariate scatterplots, which we call the weighted average illusion, where locations of larger and darker points are given more weight toward x- and y-mean estimates. This systematic bias is sensitive to a designer\u2019s choice of size or lightness ranges mapped onto the data. In this paper, we quantify this bias against varying size/lightness ranges and data correlations. We discuss possible explanations for its cause by measuring attention given to individual data points using a vision science technique called the centroid method. Our work illustrates how ensemble processing mechanisms and mental shortcuts can significantly distort visual summaries of data, and can lead to misjudgments like the demonstrated weighted average illusion.",
                        "time_start": "2021-10-28T17:30:00Z",
                        "time_end": "2021-10-28T17:45:00Z",
                        "uid": "v-full-1730",
                        "youtube_video_id": "XlFlVhIC7kc"
                    },
                    {
                        "type": "recorded",
                        "title": "Impact of Cognitive Biases on Progressive Visualization",
                        "contributors": [
                            "Marianne Procopio",
                            "Ab Mosca",
                            "Carlos Scheidegger",
                            "Eugene Wu",
                            "Remco Chang"
                        ],
                        "abstract": "Progressive visualization is fast becoming a technique in the visualization community to help users interact with large amounts of data. With progressive visualization, users can examine intermediate results of complex or long running computations, without waiting for the computation to complete. While this has shown to be beneficial to users, recent research has identified potential risks. For example, users may misjudge the uncertainty in the intermediate results and draw incorrect conclusions or see patterns that are not present in the final results. In this paper, we conduct a comprehensive set of studies to quantify the advantages and limitations of progressive visualization. Based on a recent report by Micallef et al., we examine four types of cognitive biases that can occur with progressive visualization: uncertainty bias, illusion bias, control bias, and anchoring bias. The results of the studies suggest a cautious but promising use of progressive visualization \u2014 while there can be significant savings in task completion time, accuracy can be negatively affected in certain conditions. These findings confirm earlier reports of the benefits and drawbacks of progressive visualization and that continued research into mitigating the effects of cognitive biases is necessary.",
                        "time_start": "2021-10-28T17:45:00Z",
                        "time_end": "2021-10-28T18:00:00Z",
                        "uid": "v-tvcg-9320596",
                        "youtube_video_id": "lFr0-IvunAo"
                    },
                    {
                        "type": "recorded",
                        "title": "Improving Visualization Interpretation Using Counterfactuals",
                        "contributors": [
                            "Smiti Kaul",
                            "David Borland",
                            "Nan Cao",
                            "David Gotz"
                        ],
                        "abstract": "Complex, high-dimensional data is used in a wide range of domains to explore problems and make decisions. Analysis of high-dimensional data, however, is vulnerable to the hidden influence of confounding variables, especially as users apply ad hoc filtering operations to visualize only specific subsets of an entire dataset. Thus, visual data-driven analysis can mislead users and encourage mistaken assumptions about causality or the strength of relationships between features. This work introduces a novel visual approach designed to reveal the presence of confounding variables via counterfactual possibilities during visual data analysis. It is implemented in CoFact, an interactive visualization prototype that determines and visualizes counterfactual subsets to better support user exploration of feature relationships. Using publicly available datasets, we conducted a controlled user study to demonstrate the effectiveness of our approach; the results indicate that users exposed to counterfactual visualizations formed more careful judgments about feature-to-outcome relationships.",
                        "time_start": "2021-10-28T18:00:00Z",
                        "time_end": "2021-10-28T18:15:00Z",
                        "uid": "v-full-1010",
                        "youtube_video_id": "iiCIBmXO53I"
                    },
                    {
                        "type": "recorded",
                        "title": "Lumos: Increasing Awareness of Analytic Behavior during Visual Data Analysis",
                        "contributors": [
                            "Arpit Narechania",
                            "Adam Coscia",
                            "Emily Wall",
                            "Alex Endert"
                        ],
                        "abstract": "Visual data analysis tools provide people with the agency and flexibility to explore data using a variety of interactive functionality. However, this flexibility may introduce potential consequences in situations where users unknowingly overemphasize or underemphasize specific subsets of the data or attribute space they are analyzing. For example, users may overemphasize specific attributes and/or their values (e.g., Gender is always encoded on the X axis), underemphasize others (e.g., Religion is never encoded), ignore a subset of the data (e.g., older people are filtered out), etc. In response, we present Lumos, a visual data analysis tool that captures and shows the interaction history with data to increase awareness of such analytic behaviors. Using in-situ (at the place of interaction) and ex-situ (in an external view) visualization techniques, Lumos provides real-time feedback to users for them to reflect on their activities. For example, Lumos highlights datapoints that have been previously examined in the same visualization (in-situ) and also overlays them on the underlying data distribution (i.e., baseline distribution) in a separate visualization (ex-situ). Through a user study with 24 participants, we investigate how Lumos helps users' data exploration and decision-making processes. We found that Lumos increases users' awareness of visual data analysis practices in real-time, promoting reflection upon and acknowledgement of their intentions and potentially influencing subsequent interactions.",
                        "time_start": "2021-10-28T18:15:00Z",
                        "time_end": "2021-10-28T18:30:00Z",
                        "uid": "v-full-1017",
                        "youtube_video_id": "G_pOu4OGtPI"
                    }
                ]
            },
            {
                "title": "Graphs and Trees",
                "session_id": "v-full-full9",
                "track": "room2",
                "schedule_image": "v-full-full9.png",
                "chair": [
                    "Jean-Daniel Fekete"
                ],
                "organizers": [],
                "time_start": "2021-10-28T17:00:00Z",
                "time_end": "2021-10-28T18:30:00Z",
                "discord_category": "",
                "discord_channel": "",
                "discord_channel_id": 0,
                "youtube_url": null,
                "zoom_meeting": "",
                "zoom_password": "",
                "ff_link": "",
                "ff_playlist": "",
                "time_slots": [
                    {
                        "type": "recorded",
                        "title": "Multi-level Area Balancing of Clustered Graphs",
                        "contributors": [
                            "Hsiang-Yun Wu",
                            "Martin N\u00f6llenburg",
                            "Ivan Viola"
                        ],
                        "abstract": "We present a multi-level area balancing technique for laying out clustered graphs to facilitate a comprehensive understanding of the complex relationships that exist in various fields, such as life sciences and sociology. Clustered graphs are often used to model relationships that are accompanied by attribute-based grouping information. Such information is essential for robust data analysis, such as for the study of biological taxonomies or educational backgrounds. Hence, the ability to smartly arrange textual labels and packing graphs within a certain screen space is therefore desired to successfully convey the attribute data . Here we propose to hierarchically partition the input screen space using Voronoi tessellations in multiple levels of detail. In our method, the position of textual labels is guided by the blending of constrained forces and the forces derived from centroidal Voronoi cells. The proposed algorithm considers three main factors: (1) area balancing, (2) schematized space partitioning, and (3) hairball management. We primarily focus on area balancing, which aims to allocate a uniform area for each textual label in the diagram. We achieve this by first untangling a general graph to a clustered graph through textual label duplication, and then coupling with spanning-tree-like visual integration. We illustrate the feasibility of our approach with examples and then evaluate our method by comparing it with well-known conventional approaches and collecting feedback from domain experts.",
                        "time_start": "2021-10-28T17:00:00Z",
                        "time_end": "2021-10-28T17:15:00Z",
                        "uid": "v-tvcg-9262073",
                        "youtube_video_id": "k_r4I6K04Bs"
                    },
                    {
                        "type": "recorded",
                        "title": "Understanding Missing Links in Bipartite Networks with MissBiN",
                        "contributors": [
                            "Jian Zhao",
                            "Maoyuan Sun",
                            "Francine Chen",
                            "Patrick Chiu"
                        ],
                        "abstract": "The analysis of bipartite networks is critical in a variety of application domains, such as exploring entity co-occurrences in intelligence analysis and investigating gene expression in bio-informatics. One important task is missing link prediction, which infers the existence of unseen links based on currently observed ones. In this paper, we propose a visual analysis system, MissBiN, to involve analysts in the loop for making sense of link prediction results. MissBiN equips a novel method for link prediction in a bipartite network by leveraging the information of bi-cliques in the network. It also provides an interactive visualization for understanding the algorithm outputs. The design of MissBiN is based on three high-level analysis questions (what, why, and how) regarding missing links, which are distilled from the literature and expert interviews. We conducted quantitative experiments to assess the performance of the proposed link prediction algorithm, and interviewed two experts from different domains to demonstrate the effectiveness of MissBiN as a whole. We also provide a comprehensive usage scenario to illustrate the usefulness of the tool in an application of intelligence analysis.",
                        "time_start": "2021-10-28T17:15:00Z",
                        "time_end": "2021-10-28T17:30:00Z",
                        "uid": "v-tvcg-9237128",
                        "youtube_video_id": "VUmNLkXqHuM"
                    },
                    {
                        "type": "recorded",
                        "title": "A State-of-the-Art Survey of Tasks for Tree Design and Evaluation with a Curated Task Dataset",
                        "contributors": [
                            "Aditeya Pandey",
                            "Uzma Syeda",
                            "Chaitya Shah",
                            "John Gomez",
                            "Michelle Borkin"
                        ],
                        "abstract": "In the field of information visualization, the concept of ``tasks'' is an essential component of theories and methodologies for how a visualization researcher or a practitioner understands what tasks a user needs to perform and how to approach the creation of a new design. In this paper, we focus on the collection of tasks for tree visualizations, a common visual encoding in many domains ranging from biology to computer science to geography. In spite of their commonality, no prior efforts exist to collect and abstractly define tree visualization tasks. We present a literature review of tree visualization papers and generate a curated dataset of over 200 tasks. To enable effective task abstraction for trees, we also contribute a novel extension of the Multi-Level Task Typology to include more specificity to support tree-specific tasks as well as a systematic procedure to conduct task abstractions for tree visualizations. All tasks in the dataset were abstracted with the novel typology extension and analyzed to gain a better understanding of the state of tree visualizations. These abstracted tasks can benefit visualization researchers and practitioners as they design evaluation studies or compare their analytical tasks with ones previously studied in the literature to make informed decisions about their design. We also reflect on our novel methodology and advocate more broadly for the creation of task-based knowledge repositories for different types of visualizations. The Supplemental Material will be maintained on OSF: https://osf.io/u5eh",
                        "time_start": "2021-10-28T17:30:00Z",
                        "time_end": "2021-10-28T17:45:00Z",
                        "uid": "v-tvcg-9371413",
                        "youtube_video_id": "58nS66krWlA"
                    },
                    {
                        "type": "recorded",
                        "title": "Edge-Path Bundling: A Less Ambiguous Edge Bundling Approach",
                        "contributors": [
                            "Markus Wallinger",
                            "Daniel Archambault",
                            "David Auber",
                            "Martin N\u00f6llenburg",
                            "Jaakko Peltonen"
                        ],
                        "abstract": "Edge bundling techniques cluster edges with similar attributes (i.e. similarity in direction and proximity) together to reduce the visual clutter. All edge bundling techniques to date implicitly or explicitly cluster groups of individual edges, or parts of them, together based on these attributes. These clusters can result in ambiguous connections that do not exist in the data. Confluent drawings of networks do not have these ambiguities, but require the layout to be computed as part of the bundling process. We devise a new bundling method, edge-path bundling, to simplify edge clutter while greatly reducing ambiguities compared to previous bundling techniques. Edge-path bundling takes a layout as input and clusters each edge along a weighted, shortest path to limit its deviation from a straight line. Edge-path bundling does not incur independent edge ambiguities typically seen in all edge bundling methods, and the level of bundling can be tuned through shortest path distances, Euclidean distances, and combinations of the two. Also, directed edge bundling naturally emerges from the model. Through metric evaluations, we demonstrate the advantages of edge-path bundling over other techniques.",
                        "time_start": "2021-10-28T17:45:00Z",
                        "time_end": "2021-10-28T18:00:00Z",
                        "uid": "v-full-1368",
                        "youtube_video_id": "wVRZYFOwWA8"
                    },
                    {
                        "type": "recorded",
                        "title": "STRATISFIMAL LAYOUT: A Modular Optimization Model for Laying Out Layered Node-link Network Visualizations",
                        "contributors": [
                            "Sara Di Bartolomeo",
                            "Mirek Riedewald",
                            "Wolfgang Gatterbauer",
                            "Cody Dunne"
                        ],
                        "abstract": "Node-link visualizations are a familiar and powerful tool for displaying the relationships in a network. The readability of these visualizations highly depends on the spatial layout used for the nodes. In this paper, we focus on computing layered layouts, in which nodes are aligned on a set of parallel axes to better expose hierarchical or sequential relationships. Heuristic-based layouts are widely used as they scale well to larger networks and usually create readable, albeit sub-optimal, visualizations. We instead use a layout optimization model that prioritizes optimality (as compared to scalability) because an optimal solution not only represents the best attainable result, but can also serve as a baseline to evaluate the effectiveness of layout heuristics. We take an important step towards powerful and flexible network visualization by proposing Stratisfimal Layout, a modular integer-linear-programming formulation that can consider several important readability criteria simultaneously: crossing reduction, edge bendiness, and nested and multi-layer groups. The layout can be adapted to diverse use cases through its modularity. Individual features can be enabled and customized depending on the application. We provide open-source and documented implementations of the layout, both for web-based and desktop visualizations. As a proof-of-concept, we apply it to the problem of visualizing complicated SQL queries, which have features that, to the best of our knowledge, cannot be addressed by existing layout optimization models. We also include a benchmark network generator and the results of an empirical evaluation to assess the performance trade-offs of our design choices. A full version of this paper with all appendices, data, and source code is available at https://osf.io/3vqms with live examples at https://visdunneright.github.io/stratisfimal/.",
                        "time_start": "2021-10-28T18:00:00Z",
                        "time_end": "2021-10-28T18:15:00Z",
                        "uid": "v-full-1663",
                        "youtube_video_id": "3FnX6Kc9Qf0"
                    },
                    {
                        "type": "recorded",
                        "title": "Interactive Visual Pattern Search on Graph Data via Graph Representation Learning",
                        "contributors": [
                            "Huan Song",
                            "Zeng Dai",
                            "Panpan Xu",
                            "Liu Ren"
                        ],
                        "abstract": "Graphs are a ubiquitous data structure to model processes and relations in a wide range of domains. Examples include social networks, knowledge graphs, control-flow graphs in programs, and semantic scene graphs in images. Identifying subgraph patterns (or motifs) in graphs is one important approach to understand their structural properties. We propose a visual analytics system, GraphQ to support human-in-the-loop, example-based, subgraph pattern search in a database containing many individual graphs. Our approach goes beyond a predefined set of motifs and allows users to interactively specify the patterns of interest. To support fast, interactive queries, we use graph neural networks (GNNs) to encode the topological and node attributes in a graph as fixed-length\n  latent vector representations, and perform subgraph matching in the latent space. However, due to the complexity of the problem, it is still difficult to obtain accurate one-to-one node correspondence in the matching results, which is crucial for visualization and\n  interpretation. We, therefore, propose a novel GNN for node-alignment called NeuroAlign, to facilitate easy validation and interpretation of the query results. GraphQ provides a visual query interface with a query editor and a multi-scale visualization of the results, as well as a user feedback mechanism for refining the results with additional constraints. We demonstrate GraphQ through two example usage scenarios in different application domains: analyzing reusable subroutines in program workflows and semantic scene graph search in images. Quantitative experiments show that NeuroAlign achieves 19%\u201329% improvement in node-alignment accuracy compared to baseline GNN and provides up to 100x speedup compared to combinatorial algorithms. Our qualitative study with domain experts confirms the effectiveness of GraphQ for both usage scenarios.",
                        "time_start": "2021-10-28T18:15:00Z",
                        "time_end": "2021-10-28T18:30:00Z",
                        "uid": "v-full-1494",
                        "youtube_video_id": "Wmfo-QUhe1E"
                    }
                ]
            },
            {
                "title": "Data-Driven Communication and Storytelling",
                "session_id": "v-full-full23",
                "track": "room1",
                "schedule_image": "v-full-full23.png",
                "chair": [
                    "Zhicheng Liu"
                ],
                "organizers": [],
                "time_start": "2021-10-29T13:00:00Z",
                "time_end": "2021-10-29T14:30:00Z",
                "discord_category": "",
                "discord_channel": "",
                "discord_channel_id": 0,
                "youtube_url": null,
                "zoom_meeting": "",
                "zoom_password": "",
                "ff_link": "",
                "ff_playlist": "",
                "time_slots": [
                    {
                        "type": "recorded",
                        "title": "A Design Space for Applying the Freytag's Pyramid Structure to Data Stories",
                        "contributors": [
                            "Leni Yang",
                            "Xian XU",
                            "Xingyu Lan",
                            "Ziyan Liu",
                            "Shunan Guo",
                            "Yang Shi",
                            "Huamin Qu",
                            "Nan Cao"
                        ],
                        "abstract": "Data stories integrate compelling visual content to communicate data insights in the form of narratives. The narrative structure of a data story serves as the backbone that determines its expressiveness, and it can largely influence how audiences perceive the insights. Freytag's Pyramid is a classic narrative structure that has been widely used in film and literature. While there are continuous recommendations and discussions about applying Freytag's Pyramid to data stories, there is little systematic and practical guidance for data story creators on how to use Freytag's Pyramid for structured data story creation. To bridge this gap, we examined how existing practices apply Freytag's Pyramid through analyzing stories extracted from 103 data videos. Based on our findings, we propose a design space of narrative patterns, data flows, and visual communications to provide practical guidance on achieving narrative intents, organizing data facts, and selecting visual design techniques through the story creation process. We evaluated the proposed design space through a workshop with 25 participants. The results show that our design space provides a clear framework for rapid storyboarding of data stories with Freytag's Pyramid.",
                        "time_start": "2021-10-29T13:00:00Z",
                        "time_end": "2021-10-29T13:15:00Z",
                        "uid": "v-full-1301",
                        "youtube_video_id": "IYfHUomZY_U"
                    },
                    {
                        "type": "recorded",
                        "title": "Kineticharts: Augmenting Affective Expressiveness of Charts in Data Stories with Animation Design",
                        "contributors": [
                            "Xingyu Lan",
                            "Yang Shi",
                            "Yanqiu Wu",
                            "Xiaohan Jiao",
                            "Nan Cao"
                        ],
                        "abstract": "Data stories often seek to elicit affective feelings from viewers. However, how to design affective data stories remains under-explored. In this work, we investigate one specific design factor, animation, and present Kineticharts, an animation design scheme for creating charts that express five positive affects: joy, amusement, surprise, tenderness, and excitement. These five affects were found to be frequently communicated through animation in data stories. Regarding each affect, we designed varied kinetic motions represented by bar charts, line charts, and pie charts, resulting in 60 animated charts for the five affects. We designed Kineticharts by first conducting a need-finding study with professional practitioners from data journalism and then analyzing a corpus of affective motion graphics to identify salient kinetic patterns. We evaluated Kineticharts through two user studies. The results suggest that Kineticharts can accurately convey affects, and improve the expressiveness of data stories, as well as enhance user engagement without hindering data comprehension compared to the animation design from DataClips, an authoring tool for data videos.",
                        "time_start": "2021-10-29T13:15:00Z",
                        "time_end": "2021-10-29T13:30:00Z",
                        "uid": "v-full-1072",
                        "youtube_video_id": "AQFSKJC2ygg"
                    },
                    {
                        "type": "recorded",
                        "title": "Interactive Data Comics",
                        "contributors": [
                            "Zezhong Wang",
                            "Hugo Romat",
                            "Fanny Chevalier",
                            "Nathalie Henry Riche",
                            "Dave Murray-Rust",
                            "Benjamin Bach"
                        ],
                        "abstract": "This paper investigates how to make data comics interactive. Data comics are an effective and versatile means for visual communication, leveraging the power of sequential narration and combined textual and visual content, while providing an overview of the storyline through panels assembled in expressive layouts. While a powerful static storytelling medium that works well on paper support, adding interactivity to data comics can enable non-linear storytelling, personalization, levels of details, explanations, and potentially enriched user experiences. This paper introduces a set of operations tailored to support data comics narrative goals that go beyond the traditional linear, immutable storyline curated by a story author. The goals and operations include adding and removing panels into pre-defined layouts to support branching, change of perspective, or access to detail-on-demand, as well as providing and modifying data, and interacting with data representation, to support personalization and reader-defined data focus. We propose a lightweight specification language, COMICSCRIPT, for designers to add such interactivity to static comics. To assess the viability of our authoring process, we recruited six professional illustrators, designers and data comics enthusiasts and asked them to craft an interactive comic, allowing us to understand authoring workflow and potential of our approach. We present examples of interactive comics in a gallery. This initial step towards understanding the design space of interactive comics can inform the design of creation tools and experiences for interactive storytelling.",
                        "time_start": "2021-10-29T13:30:00Z",
                        "time_end": "2021-10-29T13:45:00Z",
                        "uid": "v-full-1600",
                        "youtube_video_id": "c2j0O2-y1c4"
                    },
                    {
                        "type": "recorded",
                        "title": "Declutter and Focus: Empirically Evaluating Design Guidelines for Effective Data Communication",
                        "contributors": [
                            "Kiran Ajani",
                            "Elsie Lee",
                            "Cindy Xiong",
                            "Cole Knaflic",
                            "William Kemper",
                            "Steven Franconeri"
                        ],
                        "abstract": "Data visualization design has a powerful effect on which patterns we see as salient and how quickly we see them. The visualization practitioner community prescribes two popular guidelines for creating clear and efficient visualizations: declutter and focus. The declutter guidelines suggest removing non-critical gridlines, excessive labeling of data values, and color variability to improve aesthetics and to maximize the emphasis on the data relative to the design itself. The focus guidelines for explanatory communication recommend including a clear headline that describes the relevant data pattern, highlighting a subset of relevant data values with a unique color, and connecting those values to written annotations that contextualize them in a broader argument. We evaluated how these recommendations impact recall of the depicted information across cluttered, decluttered, and decluttered+focused designs of six graph topics. Undergraduate students were asked to redraw previously seen visualizations, to recall their topics and main conclusions, and to rate the varied designs on aesthetics, clarity, professionalism, and trustworthiness. Decluttering designs led to higher ratings on professionalism, and adding focus to the design led to higher ratings on aesthetics and clarity. They also showed better memory for the highlighted pattern in the data, as reflected across redrawings of the original visualization and typed free-response conclusions, though we do not know whether these results would generalize beyond our memory-based tasks. The results largely empirically validate the intuitions of visualization designers and practitioners. The stimuli, data, analysis code, and Supplementary Materials are available at https://osf.io/wes9u/.",
                        "time_start": "2021-10-29T13:45:00Z",
                        "time_end": "2021-10-29T14:00:00Z",
                        "uid": "v-tvcg-9385921",
                        "youtube_video_id": "1FfrxSs0IFc"
                    },
                    {
                        "type": "recorded",
                        "title": "Showing Data about People: A Design Space of Anthropographics",
                        "contributors": [
                            "Luiz Morais",
                            "Yvonne Jansen",
                            "Nazareno Andrade",
                            "Pierre Dragicevic"
                        ],
                        "abstract": "When showing data about people, visualization designers and data journalists often use design strategies that presumably help the audience relate to those people. The term anthropographics has been recently coined to refer to this practice and the resulting visualizations. Anthropographics is a rich and growing area, but the work so far has remained scattered. Despite preliminary empirical work and a few web essays written by practitioners, there is a lack of clear language for thinking about and communicating about anthropographics. We address this gap by introducing a conceptual framework and a design space for anthropographics. Our design space consists of seven elementary design dimensions that can be reasonably hypothesized to have some effect on prosocial feelings or behavior. It extends a previous design space and is informed by an analysis of 105 visualizations collected from newspapers, websites, and research papers. We use our conceptual framework and design space to discuss trade-offs, common design strategies, as well as future opportunities for design and research in the area of anthropographics.",
                        "time_start": "2021-10-29T14:00:00Z",
                        "time_end": "2021-10-29T14:15:00Z",
                        "uid": "v-tvcg-9189859",
                        "youtube_video_id": "Qs-QB39UOyQ"
                    },
                    {
                        "type": "recorded",
                        "title": "Visual Arrangements of Bar Charts Influence Comparisons in Viewer Takeaways",
                        "contributors": [
                            "Cindy Xiong",
                            "Vidya Setlur",
                            "Benjamin Bach",
                            "Kylie Lin",
                            "Eunyee Koh",
                            "Steven Franconeri"
                        ],
                        "abstract": "Well-designed data visualizations can lead to more powerful and intuitive processing by a viewer. To help a viewer intuitively compare values to quickly generate key takeaways, visualization designers can manipulate how data values are arranged in a chart to afford particular comparisons. Using simple bar charts as a case study, we empirically tested the comparison affordances of four common arrangements: vertically juxtaposed, horizontally juxtaposed, overlaid, and stacked. We asked participants to type out what patterns they perceived in a chart and we coded their takeaways into types of comparisons. In a second study, we asked data visualization design experts to predict which arrangement they would use to afford each type of comparison and found both alignments and mismatches with our findings. These results provide concrete guidelines for how both human designers and automatic chart recommendation systems can make visualizations that help viewers extract the ``right'' takeaway.",
                        "time_start": "2021-10-29T14:15:00Z",
                        "time_end": "2021-10-29T14:30:00Z",
                        "uid": "v-full-1376",
                        "youtube_video_id": "8tWNeToWXwQ"
                    }
                ]
            },
            {
                "title": "Glyphs and Sets",
                "session_id": "v-full-full11",
                "track": "room2",
                "schedule_image": "v-full-full11.png",
                "chair": [
                    "Johannes Fuchs"
                ],
                "organizers": [],
                "time_start": "2021-10-29T13:00:00Z",
                "time_end": "2021-10-29T14:30:00Z",
                "discord_category": "",
                "discord_channel": "",
                "discord_channel_id": 0,
                "youtube_url": null,
                "zoom_meeting": "",
                "zoom_password": "",
                "ff_link": "",
                "ff_playlist": "",
                "time_slots": [
                    {
                        "type": "recorded",
                        "title": "Generative Design Inspiration for Glyphs with Diatoms",
                        "contributors": [
                            "Matthew Brehmer",
                            "Robert Kosara",
                            "Carmen Hull"
                        ],
                        "abstract": "We introduce Diatoms, a technique that generates design inspiration for glyphs by sampling from palettes of mark shapes, encoding channels, and glyph scaffold shapes. Diatoms allows for a degree of randomness while respecting constraints imposed by columns in a data table: their data types and domains as well as semantic associations between columns as specified by the designer. We pair this generative design process with two forms of interactive design externalization that enable comparison and critique of the design alternatives. First, we incorporate a familiar small multiples configuration in which every data point is drawn according to a single glyph design, coupled with the ability to page between alternative glyph designs. Second, we propose a small permutables design gallery, in which a single data point is drawn according to each alternative glyph design, coupled with the ability to page between data points. We demonstrate an implementation of our technique as an extension to Tableau featuring three example palettes, and to better understand how Diatoms could fit into existing design workflows, we conducted interviews and chauffeured demos with 12 designers. Finally, we reflect on our process and the designers\u2019 reactions, discussing the potential of our technique in the context of visualization authoring systems. Ultimately, our approach to glyph design and comparison can kickstart and inspire visualization design, allowing for the serendipitous discovery of shape and channel combinations that would have otherwise been overlooked.",
                        "time_start": "2021-10-29T13:00:00Z",
                        "time_end": "2021-10-29T13:15:00Z",
                        "uid": "v-full-1064",
                        "youtube_video_id": "GKKuxvUrCf4"
                    },
                    {
                        "type": "recorded",
                        "title": "GlyphCreator: Towards Automatic Generation of Example-based Circular Glyphs",
                        "contributors": [
                            "Lu Ying",
                            "Tan Tang",
                            "Yuzhe Luo",
                            "Lvkesheng Shen",
                            "Xiao Xie",
                            "Lingyun Yu",
                            "Yingcai Wu"
                        ],
                        "abstract": "Circular glyphs are widely used in different fields because of their effectiveness in representing multidimensional data. However, the creation of circular glyphs remains a difficult task due to the demand for professional design skills and laborious design processes. This paper presents an interactive authoring tool called GlyphCreator to support the example-based generation of circular glyphs. Given an example circular glyph and multidimensional input data, GlyphCreator can promptly generate a list of design candidates and supports interactive editing on the candidates to satisfy different design requirements. To develop GlyphCreator, we first derive a design space of circular glyphs from summarizing the relation between different visual elements. With this design space, we build a circular glyph dataset and develop a deep learning model for glyph parsing. The model is able to deconstruct a circular glyph bitmap into a series of visual elements. Next, we propose an interface with effective interactions to help users bind the input data attributes to visual elements and customize visual styles. We evaluate the parsing model through a quantitative experiment and demonstrate the use of GlyphCreator through a usage scenario. The effectiveness of GlyphCreator is confirmed through user interviews.",
                        "time_start": "2021-10-29T13:15:00Z",
                        "time_end": "2021-10-29T13:30:00Z",
                        "uid": "v-full-1590",
                        "youtube_video_id": "kIQT6LjbVOU"
                    },
                    {
                        "type": "recorded",
                        "title": "Shape-driven Coordinate Ordering for Star Glyph Sets via Reinforcement Learning",
                        "contributors": [
                            "Ruizhen Hu",
                            "Bin Chen",
                            "Juzhan Xu",
                            "Oliver van Kaick",
                            "Oliver Deussen",
                            "Hui Huang"
                        ],
                        "abstract": "We present a neural optimization model trained with reinforcement learning to solve the coordinate ordering problem for sets of star glyphs. Given a set of star glyphs associated to multiple class labels, we propose to use shape context descriptors to measure the perceptual distance between pairs of glyphs, and use the derived silhouette coefficient to measure the perception of class separability within the entire set. To find the optimal coordinate order for the given set, we train a neural network using reinforcement learning to reward orderings with high silhouette coefficients. The network consists of an encoder and a decoder with an attention mechanism. The encoder employs a recurrent neural network (RNN) to encode input shape and class information, while the decoder together with the attention mechanism employs another RNN to output a sequence with the new coordinate order. In addition, we introduce a neural network to efficiently estimate the similarity between shape context descriptors, which allows to speed up the computation of silhouette coefficients and thus the training of the axis ordering network. Two user studies demonstrate that the orders provided by our method are preferred by users for perceiving class separation. We tested our model on different settings to show its robustness and generalization abilities and demonstrate that it allows to order input sets with unseen data size, data dimension, or number of classes. We also demonstrate that our model can be adapted to coordinate ordering of other types of plots such as RadViz by replacing the proposed shape-aware silhouette coefficient with the corresponding quality metric to guide network training.",
                        "time_start": "2021-10-29T13:30:00Z",
                        "time_end": "2021-10-29T13:45:00Z",
                        "uid": "v-tvcg-9328215",
                        "youtube_video_id": "_9RsHxjXKTg"
                    },
                    {
                        "type": "recorded",
                        "title": "F2-Bubbles: Faithful Bubble Set Construction and Flexible Editing",
                        "contributors": [
                            "Yunhai Wang",
                            "Da Cheng",
                            "Zhirui Wang",
                            "Jian Zhang",
                            "Liang Zhou",
                            "Gaoqi He",
                            "Oliver Deussen"
                        ],
                        "abstract": "In this paper, we propose F2-Bubbles, a set overlay visualization technique that addresses overlapping artifacts and supports interactive editing with intelligent suggestions. The core of our method is a new, ef\ufb01cient set overlay construction algorithm that approximates the optimal set overlay by considering set elements and their non-set neighbors. Thanks to the ef\ufb01ciency of the algorithm, interactive editing is achieved, and with intelligent suggestions, users can easily and \ufb02exibly edit visualizations through direct manipulations with local adaptations. A quantitative comparison with state-of-the-art set visualization techniques and case studies demonstrate the effectiveness of our method and suggests that F2-Bubbles is a helpful technique for set visualization.",
                        "time_start": "2021-10-29T13:45:00Z",
                        "time_end": "2021-10-29T14:00:00Z",
                        "uid": "v-full-1435",
                        "youtube_video_id": "BYcxY7URmAU"
                    },
                    {
                        "type": "recorded",
                        "title": "spEuler: Semantics-preserving Euler Diagrams",
                        "contributors": [
                            "Rebecca Kehlbeck",
                            "Jochen G\u00f6rtler",
                            "Yunhai Wang",
                            "Oliver Deussen"
                        ],
                        "abstract": "Creating comprehensible visualizations of highly overlapping set-typed data is a challenging task due to its complexity. \n  To facilitate insights into set connectivity and to leverage semantic relations between intersections, we propose a fast two-step layout technique for Euler diagrams that are both well-matched and well-formed. \n  Our method conforms to established form guidelines for Euler diagrams regarding semantics, aesthetics, and readability. \n  First, we establish an initial ordering of the data, which we then use to incrementally create a planar, connected, and monotone dual graph representation. \n  In the next step, the graph is transformed into a circular layout that maintains the semantics and yields simple Euler diagrams with smooth curves. \n  When the data cannot be represented by simple diagrams, our algorithm always falls back to a solution that is not well-formed but still well-matched, whereas previous methods often fail to produce expected results. \n  We show the usefulness of our method for visualizing set-typed data using examples from text analysis and infographics. \n  Furthermore, we discuss the characteristics of our approach and evaluate our method against state-of-the-art methods.",
                        "time_start": "2021-10-29T14:00:00Z",
                        "time_end": "2021-10-29T14:15:00Z",
                        "uid": "v-full-1477",
                        "youtube_video_id": "BUeN3dkBJJc"
                    },
                    {
                        "type": "recorded",
                        "title": "Visualization of 3D Stress Tensor Fields Using Superquadric Glyphs on Displacement Streamlines",
                        "contributors": [
                            "Mohak Patel",
                            "David Laidlaw"
                        ],
                        "abstract": "Stress tensor fields play a central role in solid mechanics studies, but their visualization in 3D space remains challenging as the information-dense multi-variate tensor needs to be sampled in 3D space while avoiding clutter. Taking cues from current tensor visualizations, we adapted glyph-based visualization for stress tensors in 3D space. We also developed a testing framework and performed user studies to evaluate the various glyph-based tensor visualizations for objective accuracy measures, and subjective user feedback for each visualization method. To represent the stress tensor, we color encoded the original superquadric glyph, and in the user study, we compared it to superquadric glyphs developed for second-order symmetric tensors. We found that color encoding improved the user accuracy measures, while the users also rated our method the highest. We compared our method of placing stress tensor glyphs on displacement streamlines to the glyph placement on a 3D grid. In the visualization, we modified the glyph to show both the stress tensor and the displacement vector at each sample point. The participants preferred our method of glyph placement on displacement streamlines as it highlighted the underlying continuous structure in the tensor field.",
                        "time_start": "2021-10-29T14:15:00Z",
                        "time_end": "2021-10-29T14:30:00Z",
                        "uid": "v-tvcg-8967163",
                        "youtube_video_id": "aJUKNLnOc5U"
                    }
                ]
            },
            {
                "title": "Uncertainty",
                "session_id": "v-full-full12",
                "track": "room3",
                "schedule_image": "v-full-full12.png",
                "chair": [
                    "Matthew Kay"
                ],
                "organizers": [],
                "time_start": "2021-10-29T13:00:00Z",
                "time_end": "2021-10-29T14:30:00Z",
                "discord_category": "",
                "discord_channel": "",
                "discord_channel_id": 0,
                "youtube_url": null,
                "zoom_meeting": "",
                "zoom_password": "",
                "ff_link": "",
                "ff_playlist": "",
                "time_slots": [
                    {
                        "type": "recorded",
                        "title": "Visualizing Uncertainty in Probabilistic Graphs with Network Hypothetical Outcome Plots (NetHOPs)",
                        "contributors": [
                            "Dongping Zhang",
                            "Eytan Adar",
                            "Jessica Hullman"
                        ],
                        "abstract": "Probabilistic networks are challenging to visualize using the traditional node-link diagram. Encoding edge probability using visual variables like width or fuzziness makes it dif\ufb01cult for users of static network visualizations to estimate network statistics like densities, isolates, path lengths, or clustering under uncertainty. We introduce Network Hypothetical Outcome Plots (NetHOPs), a visualization technique that animates a sequence of network realizations sampled from a network distribution de\ufb01ned by probabilistic edges. NetHOPs employ an aggregation and anchoring algorithm used in dynamic and longitudinal graph drawing to parameterize layout stability to support uncertainty estimation. We present a community matching algorithm we developed to enable visualizing the uncertainty of cluster membership and community occurrence. We describe the results of a study in which 51 network experts used NetHOPs to complete a set of common visual analysis tasks and reported how they perceived network structures and properties subject to uncertainty. Participants\u2019 estimates fell, on average, within 11% of the ground truth statistics, suggesting NetHOPs can be a reasonable approach for enabling network analysts to reason about multiple properties under uncertainty. Participants appeared able to articulate the distribution of network statistics slightly more accurately when they could manipulate the layout anchoring and the animation speed. Based on these \ufb01ndings, we synthesize design recommendations for developing and using animated network visualizations for probabilistic networks.",
                        "time_start": "2021-10-29T13:00:00Z",
                        "time_end": "2021-10-29T13:15:00Z",
                        "uid": "v-full-1220",
                        "youtube_video_id": "jSxuPN-ARbg"
                    },
                    {
                        "type": "recorded",
                        "title": "Implicit Error, Uncertainty and Confidence in Visualization: an Archaeological Case Study",
                        "contributors": [
                            "Georgia Panagiotidou",
                            "Ralf Vandam",
                            "Jeroen Poblome",
                            "Andrew Moere"
                        ],
                        "abstract": "While we know that the visualization of quantifiable uncertainty impacts the confidence in insights, little is known about whether the same is true for uncertainty that originates from aspects so inherent to the data that they can only be accounted for qualitatively. Being embedded within an archaeological project, we realized how assessing such qualitative uncertainty is crucial in gaining a holistic and accurate understanding of regional spatio-temporal patterns of human settlements over millennia. We therefore investigated the impact of visualizing qualitative implicit errors on the sense-making process via a probe that deliberately represented three distinct implicit errors, i.e. differing collection methods, subjectivity of data interpretations and assumptions on temporal continuity. By analyzing the interactions of 14 archaeologists with different levels of domain expertise, we discovered that novices became more actively aware of typically overlooked data issues and domain experts became more confident of the visualization itself. We observed how participants quoted social factors to alleviate some uncertainty, while in order to minimize it they requested additional contextual breadth or depth of the data. While our visualization did not alleviate all uncertainty, we recognized how it sparked reflective meta-insights regarding methodological directions of the data. We believe our findings inform future visualizations on how to handle the complexity of implicit errors for a range of user typologies and for highly data-critical application domains such as the digital humanities.",
                        "time_start": "2021-10-29T13:15:00Z",
                        "time_end": "2021-10-29T13:30:00Z",
                        "uid": "v-tvcg-9451614",
                        "youtube_video_id": "Rr7tK1STbG4"
                    },
                    {
                        "type": "recorded",
                        "title": "Examining Effort in 1D Uncertainty Communication Using Individual Differences in Working Memory and NASA-TLX",
                        "contributors": [
                            "Spencer Castro",
                            "Helia Hosseinpour",
                            "P. Samuel Quinan",
                            "Lace Padilla"
                        ],
                        "abstract": "As uncertainty visualizations for general audiences become increasingly common, designers must understand the full impact of uncertainty communication techniques on viewers' decision processes. Prior work demonstrates mixed performance outcomes with respect to how individuals make decisions using various visual and textual depictions of uncertainty. Part of the inconsistency across findings may be due to an over-reliance on task accuracy, which cannot, on its own, provide a comprehensive understanding of how uncertainty visualization techniques support reasoning processes. In this work, we advance the debate surrounding the efficacy of modern 1D uncertainty visualizations by conducting converging quantitative and qualitative analyses of both the effort and strategies used by individuals when provided with quantile dotplots, density plots, interval plots, mean plots, and textual descriptions of uncertainty. We utilize two approaches for examining effort across uncertainty communication techniques: a measure of individual differences in working-memory capacity known as an operation span (OSPAN) task and self-reports of perceived workload via the NASA-TLX. The results reveal that both visualization methods and working-memory capacity impact participants' decisions. Specifically, quantile dotplots and density plots (i.e., distributional annotations) result in more accurate judgments than interval plots, textual descriptions of uncertainty, and mean plots (i.e., summary annotations). Additionally, participants' open-ended responses suggest that individuals viewing distributional annotations are more likely to employ a strategy that explicitly incorporates uncertainty into their judgments than those viewing summary annotations. When comparing quantile dotplots to density plots, this work finds that both methods are equally effective for low-working-memory individuals. However, for individuals with high-working-memory capacity, quantile dotplots evoke more accurate responses with less perceived effort. Given these results, we advocate for the inclusion of converging behavioral and subjective workload metrics in addition to accuracy performance to further disambiguate meaningful differences among visualization techniques.",
                        "time_start": "2021-10-29T13:30:00Z",
                        "time_end": "2021-10-29T13:45:00Z",
                        "uid": "v-full-1696",
                        "youtube_video_id": "52cnNJBqLQA"
                    },
                    {
                        "type": "recorded",
                        "title": "Can Visualization Alleviate Dichotomous Thinking? Effects of Visual Representations on the Cliff Effect",
                        "contributors": [
                            "Jouni Helske",
                            "Satu Helske",
                            "Matthew Cooper",
                            "Anders Ynnerman",
                            "Lonni Besan\u00e7on"
                        ],
                        "abstract": "Common reporting styles for statistical results in scientific articles, such as p-values and confidence intervals (CI), have been reported to be prone to dichotomous interpretations, especially with respect to the null hypothesis significance testing framework. For example when the p-value is small enough or the CIs of the mean effects of a studied drug and a placebo are not overlapping, scientists tend to claim significant differences while often disregarding the magnitudes and absolute differences in the effect sizes. This type of reasoning has been shown to be potentially harmful to science. Techniques relying on the visual estimation of the strength of evidence have been recommended to reduce such dichotomous interpretations but their effectiveness has also been challenged. We ran two experiments on researchers with expertise in statistical analysis to compare several alternative representations of confidence intervals and used Bayesian multilevel models to estimate the effects of the representation styles on differences in researchers' subjective confidence in the results. We also asked the respondents' opinions and preferences in representation styles. Our results suggest that adding visual information to classic CI representation can decrease the tendency towards dichotomous interpretations - measured as the `cliff effect': the sudden drop in confidence around p-value 0.05 - compared with classic CI visualization and textual representation of the CI with p-values. All data and analyses are publicly available at https://github.com/helske/statvis.",
                        "time_start": "2021-10-29T13:45:00Z",
                        "time_end": "2021-10-29T14:00:00Z",
                        "uid": "v-tvcg-9405484",
                        "youtube_video_id": "cCBIiN-gz6U"
                    },
                    {
                        "type": "recorded",
                        "title": "Effect of Uncertainty Visualizations on Myopic Loss Aversion and Equity Premium Puzzle in Retirement Investment Decisions",
                        "contributors": [
                            "Ryan Wesslen",
                            "Alireza Karduni",
                            "Doug Markant",
                            "Wenwen Dou"
                        ],
                        "abstract": "For many households, investing for retirement is one of the most significant decisions and is fraught with uncertainty. In a classic study in behavioral economics, Benartzi and Thaler (1999) found evidence using bar charts that investors exhibit myopic loss aversion in retirement decisions: Investors overly focus on the potential for short-term losses, leading them to invest less in riskier assets and miss out on higher long-term returns. Recently, advances in uncertainty visualizations have shown improvements in decision-making under uncertainty in a variety of tasks. In this paper, we conduct a controlled and incentivized crowdsourced experiment replicating Benartzi and Thaler (1999) and extending it to measure the effect of different uncertainty representations on myopic loss aversion. Consistent with the original study, we find evidence of myopic loss aversion with bar charts and find that participants make better investment decisions with longer evaluation periods. We also find that common uncertainty representations such as interval plots and bar charts achieve the highest mean expected returns while other uncertainty visualizations lead to poorer long-term performance and strong effects on the equity premium. Qualitative feedback further suggests that different uncertainty representations lead to visual reasoning heuristics that can either mitigate or encourage a focus on potential short-term losses. We discuss implications of our results on using uncertainty visualizations for retirement decisions in practice and possible extensions for future work.",
                        "time_start": "2021-10-29T14:00:00Z",
                        "time_end": "2021-10-29T14:15:00Z",
                        "uid": "v-full-1028",
                        "youtube_video_id": "aSD1asOEKLs"
                    },
                    {
                        "type": "recorded",
                        "title": "Visualization Equilibrium",
                        "contributors": [
                            "Paula Kayongo",
                            "Glenn Sun",
                            "Jason Hartline",
                            "Jessica Hullman"
                        ],
                        "abstract": "In many real-world strategic settings, people use information displays to make decisions. In these settings, an information provider chooses which information to provide to strategic agents and how to present it, and agents formulate a best response based on the information and their anticipation of how others will behave. We contribute the results of a controlled online experiment to examine how the provision and presentation of information impacts people's decisions in a congestion game. Our experiment compares how different visualization approaches for displaying this information, including bar charts and hypothetical outcome plots, and different information conditions, including where the visualized information is private versus public (i.e., available to all agents), affect decision making and welfare. We characterize the effects of visualization anticipation, referring to changes to behavior when an agent goes from alone having access to a visualization to knowing that others also have access to the visualization to guide their decisions. We also empirically identify the visualization equilibrium, i.e., the visualization for which the visualized outcome of agents' decisions matches the realized decisions of the agents who view it. We reflect on the implications of visualization equilibria and visualization anticipation for designing information displays for real-world strategic settings.",
                        "time_start": "2021-10-29T14:15:00Z",
                        "time_end": "2021-10-29T14:30:00Z",
                        "uid": "v-full-1198",
                        "youtube_video_id": "1j5swOjlxOY"
                    }
                ]
            },
            {
                "title": "Model Evaluation",
                "session_id": "v-full-full10",
                "track": "room1",
                "schedule_image": "v-full-full10.png",
                "chair": [
                    "Adam Perer"
                ],
                "organizers": [],
                "time_start": "2021-10-29T15:00:00Z",
                "time_end": "2021-10-29T16:30:00Z",
                "discord_category": "",
                "discord_channel": "",
                "discord_channel_id": 0,
                "youtube_url": null,
                "zoom_meeting": "",
                "zoom_password": "",
                "ff_link": "",
                "ff_playlist": "",
                "time_slots": [
                    {
                        "type": "recorded",
                        "title": "Inspecting the Running Process of Horizontal Federated Learning via Visual Analytics",
                        "contributors": [
                            "Quan Li",
                            "Xiguang Wei",
                            "Huanbin Lin",
                            "Yang Liu",
                            "Tianjian Chen",
                            "Xiaojuan Ma"
                        ],
                        "abstract": "As a decentralized training approach, horizontal federated learning (HFL) enables distributed clients to collaboratively learn a machine learning model while keeping personal/private information on local devices. Despite the enhanced performance and efficiency of HFL over local training, clues for inspecting the behaviors of the participating clients and the federated model are usually lacking due to the privacy-preserving nature of HFL. Consequently, the users can only conduct a shallow-level analysis of potential abnormal behaviors and have limited means to assess the contributions of individual clients and implement the necessary intervention. Visualization techniques have been introduced to facilitate the HFL process inspection, usually by providing model metrics and evaluation results as a dashboard representation. Although the existing visualization methods allow a simple examination of the HFL model performance, they cannot support the intensive exploration of the HFL process. In this study, strictly following the HFL privacy-preserving protocol, we design an exploratory visual analytics system for the HFL process termed HFLens, which supports comparative visual interpretation at the overview, communication round, and client instance levels. Specifically, the proposed system facilitates the investigation of the overall process involving all clients, the correlation analysis of clients' information in one or different communication round(s), the identification of potential anomalies, and the contribution assessment of each HFL client. Two case studies confirm the efficacy of our system. Experts' feedback suggests that our approach indeed helps in understanding and diagnosing the HFL process better.",
                        "time_start": "2021-10-29T15:00:00Z",
                        "time_end": "2021-10-29T15:15:00Z",
                        "uid": "v-tvcg-9408377",
                        "youtube_video_id": "MCEADopnGNY"
                    },
                    {
                        "type": "recorded",
                        "title": "An Evaluation-Focused Framework for Visualization Recommendation Algorithms",
                        "contributors": [
                            "Zehua Zeng",
                            "Phoebe Moh",
                            "Fan Du",
                            "Jane Hoffswell",
                            "Tak Yeon Lee",
                            "Sana Malik",
                            "Eunyee Koh",
                            "Leilani Battle"
                        ],
                        "abstract": "Although we have seen a proliferation of algorithms for recommending visualizations, these algorithms are rarely compared with one another, making it difficult to ascertain which algorithm is best for a given visual analysis scenario. Though several formal frameworks have been proposed in response, we believe this issue persists because visualization recommendation algorithms are inadequately specified from an evaluation perspective. In this paper, we propose an evaluation-focused framework to contextualize and compare a broad range of visualization recommendation algorithms. We present the structure of our framework, where algorithms are specified using three components: (1) a graph representing the full space of possible visualization designs, (2) the method used to traverse the graph for potential candidates for recommendation, and (3) an oracle used to rank candidate designs. To demonstrate how our framework guides the formal comparison of algorithmic performance, we not only theoretically compare five existing representative recommendation algorithms, but also empirically compare four new algorithms generated based on our findings from the theoretical comparison. Our results show that these algorithms behave similarly in terms of user performance, highlighting the need for more rigorous formal comparisons of recommendation algorithms to further clarify their benefits in various analysis scenarios.",
                        "time_start": "2021-10-29T15:15:00Z",
                        "time_end": "2021-10-29T15:30:00Z",
                        "uid": "v-full-1199",
                        "youtube_video_id": "ZGmK-TCgduI"
                    },
                    {
                        "type": "recorded",
                        "title": "Visual Analysis of Hyperproperties for Understanding Model Checking Results",
                        "contributors": [
                            "Tom Horak",
                            "Norine Coenen",
                            "Niklas Metzger",
                            "Christopher Hahn",
                            "Tamara Flemisch",
                            "Juli\u00e1n M\u00e9ndez",
                            "Dennis Dimov",
                            "Bernd Finkbeiner",
                            "Raimund Dachselt"
                        ],
                        "abstract": "Model checkers provide algorithms for proving that a mathematical model of a system satisfies a given specification. In case of a violation, a counterexample that shows the erroneous behavior is returned. Understanding these counterexamples is challenging, especially for hyperproperty specifications, i.e., specifications that relate multiple executions of a system to each other. We aim to facilitate the visual analysis of such counterexamples through our HyperVis tool, which provides interactive visualizations of the given model, specification, and counterexample. Within an iterative and interdisciplinary design process, we developed visualization solutions that can effectively communicate the core aspects of the model checking result. Specifically, we introduce graphical representations of binary values for improving pattern recognition, color encoding for better indicating related aspects, visually enhanced textual descriptions, as well as extensive cross-view highlighting mechanisms. Further, through an underlying causal analysis of the counterexample, we are also able to identify values that contributed to the violation and use this knowledge for both improved encoding and highlighting. Finally, the analyst can modify both the specification of the hyperproperty and the system directly within HyperVis and initiate the model checking of the new version. In combination, these features notably support the analyst in understanding the error leading to the counterexample as well as iterating the provided system and specification. We ran multiple case studies with HyperVis and tested it with domain experts in qualitative feedback sessions. The participants' positive feedback confirms the considerable improvement over the manual, text-based status quo and the value of the tool for explaining hyperproperties.",
                        "time_start": "2021-10-29T15:30:00Z",
                        "time_end": "2021-10-29T15:45:00Z",
                        "uid": "v-full-1113",
                        "youtube_video_id": "ws8Q5J0lxdY"
                    },
                    {
                        "type": "recorded",
                        "title": "FairRankVis: A Visual Analytics Framework for Exploring Algorithmic Fairness in Graph Mining Models",
                        "contributors": [
                            "Tiankai Xie",
                            "Yuxin Ma",
                            "Jian Kang",
                            "Hanghang Tong",
                            "Ross Maciejewski"
                        ],
                        "abstract": "Graph mining is an essential component of recommender systems and search engines. Outputs of graph mining models typically provide a ranked list sorted by each item's relevance or utility. However, recent research has identified issues of algorithmic bias in such models, and new graph mining algorithms have been proposed to correct for bias. \n  As such, algorithm developers need tools that can help them uncover potential biases in their models while also exploring the impacts of correcting for biases when employing fairness-aware algorithms. In this paper, we present FairRankVis, a visual analytics framework designed to enable the exploration of multi-class bias in graph mining algorithms. We support both group and individual fairness levels of comparison. Our framework is designed to enable model developers to compare multi-class fairness between algorithms (for example, comparing PageRank with a debiased PageRank algorithm) to assess the impacts of algorithmic debiasing with respect to group and individual fairness. We demonstrate our framework through two usage scenarios inspecting algorithmic fairness.",
                        "time_start": "2021-10-29T15:45:00Z",
                        "time_end": "2021-10-29T16:00:00Z",
                        "uid": "v-full-1361",
                        "youtube_video_id": "7GM9FrH6O3s"
                    },
                    {
                        "type": "recorded",
                        "title": "embComp: Visual Interactive Comparison of Vector Embeddings",
                        "contributors": [
                            "Florian Heimerl",
                            "Christoph Kralj",
                            "Torsten M\u00f6ller",
                            "Michael Gleicher"
                        ],
                        "abstract": "This paper introduces embComp, a novel approach for comparing two embeddings that capture the similarity between objects, such as word and document embeddings. We survey scenarios where comparing these embedding spaces is useful. From those scenarios, we derive common tasks, introduce visual analysis methods that support these tasks, and combine them into a comprehensive system. One of embComp\u2019s central features are overview visualizations that are based on metrics for measuring differences in the local structure around objects. Summarizing these local metrics over the embeddings provides global overviews of similarities and differences. Detail views allow comparison of the local structure around selected objects and relating this local information to the global views. Integrating and connecting all of these components, embComp supports a range of analysis workflows that help understand similarities and differences between embedding spaces. We assess our approach by applying it in several use cases, including understanding corpora differences via word vector embeddings, and understanding algorithmic differences in generating embeddings.",
                        "time_start": "2021-10-29T16:00:00Z",
                        "time_end": "2021-10-29T16:15:00Z",
                        "uid": "v-tvcg-9301222",
                        "youtube_video_id": "GaSFwSXjc74"
                    },
                    {
                        "type": "recorded",
                        "title": "VBridge: Connecting the Dots Between Features, Explanations, and Data for Healthcare Models",
                        "contributors": [
                            "Furui Cheng",
                            "Dongyu Liu",
                            "Fan Du",
                            "Yanna Lin",
                            "Alexandra Zytek",
                            "Haomin Li",
                            "Huamin Qu",
                            "Kalyan Veeramachaneni"
                        ],
                        "abstract": "Machine learning (ML) is increasingly applied to Electronic Health Records (EHRs) to solve clinical prediction tasks. Although many ML models perform promisingly, issues with model transparency and interpretability limit their adoption in clinical practice. Directly using existing explainable ML techniques in clinical settings can be challenging. Through literature surveys and collaborations with six clinicians with an average of 17 years of clinical experience, we identified three key challenges, including clinicians' unfamiliarity with ML features, lack of contextual information, and the need for cohort-level evidence. Following an iterative design process, we further designed and developed VBridge, a visual analytics tool that seamlessly incorporates ML explanations into clinicians' decision-making workflow. The system includes a novel hierarchical display of contribution-based feature explanations and enriched interactions that connect the dots between ML features, explanations, and data. We demonstrated the effectiveness of VBridge through two case studies and expert interviews with four clinicians, showing that visually associating model explanations with patients' situational records can help clinicians better interpret and use model predictions when making clinician decisions. We further derived a list of design implications for developing future explainable ML tools to support clinical decision-making.",
                        "time_start": "2021-10-29T16:15:00Z",
                        "time_end": "2021-10-29T16:30:00Z",
                        "uid": "v-full-1284",
                        "youtube_video_id": "kYIk-enkxNk"
                    }
                ]
            },
            {
                "title": "Efficient Representation and Layout",
                "session_id": "v-full-full16",
                "track": "room3",
                "schedule_image": "v-full-full16.png",
                "chair": [
                    "Markus Hadwiger"
                ],
                "organizers": [],
                "time_start": "2021-10-29T15:00:00Z",
                "time_end": "2021-10-29T16:30:00Z",
                "discord_category": "",
                "discord_channel": "",
                "discord_channel_id": 0,
                "youtube_url": null,
                "zoom_meeting": "",
                "zoom_password": "",
                "ff_link": "",
                "ff_playlist": "",
                "time_slots": [
                    {
                        "type": "recorded",
                        "title": "Rapid Labels: Point-Feature Labeling on GPU",
                        "contributors": [
                            "V\u00e1clav Pavlovec",
                            "Ladislav \u010cmol\u00edk"
                        ],
                        "abstract": "Labels, short textual annotations are an important component of data visualizations, illustrations, infographics, and geographical maps. In interactive applications, the labeling method responsible for positioning the labels should not take the resources from the application itself. In other words, the labeling method should provide the result as fast as possible. In this work, we propose a greedy point-feature labeling method running on GPU. In contrast to existing methods that position the labels sequentially, the proposed method positions several labels in parallel. Yet, we guarantee that the positioned labels will not overlap, nor will they overlap important visual features. When the proposed method is searching for the label position of a point-feature, the available label candidates are evaluated with respect to overlaps with important visual features, conflicts with label candidates of other point-features, and their ambiguity. The evaluation of each label candidate is done in constant time independently from the number of point-features, the number of important visual features, and the resolution of the created image. Our measurements indicate that the proposed method is able to position more labels than existing greedy methods that do not evaluate conflicts between the label candidates. At the same time, the proposed method achieves a significant increase in performance. The increase in performance is mainly due to the parallelization and the efficient evaluation of label candidates.",
                        "time_start": "2021-10-29T15:00:00Z",
                        "time_end": "2021-10-29T15:15:00Z",
                        "uid": "v-full-1310",
                        "youtube_video_id": "hd4w9LgOMRU"
                    },
                    {
                        "type": "recorded",
                        "title": "Scalable Scalable Vector Graphics: Automatic Translation of Interactive SVGs to a Multithread VDOM for Fast Rendering",
                        "contributors": [
                            "Michail Schwab",
                            "David Saffo",
                            "Nicholas Bond",
                            "Shash Sinha",
                            "Cody Dunne",
                            "Jeff Huang",
                            "James Tompkin",
                            "Michelle Borkin"
                        ],
                        "abstract": "The dominant markup language for Web visualizations - Scalable Vector Graphics (SVG) - is comparatively easy to learn, and is open, accessible, customizable via CSS, and searchable via the DOM, with easy interaction handling and debugging. Because these attributes allow visualization creators to focus on design on implementation details, tools built on top of SVG, such as D3.js, are essential to the visualization community. However, slow SVG rendering can limit designs by effectively capping the number of on-screen data points, and this can force visualization creators to switch to Canvas or WebGL. These are less flexible (e.g., no search or styling via CSS), and harder to learn. We introduce Scalable Scalable Vector Graphics (SSVG) to reduce these limitations and allow complex and smooth visualizations to be created with SVG. SSVG automatically translates interactive SVG visualizations into a dynamic virtual DOM (VDOM) to bypass the browser's slow `to specification' rendering by intercepting JavaScript function calls. De-coupling the SVG visualization specification from SVG rendering, and obtaining a dynamic VDOM, creates flexibility and opportunity for visualization system research. SSVG uses this flexibility to free up the main thread for more interactivity and renders the visualization with Canvas or WebGL on a web worker. Together, these concepts create a drop-in JavaScript library which can improve rendering performance by 3-9X with only one line of code added. To demonstrate applicability, we describe the use of SSVG on multiple example visualizations including published visualization research. A free copy of this paper, collected data, and source code are available as open science at osf.io/ge8wp.",
                        "time_start": "2021-10-29T15:15:00Z",
                        "time_end": "2021-10-29T15:30:00Z",
                        "uid": "v-tvcg-9354592",
                        "youtube_video_id": "UiotK9Wxpqc"
                    },
                    {
                        "type": "recorded",
                        "title": "Probabilistic Data-Driven Sampling via Multi-Criteria Importance Analysis",
                        "contributors": [
                            "Ayan Biswas",
                            "Soumya Dutta",
                            "Earl Lawrence",
                            "John Patchett",
                            "Jon Calhoun",
                            "James Ahrens"
                        ],
                        "abstract": "Although supercomputers are becoming increasingly powerful, their components have thus far not scaled proportionately. Compute power is growing enormously and is enabling finely resolved simulations that produce never-before-seen features. However, I/O capabilities lag by orders of magnitude, which means only a fraction of the simulation data can be stored for post hoc analysis. Prespecified plans for saving features and quantities of interest do not work for features that have not been seen before. Data-driven intelligent sampling schemes are needed to detect and save important parts of the simulation while it is running. Here, we propose a novel sampling scheme that reduces the size of the data by orders-of-magnitude while still preserving important regions. The approach we develop selects points with unusual data values and high gradients. We demonstrate that our approach outperforms traditional sampling schemes on a number of tasks.",
                        "time_start": "2021-10-29T15:30:00Z",
                        "time_end": "2021-10-29T15:45:00Z",
                        "uid": "v-tvcg-9130956",
                        "youtube_video_id": "koE5v4-OxdI"
                    },
                    {
                        "type": "recorded",
                        "title": "An Efficient Dual-Hierarchy tSNE Minimization",
                        "contributors": [
                            "Mark van de Ruit",
                            "Markus Billeter",
                            "Elmar Eisemann"
                        ],
                        "abstract": "t-distributed Stochastic Neighbour Embedding (t-SNE) has become a standard for exploratory data analysis, as it is capable of revealing clusters even in complex data while requiring minimal user input. While its run-time complexity limited it to small datasets in the past, recent efforts improved upon the expensive similarity computations and the previously quadratic minimization. Nevertheless, t-SNE still has high runtime and memory costs when operating on millions of points. We present a novel method for executing the t-SNE minimization. While our method overall retains a linear runtime complexity, we obtain a significant performance increase in the most expensive part of the minimization. We achieve a significant improvement without a noticeable decrease in accuracy even when targeting a 3D embedding. Our method constructs a pair of spatial hierarchies over the embedding, which are simultaneously traversed to approximate many N-body interactions at once. We demonstrate an efficient GPGPU implementation and evaluate its performance against state-of-the-art methods on a variety of datasets.",
                        "time_start": "2021-10-29T15:45:00Z",
                        "time_end": "2021-10-29T16:00:00Z",
                        "uid": "v-full-1343",
                        "youtube_video_id": "Iumg2Rkiamw"
                    },
                    {
                        "type": "recorded",
                        "title": "Joint t-SNE for Comparable Projections of Multiple High-Dimensional Datasets",
                        "contributors": [
                            "Yinqiao Wang",
                            "Lu Chen",
                            "Jaemin Jo",
                            "Yunhai Wang"
                        ],
                        "abstract": "We present Joint t-Stochastic Neighbor Embedding (Joint t-SNE), a technique to generate comparable projections of multiple high-dimensional datasets. Although t-SNE has been widely employed to visualize high-dimensional datasets from various domains, it is limited to projecting a single dataset. When a series of high-dimensional datasets, such as datasets changing over time, is projected independently using t-SNE, misaligned layouts are obtained. Even items with identical features across datasets are projected to different locations, making the technique unsuitable for comparison tasks. To tackle this problem, we introduce edge similarity, which captures the similarities between two adjacent time frames based on the Graphlet Frequency Distribution (GFD). We then integrate a novel loss term into the t-SNE loss function, which we call vector constraints, to preserve the vectors between projected points across the projections, allowing these points to serve as visual landmarks for direct comparisons between projections. Using synthetic datasets whose ground-truth structures are known, we show that Joint t-SNE outperforms existing techniques, including Dynamic t-SNE, in terms of local coherence error, Kullback-Leibler divergence, and neighborhood preservation. We also showcase a real-world use case to visualize and compare the activation of different layers of a neural network.",
                        "time_start": "2021-10-29T16:00:00Z",
                        "time_end": "2021-10-29T16:15:00Z",
                        "uid": "v-full-1471",
                        "youtube_video_id": "rHoQt8ha-9I"
                    },
                    {
                        "type": "recorded",
                        "title": "Automatic Polygon Layout for Primal-Dual Visualization of Hypergraphs",
                        "contributors": [
                            "Botong Qu",
                            "Eugene Zhang",
                            "Yue Zhang"
                        ],
                        "abstract": "N-ary relationships, which relate N entities where N is not necessarily two, can be visually represented as polygons whose vertices are the entities of the relationships. Manually generating a high-quality layout using this representation is labor-intensive. In this paper, we provide an automatic polygon layout generation algorithm for the visualization of N-ary relationships. At the core of our algorithm is a set of objective functions motivated by a number of design principles that we have identified. These objective functions are then used in an optimization framework that we develop to achieve high-quality layouts. Recognizing the duality between entities and relationships in the data, we provide a second visualization in which the roles of entities and relationships in the original data are reversed. This can lead to additional insight about the data. Furthermore, we enhance our framework for a joint optimization on the primal layout (original data) and the dual layout (where the roles of entities and relationships are reversed). This allows users to inspect their data using two complementary views. We apply our visualization approach to a number of datasets that include co-authorship data and social contact pattern data.",
                        "time_start": "2021-10-29T16:15:00Z",
                        "time_end": "2021-10-29T16:30:00Z",
                        "uid": "v-full-1685",
                        "youtube_video_id": "XslpfmVYlPs"
                    }
                ]
            },
            {
                "title": "Biological and Medical Visualization",
                "session_id": "v-full-full6",
                "track": "room2",
                "schedule_image": "v-full-full6.png",
                "chair": [
                    "Barbora Kozlikova"
                ],
                "organizers": [],
                "time_start": "2021-10-29T15:00:00Z",
                "time_end": "2021-10-29T16:15:00Z",
                "discord_category": "",
                "discord_channel": "",
                "discord_channel_id": 0,
                "youtube_url": null,
                "zoom_meeting": "",
                "zoom_password": "",
                "ff_link": "",
                "ff_playlist": "",
                "time_slots": [
                    {
                        "type": "recorded",
                        "title": "COVID-view: Diagnosis of COVID-19 using Chest CT",
                        "contributors": [
                            "Shreeraj Jadhav",
                            "Gaofeng Deng",
                            "Marlene Zawin",
                            "Arie Kaufman"
                        ],
                        "abstract": "Significant work has been done towards deep learning (DL) models for automatic lung and lesion segmentation and classification of COVID-19 on chest CT data. However, comprehensive visualization systems focused on supporting the dual visual+DL diagnosis of COVID-19 are non-existent. We present COVID-view, a visualization application specially tailored for radiologists to diagnose COVID-19 from chest CT data. The system incorporates a complete pipeline of automatic lungs segmentation, localization/isolation of lung abnormalities, followed by visualization, visual and DL analysis, and measurement/quantification tools. Our system combines the traditional 2D workflow of radiologists with newer 2D and 3D visualization techniques with DL support for a more comprehensive diagnosis. COVID-view incorporates a novel DL model for classifying the patients into positive/negative COVID-19 cases, which acts as a reading aid for the radiologist using COVID-view, and provides the attention heatmap as an explainable DL for the model output. We designed and evaluated COVID-view through suggestions, close feedback and conducting case studies of real-world patient data by expert radiologists who have substantial experience diagnosing chest CT scans for COVID-19, pulmonary embolism, and other forms of lung infections. We present requirements and task analysis for the diagnosis of COVID-19 that motivate our design choices and results in a practical system which is capable of handling real-world patient cases.",
                        "time_start": "2021-10-29T15:00:00Z",
                        "time_end": "2021-10-29T15:15:00Z",
                        "uid": "v-full-1205",
                        "youtube_video_id": "CwXtaAtSzn8"
                    },
                    {
                        "type": "recorded",
                        "title": "ThreadStates: State-based Visual Analysis of Disease Progression",
                        "contributors": [
                            "Qianwen Wang",
                            "Tali Mazor",
                            "Theresa Harbig",
                            "Ethan Cerami",
                            "Nils Gehlenborg"
                        ],
                        "abstract": "A growing number of longitudinal cohort studies are generating data with extensive patient observations across multiple timepoints. Such data offers promising opportunities to better understand the progression of diseases. However, these observations are usually treated as general events in existing visual analysis tools. As a result, their capabilities in modeling disease progression are not fully utilized. To fill this gap, we designed and implemented ThreadStates, an interactive visual analytics tool for the exploration of longitudinal patient cohort data. The focus of ThreadStatesis to identify the states of disease progression by learning from observation data in a human-in-the-loop manner. We propose a novel Glyph Matrix design and combine it with a scatter plot to enable seamless identification, observation, and refinement of states. The disease progression patterns are then revealed in terms of state transitions using Sankey-based visualizations. We employ sequence clustering techniques to find patient groups with distinctive progression patterns, and to reveal the association between disease progression and patient-level features. The design and development were driven by a requirement analysis and iteratively refined based on feedback from domain experts over the course of a 10-month design study. Case studies and expert interviews demonstrate that ThreadStates can successively summarize disease states, reveal disease progression, and compare patient groups.",
                        "time_start": "2021-10-29T15:15:00Z",
                        "time_end": "2021-10-29T15:30:00Z",
                        "uid": "v-full-1558",
                        "youtube_video_id": "lrrJeQZC95k"
                    },
                    {
                        "type": "recorded",
                        "title": "Loon: Using Exemplars to Visualize Large Scale Microscopy Data",
                        "contributors": [
                            "Devin Lange",
                            "Eddie Polanco",
                            "Robert Judson-Torres",
                            "Thomas Zangle",
                            "Alexander Lex"
                        ],
                        "abstract": "Which drug is most promising for a cancer patient? This is a question a new microscopy-based approach for measuring the mass of individual cancer cells treated with different drugs promises to answer in only a few hours. However, the analysis pipeline for extracting data from these images is still far from complete automation: human intervention is necessary for quality control for preprocessing steps such as segmentation, to adjust filters, and remove noise, and for the analysis of the result. To address this workflow, we developed Loon, a visualization tool for analyzing drug screening data based on quantitative phase microscopy imaging. Loon visualizes both derived data such as growth rates, and imaging data. Since the images are collected automatically at a large scale, manual inspection of images and segmentations is infeasible. However, reviewing representative samples of cells is essential, both for quality control and for data analysis. We introduce a new approach of choosing and visualizing representative exemplar cells that retain a close connection to the low-level data. By tightly integrating the derived data visualization capabilities with the novel exemplar visualization and providing selection and filtering capabilities, Loon is well suited for making decisions about which drugs are suitable for a specific patient.",
                        "time_start": "2021-10-29T15:30:00Z",
                        "time_end": "2021-10-29T15:45:00Z",
                        "uid": "v-full-1271",
                        "youtube_video_id": "Xz5VrBXk5J0"
                    },
                    {
                        "type": "recorded",
                        "title": "Scope2Screen: Focus+Context Techniques for Pathology Tumor Assessment in Multivariate Image Data",
                        "contributors": [
                            "Jared Jessup",
                            "Robert Kr\u00fcger",
                            "Simon Warchol",
                            "John Hoffer",
                            "Jeremy Muhlich",
                            "Cecily C. Ritch",
                            "Giorgio Gaglia",
                            "Shannon Coy",
                            "Yu-An Chen",
                            "Jia-Ren Lin",
                            "Sandro Santagata",
                            "Peter Sorger",
                            "Hanspeter Pfister"
                        ],
                        "abstract": "Inspection of tissues using a light microscope is the primary method of diagnosing many diseases, notably cancer. Highly multiplexed tissue imaging builds on this foundation, enabling collection of up to 60 channels of molecular information plus cell and tissue morphology using antibody staining. This provides unique insight into disease biology and promises to help with design of patient-specific therapies. However, a substantial gap remains with respect to visualizing the resulting multivariate image data and effectively supporting pathology workflows in digital environments on screen. We, therefore, developed Scope2Screen, a scalable software system for focus+context exploration and annotation of whole-slide, high-plex, tissue images. Our approach scales to analyzing 100GB images of 10^9 or more pixels per channel, containing millions of individual cells. A multidisciplinary team of visualization experts, microscopists, and pathologists identified key image exploration and annotation tasks involving finding, magnifying, quantifying, and organizing regions of interest (ROIs) in an intuitive and cohesive manner. Building on a scope-to-screen metaphor, we present interactive lensing techniques that operate at single-cell and tissue levels. Lenses are equipped with task-specific functionality and descriptive statistics, making it possible to analyze image features, cell types, and spatial arrangements (neighborhoods) across image channels and scales. A fast sliding-window search guides users to regions similar to those under the lens; these regions can be analyzed and considered either separately or as part of a larger image collection. A novel snapshot method enables linked lens configurations and image statistics to be saved, restored, and shared with these regions. We validate our designs with domain experts and apply Scope2Screen in two case studies involving lung and colorectal cancers to discover cancer-relevant image features.",
                        "time_start": "2021-10-29T15:45:00Z",
                        "time_end": "2021-10-29T16:00:00Z",
                        "uid": "v-full-1502",
                        "youtube_video_id": "m4nrNp9T-IQ"
                    },
                    {
                        "type": "recorded",
                        "title": "Multiscale Unfolding: Illustratively Visualizing the Whole Genome at a Glance",
                        "contributors": [
                            "Sarkis Halladjian",
                            "David Kouril",
                            "Haichao Miao",
                            "Eduard Gr\u00f6ller",
                            "Ivan Viola",
                            "Tobias Isenberg"
                        ],
                        "abstract": "We present Multiscale Unfolding, an interactive technique for illustratively visualizing multiple hierarchical scales of DNA in a single view, showing the genome at different scales and demonstrating \n how one scale spatially folds into the next. The DNA's extremely long sequential structure---arranged differently on several distinct scale levels---is often lost in traditional 3D depictions, mainly due to its multiple levels of dense spatial packing and the resulting occlusion. Furthermore, interactive exploration of this complex structure is cumbersome, requiring visibility management like cut-aways. In contrast to existing temporally controlled multiscale data exploration, we allow viewers to always see and interact with any of the involved scales. For this purpose we separate the depiction into constant-scale and scale transition zones. Constant-scale zones maintain a single-scale representation, while still linearly unfolding the DNA. Inspired by illustration, scale transition zones connect adjacent constant-scale zones via level unfolding, scaling, and transparency. We thus represent the spatial structure of the whole DNA macro-molecule, maintain its local organizational characteristics, linearize its higher-level organization, and use spatially controlled, understandable interpolation between neighboring scales. We also contribute interaction techniques that provide viewers with a coarse-to-fine control for navigating within our all-scales-in-one-view representations and visual aids to illustrate the size differences. Overall, Multiscale Unfolding allows viewers to grasp the DNA's structural composition from chromosomes to the atoms, with increasing levels of \"unfoldedness,\" and can be applied in data-driven illustration and communication.",
                        "time_start": "2021-10-29T16:00:00Z",
                        "time_end": "2021-10-29T16:15:00Z",
                        "uid": "v-tvcg-9376675",
                        "youtube_video_id": "pv-3mTuu2I8"
                    }
                ]
            }
        ]
    },
    "v-keynote": {
        "event": "VIS",
        "long_name": "VIS",
        "event_type": "VIS Keynote",
        "event_description": "",
        "event_url": "",
        "sessions": [
            {
                "title": "Keynote by danah boyd",
                "session_id": "v-keynote-1",
                "track": "room1",
                "schedule_image": "v-keynote-1.png",
                "chair": [
                    "Luis Gustavo Nonato",
                    "Brian Summa"
                ],
                "organizers": [],
                "time_start": "2021-10-26T17:00:00Z",
                "time_end": "2021-10-26T18:00:00Z",
                "discord_category": "",
                "discord_channel": "",
                "discord_channel_id": 0,
                "youtube_url": null,
                "zoom_meeting": "",
                "zoom_password": "",
                "ff_link": "",
                "ff_playlist": "",
                "time_slots": [
                    {
                        "type": "live",
                        "title": "Keynote by danah boyd",
                        "contributors": [
                            "danah boyd"
                        ],
                        "abstract": null,
                        "time_start": "2021-10-26T17:00:00Z",
                        "time_end": "2021-10-26T18:00:00Z"
                    },
                    {
                        "type": "recorded",
                        "title": "VIS Full Papers Opening",
                        "contributors": "",
                        "abstract": null,
                        "time_start": "2021-10-26T17:00:00Z",
                        "time_end": "2021-10-26T18:00:00Z"
                    },
                    {
                        "type": "recorded",
                        "title": "VIS Short Papers Opening",
                        "contributors": "",
                        "abstract": null,
                        "time_start": "2021-10-26T17:00:00Z",
                        "time_end": "2021-10-26T18:00:00Z"
                    }
                ]
            }
        ]
    },
    "a-visap": {
        "event": "VIS Arts Program",
        "long_name": "VIS Arts Program",
        "event_type": "VIS Arts Program",
        "event_description": "",
        "event_url": "",
        "sessions": [
            {
                "title": "VIS Arts Program Opening Event",
                "session_id": "a-visap-opening",
                "track": "room1",
                "schedule_image": "a-visap-opening.png",
                "chair": [
                    "VISAP Chairs"
                ],
                "organizers": [
                    "VISAP Chairs"
                ],
                "time_start": "2021-10-26T19:00:00Z",
                "time_end": "2021-10-26T20:30:00Z",
                "discord_category": "",
                "discord_channel": "",
                "discord_channel_id": 0,
                "youtube_url": null,
                "zoom_meeting": "",
                "zoom_password": "",
                "ff_link": "",
                "ff_playlist": "",
                "time_slots": [
                    {
                        "type": "live",
                        "title": "Introductory Remarks",
                        "contributors": [
                            "VISAP Chairs"
                        ],
                        "abstract": null,
                        "time_start": "2021-10-26T19:00:00Z",
                        "time_end": "2021-10-26T19:10:00Z"
                    },
                    {
                        "type": "recorded",
                        "title": "VISAP FF Video Preview",
                        "contributors": [
                            "VISAP Chairs"
                        ],
                        "abstract": null,
                        "time_start": "2021-10-26T19:10:00Z",
                        "time_end": "2021-10-26T19:13:00Z",
                        "youtube_video_id": "Tv1Anrwf8z4"
                    },
                    {
                        "type": "live",
                        "title": "VISAP Website Walkthrough",
                        "contributors": [
                            "VISAP Chairs"
                        ],
                        "abstract": null,
                        "time_start": "2021-10-26T19:13:00Z",
                        "time_end": "2021-10-26T19:20:00Z"
                    },
                    {
                        "type": "recorded",
                        "title": "Untitled Interspecies Umwelten",
                        "contributors": [
                            "Joel Ong"
                        ],
                        "abstract": null,
                        "time_start": "2021-10-26T19:20:00Z",
                        "time_end": "2021-10-26T19:25:00Z",
                        "youtube_video_id": "H1dbCslT48Y"
                    },
                    {
                        "type": "recorded",
                        "title": "Not Suitable for Breathing",
                        "contributors": [
                            "Zhouyang Lu"
                        ],
                        "abstract": null,
                        "time_start": "2021-10-26T19:25:00Z",
                        "time_end": "2021-10-26T19:30:00Z",
                        "youtube_video_id": "m8qZv0TkoAs"
                    },
                    {
                        "type": "recorded",
                        "title": "Invisible Lives",
                        "contributors": [
                            "Hye Yeon Nam"
                        ],
                        "abstract": null,
                        "time_start": "2021-10-26T19:30:00Z",
                        "time_end": "2021-10-26T19:31:00Z",
                        "youtube_video_id": "H9SIWqK54ME"
                    },
                    {
                        "type": "recorded",
                        "title": "FaceType",
                        "contributors": [
                            "Kevin Maher",
                            "Fan Xiang"
                        ],
                        "abstract": null,
                        "time_start": "2021-10-26T19:31:00Z",
                        "time_end": "2021-10-26T19:36:00Z",
                        "youtube_video_id": "Tpv96wzm_d0"
                    },
                    {
                        "type": "live",
                        "title": "Live Q&A",
                        "contributors": "",
                        "abstract": null,
                        "time_start": "2021-10-26T19:36:00Z",
                        "time_end": "2021-10-26T19:50:00Z"
                    },
                    {
                        "type": "gathertown",
                        "title": "Live Q&A + Gather Town Gathering",
                        "contributors": "",
                        "abstract": null,
                        "time_start": "2021-10-26T19:50:00Z",
                        "time_end": "2021-10-26T20:30:00Z"
                    }
                ]
            },
            {
                "title": "VISAP Session 1",
                "session_id": "a-visap-visap1",
                "track": "room6",
                "schedule_image": "a-visap-visap1.png",
                "chair": [
                    "Yoon Chung Han"
                ],
                "organizers": [],
                "time_start": "2021-10-27T17:00:00Z",
                "time_end": "2021-10-27T18:30:00Z",
                "discord_category": "",
                "discord_channel": "",
                "discord_channel_id": 0,
                "youtube_url": null,
                "zoom_meeting": "",
                "zoom_password": "",
                "ff_link": "",
                "ff_playlist": "",
                "time_slots": [
                    {
                        "type": "live",
                        "title": "Welcome and introduction",
                        "contributors": [
                            "Yoon Chung Han"
                        ],
                        "abstract": null,
                        "time_start": "2021-10-27T17:00:00Z",
                        "time_end": "2021-10-27T17:05:00Z"
                    },
                    {
                        "type": "recorded",
                        "title": "Keynote lecture  - MISSING",
                        "contributors": [
                            "Jer Thorp"
                        ],
                        "abstract": null,
                        "time_start": "2021-10-27T17:05:00Z",
                        "time_end": "2021-10-27T17:20:00Z"
                    },
                    {
                        "type": "live",
                        "title": "Live Q&A for Keynote",
                        "contributors": [
                            "Jer Thorp"
                        ],
                        "abstract": null,
                        "time_start": "2021-10-27T17:20:00Z",
                        "time_end": "2021-10-27T17:25:00Z"
                    },
                    {
                        "type": "recorded",
                        "title": "Visualizing Life in the Deep: A Creative Pipeline for Data-Driven Animations to Facilitate Marine Mammal Research, Outreach, and Conservation",
                        "contributors": [
                            "Jessica Marielle Kendall-Bar",
                            "Nicolas Kendall-Bar",
                            "Angus G. Forbes",
                            "Gitte McDonald",
                            "Paul J. Ponganis",
                            "Cassondra Williams",
                            "Allyson Hindle",
                            "Holger Klinck",
                            "Markus Horning",
                            "David Wiley",
                            "Ari S. Friedlaender",
                            "Roxanne S. Beltran",
                            "Daniel P. Costa",
                            "Terrie Williams"
                        ],
                        "abstract": null,
                        "time_start": "2021-10-27T17:25:00Z",
                        "time_end": "2021-10-27T17:37:00Z",
                        "uid": "a-visap-1031",
                        "youtube_video_id": "0OvUuOxIRHI"
                    },
                    {
                        "type": "recorded",
                        "title": "Creating Meaningful Connections Through COVID-19 Data Manifestation",
                        "contributors": [
                            "Karin von Ompteda"
                        ],
                        "abstract": null,
                        "time_start": "2021-10-27T17:37:00Z",
                        "time_end": "2021-10-27T17:50:00Z",
                        "uid": "a-visap-1038",
                        "youtube_video_id": "ZPp8ytkBlRw"
                    },
                    {
                        "type": "recorded",
                        "title": "Affective Palettes for Scientific Visualization: Grounding Environmental Data in the Natural World",
                        "contributors": [
                            "Stephanie Zeller"
                        ],
                        "abstract": null,
                        "time_start": "2021-10-27T17:50:00Z",
                        "time_end": "2021-10-27T17:58:00Z",
                        "uid": "a-visap-1045",
                        "youtube_video_id": "8LeYhGu_8kg"
                    },
                    {
                        "type": "recorded",
                        "title": "Glacier\u2019s Lament",
                        "contributors": [
                            "Jiabao Li",
                            "Cooper Galvin"
                        ],
                        "abstract": null,
                        "time_start": "2021-10-27T17:58:00Z",
                        "time_end": "2021-10-27T18:08:00Z",
                        "uid": "a-visap-1046",
                        "youtube_video_id": "_9wxSZUfXos"
                    },
                    {
                        "type": "recorded",
                        "title": "Surface Tension ",
                        "contributors": [
                            "Caitlin Foley",
                            "Misha Rabinovich"
                        ],
                        "abstract": null,
                        "time_start": "2021-10-27T18:04:00Z",
                        "time_end": "2021-10-27T18:09:00Z",
                        "uid": "a-visap-1064",
                        "youtube_video_id": "mTI9CbM5dIM"
                    },
                    {
                        "type": "live",
                        "title": "Live Q&A for Papers, Pictorials, Artist talks",
                        "contributors": "",
                        "abstract": null,
                        "time_start": "2021-10-27T18:09:00Z",
                        "time_end": "2021-10-27T18:30:00Z"
                    }
                ]
            },
            {
                "title": "VISAP Session 2",
                "session_id": "a-visap-visap2",
                "track": "room6",
                "schedule_image": "a-visap-visap2.png",
                "chair": [
                    "Charles Perin"
                ],
                "organizers": [],
                "time_start": "2021-10-28T17:00:00Z",
                "time_end": "2021-10-28T18:30:00Z",
                "discord_category": "",
                "discord_channel": "",
                "discord_channel_id": 0,
                "youtube_url": null,
                "zoom_meeting": "",
                "zoom_password": "",
                "ff_link": "",
                "ff_playlist": "",
                "time_slots": [
                    {
                        "type": "live",
                        "title": "Welcome and introduction",
                        "contributors": [
                            "Charles Perin"
                        ],
                        "abstract": null,
                        "time_start": "2021-10-28T17:00:00Z",
                        "time_end": "2021-10-28T17:05:00Z"
                    },
                    {
                        "type": "recorded",
                        "title": "Deep Connection: Making Virtual Reality  Artworks with Medical Scan Data",
                        "contributors": [
                            "Marilene Oliver",
                            "Gary Joynes Joynes",
                            "Kumaradevan Punithakumar",
                            "Peter Seres"
                        ],
                        "abstract": null,
                        "time_start": "2021-10-28T17:05:00Z",
                        "time_end": "2021-10-28T17:18:00Z",
                        "uid": "a-visap-1020",
                        "youtube_video_id": "jlY4lKCuNQQ"
                    },
                    {
                        "type": "recorded",
                        "title": "Wanderlust: 3D Impressionism in Human Journeys",
                        "contributors": [
                            "Guangyu Du",
                            "Lei Dong",
                            "F\u00e1bio Duarte",
                            "Carlo Ratti"
                        ],
                        "abstract": null,
                        "time_start": "2021-10-28T17:18:00Z",
                        "time_end": "2021-10-28T17:26:00Z",
                        "uid": "a-visap-1042",
                        "youtube_video_id": "Pr654GAY0Kw"
                    },
                    {
                        "type": "recorded",
                        "title": "DaRt: Generative Art using Dimensionality Reduction Algorithms",
                        "contributors": [
                            "Rene Cutura",
                            "Katrin Angerbauer",
                            "Frank Heyen",
                            "Natalie Hube",
                            "Michael Sedlmair"
                        ],
                        "abstract": null,
                        "time_start": "2021-10-28T17:26:00Z",
                        "time_end": "2021-10-28T17:34:00Z",
                        "uid": "a-visap-1061",
                        "youtube_video_id": "pSvobL25VsY"
                    },
                    {
                        "type": "recorded",
                        "title": "Explore Mindfulness without Deflection: A Data Art Based on the Books of Songs",
                        "contributors": [
                            "Yifang Wang",
                            "Yifan Cao",
                            "Junxiu Tang",
                            "Yang Wang",
                            "Huamin Qu",
                            "Yingcai Wu"
                        ],
                        "abstract": null,
                        "time_start": "2021-10-28T17:34:00Z",
                        "time_end": "2021-10-28T17:42:00Z",
                        "uid": "a-visap-1039",
                        "youtube_video_id": "eQz6AhzjP9A"
                    },
                    {
                        "type": "live",
                        "title": "Live Q&A for Papers (15 min)",
                        "contributors": [
                            "Charles Perin"
                        ],
                        "abstract": null,
                        "time_start": "2021-10-28T17:42:00Z",
                        "time_end": "2021-10-28T18:02:00Z"
                    },
                    {
                        "type": "recorded",
                        "title": "Decoding \u00b7 Encoding \u2013 an exploration of data narrative in Tibetan characters",
                        "contributors": [
                            "Song Anqi",
                            "Xintong Song",
                            "Yuhao Chen",
                            "Guangyu Luo",
                            "Qiansheng Li"
                        ],
                        "abstract": null,
                        "time_start": "2021-10-28T18:02:00Z",
                        "time_end": "2021-10-28T18:07:00Z",
                        "uid": "a-visap-1037",
                        "youtube_video_id": "w00sDhgmLvw"
                    },
                    {
                        "type": "recorded",
                        "title": "Spectrographies: Decompositions of Music into Light",
                        "contributors": [
                            "Ignacio P\u00e9rez-Messina",
                            "Ilana Levin"
                        ],
                        "abstract": null,
                        "time_start": "2021-10-28T18:07:00Z",
                        "time_end": "2021-10-28T18:10:00Z",
                        "uid": "a-visap-1044",
                        "youtube_video_id": "1AY94U2Wcwc"
                    },
                    {
                        "type": "recorded",
                        "title": "Invisible Pixel\uff1aShort Video Narratives from Machine Perspective",
                        "contributors": [
                            "Junlin Zhu",
                            "Juanjuan Long",
                            "Yingjing Duan",
                            "Wenxuan Zhao"
                        ],
                        "abstract": null,
                        "time_start": "2021-10-28T18:10:00Z",
                        "time_end": "2021-10-28T18:14:00Z",
                        "uid": "a-visap-1035",
                        "youtube_video_id": "zapltlNw1-w"
                    },
                    {
                        "type": "live",
                        "title": "Q&A for Artist Talks (10 min and remaing time)",
                        "contributors": [
                            "Charles Perin"
                        ],
                        "abstract": null,
                        "time_start": "2021-10-28T18:17:00Z",
                        "time_end": "2021-10-28T18:30:00Z"
                    }
                ]
            },
            {
                "title": "VISAP Session 1",
                "session_id": "a-visap-visap1",
                "track": "room6",
                "schedule_image": "a-visap-visap1.png",
                "chair": [
                    "Yoon Chung Han"
                ],
                "organizers": [],
                "time_start": "2021-10-28T18:14:00Z",
                "time_end": "2021-10-28T18:17:00Z",
                "discord_category": "",
                "discord_channel": "",
                "discord_channel_id": 0,
                "youtube_url": null,
                "zoom_meeting": "",
                "zoom_password": "",
                "ff_link": "",
                "ff_playlist": "",
                "time_slots": [
                    {
                        "type": "recorded",
                        "title": "Side-view States ",
                        "contributors": [
                            "Emily Fuhrman"
                        ],
                        "abstract": null,
                        "time_start": "2021-10-28T18:14:00Z",
                        "time_end": "2021-10-28T18:17:00Z",
                        "uid": "a-visap-1063",
                        "youtube_video_id": "ry12WJ_vlfM"
                    }
                ]
            }
        ]
    },
    "x-memorial": {
        "event": "VIS Memorial",
        "long_name": "VIS Memorial",
        "event_type": "VIS Memorial",
        "event_description": "",
        "event_url": "",
        "sessions": [
            {
                "title": "Memorial Session",
                "session_id": "x-memorial-1",
                "track": "room2",
                "schedule_image": "x-memorial-1.png",
                "chair": [],
                "organizers": [],
                "time_start": "2021-10-26T19:00:00Z",
                "time_end": "2021-10-26T20:00:00Z",
                "discord_category": "",
                "discord_channel": "",
                "discord_channel_id": 0,
                "youtube_url": null,
                "zoom_meeting": "",
                "zoom_password": "",
                "ff_link": "",
                "ff_playlist": "",
                "time_slots": [
                    {
                        "type": null,
                        "title": "Memorial Session",
                        "contributors": "",
                        "abstract": null,
                        "time_start": "2021-10-26T19:00:00Z",
                        "time_end": "2021-10-26T20:00:00Z"
                    }
                ]
            }
        ]
    },
    "v-satellite": {
        "event": "VIS",
        "long_name": "VIS",
        "event_type": "VIS",
        "event_description": "",
        "event_url": "",
        "sessions": [
            {
                "title": "Satellite Shoutouts 1",
                "session_id": "v-satellite-1",
                "track": "room8",
                "schedule_image": "v-satellite-1.png",
                "chair": [],
                "organizers": [],
                "time_start": "2021-10-27T12:40:00Z",
                "time_end": "2021-10-27T13:00:00Z",
                "discord_category": "",
                "discord_channel": "",
                "discord_channel_id": 0,
                "youtube_url": null,
                "zoom_meeting": "",
                "zoom_password": "",
                "ff_link": "",
                "ff_playlist": "",
                "time_slots": [
                    {
                        "type": null,
                        "title": "Satellite Shoutouts 1",
                        "contributors": "",
                        "abstract": null,
                        "time_start": "2021-10-27T12:40:00Z",
                        "time_end": "2021-10-27T13:00:00Z"
                    }
                ]
            },
            {
                "title": "Satellite Shoutouts 2",
                "session_id": "v-satellite-1",
                "track": "room7",
                "schedule_image": "v-satellite-1.png",
                "chair": [],
                "organizers": [],
                "time_start": "2021-10-28T12:40:00Z",
                "time_end": "2021-10-28T13:00:00Z",
                "discord_category": "",
                "discord_channel": "",
                "discord_channel_id": 0,
                "youtube_url": null,
                "zoom_meeting": "",
                "zoom_password": "",
                "ff_link": "",
                "ff_playlist": "",
                "time_slots": [
                    {
                        "type": null,
                        "title": "Satellite Shoutouts 2",
                        "contributors": "",
                        "abstract": null,
                        "time_start": "2021-10-28T12:40:00Z",
                        "time_end": "2021-10-28T13:00:00Z"
                    }
                ]
            }
        ]
    },
    "v-short": {
        "event": "VIS Short Papers",
        "long_name": "VIS Short Papers",
        "event_type": "Paper Presentations",
        "event_description": "",
        "event_url": "",
        "sessions": [
            {
                "title": "Social Sciences, Software Tools, Journalism, and Storytelling",
                "session_id": "v-short-short5",
                "track": "room5",
                "schedule_image": "v-short-short5.png",
                "chair": [
                    "Siming Chen"
                ],
                "organizers": [],
                "time_start": "2021-10-27T13:00:00Z",
                "time_end": "2021-10-27T14:20:00Z",
                "discord_category": "",
                "discord_channel": "",
                "discord_channel_id": 0,
                "youtube_url": null,
                "zoom_meeting": "",
                "zoom_password": "",
                "ff_link": "",
                "ff_playlist": "",
                "time_slots": [
                    {
                        "type": "recorded",
                        "title": "GeoSneakPique: Visual autocompletion for geospatial queries",
                        "contributors": [
                            "Vidya Setlur",
                            "Sarah Battersby",
                            "Tracy Kam Hung Wong"
                        ],
                        "abstract": "How many crimes occurred in the city center? And exactly which part of town is the \u2018city center'? While location is at the heart of many data questions, geographic location can be difficult to specify in natural language (NL) queries. This is especially true when working with fuzzy cognitive regions or regions that may be defined based on data distributions instead of absolute administrative location (e.g., state, country). GeoSneakPique presents a novel method for using a mapping widget to support the NL query process, allowing users to specify location via direct manipulation with data-driven guidance on spatial distributions to help select the area of interest. Users receive feedback to help them evaluate and refine their spatial selection interactively and can save spatial definitions for re-use in subsequent queries. We conduct a qualitative evaluation of the GeoSneakPique that indicates the usefulness of the interface as well as opportunities for better supporting geospatial workflows in visual analysis tasks employing cognitive regions.",
                        "time_start": "2021-10-27T13:00:00Z",
                        "time_end": "2021-10-27T13:10:00Z",
                        "uid": "v-short-1047",
                        "youtube_video_id": "Kps5MM6qgfs"
                    },
                    {
                        "type": "recorded",
                        "title": "Atlas: Grammar-based Procedural Generation of Data Visualizations",
                        "contributors": [
                            "Zhicheng Liu",
                            "Chen Chen",
                            "Francisco Morales",
                            "Yishan Zhao"
                        ],
                        "abstract": "We present Atlas, a procedural grammar for constructing data visualizations. Unlike most visualization grammars which use declarative specifications to describe visualization components, Atlas exposes the generative process of a visualization through a set of concatenated high-level production rules. These rules define how graphical objects are created, transformed and coupled with abstract data. The input and output of each rule is clearly defined, allowing inspection of visualization states throughout the generative process. We demonstrate Atlas\u2019 expressiveness through a catalog of more than 40 visualization designs, and its extensibility through a case study of area-based visualizations.",
                        "time_start": "2021-10-27T13:10:00Z",
                        "time_end": "2021-10-27T13:20:00Z",
                        "uid": "v-short-1064",
                        "youtube_video_id": "RZD28DBM8gc"
                    },
                    {
                        "type": "recorded",
                        "title": "On The Potential of Zines as a Medium for Visualization",
                        "contributors": [
                            "Andrew M McNutt"
                        ],
                        "abstract": "Zines are a form of small-circulation self-produced publication often akin to a magazine. This free-form medium has a long history and has been used as means for personal or intimate expression, as a way for marginalized people to describe issues that are important to them, and as a venue for graphical experimentation. It would seem then that zines would make an ideal vehicle for the recent interest in applying feminist or humanist ideas to visualization. Yet, there has been little work combining visualization and zines. In this paper we explore the potential of this intersection by analyzing examples of zines that use data graphics and by describing the pedagogical value that they can have in a visualization classroom. In doing so, we argue that there are plentiful opportunities for visualization research and practice in this rich intersectional-medium.",
                        "time_start": "2021-10-27T13:20:00Z",
                        "time_end": "2021-10-27T13:30:00Z",
                        "uid": "v-short-1034",
                        "youtube_video_id": "UlSYt24CjUM"
                    },
                    {
                        "type": "recorded",
                        "title": "Narrative Sensemaking: Strategies for Narrative Maps Construction",
                        "contributors": [
                            "Brian Felipe Keith Norambuena",
                            "Tanushree Mitra",
                            "Chris North"
                        ],
                        "abstract": "Narrative sensemaking is a fundamental process to understand sequential information. Narrative maps are a visual representation framework that can aid analysts in their sensemaking process. Narrative maps allow analysts to understand the big picture of a narrative, uncover new relationships between events, and model connections between storylines. We seek to understand how analysts construct narrative maps in order to improve narrative map representation and extraction methods. We perform an experiment with a data set of news articles. Our main contribution is an analysis of how analysts construct narrative maps. The insights extracted from our study can be used to design narrative map visualizations, extraction algorithms, and visual analytics tools to support the sensemaking process.",
                        "time_start": "2021-10-27T13:30:00Z",
                        "time_end": "2021-10-27T13:40:00Z",
                        "uid": "v-short-1073",
                        "youtube_video_id": "VvJBvRZrryY"
                    },
                    {
                        "type": "recorded",
                        "title": "Text Visualization and Close Reading for Journalism with Storifier",
                        "contributors": [
                            "Nicole Sultanum",
                            "Anastasia Bezerianos",
                            "Fanny Chevalier"
                        ],
                        "abstract": "Journalistic inquiry often requires analysis and close study of large text collections around a particular topic. We argue that this practice could benefit from a more text- and reading-centered approach to journalistic text analysis, one that allows for a fluid transition between overview of entities of interest, the context of these entities in the text, down to the detailed documents they are extracted from. We present the design and development process of Storifier, informed by a close collaboration with a large francophone news office. We also discuss a case study on how our tool was used to analyze a text collection and publish a story.",
                        "time_start": "2021-10-27T13:40:00Z",
                        "time_end": "2021-10-27T13:50:00Z",
                        "uid": "v-short-1092",
                        "youtube_video_id": "y7c7blvHiN0"
                    },
                    {
                        "type": "recorded",
                        "title": "Conceptualizing Visual Analytic Interventions for Content Moderation",
                        "contributors": [
                            "Sahaj Vaidya",
                            "Jie Cai",
                            "Soumyadeep Basu",
                            "Azadeh Naderi",
                            "Donghee Yvette Wohn",
                            "Aritra Dasgupta"
                        ],
                        "abstract": "Our work introduces a visual analytic task abstraction framework for addressing data-driven problems in proactive content moderation. We also discuss the implications of the framework for influencing the future of transparent and communicative moderation practices through visual analytic solutions. As a next step, we plan to realize our proposed visual analytic tasks within existing content moderation workflows. We will also conduct empirical studies to evaluate the effectiveness of the visual analytic interventions and the resulting human-machine interfaces in reducing the cognitive load and emotional stress of content moderators.",
                        "time_start": "2021-10-27T13:50:00Z",
                        "time_end": "2021-10-27T14:00:00Z",
                        "uid": "v-short-1136",
                        "youtube_video_id": "zQrTpQb2iYI"
                    },
                    {
                        "type": "recorded",
                        "title": "How Learners Sketch Data Stories",
                        "contributors": [
                            "Rahul Bhargava",
                            "Dee Williams",
                            "Catherine D\u2019Ignazio"
                        ],
                        "abstract": "\\abstract{Learning data storytelling involves a complex web of skills. Professional and academic educational offerings typically focus on the computational skills required, but professionals in the field employ many non-technical methods. Sketching by hand on paper is a common practice. This paper introduces and classifes a corpus of 101 data sketches produced by participants as part of a guided learning activity in informal and formal settings. We manually coded each sketch against 12 metrics related to visual encodings, representations, and story structure. We find evidence for preferential use of positional and shape-based encodings, a wide variety of mixed symbolic and textual representations, and a high prevalence of stories comparing subsets of data. These findings contribute to our understanding of how learners sketch with data. They can inform tool design for learners, and help create educational programs that introduce novices to sketching practices from the field.",
                        "time_start": "2021-10-27T14:00:00Z",
                        "time_end": "2021-10-27T14:10:00Z",
                        "uid": "v-short-1157",
                        "youtube_video_id": "4fF3liDDhTM"
                    },
                    {
                        "type": "recorded",
                        "title": "Gemini\u00b2: Generating Keyframe-Oriented Animated Transitions Between Statistical Graphics",
                        "contributors": [
                            "Younghoon Kim",
                            "Jeffrey Heer"
                        ],
                        "abstract": "Complex animated transitions may be easier to understand when divided into separate, consecutive stages. However, effective staging requires careful attention to both animation semantics and timing parameters. We present Gemini\u00b2, a system for creating staged animations from a sequence of chart keyframes. Given only a start state and an end state, Gemini\u00b2 can automatically recommend intermediate keyframes for designers to consider. The Gemini\u00b2 recommendation engine leverages Gemini, our prior work, and GraphScape to itemize the given complex change into semantic edit operations and to recombine operations into stages with a guided order for clearly conveying the semantics. To evaluate Gemini\u00b2\u2019s recommendations, we conducted a human-subject study in which participants ranked recommended animations from both Gemini\u00b2 and Gemini. We find that Gemini\u00b2\u2019s animation recommendation ranking is well aligned with subjects\u2019 preferences, and Gemini\u00b2 can recommend favorable animations that Gemini cannot support.",
                        "time_start": "2021-10-27T14:10:00Z",
                        "time_end": "2021-10-27T14:20:00Z",
                        "uid": "v-short-1033",
                        "youtube_video_id": "tzIPFpiIVt8"
                    }
                ]
            },
            {
                "title": "AI+VIS",
                "session_id": "v-short-short1",
                "track": "room5",
                "schedule_image": "v-short-short1.png",
                "chair": [
                    "Aritra Dasgupta"
                ],
                "organizers": [],
                "time_start": "2021-10-27T15:00:00Z",
                "time_end": "2021-10-27T16:20:00Z",
                "discord_category": "",
                "discord_channel": "",
                "discord_channel_id": 0,
                "youtube_url": null,
                "zoom_meeting": "",
                "zoom_password": "",
                "ff_link": "",
                "ff_playlist": "",
                "time_slots": [
                    {
                        "type": "recorded",
                        "title": "CloudFindr: A Deep Learning Cloud Artifact Masker for Satellite DEM Data",
                        "contributors": [
                            "Kalina Borkiewicz",
                            "Viraj Shah",
                            "J.P. Naiman",
                            "Chuanyue Shen",
                            "Stuart Levy",
                            "Jeffrey D Carpenter"
                        ],
                        "abstract": "Artifact removal is an integral component of cinematic scientific visualization, and is especially challenging with big datasets in which artifacts are difficult to define. In this paper, we describe a method for creating cloud artifact masks which can be used to remove artifacts from satellite imagery using a combination of traditional image processing together with deep learning based on U-Net. Compared to previous methods, our approach does not require multi-channel spectral imagery but performs successfully on single-channel Digital Elevation Models (DEMs). DEMs are a representation of the topography of the Earth and have a variety applications including planetary science, geology, flood modeling, and city planning.",
                        "time_start": "2021-10-27T15:00:00Z",
                        "time_end": "2021-10-27T15:10:00Z",
                        "uid": "v-short-1013",
                        "youtube_video_id": "oyaladqlK-8"
                    },
                    {
                        "type": "recorded",
                        "title": "An Exploration And Validation of Visual Factors in Understanding Classification Rule Sets",
                        "contributors": [
                            "Jun Yuan",
                            "Oded Nov",
                            "Enrico Bertini"
                        ],
                        "abstract": "Rule sets are often used in Machine Learning (ML) as a way to communicate the model logic in settings where transparency and intelligibility are necessary. Rule sets are typically presented as a text-based list of logical statements (rules). Surprisingly, to date there has been limited work on exploring visual alternatives for presenting rules. In this paper, we explore the idea of designing alternative representations of rules, focusing on a number of visual factors we believe have a positive impact on rule readability and understanding. We then presents a user study exploring their impact. The results show that some design factors have a strong impact on how efficiently readers can process the rules while having minimal impact on accuracy. This work can help practitioners employ more effective solutions when using rules as a communication strategy to understand ML models.",
                        "time_start": "2021-10-27T15:10:00Z",
                        "time_end": "2021-10-27T15:20:00Z",
                        "uid": "v-short-1016",
                        "youtube_video_id": "Ld2766KfNc8"
                    },
                    {
                        "type": "recorded",
                        "title": "Fast & Accurate Gaussian Kernel Density Estimation",
                        "contributors": [
                            "Jeffrey Heer"
                        ],
                        "abstract": "Kernel density estimation (KDE) models a discrete sample of data as a continuous distribution, supporting the construction of visualizations such as violin plots, heatmaps, and contour plots. This paper draws on the statistics and image processing literature to survey efficient and scalable density estimation techniques for the common case of Gaussian kernel functions. We evaluate the accuracy and running time of these methods across multiple visualization contexts and find that the combination of linear binning and a recursive filter approximation by Deriche efficiently produces pixel-perfect estimates across a compelling range of kernel bandwidths.",
                        "time_start": "2021-10-27T15:20:00Z",
                        "time_end": "2021-10-27T15:30:00Z",
                        "uid": "v-short-1024",
                        "youtube_video_id": "AqHftDM4aHI"
                    },
                    {
                        "type": "recorded",
                        "title": "\"Why did my AI agent lose?\": Visual Analytics for Scaling Up AAR/AI",
                        "contributors": [
                            "Delyar Tabatabai",
                            "Anita Ruangrotsakun",
                            "Jed Irvine",
                            "Jonathan Dodge",
                            "Zeyad Shureih",
                            "Kin-Ho Lam",
                            "Margaret Burnett",
                            "Alan Paul Fern",
                            "Minsuk Kahng"
                        ],
                        "abstract": "How can we help domain-knowledgeable users who do not have expertise in AI analyze why an AI agent failed? Our research team previously developed a new structured process for such users to assess AI, called After-Action Review for AI (AAR/AI), consisting of a series of steps a human takes to assess an AI agent and formalize their understanding. In this paper, we investigate how the AAR/AI process can scale up to support reinforcement learning (RL) agents that operate in complex environments. We augment the AAR/AI process to be performed at three levels--episode-level, decision-level, and explanation-level--and integrate it into our redesigned visual analytics interface. We illustrate our approach through a usage scenario of analyzing why a RL agent lost in a complex real-time strategy game built with the StarCraft 2 engine. We believe integrating structured processes like AAR/AI into visualization tools can help visualization play a more critical role in AI interpretability.",
                        "time_start": "2021-10-27T15:30:00Z",
                        "time_end": "2021-10-27T15:40:00Z",
                        "uid": "v-short-1065",
                        "youtube_video_id": "kHRE7OHO748"
                    },
                    {
                        "type": "recorded",
                        "title": "VAINE: Visualization and AI for Natural Experiments",
                        "contributors": [
                            "Grace Guo",
                            "Maria Glenski",
                            "Zhuanyi Huang",
                            "Emily Saldanha",
                            "Alex Endert",
                            "Svitlana Volkova",
                            "Dustin L Arendt"
                        ],
                        "abstract": "Natural experiments are observational studies where the assignment of treatment conditions to different populations occur by chance ``in the wild''. Researchers from fields such as economics, healthcare, and the social sciences leverage natural experiments to conduct hypothesis testing and causal effect estimation for treatment and outcome variables that would otherwise be costly, infeasible, or unethical. In this paper, we introduce VAINE (Visualization and AI for Natural Experiments), a visual analytics tool for identifying and understanding natural experiments from observational data. We then demonstrate how VAINE can be used to validate causal relationships, estimate average treatment effects, and identify statistical phenomena such as Simpson\u2019s paradox through two use cases.",
                        "time_start": "2021-10-27T15:40:00Z",
                        "time_end": "2021-10-27T15:50:00Z",
                        "uid": "v-short-1111",
                        "youtube_video_id": "mIIqAD74iD4"
                    },
                    {
                        "type": "recorded",
                        "title": "Semantic Explanation of Interactive Dimensionality Reduction",
                        "contributors": [
                            "Yali Bian",
                            "Chris North",
                            "Eric Krokos",
                            "Sarah Joseph"
                        ],
                        "abstract": "Interactive dimensionality reduction helps analysts explore the high-dimensional data based on their personal needs and domain-specific problems. \nRecently, expressive nonlinear models are employed to support these tasks.\nHowever, the interpretation of these human-steered nonlinear models during human-in-the-loop analysis has not been explored.\nTo address this problem, we present a new visual explanation design called semantic explanation. \nSemantic explanation visualizes model behaviors in a manner that is similar to users' direct projection manipulations.\nThis design conforms to the spatial analytic process and enables analysts better understand the updated model in response to their interactions.\nWe propose a pipeline to empower interactive dimensionality reduction with semantic explanation using counterfactuals. \nBased on the pipeline, we implement a visual text analytics system with nonlinear dimensionality reduction powered by deep learning via the BERT model.\nWe demonstrate the efficacy of semantic explanation with two case studies of academic article exploration and intelligence analysis.",
                        "time_start": "2021-10-27T15:50:00Z",
                        "time_end": "2021-10-27T16:00:00Z",
                        "uid": "v-short-1161",
                        "youtube_video_id": "nEqjRjuDYnc"
                    },
                    {
                        "type": "recorded",
                        "title": "AdViCE: Aggregated Visual Counterfactual Explanations for Machine Learning Model Validation",
                        "contributors": [
                            "Steffen Holter",
                            "Oscar Alejandro Gomez",
                            "Jun Yuan",
                            "Enrico Bertini"
                        ],
                        "abstract": "Rapid improvements in the performance of machine learning models has pushed them to the forefront of data-driven decision-making. Meanwhile, the increased integration of these models into various application domains has further highlighted the need for greater interpretability and transparency. To identify problems such as bias, overfitting, and incorrect correlations, data scientists require tools that explain the mechanisms with which these model decisions are made. In this paper we introduce AdViCE, a visual analytics tool that aims to guide users in black-box model debugging and validation. The solution rests on two main visual user interface innovations: (1) an interactive visualization design that enables the comparison of decisions on user-defined data subsets; (2) an algorithm and visual design to compute and visualize counterfactual explanations - explanations that depict model outcomes when data features are perturbed from their original values. Furthermore, we provide an evaluation of the work through a number of use cases that demonstrate the capabilities and potential limitations of the proposed approach.",
                        "time_start": "2021-10-27T16:00:00Z",
                        "time_end": "2021-10-27T16:10:00Z",
                        "uid": "v-short-1178",
                        "youtube_video_id": "8ZKBATSGoA0"
                    },
                    {
                        "type": "recorded",
                        "title": "Contrastive Identification of Covariate Shift in Image Data",
                        "contributors": [
                            "Matthew Olson",
                            "Thuy-Vy Nguyen",
                            "Gaurav Dixit",
                            "Neale Ratzlaff",
                            "Weng-Keen Wong",
                            "Minsuk Kahng"
                        ],
                        "abstract": "Identifying covariate shift is crucial to making machine learning systems robust in the real world and for detecting training data biases that are not reflected in test data. However, detecting covariate shift is challenging, especially when the data is high-dimensional images, and when multiple types of localized covariate shift affect different subspaces of the data. Although automated techniques can be used to detect the existence of covariate shift, our goal is to help human users characterize the extent of covariate shift in large image datasets with visual interfaces that seamlessly integrate information obtained from the detection algorithms. In this paper, we design and evaluate a new visual analytics approach that facilitates the comparison of the local distributions of training and test data. We conduct a quantitative user study on multi-attribute facial data to compare two different learned low-dimensional latent representations (pretrained ImageNet CNN vs. density ratio) and two user analytic workflows (nearest-neighbor vs. cluster-to-cluster). Our results indicate that the latent representation of our density ratio model, combined with a nearest-neighbor comparison, is the most effective at helping humans identify covariate shift.",
                        "time_start": "2021-10-27T16:10:00Z",
                        "time_end": "2021-10-27T16:20:00Z",
                        "uid": "v-short-1050",
                        "youtube_video_id": "hotE2hDXKfQ"
                    }
                ]
            },
            {
                "title": "Graphs, Charts, and Perception",
                "session_id": "v-short-short2",
                "track": "room4",
                "schedule_image": "v-short-short2.png",
                "chair": [
                    "Tanja Blascheck"
                ],
                "organizers": [],
                "time_start": "2021-10-28T15:00:00Z",
                "time_end": "2021-10-28T16:20:00Z",
                "discord_category": "",
                "discord_channel": "",
                "discord_channel_id": 0,
                "youtube_url": null,
                "zoom_meeting": "",
                "zoom_password": "",
                "ff_link": "",
                "ff_playlist": "",
                "time_slots": [
                    {
                        "type": "recorded",
                        "title": "TimeElide: Visual Analysis of Non-Contiguous Time Series Slices",
                        "contributors": [
                            "Michael Oppermann",
                            "Luce Liu",
                            "Tamara Munzner"
                        ],
                        "abstract": "We introduce the design and implementation of TimeElide, a visual analysis tool for the novel data abstraction of non-contiguous time series slices, namely time intervals that contain a sequence of time-value pairs but are not adjacent to each other. This abstraction is relevant for analysis tasks where time periods of interest are known in advance or inferred from the data, rather than discovered through open-ended visual exploration. We present a visual encoding design space as an underpinning of TimeElide, and the new sparkbox technique for visualizing fine and coarse grained temporal structures within one view. Datasets from different domains and with varying characteristics guided the development and their analysis provides preliminary evidence of TimeElide's utility.",
                        "time_start": "2021-10-28T15:00:00Z",
                        "time_end": "2021-10-28T15:10:00Z",
                        "uid": "v-short-1025",
                        "youtube_video_id": "5bQNBTsQAVo"
                    },
                    {
                        "type": "recorded",
                        "title": "Semantic Resizing of Charts Through Generalization: A Case Study with Line Charts",
                        "contributors": [
                            "Vidya Setlur",
                            "Haeyong Chung"
                        ],
                        "abstract": "Inspired by cartographic generalization principles, we present a generalization technique for rendering line charts at different sizes, preserving the important semantics of the data at that display size.The algorithm automatically determines the generalization operators to be applied at that size based on spatial density, distance, and the semantic importance of the various visualization elements in the line chart. A qualitative evaluation of the prototype that implemented the algorithm indicates that the generalized line charts pre-served the general data shape, while minimizing visual clutter. We identify future opportunities where generalization can be extended and applied to other chart types and visual analysis authoring tools.",
                        "time_start": "2021-10-28T15:10:00Z",
                        "time_end": "2021-10-28T15:20:00Z",
                        "uid": "v-short-1048",
                        "youtube_video_id": "Jgv5tslJzUs"
                    },
                    {
                        "type": "recorded",
                        "title": "Bayesian Modelling of Alluvial Diagram Complexity",
                        "contributors": [
                            "Anjana Arunkumar",
                            "Shashank Ginjpalli",
                            "Chris Bryan"
                        ],
                        "abstract": "Alluvial diagrams are a popular technique for visualizing flow and relational data. However, successfully reading and interpreting the data shown in an alluvial diagram is likely influenced by factors such as data volume, complexity, and chart layout. To understand how alluvial diagram consumption is impacted by its visual features, we conduct two crowdsourced user studies with a set of alluvial diagrams of varying complexity, and examine (i) participant performance on analysis tasks, and (ii) the perceived complexity of the charts. Using the study results, we employ Bayesian modelling to predict participant classification of diagram complexity. We find that, while multiple visual features are important in contributing to alluvial diagram complexity, interestingly the most important feature changes depending on the type of complexity being modeled, i.e. task complexity vs. perceived complexity.",
                        "time_start": "2021-10-28T15:20:00Z",
                        "time_end": "2021-10-28T15:30:00Z",
                        "uid": "v-short-1082",
                        "youtube_video_id": "9dsE16NTGiE"
                    },
                    {
                        "type": "recorded",
                        "title": "When Red Means Good, Bad, or Canada: Exploring People's Reasoning for Choosing Color Palettes",
                        "contributors": [
                            "Jarryullah Ahmad",
                            "Elaine Huynh",
                            "Fanny Chevalier"
                        ],
                        "abstract": "Color palette selection is an essential aspect of visualization creation and design, influencing viewer interpretation of the underlying data and evoking emotions in the viewer. Best practices, grounded in perceptual science and visual arts practice, form the basis of recommendation tools to support palette design and choice, but it is unclear how the general public reconciles the varied facets of color design. Does their decision-making align with established best practices? What factors do they take into consideration? Through a crowdsourced study with 63 participants, we find that the majority of palette choices are perceptually-motivated, but other factors such as semantic associations and bias also play a role. We identify some flaws in participant reasoning, highlight clashes in opinions, and present some implications for future work in recommendation tools.",
                        "time_start": "2021-10-28T15:30:00Z",
                        "time_end": "2021-10-28T15:40:00Z",
                        "uid": "v-short-1151",
                        "youtube_video_id": "pp8y_6twHI8"
                    },
                    {
                        "type": "recorded",
                        "title": "Does the Layout Really Matter? A Study on Visual Model Accuracy Estimation",
                        "contributors": [
                            "Nicolas Grossmann",
                            "J\u00fcrgen Bernard",
                            "Michael Sedlmair",
                            "Manuela Waldner"
                        ],
                        "abstract": "In visual interactive labeling, users iteratively assign labels to data items until the machine model reaches an acceptable accuracy. A crucial step of this process is to inspect the model's accuracy and decide whether it is necessary to label additional elements. In scenarios with no or very little labeled data, visual inspection of the predictions is required. Similarity-preserving scatterplots created through a dimensionality reduction algorithm are a common visualization that is used in these cases. Previous studies investigated the effects of layout and image complexity on tasks like labeling. However, model evaluation has not been studied systematically. We present the results of an experiment studying the influence of image complexity and visual grouping of images on model accuracy estimation. We found that users outperform traditional automated approaches when estimating a model's accuracy. Furthermore, while the complexity of images impacts the overall performance, the layout of the items in the plot has little to no effect on estimations.",
                        "time_start": "2021-10-28T15:40:00Z",
                        "time_end": "2021-10-28T15:50:00Z",
                        "uid": "v-short-1170",
                        "youtube_video_id": "eB2g6-1pyZ0"
                    },
                    {
                        "type": "recorded",
                        "title": "Histogram binning revisited with a focus on human perception",
                        "contributors": [
                            "Raphael Sahann",
                            "Torsten M\u00f6ller",
                            "Johanna Schmidt"
                        ],
                        "abstract": "This paper presents a quantitative user study to evaluate how well users can visually perceive the underlying data distribution from a histogram representation. We used histograms with different sample and bin sizes and four different distributions (uniform, normal, bimodal, and gamma). The study results confirm that, in general, more bins correlate with fewer errors by the viewers. However, upon a certain number of bins, the error rate cannot be improved by adding more bins. By comparing our study results with the outcomes of existing mathematical models for histogram binning (e.g., Sturges\u2019 formula, Scott\u2019s normal reference rule, the Rice Rule, or Freedman\u2013Diaconis\u2019 choice), we can see that most of them overestimate the number of bins necessary to make the distribution visible to a human viewer.",
                        "time_start": "2021-10-28T15:50:00Z",
                        "time_end": "2021-10-28T16:00:00Z",
                        "uid": "v-short-1171",
                        "youtube_video_id": "TIXfy2CRaNA"
                    },
                    {
                        "type": "recorded",
                        "title": "Fixation and Creativity in Data Visualization Design: Experiences and Perspectives of Practitioners",
                        "contributors": [
                            "Paul Parsons",
                            "Prakash Chandra Shukla",
                            "Chorong Park"
                        ],
                        "abstract": "Data visualization design often requires creativity, and research is needed to understand its nature and means for promoting it. However, the current visualization literature on creativity is not well developed, especially with respect to the experiences of professional data visualization designers. We conducted semi-structured interviews with 15 data visualization practitioners, focusing on a specific aspect of creativity known as \\textit{design fixation}. Fixation occurs when designers adhere blindly or prematurely to a set of ideas that limit creative outcomes. We present practitioners' experiences and perspectives from their own design practice, specifically focusing on their views of (i) the nature of fixation, (ii) factors encouraging fixation, and (iii) factors discouraging fixation in a data visualization context. We identify opportunities for future research related to chart recommendations, inspiration, and perspective shifts in data visualization design.",
                        "time_start": "2021-10-28T16:00:00Z",
                        "time_end": "2021-10-28T16:10:00Z",
                        "uid": "v-short-1205",
                        "youtube_video_id": "PKpkr6HhMn8"
                    },
                    {
                        "type": "recorded",
                        "title": "Towards a Survey on Static and Dynamic Hypergraph Visualizations",
                        "contributors": [
                            "Maximilian T. Fischer",
                            "Alexander Frings",
                            "Daniel Keim",
                            "Daniel Seebacher"
                        ],
                        "abstract": "Leveraging hypergraph structures to model advanced processes has gained much attention over the last few years in many areas, ranging from protein-interaction in computational biology to image retrieval using machine learning. Hypergraph models can provide a more accurate representation of the underlying processes while reducing the overall number of links compared to regular representations. However, interactive visualization methods for hypergraphs and hypergraph-based models have rarely been explored or systematically analyzed. This paper reviews the existing research landscape for hypergraph and hypergraph model visualizations and assesses the currently employed techniques. We provide an overview and a categorization of proposed approaches, focusing on performance, scalability, interaction support, successful evaluation, and the ability to represent different underlying data structures, including a recent demand for a temporal representation of interaction networks and their improvements beyond graph-based methods. Lastly, we discuss the different strengths and weaknesses of the individual approaches and give an insight into the future challenges arising in this emerging research field.",
                        "time_start": "2021-10-28T16:10:00Z",
                        "time_end": "2021-10-28T16:20:00Z",
                        "uid": "v-short-1012",
                        "youtube_video_id": "WFJcN6K3LzQ"
                    }
                ]
            },
            {
                "title": "Applications",
                "session_id": "v-short-short4",
                "track": "room4",
                "schedule_image": "v-short-short4.png",
                "chair": [
                    "Michael Krone"
                ],
                "organizers": [],
                "time_start": "2021-10-28T17:00:00Z",
                "time_end": "2021-10-28T18:20:00Z",
                "discord_category": "",
                "discord_channel": "",
                "discord_channel_id": 0,
                "youtube_url": null,
                "zoom_meeting": "",
                "zoom_password": "",
                "ff_link": "",
                "ff_playlist": "",
                "time_slots": [
                    {
                        "type": "recorded",
                        "title": "A Visual Analytics System for Water Distribution System Optimization",
                        "contributors": [
                            "Yiran Li",
                            "Erin Musabandesu",
                            "Takanori Fujiwara",
                            "Frank J. Loge",
                            "Kwan-Liu Ma"
                        ],
                        "abstract": "The optimization of water distribution systems (WDSs) is vital to minimize energy costs required for their operations. A principal approach taken by researchers is identifying an optimal scheme for water pump controls through examining computational simulations of WDSs. However, due to a large number of possible control combinations and the complexity of WDS simulations, it remains non-trivial to identify the best pump controls by reviewing the simulation results. To address this problem, we design a visual analytics system that helps understand relationships between simulation inputs and outputs towards better optimization. Our system incorporates interpretable machine learning as well as multiple linked visualizations to capture essential input-output relationships from complex WDS simulations. We demonstrate our system's effectiveness through a practical case study and evaluate its usability through expert reviews. Our results show that our system can lessen the burden of analysis and assist in determining optimal operating schemes.",
                        "time_start": "2021-10-28T17:00:00Z",
                        "time_end": "2021-10-28T17:10:00Z",
                        "uid": "v-short-1032",
                        "youtube_video_id": "_pPMrcDKYSo"
                    },
                    {
                        "type": "recorded",
                        "title": "CellProfiler Analyst Web (CPAW) - Exploration, analysis, and classification of biological images on the web",
                        "contributors": [
                            "Bella Baidak",
                            "Yahiya Hussain",
                            "Emma Kelminson",
                            "Thouis Ray Jones",
                            "Loraine Franke",
                            "Daniel Haehn"
                        ],
                        "abstract": "CellProfiler Analyst has enabled the scientific research community to explore image-based data and classify complex biological phenotypes through an interactive user interface since its release in 2008. This paper describes CellProfiler Analyst Web (CPAW), a newly redesigned and web-based version of the software, allowing for greater accessibility, quicker setup, and facilitates a simple overall workflow for users. Installation and managing new versions has been challenging and time-consuming, historically, for CellProfiler Analyst. CPAW is an alternative that ensures installation and future updates are not a hassle to the user. CPAW ports the core iteration loop of CPA to a pure server-less browser environment using modern web-development technologies, allowing computationally heavy activities, like machine learning, to occur without freezing the UI. With a setup as simple as navigating to a website, CPAW presents a clean user interface to the user to refine their classifier and explore phenotypic data with ease. We evaluated both the old and the new version of the software in an extensive domain expert study. We found that users could complete the essential classification tasks in CPAW and CPA 3.0 with the same efficiency. Additionally, users completed the tasks 20 percent faster using CPAW compared to CPA 3.0.\nThe code of CellProfiler Analyst Web is open-source and available at \\url{https://mpsych.github.io/CellProfilerAnalystWeb/",
                        "time_start": "2021-10-28T17:10:00Z",
                        "time_end": "2021-10-28T17:20:00Z",
                        "uid": "v-short-1035",
                        "youtube_video_id": "qA-RwSe_mG8"
                    },
                    {
                        "type": "recorded",
                        "title": "Inspecting the Process of Bank Credit Rating via Visual Analytics",
                        "contributors": [
                            "Qiangqiang Liu",
                            "Tangzhi Ye",
                            "Zhihua Zhu",
                            "Xiaojuan Ma",
                            "Quan Li"
                        ],
                        "abstract": "Bank credit rating refers to classify commercial banks into different levels based on publicly disclosed and internal information, serving as an important input in financial risk management. However, experts still have a vague idea of exploring and comparing different bank credit rating schemes. A loose connection between subjective and quantitative analysis and difficulties for the experts in determining appropriate indicator weights obscure understanding of bank credit ratings. Furthermore, existing models fail to consider bank types by just applying a unified indicator weight set to all banks. We propose RatingVis to assist experts in exploring and comparing different bank credit rating schemes. It supports interactively inferring indicator weights for banks by involving domain knowledge and considers bank types in the analysis loop. A real-world case study verifies the efficacy of RatingVis.",
                        "time_start": "2021-10-28T17:20:00Z",
                        "time_end": "2021-10-28T17:30:00Z",
                        "uid": "v-short-1072",
                        "youtube_video_id": "fH2Z2K7Tf1c"
                    },
                    {
                        "type": "recorded",
                        "title": "Where and Why is My Bot Failing? A Visual Analytics Approach for Investigating Failures in Chatbot Conversation Flows",
                        "contributors": [
                            "Avi Yaeli",
                            "Sergey Zeltyn"
                        ],
                        "abstract": "The ongoing coronavirus pandemic has accelerated the adoption of AI-powered task-oriented chatbots by businesses and healthcare organizations. Despite advancements in chatbot platforms, implementing a successful and effective bot is still challenging and requires a lot of manual work. There is a strong need for tools to help conversation analysts quickly identify problem areas and, consequently, introduce changes to chatbot design. We present a visual analytics approach and tool for conversation analysts to identify and assess common patterns of failure in conversation flows. We focus on two key capabilities: path flow analysis and root cause analysis. Interim evaluation results from applying our tool in real-world customer production projects are presented.",
                        "time_start": "2021-10-28T17:30:00Z",
                        "time_end": "2021-10-28T17:40:00Z",
                        "uid": "v-short-1105",
                        "youtube_video_id": "ke0N2A1XJTs"
                    },
                    {
                        "type": "recorded",
                        "title": "AiR: An Augmented Reality Application for Visualizing Air Pollution",
                        "contributors": [
                            "Noble Saji Mathews",
                            "Sridhar Chimalakonda",
                            "Suresh Jain"
                        ],
                        "abstract": "In order to effectively combat Air Pollution, it is necessary for the government and the community to work together. Easily comprehensible visualizations can play a major role in drawing public attention and spreading awareness about seemingly intangible air pollution. Considering the widespread usage of Android-based devices, in this paper, we propose an Augmented Reality based application called AiR, to help users to visualize pollutants in the air and to create an immersive user experience. AiR aims to interactively engage a wide variety of users and create awareness without overwhelming them with data. AiR visualizes 12 pollutants through the use of unique AR generated particles and chemical models. We demonstrate our application on pollution data by CPCB from various weather stations across India collected over the initial lockdown period due to COVID-19 in India.",
                        "time_start": "2021-10-28T17:40:00Z",
                        "time_end": "2021-10-28T17:50:00Z",
                        "uid": "v-short-1139",
                        "youtube_video_id": "88Bk-2PMffM"
                    },
                    {
                        "type": "recorded",
                        "title": "ConVIScope: Visual Analytics for Exploring Patient Conversations",
                        "contributors": [
                            "Raymond Li",
                            "Enamul Hoque",
                            "Giuseppe Carenini",
                            "Richard Lester",
                            "Raymond Chau"
                        ],
                        "abstract": "The proliferation of text messaging for mobile health is generating a large amount of doctor patient conversations that can be extremely valuable to health care professionals.We present ConVIScope, a visual text analytic system that tightly integrates interactive visualization with natural language processing in analyzing doctor-patient conversations. ConVIScope was developed in collaboration with health-care professionals following a user-centered iterative design. Case studies with six domain experts suggest the potential utility of ConVIScope and reveal lessons for further developments.",
                        "time_start": "2021-10-28T17:50:00Z",
                        "time_end": "2021-10-28T18:00:00Z",
                        "uid": "v-short-1169",
                        "youtube_video_id": "lGswaFJy3cw"
                    },
                    {
                        "type": "recorded",
                        "title": "Visually Connecting Historical Figures Through Event Knowledge Graphs",
                        "contributors": [
                            "Shahid Latif",
                            "Shivam Agarwal",
                            "Simon Gottschalk",
                            "Carina Chrosch",
                            "Yanick Christian Tchenko",
                            "Felix Feit",
                            "Johannes Jahn",
                            "Tobias Braun",
                            "Elena Demidova",
                            "Fabian Beck"
                        ],
                        "abstract": "To research lives of historical figures and their interactions with other famous people of the same era, users have to usually go through long text documents. Knowledge graphs store information about entities such as historical figures and their relationships---indirectly through shared events---in a structured manner. We develop a visualization system, \\system{}, for analyzing the intertwined lives of historical figures based on the events they participated. The users' query is parsed for identifying named entities and related data is queried from an event knowledge graph. While a short textual answer to the users' query is generated using the GPT-3 language model, various linked visualizations provide context, display additional information related to the query, and allow in-depth exploration.",
                        "time_start": "2021-10-28T18:00:00Z",
                        "time_end": "2021-10-28T18:10:00Z",
                        "uid": "v-short-1087",
                        "youtube_video_id": "FG69qFHicYE"
                    },
                    {
                        "type": "recorded",
                        "title": "Understanding the Effects of Visualizing Missing Values on Visual Data Exploration",
                        "contributors": [
                            "Hayeong Song",
                            "Yu Fu",
                            "Bahador Saket",
                            "John Stasko"
                        ],
                        "abstract": "When performing data analysis, people often confront data sets containing missing values. We conducted an empirical study to understand the effect of visualizing those missing values on participants\u2019 decision-making processes while performing a visual data exploration task. More specifically, our study participants purchased a hypothetical portfolio of stocks based on a data set where some stocks had missing values for attributes such as PE ratio, beta, and EPS. The experiment used scatterplots to communicate the stock data. For one group of participants, stocks with missing values simply were not shown, while the second group saw such stocks depicted with estimated values as points with error bars. We measured participants\u2019 cognitive load involved in decision-making with data with missing values. Our results indicate that their decision-making workflow was different across two conditions.",
                        "time_start": "2021-10-28T18:10:00Z",
                        "time_end": "2021-10-28T18:20:00Z",
                        "uid": "v-short-1160",
                        "youtube_video_id": "Kzv9AnsazAY"
                    }
                ]
            },
            {
                "title": "Mathematics, Topology, and Rendering",
                "session_id": "v-short-short3",
                "track": "room4",
                "schedule_image": "v-short-short3.png",
                "chair": [
                    "Bei Wang"
                ],
                "organizers": [],
                "time_start": "2021-10-29T13:00:00Z",
                "time_end": "2021-10-29T14:20:00Z",
                "discord_category": "",
                "discord_channel": "",
                "discord_channel_id": 0,
                "youtube_url": null,
                "zoom_meeting": "",
                "zoom_password": "",
                "ff_link": "",
                "ff_playlist": "",
                "time_slots": [
                    {
                        "type": "recorded",
                        "title": "Time-Varying Fuzzy Contour Trees",
                        "contributors": [
                            "Anna-Pia Lohfink",
                            "Frederike Gartzky",
                            "Florian Wetzels",
                            "Luisa Vollmer",
                            "Christoph Garth"
                        ],
                        "abstract": "We present a holistic, topology-based visualization technique for spatial time series data based on an adaptation of Fuzzy Contour Trees. \nCommon analysis approaches for time dependent scalar fields identify and track specific features. To give a more general overview of the data, we extend Fuzzy Contour Trees, from the visualization and simultaneous analysis of the topology of multiple scalar fields, to time dependent scalar fields. The resulting time-varying Fuzzy Contour Trees allow the comparison of multiple time steps that are not required to be consecutive. We provide specific interaction and navigation possibilities that allow the exploration of individual time steps and time windows in addition to the behavior of the contour trees over all time steps. To achieve this, we reduce an existing alignment to multiple sub-alignments and adapt the Fuzzy Contour Tree-layout to continuously reflect changes and similarities in the sub-alignments. We apply time-varying Fuzzy Contour Trees to different real-world data sets and demonstrate their usefulness.",
                        "time_start": "2021-10-29T13:00:00Z",
                        "time_end": "2021-10-29T13:10:00Z",
                        "uid": "v-short-1030",
                        "youtube_video_id": "spfkv6zUa3o"
                    },
                    {
                        "type": "recorded",
                        "title": "Ray-traced Shell Traversal of Tetrahedral Meshes for Direct Volume Visualization",
                        "contributors": [
                            "Alper \u015eah\u0131stan",
                            "Serkan Demirci",
                            "Nate Morrical",
                            "Stefan Zellmann",
                            "Aytek Aman",
                            "Ingo Wald",
                            "Ugur Gudukbay"
                        ],
                        "abstract": "A well-known method for rendering unstructured volumetric data is tetrahedral marching tet marching, where rays are marched through a series of tetrahedral elements. However, existing tet marching techniques do not easily generalize to rays with arbitrary origin and direction required for advanced shading effects or non-convex meshes. Additionally, the memory footprint of these methods may exceed GPU memory limits. Interactive performance and high image quality are opposing goals. Our approach significantly lowers the burden to render unstructured datasets with high image fidelity while maintaining real-time and interactive performance even for large datasets. To this end, we leverage hardware-accelerated ray tracing to find entry and exit faces for a given ray into a volume and utilize a compact mesh representation to enable the efficient marching of arbitrary rays, thus allowing for advanced shading effects that ultimately yields more convincing and grounded images. Our approach is also robust, supporting both convex and non-convex unstructured meshes. We show that our method achieves interactive rates even with moderately-sized datasets while secondary effects are applied.",
                        "time_start": "2021-10-29T13:10:00Z",
                        "time_end": "2021-10-29T13:20:00Z",
                        "uid": "v-short-1113",
                        "youtube_video_id": "HUxFoWr-wTg"
                    },
                    {
                        "type": "recorded",
                        "title": "Segmentation driven Peeling for Visual Analysis of Electronic Transitions",
                        "contributors": [
                            "Mohit Sharma",
                            "Talha Bin Masood",
                            "Signe Sidwall Thygesen",
                            "Mathieu Linares",
                            "Ingrid Hotz",
                            "Vijay Natarajan"
                        ],
                        "abstract": "Electronic transitions in molecules due to absorption or emission of light is a complex quantum mechanical process. Their study plays an important role in the design of novel materials. A common yet challenging task in the study is to determine the nature of those electronic transitions, i.e. which subgroups of the molecule are involved in the transition by donating or accepting electrons, followed by an investigation of the variation in the donor-acceptor behavior for different transitions or conformations of the molecules. In this paper, we present a novel approach towards the study of electronic transitions based on the visual analysis of a bivariate field, namely the electron density in the hole and particle Natural Transition Orbital (NTO). The visual analysis focuses on the continuous scatter plots (CSPs) of the bivariate field linked to their spatial domain. The method supports selections in the CSP visualized as fiber surfaces in the spatial domain, the grouping of atoms, and segmentation of the density fields to peel the CSP. This peeling operator is central to the visual analysis process and helps identify donors and acceptors. We study different molecular systems, identifying local excitation and charge transfer excitations to demonstrate the utility of the method.",
                        "time_start": "2021-10-29T13:20:00Z",
                        "time_end": "2021-10-29T13:30:00Z",
                        "uid": "v-short-1122",
                        "youtube_video_id": "dhP0Y_oGHoY"
                    },
                    {
                        "type": "recorded",
                        "title": "Exact Analytical Parallel Vectors",
                        "contributors": [
                            "Hanqi Guo",
                            "Tom Peterka"
                        ],
                        "abstract": "This paper demonstrates that parallel vector curves are piecewise cubic rational curves in 3D piecewise linear vector fields. Parallel vector curves---loci of points where two vector fields are parallel---have been widely used to extract features including ridges, valleys, and vortex core lines in scientific data. We define the term generalized and underdetermined eigensystem in the form of Ax+a=Bx+b in order to derive the piecewise rational representation of 3D parallel vector curves. We discuss how singularities of the rationals lead to different types of intersections with tetrahedral cells.",
                        "time_start": "2021-10-29T13:30:00Z",
                        "time_end": "2021-10-29T13:40:00Z",
                        "uid": "v-short-1128",
                        "youtube_video_id": "LjCQ4h3j9RY"
                    },
                    {
                        "type": "recorded",
                        "title": "Uncertainty Visualization of the Marching Squares and Marching Cubes Topology Cases",
                        "contributors": [
                            "Tushar M. Athawale",
                            "Sudhanshu Sane",
                            "Chris R. Johnson"
                        ],
                        "abstract": "Marching squares (MS) and marching cubes (MC) are widely used algorithms for level-set visualization of scientific data. In this paper, we address the challenge of uncertainty visualization of the topology cases of the MS and MC algorithms for uncertain scalar field data sampled on a uniform grid. The visualization of the MS and MC topology cases for uncertain data is challenging due to their exponential nature and a possibility of multiple topology cases per cell of a grid. We propose the topology case count and entropy-based techniques for quantifying uncertainty in the topology cases of the MS and MC algorithms when noise in data is modeled with probability distributions. We demonstrate the applicability of our techniques for independent and correlated uncertainty assumptions. We visualize the quantified topological uncertainty via color mapping proportional to uncertainty, as well as with interactive probability queries in the MS case and entropy isosurfaces in the MC case. We demonstrate the utility of our uncertainty quantification framework in identifying the isovalues exhibiting relatively high topological uncertainty. We illustrate the effectiveness of our techniques via results on synthetic, simulation, and hixel datasets.",
                        "time_start": "2021-10-29T13:40:00Z",
                        "time_end": "2021-10-29T13:50:00Z",
                        "uid": "v-short-1129",
                        "youtube_video_id": "H-BcDyeBZ_M"
                    },
                    {
                        "type": "recorded",
                        "title": "Intercept Graph: An Interactive Radial Visualization for Comparison of State Changes",
                        "contributors": [
                            "Shaolun Ruan",
                            "Yong Wang",
                            "Qiang Guan"
                        ],
                        "abstract": "State change comparison of multiple data items is often necessary in multiple application domains, such as medical science, financial engineering, sociology, etc. Slope graphs and grouped bar charts have been widely used to show a \"before-and-after'' story of different data states and indicate their changes. However, they visualize state changes as either slope or difference of bars, which has been proved less effective for quantitative comparison. Also, both visual designs suffer from visual clutter issues with an increasing number of data items. In this paper, we propose Intercept Grpah, a novel visual design to facilitate effective interactive comparison of state changes. Specifically, a radial design is proposed to visualize the starting and ending states of each data item on two concentric circles and the chord length explicitly encodes the \"state change''. By interactively adjusting the inner circle radius, Intercept Grpah can smoothly filter out the large state changes and magnify the difference between similar state changes, mitigating the visual clutter issues and enhancing the effective comparison of state changes. We conducted case studies through comparing Intercept Grpah with slope graphs and grouped bar charts on real datasets to demonstrate the effectiveness of Intercept Grpah.",
                        "time_start": "2021-10-29T13:50:00Z",
                        "time_end": "2021-10-29T14:00:00Z",
                        "uid": "v-short-1146",
                        "youtube_video_id": "X4Rtu3Kq6cg"
                    },
                    {
                        "type": "recorded",
                        "title": "Automatic Y-axis Rescaling in Dynamic Visualizations",
                        "contributors": [
                            "Jacob Fisher",
                            "Remco Chang",
                            "Eugene Wu"
                        ],
                        "abstract": "Animated and interactive data visualizations dynamically change the data rendered in a visualization (e.g., bar chart). As the data changes, the y-axis may need to be rescaled as the domain of the data changes. Each axis rescaling potentially improves the readability of the current chart, but may also disorient the user. In contrast to static visualizations, where there is considerable literature to help choose the appropriate y-axis scale, there is a lack of guidance about how and when rescaling should be used in dynamic visualizations. Existing visualization systems and libraries adapt a fixed global y-axis, or rescale every time the data changes. Yet, professional visualizations, such as in data journalism, do not adopt either strategy.They instead carefully and manually choose when to rescale based on the analysis task and data. To this end, we conduct a series of Mechanical Turk experiments to study the potential of dynamic axis rescaling, the factors that affect its effectiveness. We find that the appropriate rescaling policy is both task- and data-dependent, and we do not find one clear policy choice for all situations.",
                        "time_start": "2021-10-29T14:00:00Z",
                        "time_end": "2021-10-29T14:10:00Z",
                        "uid": "v-short-1168",
                        "youtube_video_id": "4gVA-T08Dis"
                    },
                    {
                        "type": "recorded",
                        "title": "A Mixed-Initiative Visual Analytics Approach for Qualitative Causal Modeling",
                        "contributors": [
                            "Fahd Husain",
                            "Pascale Proulx",
                            "Meng-Wei Chang",
                            "Rosa Romero-G\u00f3mez",
                            "Holland Marie Vasquez"
                        ],
                        "abstract": "Modeling complex systems is a time-consuming, difficult and fragmented task, often requiring the analyst to work with disparate data, a variety of models, and expert knowledge across diverse domains. Applying a user-centered design process, we developed Causemos, an integrated visual analytics approach that allows analysts to rapidly assemble qualitative causal models of complex socio-natural systems. Through a mixed-initiative interaction workflow, this approach facilitates the construction, exploration, and curation of qualitative models bringing together data across diverse domains. Referencing a recent user evaluation, we demonstrate Causemos\u2019 ability to interactively enrich user mental models and accelerate qualitative model building.",
                        "time_start": "2021-10-29T14:10:00Z",
                        "time_end": "2021-10-29T14:20:00Z",
                        "uid": "v-short-1054",
                        "youtube_video_id": "G0sH4SM0GkE"
                    }
                ]
            }
        ]
    },
    "v-panels": {
        "event": "VIS Panels",
        "long_name": "VIS Panels",
        "event_type": "Panel",
        "event_description": "",
        "event_url": "",
        "sessions": [
            {
                "title": "Wait...when did we sign up to be economists?",
                "session_id": "v-panels-panel4",
                "track": "room6",
                "schedule_image": "v-panels-panel4.png",
                "chair": [],
                "organizers": [],
                "time_start": "2021-10-27T13:00:00Z",
                "time_end": "2021-10-27T14:30:00Z",
                "discord_category": "",
                "discord_channel": "",
                "discord_channel_id": 0,
                "youtube_url": null,
                "zoom_meeting": "",
                "zoom_password": "",
                "ff_link": "",
                "ff_playlist": "",
                "time_slots": [
                    {
                        "type": "live",
                        "title": "Wait...when did we sign up to be economists?",
                        "contributors": [
                            "Derya Akbaba",
                            "Kiran Gadhave",
                            "Salph Savage",
                            "Danielle Albers Szafir",
                            "Alexander Lex",
                            "Steve Haroz",
                            "Khairi Reda"
                        ],
                        "abstract": null,
                        "time_start": "2021-10-27T13:00:00Z",
                        "time_end": "2021-10-27T14:30:00Z"
                    }
                ]
            },
            {
                "title": "What is the Role of VIS in Combating COVID-19?",
                "session_id": "v-panels-panel2",
                "track": "room6",
                "schedule_image": "v-panels-panel2.png",
                "chair": [],
                "organizers": [],
                "time_start": "2021-10-27T15:00:00Z",
                "time_end": "2021-10-27T16:30:00Z",
                "discord_category": "",
                "discord_channel": "",
                "discord_channel_id": 0,
                "youtube_url": null,
                "zoom_meeting": "",
                "zoom_password": "",
                "ff_link": "",
                "ff_playlist": "",
                "time_slots": [
                    {
                        "type": "live",
                        "title": "What is the Role of VIS in Combating COVID-19?",
                        "contributors": [
                            "Min Chen",
                            "David Ebert",
                            "Lace Padilla",
                            "Yixuan Zhang",
                            "Alfie Abdul-Rahman"
                        ],
                        "abstract": null,
                        "time_start": "2021-10-27T15:00:00Z",
                        "time_end": "2021-10-27T16:30:00Z"
                    }
                ]
            },
            {
                "title": "Our History and Future Innovation",
                "session_id": "v-panels-panel6",
                "track": "room6",
                "schedule_image": "v-panels-panel6.png",
                "chair": [],
                "organizers": [],
                "time_start": "2021-10-28T13:00:00Z",
                "time_end": "2021-10-28T14:30:00Z",
                "discord_category": "",
                "discord_channel": "",
                "discord_channel_id": 0,
                "youtube_url": null,
                "zoom_meeting": "",
                "zoom_password": "",
                "ff_link": "",
                "ff_playlist": "",
                "time_slots": [
                    {
                        "type": "live",
                        "title": "Our History and Future Innovation",
                        "contributors": [
                            "Ben Shneiderman",
                            "Tamara Munzner",
                            "Jarke van Wijk",
                            "Jock Mackinlay",
                            "Jessica Hullman"
                        ],
                        "abstract": null,
                        "time_start": "2021-10-28T13:00:00Z",
                        "time_end": "2021-10-28T14:30:00Z"
                    }
                ]
            },
            {
                "title": "Towards Accessible Data Representations",
                "session_id": "v-panels-panel3",
                "track": "room6",
                "schedule_image": "v-panels-panel3.png",
                "chair": [],
                "organizers": [],
                "time_start": "2021-10-28T15:00:00Z",
                "time_end": "2021-10-28T16:30:00Z",
                "discord_category": "",
                "discord_channel": "",
                "discord_channel_id": 0,
                "youtube_url": null,
                "zoom_meeting": "",
                "zoom_password": "",
                "ff_link": "",
                "ff_playlist": "",
                "time_slots": [
                    {
                        "type": "live",
                        "title": "Towards Accessible Data Representations",
                        "contributors": [
                            "Arvind Satyanarayan",
                            "Danielle Albers Szafir",
                            "Crystal Lee",
                            "Alan Lundgard",
                            "Keke Wu",
                            "Cindy Bennett",
                            "Zoe Gross",
                            "Rua Williams",
                            "Ariel Schwartz"
                        ],
                        "abstract": null,
                        "time_start": "2021-10-28T15:00:00Z",
                        "time_end": "2021-10-28T16:30:00Z"
                    }
                ]
            },
            {
                "title": "Visualization Literacy for General Audiences - Can We Make A Difference?",
                "session_id": "v-panels-panel5",
                "track": "room6",
                "schedule_image": "v-panels-panel5.png",
                "chair": [],
                "organizers": [],
                "time_start": "2021-10-29T13:00:00Z",
                "time_end": "2021-10-29T14:30:00Z",
                "discord_category": "",
                "discord_channel": "",
                "discord_channel_id": 0,
                "youtube_url": null,
                "zoom_meeting": "",
                "zoom_password": "",
                "ff_link": "",
                "ff_playlist": "",
                "time_slots": [
                    {
                        "type": "live",
                        "title": "Visualization Literacy for General Audiences - Can We Make A Difference?",
                        "contributors": [
                            "Alark Joshi",
                            "Katy Borner",
                            "Robert S. Laramee",
                            "Lane Harrison",
                            "Elif E. Firat",
                            "Bum Chul Kwon"
                        ],
                        "abstract": null,
                        "time_start": "2021-10-29T13:00:00Z",
                        "time_end": "2021-10-29T14:30:00Z"
                    }
                ]
            },
            {
                "title": "Navigating Interdisciplinary Careers in Visualization",
                "session_id": "v-panels-panel1",
                "track": "room6",
                "schedule_image": "v-panels-panel1.png",
                "chair": [],
                "organizers": [],
                "time_start": "2021-10-29T15:00:00Z",
                "time_end": "2021-10-29T16:30:00Z",
                "discord_category": "",
                "discord_channel": "",
                "discord_channel_id": 0,
                "youtube_url": null,
                "zoom_meeting": "",
                "zoom_password": "",
                "ff_link": "",
                "ff_playlist": "",
                "time_slots": [
                    {
                        "type": "live",
                        "title": "Navigating Interdisciplinary Careers in Visualization",
                        "contributors": [
                            "Morgan L. Turner",
                            "Ruth West",
                            "Adrien Segal",
                            "Sheelagh Carpendale",
                            "Daniel Keefe"
                        ],
                        "abstract": null,
                        "time_start": "2021-10-29T15:00:00Z",
                        "time_end": "2021-10-29T16:30:00Z"
                    }
                ]
            }
        ]
    },
    "a-vizsec": {
        "event": "VizSec",
        "long_name": "VizSec",
        "event_type": "Symposium",
        "event_description": "",
        "event_url": "",
        "sessions": [
            {
                "title": "Opening, Keynote and Best Paper",
                "session_id": "a-vizsec-vizsec-1",
                "track": "room7",
                "schedule_image": "a-vizsec-vizsec-1.png",
                "chair": [
                    "Chris Bryan",
                    "Rosa Romero-G\u00f3mez"
                ],
                "organizers": [
                    "Rosa Romero-G\u00f3mez",
                    "Chris Bryan"
                ],
                "time_start": "2021-10-27T13:00:00Z",
                "time_end": "2021-10-27T14:30:00Z",
                "discord_category": "",
                "discord_channel": "",
                "discord_channel_id": 0,
                "youtube_url": null,
                "zoom_meeting": "",
                "zoom_password": "",
                "ff_link": "",
                "ff_playlist": "",
                "time_slots": [
                    {
                        "type": "live",
                        "title": "Opening Presentation",
                        "contributors": [
                            "Marco Angelini"
                        ],
                        "abstract": null,
                        "time_start": "2021-10-27T13:00:00Z",
                        "time_end": "2021-10-27T13:15:00Z"
                    },
                    {
                        "type": "live",
                        "title": "Keynote by John Goodall",
                        "contributors": [
                            "John Goodall"
                        ],
                        "abstract": null,
                        "time_start": "2021-10-27T13:15:00Z",
                        "time_end": "2021-10-27T14:05:00Z"
                    },
                    {
                        "type": "live",
                        "title": "Best Paper Introduction",
                        "contributors": [
                            "Rosa Romero-G\u00f3mez"
                        ],
                        "abstract": null,
                        "time_start": "2021-10-27T14:05:00Z",
                        "time_end": "2021-10-27T14:10:00Z"
                    },
                    {
                        "type": "recorded",
                        "title": "Best Paper: Automatic Narrative Summarization for Visualizing Cyber Security Logs and Incident Reports",
                        "contributors": [
                            "Robert Gove"
                        ],
                        "abstract": null,
                        "time_start": "2021-10-27T14:10:00Z",
                        "time_end": "2021-10-27T14:30:00Z",
                        "uid": "a-vizsec-4835",
                        "youtube_video_id": "4QR6IV6gCtw"
                    }
                ]
            },
            {
                "title": "Machine Learning and Privacy",
                "session_id": "a-vizsec-vizsec-2",
                "track": "room7",
                "schedule_image": "a-vizsec-vizsec-2.png",
                "chair": [
                    "Chris Bryan",
                    "Rosa Romero-G\u00f3mez"
                ],
                "organizers": [
                    "Rosa Romero-G\u00f3mez",
                    "Chris Bryan"
                ],
                "time_start": "2021-10-27T15:00:00Z",
                "time_end": "2021-10-27T16:30:00Z",
                "discord_category": "",
                "discord_channel": "",
                "discord_channel_id": 0,
                "youtube_url": null,
                "zoom_meeting": "",
                "zoom_password": "",
                "ff_link": "",
                "ff_playlist": "",
                "time_slots": [
                    {
                        "type": "recorded",
                        "title": "AI Total: A Visualization Tool for Making Sense of Security ML Models in an Imperfect World of Production Data",
                        "contributors": [
                            "Awalin Sopan"
                        ],
                        "abstract": "The metrics measured while developing machine learning models are not enough to evaluate the models\u2019 performance in the operational level, especially ML models for cyber security with ever changing new attack vectors. Usually, it is also hard to understand initially if the fundamental problem is in the model performance or if there are data issues that are causing problems in the evaluation. With this in mind, we developed a visualization system that would allow the users to quickly identify and diagnose issues with current model deployment, from model performance to data issues that prevent accurate evaluation of the model. Our application enables our security data science team to have a situational awareness of the system and quickly investigate any problems. While designing our system, we considered all the common issues we see in production. In this paper, we will describe this application, its regular usage, and some of the special example cases when it was proved valuable for introspecting our models.",
                        "time_start": "2021-10-27T15:00:00Z",
                        "time_end": "2021-10-27T15:20:00Z",
                        "uid": "a-vizsec-7354",
                        "youtube_video_id": "ni5gRVtdYng"
                    },
                    {
                        "type": "recorded",
                        "title": "BUCEPHALUS: a BUsiness CEntric cybersecurity Platform for proActive anaLysis Using visual analyticS",
                        "contributors": [
                            "Graziano Blasilli"
                        ],
                        "abstract": "Analyzing and mitigating the threats that cyber-attacks pose on the services of a critical infrastructure is not a trivial activity. Research solutions have been developed using data about the devices used for implementing the services, services dependencies, network topology, and the vulnerabilities that can be exploited to attack the network. However, most of the proposed solutions fail to consider these aspects in an integrated fashion, allowing the user to understand global dependencies and weaknesses.",
                        "time_start": "2021-10-27T15:20:00Z",
                        "time_end": "2021-10-27T15:40:00Z",
                        "uid": "a-vizsec-4000",
                        "youtube_video_id": "qMwQBTH8XpU"
                    },
                    {
                        "type": "recorded",
                        "title": "Decision Support for Sharing Data Using Differential Privacy",
                        "contributors": [
                            "Peeter Laud"
                        ],
                        "abstract": "Owners of data may wish to share some statistics with others, but they may be worried of privacy of the underlying data. An effective solution to this problem is to employ provable privacy techniques, such as differential privacy, to add noise to the statistics before releasing them. This protection lowers the risk of sharing sensitive data with more or less trusted data sharing partners. Unfortunately, applying differential privacy in its mathematical form requires one to fix certain numeric parameters, which involves subtle computations and expert knowledge that the data owners may lack.",
                        "time_start": "2021-10-27T15:40:00Z",
                        "time_end": "2021-10-27T16:00:00Z",
                        "uid": "a-vizsec-8146",
                        "youtube_video_id": "gUsVAMulsdY"
                    },
                    {
                        "type": "recorded",
                        "title": "SAGE: Intrusion Alert-driven Attack Graph Extractor (short paper)",
                        "contributors": [
                            "Azqa Nadeem"
                        ],
                        "abstract": "Attack graphs (AG) are used to assess pathways availed by cyber adversaries to penetrate a network. State-of-the-art approaches for AG generation focus mostly on deriving dependencies between system vulnerabilities based on network scans and expert knowledge. In real-world operations however, it is costly and ineffective to rely on constant vulnerability scanning and expert-crafted AGs. We propose to automatically learn AGs based on actions observed through intrusion alerts, without prior expert knowledge. Specifically, we develop an unsupervised sequence learning system, SAGE, that leverages the temporal and probabilistic dependence between alerts in a suffix-based probabilistic deterministic finite automaton(S-PDFA) \u2013 a model that accentuates infrequent severe alerts and summarizes paths leading to them. AGs are then derived from the S-PDFA. Tested with intrusion alerts collected through Collegiate Penetration Testing Competition, SAGE compresses several thousands of alerts into only a handful of AGs. These AGs reflect the strategies used by participating teams. The resulting AGs are succinct, interpretable, and enable analysts to derive actionable insights, e.g., attackers tend to follow shorter paths after they have discovered a longer one.",
                        "time_start": "2021-10-27T16:00:00Z",
                        "time_end": "2021-10-27T16:15:00Z",
                        "uid": "a-vizsec-2863",
                        "youtube_video_id": "IYddPL4q0bs"
                    },
                    {
                        "type": "recorded",
                        "title": "Towards Visual Analytics Dashboards for Provenance-driven Static Application Security Testing (short paper)",
                        "contributors": [
                            "Andreas Schreiber"
                        ],
                        "abstract": "The use of static code analysis tools can be time consuming, as the many existing tools focus on different aspects and therefore development teams often use several of these tools to keep code quality high. Displaying the results of multiple tools, such as code smells and warnings, in a unified interface can help developers get a better overview and prioritize upcoming work. We present visualizations and a dashboard that interactively display results from static code analysis for \u201cinteresting\u201d commits during development. With this, we aim to provide an effective visual analytics tool for code analysis results.",
                        "time_start": "2021-10-27T16:15:00Z",
                        "time_end": "2021-10-27T16:30:00Z",
                        "uid": "a-vizsec-7867"
                    }
                ]
            },
            {
                "title": "Threat Detection, Computer Forensics & Software Vulnerability Analysis ",
                "session_id": "a-vizsec-vizsec-3",
                "track": "room7",
                "schedule_image": "a-vizsec-vizsec-3.png",
                "chair": [
                    "Chris Bryan",
                    "Rosa Romero-G\u00f3mez"
                ],
                "organizers": [
                    "Rosa Romero-G\u00f3mez",
                    "Chris Bryan"
                ],
                "time_start": "2021-10-27T17:00:00Z",
                "time_end": "2021-10-27T18:30:00Z",
                "discord_category": "",
                "discord_channel": "",
                "discord_channel_id": 0,
                "youtube_url": null,
                "zoom_meeting": "",
                "zoom_password": "",
                "ff_link": "",
                "ff_playlist": "",
                "time_slots": [
                    {
                        "type": "recorded",
                        "title": "Developing Visualisations to Enhance an Insider Threat Product: A Case Study",
                        "contributors": [
                            "Martin Graham"
                        ],
                        "abstract": "Attack graphs (AG) are used to assess pathways availed by cyber adversaries to penetrate a network. State-of-the-art approaches for AG generation focus mostly on deriving dependencies between system vulnerabilities based on network scans and expert knowledge. In real-world operations however, it is costly and ineffective to rely on constant vulnerability scanning and expert-crafted AGs. We propose to automatically learn AGs based on actions observed through intrusion alerts, without prior expert knowledge. Specifically, we develop an unsupervised sequence learning system, SAGE, that leverages the temporal and probabilistic dependence between alerts in a suffix-based probabilistic deterministic finite automaton(S-PDFA) \u2013 a model that accentuates infrequent severe alerts and summarizes paths leading to them. AGs are then derived from the S-PDFA. Tested with intrusion alerts collected through Collegiate Penetration Testing Competition, SAGE compresses several thousands of alerts into only a handful of AGs. These AGs reflect the strategies used by participating teams. The resulting AGs are succinct, interpretable, and enable analysts to derive actionable insights, e.g., attackers tend to follow shorter paths after they have discovered a longer one.",
                        "time_start": "2021-10-27T17:00:00Z",
                        "time_end": "2021-10-27T17:20:00Z",
                        "uid": "a-vizsec-9866",
                        "youtube_video_id": "qZTmpkH2_h8"
                    },
                    {
                        "type": "recorded",
                        "title": "Visual Decision-Support for Live Digital Forensic Investigations",
                        "contributors": [
                            "Fabian B\u00f6hm"
                        ],
                        "abstract": "Performing a live digital forensic investigation on a running system is challenging due to the time pressure under which decisions have to be made. Newly proliferating and frequently applied types of malware (e.g., fileless malware) increase the need to conduct digital forensic investigations in real-time. In the course of these investigations, forensic experts are confronted with a wide range of different forensic tools. The decision, which of those are suitable for the current situation, is often based on the forensic experts' experience. Currently, there is no reliable automated solution to support this decision-making. Therefore, we derive requirements for visually supporting the decision-making process for live forensic investigations and introduce a research prototype that provides visual guidance for cyber forensic experts during a live digital forensic investigation. Our prototype collects relevant core information for a live digital forensic and provides visual representations for connections between occurring events, developments over time, and detailed information on specific events. To show the applicability of our approach, we analyze an exemplary use case using the prototype and demonstrate the support through our approach.",
                        "time_start": "2021-10-27T17:20:00Z",
                        "time_end": "2021-10-27T17:40:00Z",
                        "uid": "a-vizsec-9220",
                        "youtube_video_id": "DMfBmMDOAJg"
                    },
                    {
                        "type": "recorded",
                        "title": "User-Centered Design of Visualizations for Software Vulnerability Reports",
                        "contributors": [
                            "Steven Lamarr Reynolds"
                        ],
                        "abstract": "Today's software systems are created by software development processes that naturally include mistakes, some of which can be exploited by attackers and are therefore called vulnerabilities. Automatic software scanners enable developers to analyze their applications to detect vulnerabilities and alert them of their presence. But often these reports are hard to understand, include false positives or overwhelm users due to the sheer number of alerts, since a report may contain hundreds to thousands of vulnerabilities. Developers must undergo a process called vulnerability triage to find the relevant vulnerabilities to fix. This paper presents two interactive visualizations for developers and security experts to gain an overview of the security state of their application. Users can see the distribution of vulnerabilities, find the most relevant ones, and compare differences between application versions. Our visualization design is inspired by an initial preliminary study and has been evaluated by domain experts to investigate the usability and appropriateness.",
                        "time_start": "2021-10-27T17:40:00Z",
                        "time_end": "2021-10-27T18:00:00Z",
                        "uid": "a-vizsec-4137",
                        "youtube_video_id": "NV-zP5Md-sE"
                    },
                    {
                        "type": "recorded",
                        "title": "VulnEx: Exploring Open-Source Software Vulnerabilities in Large Development Organizations to Understand Risk Exposure (short paper)",
                        "contributors": [
                            "Frederik L. Dennig"
                        ],
                        "abstract": "The prevalent usage of open-source software (OSS) has led to an increased interest in resolving potential third-party security risks by fixing common vulnerabilities and exposures (CVEs). However, even with automated code analysis tools in place, security analysts often lack the means to obtain an overview of vulnerable OSS reuse in large software organizations. In this design study, we propose VulnEx (Vulnerability Explorer), a tool to audit entire software development organizations. We introduce three complementary table-based representations to identify and assess vulnerability exposures due to OSS, which we designed in collaboration with security analysts. The presented tool allows examining problematic projects and applications (repositories), third-party libraries, and vulnerabilities across a software organization. We show the applicability of our tool through a use case and preliminary expert feedback.",
                        "time_start": "2021-10-27T18:00:00Z",
                        "time_end": "2021-10-27T18:15:00Z",
                        "uid": "a-vizsec-2128",
                        "youtube_video_id": "9WuJDQLCNXg"
                    },
                    {
                        "type": "live",
                        "title": "Posters session",
                        "contributors": [
                            "Marco Angelini"
                        ],
                        "abstract": null,
                        "time_start": "2021-10-27T18:15:00Z",
                        "time_end": "2021-10-27T18:20:00Z"
                    },
                    {
                        "type": "live",
                        "title": "Closing",
                        "contributors": [
                            "Marco Angelini"
                        ],
                        "abstract": null,
                        "time_start": "2021-10-27T18:20:00Z",
                        "time_end": "2021-10-27T18:30:00Z"
                    }
                ]
            }
        ]
    },
    "x-sponsor": {
        "event": "Industrial Keynote",
        "long_name": "Industrial Keynote",
        "event_type": "Industrial Keynote",
        "event_description": "",
        "event_url": "",
        "sessions": [
            {
                "title": "Industrial Keynote 1",
                "session_id": "x-sponsor-1",
                "track": "room8",
                "schedule_image": "x-sponsor-1.png",
                "chair": [],
                "organizers": [],
                "time_start": "2021-10-27T14:45:00Z",
                "time_end": "2021-10-27T15:00:00Z",
                "discord_category": "",
                "discord_channel": "",
                "discord_channel_id": 0,
                "youtube_url": null,
                "zoom_meeting": "",
                "zoom_password": "",
                "ff_link": "",
                "ff_playlist": "",
                "time_slots": [
                    {
                        "type": "recorded",
                        "title": "Industrial Keynote 1",
                        "contributors": "",
                        "abstract": null,
                        "time_start": "2021-10-27T14:45:00Z",
                        "time_end": "2021-10-27T15:00:00Z"
                    }
                ]
            },
            {
                "title": "Industrial Keynote 2",
                "session_id": "x-sponsor-2",
                "track": "room7",
                "schedule_image": "x-sponsor-2.png",
                "chair": [],
                "organizers": [],
                "time_start": "2021-10-28T14:45:00Z",
                "time_end": "2021-10-28T15:00:00Z",
                "discord_category": "",
                "discord_channel": "",
                "discord_channel_id": 0,
                "youtube_url": null,
                "zoom_meeting": "",
                "zoom_password": "",
                "ff_link": "",
                "ff_playlist": "",
                "time_slots": [
                    {
                        "type": "recorded",
                        "title": "Industrial Keynote 2",
                        "contributors": "",
                        "abstract": null,
                        "time_start": "2021-10-28T14:45:00Z",
                        "time_end": "2021-10-28T15:00:00Z"
                    }
                ]
            }
        ]
    },
    "v-cga": {
        "event": "VIS CG&A Presentations",
        "long_name": "VIS CG&A Presentations",
        "event_type": "Paper Presentations",
        "event_description": "",
        "event_url": "",
        "sessions": [
            {
                "title": "Data Physicalization",
                "session_id": "v-cga-cga1",
                "track": "room4",
                "schedule_image": "v-cga-cga1.png",
                "chair": [
                    "Trevor Hogan"
                ],
                "organizers": [],
                "time_start": "2021-10-27T15:00:00Z",
                "time_end": "2021-10-27T16:30:00Z",
                "discord_category": "",
                "discord_channel": "",
                "discord_channel_id": 0,
                "youtube_url": null,
                "zoom_meeting": "",
                "zoom_password": "",
                "ff_link": "",
                "ff_playlist": "",
                "time_slots": [
                    {
                        "type": "recorded",
                        "title": "What we talk about when we talk about data physicality",
                        "contributors": [
                            "Dietmar Offenhuber"
                        ],
                        "abstract": "Data physicalizations \u201cmap data to physical form,\u201d yet many canonical examples are not based on external data sets. To address this contradiction, I argue that the practice of physicalization forces us to rethink traditional notions of data. This article proposes a conceptual framework to examine how physicalizations relate to data. This article develops a two-dimensional conceptual space for comparing different perspectives on data used in physicalization, drawing from design theory and critical data studies literature. One axis distinguishes between epistemological and ontological perspectives, focusing on the relationship between data and the mind. The second axis distinguishes how data relate to the world, differentiating between representational and relational perspectives. To clarify the aesthetic and conceptual implications of these different perspectives, the article discusses examples of data physicalization for each quadrant of the continuous space. It further uses the framework to examine the explicit and implicit assumptions about data in physicalization literature. As a theoretical article, it encourages practitioners to think about how data relate to the manifestations and the phenomena they try to capture. It invites exploration of the relationship between data and the world as a generative source of creative tension.",
                        "time_start": "2021-10-27T15:00:00Z",
                        "time_end": "2021-10-27T15:15:00Z",
                        "uid": "v-cga-9198117",
                        "youtube_video_id": "gvGPnRWnxhg"
                    },
                    {
                        "type": "recorded",
                        "title": "Data Badges: Making an Academic Profile through a DIY wearable physicalisation",
                        "contributors": [
                            "Georgia Panagiotidou",
                            "Sinem G\u00f6r\u00fcc\u00fc",
                            "Andrew Vande Moere"
                        ],
                        "abstract": "In this pictorial, we present the design and making process of Data Badges as they were deployed during a one-week academic seminar. Data Badges are customizable physical conference badges that invite participants to make their own independent and personalized expressions of their academic profile by choosing and assembling a collection of predefined physical tokens on a flat wearable canvas. As our modular and intuitive design approach allows the construction to occur as a shared, collective activity, Data Badges take advantage of the creative, affective, and social values that underlie physicalization and its construction to engage participants in reflecting on personal data. Among other unexpected phenomena, we noticed how the freedom of assembly and interpretation encouraged a variety of appropriations, which expanded its intended representational space from fully representative to more resistive and provocative forms of data expression.",
                        "time_start": "2021-10-27T15:15:00Z",
                        "time_end": "2021-10-27T15:30:00Z",
                        "uid": "v-cga-9201309",
                        "youtube_video_id": "eNdJa8finX0"
                    },
                    {
                        "type": "recorded",
                        "title": "Move&Find: The value of kinesthetic experience in a casual data representation",
                        "contributors": [
                            "J\u00f6rn Hurtienne",
                            "Franzisca Maas",
                            "Astrid Carolus",
                            "Daniel Reinhardt",
                            "Cordula Baur",
                            "Carolin Wienrich"
                        ],
                        "abstract": "The value of a data representation is traditionally judged based on aspects like effectiveness and efficiency that are important in utilitarian or work-related contexts. Most multisensory data representations, however, are employed in casual contexts where creativity, affective, physical, intellectual, and social engagement might be of greater value. We introduce Move&Find, a multisensory data representation in which people pedalled on a bicycle to exert the energy required to power a search query on Google's servers. To evaluate Move&Find, we operationalized a framework suitable to evaluate the value of data representations in casual contexts and experimentally compared Move&Find to a corresponding visualization. With Move&Find, participants achieved a higher understanding of the data. Move&Find was judged to be more creative and encouraged more physical and social engagement-components of value that would have been missed using more traditional evaluation frameworks.",
                        "time_start": "2021-10-27T15:30:00Z",
                        "time_end": "2021-10-27T15:45:00Z",
                        "uid": "v-cga-9201404",
                        "youtube_video_id": "QP4wXpaOctw"
                    },
                    {
                        "type": "recorded",
                        "title": "Slave Voyages:reflections on data sculptures",
                        "contributors": [
                            "Doris Kosminsky",
                            "Douglas Thomaz de Oliveira"
                        ],
                        "abstract": "This pictorial presents the development of a data sculpture, followed by our reflections inspired by Research through Design (RtD) and Dahlstedt's process-based model of artistic creativity. We use the notion of negotiation between concept and material representation to reflect on the ideation, design process, production, and the exhibition of \u201cSlave Voyages\u201d - a set of data sculptures that depicts slave traffic from Africa to the American continent. The work was initially produced as an assignment on physicalization for the Design course at the Federal University of Rio de Janeiro. Our aim is to open discussion on material representation and negotiation in the creative process of data physicalization.",
                        "time_start": "2021-10-27T15:45:00Z",
                        "time_end": "2021-10-27T16:00:00Z",
                        "uid": "v-cga-9200769",
                        "youtube_video_id": "mzKTerhAnjU"
                    },
                    {
                        "type": "recorded",
                        "title": "Narrative Physicalisation: Supporting Interactive Engagement with Personal Data",
                        "contributors": [
                            "Maria Karyda",
                            "Danielle Wilde",
                            "Mette Gislev Kj\u00e6rsgaard"
                        ],
                        "abstract": "Physical engagement with data necessarily influences the reflective process. However, the role of interactivity and narration are often overlooked when designing and analyzing personal data physicalizations. We introduce Narrative Physicalizations, everyday objects modified to support nuanced self-reflection through embodied engagement with personal data. Narrative physicalizations borrow from narrative visualizations, storytelling with graphs, and engagement with mundane artifacts from data-objects. Our research uses a participatory approach to research-through-design and includes two interdependent studies. In the first, personalized data physicalizations are developed for three individuals. In the second, we conduct a parallel autobiographical exploration of what constitutes personal data when using a Fitbit. Our work expands the landscape of data physicalization by introducing narrative physicalizations. It suggests an experience-centric view on data physicalization where people engage physically with their data in playful ways, making their body an active agent during the reflective process.",
                        "time_start": "2021-10-27T16:00:00Z",
                        "time_end": "2021-10-27T16:15:00Z",
                        "uid": "v-cga-9200790"
                    },
                    {
                        "type": "recorded",
                        "title": "Data Clothing and BigBarChart: designing physical data reports on indoor pollutants for individuals and communities",
                        "contributors": [
                            "Laura J. Perovich",
                            "Phoebe Cai",
                            "Amber Guo",
                            "Kristin Zimmerman",
                            "Katherine Paseman",
                            "Dayanna Espinoza Silva",
                            "Julia G. Brody"
                        ],
                        "abstract": "In response to participant preferences and new ethics guidelines, researchers are increasingly sharing data with health study participants, including data on their own household chemical exposures. Data physicalization may be a useful tool for these communications, because it is thought to be accessible to a general audience and emotionally engaged. However, there are limited studies of data physicalization in the wild with diverse communities. Our application of this method in the Green Housing Study is an early example of using data physicalization in environmental health report-back. We gathered feedback through community meetings, prototype testing, and semistructured interviews, leading to the development of data t-shirts and other garments and person-sized bar charts. We found that participants were enthusiastic about data physicalizations, it connected them to their previous experience, and they had varying desires to share their data. Our findings suggest that researchers can enhance environmental communications by further developing the human experience of physicalizations and engaging diverse communities.",
                        "time_start": "2021-10-27T16:15:00Z",
                        "time_end": "2021-10-27T16:30:00Z",
                        "uid": "v-cga-9201339",
                        "youtube_video_id": "HcbC5XYfFTg"
                    }
                ]
            },
            {
                "title": "Visualizing big issues: Culture, Climate Change, and Communities",
                "session_id": "v-cga-cga2",
                "track": "room4",
                "schedule_image": "v-cga-cga2.png",
                "chair": [
                    "Kai Xu"
                ],
                "organizers": [],
                "time_start": "2021-10-27T17:00:00Z",
                "time_end": "2021-10-27T18:15:00Z",
                "discord_category": "",
                "discord_channel": "",
                "discord_channel_id": 0,
                "youtube_url": null,
                "zoom_meeting": "",
                "zoom_password": "",
                "ff_link": "",
                "ff_playlist": "",
                "time_slots": [
                    {
                        "type": "recorded",
                        "title": "Dynamic 3D Visualization of Climate Model Development and Results",
                        "contributors": [
                            "Jeremy Walton",
                            "Samantha Adams",
                            "Wolfgang Hayek",
                            "Piotr Florek",
                            "Harold Dyson"
                        ],
                        "abstract": "Climate models play a significant role in the understanding of climate change, and the effective presentation and interpretation of their results is important for both the scientific community and the general public. In the case of the latter audience-which has become increasingly concerned with the implications of climate change for society-there is a requirement for visualizations which are compelling and engaging. We describe the use of ParaView, a well-established visualization application, to produce images and animations of results from a large set of modeling experiments, and their use in the promulgation of climate research results. Visualization can also make useful contributions to development, particularly for complex large-scale applications such as climate models. We present early results from the construction of a next-generation climate model which has been designed for use on exascale compute platforms, and show how visualization has helped in the development process, particularly with regard to higher model resolutions and novel data representations.",
                        "time_start": "2021-10-27T17:00:00Z",
                        "time_end": "2021-10-27T17:15:00Z",
                        "uid": "v-cga-9281098",
                        "youtube_video_id": "0PUzZ9O0zFQ"
                    },
                    {
                        "type": "recorded",
                        "title": "Exploring the Design Space of Sankey Diagrams for the Food-Energy-Water Nexus",
                        "contributors": [
                            "Brandon Mathis",
                            "Yuxin Ma",
                            "Michelle Mancenido",
                            "Ross Maciejewski"
                        ],
                        "abstract": "In this work, we define a set of design requirements relating to Sankey diagrams for supporting food-energy-water nexus understanding and propose the network embodied sectoral trajectory diagram design, a visualization design that incorporates a number of characteristics from Sankey diagrams, treemaps, and graphs, to improve the readability and minimize the negative impact of edge crossings that are common in traditional Sankey diagrams.",
                        "time_start": "2021-10-27T17:15:00Z",
                        "time_end": "2021-10-27T17:30:00Z",
                        "uid": "v-cga-8758151",
                        "youtube_video_id": "S4bOr4lOXMk"
                    },
                    {
                        "type": "recorded",
                        "title": "QuteVis: Visually Studying Transportation Patterns Using Multi-Sketch Query of Joint Traffic Situations",
                        "contributors": [
                            "Shamal AL-Dohuki",
                            "Ye Zhao",
                            "Farah Kamw",
                            "Jing Yang",
                            "Xinyue Ye",
                            "Wei Chen"
                        ],
                        "abstract": "QuteVis uses multisketch query and visualization to discover specific times and days in history with specified joint traffic patterns at different city locations. Users can use touch input devices to define, edit, and modify multiple sketches on a city map. A set of visualizations and interactions is provided to help users browse and compare retrieved traffic situations and discover potential influential factors. QuteVis is built upon a transport database that integrates heterogeneous data sources with an optimized spatial indexing and weighted similarity computation. An evaluation with real-world data and domain experts demonstrates that QuteVis is useful in urban transportation applications in modern cities.",
                        "time_start": "2021-10-27T17:30:00Z",
                        "time_end": "2021-10-27T17:45:00Z",
                        "uid": "v-cga-8691491"
                    },
                    {
                        "type": "recorded",
                        "title": "Many Views Are Not Enough: Designing for Synoptic Insights in Cultural Collections",
                        "contributors": [
                            "Florian Windhager",
                            "Saminu Salisu",
                            "Roger A. Leite",
                            "Velitchko Filipov",
                            "Silvia Miksch",
                            "G\u00fcnther Schreder",
                            "Eva Mayr"
                        ],
                        "abstract": "Cultural object collections attract and delight spectators since ancient times. Yet, they also easily overwhelm visitors due to their perceptual richness and associated information. Similarly, digitized collections appear as complex, multifaceted phenomena, which can be challenging to grasp and navigate. Though visualizations can create various types of collection overviews for that matter, they do not easily assemble into a \u201cbig picture\u201d or lead to an integrated understanding. We introduce coherence techniques to maximize connections between multiple views and apply them to the prototype PolyCube system of collection visualization: with map, set, and network visualizations it makes spatial, categorical, and relational collection aspects visible. For the essential temporal dimension, it offers four different views: superimposition, animation, juxtaposition, and space-time cube representations. A user study confirmed that better integrated visualizations support synoptic, cross-dimensional insights. An outlook is dedicated to the system's applicability within other arts and humanities data domains.",
                        "time_start": "2021-10-27T17:45:00Z",
                        "time_end": "2021-10-27T18:00:00Z",
                        "uid": "v-cga-9057396",
                        "youtube_video_id": "zzLnpxpHis4"
                    },
                    {
                        "type": "recorded",
                        "title": "CLEVis: A Semantic Driven Visual Analytics System for Community Level Events",
                        "contributors": [
                            "Chao Ma",
                            "Ye Zhao",
                            "Andrew Curtis",
                            "Farah Kamw",
                            "Shamal AL-Dohuki",
                            "Jing Yang",
                            "Suphanut Jamonnak",
                            "Ismael Ali"
                        ],
                        "abstract": "Community-level event (CLE) datasets, such as police reports of crime events, contain abundant semantic information of event situations, and descriptions in a geospatial-temporal context. They are critical for frontline users, such as police officers and social workers, to discover and examine insights about community neighborhoods. We propose CLEVis, a neighborhood visual analytics system for CLE datasets, to help frontline users explore events for insights at community regions of interest, namely fine-grained geographical resolutions, such as small neighborhoods around local restaurants, churches, and schools. CLEVis fully utilizes semantic information by integrating automatic algorithms and interactive visualizations. The design and development of CLEVis are conducted with solid collaborations with real-world community workers and social scientists. Case studies and user feedback are presented with real-world datasets and applications.",
                        "time_start": "2021-10-27T18:00:00Z",
                        "time_end": "2021-10-27T18:15:00Z",
                        "uid": "v-cga-8999535",
                        "youtube_video_id": "gf7Ss71QXv0"
                    }
                ]
            },
            {
                "title": "SIGGRAPH Invited Talks",
                "session_id": "v-cga-cga3",
                "track": "room4",
                "schedule_image": "v-cga-cga3.png",
                "chair": [
                    "Jian Chen",
                    "Klaus Mueller"
                ],
                "organizers": [],
                "time_start": "2021-10-29T15:00:00Z",
                "time_end": "2021-10-29T16:30:00Z",
                "discord_category": "",
                "discord_channel": "",
                "discord_channel_id": 0,
                "youtube_url": null,
                "zoom_meeting": "",
                "zoom_password": "",
                "ff_link": "",
                "ff_playlist": "",
                "time_slots": [
                    {
                        "type": "recorded",
                        "title": "Selective Region-based Photo Color Adjustment for Graphic Designs",
                        "contributors": [
                            "Nanxuan Zhao"
                        ],
                        "abstract": "When adding a photo onto a graphic design, professional graphic designers often adjust its colors based on some target colors obtained from the brand or product to make the entire design more memorable to audiences and establish a consistent brand identity. However, adjusting the colors of a photo in the context of a graphic design is a difficult task, with two major challenges: (1) Locality: The color is often adjusted locally to preserve the semantics and atmosphere of the original image; and (2) Naturalness: The modified region needs to be carefully chosen and recolored to obtain a semantically valid and visually natural result. To address these challenges, we propose a learning-based approach to photo color adjustment for graphic designs, which maps an input photo along with the target colors to a recolored result. Our method decomposes the color adjustment process into two successive stages: modifiable region selection and target color propagation. The first stage aims to solve the core, challenging problem of which local image region(s) should be adjusted, which requires not only a common sense of colors appearing in our visual world but also understanding of subtle visual design heuristics. To this end, we capitalize on both natural photos and graphic designs to train a region selection network, which detects the most likely regions to be adjusted to the target colors. The second stage trains a recoloring network to naturally propagate the target colors in the detected regions. Through extensive experiments and a user study, we demonstrate the effectiveness of our selective region-based photo recoloring framework.",
                        "time_start": "2021-10-29T15:00:00Z",
                        "time_end": "2021-10-29T15:15:00Z",
                        "uid": "v-siggraph-115"
                    },
                    {
                        "type": "recorded",
                        "title": "Tracing Versus Freehand for Evaluating Computer-Generated Drawings",
                        "contributors": [
                            "Zeyu Wang"
                        ],
                        "abstract": "Non-photorealistic rendering (NPR) and image processing algorithms are widely assumed as a proxy for drawing. However, this assumption is not well assessed due to the difficulty in collecting and registering freehand drawings. Alternatively, tracings are easier to collect and register, but there is no quantitative evaluation of tracing as a proxy for freehand drawing. In this paper, we compare tracing, freehand drawing, and computer-generated drawing approximation (CGDA) to understand their similarities and differences. We collected a dataset of 1,498 tracings and freehand drawings by 110 participants for 100 image prompts. Our drawings are registered to the prompts and include vector-based timestamped strokes collected via stylus input. Comparing tracing and freehand drawing, we found a high degree of similarity in stroke placement and types of strokes used over time. We show that tracing can serve as a viable proxy for freehand drawing because of similar correlations between spatio-temporal stroke features and labeled stroke types. Comparing hand-drawn content and current CGDA output, we found that 60% of drawn pixels corresponded to computer-generated pixels on average. The overlap tended to be commonly drawn content, but people's artistic choices and temporal tendencies remained largely uncaptured. We present an initial analysis to inform new CGDA algorithms and drawing applications, and provide the dataset for use by the community.",
                        "time_start": "2021-10-29T15:15:00Z",
                        "time_end": "2021-10-29T15:30:00Z",
                        "uid": "v-siggraph-332"
                    },
                    {
                        "type": "recorded",
                        "title": "A Non-exponential Transmittance Model for Volumetric Scene Representations",
                        "contributors": [
                            "Delio Vicini"
                        ],
                        "abstract": "We introduce a novel transmittance model to improve the volumetric representation of 3D scenes. The model can represent opaque surfaces in the volumetric light transport framework. Volumetric representations are useful for complex scenes, and become increasingly popular for level of detail and scene reconstruction. The traditional exponential transmittance model found in volumetric light transport cannot capture correlations in visibility across volume elements. When representing opaque surfaces as volumetric density, this leads to both bloating of silhouettes and light leaking artifacts. By introducing a parametric non-exponential transmittance model, we are able to approximate these correlation effects and significantly improve the accuracy of volumetric appearance representation of opaque scenes. Our parametric transmittance model can represent a continuum between the linear transmittance that opaque surfaces exhibit and the traditional exponential transmittance encountered in participating media and unstructured geometries. This covers a large part of the spectrum of geometric structures encountered in complex scenes. In order to handle the spatially varying transmittance correlation effects, we further extend the theory of non-exponential participating media to a heterogeneous transmittance model. Our model is compact in storage and computationally efficient both for evaluation and for reverse-mode gradient computation. Applying our model to optimization algorithms yields significant improvements in volumetric scene appearance quality. We further show improvements for relevant applications, such as scene appearance prefiltering, image-based scene reconstruction using differentiable rendering, neural representations, and compare it to a conventional exponential model.",
                        "time_start": "2021-10-29T15:30:00Z",
                        "time_end": "2021-10-29T15:45:00Z",
                        "uid": "v-siggraph-322"
                    },
                    {
                        "type": "recorded",
                        "title": "Editable Free-viewpoint Video using a Layered Neural Representation",
                        "contributors": [
                            "Jiakai Zhang"
                        ],
                        "abstract": "Generating free-viewpoint videos is critical for immersive VR/AR experience but recent neural advances still lack the editing ability to manipulate the visual perception for large dynamic scenes. To fill this gap, in this paper we propose the first approach for editable photo-realistic free-viewpoint video generation for large-scale dynamic scenes using only sparse 16 cameras. The core of our approach is a new layered neural representation, where each dynamic entity including the environment itself is formulated into a space-time coherent neural layered radiance representation called ST-NeRF. Such layered representation supports fully perception and realistic manipulation of the dynamic scene whilst still supporting a free viewing experience in a wide range. In our ST-NeRF, the dynamic entity/layer is represented as continuous functions, which achieves the disentanglement of location, deformation as well as the appearance of the dynamic entity in a continuous and self-supervised manner. We propose a scene parsing 4D label map tracking to disentangle the spatial information explicitly, and a continuous deform module to disentangle the temporal motion implicitly. An object-aware volume rendering scheme is further introduced for the re-assembling of all the neural layers. We adopt a novel layered loss and motion-aware ray sampling strategy to enable efficient training for a large dynamic scene with multiple performers, Our framework further enables a variety of editing functions, i.e., manipulating the scale and location, duplicating or retiming individual neural layers to create numerous visual effects while preserving high realism. Extensive experiments demonstrate the effectiveness of our approach to achieve high-quality, photo-realistic, and editable free-viewpoint video generation for dynamic scenes.",
                        "time_start": "2021-10-29T15:45:00Z",
                        "time_end": "2021-10-29T16:00:00Z",
                        "uid": "v-siggraph-173"
                    },
                    {
                        "type": "recorded",
                        "title": "Pareto Gamuts : exploring optimal designs across varying contexts",
                        "contributors": [
                            "Liane Makatura"
                        ],
                        "abstract": "Manufactured parts are meticulously engineered to perform well with respect to several conflicting metrics, like weight, stress, and cost. The best achievable trade-offs reside on the Pareto front which can be discovered via performance-driven optimization. The objectives that define this Pareto front often incorporate assumptions about the context in which a part will be used, including loading conditions, environmental influences, material properties, or regions that must be preserved to interface with a surrounding assembly. Existing multi-objective optimization tools are only equipped to study one context at a time, so engineers must run independent optimizations for each context of interest. However, engineered parts frequently appear in many contexts: wind turbines must perform well in many wind speeds, and a bracket might be optimized several times with its bolt-holes fixed in different locations on each run. In this paper, we formulate a framework for variable-context multi-objective optimization. We introduce the Pareto gamut, which captures Pareto fronts over a range of contexts. We develop a global/local optimization algorithm to discover the Pareto gamut directly, rather than discovering a single fixed-context \"slice\" at a time. To validate our method, we adapt existing multi-objective optimization benchmarks to contextual scenarios. We also demonstrate the practical utility of Pareto gamut exploration for several engineering design problems.",
                        "time_start": "2021-10-29T16:00:00Z",
                        "time_end": "2021-10-29T16:15:00Z",
                        "uid": "v-siggraph-153"
                    },
                    {
                        "type": "recorded",
                        "title": "NeuMIP: Multi-Resolution Neural Materials",
                        "contributors": [
                            "Alexandr Kuznetsov"
                        ],
                        "abstract": "We propose NeuMIP, a neural method for representing and rendering a variety of material appearances at different scales. Classical prefiltering (mipmapping) methods work well on simple material properties such as diffuse color, but fail to generalize to normals, self-shadowing, fibers or more complex microstructures and reflectances. In this work, we generalize traditional mipmap pyramids to pyramids of neural textures, combined with a fully connected network. We also introduce neural offsets, a novel method which allows rendering materials with intricate parallax effects without any tessellation. This generalizes classical parallax mapping, but is trained without supervision by any explicit heightfield. Neural materials within our system support a 7-dimensional query, including position, incoming and outgoing direction, and the desired filter kernel size. The materials have small storage (on the order of standard mipmapping except with more texture channels), and can be integrated within common Monte-Carlo path tracing systems. We demonstrate our method on a variety of materials, resulting in complex appearance across levels of detail, with accurate parallax, self-shadowing, and other effects.",
                        "time_start": "2021-10-29T16:15:00Z",
                        "time_end": "2021-10-29T16:30:00Z",
                        "uid": "v-siggraph-287"
                    }
                ]
            }
        ]
    },
    "v-spotlights": {
        "event": "Application Spotlights",
        "long_name": "Application Spotlights",
        "event_type": "Application Spotlight",
        "event_description": "",
        "event_url": "",
        "sessions": [
            {
                "title": "In Situ Inference: Advanced Data Science for Space Weather Modeling",
                "session_id": "v-spotlights-app2",
                "track": "room5",
                "schedule_image": "v-spotlights-app2.png",
                "chair": [
                    "Ayan Biswas",
                    "Subhashis Hazarika",
                    "Natalie Klein",
                    "Earl Lawrence",
                    "Divya Banesh",
                    "Steven Morley"
                ],
                "organizers": [
                    "Ayan Biswas",
                    "Divya Banesh",
                    "Natalie Klein",
                    "Steven Morley",
                    "Subhashis Hazarika",
                    "Earl Lawrence"
                ],
                "time_start": "2021-10-27T17:00:00Z",
                "time_end": "2021-10-27T18:30:00Z",
                "discord_category": "",
                "discord_channel": "",
                "discord_channel_id": 0,
                "youtube_url": null,
                "zoom_meeting": "",
                "zoom_password": "",
                "ff_link": "",
                "ff_playlist": "",
                "time_slots": [
                    {
                        "type": "live",
                        "title": "In Situ Inference: Advanced Data Science for Space Weather Modeling",
                        "contributors": [
                            "Ayan Biswas",
                            "Divya Banesh",
                            "Natalie Klein",
                            "Steven Morley",
                            "Subhashis Hazarika",
                            "Earl Lawrence"
                        ],
                        "abstract": null,
                        "time_start": "2021-10-27T17:00:00Z",
                        "time_end": "2021-10-27T18:30:00Z"
                    }
                ]
            },
            {
                "title": "Bridging Visualization with Radiation Oncology",
                "session_id": "v-spotlights-app3",
                "track": "room5",
                "schedule_image": "v-spotlights-app3.png",
                "chair": [
                    "Ludvig P. Muren",
                    "Renata G. Raidou",
                    "Katja B\u00fchler",
                    "Uulke van der Heide",
                    "Wouter van Elmpt",
                    "Vitali Moiseenko",
                    "Noeska Smit"
                ],
                "organizers": [
                    "Renata G. Raidou",
                    "Ludvig P. Muren",
                    "Wouter van Elmpt",
                    "Katja B\u00fchler",
                    "Noeska Smit",
                    "Vitali Moiseenko",
                    "Uulke van der Heide"
                ],
                "time_start": "2021-10-28T13:00:00Z",
                "time_end": "2021-10-28T14:30:00Z",
                "discord_category": "",
                "discord_channel": "",
                "discord_channel_id": 0,
                "youtube_url": null,
                "zoom_meeting": "",
                "zoom_password": "",
                "ff_link": "",
                "ff_playlist": "",
                "time_slots": [
                    {
                        "type": "live",
                        "title": "Bridging Visualization with Radiation Oncology",
                        "contributors": [
                            "Renata G. Raidou",
                            "Katja Buehler",
                            "Noeska Smit",
                            "Vitali\u00a0Moiseenko",
                            "Uulke van der Heide",
                            "Katar\u00edna Furmanov\u00e1",
                            "Ludvig P. Muren",
                            "Wouter van Elmpt"
                        ],
                        "abstract": null,
                        "time_start": "2021-10-28T13:00:00Z",
                        "time_end": "2021-10-28T14:30:00Z"
                    }
                ]
            },
            {
                "title": "Challenges for Visualization in Immersive Planetarium Domes",
                "session_id": "v-spotlights-app4",
                "track": "room5",
                "schedule_image": "v-spotlights-app4.png",
                "chair": [
                    "Alexander Bock",
                    "Mark SubbaRao",
                    "Anders Ynnerman"
                ],
                "organizers": [
                    "Anders Ynnerman",
                    "Mark SubbaRao",
                    "Alexander Bock"
                ],
                "time_start": "2021-10-28T15:00:00Z",
                "time_end": "2021-10-28T16:30:00Z",
                "discord_category": "",
                "discord_channel": "",
                "discord_channel_id": 0,
                "youtube_url": null,
                "zoom_meeting": "",
                "zoom_password": "",
                "ff_link": "",
                "ff_playlist": "",
                "time_slots": [
                    {
                        "type": "live",
                        "title": "Challenges for Visualization in Immersive Planetarium Domes",
                        "contributors": [
                            "Anders Ynnerman",
                            "Mark SubbaRao",
                            "Alexander Bock"
                        ],
                        "abstract": null,
                        "time_start": "2021-10-28T15:00:00Z",
                        "time_end": "2021-10-28T16:30:00Z"
                    }
                ]
            },
            {
                "title": "Uncertainty-aware visual analytics in applications - Towards a systematic approach",
                "session_id": "v-spotlights-app1",
                "track": "room5",
                "schedule_image": "v-spotlights-app1.png",
                "chair": [
                    "Karsten Rink",
                    "Christina Gillmann",
                    "Petra Gospodnetic"
                ],
                "organizers": [
                    "Christina Gillmann",
                    "Petra Gospodnetic",
                    "Karsten Rink"
                ],
                "time_start": "2021-10-29T13:00:00Z",
                "time_end": "2021-10-29T14:30:00Z",
                "discord_category": "",
                "discord_channel": "",
                "discord_channel_id": 0,
                "youtube_url": null,
                "zoom_meeting": "",
                "zoom_password": "",
                "ff_link": "",
                "ff_playlist": "",
                "time_slots": [
                    {
                        "type": "live",
                        "title": "Uncertainty-aware visual analytics in applications - Towards a systematic approach",
                        "contributors": [
                            "Christina Gillmann",
                            "Petra Gospodnetic",
                            "Karsten Rink",
                            "Katja Schladitz",
                            "Tushar Athawale",
                            "Ibrahim Demir"
                        ],
                        "abstract": null,
                        "time_start": "2021-10-29T13:00:00Z",
                        "time_end": "2021-10-29T14:30:00Z"
                    }
                ]
            },
            {
                "title": "Visualization Challenges in Deep Uncertainty",
                "session_id": "v-spotlights-app5",
                "track": "room5",
                "schedule_image": "v-spotlights-app5.png",
                "chair": [
                    "Evan Savage"
                ],
                "organizers": [],
                "time_start": "2021-10-29T15:00:00Z",
                "time_end": "2021-10-29T16:30:00Z",
                "discord_category": "",
                "discord_channel": "",
                "discord_channel_id": 0,
                "youtube_url": null,
                "zoom_meeting": "",
                "zoom_password": "",
                "ff_link": "",
                "ff_playlist": "",
                "time_slots": [
                    {
                        "type": "live",
                        "title": "Visualization Challenges in Deep Uncertainty",
                        "contributors": [
                            "Steven Popper",
                            "Kristi Potter",
                            "Nathan Lee",
                            "David Gold",
                            "Robert Lempert"
                        ],
                        "abstract": null,
                        "time_start": "2021-10-29T15:00:00Z",
                        "time_end": "2021-10-29T16:30:00Z"
                    }
                ]
            }
        ]
    },
    "a-sciviscontest": {
        "event": "SciVis Contest",
        "long_name": "SciVis Contest",
        "event_type": "Associated Event",
        "event_description": "",
        "event_url": "",
        "sessions": [
            {
                "title": "SciVis Contest",
                "session_id": "a-sciviscontest-contest",
                "track": "room5",
                "schedule_image": "a-sciviscontest-contest.png",
                "chair": [
                    "Thomas Theu\u00dfl",
                    "Alex Razoumov"
                ],
                "organizers": [
                    "Thomas Theu\u00dfl",
                    "Alex Razoumov"
                ],
                "time_start": "2021-10-28T17:00:00Z",
                "time_end": "2021-10-28T18:30:00Z",
                "discord_category": "",
                "discord_channel": "",
                "discord_channel_id": 0,
                "youtube_url": null,
                "zoom_meeting": "",
                "zoom_password": "",
                "ff_link": "",
                "ff_playlist": "",
                "time_slots": [
                    {
                        "type": "live",
                        "title": "Opening Remarks",
                        "contributors": [
                            "Alex Razoumov"
                        ],
                        "abstract": null,
                        "time_start": "2021-10-28T17:00:00Z",
                        "time_end": "2021-10-28T17:10:00Z"
                    },
                    {
                        "type": "recorded",
                        "title": "Domain scientist talk",
                        "contributors": [
                            "Hosein Shahnas"
                        ],
                        "abstract": null,
                        "time_start": "2021-10-28T17:10:00Z",
                        "time_end": "2021-10-28T17:22:00Z",
                        "youtube_video_id": "0XEpN63trTA"
                    },
                    {
                        "type": "recorded",
                        "title": "Hybrid rendering for interactive visualization of mantle convection",
                        "contributors": [
                            "Tim McGraw"
                        ],
                        "abstract": null,
                        "time_start": "2021-10-28T17:22:00Z",
                        "time_end": "2021-10-28T17:30:00Z",
                        "youtube_video_id": "FzW24hbl6q8"
                    },
                    {
                        "type": "recorded",
                        "title": "RayPC: Interactive Ray Tracing Meets Parallel Coordinates",
                        "contributors": [
                            "Simon Schneegans"
                        ],
                        "abstract": null,
                        "time_start": "2021-10-28T17:30:00Z",
                        "time_end": "2021-10-28T17:38:00Z",
                        "youtube_video_id": "QmT5zZ_zwkw"
                    },
                    {
                        "type": "recorded",
                        "title": "Visualization of simulated convection dynamics in Earth's mantle",
                        "contributors": [
                            "Lucas Temor"
                        ],
                        "abstract": null,
                        "time_start": "2021-10-28T17:38:00Z",
                        "time_end": "2021-10-28T17:46:00Z",
                        "youtube_video_id": "dvNZkvMp390"
                    },
                    {
                        "type": "recorded",
                        "title": "Investigating Multivariate, Vector, and Topological Data Analysis Techniques for Mantle Flow Pattern Visualization",
                        "contributors": [
                            "Sudhanshu Sane"
                        ],
                        "abstract": null,
                        "time_start": "2021-10-28T17:46:00Z",
                        "time_end": "2021-10-28T17:54:00Z",
                        "youtube_video_id": "O0V687MDF6Y"
                    },
                    {
                        "type": "recorded",
                        "title": "Visual Analysis of Spatio-temporal Features in Multi-field Earth's Mantle Convection Simulations",
                        "contributors": [
                            "Karim Huesmann"
                        ],
                        "abstract": null,
                        "time_start": "2021-10-28T17:54:00Z",
                        "time_end": "2021-10-28T18:02:00Z",
                        "youtube_video_id": "emjwHUtTxIk"
                    },
                    {
                        "type": "recorded",
                        "title": "EarthGAN: Can we visualize the Earth\u2019s mantle convection using a surrogate model?",
                        "contributors": [
                            "Tim von Hahn"
                        ],
                        "abstract": null,
                        "time_start": "2021-10-28T18:02:00Z",
                        "time_end": "2021-10-28T18:10:00Z",
                        "youtube_video_id": "HAt8huF3CIY"
                    },
                    {
                        "type": "recorded",
                        "title": "Interactive Multidimensional Visual Analytics for Earth's Mantle Convection",
                        "contributors": [
                            "Jansen Wong"
                        ],
                        "abstract": null,
                        "time_start": "2021-10-28T18:10:00Z",
                        "time_end": "2021-10-28T18:18:00Z",
                        "youtube_video_id": "VJ3jwVq1m4w"
                    },
                    {
                        "type": "live",
                        "title": "Competition results",
                        "contributors": [
                            "Alex Razoumov"
                        ],
                        "abstract": null,
                        "time_start": "2021-10-28T18:18:00Z",
                        "time_end": "2021-10-28T18:23:00Z"
                    },
                    {
                        "type": "recorded",
                        "title": "2022 SciVis Contest preview",
                        "contributors": [
                            "Divya Banesh"
                        ],
                        "abstract": null,
                        "time_start": "2021-10-28T18:23:00Z",
                        "time_end": "2021-10-28T18:30:00Z",
                        "youtube_video_id": "hdcW5-7o6hY"
                    }
                ]
            }
        ]
    },
    "x-vis22-kickoff": {
        "event": "VIS",
        "long_name": "VIS",
        "event_type": "VIS",
        "event_description": "",
        "event_url": "",
        "sessions": [
            {
                "title": "VIS 2022 Kickoff Meeting",
                "session_id": "x-vis22-kickoff-1",
                "track": "room7",
                "schedule_image": "x-vis22-kickoff-1.png",
                "chair": [],
                "organizers": [],
                "time_start": "2021-10-28T18:30:00Z",
                "time_end": "2021-10-28T19:00:00Z",
                "discord_category": "",
                "discord_channel": "",
                "discord_channel_id": 0,
                "youtube_url": null,
                "zoom_meeting": "",
                "zoom_password": "",
                "ff_link": "",
                "ff_playlist": "",
                "time_slots": [
                    {
                        "type": "Streamed?",
                        "title": "VIS 2022 Kickoff Meeting",
                        "contributors": "",
                        "abstract": null,
                        "time_start": "2021-10-28T18:30:00Z",
                        "time_end": "2021-10-28T19:00:00Z"
                    }
                ]
            }
        ]
    },
    "x-vis-townhall": {
        "event": "VIS",
        "long_name": "VIS",
        "event_type": "VIS",
        "event_description": "",
        "event_url": "",
        "sessions": [
            {
                "title": "VIS Townhall",
                "session_id": "x-vis-townhall-1",
                "track": "room1",
                "schedule_image": "x-vis-townhall-1.png",
                "chair": [],
                "organizers": [],
                "time_start": "2021-10-28T19:00:00Z",
                "time_end": "2021-10-28T20:30:00Z",
                "discord_category": "",
                "discord_channel": "",
                "discord_channel_id": 0,
                "youtube_url": null,
                "zoom_meeting": "",
                "zoom_password": "",
                "ff_link": "",
                "ff_playlist": "",
                "time_slots": [
                    {
                        "type": "Streamed?",
                        "title": "VIS Townhall",
                        "contributors": "",
                        "abstract": null,
                        "time_start": "2021-10-28T19:00:00Z",
                        "time_end": "2021-10-28T20:30:00Z"
                    }
                ]
            }
        ]
    },
    "x-vis": {
        "event": "VIS",
        "long_name": "VIS",
        "event_type": "VIS",
        "event_description": "",
        "event_url": "",
        "sessions": [
            {
                "title": "Student Mentorship Session",
                "session_id": "x-vis-student-mentoring",
                "track": "room2",
                "schedule_image": "x-vis-student-mentoring.png",
                "chair": [],
                "organizers": [],
                "time_start": "2021-10-28T19:00:00Z",
                "time_end": "2021-10-28T20:00:00Z",
                "discord_category": "",
                "discord_channel": "",
                "discord_channel_id": 0,
                "youtube_url": null,
                "zoom_meeting": "",
                "zoom_password": "",
                "ff_link": "",
                "ff_playlist": "",
                "time_slots": [
                    {
                        "type": "Streamed?",
                        "title": "Student Mentorship Session",
                        "contributors": "",
                        "abstract": null,
                        "time_start": "2021-10-28T19:00:00Z",
                        "time_end": "2021-10-28T20:00:00Z"
                    }
                ]
            }
        ]
    },
    "x-vis-closing": {
        "event": "VIS",
        "long_name": "VIS",
        "event_type": "VIS Closing",
        "event_description": "",
        "event_url": "",
        "sessions": [
            {
                "title": "VIS Capstone by Fernanda Vi\u00e9gas and Martin Wattenberg",
                "session_id": "x-vis-closing1",
                "track": "room1",
                "schedule_image": "x-vis-closing1.png",
                "chair": [],
                "organizers": [],
                "time_start": "2021-10-29T17:00:00Z",
                "time_end": "2021-10-29T18:30:00Z",
                "discord_category": "",
                "discord_channel": "",
                "discord_channel_id": 0,
                "youtube_url": null,
                "zoom_meeting": "",
                "zoom_password": "",
                "ff_link": "",
                "ff_playlist": "",
                "time_slots": [
                    {
                        "type": "live",
                        "title": "VIS Capstone by Fernanda Vi\u00e9gas and Martin Wattenberg",
                        "contributors": [
                            "Fernanda Vi\u00e9gas",
                            "Martin Wattenberg"
                        ],
                        "abstract": null,
                        "time_start": "2021-10-29T17:00:00Z",
                        "time_end": "2021-10-29T18:30:00Z"
                    },
                    {
                        "type": "live",
                        "title": "VIS Closing",
                        "contributors": "",
                        "abstract": null,
                        "time_start": "2021-10-29T17:00:00Z",
                        "time_end": "2021-10-29T18:30:00Z"
                    }
                ]
            }
        ]
    }
}