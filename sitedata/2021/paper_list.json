{
    "a-vds-11008": {
        "authors": [
            "Subhajit Das",
            "Alex Endert"
        ],
        "title": "CACTUS: Detecting and Resolving Conflicts in Objective Functions",
        "session_id": "a-vds",
        "abstract": "Machine learning (ML) models are constructed by expert ML practitioners using various coding languages, in which they tune and select model hyperparameters and learning algorithms for a given problem domain. In multi-objective optimization, conflicting objectives and constraints is a major area of concern. In such problems, several competing objectives are seen for which no single optimal solution is found that satisfies all desired objectives simultaneously. In the past, visual analytic (VA) systems have allowed users to interactively construct objective functions for a classifier. In this paper, we extend this line of work by prototyping a technique to visualize multi-objective objective functions either defined in a Jupyter notebook or defined using an interactive visual interface to help users to detect and resolve conflicting objectives. Visualization of the objective function enlightens potentially conflicting objectives that obstructs selecting correct solution(s) for the desired ML task or goal. We also present an enumeration of potential conflicts in objective specification in multi-objective objective functions for classifier selection. Furthermore, we demonstrate our approach in a VA system that helps users in specifying meaningful objective functions to a classifier by detecting and resolving conflicting objectives.",
        "keywords": [],
        "uid": "a-vds-11008",
        "time_stamp": "2021-10-24T14:25:00Z",
        "has_image": false,
        "paper_award": "",
        "image_caption": "",
        "external_paper_link": "",
        "has_pdf": false,
        "ff_link": ""
    },
    "a-vds-21006": {
        "authors": [
            "Joseph Cottam",
            "Maria Glenski",
            "Zhuanyi Huang",
            "Ryan Rabello",
            "Austin Golding",
            "Svitlana Volkova",
            "Dustin L Arendt"
        ],
        "title": "Graph Comparison for Causal Discovery",
        "session_id": "a-vds",
        "abstract": "Reasoning about cause and effect is one of the frontiers for modern machine learning. Many causality techniques reason over a ``causal graph'' provided as input to the problem. When a causal graph cannot be produced from human expertise, ``causal discovery'' algorithms can be used to generate one from data. Unfortunately, causal discovery algorithms vary wildly in their results due to unrealistic data and modeling assumptions, so the results still need to be manually validated and adjusted. This paper presents a graph comparison tool designed to help analysts curate causal discovery results. This tool facilitates feedback loops whereby an analyst compares proposed graphs from multiple algorithms (or ensembles) and then uses insights from the comparison to refine parameters and inputs to the algorithms. We illustrate different types of comparisons and show how the interplay of causal discovery and graph comparison improves causal discovery.",
        "keywords": [],
        "uid": "a-vds-21006",
        "time_stamp": "2021-10-24T14:40:00Z",
        "has_image": false,
        "paper_award": "",
        "image_caption": "",
        "external_paper_link": "",
        "has_pdf": false,
        "ff_link": ""
    },
    "a-vds-31005": {
        "authors": [
            "Deepthi Raghunandan",
            "Zhe Cui",
            "Kartik Krishnan",
            "Segen Tirfe",
            "Shenzhi Shi",
            "Tejaswi Darshan Shrestha",
            "Leilani Battle",
            "Niklas Elmqvist"
        ],
        "title": "Lodestar: Supporting Independent Learning and Rapid Experimentation Through Data-Driven Analysis Recommendations",
        "session_id": "a-vds",
        "abstract": "Keeping abreast of current trends, technologies, and best practices in visualization and data analysis is becoming increasingly difficult, especially for fledgling data scientists. In this paper, we propose Lodestar, an interactive computational notebook that allows users to quickly explore and construct new data science workflows by selecting from a list of automated analysis recommendations. We derive our recommendations from directed graphs of known analysis states, with two input sources: one manually curated from online data science tutorials, and another extracted through semi-automatic analysis of a corpus of over 6,000 Jupyter notebooks. We evaluate Lodestar in a formative study guiding our next set of improvements to the tool. The evaluation suggests that users find Lodestar useful for rapidly creating data science workflows.",
        "keywords": [],
        "uid": "a-vds-31005",
        "time_stamp": "2021-10-24T14:55:00Z",
        "has_image": false,
        "paper_award": "",
        "image_caption": "",
        "external_paper_link": "",
        "has_pdf": false,
        "ff_link": ""
    },
    "a-vds-41007": {
        "authors": [
            "Anamaria Crisan",
            "Vidya Setlur"
        ],
        "title": "Natto: Rapid Visual Iteration of Analytic Data Models with Intelligent Assistance",
        "session_id": "a-vds",
        "abstract": "Data analysts need to routinely transform data into a form conducive for deeper investigation. While there exists a myriad of tools to support this task on tabular data, few tools exist to support analysts with more complex data types. In this study, we investigate how analysts process and transform large sets of XML data to create an analytic data model useful to further their analysis. We conduct a set of formative interviews with four experts that have diverse yet specialized knowledge of a common dataset. From these interviews, we derive a set of goals, tasks, and design requirements for transforming XML data into an analytic data model. We implement Natto as a proof-of-concept prototype that actualizes these design requirements into a set of visual and interaction design choices. We demonstrate the utility of the system through the presentation of analysis scenarios using real-world data. Our research contributes novel insights into the unique challenges of transforming data that is both hierarchical and internally linked. Further, it extends the knowledge of the visualization community in the areas of data preparation and wrangling.",
        "keywords": [],
        "uid": "a-vds-41007",
        "time_stamp": "2021-10-24T15:10:00Z",
        "has_image": false,
        "paper_award": "",
        "image_caption": "",
        "external_paper_link": "",
        "has_pdf": false,
        "ff_link": ""
    },
    "a-vahc-1007": {
        "authors": [
            "Fahd Husain",
            "Rosa Romero-G\u00f3mez",
            "Emily Kuang",
            "Dario Segura",
            "Adamo Carolli Carolli",
            "Lai Chung Liu",
            "Manfred Cheung",
            "Yohann Paris"
        ],
        "title": "A Multi-scale Visual Analytics Approach for Exploring Biomedical Knowledge",
        "session_id": "a-vahc-1",
        "abstract": "This paper describes an ongoing multi-scale visual analytics approach for exploring and analyzing biomedical knowledge at scale. We utilize global and local views, hierarchical and flow-based graph layouts, multi-faceted search, neighborhood recommendations, and document visualizations to help researchers interactively explore, query, and analyze biological graphs against the backdrop of biomedical knowledge. The generality of our approach - insofar as it re-quires only knowledge graphs linked to documents - means it can support a range of therapeutic use cases across different domains, from disease propagation to drug discovery. Early interactions with domain experts support our approach for use cases with graphs with over 40,000 nodes and 350,000 edges.",
        "keywords": [],
        "uid": "a-vahc-1007",
        "time_stamp": "2021-10-24T14:10:00Z",
        "has_image": true,
        "paper_award": "",
        "image_caption": "A biomedical researcher uses the graph prototype to investigate potential drug treatments for SARS-CoV-2 using the COVID-19 biological graph that was automatically derived from literature. (A) The Global View provides an overview of the biological graph with bundled edges and nodes organized hierarchically in a biomedical ontology. The results of searching for links from several articles using DOIs are highlighted. (B) The Local View shows the highlighted results extracted as a node-link flow graph for further analysis. (C) The Drill-down Panel displays the underlying evidence extracted from scientific articles for the inhibition relationship between tocilizumab and IL6. \n",
        "external_paper_link": "",
        "has_pdf": true,
        "ff_link": ""
    },
    "w-trex-1418": {
        "authors": [
            "Nadia Boukhelifa",
            "Evelyne Lutton",
            "Anastasia Bezerianos"
        ],
        "title": "A Case Study of Using Analytic Provenance to Reconstruct User Trust in a Guided Visual Analytics System",
        "session_id": "w-trex",
        "abstract": "In this paper, we demonstrate how analytic provenance can be exploited to re-construct user trust in a guided Visual Analytics (VA) system, and suggest that interaction log data analysis can be a valuable tool for on-line trust monitoring. Our approach moves away from the subjective and often a-posteriori evaluations of user trust, towards more objective measures that are not only continuously tracked and updated, but also reflect both the confidence of the user in system suggestions, and the uncertainty of the system with regards to user goals. We argue that this approach is more suitable for guided visual analytics systems such as ours, where user strategies, goals and even trust can evolve over time, in reaction to new system feedback and insights from the exploration. Through the analysis of log data from a past user study with twelve participants performing a guided visual analysis task, we found that the stability of user exploration strategies is a promising factor to study \u201ctrust\u201d. However, indirect metrics based on provenance, such as user evaluation counts and disagreement rates, are alone not sufficient to study trust reliably in guided VAs. We conclude with open challenges and opportunities for exploiting analytic provenance to support trust monitoring in guided VA systems.",
        "keywords": [],
        "uid": "w-trex-1418",
        "time_stamp": "2021-10-24T13:15:00Z",
        "has_image": false,
        "paper_award": "",
        "image_caption": "",
        "external_paper_link": "",
        "has_pdf": false,
        "ff_link": "https://youtu.be/DI5zEkurlYk"
    },
    "w-trex-8188": {
        "authors": [
            "Anna-Pia Lohfink",
            "Vera Marie Memmesheimer",
            "Frederike Gartzky",
            "Christoph Garth"
        ],
        "title": "The Enhanced Security in Process System - Evaluating Knowledge Assistance",
        "session_id": "w-trex",
        "abstract": "We present evaluation results of our enhancements to the Security in Process System developed by Lohfink et al. to support triage analysis in operational technology networks. To ensure fast and appropriate reactions to anomalies in device readings, this system communicates anomaly detection results and device readings to incorporate human expertise and experience. It exploits periodical behavior in the data combining spiral plots with results from anomaly detection. To support decisions, increase trust, and support cooperation in the system we enhanced it to be knowledge-assisted. A central knowledge base allows sharing knowledge between users and support during analysis. It consists of an ontology describing incidents, and a data base holding collections of exemplary sensor readings with annotations and visualization parameters. Related knowledge is proposed automatically and incorporated directly in the visualization to provide assistance that is closely coupled to the application, without additional hurdles. This integration is designed aiming on additional support for correct and fast detection of anomalies in the visualized device readings. We evaluate our enhancements to the Security in Process System in terms of effectiveness, efficiency, user satisfaction, and cognitive load with a detailed user study. Comparing the original and enhanced system, we are able to draw conclusions as to how our design narrows the knowledge gap between experts and laymen. Furthermore, we present and discuss the results and impact on our future research.",
        "keywords": [],
        "uid": "w-trex-8188",
        "time_stamp": "2021-10-24T13:25:00Z",
        "has_image": false,
        "paper_award": "",
        "image_caption": "",
        "external_paper_link": "",
        "has_pdf": false,
        "ff_link": "https://youtu.be/Z2l2-n8sURo"
    },
    "w-trex-8311": {
        "authors": [
            "Wenkai Han"
        ],
        "title": "Making and Trusting Decisions in Visual Analytics",
        "session_id": "w-trex",
        "abstract": "Decision making and trust have both become rising topics in the research community of Visual Analytics (VA).\nMany efforts have been made to understand and facilitate making decisions with VA, as well as build and calibrate trust.\nHowever, previous research largely took VA as a tool to facilitate decision making, but did not explore the possibility to dissect each analytical step in VA as decision making and discuss how decision making theories can be utilized to improve the trustworthiness of decisions in VA.\nTherefore, this paper instead proposes such alternative take on the relation between decision making and VA, inspects the processes of visually analyzing data as decision making, and discusses how to leverage decision making theories to facilitate trustworthy decision making in VA.",
        "keywords": [],
        "uid": "w-trex-8311",
        "time_stamp": "2021-10-24T13:35:00Z",
        "has_image": false,
        "paper_award": "",
        "image_caption": "",
        "external_paper_link": "",
        "has_pdf": false,
        "ff_link": "https://youtu.be/0Wpt5Rj1wl0"
    },
    "w-trex-5186": {
        "authors": [
            "John Wenskovitch",
            "Corey Fallon",
            "Kate Miller",
            "Aritra Dasgupta"
        ],
        "title": "Beyond Visual Analytics: Human-Machine Teaming for AI-Driven Data Sensemaking",
        "session_id": "w-trex",
        "abstract": "\u201cDetect the expected, discover the unexpected\u201d was the founding principle of the field of visual analytics. This mantra implies human stakeholders, like a domain expert or data analyst, could leverage visual analytics techniques to seek answers to known unknowns and discover unknown unknowns in the course of the data sensemaking process. We argue that in the era of AI-driven automation, the roles of humans and machines (e.g., a machine learning model) need to be recalibrated in terms of a team. We posit that by realizing human-machine teams as a stakeholder unit, we can better achieve the best of both worlds: automation transparency and human reasoning efficacy. However, this also increases the burden on analysts and domain experts towards performing more cognitively demanding tasks than they are used to. In this paper, we reflect on the complementary roles in a human-machine team through the lens of cognitive psychology and map them to existing and emerging research in the visual analytics community. We discuss open questions around the nature of human agency and analyze the shared responsibilities in human-machine teams.",
        "keywords": [],
        "uid": "w-trex-5186",
        "time_stamp": "2021-10-24T13:45:00Z",
        "has_image": false,
        "paper_award": "",
        "image_caption": "",
        "external_paper_link": "",
        "has_pdf": false,
        "ff_link": "https://youtu.be/sPesrLfBz0A"
    },
    "w-trex-6127": {
        "authors": [
            "Yafeng Lu",
            "Michael Steptoe",
            "Verica Buchanan",
            "Nancy Cooke",
            "Ross Maciejewski"
        ],
        "title": "Evaluating Forecasting, Knowledge, and Visual Analytics",
        "session_id": "w-trex",
        "abstract": "In this paper, we explore the intersection of knowledge and the forecasting accuracy of humans when supported by visual analytics. We have recruited 40 experts in machine learning and trained them in the use of a box office forecasting visual analytics system.\nOur goal was to explore the impact of visual analytics and knowledge in human-machine forecasting. This paper reports on how participants explore and reason with data and develop a forecast when provided with a predictive model of middling performance ($R^2 \\approx .7$). We vary the knowledge base of the participants through training, compare the forecasts to the baseline model, and discuss performance in the context of previous work on algorithmic aversion and trust.",
        "keywords": [],
        "uid": "w-trex-6127",
        "time_stamp": "2021-10-24T13:55:00Z",
        "has_image": false,
        "paper_award": "",
        "image_caption": "",
        "external_paper_link": "",
        "has_pdf": false,
        "ff_link": "https://youtu.be/OWfUNDsSSrk"
    },
    "w-trex-1899": {
        "authors": [
            "Udo Schlegel",
            "Daniel Keim"
        ],
        "title": "Time Series Model Attribution Visualizations as Explanations",
        "session_id": "w-trex",
        "abstract": "Attributions are a common local explanation technique for deep learning models on single samples as they are easily extractable and demonstrate the relevance of input values.\nIn many cases, heatmaps visualize such attributions for samples, for instance, on images.\nHowever, heatmaps are not always the ideal visualization to explain certain model decisions for other data types.\nIn this survey, we focus on attribution visualizations for time series.\nWe collect attribution heatmap visualizations and some alternatives, discuss the advantages as well as disadvantages and give a short position towards future opportunities for attributions and explanations for time series.",
        "keywords": [],
        "uid": "w-trex-1899",
        "time_stamp": "2021-10-24T15:40:00Z",
        "has_image": false,
        "paper_award": "",
        "image_caption": "",
        "external_paper_link": "",
        "has_pdf": false,
        "ff_link": "https://youtu.be/yK0zCVIVoWY"
    },
    "w-trex-8523": {
        "authors": [
            "Jeroen Ooge",
            "Katrien Verbert"
        ],
        "title": "Trust in Prediction Models: a Mixed-Methods Pilot Study on the Impact of Domain Expertise",
        "session_id": "w-trex",
        "abstract": "Users' trust in prediction models can be affected by many factors, including domain knowledge and experience with predictive modelling. However, it is not entirely clear to what extent and why this domain expertise impacts users' trust perceptions and evolutions. In addition, it remains challenging to accurately measure users' trust. We share our results and experiences of an exploratory case-study with a visual analytics system that incorporates a prediction model for time series. Through a mixed-methods approach involving Likert-type questionnaires and semi-structured interviews, we investigate users' trust evolutions and factors that affect their trust in the prediction model. Our results underline the multi-faceted nature of user trust, and suggest that domain expertise certainly affects trust, though it cannot fully foresee users' trust evolutions.",
        "keywords": [],
        "uid": "w-trex-8523",
        "time_stamp": "2021-10-24T15:50:00Z",
        "has_image": false,
        "paper_award": "",
        "image_caption": "",
        "external_paper_link": "",
        "has_pdf": false,
        "ff_link": "https://youtu.be/UEcJPJrJ8mQ"
    },
    "w-trex-2998": {
        "authors": [
            "Dirk Leffrang",
            "Oliver Mueller"
        ],
        "title": "Should I Follow this Model? The Effect of Uncertainty Visualization on the Acceptance of Time Series Forecasts",
        "session_id": "w-trex",
        "abstract": "Time series forecasts are ubiquitous, ranging from daily weather forecasts to projections of pandemics such as COVID-19. Communicating the uncertainty associated with such forecasts is important, because it may affect users' trust in a forecasting model and, in turn, the decisions made based on the model. Although there exists a growing body of research on visualizing uncertainty in general, the important case of visualizing prediction uncertainty in time series forecasting has not been studied yet. Against this background, we investigated how different visualizations of predictive uncertainty affect the extent to which people follow predictions of a time series forecasting model. More specifically, we conducted an online experiment on forecasting occupied hospital beds due to the COVID-19 pandemic, measuring the influence of uncertainty visualization of algorithmic predictions on participants' own predictions. In contrast to prior studies, our empirical results suggest that more salient visualizations of uncertainty lead to decreased willingness to follow algorithmic forecasts.",
        "keywords": [],
        "uid": "w-trex-2998",
        "time_stamp": "2021-10-24T16:00:00Z",
        "has_image": false,
        "paper_award": "",
        "image_caption": "",
        "external_paper_link": "",
        "has_pdf": false,
        "ff_link": "https://youtu.be/BaKlTFM8ZSk"
    },
    "w-trex-8391": {
        "authors": [
            "Christina Gillmann",
            "Dorothee Saur",
            "Gerik Scheuermann"
        ],
        "title": "How to deal with Uncertainty in Machine Learning for Medical Applications?",
        "session_id": "w-trex",
        "abstract": "Recently, machine learning is massively on the rise in medical applications providing the ability to predict diseases, plan treatment, and monitor progress. Still, the use in a clinical context of this technology is rather rare, mostly due to the missing trust of clinicians. In this position paper, we aim to show how uncertainty is introduced in the machine learning process when applying it to the medical do-main at multiple points and how this influences the decision-making process of clinicians in machine learning approaches. Based on this knowledge, we aim to refine the guidelines for trust in visual analytics to assist clinicians in using and understanding systems that are based on machine learning.",
        "keywords": [],
        "uid": "w-trex-8391",
        "time_stamp": "2021-10-24T16:10:00Z",
        "has_image": false,
        "paper_award": "",
        "image_caption": "",
        "external_paper_link": "",
        "has_pdf": false,
        "ff_link": ""
    },
    "a-vahc-1003": {
        "authors": [
            "Salmah Ahmad",
            "David Sessler",
            "J\u00f6rn Kohlhammer"
        ],
        "title": "Towards a Comprehensive Cohort Visualization of Patients with Inflammatory Bowel Disease",
        "session_id": "a-vahc-2",
        "abstract": "This paper reports on a joint project with medical experts on inflammatory bowel disease (IBD). Patients suffering from IBD, e.g. Crohn\u2019s disease or ulcerative colitis, do not have a reduced life expectancy and disease progressions easily span several decades. We designed a visualization to highlight information that is vital for comparing patients and progressions, especially with respect to the treatments administered over the years. Medical experts can interactively determine the amount of information displayed and can synchronize the progressions to the beginning of certain treatments and medications. While the visualization was designed in close collaboration with IBD experts, we additionally evaluated our approach with 35 participants to ensure good usability and accessibility. The paper also highlights the future work on similarity definition and additional visual features in this on-going project.",
        "keywords": [],
        "uid": "a-vahc-1003",
        "time_stamp": "2021-10-24T15:00:00Z",
        "has_image": true,
        "paper_award": "",
        "image_caption": "Visual cohort analysis of patients with inflammatory bowel disease. \nThe visualization shows patients as rows filled with data selected in the menu on the left.\nPatients can be synchronized to a specific element.\nAdditional information for each element is shown in a tooltip.\n",
        "external_paper_link": "",
        "has_pdf": true,
        "ff_link": ""
    },
    "a-vahc-1009": {
        "authors": [
            "Jinbin Huang",
            "Jonathan Douglas Plasencia",
            "Dianna M.E. Bardo",
            "Nicholas C. Rubert",
            "Erik G. Ellsworth",
            "Steven D. Zangwill",
            "Chris Bryan"
        ],
        "title": "Phoenix Virtual Heart: A Hybrid VR-Desktop Visualization System for Cardiac Surgery Planning and Education",
        "session_id": "a-vahc-2",
        "abstract": "Physicians diagnosing and treating complex, structural congenital heart disease (CHD), i.e., heart defects present at birth, often rely on visualization software that scrolls through a volume stack of two-dimensional (2D) medical images. Due to limited display dimensions, conventional desktop-based applications have difficulties facilitating physicians converting 2D images to 3D intelligence. Recently, 3D printing of anatomical models has emerged as a technique to analyze CHD, but current workflows are tedious. To this end, we introduce and describe our ongoing work developing the Phoenix Virtual Heart (PVH), a hybrid VR-desktop software to aid in CHD surgical planning and family consultation. PVH is currently being integrated into a 3D printing workflow at a children's hospital as a way to increase physician efficiency and confidence, allowing physicians to analyze virtual anatomical models for surgical planning and family consultation.",
        "keywords": [],
        "uid": "a-vahc-1009",
        "time_stamp": "2021-10-24T15:20:00Z",
        "has_image": true,
        "paper_award": "",
        "image_caption": "",
        "external_paper_link": "",
        "has_pdf": false,
        "ff_link": ""
    },
    "a-vahc-1012": {
        "authors": [
            "Ashley Suh",
            "Gabriel Appleby",
            "Erik W Anderson",
            "Luca Finelli",
            "Dylan Cashman"
        ],
        "title": "Communicating Performance of Regression Models Using Visualization in Pharmacovigilance",
        "session_id": "a-vahc-2",
        "abstract": "We describe the iterative design process that led to PVH, discuss how it fits into a 3D printing workflow, and present formative feedback from clinicians that are beginning to use the application.",
        "keywords": [],
        "uid": "a-vahc-1012",
        "time_stamp": "2021-10-24T15:40:00Z",
        "has_image": true,
        "paper_award": "",
        "image_caption": "",
        "external_paper_link": "",
        "has_pdf": true,
        "ff_link": ""
    },
    "a-vahc-1010": {
        "authors": [
            "Tianyi Zhang",
            "Thomas H. McCoy",
            "Roy H. Perlis",
            "Finale Doshi-Velez",
            "Prof. Elena L. Glassman"
        ],
        "title": "Interactive Cohort Analysis and Hypothesis Discovery by Exploring Temporal Patterns in Population-Level Health Records",
        "session_id": "a-vahc-2",
        "abstract": "It is challenging to visualize temporal patterns in electronic health records (EHRs) due to the high volume and high dimensionality of EHRs. In this paper, we conduct a formative study with three clinical researchers to understand their needs of exploring temporal patterns in EHRs. Based on those insights, we develop a new visualization interface that renders medical event trajectories in a holistic timeline view and guides users towards interesting patterns using an information scent based method. We demonstrate how a clinical researcher can use our tool to discover interesting sub-cohorts with unique disease progression and treatment trajectories in a case study.",
        "keywords": [],
        "uid": "a-vahc-1010",
        "time_stamp": "2021-10-24T16:00:00Z",
        "has_image": false,
        "paper_award": "",
        "image_caption": "",
        "external_paper_link": "",
        "has_pdf": true,
        "ff_link": ""
    },
    "a-vis4dh-1007": {
        "authors": [
            "Arianna Ciula",
            "Miguel Vieira",
            "Ginestra Ferraro",
            "Tiffany Ong",
            "Sanja Perovic",
            "Rosa Mucignat",
            "Niccol\u00f2 Valmori",
            "Brecht Deseure",
            "Erica Joy Mannucci"
        ],
        "title": "Paper 1: Small Data and Process in Data Visualization: The Radical Translations Case Study",
        "session_id": "a-vis4dh-2",
        "abstract": "This paper uses the collaborative project Radical Translations [1]\nas case study to examine some of the theoretical perspectives\ninforming the adoption and critique of data visualization in the\ndigital humanities with applied examples in context. It showcases\nhow data visualization is used within a King\u2019s Digital Lab project\nlifecycle to facilitate collaborative data exploration within the\nproject interdisciplinary team \u2013 to support data curation and\ncleaning and/or to guide the design process \u2013 as well as data\nanalysis by users external to the team. Theoretical issues around\nbridging the gap between approaches adopted for small and/or\nlarge-scale datasets are addressed from functional perspectives\nwith reference to evolving data modelling and software\ndevelopment lifecycle approaches and workflows. While anchored\nto the specific context of the project under examination, some of\nthe identified trade-offs have epistemological value beyond the\nspecific case study iterations and its design solutions.",
        "keywords": [],
        "uid": "a-vis4dh-1007",
        "time_stamp": "2021-10-24T15:10:00Z",
        "has_image": true,
        "paper_award": "",
        "image_caption": "This paper examines some of the theoretical perspectives informing the adoption and\ncritique of data visualization in the digital humanities with applied examples in\ncontext. It showcases how data visualization is used within a King\u2019s Digital Lab\nproject to facilitate collaborative data exploration \u2013 to support data curation and\ncleaning and/or to guide the design process \u2013 as well as data analysis by users\nexternal to the team. Theoretical issues around bridging the gap between approaches\nadopted for small and/or large-scale datasets are addressed from functional\nperspectives with reference to evolving data modelling and software development\nlifecycle approaches and workflows.\n",
        "external_paper_link": "",
        "has_pdf": true,
        "ff_link": ""
    },
    "a-vis4dh-1014": {
        "authors": [
            "Valerie M\u00fc\u0308ller",
            "Christian Sieg",
            "Lars Linsen"
        ],
        "title": "Paper 2: Uncertainty-aware Topic Modeling Visualization",
        "session_id": "a-vis4dh-2",
        "abstract": "Topic modeling is a state-of-the-art technique for analyzing text\ncorpora. It uses a statistical model, most commonly Latent Dirichlet\nAllocation (LDA), to discover abstract topics that occur in the document collection. However, the LDA-based topic modeling procedure\nis based on a randomly selected initial configuration as well as a\nnumber of parameter values than need to be chosen. This induces\nuncertainties on the topic modeling results, and visualization methods should convey these uncertainties during the analysis process.\nWe propose a visual uncertainty-aware topic modeling analysis. We\ncapture the uncertainty by computing topic modeling ensembles and\npropose measures for estimating topic modeling uncertainty from\nthe ensemble. Then, we propose to enhance state-of-the-art topic\nmodeling visualization methods to convey the uncertainty in the\ntopic modeling process. We visualize the entire ensemble of topic\nmodeling results at different levels for topic and document analysis.\nWe apply our visualization methods to a text corpus to document the\nimpact of uncertainty on the analysis.",
        "keywords": [],
        "uid": "a-vis4dh-1014",
        "time_stamp": "2021-10-24T15:40:00Z",
        "has_image": true,
        "paper_award": "",
        "image_caption": "Ensemble overview over all topics using a t-SNE layout based on topic similarity. Topic models are encoded by color, uncertainty by size. 11 groups of topics have been merged to clusters. Other groups (A,B, and C) are subject to more detailed analysis.",
        "external_paper_link": "",
        "has_pdf": true,
        "ff_link": ""
    },
    "a-vis4dh-1006": {
        "authors": [
            "Richard Brath"
        ],
        "title": "Paper 3: Visualizing Wonderland for many more Literature Visualization Techniques",
        "session_id": "a-vis4dh-2",
        "abstract": null,
        "keywords": [],
        "uid": "a-vis4dh-1006",
        "time_stamp": "2021-10-24T16:10:00Z",
        "has_image": true,
        "paper_award": "",
        "image_caption": "Visualizing Wonderland for many more Literature Visualization techniques. Richard Brath. ",
        "external_paper_link": "",
        "has_pdf": false,
        "ff_link": ""
    },
    "a-vastchallenge-1010": {
        "authors": [
            "Shaocong Tan",
            "Sihang Li",
            "Shuai Chen",
            "Jiacheng Yu",
            "Chuanming Huang",
            "Liang Tang",
            "Qinghua Shang",
            "Zixi Fu",
            "Zhuo Zhang",
            "Xiaoru Yuan"
        ],
        "title": "RelationVis: Visual Analytics of Multifaceted Relationships between Different Entities in Text Collections (Award for Effective Design of Interactive Analytics)",
        "session_id": "a-vastchallenge",
        "abstract": null,
        "keywords": [],
        "uid": "a-vastchallenge-1010",
        "time_stamp": "2021-10-24T17:20:00Z",
        "has_image": false,
        "paper_award": "",
        "image_caption": "",
        "external_paper_link": "",
        "has_pdf": false,
        "ff_link": "https://youtu.be/prqZTuSkmfU"
    },
    "a-vastchallenge-1001": {
        "authors": [
            "Shichao Jia",
            "Nuo Chen",
            "Chen Li",
            "Yi Zhang",
            "Jiawan Zhang"
        ],
        "title": "Visual Analytics of Media Bias (Award for Effective Combination of Analytics and Visualizations",
        "session_id": "a-vastchallenge",
        "abstract": null,
        "keywords": [],
        "uid": "a-vastchallenge-1001",
        "time_stamp": "2021-10-24T17:40:00Z",
        "has_image": false,
        "paper_award": "",
        "image_caption": "",
        "external_paper_link": "",
        "has_pdf": false,
        "ff_link": ""
    },
    "a-vastchallenge-1012": {
        "authors": [
            "Junting Gao",
            "Siqi Shen",
            "Xingui Lai",
            "QingHong Wang",
            "Jiaqi Dong",
            "Ziyue Lin",
            "Lei Peng",
            "Yijie Hou",
            "Yuheng Zhao",
            "Siming Chen"
        ],
        "title": "An Interactive Visualization System for Spatio-temporal Situation Awareness with Multi-data Fusion (Award for Outstanding Comprehensive MC2 Submission)",
        "session_id": "a-vastchallenge",
        "abstract": null,
        "keywords": [],
        "uid": "a-vastchallenge-1012",
        "time_stamp": "2021-10-24T18:10:00Z",
        "has_image": false,
        "paper_award": "",
        "image_caption": "",
        "external_paper_link": "",
        "has_pdf": false,
        "ff_link": ""
    },
    "a-vastchallenge-1008": {
        "authors": [
            "Dani\u00ebl M. Bot",
            "Jannes Peeters",
            "Danai Kafetzaki",
            "Jan Aerts"
        ],
        "title": "Awareness with Multi-data Fusion (Award for Outstanding Comprehensive MC2 Submission)",
        "session_id": "a-vastchallenge",
        "abstract": null,
        "keywords": [],
        "uid": "a-vastchallenge-1008",
        "time_stamp": "2021-10-24T19:00:00Z",
        "has_image": false,
        "paper_award": "",
        "image_caption": "",
        "external_paper_link": "",
        "has_pdf": false,
        "ff_link": ""
    },
    "a-vastchallenge-1011": {
        "authors": [
            "Zeyu Li",
            "Teng Wang",
            "RuiZhi Shi Zhaohui Li",
            "Jiawan Zhang"
        ],
        "title": "Mining and Understanding Stories in Text Sequences with Narrative Visualization (Award for Innovative Narrative Visualization and Analysis Methodology)",
        "session_id": "a-vastchallenge",
        "abstract": null,
        "keywords": [],
        "uid": "a-vastchallenge-1011",
        "time_stamp": "2021-10-24T19:30:00Z",
        "has_image": false,
        "paper_award": "",
        "image_caption": "",
        "external_paper_link": "",
        "has_pdf": false,
        "ff_link": ""
    },
    "a-vastchallenge-1019": {
        "authors": [
            "Lei Peng",
            "Yuheng Zhao",
            "Yijie Hou",
            "Qinghong Wang",
            "Siqi Shen",
            "Xingui Lai",
            "Junting Gao",
            "Jiaqi Dong",
            "Ziyue Lin",
            "Siming Chen"
        ],
        "title": "Mixed-Initiative Visual Exploration of Social Media Text and Events (Award for Strong Human-in-the-Loop Analysis Methodology)",
        "session_id": "a-vastchallenge",
        "abstract": null,
        "keywords": [],
        "uid": "a-vastchallenge-1019",
        "time_stamp": "2021-10-24T19:50:00Z",
        "has_image": false,
        "paper_award": "",
        "image_caption": "",
        "external_paper_link": "",
        "has_pdf": false,
        "ff_link": ""
    },
    "a-vastchallenge-1015": {
        "authors": [
            "Chen Guo",
            "Zuotian Li",
            "Pengyu Ren",
            "Regan O'Connor",
            "Rachael Cai Cai",
            "Justine Xue",
            "Zhenyu Cheryl Qian",
            "Yingjie Victor Chen"
        ],
        "title": "CloudAnnotator: Clustering and Annotating Streaming Text Data (Honorable Mention for Effective Visual Design and for Academic Outreach)",
        "session_id": "a-vastchallenge",
        "abstract": null,
        "keywords": [],
        "uid": "a-vastchallenge-1015",
        "time_stamp": "2021-10-24T20:10:00Z",
        "has_image": false,
        "paper_award": "",
        "image_caption": "",
        "external_paper_link": "",
        "has_pdf": false,
        "ff_link": ""
    },
    "a-vahc-1014": {
        "authors": [
            "David Borland",
            "Irena Brain",
            "Karamarie Fecho",
            "Emily Pfaff",
            "Hao Xu",
            "James Champion",
            "Chris Bizon",
            "David Gotz"
        ],
        "title": "Enabling Longitudinal Exploratory Analysis of Clinical COVID Data",
        "session_id": "a-vahc-3",
        "abstract": "As the COVID-19 pandemic continues to impact the world, data is being gathered and analyzed to better understand the disease. Recognizing the potential for visual analytics technologies to support exploratory analysis and hypothesis generation from longitudinal clinical data, a team of collaborators worked to apply existing event sequence visual analytics technologies to a longitudinal clinical data from a cohort of 998 patients with high rates of COVID-19 infection. This paper describes the initial steps toward this goal, including: (1) the data transformation and processing work required to prepare the data for visual analysis, (2) initial findings and observations, and (3) qualitative feedback and lessons learned which highlight key features as well as limitations to address in future work",
        "keywords": [],
        "uid": "a-vahc-1014",
        "time_stamp": "2021-10-24T17:00:00Z",
        "has_image": true,
        "paper_award": "",
        "image_caption": "",
        "external_paper_link": "",
        "has_pdf": true,
        "ff_link": ""
    },
    "a-vahc-1022": {
        "authors": [
            "Elisha Peterson",
            "Philip B Graff",
            "Peter Gu",
            "Max Robinson"
        ],
        "title": "Visual Analytics for Decision-Makers and Public Audiences within the United States National COVID-19 Response",
        "session_id": "a-vahc-3",
        "abstract": "The COVID-19 pandemic launched a worldwide effort to collect, process, and communicate public health data at unprecedented scales, and a host of visualization capabilities have been launched and maintained to meet the need for presenting data in ways that the general public can understand. This paper presents a selection of visualizations developed in support of the United States National COVID-19 Response, describes the unique set of constraints and challenges of operational visualization in this context, and reflects on ways the visualization community might be able to support public health operations moving forward.",
        "keywords": [],
        "uid": "a-vahc-1022",
        "time_stamp": "2021-10-24T17:20:00Z",
        "has_image": true,
        "paper_award": "",
        "image_caption": "",
        "external_paper_link": "",
        "has_pdf": false,
        "ff_link": ""
    },
    "a-vahc-1020": {
        "authors": [
            "Sanjana Srabanti",
            "G. Elisabeta Marai",
            "Fabio Miranda"
        ],
        "title": "COVID-19 EnsembleVis: Visual Analysis of County-level Ensemble Forecast Models",
        "session_id": "a-vahc-3",
        "abstract": "The spread of the SARS-CoV-2 virus and its contagious disease COVID-19 has impacted countries to an extent not seen since the 1918 flu pandemic. In the absence of an effective vaccine and as cases surge worldwide, governments were forced to adopt measures to inhibit the spread of the disease. To reduce its impact and to guide policy planning and resource allocation, researchers have been developing models to forecast the infectious disease. Ensemble models, by aggregating forecasts from multiple individual models, have been shown to be a useful forecasting method. However, these models can still provide less-than-adequate forecasts at higher spatial resolutions. In this paper, we built COVID-19 EnsembleVis, a web-based interactive visual interface that allows the assessment of the errors of ensembles and individual models by enabling users to effortlessly navigate through and compare the outputs of models considering their space and time dimensions. COVID-19 EnsembleVis enables a more detailed understanding of uncertainty and the range of forecasts generated by individual models.",
        "keywords": [],
        "uid": "a-vahc-1020",
        "time_stamp": "2021-10-24T17:40:00Z",
        "has_image": true,
        "paper_award": "",
        "image_caption": "",
        "external_paper_link": "",
        "has_pdf": true,
        "ff_link": ""
    },
    "a-vahc-1019": {
        "authors": [
            "Pallavi Jonnalagadda",
            "Christine M Swoboda",
            "Priti Singh",
            "Harish Gureddygari",
            "Seth Scarborough",
            "Ian Dunn",
            "Nathan Doogan",
            "Naleef Fareed"
        ],
        "title": "Communicating Area-level Social Determinants of Health Information: The Ohio Children\u2019s Opportunity Index Dashboards",
        "session_id": "a-vahc-3",
        "abstract": "Social determinants of health (SDoH) can be measured at the geographic level to convey information about neighborhood deprivation. The Ohio Children\u2019s Opportunity Index (OCOI) is a multi-dimensional area-level opportunity index comprised of eight health dimensions. Our research team has documented the design, development, and use cases of dashboard solutions to visualize OCOI. The OCOI is a multi-dimensional index spanning the following eight domains or dimensions: family stability, infant health, children\u2019s health, access, education, housing, environment, and criminal justice. Information on these eight domains is derived from the American Community Survey and other administrative datasets maintained by the state of Ohio. Our team used the Tableau Desktop visualization software and applied a user-centered design approach to developing the two OCOI dashboards\u2014 main OCOI dashboard and OCOI-race dashboard. We also performed convergence analysis to visualize the census tracts, where different health indicators simultaneously exist at their worst levels. The OCOI dashboards have multiple, interactive components: a choropleth map of Ohio displaying OCOI scores for a specific census tract, graphs presenting OCOI or domain scores to compare relative positions for tracts, and a sortable table to visualize scores for specific county and census tracts. Stakeholders provided iterative feedback on dashboard design in regard to functionality, content, and aesthetics. A case study using the two dashboards for convergence analysis revealed census tracts in neighborhoods with low infant health scores and a high proportion of minority population. The OCOI dashboards could assist end-users in making decisions that tailor health care delivery and policy decision-making regarding children\u2019s health particularly in areas where multiple health indicators exist at their worst levels.",
        "keywords": [],
        "uid": "a-vahc-1019",
        "time_stamp": "2021-10-24T18:00:00Z",
        "has_image": true,
        "paper_award": "",
        "image_caption": "",
        "external_paper_link": "",
        "has_pdf": false,
        "ff_link": ""
    },
    "w-viscomm-1015": {
        "authors": [
            "Qian Ma"
        ],
        "title": "Unfolding the Decision-Making Dynamics of News Visualization Production in China",
        "session_id": "w-viscomm",
        "abstract": "News visualization is a journalistic form that centers on delivering information through data visualizations. It is arousing the interest of journalism professionals and scholars worldwide, including those in non-western societies. Limited studies have identified the challenges and opportunities of news visualization's adoption in non-western newsrooms, but the professionals' considerations and actions to deal with these challenges and seize the opportunities are unknown. This study explores the criteria for a valuable news visualization and how professionals balance the criteria and constraints to make news decisions in the newsrooms in China. A grounded analysis drawing on 15 in-depth interviews with professionals reveals the dynamics of source selection, interpretation, and presentation in news visualization production. It also shows that news visualization stories result from the debating and tradeoffs between the \"western norms\" and the organization's expectations, audiences' preferences, news commercialization, and macro environment in China.",
        "keywords": [],
        "uid": "w-viscomm-1015",
        "time_stamp": "2021-10-24T18:05:00Z",
        "has_image": false,
        "paper_award": "",
        "image_caption": "",
        "external_paper_link": "",
        "has_pdf": false,
        "ff_link": ""
    },
    "w-viscomm-1008": {
        "authors": [
            "Sabrina Mangal",
            "Meghan Reading",
            "Leslie Park",
            "Yujie Hai",
            "Annie C Myers",
            "Lydia S Dugdale",
            "Parag Goyal",
            "Lisa V. Grossman Liu"
        ],
        "title": "Know Your Audience: Comprehension of Health Information Varies by Visual Format",
        "session_id": "w-viscomm",
        "abstract": "Patient-reported outcomes provide key information about a patient\u2019s health status and are increasingly being integrated into electronic health records as a means of engaging patients in their own care. However, there is a lack of research on how to effectively return this information in a clear and comprehensible way to a wide audience. We conducted a national survey, testing various formats of health information visualizations, and found that overall comprehension is high across all formats, particularly among line graphs, with variations based on literacy and numeracy. This study reinforces the importance of testing various formats among target populations when returning key health information to patients.",
        "keywords": [],
        "uid": "w-viscomm-1008",
        "time_stamp": "2021-10-24T19:05:00Z",
        "has_image": true,
        "paper_award": "",
        "image_caption": "",
        "external_paper_link": "",
        "has_pdf": false,
        "ff_link": ""
    },
    "w-viscomm-1005": {
        "authors": [
            "Lorenzo Amabili",
            "Nicole Sultanum"
        ],
        "title": "Paper Maps: Improving the Readability of Scientific Papers via Concept Maps",
        "session_id": "w-viscomm",
        "abstract": "Given the wealth of scientific publications, perusing papers is becoming a larger and more complex burden, especially for junior researchers. In this work, we suggest a visualization-based method to mitigate this problem via the use of paper maps, i.e., concept maps for the summarization of scientific papers. We provide design principles of paper maps and discuss design considerations based on exploratory design studies. We also conducted an initial evaluation for assessing the effectiveness of paper maps in summarizing scientific papers, suggesting that paper maps can improve the readability of scientific papers.",
        "keywords": [],
        "uid": "w-viscomm-1005",
        "time_stamp": "2021-10-24T19:20:00Z",
        "has_image": true,
        "paper_award": "",
        "image_caption": "The image shows the title of the work, and a paper map for this publication -- i.e. a visual representations for knowledge composed of nodes and links to summarize an academic paper. The paper map has 8 nodes, and the links compose the following stories. \"This work\"  investigates the use of \"visualization\" and is about \"Research communication\". This work also suggests \"design principles\" for creating \"paper maps\", which include \"concept maps\" and can be used for \"presenting, schematizing, summarizing\" \"scientific papers\".",
        "external_paper_link": "",
        "has_pdf": false,
        "ff_link": ""
    },
    "w-viscomm-1013": {
        "authors": [
            "Maryam Riahi",
            "Benjamin Watson"
        ],
        "title": "Aesthetics for Communicative Visualization: A Brief Review",
        "session_id": "w-viscomm",
        "abstract": "We present a brief and somewhat selective review of research on the applied value of aesthetics for communicative visualization. Because aesthetics research in the field of visualization is relatively new, we survey aesthetics research in closely related fields, including psychology, graphic design, and marketing. Because of space limitations, we constrain our discussion of marketing research and omit any examination of related work from human-computer interaction researchers. We conclude with implications for communicative visualization practitioners and researchers, discussing the value of aesthetics and how to achieve it, as well as the need for research on longer-term aesthetic impacts on viewers.",
        "keywords": [],
        "uid": "w-viscomm-1013",
        "time_stamp": "2021-10-24T19:35:00Z",
        "has_image": false,
        "paper_award": "",
        "image_caption": "",
        "external_paper_link": "",
        "has_pdf": false,
        "ff_link": ""
    },
    "w-viscomm-1010": {
        "authors": [
            "Michael J Parker"
        ],
        "title": "Visualizing Student Behavior and Performance in an Online Course",
        "session_id": "w-viscomm",
        "abstract": "Online learning has been an area of tremendous growth in recent years [1], further accelerated by necessity during the coronavirus pandemic [2]. Without the feedback provided by synchronous sessions, however, instructors may lack ongoing insight into students\u2019 progress and performance in fully asynchronous online offerings. Providing greater visibility into students\u2019 online learning behavior has several potential benefits: 1) teachers who seek to provide live sessions that depend on knowledge from asynchronous resources will have a better gauge of students\u2019 preparation; 2) enhanced understanding of the relationship between student pacing and performance can help teachers and researchers characterize the impact of timely engagement with online course material; 3) providing a means of monitoring the effect of changes in course structure or incentives may guide course designers/planners in continuous improvement; and 4) visualizations that illustrate the relationship between students\u2019 course progress and performance have the promise of allowing teachers to take steps early to positively affect students\u2019 outcomes. In this visual case study, visualizations are provided for exploration of the relationship between student progress and performance in an online medicine-related course.",
        "keywords": [],
        "uid": "w-viscomm-1010",
        "time_stamp": "2021-10-24T19:50:00Z",
        "has_image": false,
        "paper_award": "",
        "image_caption": "",
        "external_paper_link": "",
        "has_pdf": false,
        "ff_link": ""
    },
    "a-vis4dh-1016": {
        "authors": [
            "Anessa Petteruti",
            "Cindy Nguyen",
            "David H. Laidlaw"
        ],
        "title": "Paper 4: The Virtual Rosetta A Tool for Exploring Historical Drawings in VR",
        "session_id": "a-vis4dh-4",
        "abstract": "We present the results of a comparative analysis of various layout and\ndesign displays for historical drawings. This involved developing a\nnew visualization environment, known as the \u201dVirtual Rosetta\u201d for a\ncollection of early twentieth century Vietnamese drawings known as\nTechnique du Peuple Annamite (Mechanics and Crafts of the People\nof Annam) compiled by French colonial administrator Henri J. Oger\n(1872-1929). In this paper, we discuss similar work that has been\npursued by researchers in digital humanities, specifically working\nin virtual displays of artwork, the design of the Virtual Rosetta,\nincluding text readability, wall and text contrast, and organization\nof drawings, as well as the associated results. We used an analysis technique known as hierarchical clustering utilizing sentence\nembeddings, a technique in natural language processing in which\nsentences are mapped to numerical vectors, to organize a subset\nof the drawings so that drawings with similar text descriptions are\nlocated near each other. Performing this on an entire dataset would\nallow humanities researchers to most effectively and efficiently visualize their findings. We report on user experience and efficacy for a\nnumber of virtual environment layouts for displaying drawings and\nvisual information for research analysis in virtual reality.",
        "keywords": [],
        "uid": "a-vis4dh-1016",
        "time_stamp": "2021-10-24T19:10:00Z",
        "has_image": true,
        "paper_award": "",
        "image_caption": "",
        "external_paper_link": "",
        "has_pdf": true,
        "ff_link": ""
    },
    "a-vis4dh-1008": {
        "authors": [
            "Annie Chen",
            "Camille Lyans Cole"
        ],
        "title": "Paper 5: Reflexivity in Issues of Scale and Representation in a Digital Humanities Project",
        "session_id": "a-vis4dh-4",
        "abstract": null,
        "keywords": [],
        "uid": "a-vis4dh-1008",
        "time_stamp": "2021-10-24T19:40:00Z",
        "has_image": true,
        "paper_award": "",
        "image_caption": "Reflexivity in Issues of Scale and Representation in a Digital Humanities Project. Annie T. Chen and Camille Cole.",
        "external_paper_link": "",
        "has_pdf": false,
        "ff_link": ""
    },
    "a-ldav-1002": {
        "authors": [
            "Hyungman Park",
            "Donald Fussell",
            "Paul Navr\u00e1til"
        ],
        "title": "Data-Aware Predictive Scheduling for Distributed-Memory Ray Tracing",
        "session_id": "a-ldav-1",
        "abstract": null,
        "keywords": [],
        "uid": "a-ldav-1002",
        "time_stamp": "2021-10-25T14:00:00Z",
        "has_image": true,
        "paper_award": "",
        "image_caption": "\ufeffOur predictive scheduling framework generalizes existing ray scheduling methods to a unified method that adapts to the characteristics of underlying data by adjusting the scheduling depth of each ray. We have path traced the distributed partitions of the Lambda2 dataset of simulated vortices using the Frontera supercomputer at the Texas Advanced Computing Center. With our predictive scheduling method, we are able to achieve a throughput of 7-33 MRays/s while sending rays across 4-128 distributed compute nodes. Lambda2 contains an aggregate of 2.3 billion unique triangles in 1024 domains extracted from a 77.3GB HDF5 file.",
        "external_paper_link": "",
        "has_pdf": true,
        "ff_link": ""
    },
    "a-ldav-1000": {
        "authors": [
            "Jules Vidal",
            "Julien Tierny"
        ],
        "title": "Fast Approximation of Persistence Diagrams with Guarantees",
        "session_id": "a-ldav-2",
        "abstract": null,
        "keywords": [],
        "uid": "a-ldav-1000",
        "time_stamp": "2021-10-25T15:00:00Z",
        "has_image": true,
        "paper_award": "",
        "image_caption": "We introduce a method for the fast approximation of the saddle-extremum\npersistence diagram of a scalar field, with user-controlled guarantees on the\nbottleneck error of the approximate diagram.\nWe illustrate the utility of our approach for interactive data exploration and\ndocument visualization strategies to convey the uncertainty related to our\napproximations.\n",
        "external_paper_link": "",
        "has_pdf": true,
        "ff_link": ""
    },
    "a-ldav-1003": {
        "authors": [
            "Dmitriy Morozov",
            "Tom Peterka",
            "Hanqu Guo",
            "Mukund Raj",
            "Jiayi Xu",
            "Han-Wei Shen"
        ],
        "title": "IExchange: Asynchronous Communication and Termination Detection for Iterative Algorithms",
        "session_id": "a-ldav-2",
        "abstract": null,
        "keywords": [],
        "uid": "a-ldav-1003",
        "time_stamp": "2021-10-25T15:20:00Z",
        "has_image": true,
        "paper_award": "",
        "image_caption": "Comparison of synchronous exchange (left) and asynchronous iexchange (right) Gantt charts show that the asynchronous method is several times faster because communication and computation are interleaved",
        "external_paper_link": "",
        "has_pdf": true,
        "ff_link": ""
    },
    "a-ldav-1009": {
        "authors": [
            "Matthew Larsen",
            "Cyrus Harrison",
            "Terece Turton",
            "Sudhanshu Sane",
            "Stephanie Brink",
            "Hank Childs"
        ],
        "title": "Trigger Happy: Assessing the Viability of Trigger-Based In Situ Analysis",
        "session_id": "a-ldav-2",
        "abstract": null,
        "keywords": [],
        "uid": "a-ldav-1009",
        "time_stamp": "2021-10-25T15:40:00Z",
        "has_image": false,
        "paper_award": "",
        "image_caption": "",
        "external_paper_link": "",
        "has_pdf": true,
        "ff_link": ""
    },
    "a-ldav-1015": {
        "authors": [
            "Duong Hoang",
            "Harsh Bhatia",
            "Peter Lindstrom",
            "Valerio Pascucci"
        ],
        "title": "High-quality and Low-memory-footprint Progressive Decoding of Large-scale Particle Data",
        "session_id": "a-ldav-2",
        "abstract": null,
        "keywords": [],
        "uid": "a-ldav-1015",
        "time_stamp": "2021-10-25T16:00:00Z",
        "has_image": true,
        "paper_award": "",
        "image_caption": "We present particle compression approaches with asymptotically constant memory footprint and high-quality lossy reconstruction of large particle datasets. Our method supports progressive, random-access, and error-driven decoding, while being much faster than the MPEG standard.",
        "external_paper_link": "",
        "has_pdf": true,
        "ff_link": ""
    },
    "a-ldav-1007": {
        "authors": [
            "Riley D. Lipinski",
            "Kenneth Moreland",
            "Michael E. Papka",
            "Thomas Marrinan"
        ],
        "title": "GPU-based Image Compression for Efficient Compositing in Distributed Rendering Applications",
        "session_id": "a-ldav-3",
        "abstract": null,
        "keywords": [],
        "uid": "a-ldav-1007",
        "time_stamp": "2021-10-25T17:00:00Z",
        "has_image": true,
        "paper_award": "",
        "image_caption": "Composited rendering of a nuclear power station using 16 processes. Each process has drawn its portion of the model using a unique color.",
        "external_paper_link": "",
        "has_pdf": true,
        "ff_link": "https://youtu.be/ZwWVL4Szuhk"
    },
    "a-ldav-1019": {
        "authors": [
            "Florian Frie\u00df",
            "Michael Becher",
            "Guido Reina",
            "Thomas Ertl"
        ],
        "title": "Amortised Encoding for Large High-Resolution Displays",
        "session_id": "a-ldav-3",
        "abstract": null,
        "keywords": [],
        "uid": "a-ldav-1019",
        "time_stamp": "2021-10-25T17:20:00Z",
        "has_image": true,
        "paper_award": "",
        "image_caption": "We present an approach that reduces the resolution before the encoding and uses temporal upscaling to reconstruct the full resolution image. Our approach takes advantage of the fact that humans do not perceive the full details of moving objects by providing a perfect reconstruction for static parts of the image, while non-static parts are reconstructed with a lower quality. This strategy enables a substantial reduction of the encoding latency and the required bandwidth with barely noticeable changes in visual quality, which is crucial for collaborative analysis across display walls at different locations. ",
        "external_paper_link": "",
        "has_pdf": true,
        "ff_link": "https://youtu.be/ECFxGX4XlJM"
    },
    "a-ldav-1021": {
        "authors": [
            "Sergei Shudler",
            "Steve Petruzza",
            "Valerio Pascucci",
            "Peer-Timo Bremer"
        ],
        "title": "Portable and Composable Flow Graphs for In Situ Analytics",
        "session_id": "a-ldav-3",
        "abstract": null,
        "keywords": [],
        "uid": "a-ldav-1021",
        "time_stamp": "2021-10-25T17:40:00Z",
        "has_image": true,
        "paper_award": "",
        "image_caption": "Portable and Composable Flow Graphs for In Situ Analytics\n",
        "external_paper_link": "",
        "has_pdf": true,
        "ff_link": ""
    },
    "a-ldav-1022": {
        "authors": [
            "Nicole J. Marsaglia",
            "Yuya Kawakami",
            "Samuel David Schwartz",
            "Stefan Fields",
            "Hank Childs"
        ],
        "title": "An Entropy-Based Approach for Identifying User-Preferred Camera Positions",
        "session_id": "a-ldav-3",
        "abstract": null,
        "keywords": [],
        "uid": "a-ldav-1022",
        "time_stamp": "2021-10-25T18:00:00Z",
        "has_image": false,
        "paper_award": "",
        "image_caption": "",
        "external_paper_link": "",
        "has_pdf": true,
        "ff_link": ""
    },
    "v-full-1346": {
        "authors": [
            "Nathan van Beusekom",
            "Wouter Meulemans",
            "Bettina Speckmann"
        ],
        "title": "Simultaneous Matrix Orderings for Graph Collections",
        "session_id": "v-full-full1",
        "abstract": "Undirected graphs are frequently used to model phenomena that deal with interacting objects, such as social networks, brain activity and communication networks. The topology of an undirected graph G can be captured by an adjacency matrix; this matrix in turn can be visualized directly to give insight into the graph structure. Which visual patterns appear in such a matrix visualization crucially depends on the ordering of its rows and columns. Formally defining the quality of an ordering and then automatically computing a high-quality ordering are both challenging problems; however, effective heuristics exist and are used in practice.\n  \n  Often, graphs do not exist in isolation but as part of a collection of graphs on the same set of vertices, for example, brain scans over time or of different people. To visualize such graph collections, we need a single ordering that works well for all matrices simultaneously. The current state-of-the-art solves this problem by taking a (weighted) union over all graphs and applying existing heuristics. However, this union leads to a loss of information, specifically in those parts of the graphs which are different. We propose a collection-aware approach to avoid this loss of information and apply it to two popular heuristic methods: leaf order and barycenter.\n  \n  The de-facto standard computational quality metrics for matrix ordering capture only block-diagonal patterns (cliques). Instead, we propose to use Moran's I, a spatial auto-correlation metric, which captures the full range of established patterns. Moran's I refines previously proposed stress measures. Furthermore, the popular leaf order method heuristically optimizes a similar measure which further supports the use of Moran's I in this context. An ordering that maximizes Moran's I can be computed via solutions to the Traveling Salesperson Problem (TSP); approximate orderings can be computed more efficiently, using any of the approximation algorithms for metric TSP.\n  \n  We evaluated our methods for simultaneous orderings on real-world datasets using Moran's I as the quality metric. Our results show that our collection-aware approach matches or improves performance compared to the union approach, depending on the similarity of the graphs in the collection. Specifically, our Moran's I-based collection-aware leaf order implementation consistently outperforms other implementations. Our collection-aware implementations carry no significant additional computational costs.",
        "keywords": [],
        "uid": "v-full-1346",
        "time_stamp": "2021-10-26T15:00:00Z",
        "has_image": true,
        "paper_award": "best",
        "image_caption": "A collection of two matrices (top left). The state-of-the-art first computes a (weighted) union (top middle and right, blue squares have weight 2), then orders the union, and finally applies this ordering to all matrices in the collection. The union leads to a loss of information, specifically, on those parts of the matrices which are different (bottom right). We propose a collection-aware approach to compute orderings which avoids this loss of information (bottom left). Our approach can be applied to existing ordering methods; examples in this figure use the popular leaf order heuristic.",
        "external_paper_link": "",
        "has_pdf": true,
        "ff_link": "https://youtu.be/0BIMBxNBSgk"
    },
    "v-full-1187": {
        "authors": [
            "Joscha Eirich",
            "Jakob Bonart",
            "Dominik J\u00e4ckle",
            "Michael Sedlmair",
            "Ute Schmid",
            "Kai Fischbach",
            "Tobias Schreck",
            "J\u00fcrgen Bernard"
        ],
        "title": "IRVINE: A Design Study on Analyzing Correlation Patterns of Electrical Engines",
        "session_id": "v-full-full1",
        "abstract": "In this design study, we present IRVINE, a Visual Analytics (VA) system, which facilitates the analysis of acoustic data to detect and understand previously unknown errors in the manufacturing of electrical engines. In serial manufacturing processes, signatures from acoustic data provide valuable information on how the relationship between multiple produced engines serves to detect and understand previously unknown errors. To analyze such signatures, IRVINE leverages interactive clustering and data labeling techniques, allowing users to analyze clusters of engines with similar signatures, drill down to groups of engines, and select an engine of interest. Furthermore, IRVINE allows to assign labels to engines and clusters and annotate the cause of an error in the acoustic raw measurement of an engine. Since labels and annotations represent valuable knowledge, they are conserved in a knowledge database to be available for other stakeholders. We contribute a design study, where we developed IRVINE in four main iterations with engineers from a company in the automotive sector. To validate IRVINE, we conducted a field study with six domain experts. Our results suggest a high usability and usefulness of IRVINE as part of the improvement of a real-world manufacturing process. Specifically, with IRVINE domain experts were able to label and annotate produced electrical engines more than 30% faster.",
        "keywords": [],
        "uid": "v-full-1187",
        "time_stamp": "2021-10-26T15:15:00Z",
        "has_image": true,
        "paper_award": "best",
        "image_caption": "The IRVINE system. Users have an overview over clusters in (A). They can select clusters in (A) and engines in (B). After selecting an engine in (B), the acoustic signature of the engine is displayed in (C) and respective raw acoustic measurements in (D). Detailed information about selections from (C) is shown as line chart and scatter-plot and bar chart in (F). After the analysis of an engine, the user can assign a label in (E) and provide an annotation for the label in (D).",
        "external_paper_link": "",
        "has_pdf": true,
        "ff_link": "https://youtu.be/EKO-fgUCF5w"
    },
    "v-full-1160": {
        "authors": [
            "Wesley Willett",
            "Bon Adriel Aseniero",
            "Sheelagh Carpendale",
            "Pierre Dragicevic",
            "Yvonne Jansen",
            "Lora Oehlberg",
            "Petra Isenberg"
        ],
        "title": "Perception! Immersion! Empowerment! Superpowers as Inspiration for Visualization",
        "session_id": "v-full-full1",
        "abstract": "We explore how the lens of fictional superpowers can help characterize how visualizations empower people and provide inspiration for new visualization systems. Researchers and practitioners often tout visualizations\u2019 ability to \u201cmake the invisible visible\u201dand to \u201cenhance cognitive abilities.\u201d Meanwhile superhero comics and other modern fiction often depict characters with similarly fantastic abilities that allow them to see and interpret the world in ways that transcend traditional human perception. We investigate the intersection of these domains, and show how the language of superpowers can be used to characterize existing visualization systems and suggest opportunities for new and empowering ones. We introduce two frameworks: The first characterizes seven underlying mechanics that form the basis for a variety of visual superpowers portrayed in fiction. The second identifies seven ways in which visualization tools and interfaces can instill a sense of empowerment in the people who use them. Building on these observations, we illustrate a diverse set of \u201cvisualization superpowers\u201d and highlight opportunities for the visualization community to create new system sand interactions that empower new experiences with data",
        "keywords": [],
        "uid": "v-full-1160",
        "time_stamp": "2021-10-26T15:30:00Z",
        "has_image": true,
        "paper_award": "best",
        "image_caption": "A conceptual illustration of a possible \"Building Vision\" visualization approach inspired by enhanced see-through vision abilities often seen in superhero comics.",
        "external_paper_link": "",
        "has_pdf": true,
        "ff_link": "https://youtu.be/Y-6GdB_nVeg"
    },
    "v-full-1426": {
        "authors": [
            "Shih-Hsuan Hung",
            "Yue Zhang",
            "Harry Yeh",
            "Eugene Zhang"
        ],
        "title": "Feature Curves and Surfaces of 3D Asymmetric Tensor Fields",
        "session_id": "v-full-full1",
        "abstract": "3D asymmetric tensor fields have found many applications in science and engineering domains, such as fluid dynamics and solid mechanics. 3D asymmetric tensors can have complex eigenvalues, which makes their analysis and visualization more challenging than 3D symmetric tensors. Existing research in tensor field visualization focuses on 2D asymmetric tensor fields and 3D symmetric tensor fields. In this paper, we address the analysis and visualization of 3D asymmetric tensor fields. We introduce six topological surfaces and one topological curve, which lead to an eigenvalue space based on the tensor mode that we define. In addition, we identify several non-topological feature surfaces that are nonetheless physically important. Included in our analysis are the realizations that triple degenerate tensors are structurally stable and form curves, unlike the case for 3D symmetric tensors fields. Furthermore, there are two different ways of measuring the relative strengths of rotation and angular deformation in the tensor fields, unlike the case for 2D asymmetric tensor fields. We extract these feature surfaces using the A-patches algorithm. However, since three of our feature surfaces are quadratic, we develop a method to extract quadratic surfaces at any given accuracy. To facilitate the analysis of eigenvector fields, we visualize a hyperstreamline as a tree stem with the other two eigenvectors represented as thorns in the real domain or the dual-eigenvectors as leaves in the complex domain. To demonstrate the effectiveness of our analysis and visualization, we apply our approach to datasets from solid mechanics and fluid dynamics.",
        "keywords": [],
        "uid": "v-full-1426",
        "time_stamp": "2021-10-26T15:45:00Z",
        "has_image": true,
        "paper_award": "best",
        "image_caption": "This paper introduces topological feature curves and surfaces for the analysis of 3D asymmetric tensor fields, such as the velocity gradient tensor of the Lorenz attractor (left).  Note all these surfaces intersect at the triple degenerate curves. Furthermore, this analysis leads to an eigenvalue space for 3D asymmetric tensor fields. To enable a holistic view of the eigenvectors and dual-eigenvectors, this paper proposes an augmented hyperstreamline following one eigenvector field as a tree stem with attached thorns and leaves to show the other eigenvectors or dual-eigenvectors (right).",
        "external_paper_link": "",
        "has_pdf": true,
        "ff_link": "https://youtu.be/kIYX3lWuIew"
    },
    "v-short-1184": {
        "authors": [
            "Tal Boger",
            "Steven Most",
            "Steven Franconeri"
        ],
        "title": "Jurassic Mark: Inattentional Blindness for a Datasaurus Reveals that Visualizations are Explored, not Seen",
        "session_id": "v-full-full1",
        "abstract": "Graphs effectively communicate data because they capitalize on the visual system\u2019s ability to rapidly extract patterns. Yet, this pattern extraction does not occur in a single glance. Instead, research on visual attention suggests that the visual system iteratively applies a sequence of filtering operations on an image, extracting patterns from subsets of visual information over time, and selectively inhibiting other information at each of these moments. To demonstrate that this powerful series of filtering operations also occurs during the perception of visualized data, we designed a task where participants made judgments from one class of marks on a scatterplot, presumably incentivizing them to relatively ignore other classes of marks. Participants consistently missed a conspicuous dinosaur in the ignored collection of marks (93% for a 1s presentation, and 61% for 2.5s), but not in a control condition where the judgment task was removed (25% for a 1s presentation, and 11% for 2.5s), suggesting that data visualizations are not \"seen\" in a single glance, and instead require an active process of exploration.",
        "keywords": [
            "Charts, Diagrams, and Plots",
            "Communication/Presentation, Storytelling",
            "Perception & Cognition"
        ],
        "uid": "v-short-1184",
        "time_stamp": "2021-10-26T16:00:00Z",
        "has_image": true,
        "paper_award": "best",
        "image_caption": "We often feel like we extract patterns in data from visualizations in an instant. But, perceptual psychology literature shows that extracting such patterns can require a series of filtering operations over time, where we attend to some subsets information, and inhibit others. We demonstrate the presence of this powerful filter when viewing data visualizations; 93% of viewers can miss a conspicuous dinosaur in an inhibited set of dots within a scatterplot. Therefore, data visualizations are not instantly seen, but rather actively explored over time.\n",
        "external_paper_link": "",
        "has_pdf": true,
        "ff_link": "https://youtu.be/Lc17AABMkGM"
    },
    "v-tvcg-9275378": {
        "authors": [
            "Ian Duncan",
            "Shi Tingsheng",
            "Simon Perrault",
            "Michael Gastner"
        ],
        "title": "Task-Based Effectiveness of Interactive Contiguous Area Cartograms",
        "session_id": "v-full-full19",
        "abstract": "Cartograms are map-based data visualizations in which the area of each map region is proportional to an associated numeric data value (e.g., population or gross domestic product). A cartogram is called contiguous if it conforms to this area principle while also keeping neighboring regions connected. Because of their distorted appearance, contiguous cartograms have been criticized as difficult to read. Some authors have suggested that cartograms may be more legible if they are accompanied by interactive features (e.g., animations, linked brushing, or infotips). We conducted an experiment to evaluate this claim. Participants had to perform visual analysis tasks with interactive and noninteractive contiguous cartograms. The task types covered various aspects of cartogram readability, ranging from elementary lookup tasks to synoptic tasks (i.e., tasks in which participants had to summarize high-level differences between two cartograms). Elementary tasks were carried out equally well with and without interactivity. Synoptic tasks, by contrast, were more difficult without interactive features. With access to interactivity, however, most participants answered even synoptic questions correctly. In a subsequent survey, participants rated the interactive features as \u201ceasy to use\u201d and \u201chelpful.\u201d Our study suggests that interactivity has the potential to make contiguous cartograms accessible even for those readers who are unfamiliar with interactive computer graphics or do not have a prior affinity to working with maps. Among the interactive features, animations had the strongest positive effect, so we recommend them as a minimum of interactivity when contiguous cartograms are displayed on a computer screen.",
        "keywords": [
            "task analysis",
            "economic indicators",
            "data visualization",
            "animation",
            "switches",
            "software",
            "shape",
            "cartogram",
            "geovisualization",
            "interactive data exploration",
            "quantitative evaluation"
        ],
        "uid": "v-tvcg-9275378",
        "time_stamp": "2021-10-27T13:00:00Z",
        "has_image": true,
        "paper_award": "",
        "image_caption": "",
        "external_paper_link": "https://doi.org/10.1109/TVCG.2020.3041745",
        "has_pdf": false,
        "ff_link": "https://youtu.be/QD4Ekv80ZUU"
    },
    "v-tvcg-9417674": {
        "authors": [
            "Ran Chen",
            "Xinhuan Shu",
            "Jiahui Chen",
            "Di Weng",
            "Junxiu Tang",
            "Siwei Fu",
            "Yingcai Wu"
        ],
        "title": "Nebula: A Coordinating Grammar of Graphics",
        "session_id": "v-full-full19",
        "abstract": "In multiple coordinated views (MCVs), visualizations across views update their content in response to users interactions in other views. Interactive systems provide direct manipulation to create coordination between views, but are restricted to limited types of predefined templates. By contrast, textual specification languages enable flexible coordination but expose technical burden. To bridge the gap, we contribute Nebula, a grammar based on natural language for coordinating visualizations in MCVs. The grammar design is informed by a novel framework based on a systematic review of 176 coordinations from existing theories and applications, which describes coordination by demonstration, i.e., how coordination is performed by users. With the framework, Nebula specification formalizes coordination as a composition of user- and coordination-triggered interactions in origin and destination views, respectively, along with potential data transformation between the interactions. We evaluate Nebula by demonstrating its expressiveness with a gallery of diverse examples and analyzing its usability on cognitive dimensions.",
        "keywords": [
            "Data visualization",
            "Grammar",
            "Visualization",
            "Usability",
            "Libraries",
            "Data models",
            "Natural languages",
            "Coordination",
            "Multiple coordinated views",
            "Interactive visualization",
            "Grammar of graphics"
        ],
        "uid": "v-tvcg-9417674",
        "time_stamp": "2021-10-27T13:15:00Z",
        "has_image": true,
        "paper_award": "",
        "image_caption": "An example of multiple coordinated views specified in a natural language sentence using Nebula, a visualization grammar. Users can (1) select points in any scatterplot, and (2) the corresponding points in the other scatterplots will be highlighted.",
        "external_paper_link": "https://doi.org/10.1109/TVCG.2021.3076222",
        "has_pdf": false,
        "ff_link": "https://youtu.be/aQgXgBpp2lo"
    },
    "v-full-1155": {
        "authors": [
            "Kun-Ting Chen",
            "Tim Dwyer",
            "Benjamin Bach",
            "Kim Marriott"
        ],
        "title": "Rotate or Wrap? Interactive Visualisations of Cyclical Data on Cylindrical or Toroidal Topologies",
        "session_id": "v-full-full19",
        "abstract": "In this paper, we report on a study of visual representations for cyclical data and the effect of interactively wrapping a bar chart \u2018around its boundaries\u2019. Compared to linear bar chart, polar (or radial) visualisations have the advantage that cyclical data can be presented continuously without mentally bridging the visual \u2018cut\u2019 across the left-and-right boundaries. To investigate this hypothesis and to assess the effect the cut has on analysis performance, this paper presents results from a crowdsourced, controlled experiment with 72 participants comparing new continuous panning technique to linear bar charts (interactive wrapping). Our results show that bar charts with interactive wrapping lead to less errors compared to standard bar charts or polar charts. Inspired by these results, we generalise the concept of interactive wrapping to other visualisations for cyclical or relational data. We describe a design space based on the concept of one-dimensional wrapping and two-dimensional wrapping, linked to two common 3D topologies; cylinder and torus that can be used to metaphorically explain one- and two-dimensional wrapping. This design space suggests that interactive wrapping is widely applicable to many different data types.",
        "keywords": [],
        "uid": "v-full-1155",
        "time_stamp": "2021-10-27T13:30:00Z",
        "has_image": true,
        "paper_award": "",
        "image_caption": "Rotate or Wrap, centre-row: average traffic accidents per hour on Thursdays in Manhattan in 2016. bottom-row: average traffic accidents per hour across the week in Manhattan in 2016",
        "external_paper_link": "",
        "has_pdf": true,
        "ff_link": "https://youtu.be/-SMB0pJ17rI"
    },
    "v-full-1275": {
        "authors": [
            "Yifan Wu",
            "Remco Chang",
            "Joseph Hellerstein",
            "Arvind Satyanarayan",
            "Eugene Wu"
        ],
        "title": "DIEL: Interactive Visualization Beyond the Here and Now",
        "session_id": "v-full-full19",
        "abstract": "Interactive visualization design and research have primarily focused on local data and synchronous events. However, for more complex use cases\u2014e.g., remote database access and streaming data sources\u2014developers must grapple with distributed data and asynchronous events. Currently, constructing these use cases is difficult and time-consuming; developers are forced to operationally program low-level details like asynchronous database querying and reactive event handling. This approach is in stark contrast to modern methods for browser-based interactive visualization, which feature high-level declarative specifications. In response, we present DIEL, a declarative framework that supports asynchronous events over distributed data. Like many declarative visualization languages, DIEL developers need only specify what data they want, rather than procedural steps for how to assemble it; uniquely, DIEL models asynchronous events (e.g., user interactions or server responses) as streams of data that are captured in event logs. To specify the state of a user interface at any time, developers author declarative queries over the data and event logs; DIEL compiles and optimizes a corresponding dataflow graph, and synthesizes necessary low-level distributed systems details. We demonstrate DIEL\u2019s performance and expressivity through ex-ample interactive visualizations that make diverse use of remote data and coordination of asynchronous events. We further evaluate DIEL\u2019s usability using the Cognitive Dimensions of Notations framework, revealing wins such as ease of change, and compromises such as premature commitments.",
        "keywords": [],
        "uid": "v-full-1275",
        "time_stamp": "2021-10-27T13:45:00Z",
        "has_image": true,
        "paper_award": "",
        "image_caption": "To help reduce the complexities of imperative data exchange and asynchronous event handling that arises when scaling up interactive data visualizations, DIEL abstracts away the frontend and backends and treats the application as one unified distributed system. ",
        "external_paper_link": "",
        "has_pdf": true,
        "ff_link": "https://youtu.be/PB3D4RRI7Es"
    },
    "v-full-1204": {
        "authors": [
            "Michael Oppermann",
            "Tamara Munzner"
        ],
        "title": "VizSnippets: Compressing Visualization Bundles Into Representative Previews for Browsing Visualization Collections",
        "session_id": "v-full-full19",
        "abstract": "Visualization collections, accessed by platforms such as Tableau Online or Power BI, are used by millions of people to share and access diverse analytical knowledge in the form of interactive visualization bundles. Result snippets, compact previews of these bundles, are presented to users to help them identify relevant content when browsing collections. Our engagement with Tableau product teams and review of existing snippet designs on five platforms showed us that current practices fail to help people judge the relevance of bundles because they include only the title and one image. Users frequently need to undertake the time-consuming endeavour of opening a bundle within its visualization system to examine its many views and dashboards. In response, we contribute the first systematic approach to visualization snippet design. We propose a framework for snippet design that addresses eight key challenges that we identify. We present a computational pipeline to compress the visual and textual content of bundles into representative previews that is adaptive to a provided pixel budget and provides high information density with multiple images and carefully chosen keywords. We also reflect on the method of visual inspection through random sampling to gain confidence in model and parameter choices.",
        "keywords": [],
        "uid": "v-full-1204",
        "time_stamp": "2021-10-27T14:00:00Z",
        "has_image": true,
        "paper_award": "honorable",
        "image_caption": "Visualization snippets of the same Observable notebook automatically generated with our proposed VizSnippets computational pipeline and design framework. The visual and textual content of bundles is compressed into representative snippets of different form factors which provide high information density with multiple images and carefully chosen keywords.",
        "external_paper_link": "",
        "has_pdf": true,
        "ff_link": "https://youtu.be/3Wrf2_kXLEg"
    },
    "v-full-1570": {
        "authors": [
            "Takanori Fujiwara",
            "Xinhai Wei",
            "Jian Zhao",
            "Kwan-Liu Ma"
        ],
        "title": "Interactive Dimensionality Reduction for Comparative Analysis",
        "session_id": "v-full-full19",
        "abstract": "Finding the similarities and differences between two or more groups of datasets is a fundamental analysis task. For high-dimensional data, dimensionality reduction (DR) methods are often used to find the characteristics of each group. However, existing DR methods provide limited capability and flexibility for such comparative analysis as each method is designed only for a narrow analysis target, such as identifying factors that most differentiate groups. In this work, we introduce an interactive DR framework where we integrate our new DR method, called ULCA (unified linear comparative analysis), with an interactive visual interface. ULCA unifies two DR schemes, discriminant analysis and contrastive learning, to support various comparative analysis tasks. To provide flexibility for comparative analysis, we develop an optimization algorithm that enables analysts to interactively refine ULCA results. Additionally, we provide an interactive visualization interface to examine ULCA results with a rich set of analysis libraries. We evaluate ULCA and the optimization algorithm to show their efficiency as well as present multiple case studies using real-world datasets to demonstrate the usefulness of our framework.",
        "keywords": [],
        "uid": "v-full-1570",
        "time_stamp": "2021-10-27T14:15:00Z",
        "has_image": true,
        "paper_award": "",
        "image_caption": "The analyst is analyzing the Wine dataset with our interactive dimensionality reduction (DR) framework in the Jupyter Notebook. The result produced with a new DR method, Unified Linear Comparative Analysis (ULCA), is visualized with the UI. (a) shows interactively adjustable parameters. (b) depicts a lower-dimensional representation produced by ULCA. The analyst can directly manipulate the centroid or scatteredness of each group by moving or scaling the corresponding confidence ellipse to trigger the backward parameter selection. (c) informs a numerical mapping from the original attributes to each component. With (d), the analyst can save the current state of visualizations and parameters.\n",
        "external_paper_link": "",
        "has_pdf": true,
        "ff_link": ""
    },
    "v-full-1486": {
        "authors": [
            "Hamza Elhamdadi",
            "Shaun Canavan",
            "Paul Rosen"
        ],
        "title": "AffectiveTDA: Using Topological Data Analysis to Improve Analysis and Explainability in Affective Computing",
        "session_id": "v-full-full20",
        "abstract": "We present an approach utilizing Topological Data Analysis to study the structure of face poses used in affective computing, i.e., the process of recognizing human emotion. The approach uses a conditional comparison of different emotions, both respective and irrespective of time, with multiple topological distance metrics, dimension reduction techniques, and face subsections (e.g., eyes, nose, mouth, etc.). The results confirm that our topology-based approach captures known patterns, distinctions between emotions, and distinctions between individuals, which is an important step towards more robust and explainable emotion recognition by machines.",
        "keywords": [],
        "uid": "v-full-1486",
        "time_stamp": "2021-10-27T13:00:00Z",
        "has_image": true,
        "paper_award": "",
        "image_caption": "Our affective computing visualization calculates persistent homology on 83 facial landmarks (top) to detect the topological features of emotions. By comparing the topology distance of various facial poses (bottom), our approach clusters emotions of anger (brown), disgust (purple), fear (red), happiness (green), sadness (orange), and surprise (blue).",
        "external_paper_link": "",
        "has_pdf": true,
        "ff_link": "https://youtu.be/KgQhn8BgztQ"
    },
    "v-full-1621": {
        "authors": [
            "Zhenge Zhao",
            "Panpan Xu",
            "Carlos Scheidegger",
            "Liu Ren"
        ],
        "title": "Human-in-the-loop Extraction of Interpretable Concepts in Deep Learning Models",
        "session_id": "v-full-full20",
        "abstract": "The interpretation of deep neural networks (DNNs) has become a key topic as more people apply them to solve various problems and making critical decisions. Recently, concept-based explanation has become a popular approach for post-hoc interpretation of DNNs. Instead of focusing on a single data sample to obtain local interpretation such as saliency maps, concept-based explanation provides a global interpretation of model predictions by analyzing how visual concepts affects model decision. For example, how the presence of shadow affects an object detection model. However, identifying human-friendly visual concepts that affect model decisions is a challenging task that can not be easily addressed with automatic approaches. In this paper, we present a novel human-in-the-loop visual analytics framework to generate user-defined concepts for model interpretation and diagnostics. The core of our approach is the use of active learning, where we integrate human knowledge and feedback to train a concept extractor in each stage. We crop or segment the original images into small image patches, extract the latent presentations from the hidden layer of the task model, select image patches sharing a common concept, and train a shallow net on top of the latent representation to collect image patches containing the visual concept. We combine these processes into an interactive system, ConceptExtract. Through two case studies, we show how our approach helps analyze model behavior and extract human-friendly concepts for different machine learning tasks and datasets and how to use these concepts to understand the predictions, compare model performance and make suggestions for model refinement. Quantitative experiments show that our active learning approach can accurately extract meaningful visual concepts. More importantly, by identifying visual concepts that negatively affect model performance, we develop the corresponding data augmentation strategy that consistently improves model performance.",
        "keywords": [],
        "uid": "v-full-1621",
        "time_stamp": "2021-10-27T13:15:00Z",
        "has_image": true,
        "paper_award": "",
        "image_caption": "Deep learning model developers encounter different problems when they are trying to analyze their model and make further improvements. Our system ConceptExtract enables users to explore image patches, control the active learning process and use the resulting concepts for model comparison and diagnosis.",
        "external_paper_link": "",
        "has_pdf": true,
        "ff_link": "https://youtu.be/OrnFfB7rCKY"
    },
    "v-full-1123": {
        "authors": [
            "Shichao Jia",
            "zeyu li",
            "Nuo Chen",
            "Jiawan Zhang"
        ],
        "title": "Towards Visual Explainable Active Learning for Zero-Shot Classification",
        "session_id": "v-full-full20",
        "abstract": "Zero-shot classification is a promising paradigm to solve an applicable problem when the training classes and test classes are disjoint. Achieving this usually needs experts to externalize their domain knowledge by manually specifying a class-attribute matrix to define which classes have which attributes. Designing a suitable class-attribute matrix is the key to the subsequent procedure, but this design process is tedious and trial-and-error with no guidance. This paper proposes a visual explainable active learning approach with its design and implementation called semantic navigator to solve the above problems. This approach promotes human-AI teaming with four actions (ask, explain, recommend, respond) in each interaction loop. The machine asks contrastive questions to guide humans in the thinking process of attributes. A novel visualization called semantic map explains the current status of the machine. Therefore analysts can better understand why the machine misclassifies objects. Moreover, the machine recommends the labels of classes for each attribute to ease the labeling burden. Finally, humans can steer the model by modifying the labels interactively, and the machine adjusts its recommendations. The visual explainable active learning approach improves humans' efficiency of building zero-shot classification models interactively, compared with the method without guidance. We justify our results with user studies using the standard benchmarks for zero-shot classification.",
        "keywords": [],
        "uid": "v-full-1123",
        "time_stamp": "2021-10-27T13:30:00Z",
        "has_image": true,
        "paper_award": "",
        "image_caption": "The semantic navigator is a mixed-initiative visual analytics system for zero-shot classification. It supports human-AI teaming for zero-shot classification with four actions (ask, explain, recommend, respond) in each interaction loop.  The machine asks contrastive questions to guide humans in the thinking process of attributes. A novel visualization called semantic map explains the current status of the machine. Therefore analysts can better understand why the machine misclassifies objects. Moreover, the machine recommends the labels of classes for each attribute to ease the labeling burden. Finally, humans can steer the model by modifying the labels interactively, and the machine adjusts its recommendations. The semantic navigator improves the efficiency of building zero-shot classification models interactively compared with the method without guidance. ",
        "external_paper_link": "",
        "has_pdf": true,
        "ff_link": "https://youtu.be/uDq00Plsct4"
    },
    "v-full-1112": {
        "authors": [
            "Xingbo Wang",
            "Jianben He",
            "Zhihua Jin",
            "Muqiao Yang",
            "Yong Wang",
            "Huamin Qu"
        ],
        "title": "M^2Lens: Visualizing and Explaining Multimodal Models for Sentiment Analysis",
        "session_id": "v-full-full20",
        "abstract": "Multimodal sentiment analysis aims to recognize people\u2019s attitudes from\n  multiple communication channels such as verbal content (i.e., text), voice, and facial expressions. It has become a vibrant and important research topic in natural language processing. Much research focuses on modeling the complex intra- and inter-modal interactions between different communication channels.\n  However, current multimodal models with strong performance are often deep-learning-based techniques and work like black boxes.\n  It is not clear how models utilize multimodal information for sentiment predictions.\n  Despite recent advances in techniques for enhancing the explainability of machine learning models, they often target unimodal scenarios (e.g., images, sentences),\n  and little research has been done on explaining multimodal models.\n  In this paper, we present an interactive visual analytics system, M2Lens, to visualize and explain multimodal models for sentiment analysis. M2Lens provides explanations on intra- and inter-modal interactions at the global, subset, and local levels. Specifically, it summarizes the influence of three typical interaction types (i.e., dominance, complement, and conflict) on the model predictions. \n  Moreover, M2Lens identifies frequent and influential multimodal features and supports the multi-faceted exploration of model behaviors from language, acoustic, and visual modalities.\n  Through two case studies and expert interviews, we demonstrate our system can help users gain deep insights into the multimodal models for sentiment analysis.",
        "keywords": [],
        "uid": "v-full-1112",
        "time_stamp": "2021-10-27T13:45:00Z",
        "has_image": true,
        "paper_award": "honorable",
        "image_caption": "In this paper, we present M2Lens, a novel explanatory visual analytics system to visualize and explain multimodal models for sentiment analysis.\nSpecifically, it facilitates multi-level and multi-faceted exploration of intra- and inter-modal interactions.\n",
        "external_paper_link": "",
        "has_pdf": true,
        "ff_link": "https://youtu.be/AGpbSwRnhaQ"
    },
    "v-full-1194": {
        "authors": [
            "Haekyu Park",
            "Nilaksh Das",
            "Rahul Duggal",
            "Austin Wright",
            "Omar Shaikh",
            "Fred Hohman",
            "Duen Horng Chau"
        ],
        "title": "NeuroCartography: Scalable Automatic Visual Summarization of Concepts in Deep Neural Networks",
        "session_id": "v-full-full20",
        "abstract": "Existing research on making sense of deep neural networks often focuses on neuron-level interpretation, which may not adequately capture the bigger picture of how concepts are collectively encoded by multiple neurons. We present NeuroCartography, an interactive system that scalably summarizes and visualizes concepts learned by neural networks. It automatically discovers and groups neurons that detect the same concepts, and describes how such neuron groups interact to form higher-level concepts and the subsequent predictions. NeuroCartography introduces two scalable summarization techniques: (1) neuron clustering groups neurons based on the semantic similarity of the concepts detected by neurons (e.g., neurons detecting \u201cdog faces\u201d of different breeds are grouped); and (2) neuron embedding encodes the associations between related concepts based on how often they co-occur (e.g., neurons detecting \u201cdog face\u201d and \u201cdog tail\u201d are placed closer in the embedding space). Key to our scalable techniques is the ability to efficiently compute all neuron pairs\u2019 relationships, in time linear to the number of neurons instead of quadratic time. NeuroCartography scales to large data, such as the ImageNet dataset with 1.2M images. The system\u2019s tightly coordinated views integrate the scalable techniques to visualize the concepts and their relationships, projecting the concept associations to a 2D space in Neuron Projection View, and summarizing neuron clusters and their relationships in Graph View. Through a large-scale human evaluation, we demonstrate that our technique discovers neuron groups that represent coherent, human-meaningful concepts. And through usage scenarios, we describe how our approaches enable interesting and surprising discoveries, such as concept cascades of related and isolated concepts. The NeuroCartography visualization runs in modern browsers and is open-sourced.",
        "keywords": [],
        "uid": "v-full-1194",
        "time_stamp": "2021-10-27T14:00:00Z",
        "has_image": true,
        "paper_award": "",
        "image_caption": "",
        "external_paper_link": "",
        "has_pdf": true,
        "ff_link": ""
    },
    "v-tvcg-9420254": {
        "authors": [
            "Junpeng Wang",
            "Wei Zhang",
            "Hao Yang",
            "Chin-Chia Yeh",
            "Liang Wang"
        ],
        "title": "Visual Analytics for RNN-Based Deep Reinforcement Learning",
        "session_id": "v-full-full20",
        "abstract": "Deep reinforcement learning (DRL) targets to train an autonomous agent to interact with a pre-defined environment and strives to achieve specific goals through deep neural networks (DNN). Recurrent neural network (RNN) based DRL has demonstrated superior performance, as RNNs can effectively capture the temporal evolution of the environment and respond with proper agent actions. However, apart from the outstanding performance, little is known about how RNNs understand the environment internally and what has been memorized over time. Revealing these details is extremely important for deep learning experts to understand and improve DRLs, which in contrast, is also challenging due to the complicated data transformations inside these models. In this paper, we propose Deep Reinforcement Learning Interactive Visual Explorer (DRLIVE), a visual analytics system to effectively explore, interpret, and diagnose RNN-based DRLs. Focused on DRL agents trained for different Atari games, DRLIVE targets to accomplish three tasks: game episode exploration, RNN hidden/cell state examination, and interactive model perturbation. Using the system, one can flexibly explore a DRL agent through interactive visualizations, discover interpretable RNN cells by prioritizing RNN hidden/cell states with a set of metrics, and further diagnose the DRL model by interactively perturbing its inputs. Through concrete studies with multiple deep learning experts, we validated the efficacy of DRLIVE.",
        "keywords": [
            "Deep reinforcement learning (DRL)",
            "recurrent neural network (RNN)",
            "model interpretation",
            "visual analytics."
        ],
        "uid": "v-tvcg-9420254",
        "time_stamp": "2021-10-27T14:15:00Z",
        "has_image": true,
        "paper_award": "",
        "image_caption": "RNN-based deep reinforcement learning models extract static state information from game screens and memorize dynamic state information through RNN cells (i.e., hidden or cell states). Through visual analytics, we (1) provide an efficient visual summary of long game episodes, (2) prioritize important RNN internal cells by sorting them with different metrics, and (3) interpret what the RNN cells have captured or memorized through interactive perturbations.",
        "external_paper_link": "https://doi.org/10.1109/TVCG.2021.3076749",
        "has_pdf": false,
        "ff_link": "https://youtu.be/QJrHluJ3NgQ"
    },
    "v-full-1314": {
        "authors": [
            "Kushin Mukherjee",
            "Brian Yin",
            "Brianne Sherman",
            "Laurent Lessard",
            "Karen Schloss"
        ],
        "title": "Context Matters: A Theory of Semantic Discriminability for Perceptual Encoding Systems",
        "session_id": "v-full-full18",
        "abstract": "People\u2019s associations between colors and concepts influence their ability to interpret the meanings of colors in information visualizations. Previous work has suggested such effects are limited to concepts that have strong, specific associations with colors. However, although a concept may not be strongly associated with any colors, its mapping can be disambiguated in the context of other concepts in an encoding system. We articulate this view in Semantic Discriminability Theory, a general framework for understanding conditions determining when people can infer meaning from perceptual features. Semantic discriminability is the degree to which observers can infer a unique mapping between visual features and concepts. Semantic Discriminability Theory posits that the capacity for semantic discriminability for a set of concepts is constrained by the difference between the feature-concept association distributions across the concepts in the set. We define formal properties of this theory, and test its implications in two experiments. The results show that the capacity to produce semantically discriminable colors for sets of concepts was indeed constrained by the statistical distance between color-concept association distributions (Experiment 1). Moreover, people could interpret meanings of colors in bar graphs insofar as the colors were semantically discriminable, even for concepts previously deemed \u201cnon-colorable\u201d (Experiment 2). The results suggest that colors are more robust for visual communication than previously thought.",
        "keywords": [],
        "uid": "v-full-1314",
        "time_stamp": "2021-10-27T13:00:00Z",
        "has_image": true,
        "paper_award": "honorable",
        "image_caption": "The image shows shows the authors along with color concept association profiles for six concepts - (from top left to bottom right) peach, driving, eggplant, celery, comfort, and grape. The first column has concepts with high distribution difference, the middle column with medium distribution difference, and the third with low distribution difference. The paper can be found at https://arxiv.org/pdf/2108.03685.pdf",
        "external_paper_link": "",
        "has_pdf": true,
        "ff_link": ""
    },
    "v-tvcg-9451590": {
        "authors": [
            "Greg Woodin",
            "Bodo Winter",
            "Lace Padilla"
        ],
        "title": "Conceptual Metaphor and Graphical Convention Influence the Interpretation of Line Graphs",
        "session_id": "v-full-full18",
        "abstract": "Many metaphors in language reflect conceptual metaphors that structure thought. In line with metaphorical expressions such as \u2018high number\u2019, experiments show that people associate larger numbers with upward space. Consistent with this metaphor, high numbers are conventionally depicted in high positions on the y-axis of line graphs. People also associate good and bad (emotional valence) with upward and downward locations, in line with metaphorical expressions such as \u2018uplifting\u2019 and \u2018down in the dumps\u2019. Graphs depicting good quantities (e.g., vacation days) are consistent with graphical convention and the valence metaphor, because \u2018more\u2019 of the good quantity is represented by higher y-axis positions. In contrast, graphs depicting bad quantities (e.g., murders) are consistent with graphical convention, but not the valence metaphor, because more of the bad quantity is represented by higher (rather than lower) y-axis positions. We conducted two experiments (N = 300 per experiment) where participants answered questions about line graphs depicting good and bad quantities. For some graphs, we inverted the conventional axis ordering of numbers. Line graphs that aligned (vs misaligned) with valence metaphors (up = good) were easier to interpret, but this beneficial effect did not outweigh the adverse effect of inverting the axis numbering. Line graphs depicting good (vs bad) quantities were easier to interpret, as were graphs that depicted quantity using the x-axis (vs y-axis). Our results suggest that conceptual metaphors matter for the interpretation of line graphs. However, designers of line graphs are warned against subverting graphical convention to align with conceptual metaphors.",
        "keywords": [
            "Conceptual Metaphor Theory",
            "More is Up",
            "Mental Number Line",
            "Cognition",
            "Linguistics",
            "Emotional Valence",
            "Line Graph",
            "Axis Reversal",
            "Handedness",
            "Empirical Evaluation"
        ],
        "uid": "v-tvcg-9451590",
        "time_stamp": "2021-10-27T13:15:00Z",
        "has_image": true,
        "paper_award": "",
        "image_caption": "Title: Conceptual metaphor and graphical convention influence the interpretation of line graphs. Authors: Greg Woodin, Bodo Winter, and Lace Padilla.\n",
        "external_paper_link": "https://doi.org/10.1109/TVCG.2021.3088343",
        "has_pdf": false,
        "ff_link": "https://youtu.be/W_VnI5lc_uw"
    },
    "v-full-1398": {
        "authors": [
            "Caitlyn McColeman",
            "Fumeng Yang",
            "Timothy F. Brady",
            "Steven Franconeri"
        ],
        "title": "Rethinking the Ranks of Visual Channels",
        "session_id": "v-full-full18",
        "abstract": "Data can be visually represented using visual channels like position, length or luminance. An existing ranking of these visual channels is based on how accurately participants could report the ratio between two depicted values. There is an assumption that this ranking should hold for different tasks and for different numbers of marks. However, there is surprisingly little existing work that tests this assumption, especially given that visually computing ratios is relatively unimportant in real-world visualizations, compared to seeing, remembering, and comparing trends and motifs, across displays that almost universally depict more than two values. To simulate the information extracted from a glance at a visualization, we instead asked participants to immediately reproduce a set of values from memory after they were shown the visualization. These values could be shown in a bar graph (position (bar)), line graph (position (line)), heat map (luminance), bubble chart (area), misaligned bar graph (length), or \u2018wind map\u2019 (angle). \n  With a Bayesian multilevel modeling approach, we observed how the relevant rank positions of visual channels shift across different numbers of marks (2, 4 or 8) and for bias, precision, and error measures. The ranking did not hold, even for reproductions of only 2 marks, and the new ranking was highly inconsistent for reproductions of different numbers of marks. Other factors besides channel choice had an order of magnitude more influence on performance, such as the number of values in the series (e.g., more marks led to larger errors), or the value of each mark (e.g., small values were systematically overestimated). \n  Every visual channel was worse for displays with 8 marks than 4, consistent with established limits on visual memory.\n  These results point to the need for a body of empirical studies that move beyond two-value ratio judgments as a baseline for ranking the quality of a visual channel, including testing new tasks (detection of trends or motifs), timescales (immediate computation, or later comparison), and the number of values (from a handful, to thousands).",
        "keywords": [],
        "uid": "v-full-1398",
        "time_stamp": "2021-10-27T13:30:00Z",
        "has_image": true,
        "paper_award": "honorable",
        "image_caption": "An existing ranking of visual channels is based on how accurately participants could report the ratio between two depicted values. There is an assumption that this ranking should hold for different tasks and for different numbers of marks. However, there is surprisingly little existing work that tests this assumption, especially given that visually computing ratios is relatively unimportant in real-world visualizations, compared to seeing, remembering, and comparing trends and motifs. We asked participants to reproduce a set of values from memory after they were shown the visualization. These values could be encoded by six visual channels.\n",
        "external_paper_link": "",
        "has_pdf": true,
        "ff_link": ""
    },
    "v-tvcg-9318559": {
        "authors": [
            "Cary Anderson",
            "Anthony Robinson"
        ],
        "title": "Affective Congruence in Visualization Design: Influences on Reading Categorical Maps",
        "session_id": "v-full-full18",
        "abstract": "Recent work in data visualization has demonstrated that small, perceptually-distinct color palettes\u2014such as those used in categorical mapping\u2014can connote significant affective qualities. Data that are mapped or otherwise visualized are also often emotive in nature, either inherently (e.g., climate change, disease mortality rates), or by design, such as can be found in visual storytelling. However, little is known about how the affective qualities of color interact with those of data context in visualization design. This paper describes the results of a crowdsourced study on the influence of affectively congruent versus incongruent color schemes on categorical map-reading response. We report both objective (pattern detection; area comparison) and subjective (affective quality; appropriateness; preference) measures of map-reader response. Our results suggest that affectively congruent colors amplify perceptions of the affective qualities of maps with emotive topics, affective incongruence may cause confusion, and that affective congruence is particularly influential in maps of positive-leaning data topics. Finally, we offer preliminary design recommendations for balancing color congruence with other design factors, and for synthesizing color and affective context in thematic map design.",
        "keywords": [
            "."
        ],
        "uid": "v-tvcg-9318559",
        "time_stamp": "2021-10-27T13:45:00Z",
        "has_image": true,
        "paper_award": "",
        "image_caption": "Example affectively congruent (left) and affectively incongruent (right) maps.",
        "external_paper_link": "https://doi.ieeecomputersociety.org/10.1109/TVCG.2021.3050118",
        "has_pdf": false,
        "ff_link": "https://youtu.be/ELPML9RGhEY"
    },
    "v-full-1633": {
        "authors": [
            "Min Lu",
            "Joel Lanir",
            "Chufeng Wang",
            "Yucong Yao",
            "Wen Zhang",
            "Oliver Deussen",
            "Hui Huang"
        ],
        "title": "Modeling Just Noticeable Differences in Charts",
        "session_id": "v-full-full18",
        "abstract": "One of the fundamental tasks in visualization is to compare two or more visual elements. However, it is often difficult to visually differentiate graphical elements encoding a small difference in value, such as the heights of similar bars in bar chart or angles of similar sections in pie chart. Perceptual laws can be used in order to model when and how we perceive this difference. In this work, we model the perception of Just Noticeable Differences (JNDs), the minimum difference in visual attributes that allow faithfully comparing similar elements, in charts. Specifically, we explore the relation between JNDs and two major visual variables: the intensity of visual elements and the distance between them, and study it in three charts: bar chart, pie chart and bubble chart. Through an empirical study, we identify main effects on JND for distance in bar charts, intensity in pie charts, and both distance and intensity in bubble charts. By fitting a linear mixed effects model, we model JND and find that JND grows as the exponential function of variables. We highlight several usage scenarios that make use of the JND modeling in which elements below the fitted JND are detected and enhanced with secondary visual cues for better discrimination.",
        "keywords": [],
        "uid": "v-full-1633",
        "time_stamp": "2021-10-27T14:00:00Z",
        "has_image": true,
        "paper_award": "",
        "image_caption": "We modeled Just Noticeable Differences in Charts. Four charts of the same dataset are detected with different pairs below the Just Noticeable Difference (JND) threshold. From left to right: a bar chart with no pairs below JND, the difference between all pairs of bars in the graph is noticeable; with a different order, there is a pair of bars below JND (A-B); two pairs of indistinguishable fans are detected in the pie chart (A-B and C-F); three pairs of circles are detected as not distinguishable in the bubble chart (A-B, C-F and D-E).",
        "external_paper_link": "",
        "has_pdf": true,
        "ff_link": ""
    },
    "v-tvcg-9262081": {
        "authors": [
            "Jinwook Bok",
            "Bohyoung Kim",
            "Jinwook Seo"
        ],
        "title": "Augmenting Parallel Coordinates Plots with Color-coded Stacked Histograms",
        "session_id": "v-full-full18",
        "abstract": "We introduce Parallel Histogram Plot (PHP), a technique that overcomes the innate limitations of parallel coordinates plot (PCP) by attaching stacked-bar histograms with discrete color schemes to PCP. The color-coded histograms enable users to see an overview of the whole data without cluttering or scalability issues. Each rectangle in the PHP histograms is color coded according to the data ranking by a selected attribute. This color-coding scheme allows users to visually examine relationships between attributes, even between those that are displayed far apart, without repositioning or reordering axes. We adopt the Visual Information Seeking Mantra so that the polylines of the original PCP can be used to show details of a small number of selected items when the cluttering problem subsides. We also design interactions, such as a focus+context technique, to help users investigate small regions of interest in a space-efficient manner. We provide a real-world example in which PHP is effectively utilized compared with other visualizations, and we perform a controlled user study to evaluate the performance of PHP in helping users estimate the correlation between attributes. The results demonstrate that the performance of PHP was consistent in the estimation of correlations between two attributes regardless of the distance between them.",
        "keywords": [
            "Parallel Coordinates Plots",
            "Parallel Histogram Plots",
            "Color-coded Stacked Histogram"
        ],
        "uid": "v-tvcg-9262081",
        "time_stamp": "2021-10-27T14:15:00Z",
        "has_image": true,
        "paper_award": "",
        "image_caption": "Parallel Histogram Plot (PHP) is a technique that overcomes the innate limitations of parallel coordinates plot (PCP) by attaching stacked-bar histograms with discrete color schemes to PCP. The histograms provide an overview of the whole data without cluttering or scalability issues. Each rectangle in the histograms is color-coded according to the data ranking by a selected attribute. This color-coding enables observation of relationships between attributes, even between those that are displayed far apart. Adopting the Visual Information Seeking Mantra, polylines of PCP are used to show details of a small number of selected items when the cluttering problem subsides.",
        "external_paper_link": "https://doi.org/10.1109/TVCG.2020.3038446",
        "has_pdf": false,
        "ff_link": "https://youtu.be/jwkJcz0HIls"
    },
    "v-tvcg-9258424": {
        "authors": [
            "Jose Iglesias-Guitian",
            "Prajita Mane",
            "Bochang Moon"
        ],
        "title": "Real-Time Denoising of Volumetric Path Tracing for Direct Volume Rendering",
        "session_id": "v-full-full7",
        "abstract": "Direct Volume Rendering (DVR) using Volumetric Path Tracing (VPT) is a scientific visualization technique that simulates light transport with objects' matter using physically-based lighting models. Monte Carlo (MC) path tracing is often used with surface models, yet its application for volumetric models is difficult due to the complexity of integrating MC light-paths in volumetric media with none or smooth material boundaries. Moreover, auxiliary geometry-buffers (G-buffers) produced for volumes are typically very noisy, failing to guide image denoisers relying on that information to preserve image details. This makes existing real-time denoisers, which take noise-free G-buffers as their input, less effective when denoising VPT images. We propose the necessary modifications to an image-based denoiser previously used when rendering surface models, and demonstrate effective denoising of VPT images. In particular, our denoising exploits temporal coherence between frames, without relying on noise-free G-buffers, which has been a common assumption of existing denoisers for surface-models. Our technique preserves high-frequency details through a weighted recursive least squares that handles heterogeneous noise for volumetric models. We show for various real data sets that our method improves the visual fidelity and temporal stability of VPT during classic DVR operations such as camera movements, modifications of the light sources, and editions to the volume transfer function.",
        "keywords": [
            "Volume rendering",
            "global illumination",
            "path-tracing",
            "participating media",
            "image-space filtering",
            "real-time denoising."
        ],
        "uid": "v-tvcg-9258424",
        "time_stamp": "2021-10-27T13:00:00Z",
        "has_image": true,
        "paper_award": "",
        "image_caption": "Volumetric Path Tracing (VPT) results were generated using the same source volume but during the interactive manipulation using Direct Volume Rendering (DVR) with different transfer functions. Multiple scattering bounces per ray are simulated in real-time.\nOur real-time denoising improves VPT images (MC-DVR with only 2 spp) while reducing its noise effectively. Offline VPT with 1024 spp, taking minutes to produce a single image, is shown as a reference.\n minutes to produce a single image, is shown as reference",
        "external_paper_link": "https://doi.org/10.1109/TVCG.2020.3037680",
        "has_pdf": false,
        "ff_link": "https://youtu.be/zMbNUhlLgIE"
    },
    "v-tvcg-9264699": {
        "authors": [
            "Sebastian Weiss",
            "Mustafa Isik",
            "Justus Thies",
            "R\u00fcdiger Westermann"
        ],
        "title": "Learning Adaptive Sampling and Reconstruction for Volume Visualization",
        "session_id": "v-full-full7",
        "abstract": "A central challenge in data visualization is to understand which data samples are required to generate an image of a data set in which the relevant information is encoded. In this work, we make a first step towards answering the question of whether an artificial neural network can predict where to sample the data with higher or lower density, by learning of correspondences between the data, the sampling patterns and the generated images. We introduce a novel neural rendering pipeline, which is trained end-to-end to generate a sparse adaptive sampling structure from a given low-resolution input image, and reconstructs a high-resolution image from the sparse set of samples. For the first time, to the best of our knowledge, we demonstrate that the selection of structures that are relevant for the final visual representation can be jointly learned together with the reconstruction of this representation from these structures. Therefore, we introduce differentiable sampling and reconstruction stages, which can leverage back-propagation based on supervised losses solely on the final image. We shed light on the adaptive sampling patterns generated by the network pipeline and analyze its use for volume visualization including isosurface and direct volume rendering.",
        "keywords": [
            "Volume visualization",
            "adaptive sampling",
            "deep learning"
        ],
        "uid": "v-tvcg-9264699",
        "time_stamp": "2021-10-27T13:15:00Z",
        "has_image": true,
        "paper_award": "",
        "image_caption": "From left to right: An importance network, together with a differentiable sampler and a reconstruction network, takes a low resolution visualization and infers an importance map from it. From this map, an adaptive sampling pattern with adjustable number of samples is derived, and a volume ray-caster samples the data according to these samples. The reconstruction network completes the visual representation from the sparse set of samples (d) The ground truth visualizations are shown in. The proposed network pipeline works on images of iso-surfaces (top) and direct volume renderings (bottom).",
        "external_paper_link": "https://doi.org/10.1109/TVCG.2020.3039340",
        "has_pdf": false,
        "ff_link": "https://youtu.be/UYobDzRMv8Y"
    },
    "v-full-1098": {
        "authors": [
            "Jun Han",
            "Hao Zheng",
            "Danny Chen",
            "Chaoli Wang"
        ],
        "title": "STNet: An End-to-End Generative Framework for Synthesizing Spatiotemporal Super-Resolution Volumes",
        "session_id": "v-full-full7",
        "abstract": "We present STNet, an end-to-end generative framework that synthesizes spatiotemporal super-resolution volumes with high fidelity for time-varying data. STNet includes two modules: a generator and a spatiotemporal discriminator. The input to the generator is two low-resolution volumes at both ends, and the output is the intermediate and the two-ending spatiotemporal super- resolution volumes. The spatiotemporal discriminator, leveraging convolutional long short-term memory, accepts a spatiotemporal super-resolution sequence as input and predicts a conditional score for each volume based on its spatial (the volume itself) and temporal (the previous volumes) information. We propose an unsupervised pre-training stage using cycle loss to improve the generalization of STNet. Once trained, STNet can generate spatiotemporal super-resolution volumes from low-resolution ones, offering scientists an option to save data storage (i.e., sparsely sampling the simulation output in both spatial and temporal dimensions). We compare STNet with the baseline bicubic+linear interpolation, two deep learning solutions (SSR+TSR, STD), and a state-of-the-art tensor compression solution (TTHRESH) to show the effectiveness of STNet.",
        "keywords": [],
        "uid": "v-full-1098",
        "time_stamp": "2021-10-27T13:30:00Z",
        "has_image": true,
        "paper_award": "",
        "image_caption": "We propose STNet, a spatiotemporal deep learning framework for generating spatiotemporal volumes. The network consists of several feature extraction and interpolation modules for representing spatiotemporal features and one feature upscaling module for generating super-resolution volumes. After that, a spatiotemporal discriminator is utilized to discern the spatial and temporal realness. Pretraining techniques are applied during optimization to boost the capability of network generalization. ",
        "external_paper_link": "",
        "has_pdf": true,
        "ff_link": "https://youtu.be/AezFUomjfzI"
    },
    "v-full-1582": {
        "authors": [
            "Xingdi Zhang",
            "Markus Hadwiger",
            "Thomas Theussl",
            "Peter Rautek"
        ],
        "title": "Interactive Exploration of Physically-Observable Objective Vortices in Unsteady 2D Flow",
        "session_id": "v-full-full7",
        "abstract": "State-of-the-art computation and visualization of vortices in unsteady fluid flow employ objective vortex criteria, which makes them independent of reference frames or observers. However, objectivity by itself, although crucial, is not sufficient to guarantee that one can identify physically-realizable observers that would perceive or detect the same vortices. Moreover, a significant challenge is that a single reference frame is often not sufficient to accurately observe multiple vortices that follow different motions. This paper presents a novel framework for the exploration and use of an interactively-chosen set of observers, of the resulting relative velocity fields, and of objective vortex structures. We show that our approach facilitates the objective detection and visualization of vortices relative to well-adapted reference frame motions, while at the same time guaranteeing that these observers are in fact physically realizable. In order to represent and manipulate observers efficiently, we make use of the low-dimensional vector space structure of the Lie algebra of physically-realizable observer motions. We illustrate that our framework facilitates the efficient choice and guided exploration of objective vortices in unsteady 2D flow, on planar as well as on spherical domains, using well-adapted reference frames.",
        "keywords": [],
        "uid": "v-full-1582",
        "time_stamp": "2021-10-27T13:45:00Z",
        "has_image": true,
        "paper_award": "honorable",
        "image_caption": "We show six different observer-relative visualizations of the same unsteady 2D input flow field, with respect to six different observers (vertical axes correspond to time). Each visualization is relative to a specific physically-realizable observer, depicted via an observer world line in each inset. Different vortex structures become visible relative to different observers, with colors and opacities encoding the coherence of path lines. Our interactive framework enables smoothly choosing, interpolating, and averaging observers, leading to smoothly changing observer-relative visualizations.\n",
        "external_paper_link": "",
        "has_pdf": true,
        "ff_link": "https://youtu.be/0kdWTHGd5yQ"
    },
    "v-tvcg-9184129": {
        "authors": [
            "Shreeraj Jadhav",
            "Konstantin Dmitriev",
            "Joseph Marino",
            "Matthew Barish",
            "Arie Kaufman"
        ],
        "title": "3D Virtual Pancreatography",
        "session_id": "v-full-full7",
        "abstract": "We present 3D virtual pancreatography (VP), a novel visualization procedure and application for non-invasive diagnosis and classification of pancreatic lesions, the precursors of pancreatic cancer. Currently, non-invasive screening of patients is performed through visual inspection of 2D axis-aligned CT images, though the relevant features are often not clearly visible nor automatically detected. VP is an end-to-end visual diagnosis system that includes: a machine learning based automatic segmentation of the pancreatic gland and the lesions, a semi-automatic approach to extract the primary pancreatic duct, a machine learning based automatic classification of lesions into four prominent types, and specialized 3D and 2D exploratory visualizations of the pancreas, lesions and surrounding anatomy. We combine volume rendering with pancreas- and lesion-centric visualizations and measurements for effective diagnosis. We designed VP through close collaboration and feedback from expert radiologists, and evaluated it on multiple real-world CT datasets with various pancreatic lesions and case studies examined by the expert radiologists.",
        "keywords": [
            "Visual diagnosis",
            "Pancreatic cancer",
            "Automatic segmentation",
            "Lesion classification",
            "Planar reformation",
            "Pancreas",
            "Three-dimensional displays",
            "Ducts",
            "Visualization",
            "Computed tomography",
            "Two dimensional displays"
        ],
        "uid": "v-tvcg-9184129",
        "time_stamp": "2021-10-27T14:00:00Z",
        "has_image": true,
        "paper_award": "",
        "image_caption": "Figure shows a snapshot of the user-interface of 3D virtual pancreatography system (3DVP). 3DVP is a comprehensive visualization system for detection and diagnosis of pancreatic lesions. We incorporate fully automatic segmentation of the pancreas and lesions, semi-automatic segmentation of pancreatic duct, and automatic classification of lesions into four prominent types. Additionally, our system supports multiple exploratory 2D and 3D views for thorough examination of the abdominal CT scan.",
        "external_paper_link": "https://doi.ieeecomputersociety.org/10.1109/TVCG.2020.3020958",
        "has_pdf": false,
        "ff_link": "https://youtu.be/Pjrd657eGdY"
    },
    "v-tvcg-9362264": {
        "authors": [
            "Alexander Kumpf",
            "Josef Stumpfegger",
            "Patrick H\u00e4rtl",
            "R\u00fcdiger Westermann"
        ],
        "title": "Visual Analysis of Multi-Parameter Distributions across Ensembles of 3D Fields",
        "session_id": "v-full-full7",
        "abstract": "For an ensemble of 3D multi-parameter fields, we present a visual analytics workflow to analyse whether and which parts of a selected multi-parameter distribution is present in all ensemble members. Supported by a parallel coordinate plot, a multi-parameter brush is applied to all ensemble members to select data points with similar multi-parameter distribution. By a combination of spatial sub-division and a covariance analysis of partitioned sub-sets of data points, a tight partition in multi-parameter space with reduced number of selected data points is obtained. To assess the representativeness of the selected multi-parameter distribution across the ensemble, we propose a novel extension of violin plots that can show multiple parameter distributions simultaneously. We investigate the visual design that effectively conveys (dis-)similarities in multi-parameter distributions, and demonstrate that users can quickly comprehend parameter-specific differences regarding distribution shape and representativeness from a side-by-side view of these plots. In a 3D spatial view, users can analyse and compare the spatial distribution of selected data points in different ensemble members via interval-based isosurface raycasting. In two real-world application cases we show how our approach is used to analyse the multi-parameter distributions across an ensemble of 3D fields.",
        "keywords": [
            "Ensemble visualization",
            "multi-parameter visualization",
            "3D rendering",
            "distribution comparison",
            "parallel coordinate"
        ],
        "uid": "v-tvcg-9362264",
        "time_stamp": "2021-10-27T14:15:00Z",
        "has_image": true,
        "paper_award": "",
        "image_caption": "We present a workflow and high-performance system for analysing the\nrepresentativeness of a selected multi-parameter distribution in a\nsimulation ensemble. An automatic refinement in multi-parameter\nspace (top row) computes tight hyper-boxes in multi-parameter space\nthat are applied to all ensemble members.(d) A novel variant of\nviolin plots enables the effective visualization of multi-parameter\ndistributions and comparison between different members. (e) A\nlinked 3D view shows the locations of data points falling within\nthe selected multi-parameter distribution.\n",
        "external_paper_link": "https://doi.org/10.1109/TVCG.2021.3061925",
        "has_pdf": false,
        "ff_link": "https://youtu.be/cBX2JPTzovw"
    },
    "v-short-1047": {
        "authors": [
            "Vidya Setlur",
            "Sarah Battersby",
            "Tracy Kam Hung Wong"
        ],
        "title": "GeoSneakPique: Visual autocompletion for geospatial queries",
        "session_id": "v-short-short5",
        "abstract": "How many crimes occurred in the city center? And exactly which part of town is the \u2018city center'? While location is at the heart of many data questions, geographic location can be difficult to specify in natural language (NL) queries. This is especially true when working with fuzzy cognitive regions or regions that may be defined based on data distributions instead of absolute administrative location (e.g., state, country). GeoSneakPique presents a novel method for using a mapping widget to support the NL query process, allowing users to specify location via direct manipulation with data-driven guidance on spatial distributions to help select the area of interest. Users receive feedback to help them evaluate and refine their spatial selection interactively and can save spatial definitions for re-use in subsequent queries. We conduct a qualitative evaluation of the GeoSneakPique that indicates the usefulness of the interface as well as opportunities for better supporting geospatial workflows in visual analysis tasks employing cognitive regions.",
        "keywords": [
            "Software Prototype",
            "Geospatial Data"
        ],
        "uid": "v-short-1047",
        "time_stamp": "2021-10-27T13:00:00Z",
        "has_image": true,
        "paper_award": "",
        "image_caption": "Screenshot of GeoSneakPique showing a query for largest magnitude earthquakes (a) within a user-specified free draw polygon for a geographic area of interest (c). The GeoSneakPique widget (b) provides a hexbin-based preview of the data distribution as well as a detailed basemap for additional spatial context. The system interface provides feedback (d) on administrative geographic regions included (i.e., states) and allows for naming and saving the region for future queries ('middle us').\n",
        "external_paper_link": "",
        "has_pdf": true,
        "ff_link": "https://youtu.be/qKJtJHvtCQc"
    },
    "v-short-1064": {
        "authors": [
            "Zhicheng Liu",
            "Chen Chen",
            "Francisco Morales",
            "Yishan Zhao"
        ],
        "title": "Atlas: Grammar-based Procedural Generation of Data Visualizations",
        "session_id": "v-short-short5",
        "abstract": "We present Atlas, a procedural grammar for constructing data visualizations. Unlike most visualization grammars which use declarative specifications to describe visualization components, Atlas exposes the generative process of a visualization through a set of concatenated high-level production rules. These rules define how graphical objects are created, transformed and coupled with abstract data. The input and output of each rule is clearly defined, allowing inspection of visualization states throughout the generative process. We demonstrate Atlas\u2019 expressiveness through a catalog of more than 40 visualization designs, and its extensibility through a case study of area-based visualizations.",
        "keywords": [
            "Software Architecture, Toolkit/Library, Language",
            "Software Prototype",
            "Tabular Data"
        ],
        "uid": "v-short-1064",
        "time_stamp": "2021-10-27T13:10:00Z",
        "has_image": true,
        "paper_award": "",
        "image_caption": "Atlas is a procedural visualization grammar. The grammar describes how an input graphical object is transformed into an output object through a set of high-level production rules. By concatenating these rules, we can create expressive data visualization designs in an explainable, stepwise process. ",
        "external_paper_link": "",
        "has_pdf": true,
        "ff_link": "https://youtu.be/rArAXIrquvk"
    },
    "v-short-1034": {
        "authors": [
            "Andrew M McNutt"
        ],
        "title": "On The Potential of Zines as a Medium for Visualization",
        "session_id": "v-short-short5",
        "abstract": "Zines are a form of small-circulation self-produced publication often akin to a magazine. This free-form medium has a long history and has been used as means for personal or intimate expression, as a way for marginalized people to describe issues that are important to them, and as a venue for graphical experimentation. It would seem then that zines would make an ideal vehicle for the recent interest in applying feminist or humanist ideas to visualization. Yet, there has been little work combining visualization and zines. In this paper we explore the potential of this intersection by analyzing examples of zines that use data graphics and by describing the pedagogical value that they can have in a visualization classroom. In doing so, we argue that there are plentiful opportunities for visualization research and practice in this rich intersectional-medium.",
        "keywords": [
            "Social Science, Education, Humanities, Journalism, Intelligence Analysis, Knowledge Work",
            "Communication/Presentation, Storytelling",
            "General Public"
        ],
        "uid": "v-short-1034",
        "time_stamp": "2021-10-27T13:20:00Z",
        "has_image": true,
        "paper_award": "",
        "image_caption": "\ufeffWhere are the zines using visualization? Despite a seemingly natural overlap there has been little work combining zines and visualization. In this paper we explore this gap and highlight the potential of embracing zines as a medium for research and practice.\n",
        "external_paper_link": "",
        "has_pdf": true,
        "ff_link": "https://youtu.be/2GdKXvh5iNE"
    },
    "v-short-1073": {
        "authors": [
            "Brian Felipe Keith Norambuena",
            "Tanushree Mitra",
            "Chris North"
        ],
        "title": "Narrative Sensemaking: Strategies for Narrative Maps Construction",
        "session_id": "v-short-short5",
        "abstract": "Narrative sensemaking is a fundamental process to understand sequential information. Narrative maps are a visual representation framework that can aid analysts in their sensemaking process. Narrative maps allow analysts to understand the big picture of a narrative, uncover new relationships between events, and model connections between storylines. We seek to understand how analysts construct narrative maps in order to improve narrative map representation and extraction methods. We perform an experiment with a data set of news articles. Our main contribution is an analysis of how analysts construct narrative maps. The insights extracted from our study can be used to design narrative map visualizations, extraction algorithms, and visual analytics tools to support the sensemaking process.",
        "keywords": [
            "Social Science, Education, Humanities, Journalism, Intelligence Analysis, Knowledge Work",
            "Data Analysis, Reasoning, Problem Solving, and Decision Making",
            "Visual Representation Design",
            "Human-Subjects Qualitative Studies",
            "Text/Document Data"
        ],
        "uid": "v-short-1073",
        "time_stamp": "2021-10-27T13:30:00Z",
        "has_image": true,
        "paper_award": "",
        "image_caption": "In this paper, we conduct a study to understand how analysts manually construct narrative maps. Our findings provide a characterization of the types of connections, the construction strategies, and the graph and layout properties of the resulting narrative maps.",
        "external_paper_link": "",
        "has_pdf": true,
        "ff_link": ""
    },
    "v-short-1092": {
        "authors": [
            "Nicole Sultanum",
            "Anastasia Bezerianos",
            "Fanny Chevalier"
        ],
        "title": "Text Visualization and Close Reading for Journalism with Storifier",
        "session_id": "v-short-short5",
        "abstract": "Journalistic inquiry often requires analysis and close study of large text collections around a particular topic. We argue that this practice could benefit from a more text- and reading-centered approach to journalistic text analysis, one that allows for a fluid transition between overview of entities of interest, the context of these entities in the text, down to the detailed documents they are extracted from. We present the design and development process of Storifier, informed by a close collaboration with a large francophone news office. We also discuss a case study on how our tool was used to analyze a text collection and publish a story.",
        "keywords": [
            "Social Science, Education, Humanities, Journalism, Intelligence Analysis, Knowledge Work",
            "Charts, Diagrams, and Plots",
            "Coordinated and Multiple Views",
            "Multi-Resolution and Level of Detail Techniques",
            "Data Analysis, Reasoning, Problem Solving, and Decision Making",
            "Application Motivated Visualization",
            "Deployment",
            "Human-Subjects Qualitative Studies",
            "Text/Document Data"
        ],
        "uid": "v-short-1092",
        "time_stamp": "2021-10-27T13:40:00Z",
        "has_image": true,
        "paper_award": "",
        "image_caption": "A screenshot of the user interface of Storifier, overlaid by the title of the work (Text Visualization and Close Reading for Journalism with Storifier) and by the URL of the system (storifier.cs.toronto.edu).",
        "external_paper_link": "",
        "has_pdf": true,
        "ff_link": ""
    },
    "v-short-1136": {
        "authors": [
            "Sahaj Vaidya",
            "Jie Cai",
            "Soumyadeep Basu",
            "Azadeh Naderi",
            "Donghee Yvette Wohn",
            "Aritra Dasgupta"
        ],
        "title": "Conceptualizing Visual Analytic Interventions for Content Moderation",
        "session_id": "v-short-short5",
        "abstract": "Our work introduces a visual analytic task abstraction framework for addressing data-driven problems in proactive content moderation. We also discuss the implications of the framework for influencing the future of transparent and communicative moderation practices through visual analytic solutions. As a next step, we plan to realize our proposed visual analytic tasks within existing content moderation workflows. We will also conduct empirical studies to evaluate the effectiveness of the visual analytic interventions and the resulting human-machine interfaces in reducing the cognitive load and emotional stress of content moderators.",
        "keywords": [
            "Social Science, Education, Humanities, Journalism, Intelligence Analysis, Knowledge Work",
            "Task Abstractions & Application Domains"
        ],
        "uid": "v-short-1136",
        "time_stamp": "2021-10-27T13:50:00Z",
        "has_image": true,
        "paper_award": "",
        "image_caption": "Modern social media platforms like Twitch, YouTube, etc., embody an open space for content creation and consumption. \nHowever, an unintended consequence of such content democratization is the proliferation of toxicity that \ncontent creators get subjected to. Commercial and volunteer content moderators play an \nindispensable role in identifying bad actors and minimizing the scale and degree of harmful content. \nModeration tasks are often laborious, complex, and even if semi-automated, they involve high-consequence human decision making. \nIn this paper, through an interdisciplinary collaboration among researchers from social science, human-computer interaction, \nand visualization, we contribute a characterization of the data-driven problems \nand a mapping between the needs and visual analytic tasks through a task abstraction framework. ",
        "external_paper_link": "",
        "has_pdf": true,
        "ff_link": ""
    },
    "v-short-1157": {
        "authors": [
            "Rahul Bhargava",
            "Dee Williams",
            "Catherine D\u2019Ignazio"
        ],
        "title": "How Learners Sketch Data Stories",
        "session_id": "v-short-short5",
        "abstract": "\\abstract{Learning data storytelling involves a complex web of skills. Professional and academic educational offerings typically focus on the computational skills required, but professionals in the field employ many non-technical methods. Sketching by hand on paper is a common practice. This paper introduces and classifes a corpus of 101 data sketches produced by participants as part of a guided learning activity in informal and formal settings. We manually coded each sketch against 12 metrics related to visual encodings, representations, and story structure. We find evidence for preferential use of positional and shape-based encodings, a wide variety of mixed symbolic and textual representations, and a high prevalence of stories comparing subsets of data. These findings contribute to our understanding of how learners sketch with data. They can inform tool design for learners, and help create educational programs that introduce novices to sketching practices from the field.",
        "keywords": [
            "Communication/Presentation, Storytelling",
            "General Public",
            "Methodologies",
            "Taxonomy, Models, Frameworks, Theory",
            "Visual Representation Design"
        ],
        "uid": "v-short-1157",
        "time_stamp": "2021-10-27T14:00:00Z",
        "has_image": true,
        "paper_award": "",
        "image_caption": "A collage of sketches of data stories made by workshop participants.\n",
        "external_paper_link": "",
        "has_pdf": true,
        "ff_link": "https://youtu.be/PxmXsBku_w8"
    },
    "v-short-1033": {
        "authors": [
            "Younghoon Kim",
            "Jeffrey Heer"
        ],
        "title": "Gemini\u00b2: Generating Keyframe-Oriented Animated Transitions Between Statistical Graphics",
        "session_id": "v-short-short5",
        "abstract": "Complex animated transitions may be easier to understand when divided into separate, consecutive stages. However, effective staging requires careful attention to both animation semantics and timing parameters. We present Gemini\u00b2, a system for creating staged animations from a sequence of chart keyframes. Given only a start state and an end state, Gemini\u00b2 can automatically recommend intermediate keyframes for designers to consider. The Gemini\u00b2 recommendation engine leverages Gemini, our prior work, and GraphScape to itemize the given complex change into semantic edit operations and to recombine operations into stages with a guided order for clearly conveying the semantics. To evaluate Gemini\u00b2\u2019s recommendations, we conducted a human-subject study in which participants ranked recommended animations from both Gemini\u00b2 and Gemini. We find that Gemini\u00b2\u2019s animation recommendation ranking is well aligned with subjects\u2019 preferences, and Gemini\u00b2 can recommend favorable animations that Gemini cannot support.",
        "keywords": [
            "Animation and Motion-related Techniques",
            "Algorithms",
            "Guidelines",
            "Software Architecture, Toolkit/Library, Language",
            "Software Prototype",
            "Human-Subjects Quantitative Studies"
        ],
        "uid": "v-short-1033",
        "time_stamp": "2021-10-27T14:10:00Z",
        "has_image": true,
        "paper_award": "",
        "image_caption": "We are presenting 'Gemini2: Generating Keyframe-Oriented Animated Transitions Between Statistical Graphics'. Gemini2 extends Gemini to support keyframe animations, which provide more expressiveness and make it easier to use. Also, it presents keyframe and animation recommendations for a given transition from one chart to another to help the authoring process.",
        "external_paper_link": "",
        "has_pdf": true,
        "ff_link": ""
    },
    "a-vizsec-4835": {
        "authors": [
            "Robert Gove"
        ],
        "title": "Automatic Narrative Summarization for Visualizing Cyber Security Logs and Incident Reports",
        "session_id": "a-vizsec-vizsec-1",
        "abstract": null,
        "keywords": [],
        "uid": "a-vizsec-4835",
        "time_stamp": "2021-10-27T14:10:00Z",
        "has_image": true,
        "paper_award": "",
        "image_caption": "On the left is the 80 characters from Les Mis\u00e9rables, with connections between characters that interacted with each other. On the right is a similar set of character interactions but for a selected subset of characters, which were determined using a summarization algorithm. This summary contains only 19 characters, compared to the 80 characters in the unsummarized character interactions.",
        "external_paper_link": "",
        "has_pdf": true,
        "ff_link": "https://youtu.be/A9OND3bvtsQ"
    },
    "v-full-1226": {
        "authors": [
            "Yifang Wang",
            "Tai-Quan Peng",
            "Huihua Lu",
            "Haoren Wang",
            "Xiao Xie",
            "Huamin Qu",
            "Yingcai Wu"
        ],
        "title": "Seek for Success: A Visualization Approach for Understanding the Dynamics of Academic Careers",
        "session_id": "v-full-full13",
        "abstract": "How to achieve academic career success has been a long-standing research question in social science research. With the growing availability of large-scale well-documented academic profiles and career trajectories, scholarly interest in career success has been reinvigorated, which has emerged to be an active research domain called the Science of Science (i.e., SciSci). In this study, we adopt an innovative dynamic perspective to examine how individual and social factors will influence career success over time. We propose ACSeeker, an interactive visual analytics approach to explore the potential factors of success and how the influence of multiple factors changes at different stages of academic careers. We first applied a Multi-factor Impact Analysis framework to estimate the effect of different factors on academic career success over time. We then developed a visual analytics system to understand the dynamic effects interactively. A novel timeline is designed to reveal and compare the factor impacts based on the whole population. A customized career line showing the individual career development is provided to allow a detailed inspection. To validate the effectiveness and usability of ACSeeker, we report two case studies and interviews with a social scientist and general researchers.",
        "keywords": [],
        "uid": "v-full-1226",
        "time_stamp": "2021-10-27T15:00:00Z",
        "has_image": true,
        "paper_award": "",
        "image_caption": "How to achieve career success is a long-standing research question that has been studied in various social science disciplines. With the increased availability of academic profiles such as researchers\u2019 careers and scientific outputs, academic careers have become one of the prominent topics in the study of careers. This line of research has regained its prominence with the emergence of Science of Science in the age of computational social science. We use a visual analytics approach to explore the potential factors of academic career success and how their effects change at different stages of a research field. ",
        "external_paper_link": "",
        "has_pdf": true,
        "ff_link": "https://youtu.be/vXNPPudEGtg"
    },
    "v-full-1149": {
        "authors": [
            "Arpit Narechania",
            "Alireza Karduni",
            "Ryan Wesslen",
            "Emily Wall"
        ],
        "title": "VitaLITy: Promoting Serendipitous Discovery of Academic Literature with Transformers & Visual Analytics",
        "session_id": "v-full-full13",
        "abstract": "There are a few prominent practices for conducting reviews of academic literature, including searching for specific keywords on Google Scholar or checking citations from some initial seed paper(s). These approaches serve a critical purpose for academic literature reviews, yet there remain challenges in identifying relevant literature when similar work may utilize different terminology (e.g., mixed-initiative visual analytics papers may not use the same terminology as papers on model-steering, yet the two topics are relevant to one another). In this paper, we introduce a system, VITALITY, intended to complement existing practices. In particular, VITALITY promotes serendipitous discovery of relevant literature using transformer language models, allowing users to find semantically similar papers in a word embedding space given (1) a list of input paper(s) or (2) a working abstract. VITALITY visualizes this document-level embedding space in an interactive 2-D scatterplot using dimension reduction. VITALITY also summarizes meta information about the document corpus or search query, including keywords and co authors, and allows users to save and export papers for use in a literature review. We present qualitative findings from an evaluation of VITALITY, suggesting it can be a promising complementary technique for conducting academic literature reviews. Furthermore, we contribute data from 38 popular data visualization publication venues in VITALITY, and we provide scrapers for the open-source community to continue to grow the list of supported venues.",
        "keywords": [],
        "uid": "v-full-1149",
        "time_stamp": "2021-10-27T15:15:00Z",
        "has_image": true,
        "paper_award": "",
        "image_caption": "VitaLITy is a visual analytic system to facilitate serendipitous discovery of academic literature. We contribute a system informed by results of a formative study with visualization researchers and a dataset of metadata from more than 59k visualization papers. We open-sourced our system and web-scrapers at vitality-vis.github.io.",
        "external_paper_link": "",
        "has_pdf": true,
        "ff_link": "https://youtu.be/n_VReTG1GgE"
    },
    "v-tvcg-9337213": {
        "authors": [
            "Jian Chen",
            "Meng Ling",
            "Rui Li",
            "Petra Isenberg",
            "Tobias Isenberg",
            "Michael Sedlmair",
            "Torsten M\u00f6ller",
            "Robert S. Laramee",
            "Han-Wei Shen",
            "Katharina W\u00fcsche",
            "Qiru Wang"
        ],
        "title": "VIS30K: A Collection of Figures and Tables From IEEE Visualization Conference Publications",
        "session_id": "v-full-full13",
        "abstract": "We present the VIS30K dataset, a collection of 29,689 images that represents 30 years of figures and tables from each track of the IEEE Visualization conference series (Vis, SciVis, InfoVis, VAST). VIS30K\u2019s comprehensive coverage of the scientific literature in visualization not only reflects the progress of the field but also enables researchers to study the evolution of the state-of-the-art and to find relevant work based on graphical content. We describe the dataset and our semi-automatic collection process, which couples convolutional neural networks (CNN) with curation. Extracting figures and tables semi-automatically allows us to verify that no images are overlooked or extracted erroneously. To improve quality further, we engaged in a peer-search process for high-quality figures from early IEEE Visualization papers. With the resulting data, we also contribute VISImageNavigator (VIN,visimagenavigator.github.io), a web-based tool that facilitates searching and exploring VIS30K by author names, paper keywords, title and abstract, and years.",
        "keywords": [
            "."
        ],
        "uid": "v-tvcg-9337213",
        "time_stamp": "2021-10-27T15:30:00Z",
        "has_image": true,
        "paper_award": "",
        "image_caption": "The VIS30K dataset is a collection of 31,481 (1990-2020) images representing 31 years of figures and tables from four tracks of the IEEE Visualization Conference series. We also contribute VISImageNavigator (VIN, visimagenavigator.github.io), a web-based tool that facilitates search and exploring VIS30K by author names, paper keywords, title and abstract, and years. This figure shows a timeline view in VIN of selected images from the first 30 years (1990-2019) of the conference showing diverse and trending research work. (Best viewed electronically, zoomed in.)",
        "external_paper_link": "https://doi.org/10.1109/TVCG.2021.3054916",
        "has_pdf": false,
        "ff_link": "https://youtu.be/Jw6X8RyQ77Q"
    },
    "v-full-1212": {
        "authors": [
            "Natkamon Tovanich",
            "Pierre Dragicevic",
            "Petra Isenberg"
        ],
        "title": "Gender in 30 Years of IEEE Visualization",
        "session_id": "v-full-full13",
        "abstract": "We present an exploratory analysis of gender representation among the authors, committee members, and award winners at the IEEE Visualization (VIS) conference over the last 30 years. Our goal is to provide descriptive data on which diversity discussions and efforts in the community can build. We look in particular at the gender of VIS authors as a proxy for the community at large. We consider measures of overall gender representation among authors, differences in careers, positions in author lists, and collaborations. We found that the proportion of female authors has increased from 9% in the first five years to 22% in the last five years of the conference. Over the years, we found the same representation of women in program committees and slightly more women in organizing committees. Women are less likely to appear in the last author position, but more in the middle positions. In terms of collaboration patterns, female authors tend to collaborate more than expected with other women in the community. All non-gender related data is available on https://osf.io/ydfj4/ and the gender-author matching can be accessed through https://nyu.databrary.org/volume/1301.",
        "keywords": [],
        "uid": "v-full-1212",
        "time_stamp": "2021-10-27T15:45:00Z",
        "has_image": true,
        "paper_award": "",
        "image_caption": "We present an exploratory analysis of gender representation among the authors, committee members, and award winners at the IEEE Visualization (VIS) conference. The proportion of female authors has increased from 9% in the first five years to 22% in the last five years of the conference. We found the same representation of women in program committees and slightly more women in organizing committees. Women are less likely to appear in the last author position but more in the middle positions. In terms of collaboration patterns, female authors tend to collaborate more than expected with other women in the community.",
        "external_paper_link": "",
        "has_pdf": true,
        "ff_link": "https://youtu.be/00FBt95k2-s"
    },
    "v-full-1107": {
        "authors": [
            "Kevin Maher",
            "Zeyuan Huang",
            "Jiancheng Song",
            "Xiaoming Deng",
            "Yu-Kun Lai",
            "Cuixia Ma",
            "Hao Wang",
            "Yong-Jin Liu",
            "Hongan Wang"
        ],
        "title": "E-ffective: A Visual Analytic System for Exploring the Emotion and Effectiveness of Inspirational Speeches",
        "session_id": "v-full-full13",
        "abstract": "What makes speeches effective has long been a subject for debate, and until today there is broad controversy among public speaking experts about what factors make a speech effective as well as the roles of these factors in speeches. Moreover, there is a lack of quantitative analysis methods to help understand effective speaking strategies. In this paper, we propose E-ffective, a visual analytic system allowing speaking experts and novices to analyze both the role of speech factors and their contribution in effective speeches. From interviews with domain experts and investigating existing literature, we figured out important factors to consider in inspirational speeches. We obtained the generated factors from multi-modal data that were then related to effectiveness data. Our system supports rapid understanding of critical factors in inspirational speeches, including the influence of emotions by means of novel visualization methods and interaction. Two novel visualizations include E-spiral (that shows the emotional shifts in speeches in a visually compact way) and E-script (that connects speech content with key speech delivery information). In our evaluation we studied the influence of our system on experts' domain knowledge about speech factors. We further studied the usability of the system by speaking novices and experts on assisting analysis of inspirational speech effectiveness.",
        "keywords": [],
        "uid": "v-full-1107",
        "time_stamp": "2021-10-27T16:00:00Z",
        "has_image": true,
        "paper_award": "",
        "image_caption": "Our E-ffective system supports understanding and exploring the effectiveness of different factors in public speaking.",
        "external_paper_link": "",
        "has_pdf": true,
        "ff_link": "https://youtu.be/TyKuaEtYQm0"
    },
    "v-full-1316": {
        "authors": [
            "Jonathan Roberts",
            "Peter Butcher",
            "Ann Sherlock",
            "Sarah Nason"
        ],
        "title": "Explanatory Journeys: Visualising to Understand and Explain Administrative Justice Paths of Redress",
        "session_id": "v-full-full13",
        "abstract": "Administrative justice concerns the relationships between individuals and the state. It includes redress and complaints on decisions of a child\u2019s education, social care, licensing, planning, environment, housing and homelessness. However, if someone has a complaint or an issue, it is challenging for people to understand different possible redress paths and explore what path is suitable for their situation. Explanatory visualisation has the potential to display these paths of redress in a clear way, such that people can see, understand and explore their options. The visualisation challenge is further complicated because information is spread across many documents, laws, guidance and policies and requires judicial interpretation. Consequently, there is not a single database of paths of redress. In this work we present how we have co-designed a system to visualise administrative justice paths of redress. Simultaneously, we classify, collate and organise the underpinning data, from expert workshops, heuristic evaluation and expert critical reflection. We make four contributions: (i) an application design study of the explanatory visualisation tool (Artemus), (ii) coordinated and co-design approach to aggregating the data, (iii) two in-depth case studies in housing and education demonstrating explanatory paths of redress in administrative law, and (iv) reflections on the expert co-design process and expert data gathering and explanatory visualisation for administrative justice and law.",
        "keywords": [],
        "uid": "v-full-1316",
        "time_stamp": "2021-10-27T16:15:00Z",
        "has_image": true,
        "paper_award": "honorable",
        "image_caption": "Screenshot from Artemus. An explanatory visualization tool,\nin the domain of administrative justice, to enable\npeople to understand potential paths of redress.\n",
        "external_paper_link": "",
        "has_pdf": true,
        "ff_link": "https://youtu.be/uwJeTbqri0M"
    },
    "v-tvcg-9408391": {
        "authors": [
            "Evanthia Dimara",
            "Harry Zhang",
            "Melanie Tory",
            "Steven Franconeri"
        ],
        "title": "The Unmet Data Visualization Needs of Decision Makers within Organizations",
        "session_id": "v-full-full27",
        "abstract": "When an organization chooses one course of action over alternatives, this task typically falls on a decision maker with relevant knowledge, experience, and understanding of context. Decision makers rely on data analysis, which is either delegated to analysts, or done on their own. Often the decision maker combines data, likely uncertain or incomplete, with non-formalized knowledge within a multi-objective problem space, weighing the recommendations of analysts within broader contexts and goals. As most past research in visual analytics has focused on understanding the needs and challenges of data analysts, less is known about the tasks and challenges of organizational decision makers, and how visualization support tools might help. Here we characterize the decision maker as a domain expert, review relevant literature in management theories, and report the results of an empirical survey and interviews with people who make organizational decisions. We identify challenges and opportunities for novel visualization tools, including trade-off overviews, scenario-based analysis, interrogation tools, flexible data input and collaboration support. Our findings stress the need to expand visualization design beyond data analysis into tools for information management.",
        "keywords": [
            "Decision making",
            "visualization",
            "interview",
            "survey",
            "organizations",
            "management",
            "business intelligence."
        ],
        "uid": "v-tvcg-9408391",
        "time_stamp": "2021-10-27T15:00:00Z",
        "has_image": false,
        "paper_award": "",
        "image_caption": "",
        "external_paper_link": "https://doi.org/10.1109/TVCG.2021.3074023",
        "has_pdf": false,
        "ff_link": "https://youtu.be/UAUeKgefNn4"
    },
    "v-full-1308": {
        "authors": [
            "Anna-Pia Lohfink",
            "Simon Duque Anton",
            "Heike Leitte",
            "Christoph Garth"
        ],
        "title": "Knowledge Rocks: Adding Knowledge Assistance to Visualization Systems",
        "session_id": "v-full-full27",
        "abstract": "We present Knowledge Rocks, an implementation strategy and guideline for augmenting visualization systems to knowledge-assisted visualization systems, as defined by the KAVA model. Visualization systems become more and more sophisticated. Hence, it is increasingly important to support users with an integrated knowledge base in making constructive choices and drawing the right conclusions. We support the effective reactivation of visualization software resources by augmenting them with knowledge-assistance. To provide a general and yet supportive implementation strategy, we propose an implementation process that bases on an application-agnostic architecture. This architecture is derived from existing knowledge-assisted visualization systems and the KAVA model. Its centerpiece is an ontology that is able to automatically analyze and classify input data, linked to a database to store classified instances. We discuss design decisions and advantages of the KR framework and illustrate its broad area of application in diverse integration possibilities of this architecture into an existing visualization system. In addition, we provide a detailed case study by augmenting an it-security system with knowledge-assistance facilities.",
        "keywords": [],
        "uid": "v-full-1308",
        "time_stamp": "2021-10-27T15:15:00Z",
        "has_image": true,
        "paper_award": "",
        "image_caption": "The Knowledge Rocks Framework supports adding knowledge assistance to existing visualization systems. We present the framework architecture, discuss possible integrations in visualization systems, and present a detailed case study.\n",
        "external_paper_link": "",
        "has_pdf": true,
        "ff_link": "https://youtu.be/4laWRkt2rC0"
    },
    "v-full-1341": {
        "authors": [
            "Evanthia Dimara",
            "John Stasko"
        ],
        "title": "A Critical Reflection on Visualization Research: Where Do Decision Making Tasks Hide?",
        "session_id": "v-full-full27",
        "abstract": "It has been widely suggested that a key goal of visualization systems is to assist decision making, but is this true? We conduct a critical investigation on whether the activity of decision making is indeed central to the visualization domain. By approaching decision making as a user task, we explore the degree to which decision tasks are evident in visualization research and user studies. Our analysis suggests that decision tasks are not commonly found in current visualization task taxonomies and that the visualization field has yet to leverage guidance from decision theory domains on how to study such tasks. We further found that the majority of visualizations addressing decision making were not evaluated based on their ability to assist decision tasks. Finally, to help expand the impact of visual analytics in organizational as well as casual decision making activities, we initiate a research agenda on how decision making assistance could be elevated throughout visualization research.",
        "keywords": [],
        "uid": "v-full-1341",
        "time_stamp": "2021-10-27T15:30:00Z",
        "has_image": true,
        "paper_award": "",
        "image_caption": "Well-established taxonomies ranging from low to high-level visualization tasks. The red annotations illustrate our main research question: Has decision making been studied explicitly within visualization research, and, if not, should it?",
        "external_paper_link": "",
        "has_pdf": true,
        "ff_link": "https://youtu.be/UDfr73S95YU"
    },
    "v-full-1065": {
        "authors": [
            "Matthew Brehmer",
            "Robert Kosara"
        ],
        "title": "From Jam Session to Recital: Synchronous Communication and Collaboration Around Data in Organization",
        "session_id": "v-full-full27",
        "abstract": "Prior research on communicating with visualization has focused on public presentation and asynchronous individual consumption, such as in the domain of journalism. The visualization research community knows comparatively little about synchronous and multimodal communication around data within organizations, from team meetings to executive briefings. We conducted two qualitative interview studies with individuals who prepare and deliver presentations about data to audiences in organizations. In contrast to prior work, we did not limit our interviews to those who self-identify as data analysts or data scientists. Both studies examined aspects of speaking about data with visual aids such as charts, dashboards, and tables. One study was a retrospective examination of current practices and difficulties, from which we identified three scenarios involving presentations of data. We describe these scenarios using an analogy to musical performance: small collaborative team meetings are akin to jam session, while more structured presentations can range from semi-improvisational performances among peers to formal recitals given to executives or customers. In our second study, we grounded the discussion around three design probes, each examining a different aspect of presenting data: the progressive reveal of visualization to direct attention and advance a narrative, visualization presentation controls that are hidden from the audience\u2019s view, and the coordination of a presenter\u2019s video with interactive visualization. Our distillation of interviewees\u2019 responses surfaced twelve themes, from ways of authoring presentations to creating accessible and engaging audience experiences.",
        "keywords": [],
        "uid": "v-full-1065",
        "time_stamp": "2021-10-27T15:45:00Z",
        "has_image": true,
        "paper_award": "",
        "image_caption": "Top: Three performative data presentation scenarios, using an analogy to musical performance: small collaborative team meetings are akin to jam sessions, while more structured presentations can range from semi-improvisational performances among peers to formal recitals given to executives or customers. Bottom: Our first design probe suggested flexible interactive controls for progressively revealing parts of a chart or dashboard. Our second was a suggested dual-screen setup for presenting data. Our third design probe suggested a way of integrating a presenter's video within an interactive dashboard.",
        "external_paper_link": "",
        "has_pdf": true,
        "ff_link": "https://youtu.be/D0vku033WX8"
    },
    "v-full-1417": {
        "authors": [
            "Alex Kale",
            "Yifan Wu",
            "Jessica Hullman"
        ],
        "title": "Causal Support: Modeling Causal Inferences with Visualizations",
        "session_id": "v-full-full27",
        "abstract": "Analysts often make visual causal inferences about possible data-generating models. However, visual analytics (VA) software tends to leave these models implicit in the mind of the analyst, which casts doubt on the statistical validity of informal visual \"insights\". We formally evaluate the quality of causal inferences from visualizations by adopting causal support---a Bayesian cognition model that learns the probability of alternative causal explanations given some data---as a normative benchmark for causal inferences. We contribute two experiments assessing how well crowdworkers can detect (1) a treatment effect and (2) a confounding relationship. We find that chart users\u2019 causal inferences tend to be insensitive to sample size such that they deviate from our normative benchmark. While interactively cross-filtering data in visualizations can improve sensitivity, on average users do not perform reliably better with common visualizations than they do with textual contingency tables. These experiments demonstrate the utility of causal support as an evaluation framework for inferences in VA and point to opportunities to make analysts' mental models more explicit in VA software.",
        "keywords": [],
        "uid": "v-full-1417",
        "time_stamp": "2021-10-27T16:00:00Z",
        "has_image": true,
        "paper_award": "honorable",
        "image_caption": "Modeling causal inferences with visualizations: (A) Users view and may interact with data visualizations; (B) Ideally, users reason through a series of comparisons that allow them to allocate subjective probabilities to possible data generating processes; and (C) We elicit users\u2019 subjective probabilities as a Dirichlet distribution across possible causal explanations and compare these causal inferences to a computed benchmark of causal support, which we derive from Bayesian inference across possible causal models. ",
        "external_paper_link": "",
        "has_pdf": true,
        "ff_link": "https://youtu.be/Tl6gXHw-EvU"
    },
    "v-full-1492": {
        "authors": [
            "Alexandra Zytek",
            "Dongyu Liu",
            "Rhema Vaithianathan",
            "Kalyan Veeramachaneni"
        ],
        "title": "Sibyl: Understanding and Addressing the Usability Challenges of Machine Learning In High-Stakes Decision Making",
        "session_id": "v-full-full27",
        "abstract": "Machine learning (ML) is being applied to a diverse and ever-growing set of domains. In many cases, domain experts --- who often have no expertise in ML or data science --- are asked to use ML predictions to make high-stakes decisions. Multiple ML usability challenges can appear as result, such as lack of user trust in the model, inability to reconcile human-ML disagreement, and ethical concerns about oversimplification of complex problems to a single algorithm output. In this paper, we investigate the ML usability challenges that present in the domain of child welfare screening through a series of collaborations with child welfare screeners. Following the iterative design process between the ML scientists, visualization researchers, and domain experts (child screeners), we first identified four key ML challenges and honed in on one promising explainable ML technique to address them (local factor contributions). Then we designed, implemented, and evaluated our visual analytics tool, Sibyl, to increase the interpretability and interactivity of local factor contributions. The effectiveness of our tool is demonstrated by two formal user studies with 12 non-expert participants and 13 expert participants respectively. Valuable feedback is collected, from which we also composed a list of design implications as a useful guideline for researchers that aim to develop an interpretable and interactive visualization tool for ML prediction models deployed for child welfare screeners and other similar domain experts.",
        "keywords": [],
        "uid": "v-full-1492",
        "time_stamp": "2021-10-27T16:15:00Z",
        "has_image": true,
        "paper_award": "",
        "image_caption": "Here, we show the Sibyl logo and a snippet from the Sibyl case-specific details interface. This interface visualizes the relative contribution of each feature to the child welfare predictive risk model output. Red bars represent an increased risk associated with this feature, and blue bars represent a decreased risk. ",
        "external_paper_link": "",
        "has_pdf": true,
        "ff_link": "https://youtu.be/ClATgmKwVCs"
    },
    "v-full-1349": {
        "authors": [
            "Yngve S. Kristiansen",
            "Laura Garrison",
            "Stefan Bruckner"
        ],
        "title": "Semantic Snapping for Guided Multi-View Visualization Design",
        "session_id": "v-full-full2",
        "abstract": "Visual information displays are typically composed of multiple visualizations that are used to facilitate an understanding of the underlying data. A common example are dashboards, which are frequently used in domains such as finance, process monitoring and business intelligence. However, users may not be aware of existing guidelines and lack expert design knowledge when composing such multi-view visualizations. In this paper, we present semantic snapping, an approach to help non-expert users design effective multi-view visualizations from sets of pre-existing views. When a particular view is placed on a canvas, it is \"aligned'' with the remaining views--not with respect to its geometric layout, but based on aspects of the visual encoding itself, such as how data dimensions are mapped to channels. Our method uses an on-the-fly procedure to detect and suggest resolutions for conflicting, misleading, or ambiguous designs, as well as to provide suggestions for alternative presentations. With this approach, users can be guided to avoid common pitfalls encountered when composing visualizations. Our provided examples and case studies demonstrate the usefulness and validity of our approach.",
        "keywords": [],
        "uid": "v-full-1349",
        "time_stamp": "2021-10-27T15:00:00Z",
        "has_image": true,
        "paper_award": "",
        "image_caption": "Conceptual figure showing the semantic space with relations and\noperations for our semantic model. Operation 4 displays the homogenize\noperation which is available as a result of a multiples relation (the two\naxis scales are different, but should be the same if the underlying data\nrepresents the same quantity).",
        "external_paper_link": "",
        "has_pdf": true,
        "ff_link": "https://youtu.be/YZ95jlPuym8"
    },
    "v-full-1293": {
        "authors": [
            "Maoyuan Sun",
            "Abdul Rahman Shaikh",
            "Hamed Alhoori",
            "Jian Zhao"
        ],
        "title": "SightBi: Exploring Cross-View Data Relationships with Biclusters",
        "session_id": "v-full-full2",
        "abstract": "Multiple-view visualization (MV) has been heavily used in visual analysis tools for sensemaking of data in various domains (e.g., bioinformatics, cybersecurity and text analytics). One common task of visual analysis with multiple views is to relate data across different views. For example, to identify threats, an intelligence analyst needs to link people from a social network graph with locations on a crime-map, and then search and read relevant documents. Currently, exploring cross-view data relationships heavily relies on view-coordination techniques (e.g., brushing and linking). They may require significant user effort on many trial-and-error attempts, such as repetitiously selecting elements in one view, observing and following elements highlighted in other views. To address this, we present SightBi, a visual analytics approach for supporting cross-view data relationship explorations. We discuss the design rationale of SightBi in detail, with identified user tasks regarding the usage of cross-view data relationships. SightBi formalize cross-view data relationships as biclusters and compute them from a dataset. SightBi uses a bi-context design that highlights creating stand-alone relationship-views. This helps to preserve existing views and serves as an overview of cross-view data relationships to guide user explorations. Moreover, SightBi allows users to interactively manage the layout of multiple views by using newly created relationship-views. With a usage scenario, we demonstrate the usefulness of SightBi for sensemaking of cross-view data relationships.",
        "keywords": [],
        "uid": "v-full-1293",
        "time_stamp": "2021-10-27T15:15:00Z",
        "has_image": true,
        "paper_award": "honorable",
        "image_caption": "",
        "external_paper_link": "",
        "has_pdf": true,
        "ff_link": "https://youtu.be/yF3sUH1gQBQ"
    },
    "v-full-1462": {
        "authors": [
            "Jiansu Pu",
            "Hui Shao",
            "Boyang Gao",
            "Zhengguo Zhu",
            "Yanlin Zhu",
            "Yunbo Rao",
            "Yong Xiang"
        ],
        "title": "matExplorer: Visual Exploration on Predicting Ionic Conductivity for Solid-State Electrolytes",
        "session_id": "v-full-full2",
        "abstract": "Lithium ion batteries (LIBs) are widely used as important energy sources for mobile phones, electric vehicles, and drones. Experts have attempted to replace liquid electrolytes with solid electrolytes that have wider electrochemical window and higher stability due to the potential safety risks, such as electrolyte leakage, flammable solvents, poor thermal stability, and many side reactions caused by liquid electrolytes. However, finding suitable alternative materials using traditional approaches is very difficult due to the incredibly high cost in searching. Machine learning (ML)-based methods are currently introduced and used for material prediction. However, learning tools designed for domain experts to conduct intuitive performance comparison and analysis of ML models are rare. In this case, we propose an interactive visualization system for experts to select suitable ML models and understand and explore the predication results comprehensively. Our system uses a multifaceted visualization scheme designed to support analysis from various perspectives, such as feature distribution, data similarity, model performance, and result presentation. Case studies with actual lab experiments have been conducted by the experts, and the final results confirmed the effectiveness and helpfulness of our system.",
        "keywords": [],
        "uid": "v-full-1462",
        "time_stamp": "2021-10-27T15:30:00Z",
        "has_image": true,
        "paper_award": "",
        "image_caption": "",
        "external_paper_link": "",
        "has_pdf": true,
        "ff_link": "https://youtu.be/HHZZYyepSsY"
    },
    "v-tvcg-9200781": {
        "authors": [
            "Naoko Sawada",
            "Makoto Uemura",
            "Johanna Beyer",
            "Hanspeter Pfister",
            "Issei Fujishiro"
        ],
        "title": "TimeTubesX: A Query-Driven Visual Exploration of Observable, Photometric, and Polarimetric Behaviors of Blazars",
        "session_id": "v-full-full2",
        "abstract": "Blazars are celestial bodies of high interest to astronomers. In particular, through the analysis of photometric and polarimetric observations of blazars, astronomers aim to understand the physics of the blazar's relativistic jet. However, it is challenging to recognize correlations and time variations of the observed polarization, intensity, and color of the emitted light. In our prior study, we proposed TimeTubes to visualize a blazar dataset as a 3D volumetric tube. In this paper, we build primarily on the TimeTubes representation of blazar datasets to present a new visual analytics environment named TimeTubesX, into which we have integrated sophisticated feature and pattern detection techniques for effective location of observable and recurring time variation patterns in long-term, multi-dimensional datasets. Automatic feature extraction detects time intervals corresponding to well-known blazar behaviors. Dynamic visual querying allows users to search long-term observations for time intervals similar to a time interval of interest (query-by-example) or a sketch of temporal patterns (query-by-sketch). Users are also allowed to build up another visual query guided by the time interval of interest found in the previous process and refine the results. We demonstrate how TimeTubesX has been used successfully by domain experts for the detailed analysis of blazar datasets and report on the results.",
        "keywords": [
            "Visual analytics",
            "feature extraction",
            "visual query",
            "multi-dimensional",
            "time-dependent visualization",
            "astrophysics",
            "blazar"
        ],
        "uid": "v-tvcg-9200781",
        "time_stamp": "2021-10-27T15:45:00Z",
        "has_image": true,
        "paper_award": "",
        "image_caption": "We have developed a visual analytics environment for multi-dimensional time-dependent observation datasets for blazars, named TimeTubesX. Two feature and pattern detection techniques, i.e., automatic feature extraction and dynamic visual querying, are integrated for the effective location of observable and recurring time variation patterns in long-term datasets. Users can visually explore the detected results through various federated views. The fact-guided querying is equipped to iteratively refine queries and results by specifying a query based on the result of a previous query.\n",
        "external_paper_link": "https://doi.org/10.1109/TVCG.2020.3025090",
        "has_pdf": false,
        "ff_link": "https://youtu.be/rKoOj07aWdM"
    },
    "v-full-1681": {
        "authors": [
            "Aniketh Venkat",
            "Attila Gyulassy",
            "Graham Kosiba",
            "Amitesh Maiti",
            "Henry Reinstein",
            "Richard Gee",
            "Peer-Timo Bremer",
            "Valerio Pascucci"
        ],
        "title": "Towards replacing physical testing of granular materials with a Topology-based Model",
        "session_id": "v-full-full2",
        "abstract": "In the study of packed granular materials, the performance of a sample (e.g., the detonation of a high-energy explosive) often correlates to measurements of a fluid flowing through it. The \u201ceffective surface area,\u201d the surface area accessible to the airflow, is typically measured using a permeametry apparatus that relates the flow conductance to the permeable surface area via the Carman-Kozeny equation. This equation allows calculating the flow rate of a fluid flowing through the granules packed in the sample for a given pressure drop. However, Carman-Kozeny makes inherent assumptions about tunnel shapes and flow paths that may not accurately hold in situations where the particles possess a wide distribution in shapes, sizes, and aspect ratios, as is true with many powdered systems of technological and commercial interest. To address this challenge, we replicate these measurements virtually on micro-CT images of the powdered material, introducing a new Pore Network Model (PNM) based on the skeleton of the Morse-Smale complex. Pores are identified as basins of the complex, their incidence encodes adjacency, and the conductivity of the capillary between them computed from the cross-section at their interface. We build and solve a resistive network to compute an approximate laminar fluid flow through the pore structure. We provide two means of estimating flow-permeable surface area: (i) by direct computation of conductivity, and (ii) by identifying dead-ends in the flow coupled with isosurface extraction and the application of the Carman-Kozeny equation, with the aim of establishing consistency over a range of particle shapes, sizes, porosity levels, and void distribution patterns.",
        "keywords": [],
        "uid": "v-full-1681",
        "time_stamp": "2021-10-27T16:00:00Z",
        "has_image": true,
        "paper_award": "",
        "image_caption": "We derive a pore network model from topological decomposition and connectivity to estimate the flow properties through a packed powder bed imaged by micro-CT. From left to right: Regions are selected from the raw micro-CT images, and the pore network model is computed; The fluid flow is solved by converting it to a resistive network, which allows the analysis of flow paths, flow through pores, and flow-permeable surface area, all of which correlate to the performance characteristics of porous solids.",
        "external_paper_link": "",
        "has_pdf": true,
        "ff_link": ""
    },
    "v-tvcg-9324971": {
        "authors": [
            "Robin Sk\u00e5nberg",
            "Martin Falk",
            "Mathieu Linares",
            "Anders Ynnerman",
            "Ingrid Hotz"
        ],
        "title": "Tracking Internal Frames of Reference for Consistent Molecular Distribution Functions",
        "session_id": "v-full-full2",
        "abstract": "In molecular analysis, Spatial Distribution Functions (SDF) are fundamental instruments in answering questions related to spatial occurrences and relations of atomic structures over time. Given a molecular trajectory, SDFs can, for example, reveal the occurrence of water in relation to particular structures and hence provide clues of hydrophobic and hydrophilic regions. For the computation of meaningful distribution functions, the definition of molecular reference structures is essential. Therefore we introduce the concept of an internal frame of reference (IFR) for labeled point sets that represent selected molecular structures, and we propose an algorithm for tracking the IFR over time and space using a variant of Kabsch\u2019s algorithm. This approach lets us generate a consistent space for the aggregation of the SDF for molecular trajectories and molecular ensembles. We demonstrate the usefulness of the technique by applying it to temporal molecular trajectories as well as ensemble datasets. The examples include different docking scenarios with DNA, insulin, and aspirin.",
        "keywords": [
            "Molecule Visualization",
            "Molecular Dynamics",
            "Interactive Exploration"
        ],
        "uid": "v-tvcg-9324971",
        "time_stamp": "2021-10-27T16:15:00Z",
        "has_image": true,
        "paper_award": "",
        "image_caption": "",
        "external_paper_link": "https://doi.org/10.1109/TVCG.2021.3051632",
        "has_pdf": false,
        "ff_link": "https://youtu.be/rtv5qHcD9c0"
    },
    "v-short-1013": {
        "authors": [
            "Kalina Borkiewicz",
            "Viraj Shah",
            "J.P. Naiman",
            "Chuanyue Shen",
            "Stuart Levy",
            "Jeffrey D Carpenter"
        ],
        "title": "CloudFindr: A Deep Learning Cloud Artifact Masker for Satellite DEM Data",
        "session_id": "v-short-short1",
        "abstract": "Artifact removal is an integral component of cinematic scientific visualization, and is especially challenging with big datasets in which artifacts are difficult to define. In this paper, we describe a method for creating cloud artifact masks which can be used to remove artifacts from satellite imagery using a combination of traditional image processing together with deep learning based on U-Net. Compared to previous methods, our approach does not require multi-channel spectral imagery but performs successfully on single-channel Digital Elevation Models (DEMs). DEMs are a representation of the topography of the Earth and have a variety applications including planetary science, geology, flood modeling, and city planning.",
        "keywords": [
            "Data Management, Processing, Wrangling",
            "Feature Detection, Extraction, Tracking & Transformation",
            "Machine Learning Techniques",
            "Cartography, Maps",
            "Image and Signal Processing",
            "General Public",
            "Application Motivated Visualization",
            "Geospatial Data",
            "Image and Video Data"
        ],
        "uid": "v-short-1013",
        "time_stamp": "2021-10-27T15:00:00Z",
        "has_image": true,
        "paper_award": "",
        "image_caption": "When creating a cinematic scientific visualization for documentary films or museums, visualizations must be not only accurate, but also understandable to audiences of all ages and aesthetically pleasing. Data artifacts are therefore undesirable in these situations. CloudFindr is a method for masking out cloud artifacts in digital elevation model (DEM) data gathered by satellites, which would otherwise create unrealistic and distracting spikes on visualized landscapes. CloudFindr performs image segmentation using a U-Net based machine learning model on GLCM pre-processed data run with different parameters and ensemble voting for final mask results.\n",
        "external_paper_link": "",
        "has_pdf": true,
        "ff_link": "https://youtu.be/-Xox4RsgoVw"
    },
    "v-short-1016": {
        "authors": [
            "Jun Yuan",
            "Oded Nov",
            "Enrico Bertini"
        ],
        "title": "An Exploration And Validation of Visual Factors in Understanding Classification Rule Sets",
        "session_id": "v-short-short1",
        "abstract": "Rule sets are often used in Machine Learning (ML) as a way to communicate the model logic in settings where transparency and intelligibility are necessary. Rule sets are typically presented as a text-based list of logical statements (rules). Surprisingly, to date there has been limited work on exploring visual alternatives for presenting rules. In this paper, we explore the idea of designing alternative representations of rules, focusing on a number of visual factors we believe have a positive impact on rule readability and understanding. We then presents a user study exploring their impact. The results show that some design factors have a strong impact on how efficiently readers can process the rules while having minimal impact on accuracy. This work can help practitioners employ more effective solutions when using rules as a communication strategy to understand ML models.",
        "keywords": [
            "Machine Learning, Statistics, Modelling, and Simulation Applications",
            "Visual Representation Design",
            "Human-Subjects Quantitative Studies",
            "Tabular Data"
        ],
        "uid": "v-short-1016",
        "time_stamp": "2021-10-27T15:10:00Z",
        "has_image": true,
        "paper_award": "",
        "image_caption": "We identified a set of visual factors that have a positive influence on rule understanding: (a) Feature Alignment, (b) Feature Ordering, (c) Rule Ordering, (d) Predicate Encoding.",
        "external_paper_link": "",
        "has_pdf": true,
        "ff_link": ""
    },
    "v-short-1024": {
        "authors": [
            "Jeffrey Heer"
        ],
        "title": "Fast & Accurate Gaussian Kernel Density Estimation",
        "session_id": "v-short-short1",
        "abstract": "Kernel density estimation (KDE) models a discrete sample of data as a continuous distribution, supporting the construction of visualizations such as violin plots, heatmaps, and contour plots. This paper draws on the statistics and image processing literature to survey efficient and scalable density estimation techniques for the common case of Gaussian kernel functions. We evaluate the accuracy and running time of these methods across multiple visualization contexts and find that the combination of linear binning and a recursive filter approximation by Deriche efficiently produces pixel-perfect estimates across a compelling range of kernel bandwidths.",
        "keywords": [
            "Data Clustering and Aggregation",
            "Large-Scale Data Techniques",
            "Domain Agnostic",
            "Charts, Diagrams, and Plots",
            "Image and Signal Processing",
            "Uncertainty Visualization",
            "Algorithms",
            "Computational Benchmark Studies",
            "Data Type Agnostic"
        ],
        "uid": "v-short-1024",
        "time_stamp": "2021-10-27T15:20:00Z",
        "has_image": true,
        "paper_award": "",
        "image_caption": "Heatmap with contour lines from three different density estimation methods. Deriche\u2019s approximation matches the ground truth, while box filter methods can result in missing contour lines and inaccurate shapes.",
        "external_paper_link": "",
        "has_pdf": true,
        "ff_link": ""
    },
    "v-short-1065": {
        "authors": [
            "Delyar Tabatabai",
            "Anita Ruangrotsakun",
            "Jed Irvine",
            "Jonathan Dodge",
            "Zeyad Shureih",
            "Kin-Ho Lam",
            "Margaret Burnett",
            "Alan Paul Fern",
            "Minsuk Kahng"
        ],
        "title": "Why did my AI Agent Lose? Visual Analytics for Scaling Up After-Action Review",
        "session_id": "v-short-short1",
        "abstract": "How can we help domain-knowledgeable users who do not have expertise in AI analyze why an AI agent failed? Our research team previously developed a new structured process for such users to assess AI, called After-Action Review for AI (AAR/AI), consisting of a series of steps a human takes to assess an AI agent and formalize their understanding. In this paper, we investigate how the AAR/AI process can scale up to support reinforcement learning (RL) agents that operate in complex environments. We augment the AAR/AI process to be performed at three levels--episode-level, decision-level, and explanation-level--and integrate it into our redesigned visual analytics interface. We illustrate our approach through a usage scenario of analyzing why a RL agent lost in a complex real-time strategy game built with the StarCraft 2 engine. We believe integrating structured processes like AAR/AI into visualization tools can help visualization play a more critical role in AI interpretability.",
        "keywords": [
            "Machine Learning, Statistics, Modelling, and Simulation Applications",
            "Data Analysis, Reasoning, Problem Solving, and Decision Making",
            "Process/Workflow Design"
        ],
        "uid": "v-short-1065",
        "time_stamp": "2021-10-27T15:30:00Z",
        "has_image": true,
        "paper_award": "",
        "image_caption": "Our visual analytics tool that integrates the After-Action Review for AI (AAR/AI) process allows domain experts to analyze complex AI agents by following a series of steps.",
        "external_paper_link": "",
        "has_pdf": true,
        "ff_link": "https://youtu.be/KcD7MuFJT9g"
    },
    "v-short-1111": {
        "authors": [
            "Grace Guo",
            "Maria Glenski",
            "Zhuanyi Huang",
            "Emily Saldanha",
            "Alex Endert",
            "Svitlana Volkova",
            "Dustin L Arendt"
        ],
        "title": "VAINE: Visualization and AI for Natural Experiments",
        "session_id": "v-short-short1",
        "abstract": "Natural experiments are observational studies where the assignment of treatment conditions to different populations occur by chance ``in the wild''. Researchers from fields such as economics, healthcare, and the social sciences leverage natural experiments to conduct hypothesis testing and causal effect estimation for treatment and outcome variables that would otherwise be costly, infeasible, or unethical. In this paper, we introduce VAINE (Visualization and AI for Natural Experiments), a visual analytics tool for identifying and understanding natural experiments from observational data. We then demonstrate how VAINE can be used to validate causal relationships, estimate average treatment effects, and identify statistical phenomena such as Simpson\u2019s paradox through two use cases.",
        "keywords": [
            "Domain Agnostic",
            "Data Analysis, Reasoning, Problem Solving, and Decision Making",
            "Mixed Initiative Human-Machine Analysis",
            "Software Prototype"
        ],
        "uid": "v-short-1111",
        "time_stamp": "2021-10-27T15:40:00Z",
        "has_image": true,
        "paper_award": "",
        "image_caption": "VAINE (Visualization and AI for Natural Experiments) is a visual analytics system that enables users to identify natural experiments in observational data and estimate the impact of treatment variables on outcomes. The visual representations in VAINE allow domain experts to explore potential causalities and contextualize results by adjusting clusters, identifying outliers, and inspecting covariates. VAINE is a domain-agnostic tool implemented as a python package and widget for the popular Jupyter notebook computational environment.\n",
        "external_paper_link": "",
        "has_pdf": true,
        "ff_link": ""
    },
    "v-short-1161": {
        "authors": [
            "Yali Bian",
            "Chris North",
            "Eric Krokos",
            "Sarah Joseph"
        ],
        "title": "Semantic Explanation of Interactive Dimensionality Reduction",
        "session_id": "v-short-short1",
        "abstract": "Interactive dimensionality reduction helps analysts explore the high-dimensional data based on their personal needs and domain-specific problems. \nRecently, expressive nonlinear models are employed to support these tasks.\nHowever, the interpretation of these human-steered nonlinear models during human-in-the-loop analysis has not been explored.\nTo address this problem, we present a new visual explanation design called semantic explanation. \nSemantic explanation visualizes model behaviors in a manner that is similar to users' direct projection manipulations.\nThis design conforms to the spatial analytic process and enables analysts better understand the updated model in response to their interactions.\nWe propose a pipeline to empower interactive dimensionality reduction with semantic explanation using counterfactuals. \nBased on the pipeline, we implement a visual text analytics system with nonlinear dimensionality reduction powered by deep learning via the BERT model.\nWe demonstrate the efficacy of semantic explanation with two case studies of academic article exploration and intelligence analysis.",
        "keywords": [
            "Dimensionality Reduction",
            "Machine Learning Techniques",
            "Data Analysis, Reasoning, Problem Solving, and Decision Making",
            "Visual Representation Design",
            "High-dimensional Data",
            "Text/Document Data"
        ],
        "uid": "v-short-1161",
        "time_stamp": "2021-10-27T15:50:00Z",
        "has_image": true,
        "paper_award": "",
        "image_caption": "The interpretation of human-steered nonlinear models during human-in-the-loop analysis has not been explored.  To address this problem, we present a new visual explanation design called semantic explanation. Semantic explanation visualizes model behaviors in a manner that is similar to users\u2019 direct projection manipulations. This design conforms to the spatial analytic process and enables analysts better understand the updated model in response to their interactions. ",
        "external_paper_link": "",
        "has_pdf": true,
        "ff_link": ""
    },
    "v-short-1178": {
        "authors": [
            "Steffen Holter",
            "Oscar Alejandro Gomez",
            "Jun Yuan",
            "Enrico Bertini"
        ],
        "title": "AdViCE: Aggregated Visual Counterfactual Explanations for Machine Learning Model Validation",
        "session_id": "v-short-short1",
        "abstract": "Rapid improvements in the performance of machine learning models has pushed them to the forefront of data-driven decision-making. Meanwhile, the increased integration of these models into various application domains has further highlighted the need for greater interpretability and transparency. To identify problems such as bias, overfitting, and incorrect correlations, data scientists require tools that explain the mechanisms with which these model decisions are made. In this paper we introduce AdViCE, a visual analytics tool that aims to guide users in black-box model debugging and validation. The solution rests on two main visual user interface innovations: (1) an interactive visualization design that enables the comparison of decisions on user-defined data subsets; (2) an algorithm and visual design to compute and visualize counterfactual explanations - explanations that depict model outcomes when data features are perturbed from their original values. Furthermore, we provide an evaluation of the work through a number of use cases that demonstrate the capabilities and potential limitations of the proposed approach.",
        "keywords": [
            "Domain Agnostic",
            "Charts, Diagrams, and Plots",
            "Coordinated and Multiple Views",
            "Other Topics and Techniques",
            "Data Analysis, Reasoning, Problem Solving, and Decision Making",
            "Algorithms",
            "Interaction Design",
            "Process/Workflow Design",
            "Visual Representation Design",
            "Tabular Data"
        ],
        "uid": "v-short-1178",
        "time_stamp": "2021-10-27T16:00:00Z",
        "has_image": true,
        "paper_award": "",
        "image_caption": "Complete visual interface for AdViCE. (1) main visualization, (2) filtering section, (3) model prediction percentage, (4) confusion matrix, (5) feature range selector, (6) feature sub-column, (7) information toggles,  (8) median value, (9) histogram bin, (10) set of counterfactual explanations, (11) sorting function.",
        "external_paper_link": "",
        "has_pdf": true,
        "ff_link": ""
    },
    "v-short-1050": {
        "authors": [
            "Matthew Olson",
            "Thuy-Vy Nguyen",
            "Gaurav Dixit",
            "Neale Ratzlaff",
            "Weng-Keen Wong",
            "Minsuk Kahng"
        ],
        "title": "Contrastive Identification of Covariate Shift in Image Data",
        "session_id": "v-short-short1",
        "abstract": "Identifying covariate shift is crucial to making machine learning systems robust in the real world and for detecting training data biases that are not reflected in test data. However, detecting covariate shift is challenging, especially when the data is high-dimensional images, and when multiple types of localized covariate shift affect different subspaces of the data. Although automated techniques can be used to detect the existence of covariate shift, our goal is to help human users characterize the extent of covariate shift in large image datasets with visual interfaces that seamlessly integrate information obtained from the detection algorithms. In this paper, we design and evaluate a new visual analytics approach that facilitates the comparison of the local distributions of training and test data. We conduct a quantitative user study on multi-attribute facial data to compare two different learned low-dimensional latent representations (pretrained ImageNet CNN vs. density ratio) and two user analytic workflows (nearest-neighbor vs. cluster-to-cluster). Our results indicate that the latent representation of our density ratio model, combined with a nearest-neighbor comparison, is the most effective at helping humans identify covariate shift.",
        "keywords": [
            "Machine Learning Techniques",
            "Machine Learning, Statistics, Modelling, and Simulation Applications",
            "Comparison and Similarity",
            "Image and Video Data"
        ],
        "uid": "v-short-1050",
        "time_stamp": "2021-10-27T16:10:00Z",
        "has_image": true,
        "paper_award": "",
        "image_caption": "The figure shows a screenshot of our new side-by-side histogram view for comparing two sets of images in a selected local region around the selected image of interest and characterizing shifts. \nThe interface facilitates a user's ability to compare and contrast against a focal image in order to characterize covariate shift between train and test data.",
        "external_paper_link": "",
        "has_pdf": true,
        "ff_link": ""
    },
    "v-cga-9198117": {
        "authors": [
            "Dietmar Offenhuber"
        ],
        "title": "What we talk about when we talk about data physicality",
        "session_id": "v-cga-cga1",
        "abstract": "Data physicalizations \u201cmap data to physical form,\u201d yet many canonical examples are not based on external data sets. To address this contradiction, I argue that the practice of physicalization forces us to rethink traditional notions of data. This article proposes a conceptual framework to examine how physicalizations relate to data. This article develops a two-dimensional conceptual space for comparing different perspectives on data used in physicalization, drawing from design theory and critical data studies literature. One axis distinguishes between epistemological and ontological perspectives, focusing on the relationship between data and the mind. The second axis distinguishes how data relate to the world, differentiating between representational and relational perspectives. To clarify the aesthetic and conceptual implications of these different perspectives, the article discusses examples of data physicalization for each quadrant of the continuous space. It further uses the framework to examine the explicit and implicit assumptions about data in physicalization literature. As a theoretical article, it encourages practitioners to think about how data relate to the manifestations and the phenomena they try to capture. It invites exploration of the relationship between data and the world as a generative source of creative tension.",
        "keywords": [],
        "uid": "v-cga-9198117",
        "time_stamp": "2021-10-27T15:00:00Z",
        "has_image": true,
        "paper_award": "",
        "image_caption": "Historical measure of length (one cubit) on the wall of the cathedral in Freiburg, Germany. Photo: Birgit Rucker. This theoretical paper encourages practitioners to think about how data relate to the manifestations and the phenomena they try to capture. It invites exploration of the relationship between data and the world as a generative source of creative tension.",
        "external_paper_link": "https://dx.doi.org/10.1109/MCG.2020.3024146",
        "has_pdf": false,
        "ff_link": "https://youtu.be/B-Vy8euUdLI"
    },
    "v-cga-9201309": {
        "authors": [
            "Georgia Panagiotidou",
            "Sinem G\u00f6r\u00fcc\u00fc",
            "Andrew Vande Moere"
        ],
        "title": "Data Badges: Making an Academic Profile through a DIY wearable physicalisation",
        "session_id": "v-cga-cga1",
        "abstract": "In this pictorial, we present the design and making process of Data Badges as they were deployed during a one-week academic seminar. Data Badges are customizable physical conference badges that invite participants to make their own independent and personalized expressions of their academic profile by choosing and assembling a collection of predefined physical tokens on a flat wearable canvas. As our modular and intuitive design approach allows the construction to occur as a shared, collective activity, Data Badges take advantage of the creative, affective, and social values that underlie physicalization and its construction to engage participants in reflecting on personal data. Among other unexpected phenomena, we noticed how the freedom of assembly and interpretation encouraged a variety of appropriations, which expanded its intended representational space from fully representative to more resistive and provocative forms of data expression.",
        "keywords": [],
        "uid": "v-cga-9201309",
        "time_stamp": "2021-10-27T15:15:00Z",
        "has_image": true,
        "paper_award": "",
        "image_caption": "Data Badges are customisable wearable physicalisations that express participant academic profiles in professional events. The canvas and encoded tokens are designed to be modular, interlocking and simple to attach so as to be assembled directly by participants on site using an instruction sheet. The DIY assembling of the Data Badges, allowed for a unique ice-breaker and permitted the appropriation of the physical tokens in personally comfortable and creative ways. The image depicts a Data Badge on a backdrop of its instruction sheet.",
        "external_paper_link": "https://dx.doi.org/10.1109/MCG.2020.3025504",
        "has_pdf": false,
        "ff_link": "https://youtu.be/ZFtZ4mj63TA"
    },
    "v-cga-9201404": {
        "authors": [
            "J\u00f6rn Hurtienne",
            "Franzisca Maas",
            "Astrid Carolus",
            "Daniel Reinhardt",
            "Cordula Baur",
            "Carolin Wienrich"
        ],
        "title": "Move&Find: The value of kinesthetic experience in a casual data representation",
        "session_id": "v-cga-cga1",
        "abstract": "The value of a data representation is traditionally judged based on aspects like effectiveness and efficiency that are important in utilitarian or work-related contexts. Most multisensory data representations, however, are employed in casual contexts where creativity, affective, physical, intellectual, and social engagement might be of greater value. We introduce Move&Find, a multisensory data representation in which people pedalled on a bicycle to exert the energy required to power a search query on Google's servers. To evaluate Move&Find, we operationalized a framework suitable to evaluate the value of data representations in casual contexts and experimentally compared Move&Find to a corresponding visualization. With Move&Find, participants achieved a higher understanding of the data. Move&Find was judged to be more creative and encouraged more physical and social engagement-components of value that would have been missed using more traditional evaluation frameworks.",
        "keywords": [],
        "uid": "v-cga-9201404",
        "time_stamp": "2021-10-27T15:30:00Z",
        "has_image": true,
        "paper_award": "",
        "image_caption": "We introduce Move&Find, a multisensory data representation in which people pedalled on a bicycle to exert the energy required to power a search query on Google\u2019s servers. To evaluate Move&Find we operationalized a framework suitable to evaluate the value of data representations in casual contexts. We experimentally compared Move&Find to a corresponding visualisation. With Move&Find, participants achieved a higher understanding of the data. Move&Find was judged to be more creative and encouraged more physical and social engagement \u2013 components of value that would have been missed using more traditional evaluation frameworks.",
        "external_paper_link": "https://dx.doi.org/10.1109/MCG.2020.3025385",
        "has_pdf": false,
        "ff_link": "https://youtu.be/QpPU2ceuimg"
    },
    "v-cga-9200769": {
        "authors": [
            "Doris Kosminsky",
            "Douglas Thomaz de Oliveira"
        ],
        "title": "Slave Voyages:reflections on data sculptures",
        "session_id": "v-cga-cga1",
        "abstract": "This pictorial presents the development of a data sculpture, followed by our reflections inspired by Research through Design (RtD) and Dahlstedt's process-based model of artistic creativity. We use the notion of negotiation between concept and material representation to reflect on the ideation, design process, production, and the exhibition of \u201cSlave Voyages\u201d - a set of data sculptures that depicts slave traffic from Africa to the American continent. The work was initially produced as an assignment on physicalization for the Design course at the Federal University of Rio de Janeiro. Our aim is to open discussion on material representation and negotiation in the creative process of data physicalization.",
        "keywords": [],
        "uid": "v-cga-9200769",
        "time_stamp": "2021-10-27T15:45:00Z",
        "has_image": true,
        "paper_award": "",
        "image_caption": "Image of the exhibition of Slave Voyage, a set of data sculptures that depicts slave traffic from Africa to the American continent. Each bead represents 10000 people.\n",
        "external_paper_link": "https://dx.doi.org/10.1109/MCG.2020.3025183",
        "has_pdf": false,
        "ff_link": "https://youtu.be/NXN1N0Fq5AA"
    },
    "v-cga-9200790": {
        "authors": [
            "Maria Karyda",
            "Danielle Wilde",
            "Mette Gislev Kj\u00e6rsgaard"
        ],
        "title": "Narrative Physicalisation: Supporting Interactive Engagement with Personal Data",
        "session_id": "v-cga-cga1",
        "abstract": "Physical engagement with data necessarily influences the reflective process. However, the role of interactivity and narration are often overlooked when designing and analyzing personal data physicalizations. We introduce Narrative Physicalizations, everyday objects modified to support nuanced self-reflection through embodied engagement with personal data. Narrative physicalizations borrow from narrative visualizations, storytelling with graphs, and engagement with mundane artifacts from data-objects. Our research uses a participatory approach to research-through-design and includes two interdependent studies. In the first, personalized data physicalizations are developed for three individuals. In the second, we conduct a parallel autobiographical exploration of what constitutes personal data when using a Fitbit. Our work expands the landscape of data physicalization by introducing narrative physicalizations. It suggests an experience-centric view on data physicalization where people engage physically with their data in playful ways, making their body an active agent during the reflective process.",
        "keywords": [],
        "uid": "v-cga-9200790",
        "time_stamp": "2021-10-27T16:00:00Z",
        "has_image": true,
        "paper_award": "",
        "image_caption": "The one-string instrument is an interactive narrative physicalisation which can afford a double interaction with personal data facilitating nuanced self-reflections. ",
        "external_paper_link": "https://dx.doi.org/10.1109/MCG.2020.3025078",
        "has_pdf": false,
        "ff_link": "https://youtu.be/GfuVh5hknRw"
    },
    "v-cga-9201339": {
        "authors": [
            "Laura J. Perovich",
            "Phoebe Cai",
            "Amber Guo",
            "Kristin Zimmerman",
            "Katherine Paseman",
            "Dayanna Espinoza Silva",
            "Julia G. Brody"
        ],
        "title": "Data Clothing and BigBarChart: designing physical data reports on indoor pollutants for individuals and communities",
        "session_id": "v-cga-cga1",
        "abstract": "In response to participant preferences and new ethics guidelines, researchers are increasingly sharing data with health study participants, including data on their own household chemical exposures. Data physicalization may be a useful tool for these communications, because it is thought to be accessible to a general audience and emotionally engaged. However, there are limited studies of data physicalization in the wild with diverse communities. Our application of this method in the Green Housing Study is an early example of using data physicalization in environmental health report-back. We gathered feedback through community meetings, prototype testing, and semistructured interviews, leading to the development of data t-shirts and other garments and person-sized bar charts. We found that participants were enthusiastic about data physicalizations, it connected them to their previous experience, and they had varying desires to share their data. Our findings suggest that researchers can enhance environmental communications by further developing the human experience of physicalizations and engaging diverse communities.",
        "keywords": [],
        "uid": "v-cga-9201339",
        "time_stamp": "2021-10-27T16:15:00Z",
        "has_image": true,
        "paper_award": "",
        "image_caption": "We develop and test three data physicalizations--Dressed in Data, Data Shirts, and BigBarChart--for sharing results with participants in environmental health studies. Dressed in Data, above, takes an artist approach to displaying levels of environmental pollutants using a textile lace pattern. This project surfaces opportunities for researchers to understand how data physicalization may be useful to environmental health applications and to learn how diverse communities perceive these emerging approaches in information design.\n",
        "external_paper_link": "https://dx.doi.org/10.1109/MCG.2020.3025322",
        "has_pdf": false,
        "ff_link": "https://youtu.be/ryuald1z828"
    },
    "a-vizsec-7354": {
        "authors": [
            "Awalin Sopan",
            "Konstantin Berlin"
        ],
        "title": "AI Total: A Visualization Tool for Making Sense of Security ML Models in an Imperfect World of Production Data",
        "session_id": "a-vizsec-vizsec-2",
        "abstract": "The metrics measured while developing machine learning models are not enough to evaluate the models\u2019 performance in the operational level, especially ML models for cyber security with ever changing new attack vectors. Usually, it is also hard to understand initially if the fundamental problem is in the model performance or if there are data issues that are causing problems in the evaluation. With this in mind, we developed a visualization system that would allow the users to quickly identify and diagnose issues with current model deployment, from model performance to data issues that prevent accurate evaluation of the model. Our application enables our security data science team to have a situational awareness of the system and quickly investigate any problems. While designing our system, we considered all the common issues we see in production. In this paper, we will describe this application, its regular usage, and some of the special example cases when it was proved valuable for introspecting our models.",
        "keywords": [],
        "uid": "a-vizsec-7354",
        "time_stamp": "2021-10-27T15:05:00Z",
        "has_image": true,
        "paper_award": "",
        "image_caption": "AI Total Landing page showing model performance\n",
        "external_paper_link": "",
        "has_pdf": false,
        "ff_link": "https://youtu.be/_piS1Ov3bU4"
    },
    "a-vizsec-4000": {
        "authors": [
            "Marco Angelini",
            "Graziano Blasilli",
            "Silvia Bonomi",
            "Simone Lenti",
            "Alessia Palleschi",
            "Giuseppe Santucci",
            "Emiliano De Paoli"
        ],
        "title": "BUCEPHALUS: a BUsiness CEntric cybersecurity Platform for proActive anaLysis Using visual analyticS",
        "session_id": "a-vizsec-vizsec-2",
        "abstract": "Analyzing and mitigating the threats that cyber-attacks pose on the services of a critical infrastructure is not a trivial activity. Research solutions have been developed using data about the devices used for implementing the services, services dependencies, network topology, and the vulnerabilities that can be exploited to attack the network. However, most of the proposed solutions fail to consider these aspects in an integrated fashion, allowing the user to understand global dependencies and weaknesses.",
        "keywords": [],
        "uid": "a-vizsec-4000",
        "time_stamp": "2021-10-27T15:20:00Z",
        "has_image": true,
        "paper_award": "",
        "image_caption": "BUCEPHALUS: a BUsiness CEntric cybersecurity Platform for proActive anaLysis Using visual analyticS.\nAuthors are: Marco Angelini, Graziano Blasilli, Silvia Bonomi, Simone Lenti\nAlessia Palleschi, Giuseppe Santucci, Emiliano De Paoli.\nFrom Sapienza University of Rome and MBDA Italia.\n\n",
        "external_paper_link": "",
        "has_pdf": true,
        "ff_link": "https://youtu.be/jySezlPOWVo"
    },
    "a-vizsec-8146": {
        "authors": [
            "Mark F. St. John",
            "Grit Denker",
            "Peeter Laud",
            "Karsten Martiny",
            "Alisa Pankova",
            "Dusko Pavlovic"
        ],
        "title": "Decision Support for Sharing Data Using Differential Privacy",
        "session_id": "a-vizsec-vizsec-2",
        "abstract": "Owners of data may wish to share some statistics with others, but they may be worried of privacy of the underlying data. An effective solution to this problem is to employ provable privacy techniques, such as differential privacy, to add noise to the statistics before releasing them. This protection lowers the risk of sharing sensitive data with more or less trusted data sharing partners. Unfortunately, applying differential privacy in its mathematical form requires one to fix certain numeric parameters, which involves subtle computations and expert knowledge that the data owners may lack.",
        "keywords": [],
        "uid": "a-vizsec-8146",
        "time_stamp": "2021-10-27T15:40:00Z",
        "has_image": true,
        "paper_award": "",
        "image_caption": "A screenshot of the proposed differential privacy policy tool. For the given levels of trust, data sensitivity, and the maximum tolerated risk, the tool proposes the minimum recommended noise. The user can explore the graph and e.g. choose a slightly smaller noise that still provides a similar level of risk.\n",
        "external_paper_link": "",
        "has_pdf": true,
        "ff_link": "https://youtu.be/1IBJU1ZK3s8"
    },
    "a-vizsec-2863": {
        "authors": [
            "Azqa Nadeem",
            "Sicco Verwer",
            "Shanchieh Jay Yang"
        ],
        "title": "SAGE: Intrusion Alert-driven Attack Graph Extractor (short paper)",
        "session_id": "a-vizsec-vizsec-2",
        "abstract": "Attack graphs (AG) are used to assess pathways availed by cyber adversaries to penetrate a network. State-of-the-art approaches for AG generation focus mostly on deriving dependencies between system vulnerabilities based on network scans and expert knowledge. In real-world operations however, it is costly and ineffective to rely on constant vulnerability scanning and expert-crafted AGs. We propose to automatically learn AGs based on actions observed through intrusion alerts, without prior expert knowledge. Specifically, we develop an unsupervised sequence learning system, SAGE, that leverages the temporal and probabilistic dependence between alerts in a suffix-based probabilistic deterministic finite automaton(S-PDFA) \u2013 a model that accentuates infrequent severe alerts and summarizes paths leading to them. AGs are then derived from the S-PDFA. Tested with intrusion alerts collected through Collegiate Penetration Testing Competition, SAGE compresses several thousands of alerts into only a handful of AGs. These AGs reflect the strategies used by participating teams. The resulting AGs are succinct, interpretable, and enable analysts to derive actionable insights, e.g., attackers tend to follow shorter paths after they have discovered a longer one.",
        "keywords": [],
        "uid": "a-vizsec-2863",
        "time_stamp": "2021-10-27T16:00:00Z",
        "has_image": true,
        "paper_award": "",
        "image_caption": "An alert-driven attack graph shows attacker strategies extracted from actions observed through intrusion alerts. \nEach graph shows the strategies of all attackers that obtain an objective on a particular victim. \nThe image shows an attack graph of data_manipulation over remoteware-cl, where 3 teams successfully exploit it: \nTeam 5 exploits it twice, while Teams 1 and 8 exploit it once. \nThe S-PDFA identifies three ways of exploiting the objective based on the actions that lead up to it. \nTeams 5 and 8 share a significant portion of a strategy, \nwhile Team 1 has found the shortest path to reach the objective.",
        "external_paper_link": "",
        "has_pdf": true,
        "ff_link": "https://youtu.be/TxUye31Eqjs"
    },
    "a-vizsec-7867": {
        "authors": [
            "Andreas Schreiber",
            "Tim Sonnekalb",
            "Lynn von Kurnatowski"
        ],
        "title": "Towards Visual Analytics Dashboards for Provenance-driven Static Application Security Testing (short paper)",
        "session_id": "a-vizsec-vizsec-2",
        "abstract": "The use of static code analysis tools can be time consuming, as the many existing tools focus on different aspects and therefore development teams often use several of these tools to keep code quality high. Displaying the results of multiple tools, such as code smells and warnings, in a unified interface can help developers get a better overview and prioritize upcoming work. We present visualizations and a dashboard that interactively display results from static code analysis for \u201cinteresting\u201d commits during development. With this, we aim to provide an effective visual analytics tool for code analysis results.",
        "keywords": [],
        "uid": "a-vizsec-7867",
        "time_stamp": "2021-10-27T16:15:00Z",
        "has_image": true,
        "paper_award": "",
        "image_caption": "We show here a screenshot of our Provenance-driven Automated Security Dashboard. It visualizes the content of our provenance database and static analysis database. The icycle graph helps the developer to quickly identfy the most important security issues.",
        "external_paper_link": "",
        "has_pdf": true,
        "ff_link": ""
    },
    "v-tvcg-9444798": {
        "authors": [
            "Lin-Ping Yuan",
            "Ziqi Zhou",
            "Jian Zhao",
            "Yiqiu Guo",
            "Fan Du",
            "Huamin Qu"
        ],
        "title": "InfoColorizer: Interactive Recommendation of Color Palettes for Infographics",
        "session_id": "v-full-full5",
        "abstract": "When designing infographics, general users usually struggle with getting desired color palettes using existing infographic authoring tools, which sometimes sacrifice customizability, require design expertise, or neglect the influence of elements\u2019 spatial arrangement. We propose a data-driven method that provides flexibility by considering users\u2019 preferences, lowers the expertise barrier via automation, and tailors suggested palettes to the spatial layout of elements. We build a recommendation engine by utilizing deep learning techniques to characterize good color design practices from data, and further develop InfoColorizer, a tool that allows users to obtain color palettes for their infographics in an interactive and dynamic manner. To validate our method, we conducted a comprehensive four-part evaluation, including case studies, a controlled user study, a survey study, and an interview study. The results indicate that InfoColorizer can provide compelling palette recommendations with adequate flexibility, allowing users to effectively obtain high-quality color design for input infographics with low effort.",
        "keywords": [
            "Color palettes design",
            "infographics",
            "visualization recommendation",
            "machine learning."
        ],
        "uid": "v-tvcg-9444798",
        "time_stamp": "2021-10-27T17:00:00Z",
        "has_image": true,
        "paper_award": "",
        "image_caption": "We propose a data-driven method that provides flexibility by considering users\u2019 preferences, lowers the expertise barrier via automation, and tailors suggested palettes to the spatial layout of elements. We build a recommendation engine by utilizing deep learning techniques to characterize good color design practices from data, and further develop InfoColorizer, a tool that allows users to obtain color palettes for their infographics in an interactive and dynamic manner.",
        "external_paper_link": "https://dx.doi.org/10.1109/TVCG.2021.3085327",
        "has_pdf": false,
        "ff_link": "https://youtu.be/8m1TdUVlmfQ"
    },
    "v-full-1637": {
        "authors": [
            "Weiwei Cui",
            "Jinpeng Wang",
            "He Huang",
            "Yun Wang",
            "Chin-Yew Lin",
            "Haidong Zhang",
            "Dongmei Zhang"
        ],
        "title": "A Mixed-Initiative Approach to Reusing Infographic Charts",
        "session_id": "v-full-full5",
        "abstract": "Infographic bar charts have been widely adopted for communicating numerical information because of their attractiveness and memorability. However, these infographics are often created manually with general tools, such as PowerPoint and Adobe Illustrator, and merely composed of primitive visual elements, such as text blocks and shapes. With the absence of chart models, updating or reusing these infographics requires tedious and error-prone manual edits. In this paper, we propose a mixed-initiative approach to mitigate this pain point. On one hand, machines are adopted to perform precise and trivial operations, such as mapping numerical values to shape attributes and aligning shapes. On the other hand, we rely on humans to perform subjective and creative tasks, such as changing embellishments or approving the edits made by machines. We encapsulate our technique in a PowerPoint add-in prototype and demonstrate the effectiveness by applying our technique on a diverse set of infographic bar chart examples.",
        "keywords": [],
        "uid": "v-full-1637",
        "time_stamp": "2021-10-27T17:15:00Z",
        "has_image": true,
        "paper_award": "",
        "image_caption": "Existing infographic charts are often merely composed of primitive visual elements, such as text blocks and shapes. With the absence of chart models, updating or reusing them requires tedious and error-prone manual edits. We propose a mixed-initiative approach to mitigate this pain point. With one simple click, our model will analyze the design and extract the chart model, such as the visual structure and data-visual binding. Then users can simply provide new data values and our method will update the chart accordingly.",
        "external_paper_link": "",
        "has_pdf": true,
        "ff_link": ""
    },
    "v-full-1008": {
        "authors": [
            "Shahid Latif",
            "Zheng Zhou",
            "Yoon Kim",
            "Fabian Beck",
            "Nam Wook Kim"
        ],
        "title": "Kori: Interactive Synthesis of Text and Charts in Data Documents",
        "session_id": "v-full-full5",
        "abstract": "Charts go hand in hand with text to communicate complex data and are widely adopted in news articles, online blogs, and academic papers. They provide graphical summaries of the data, while text explains the message and context. However, synthesizing information across text and charts is difficult; it requires readers to frequently shift their attention. We investigated ways to support the tight coupling of text and charts in data documents. To understand their interplay, we analyzed the design space of such references through news articles and scientific papers. Informed by the analysis, we developed a mixed-initiative interface enabling users to construct interactive references between text and charts. It leverages natural language processing to automatically suggest references as well as allows users to manually construct other references effortlessly. A user study complemented with algorithmic evaluation of the system suggests that the interface provides an effective way to compose interactive data documents.",
        "keywords": [],
        "uid": "v-full-1008",
        "time_stamp": "2021-10-27T17:30:00Z",
        "has_image": true,
        "paper_award": "",
        "image_caption": "Kori uses an intelligent mixed-initiative interface to facilitate authoring of interactive data documents. The Kori system consists of a chart gallery (left), edit area (middle), and a link setting panel (right). Kori automatically suggests potential references (dotted gray underline) as a user types. Besides, it supports manual creation of links through simple interactions (4\u20136). The steps 1\u201310 describe a usage scenario to create an interactive story.",
        "external_paper_link": "",
        "has_pdf": true,
        "ff_link": ""
    },
    "v-tvcg-9444894": {
        "authors": [
            "Doris Lee",
            "Vidya Setlur",
            "Melanie Tory",
            "Karrie Karahalios",
            "Aditya Parameswaran"
        ],
        "title": "Deconstructing Categorization in Visualization Recommendation: A Taxonomy and Comparative Study",
        "session_id": "v-full-full5",
        "abstract": "Visualization recommendation (VisRec) systems provide users with suggestions for potentially interesting and useful next steps during exploratory data analysis. These recommendations are typically organized into categories based on their analytical actions, i.e., operations employed to transition from the current exploration state to a recommended visualization. However, despite the emergence of a plethora of VisRec systems in recent work, the utility of the categories employed by these systems in analytical workflows has not been systematically investigated. Our paper explores the efficacy of recommendation categories by formalizing a taxonomy of common categories and developing a system, Frontier, that implements these categories. Using Frontier, we evaluate workflow strategies adopted by users and how categories influence those strategies. Participants found recommendations that add attributes to enhance the current visualization and recommendations that filter to sub-populations to be comparatively most useful during data exploration. Our findings pave the way for next-generation VisRec systems that are adaptive and personalized via carefully chosen, effective recommendation categories.",
        "keywords": [
            "visual analysis, analytical workflow",
            "discovery-driven analysis",
            "visualization recommendations"
        ],
        "uid": "v-tvcg-9444894",
        "time_stamp": "2021-10-27T17:45:00Z",
        "has_image": false,
        "paper_award": "",
        "image_caption": "",
        "external_paper_link": "https://dx.doi.org/10.1109/TVCG.2021.3085751",
        "has_pdf": false,
        "ff_link": "https://youtu.be/21ArbHmwTiI"
    },
    "v-full-1103": {
        "authors": [
            "Haotian Li",
            "Yong Wang",
            "Songheng Zhang",
            "Yangqiu Song",
            "Huamin Qu"
        ],
        "title": "KG4Vis: A Knowledge Graph-Based Approach for Visualization Recommendation",
        "session_id": "v-full-full5",
        "abstract": "Visualization recommendation or automatic visualization generation can significantly lower the barriers for general users to rapidly create effective data visualizations, especially for those users without a background in data visualizations. However, existing rule-based approaches require tedious manual specifications of visualization rules by visualization experts. Other machine learning-based approaches often work like black-box and are difficult to understand why a specific visualization is recommended, limiting the wider adoption of these approaches. This paper fills the gap by presenting KG4Vis, a knowledge graph (KG)-based approach for visualization recommendation. It does not require manual specifications of visualization rules and can also guarantee good explainability. Specifically, we propose a framework for building knowledge graphs, consisting of three types of entities (i.e., data features, data columns and visualization design choices) and the relations between them, to model the mapping rules between data and effective visualizations. A TransE-based embedding technique is employed to learn the embeddings of both entities and relations of the knowledge graph from existing dataset-visualization pairs. Such embeddings intrinsically model the desirable visualization rules. Then, given a new dataset, effective visualizations can be inferred from the knowledge graph with semantically meaningful rules. We conducted extensive evaluations to assess the proposed approach, including quantitative comparisons, case studies and expert interviews. The results demonstrate the effectiveness of our approach.",
        "keywords": [],
        "uid": "v-full-1103",
        "time_stamp": "2021-10-27T18:00:00Z",
        "has_image": true,
        "paper_award": "honorable",
        "image_caption": "This figure illustrates the overall workflow of KG4Vis. We extract features from existing dataset-visualization pairs and construct a knowledge graph (KG). Then the embeddings of entities and relations in the KG are learned. Based on the embeddings, we conduct inference on a new dataset and finally recommend multiple visualizations. Also, various rules are extracted based on the embeddings and presented together with recommended visualizations to improve the interpretability of visualization recommendation.",
        "external_paper_link": "",
        "has_pdf": true,
        "ff_link": "https://youtu.be/RVX1jFGNLdw"
    },
    "v-full-1286": {
        "authors": [
            "Qing Chen",
            "Fuling Sun",
            "Xinyue Xu",
            "Zui Chen",
            "Jiazhe Wang",
            "Nan Cao"
        ],
        "title": "VizLinter: A Linter and Fixer Framework for Data Visualization",
        "session_id": "v-full-full5",
        "abstract": "Despite the rising popularity of automated visualization tools, existing systems tend to provide direct results which do not always fit the input data or meet visualization requirements. Therefore, additional specification adjustments are still required in real-world use cases. However, manual adjustments are difficult since most users do not necessarily possess adequate skills or visualization knowledge. Even experienced users might create imperfect visualizations that involve chart construction errors. We present a framework, VizFixer, to help users detect flaws and rectify already-built but defective visualizations. The framework consists of two components, (1) a visualization linter, which applies well-recognized principles to inspect the legitimacy of rendered visualizations, and (2) a visualization fixer, which automatically corrects the detected violations according to the linter. We implement the framework into an online editor prototype based on Vega-Lite specifications. To further evaluate the system, we conduct an in-lab user study. The results prove its effectiveness and efficiency in identifying and fixing errors for data visualizations.",
        "keywords": [],
        "uid": "v-full-1286",
        "time_stamp": "2021-10-27T18:15:00Z",
        "has_image": true,
        "paper_award": "",
        "image_caption": "VizLinter is a linter and fixer framework for data visualization. It can help detect problems in a given defective visualization and correct them. The framework consists of two components, a linter and a fixer. The linter inspects the legitimacy of a visualization against well-established design principles, and the fixer automatically resolves the violations detected by the linter.",
        "external_paper_link": "",
        "has_pdf": true,
        "ff_link": "https://youtu.be/GGLacuuuUdo"
    },
    "v-full-1119": {
        "authors": [
            "Saiful Khan",
            "Phong Nguyen",
            "Alfie Abdul-Rahman",
            "Benjamin Bach",
            "Min Chen",
            "Euan Freeman",
            "Cagatay Turkay"
        ],
        "title": "Propagating Visual Designs to Numerous Plots and Dashboards",
        "session_id": "v-full-full3",
        "abstract": "In the process of developing an infrastructure for providing visualization and visual analytics (VIS) tools to epidemiologists and modeling scientists, we encountered a technical challenge for applying a number of visual designs to numerous datasets rapidly and reliably with limited development resources. In this paper, we present a technical solution to address this challenge. Operationally, we separate the tasks of data management, visual designs, and plots and dashboard deployment in order to streamline the development workflow. Technically, we utilize: an ontology to bring datasets, visual designs, and deployable plots and dashboards under the same management framework; multi-criteria search and ranking algorithms for discovering potential datasets that match a visual design; and a purposely-design user interface for propagating each visual design to appropriate datasets (often in tens and hundreds) and quality-assuring the propagation before the deployment. This technical solution has been used in the development of the RAMPVIS infrastructure for supporting a consortium of epidemiologists and modeling scientists through visualization.",
        "keywords": [],
        "uid": "v-full-1119",
        "time_stamp": "2021-10-27T17:00:00Z",
        "has_image": true,
        "paper_award": "",
        "image_caption": "Our novel propagation workflow makes it easy to propagate visual designs to numerous datasets. Reference visualizations are created for data streams. A search and activate process is used to propagate the reference visualization to other appropriate data streams. Ontology keywords are used to construct a query for suitable data stream combinations. Search results consist of ranked data stream combinations that match query parameters, although some results may not be suitable for propagation. A quality assurance carried out by an expert ensures the visual design is only propagated to suitable data, resulting in new visualizations that are deployed as web pages.",
        "external_paper_link": "",
        "has_pdf": true,
        "ff_link": "https://youtu.be/WVsrMdvjQlk"
    },
    "v-full-1022": {
        "authors": [
            "Jimmy Moore",
            "Pascal Goffin",
            "Jason Wiese",
            "Miriah Meyer"
        ],
        "title": "Exploring the Personal Informatics Analysis Gap: \"There's a Lot of Bacon\"",
        "session_id": "v-full-full3",
        "abstract": "Personal informatics research supports people in tracking personal data for the purposes of self-reflection and gaining self-knowledge. This field, however, has predominantly focused on the data collection and insight-generation elements of self-tracking, with less attention paid to flexible data analysis. As a result, this inattention has lead to inflexible analytic pipelines that do not reflect or support the diverse ways people want to engage their data. This paper contributes a review of personal informatics and visualization research literature to expose a gap in our knowledge for designing flexible tools that assist people with engaging and analyzing personal data in personal contexts, which we call the personal informatics analysis gap. We explore this gap through a multi-stage longitudinal study on how asthmatics engage personal air quality data, and we report how participants: are motivated by broad and diverse goals; exhibited patterns in the way they explored their data; engaged with their data in playful ways; discovered new insights through serendipitous exploration; and were reluctant to use analysis tools on their own. These results present new opportunities for visual analysis research and suggest the need for fundamental shifts in how and what we design for supporting analysis of personal data.",
        "keywords": [],
        "uid": "v-full-1022",
        "time_stamp": "2021-10-27T17:15:00Z",
        "has_image": true,
        "paper_award": "",
        "image_caption": "Visualization research traditionally involves close collaborations with domain experts working in professional contexts. This work identifies a lack of focused research and design of systems that allow people to flexibly analyze their personal data in personal contexts: what we call the personal informatics analysis gap. We explore this gap through a review of personal informatics and visualization literature, and our own experiences conducting a three year longitudinal study with asthmatic families.  This work highlights how our study helped to reveal this gap, along with design suggestions for helping bridge it.",
        "external_paper_link": "",
        "has_pdf": true,
        "ff_link": "https://youtu.be/8U4hGTpmzXE"
    },
    "v-full-1106": {
        "authors": [
            "Nathalie Bressa",
            "Henrik Korsgaard",
            "Aur\u00e9lien Tabard",
            "Steven Houben",
            "Jo Vermeulen"
        ],
        "title": "What's the Situation with Situated Visualization? A Survey and Perspectives on Situatedness",
        "session_id": "v-full-full3",
        "abstract": "Situated visualization is an emerging concept within information visualization, in which data is visualized in situ, where it is relevant to people. The concept has gained interest from multiple research communities, including information visualization, human-computer interaction and augmented reality. This has led to a range of explorations and applications of the concept, however, this early work has focused on the operational aspect of situatedness leading to inconsistent adoption of the concept and terminology. \n  First, we contribute a literature survey in which we analyze 40 papers that explicitly use the term \"situated visualization\" to provide an overview of the research area, how it defines situated visualization, common application areas and technology used, as well as type of data and type of visualizations. Our survey shows that research on situated visualization has focused on technology-centric approaches that forefront a spatial understanding of situatedness. \n  Secondly, we contribute five perspectives on situatedness (space, time, place, activity, and community) that expand on the prevalent notion of situatedness in the corpus. We draw from six case studies and prior theoretical developments in HCI. Each perspective develops a generative way of looking at and working with situatedness in design and research. We outline future directions, including considering technology, material and aesthetics, leveraging the perspectives for design, and methods for stronger engagement with target audiences. We conclude with opportunities to consolidate situated visualization research.",
        "keywords": [],
        "uid": "v-full-1106",
        "time_stamp": "2021-10-27T17:30:00Z",
        "has_image": true,
        "paper_award": "",
        "image_caption": "Top: Title of the paper (\"What\u2019s the Situation with Situated Visualization? A Survey and Perspectives on Situatedness\") and author information (Nathalie Bressa, Henrik Korsgaard, Aur\u00e9lien Tabard, Steven Houben, Jo Vermeulen).\nLeft: The five perspectives that we introduce (space, time, place, activity, and community) and photos of the six representative cases (Corsican Twin, Situated Glyphs, Cairn, Chemicals in the Creek, Activity Clock, and Public Polling Displays).\nRight: Photo gallery of the corpus for our literature survey consisting of 44 papers using the term \"situated visualization\".\nBottom: Affiliation logos (Aarhus University, Universit\u00e9 Claude Bernard Lyon 1, Eindhoven University of Technology, Autodesk Research).",
        "external_paper_link": "",
        "has_pdf": true,
        "ff_link": "https://youtu.be/fzHGkDlVKdc"
    },
    "v-full-1038": {
        "authors": [
            "Xiangtong Chu",
            "Xiao Xie",
            "Shuainan Ye",
            "Haolin Lu",
            "Hongguang Xiao",
            "Zeqing Yuan",
            "Zhutian Chen",
            "Hui Zhang",
            "Yingcai Wu"
        ],
        "title": "TIVEE: Visual Exploration and Explanation of Badminton Tactics in Immersive Visualizations",
        "session_id": "v-full-full3",
        "abstract": "Tactic analysis is a major issue in badminton as the effective usage of tactics is the key to win. The tactic in badminton is defined as a sequence of consecutive strokes. Most existing methods use statistical models to find sequential patterns of strokes and apply 2D visualizations such as glyphs and statistical charts to explore and analyze the discovered patterns. However, in badminton, spatial information like the shuttle trajectory, which is inherently 3D, is the core of a tactic. The lack of sufficient spatial awareness in 2D visualizations largely limited the tactic analysis of badminton. In this work, we collaborate with domain experts to study the tactic analysis of badminton in a 3D environment and propose an immersive visual analytics system, TIVEE, to assist users in exploring and explaining badminton tactics from multi-levels. Users can first explore various tactics from the third-person perspective using an unfolded visual presentation of stroke sequences. By selecting a tactic of interest, users can turn to the first-person perspective to perceive the detailed kinematic characteristics and explain its effects on the game result. The effectiveness and usefulness of TIVEE are demonstrated by case studies and an expert interview.",
        "keywords": [],
        "uid": "v-full-1038",
        "time_stamp": "2021-10-27T17:45:00Z",
        "has_image": true,
        "paper_award": "",
        "image_caption": "TIVEE is used to analyze badminton tactics in an immersive environment. Users can first obtain an overview (a) of commonly used tactics of Chen Long. The overview shows aggregated trajectories and statistical information (i.e., Rusage and Rscoring) of each tactic group. Users can set a specific game scenario with the menu and the overview will be updated to show the corresponding tactics (b). Users can further inspect a tactic group (c), a tactic (d), and the origin trajectories of the selected tactic (e).",
        "external_paper_link": "",
        "has_pdf": true,
        "ff_link": "https://youtu.be/9iJprmMHhfc"
    },
    "v-tvcg-9193986": {
        "authors": [
            "Kurtis Danyluk",
            "Teoman Ulusoy",
            "Wei Wei",
            "Wesley Willett"
        ],
        "title": "Touch and Beyond: Comparing Physical and Virtual Reality Visualizations",
        "session_id": "v-full-full3",
        "abstract": "We compare physical and virtual reality (VR) versions of simple data visualizations. We also explore how the addition of virtual annotation and filtering tools affects how viewers solve basic data analysis tasks. We report on two studies, inspired by previous examinations of data physicalizations. The first study examined differences in how viewers interact with physical hand-scale, virtual hand-scale, and virtual table-scale visualizations and the impact that the different forms had on viewer's problem solving behavior. A second study examined how interactive annotation and filtering tools might sup-port new modes of use that transcend the limitations of physical representations. Our results highlight challenges associated with virtual reality representations and hint at the potential of interactive annotation and filtering tools in VR visualizations.",
        "keywords": [
            "Data Visualization",
            "Tools",
            "Three Dimensional Displays",
            "Virtual Reality",
            "Bars",
            "Task Analysis",
            "Visualization",
            "Human Computer Interaction",
            "Visualization",
            "Data Visualization",
            "Virtual Reality",
            "Physicalization"
        ],
        "uid": "v-tvcg-9193986",
        "time_stamp": "2021-10-27T18:00:00Z",
        "has_image": true,
        "paper_award": "",
        "image_caption": "This preview for the Paper \"Touch and Beyond: Comparing Physical and Virtual Reality Visualizations\" shows a large room-sized virtual 3D bar chart, a table-sized virtual 3D bar chart and a hand-sized lego bar chart. On the bottom is an image of the authors.",
        "external_paper_link": "https://doi.org/10.1109/TVCG.2020.3023336",
        "has_pdf": false,
        "ff_link": "https://youtu.be/m5F9SbQ7cgc"
    },
    "v-full-1219": {
        "authors": [
            "Hyeok Kim",
            "Ryan Rossi",
            "Abhraneel Sarma",
            "Dominik Moritz",
            "Jessica Hullman"
        ],
        "title": "An Automated Approach to Reasoning About Task-Oriented Insights in Responsive Visualization",
        "session_id": "v-full-full3",
        "abstract": "Authors often transform a large screen visualization for smaller displays through rescaling, aggregation and other techniques when creating visualizations for both desktop and mobile devices (i.e., responsive visualization). However, transformations can alter relationships or patterns implied by the large screen view, requiring authors to reason carefully about what information to preserve while adjusting their design for the smaller display. We propose an automated approach to approximating the loss of support for task-oriented visualization insights (identification, comparison, and trend) in responsive transformation of a source visualization. We operationalize identification, comparison, and trend loss as objective functions calculated by comparing properties of the rendered source visualization to each realized target (small screen) visualization. To evaluate the utility of our approach, we train machine learning models on human ranked small screen alternative visualizations across a set of source visualizations. We find that our approach achieves an accuracy of 84% (random forest model) in ranking visualizations. We demonstrate this approach in a prototype responsive visualization recommender that enumerates responsive transformations using Answer Set Programming and evaluates the preservation of task-oriented insights using our loss measures. We discuss implications of our approach for the development of automated and semi-automated responsive visualization recommendation.",
        "keywords": [],
        "uid": "v-full-1219",
        "time_stamp": "2021-10-27T18:15:00Z",
        "has_image": true,
        "paper_award": "",
        "image_caption": "We characterize three types of changes to task-oriented insights under responsive transformations. First, identification loss is about whether data point are still identifiable, estimated by difference in entropy of visual attributes. Second, comparison loss means how similarly pairs of data points are comparable between two views, which we approximate by the earth mover's distance between distributions of pairwise distances. Lastly, we have trend loss which is about how similarly the relationships between two variables appear between two views. We estimate this using the difference between LOESS-based trend estimates. We use these loss measures in a raking model-based responsive design recommender.",
        "external_paper_link": "",
        "has_pdf": true,
        "ff_link": ""
    },
    "v-tvcg-9288641": {
        "authors": [
            "Yan Lyu",
            "Fan Gao",
            "I-Shuen Wu",
            "Brian Lim"
        ],
        "title": "Imma Sort by Two or More Attributes With Interpretable Monotonic Multi-Attribute Sorting",
        "session_id": "v-full-full14",
        "abstract": "Many choice problems often involve multiple attributes which are mentally challenging, because only one attribute is neatly sorted while others could be randomly arranged. We hypothesize that perceiving approximately monotonic trends across multiple attributes is key to the overall interpretability of sorted results, because users can easily predict the attribute values of the next items. We extend a ranking principal curve model to tune monotonic trends in attributes and present Imma Sort to sort items by multiple attributes simultaneously by trading-off the monotonicity in the primary sorted attribute to increase the human predictability for other attributes. We characterize how it performs for varying attribute correlations, attribute preferences, list lengths and number of attributes. We further extend Imma Sort with ImmaAnchor and ImmaCenter to improve the learnability and efficiency to search sorted items with conflicting attributes. We demonstrate usage scenarios for two applications and evaluate its learnability, usability, interpretability and user performance in prediction and search tasks. We find that Imma Sort improves the interpretability and satisfaction of sorting by \u2265 2 attributes. We discuss why, when, where, and how to deploy Imma Sort for real-world applications.",
        "keywords": [
            "Multi-attribute sorting",
            "decision making",
            "interpretability",
            "human predictability",
            "predictive interpretability."
        ],
        "uid": "v-tvcg-9288641",
        "time_stamp": "2021-10-27T17:00:00Z",
        "has_image": true,
        "paper_award": "",
        "image_caption": "Imma Sort orders items such that they are approximately monotonic in multiple attributes. In the sorted hotel list, the prices (blue line) are mostly increasing while the distances (orange line) to downtown are mostly decreasing. With Imma Sort, users will be able to perceive approximate monotonic trends for more than one attribute, more easily predict values of multiple attributes as they navigate down the sorted list, and more easily identify the best item as a compromise between the conflicting attributes.",
        "external_paper_link": "https://doi.org/10.1109/TVCG.2020.3043487",
        "has_pdf": false,
        "ff_link": "https://youtu.be/m3ojNqQbScI"
    },
    "v-tvcg-9346003": {
        "authors": [
            "Juliane M\u00fcller",
            "Laura Garrison",
            "Philipp Ulbrich",
            "Stefanie Schreiber",
            "Stefan Bruckner",
            "Helwig Hauser",
            "Steffen Oeltze-Jafra"
        ],
        "title": "Integrated Dual Analysis of Quantitative and Qualitative High-Dimensional Data",
        "session_id": "v-full-full14",
        "abstract": "The Dual Analysis framework is a powerful enabling technology for the exploration of high dimensional quantitative data by treating data dimensions as first-class objects that can be explored in tandem with data values. In this work, we extend the Dual Analysis framework through the joint treatment of quantitative (numerical) and qualitative (categorical) dimensions. Computing common measures for all dimensions allows us to visualize both quantitative and qualitative dimensions in the same view. This enables a natural joint treatment of mixed data during interactive visual exploration and analysis. Several measures of variation for nominal qualitative data can also be applied to ordinal qualitative and quantitative data. For example, instead of measuring variability from a mean or median, other measures assess inter-data variation or average variation from a mode. In this work, we demonstrate how these measures can be integrated into the Dual Analysis framework to explore and generate hypotheses about high-dimensional mixed data. A medical case study using clinical routine data of patients suffering from Cerebral Small Vessel Disease (CSVD), conducted with a senior neurologist and a medical student, shows that a joint Dual Analysis approach for quantitative and qualitative data can rapidly lead to new insights based on which new hypotheses may be generated.",
        "keywords": [
            "Dual Analysis approach",
            "High-dimensional data",
            "Mixed data",
            "Mixed statistical analysis"
        ],
        "uid": "v-tvcg-9346003",
        "time_stamp": "2021-10-27T17:15:00Z",
        "has_image": true,
        "paper_award": "",
        "image_caption": "Common approaches in visual data analysis, such as the Dual Analysis Approach by Turkay et al., focus on quantitative data without accounting for the qualitative dimensions. This may overlook relevant relationships. In this work, we present a solution for extending this approach to the joint analysis of quantitative AND qualitative data.\nOur conceptual workflow starts with an overview about the dataset in dimension and item view, where all data are visualized together. Subset selection and analysis allows the user to select a data subset and to track the resulting changes in descriptive statistics. These steps may be reset or repeated. \n",
        "external_paper_link": "https://doi.org/10.1109/TVCG.2021.3056424",
        "has_pdf": false,
        "ff_link": "https://youtu.be/rxIeRpwFKqM"
    },
    "v-full-1532": {
        "authors": [
            "Jiazhi Xia",
            "Yuchen Zhang",
            "Jie Song",
            "Yang Chen",
            "Yunhai Wang",
            "Shixia Liu"
        ],
        "title": "Revisiting Dimensionality Reduction Techniques for Visual Cluster Analysis: An Empirical Study",
        "session_id": "v-full-full14",
        "abstract": "Dimensionality Reduction (DR) techniques can generate 2D projections and enable visual exploration of cluster structures of high-dimensional datasets. However, different DR techniques would yield various patterns, which significantly affect the performance of visual cluster analysis tasks. We present the results of a user study that investigates the influence of different DR techniques on visual cluster analysis. Our study focuses on the most concerned property types, namely the linearity and locality, and evaluates twelve representative DR techniques that cover the concerned properties. Four controlled experiments were conducted to evaluate how the DR techniques facilitate the tasks of 1) identifying clusters, 2) associating cluster members, 3) comparing distances among clusters, and 4) comparing cluster densities, respectively. We also evaluated users\u2019 subjective preference of the DR techniques regarding the quality of projected clusters. The results show that: 1) Non-linear and Local techniques are preferred in identifying clusters and associating cluster members; 2) Linear techniques perform better than non-linear techniques in comparing cluster densities; 3) UMAP (Uniform Manifold Approximation and Projection) and t-SNE (t-Distributed Stochastic Neighbor Embedding) perform the best in identifying clusters and associating cluster members; 4) NMF(Nonnegative Matrix Factorization) has competitive performance in comparing distances among clusters; 5) t-SNLE(t-Distributed Stochastic Neighbor Linear Embedding) has competitive performance in comparing cluster densities.",
        "keywords": [],
        "uid": "v-full-1532",
        "time_stamp": "2021-10-27T17:30:00Z",
        "has_image": true,
        "paper_award": "",
        "image_caption": "Dimensionality Reduction (DR) techniques can generate 2D projections and enable visual exploration of cluster structures of high-dimensional datasets. However, different DR techniques would yield various patterns, which significantly affect the performance of visual cluster analysis tasks. We present the results of a user study that investigates the influence of different DR techniques on visual cluster analysis. Our study focuses on the most concerned property types, namely the linearity and locality, and evaluates twelve representative DR techniques that cover the concerned properties.",
        "external_paper_link": "",
        "has_pdf": true,
        "ff_link": "https://youtu.be/aNjcy5KLfd8"
    },
    "v-tvcg-9349198": {
        "authors": [
            "Laura Garrison",
            "Juliane M\u00fcller",
            "Stefanie Schreiber",
            "Steffen Oeltze-Jafra",
            "Helwig Hauser",
            "Stefan Bruckner"
        ],
        "title": "DimLift: Interactive Hierarchical Data Exploration through Dimensional Bundling",
        "session_id": "v-full-full14",
        "abstract": "The identification of interesting patterns and relationships is essential to exploratory data analysis. This becomes increasingly difficult in high dimensional datasets. While dimensionality reduction techniques can be utilized to reduce the analysis space, these may unintentionally bury key dimensions within a larger grouping and obfuscate meaningful patterns. With this work we introduce DimLift, a novel visual analysis method for creating and interacting with dimensional bundles. Generated through an iterative dimensionality reduction or user-driven approach, dimensional bundles are expressive groups of dimensions that contribute similarly to the variance of a dataset. Interactive exploration and reconstruction methods via a layered parallel coordinates plot allow users to lift interesting and subtle relationships to the surface, even in complex scenarios of missing and mixed data types. We exemplify the power of this technique in an expert case study on clinical cohort data alongside two additional case examples from nutrition and ecology.",
        "keywords": [
            "Dimensionality reduction",
            "interactive visual analysis",
            "visual analytics",
            "parallel coordinates"
        ],
        "uid": "v-tvcg-9349198",
        "time_stamp": "2021-10-27T17:45:00Z",
        "has_image": true,
        "paper_award": "",
        "image_caption": "DimLift is a mixed-initiative approach to creating and navigating dimensional bundles, defined as a subset of similarly contributing dimensions to the overall variation of a dataset. Parallel coordinate axes (A) map to the first or second principal components (PC1, PC2) of a dimensional bundle. Glyphs (A1) provide feedback on bundle characteristics and composition. Interactions (A2) allow users to pan (D) through the dataset, swap axes between PC1 and PC2, drill-down into a PC1 vs. PC2 score plot (B), or drill-down further to contained dimensions (C) and their relationships (C1). (E) provides an overview of all dimensional bundles and unbundled dimensions.",
        "external_paper_link": "https://doi.org/10.1109/TVCG.2021.3057519",
        "has_pdf": false,
        "ff_link": "https://youtu.be/uildnYPOsQ0"
    },
    "v-full-1440": {
        "authors": [
            "Jan-Tobias Sohns",
            "Michaela Schmitt",
            "Fabian Jirasek",
            "Hans Hasse",
            "Heike Leitte"
        ],
        "title": "Attribute-based Explanation of Non-Linear Embeddings of High-Dimensional Data",
        "session_id": "v-full-full14",
        "abstract": "Embeddings of high-dimensional data are widely used to explore data, to verify analysis results, and to communicate information. Their explanation, in particular with respect to the input attributes, is often difficult. With linear projects like PCA the axes can still be annotated meaningfully. With non-linear projections this is no longer possible and alternative strategies such as attribute-based color coding are required. In this paper, we review existing augmentation techniques and discuss their limitations. We present the Non-Linear Embeddings Surveyor (NoLiES) that combines a novel augmentation strategy for projected data (rangesets) with interactive analysis in a small multiples setting. Rangesets use a set-based visualization approach for binned attribute values that enable the user to quickly observe structure and detect outliers. We detail the link between algebraic topology and rangesets and demonstrate the utility of NoLiES in case studies with various challenges (complex attribute value distribution, many attributes, many data points) and a real-world application to understand latent features of matrix completion in thermodynamics.",
        "keywords": [],
        "uid": "v-full-1440",
        "time_stamp": "2021-10-27T18:00:00Z",
        "has_image": true,
        "paper_award": "",
        "image_caption": "",
        "external_paper_link": "",
        "has_pdf": true,
        "ff_link": ""
    },
    "v-full-1020": {
        "authors": [
            "Hyeon Jeon",
            "Hyung-Kwon Ko",
            "Jaemin Jo",
            "Youngtaek Kim",
            "Jinwook Seo"
        ],
        "title": "Measuring and Explaining the Inter-Cluster Reliability of Multidimensional Projections",
        "session_id": "v-full-full14",
        "abstract": "We propose Steadiness and Cohesiveness, two novel metrics to measure the inter-cluster reliability of multidimensional projection (MDP), specifically how well the inter-cluster structures are preserved between the original high-dimensional space and the low-dimensional projection space. Measuring inter-cluster reliability is crucial as it directly affects how well inter-cluster tasks (e.g., identifying cluster relationships in the original space from a projected view) can be conducted; however, despite the importance of inter-cluster tasks, we found that previous metrics, such as Trustworthiness and Continuity, fail to measure inter-cluster reliability. Our metrics consider two aspects of the inter-cluster reliability: Steadiness measures the extent to which clusters in the projected space form clusters in the original space, and Cohesiveness measures the opposite. They extract random clusters with arbitrary shapes and positions in one space and evaluate how much the clusters are stretched or dispersed in the other space. Furthermore, our metrics can quantify pointwise distortions, allowing for the visualization of inter-cluster reliability in a projection, which we call a reliability map. Through quantitative experiments, we verify that our metrics precisely capture the distortions that harm inter-cluster reliability while previous metrics have difficulty capturing the distortions. A case study also demonstrates that our metrics and the reliability map 1) support users in selecting the proper projection techniques or hyperparameters and 2) prevent misinterpretation while performing inter-cluster tasks, thus allow an adequate identification of inter-cluster structure.",
        "keywords": [],
        "uid": "v-full-1020",
        "time_stamp": "2021-10-27T18:15:00Z",
        "has_image": true,
        "paper_award": "",
        "image_caption": "We propose Steadiness and Cohesiveness, two novel metrics to measure the inter-cluster reliability of multidimensional projection (MDP), specifically how well the inter-cluster structures are preserved between the original high-dimensional space and the low-dimensional projection space. Our metrics can also quantify pointwise distortions, allowing for the visualization of inter-cluster reliability in a projection, which we call a reliability map. This figure depicts the UMAP, Isomap, and LLE projections of the MNIST test dataset and the reliability maps that visualize each projection\u2019s inter-cluster distortion, along with overall Steadiness (St) and Cohesiveness (Co) scores depicted under each technique.",
        "external_paper_link": "",
        "has_pdf": true,
        "ff_link": "https://youtu.be/EFaKO5hvtcw"
    },
    "v-cga-9281098": {
        "authors": [
            "Jeremy Walton",
            "Samantha Adams",
            "Wolfgang Hayek",
            "Piotr Florek",
            "Harold Dyson"
        ],
        "title": "Dynamic 3D Visualization of Climate Model Development and Results",
        "session_id": "v-cga-cga2",
        "abstract": "Climate models play a significant role in the understanding of climate change, and the effective presentation and interpretation of their results is important for both the scientific community and the general public. In the case of the latter audience-which has become increasingly concerned with the implications of climate change for society-there is a requirement for visualizations which are compelling and engaging. We describe the use of ParaView, a well-established visualization application, to produce images and animations of results from a large set of modeling experiments, and their use in the promulgation of climate research results. Visualization can also make useful contributions to development, particularly for complex large-scale applications such as climate models. We present early results from the construction of a next-generation climate model which has been designed for use on exascale compute platforms, and show how visualization has helped in the development process, particularly with regard to higher model resolutions and novel data representations.",
        "keywords": [],
        "uid": "v-cga-9281098",
        "time_stamp": "2021-10-27T17:00:00Z",
        "has_image": true,
        "paper_award": "",
        "image_caption": "Four possibilities for temperatures at the Earth\u2019s surface in 2100, determined by our climate model (UKESM1) using four scenarios for future emissions of greenhouse gases such as CO2.  These range from one where the world adopts \u201csustainable development\u201d to a \u201cbusiness as usual\u201d Earth which continues to rely on fossil fuels, resulting in rising emissions of CO2.  Warming increases with emissions levels, and is not distributed evenly around the world.  Thus, although the \u201cmiddle of the road\u201d scenario sees a global average temperature rise of 4 degrees by 2100, parts of the Arctic warm by as much as 20 degrees.   ",
        "external_paper_link": "https://doi.org/10.1109/MCG.2020.3042587",
        "has_pdf": false,
        "ff_link": "https://youtu.be/OcscgKSYk_8"
    },
    "v-cga-8758151": {
        "authors": [
            "Brandon Mathis",
            "Yuxin Ma",
            "Michelle Mancenido",
            "Ross Maciejewski"
        ],
        "title": "Exploring the Design Space of Sankey Diagrams for the Food-Energy-Water Nexus",
        "session_id": "v-cga-cga2",
        "abstract": "In this work, we define a set of design requirements relating to Sankey diagrams for supporting food-energy-water nexus understanding and propose the network embodied sectoral trajectory diagram design, a visualization design that incorporates a number of characteristics from Sankey diagrams, treemaps, and graphs, to improve the readability and minimize the negative impact of edge crossings that are common in traditional Sankey diagrams.",
        "keywords": [],
        "uid": "v-cga-8758151",
        "time_stamp": "2021-10-27T17:15:00Z",
        "has_image": false,
        "paper_award": "",
        "image_caption": "",
        "external_paper_link": "https://doi.org/10.1109/MCG.2019.2927556",
        "has_pdf": false,
        "ff_link": "https://youtu.be/Vac7O-O9BE4"
    },
    "v-cga-8691491": {
        "authors": [
            "Shamal AL-Dohuki",
            "Ye Zhao",
            "Farah Kamw",
            "Jing Yang",
            "Xinyue Ye",
            "Wei Chen"
        ],
        "title": "QuteVis: Visually Studying Transportation Patterns Using Multi-Sketch Query of Joint Traffic Situations",
        "session_id": "v-cga-cga2",
        "abstract": "QuteVis uses multisketch query and visualization to discover specific times and days in history with specified joint traffic patterns at different city locations. Users can use touch input devices to define, edit, and modify multiple sketches on a city map. A set of visualizations and interactions is provided to help users browse and compare retrieved traffic situations and discover potential influential factors. QuteVis is built upon a transport database that integrates heterogeneous data sources with an optimized spatial indexing and weighted similarity computation. An evaluation with real-world data and domain experts demonstrates that QuteVis is useful in urban transportation applications in modern cities.",
        "keywords": [],
        "uid": "v-cga-8691491",
        "time_stamp": "2021-10-27T17:30:00Z",
        "has_image": false,
        "paper_award": "",
        "image_caption": "",
        "external_paper_link": "https://doi.org/10.1109/MCG.2019.2911230",
        "has_pdf": false,
        "ff_link": "https://youtu.be/yAzqhCEJSvI"
    },
    "v-cga-9057396": {
        "authors": [
            "Florian Windhager",
            "Saminu Salisu",
            "Roger A. Leite",
            "Velitchko Filipov",
            "Silvia Miksch",
            "G\u00fcnther Schreder",
            "Eva Mayr"
        ],
        "title": "Many Views Are Not Enough: Designing for Synoptic Insights in Cultural Collections",
        "session_id": "v-cga-cga2",
        "abstract": "Cultural object collections attract and delight spectators since ancient times. Yet, they also easily overwhelm visitors due to their perceptual richness and associated information. Similarly, digitized collections appear as complex, multifaceted phenomena, which can be challenging to grasp and navigate. Though visualizations can create various types of collection overviews for that matter, they do not easily assemble into a \u201cbig picture\u201d or lead to an integrated understanding. We introduce coherence techniques to maximize connections between multiple views and apply them to the prototype PolyCube system of collection visualization: with map, set, and network visualizations it makes spatial, categorical, and relational collection aspects visible. For the essential temporal dimension, it offers four different views: superimposition, animation, juxtaposition, and space-time cube representations. A user study confirmed that better integrated visualizations support synoptic, cross-dimensional insights. An outlook is dedicated to the system's applicability within other arts and humanities data domains.",
        "keywords": [],
        "uid": "v-cga-9057396",
        "time_stamp": "2021-10-27T17:45:00Z",
        "has_image": true,
        "paper_award": "",
        "image_caption": "The PolyCube system of collection visualization provides a time-oriented distant viewing perspective on spatial, taxonomic and relational aspects of cultural collections, together with three other visualizations of the historical data dimension.",
        "external_paper_link": "https://doi.org/10.1109/MCG.2020.2985368",
        "has_pdf": false,
        "ff_link": "https://youtu.be/3pJTgZsZqEY"
    },
    "v-cga-8999535": {
        "authors": [
            "Chao Ma",
            "Ye Zhao",
            "Andrew Curtis",
            "Farah Kamw",
            "Shamal AL-Dohuki",
            "Jing Yang",
            "Suphanut Jamonnak",
            "Ismael Ali"
        ],
        "title": "CLEVis: A Semantic Driven Visual Analytics System for Community Level Events",
        "session_id": "v-cga-cga2",
        "abstract": "Community-level event (CLE) datasets, such as police reports of crime events, contain abundant semantic information of event situations, and descriptions in a geospatial-temporal context. They are critical for frontline users, such as police officers and social workers, to discover and examine insights about community neighborhoods. We propose CLEVis, a neighborhood visual analytics system for CLE datasets, to help frontline users explore events for insights at community regions of interest, namely fine-grained geographical resolutions, such as small neighborhoods around local restaurants, churches, and schools. CLEVis fully utilizes semantic information by integrating automatic algorithms and interactive visualizations. The design and development of CLEVis are conducted with solid collaborations with real-world community workers and social scientists. Case studies and user feedback are presented with real-world datasets and applications.",
        "keywords": [],
        "uid": "v-cga-8999535",
        "time_stamp": "2021-10-27T18:00:00Z",
        "has_image": true,
        "paper_award": "",
        "image_caption": "Community-level event (CLE) datasets, such as police reports of crime events, contain abundant semantic information of event situations, and descriptions in a geospatial-temporal context. They are critical for frontline users, such as police officers and social workers, to discover and examine insights about community neighborhoods. We propose CLEVis, a neighborhood visual analytics system for CLE datasets, to help frontline users explore events for insights at community regions of interest, namely fine-grained geographical resolutions.",
        "external_paper_link": "https://doi.org/10.1109/MCG.2020.2973939",
        "has_pdf": false,
        "ff_link": "https://youtu.be/EVfDSX6M_KA"
    },
    "a-visap-1031": {
        "authors": [
            "Jessica Marielle Kendall-Bar",
            "Nicolas Kendall-Bar",
            "Angus G. Forbes",
            "Gitte McDonald",
            "Paul J. Ponganis",
            "Cassondra Williams",
            "Allyson Hindle",
            "Holger Klinck",
            "Markus Horning",
            "David Wiley",
            "Ari S. Friedlaender",
            "Roxanne S. Beltran",
            "Daniel P. Costa",
            "Terrie Williams"
        ],
        "title": "Visualizing Life in the Deep: A Creative Pipeline for Data-Driven Animations to Facilitate Marine Mammal Research, Outreach, and Conservation",
        "session_id": "a-visap-visap1",
        "abstract": null,
        "keywords": [],
        "uid": "a-visap-1031",
        "time_stamp": "2021-10-27T17:25:00Z",
        "has_image": true,
        "paper_award": "",
        "image_caption": "Illustration of an animated 3D humpback whale combined with a representation of several data streams used in our data-driven animation pipeline. The ribbons of data shown include: swimming and gliding data from an elephant seal, the waveform of a soundtrack generated from the beating heart of a narwhal, and notes of a custom musical score for one of our animations.",
        "external_paper_link": "",
        "has_pdf": true,
        "ff_link": "https://youtu.be/2ymFXwJDDH8"
    },
    "a-visap-1038": {
        "authors": [
            "Karin von Ompteda"
        ],
        "title": "Creating Meaningful Connections Through COVID-19 Data Manifestation",
        "session_id": "a-visap-visap1",
        "abstract": null,
        "keywords": [],
        "uid": "a-visap-1038",
        "time_stamp": "2021-10-27T17:37:00Z",
        "has_image": true,
        "paper_award": "",
        "image_caption": "A Blinding Truth by Michael Zhang",
        "external_paper_link": "",
        "has_pdf": true,
        "ff_link": "https://youtu.be/yHtG6sb28c4"
    },
    "a-visap-1045": {
        "authors": [
            "Stephanie Zeller"
        ],
        "title": "Affective Palettes for Scientific Visualization: Grounding Environmental Data in the Natural World",
        "session_id": "a-visap-visap1",
        "abstract": null,
        "keywords": [],
        "uid": "a-visap-1045",
        "time_stamp": "2021-10-27T17:50:00Z",
        "has_image": true,
        "paper_award": "",
        "image_caption": "",
        "external_paper_link": "",
        "has_pdf": true,
        "ff_link": "https://youtu.be/QoyVTdczrmQ"
    },
    "a-visap-1046": {
        "authors": [
            "Jiabao Li",
            "Cooper Galvin"
        ],
        "title": "Glacier\u2019s Lament",
        "session_id": "a-visap-visap1",
        "abstract": null,
        "keywords": [],
        "uid": "a-visap-1046",
        "time_stamp": "2021-10-27T17:58:00Z",
        "has_image": true,
        "paper_award": "",
        "image_caption": "",
        "external_paper_link": "",
        "has_pdf": true,
        "ff_link": "https://youtu.be/CiohDzBNNMM"
    },
    "a-visap-1064": {
        "authors": [
            "Caitlin Foley",
            "Misha Rabinovich"
        ],
        "title": "Surface Tension ",
        "session_id": "a-visap-visap1",
        "abstract": null,
        "keywords": [],
        "uid": "a-visap-1064",
        "time_stamp": "2021-10-27T18:04:00Z",
        "has_image": true,
        "paper_award": "",
        "image_caption": "Surface Tension (Mississippi Flooding), 2019\u2013ongoing, custom browser-based software and graphics, responsive design (dimensions variable) This is an artistic visualization driven by water data from the USA. Water supports life but can also drown and destroy. Because Earth\u2019s surface water is interconnected, the disturbances at the specific points ripple out and interfere with each other, influencing one another and combining into a pattern of interdependence.",
        "external_paper_link": "",
        "has_pdf": false,
        "ff_link": "https://youtu.be/22m5fRjTWoA"
    },
    "a-vizsec-9866": {
        "authors": [
            "Martin Graham",
            "Robert Kukla",
            "Oleksii Mandrychenko",
            "Darren Hart",
            "Jessie B Kennedy"
        ],
        "title": "Developing Visualisations to Enhance an Insider Threat Product: A Case Study",
        "session_id": "a-vizsec-vizsec-3",
        "abstract": "Attack graphs (AG) are used to assess pathways availed by cyber adversaries to penetrate a network. State-of-the-art approaches for AG generation focus mostly on deriving dependencies between system vulnerabilities based on network scans and expert knowledge. In real-world operations however, it is costly and ineffective to rely on constant vulnerability scanning and expert-crafted AGs. We propose to automatically learn AGs based on actions observed through intrusion alerts, without prior expert knowledge. Specifically, we develop an unsupervised sequence learning system, SAGE, that leverages the temporal and probabilistic dependence between alerts in a suffix-based probabilistic deterministic finite automaton(S-PDFA) \u2013 a model that accentuates infrequent severe alerts and summarizes paths leading to them. AGs are then derived from the S-PDFA. Tested with intrusion alerts collected through Collegiate Penetration Testing Competition, SAGE compresses several thousands of alerts into only a handful of AGs. These AGs reflect the strategies used by participating teams. The resulting AGs are succinct, interpretable, and enable analysts to derive actionable insights, e.g., attackers tend to follow shorter paths after they have discovered a longer one.",
        "keywords": [
            "Fortinet Ltd, Edinburgh"
        ],
        "uid": "a-vizsec-9866",
        "time_stamp": "2021-10-27T17:00:00Z",
        "has_image": true,
        "paper_award": "",
        "image_caption": "Calendar view of daily alerts across a large organisation monitoring insider threat. Weekends are quiet, and a public holiday weekend especially so, but may be when a malicious user chooses to strike.",
        "external_paper_link": "",
        "has_pdf": true,
        "ff_link": "https://youtu.be/8OHVNhcBRtk"
    },
    "a-vizsec-9220": {
        "authors": [
            "Fabian B\u00f6hm",
            "Ludwig Englbrecht",
            "Sabrina Friedl",
            "Guenther Pernul"
        ],
        "title": "Visual Decision-Support for Live Digital Forensic Investigations",
        "session_id": "a-vizsec-vizsec-3",
        "abstract": "Performing a live digital forensic investigation on a running system is challenging due to the time pressure under which decisions have to be made. Newly proliferating and frequently applied types of malware (e.g., fileless malware) increase the need to conduct digital forensic investigations in real-time. In the course of these investigations, forensic experts are confronted with a wide range of different forensic tools. The decision, which of those are suitable for the current situation, is often based on the forensic experts' experience. Currently, there is no reliable automated solution to support this decision-making. Therefore, we derive requirements for visually supporting the decision-making process for live forensic investigations and introduce a research prototype that provides visual guidance for cyber forensic experts during a live digital forensic investigation. Our prototype collects relevant core information for a live digital forensic and provides visual representations for connections between occurring events, developments over time, and detailed information on specific events. To show the applicability of our approach, we analyze an exemplary use case using the prototype and demonstrate the support through our approach.",
        "keywords": [
            "University of Regensburg"
        ],
        "uid": "a-vizsec-9220",
        "time_stamp": "2021-10-27T17:20:00Z",
        "has_image": true,
        "paper_award": "",
        "image_caption": "Fabian B\u00f6hm received his master's degree in Management Information Systems within the Honors Elite Program at the University of Regensburg, Germany and the Polytechnic University of Catalonia in Barcelona, Spain, in 2016. Since 2017, he is a Ph.D. candidate and research assistant at the Chair of Information Systems at the University of Regensburg. His research interests cycle around the application of Visual Analytics for cybersecurity. The core research results show the possibilities offered by Visual Analytics in crucial security domains as Cyber Threat lntelligence, ldentity and Access Management, Security Analytics, and Digital Forensics.",
        "external_paper_link": "",
        "has_pdf": true,
        "ff_link": "https://youtu.be/qiaFAMb-v38"
    },
    "a-vizsec-4137": {
        "authors": [
            "Steven Lamarr Reynolds",
            "Tobias Mertz",
            "Steven Arzt",
            "J\u00f6rn Kohlhammer"
        ],
        "title": "User-Centered Design of Visualizations for Software Vulnerability Reports",
        "session_id": "a-vizsec-vizsec-3",
        "abstract": "Today's software systems are created by software development processes that naturally include mistakes, some of which can be exploited by attackers and are therefore called vulnerabilities. Automatic software scanners enable developers to analyze their applications to detect vulnerabilities and alert them of their presence. But often these reports are hard to understand, include false positives or overwhelm users due to the sheer number of alerts, since a report may contain hundreds to thousands of vulnerabilities. Developers must undergo a process called vulnerability triage to find the relevant vulnerabilities to fix. This paper presents two interactive visualizations for developers and security experts to gain an overview of the security state of their application. Users can see the distribution of vulnerabilities, find the most relevant ones, and compare differences between application versions. Our visualization design is inspired by an initial preliminary study and has been evaluated by domain experts to investigate the usability and appropriateness.",
        "keywords": [
            "Fraunhofer IGD, Darmstadt"
        ],
        "uid": "a-vizsec-4137",
        "time_stamp": "2021-10-27T17:40:00Z",
        "has_image": true,
        "paper_award": "",
        "image_caption": "The Report Overview and Report Comparison visualizations displaying the vulnerabilities of an application. On the left, a matrix visualization is showing a single report and the distribution of vulnerabilities across relevant attributes such as severity and origin. The red saturation marks the matrix cells with the largest amount of vulnerabilities. On the right, a unit bar chart shows the differences in vulnerabilities present in two versions of an application. The red and blue colors of units correspond to the individual application versions while the grey color marks vulnerabilities that match between both versions.",
        "external_paper_link": "",
        "has_pdf": true,
        "ff_link": "https://youtu.be/Mu4wcRL0HP8"
    },
    "a-vizsec-2128": {
        "authors": [
            "Frederik L. Dennig",
            "Eren Cakmak",
            "Henrik Plate",
            "Daniel Keim"
        ],
        "title": "VulnEx: Exploring Open-Source Software Vulnerabilities in Large Development Organizations to Understand Risk Exposure (short paper)",
        "session_id": "a-vizsec-vizsec-3",
        "abstract": "The prevalent usage of open-source software (OSS) has led to an increased interest in resolving potential third-party security risks by fixing common vulnerabilities and exposures (CVEs). However, even with automated code analysis tools in place, security analysts often lack the means to obtain an overview of vulnerable OSS reuse in large software organizations. In this design study, we propose VulnEx (Vulnerability Explorer), a tool to audit entire software development organizations. We introduce three complementary table-based representations to identify and assess vulnerability exposures due to OSS, which we designed in collaboration with security analysts. The presented tool allows examining problematic projects and applications (repositories), third-party libraries, and vulnerabilities across a software organization. We show the applicability of our tool through a use case and preliminary expert feedback.",
        "keywords": [
            "University of Konstanz"
        ],
        "uid": "a-vizsec-2128",
        "time_stamp": "2021-10-27T18:00:00Z",
        "has_image": true,
        "paper_award": "",
        "image_caption": "",
        "external_paper_link": "",
        "has_pdf": true,
        "ff_link": "https://youtu.be/9_sWqLtZX6k"
    },
    "v-full-1578": {
        "authors": [
            "Zhutian Chen",
            "Shuainan Ye",
            "Xiangtong Chu",
            "Haijun Xia",
            "Hui Zhang",
            "Huamin Qu",
            "Yingcai Wu"
        ],
        "title": "Augmenting Sports Videos with VisCommentator",
        "session_id": "v-full-full21",
        "abstract": "Visualizing data in sports videos is gaining traction in sports analytics, given its ability to communicate insights and explicate player strategies engagingly. However, augmenting sports videos with such data visualizations is challenging, especially for sports analysts, as it requires considerable expertise in video editing. To ease the creation process, we present a design space that characterizes augmented sports videos at an element-level (what the constituents are) and clip-level (how those constituents are organized). We do so by systematically reviewing 233 examples of augmented sports videos collected from TV channels, teams, and leagues. The design space guides selection of data insights and visualizations for various purposes. Informed by the design space and close collaboration with domain experts, we design VisCommentator, a fast prototyping tool, to eases the creation of augmented table tennis videos by leveraging machine learning-based data extractors and design space-based visualization recommendations. With VisCommentator, sports analysts can create an augmented video by selecting the data to visualize instead of manually drawing the graphical marks. Our system can be generalized to other racket sports (e.g., tennis, badminton) once the underlying datasets and models are available. A user study with seven domain experts shows high satisfaction with our system, confirms that the participants can reproduce augmented sports videos in a short period, and provides insightful implications into future improvements and opportunities.",
        "keywords": [],
        "uid": "v-full-1578",
        "time_stamp": "2021-10-28T13:00:00Z",
        "has_image": false,
        "paper_award": "honorable",
        "image_caption": "",
        "external_paper_link": "",
        "has_pdf": true,
        "ff_link": "https://youtu.be/XyvyPYvd54k"
    },
    "v-full-1135": {
        "authors": [
            "Jiang Wu",
            "Dongyu Liu",
            "Ziyang Guo",
            "Qingyang Xu",
            "Yingcai Wu"
        ],
        "title": "TacticFlow: Visual Analytics of Ever-Changing Tactics in Racket Sports",
        "session_id": "v-full-full21",
        "abstract": "Event sequence mining is often used to summarize patterns from hundreds of sequences but faces special challenges when handling racket sports data. In racket sports (e.g., tennis and badminton), a player hitting the ball is considered a multivariate event consisting of multiple attributes (e.g., hit technique and ball position). A rally (i.e., a series of consecutive hits beginning with one player serving the ball and ending with one player winning a point) thereby can be viewed as a multivariate event sequence. Mining frequent patterns and depicting how patterns change over time is instructive and meaningful to players who want to learn more short-term competitive strategies (i.e., tactics) that encompass multiple hits. However, players in racket sports usually change their tactics rapidly according to the opponent\u2019s reaction, resulting in ever-changing tactic progression. In this work, we introduce a tailored visualization system built on a novel multivariate sequence pattern mining algorithm to facilitate explorative identification and analysis of various tactics and tactic progression. The algorithm can mine multiple non-overlapping multivariate patterns from hundreds of sequences effectively. Based on the mined results, we propose a glyph-based Sankey diagram to visualize the ever-changing tactic progression and support interactive data exploration. Through two case studies with four domain experts in tennis and badminton, we demonstrate that our system can effectively obtain insights about tactic progression in most racket sports. We further discuss the strengths and the limitations of our system based on domain experts\u2019 feedback.",
        "keywords": [],
        "uid": "v-full-1135",
        "time_stamp": "2021-10-28T13:15:00Z",
        "has_image": true,
        "paper_award": "",
        "image_caption": "A screenshot of the user interface for visually analyzing the tactic progression in racket sports. There exist five main views: the Control Bar (A), the Flow View (B), the Tactic View (C), the Projection View (D), and the Rally View (E). An analysis process may be that users first filter the sequences to be analyzed in the Control Bar. Then, the Flow View visualized the tactic progression, and the Projection View projects the tactics into a 2-D plane. Users can further select a tactic to observe it in detail in the Tactic View and the Rally View.",
        "external_paper_link": "",
        "has_pdf": true,
        "ff_link": ""
    },
    "v-full-1218": {
        "authors": [
            "Tan Tang",
            "Yanhong Wu",
            "Lingyun Yu",
            "Yuhong Li",
            "Yingcai Wu"
        ],
        "title": "VideoModerator: A Risk-aware Framework for Multimodal Video Moderation in E-Commerce",
        "session_id": "v-full-full21",
        "abstract": "Video moderation, which refers to remove deviant or explicit content from e-commerce livestreams, has become prevalent owing to social and engaging features. However, this task is tedious and time consuming due to the difficulties associated with watching and reviewing multimodal video content, including video frames and audio clips. To ensure effective video moderation, we propose VideoModerator, a risk-aware framework that seamlessly integrates human knowledge with machine insights. This framework incorporates a set of advanced machine learning models to extract the risk-aware features from multimodal video content and discover potentially deviant videos. Moreover, this framework introduces an interactive visualization interface with three views, namely, a video view, a frame view, and an audio view. In the video view, we adopt a segmented timeline and highlight high-risk periods that may contain deviant information. In the frame view, we present a novel visual summarization method that combines risk-aware features and video context to enable quick video navigation. In the audio view, we employ a storyline-based design to provide a multi-faceted overview which can be used to explore audio content. Furthermore, we report the usage of VideoModerator through a case scenario and conduct experiments and a controlled user study to validate its effectiveness.",
        "keywords": [],
        "uid": "v-full-1218",
        "time_stamp": "2021-10-28T13:30:00Z",
        "has_image": true,
        "paper_award": "",
        "image_caption": "VideoModerator is developed to facilitate the easy moderation of multimodal video content, which is comprised of (a) a video view with a segmented timeline that demonstrates risk distributions; (b) an audio view with a combination of histograms and a storyline-based words visualization; (c) a frame view that summarizes the video frames; (d) a control panel that integrates a color legend encoding four risk categories; (e) and (f) are discovered insights for video moderation; (g) a moving window that visually associates audio and frame content; (h) circular glyphs that visualize risk-aware information.",
        "external_paper_link": "",
        "has_pdf": true,
        "ff_link": ""
    },
    "v-full-1421": {
        "authors": [
            "Junxiu Tang",
            "Yuhua Zhou",
            "Tan Tang",
            "Di Weng",
            "Boyang Xie",
            "Lingyun Yu",
            "Huaqiang Zhang",
            "Yingcai Wu"
        ],
        "title": "A Visualization Approach for Monitoring Order Processing in E-Commerce Warehouse",
        "session_id": "v-full-full21",
        "abstract": "The efficiency of warehouses is vital to e-commerce. Fast order processing at the warehouses ensures timely deliveries and improves customer satisfaction. However, monitoring, analyzing, and manipulating order processing in the warehouses in real time are challenging for traditional methods due to the sheer volume of incoming orders, the fuzzy definition of delayed order patterns, and the complex decision-making of order handling priorities. In this paper, we adopt a data-driven approach and propose OrderMonitor, a visual analytics system that assists warehouse managers in analyzing and improving order processing efficiency in real time based on streaming warehouse event data. Specifically, the order processing pipeline is visualized with a novel pipeline design based on the sedimentation metaphor to facilitate real-time order monitoring and suggest potentially abnormal orders. We also design a novel visualization that depicts order timelines based on the Gantt charts and Marey's graphs. Such a visualization helps the managers gain insights into the performance of order processing and find major blockers for delayed orders. Furthermore, an evaluating view is provided to assist users in inspecting order details and assigning priorities to improve the processing performance. The effectiveness of OrderMonitor is evaluated with two case studies on a real-world warehouse dataset.",
        "keywords": [],
        "uid": "v-full-1421",
        "time_stamp": "2021-10-28T13:45:00Z",
        "has_image": true,
        "paper_award": "",
        "image_caption": "Efficient order processing in the warehouse can ensure timely delivery for e-commerce. However, managing order processing faces unpredictable order streams, fuzzy delay detection, and changeable task assignment. In this work, we propose OrderMonitor, a visual analytics system to support warehouse managers in monitoring real-time status, analyzing historical records, and evaluating the task priorities of order processing.",
        "external_paper_link": "",
        "has_pdf": true,
        "ff_link": "https://youtu.be/GZCKFCVMYFs"
    },
    "v-full-1211": {
        "authors": [
            "Natkamon Tovanich",
            "Nicolas Souli\u00e9",
            "Nicolas Heulot",
            "Petra Isenberg"
        ],
        "title": "MiningVis: Visual Analytics of the Bitcoin Mining Economy",
        "session_id": "v-full-full21",
        "abstract": "We present a visual analytics tool, MiningVis, to explore the long-term historical evolution and dynamics of the Bitcoin mining ecosystem. Bitcoin is a cryptocurrency that attracts much attention but remains difficult to understand. Particularly important to the success, stability, and security of Bitcoin is a component of the system called \"mining.'' Miners are responsible for validating transactions and are incentivized to participate by the promise of a monetary reward. Mining pools have emerged as collectives of miners that ensure a more stable and predictable income. MiningVis aims to help analysts understand the evolution and dynamics of the Bitcoin mining ecosystem, including mining market statistics, multi-measure mining pool rankings, and pool hopping behavior. Each of these features can be compared to external data concerning pool characteristics and Bitcoin news. In order to assess the value of MiningVis, we conducted online interviews and insight-based user studies with Bitcoin miners. We describe research questions tackled and insights made by our participants and illustrate practical implications for visual analytics systems for Bitcoin mining.",
        "keywords": [],
        "uid": "v-full-1211",
        "time_stamp": "2021-10-28T14:00:00Z",
        "has_image": true,
        "paper_award": "",
        "image_caption": "MiningVis is a visual analytics tool to explore the long-term historical evolution and dynamics of the Bitcoin mining ecosystem. Mining is a critical activity in cryptocurrencies that involves multiple longitudinal factors, both internal and external to the Bitcoin ecosystem. MiningVis allows analysts to relate measures in a multi-coordinated view, including mining market statistics, multi-measure mining pool rankings, and pool hopping behavior. Each of these features can be compared to external data concerning pool characteristics and Bitcoin news. The tool helped our economist collaborator discover new findings and is currently used to develop an economic model.",
        "external_paper_link": "",
        "has_pdf": true,
        "ff_link": "https://youtu.be/aPh_QMZ1xfk"
    },
    "v-full-1607": {
        "authors": [
            "Johannes Knittel",
            "Steffen Koch",
            "Tan Tang",
            "Wei Chen",
            "Yingcai Wu",
            "Shixia Liu",
            "Thomas Ertl"
        ],
        "title": "Real-Time Visual Analysis of High-Volume Social Media Posts",
        "session_id": "v-full-full21",
        "abstract": "Breaking news and first-hand reports often trend on social media platforms before traditional news outlets cover them. The real-time analysis of posts on such platforms can reveal valuable and timely insights for journalists, politicians, business analysts, and first responders, but the high number and diversity of new posts pose a challenge. In this work, we present an interactive system that enables the visual analysis of streaming social media data on a large scale in real-time. We propose an efficient and explainable dynamic clustering algorithm that powers a continuously updated visualization of the current thematic landscape as well as detailed visual summaries of specific topics of interest. Our parallel clustering strategy provides an adaptive stream with a digestible but diverse selection of recent posts related to relevant topics. We also integrate familiar visual metaphors that are highly interlinked for enabling both explorative and more focused monitoring tasks. Analysts can gradually increase the resolution to dive deeper into particular topics. In contrast to previous work, our system also works with non-geolocated posts and avoids extensive preprocessing such as detecting events. We evaluated our dynamic clustering algorithm and discuss several use cases that show the utility of our system.",
        "keywords": [],
        "uid": "v-full-1607",
        "time_stamp": "2021-10-28T14:15:00Z",
        "has_image": true,
        "paper_award": "honorable",
        "image_caption": "This work presents an interactive system that enables the visual analysis of streaming social media data on a large scale in real-time. We propose an efficient and explainable dynamic clustering algorithm that powers a continuously updated visualization of the current thematic landscape as well as detailed visual summaries of specific topics of interest. Our parallel clustering strategy provides an adaptive stream with a digestible but diverse selection of recent posts related to relevant topics. We also integrate familiar visual metaphors that are highly interlinked for enabling both explorative and more focused monitoring tasks.",
        "external_paper_link": "",
        "has_pdf": true,
        "ff_link": ""
    },
    "v-full-1209": {
        "authors": [
            "Ehsan Jahangirzadeh Soure",
            "Emily Kuang",
            "Mingming Fan",
            "Jian Zhao"
        ],
        "title": "CoUX: Collaborative Visual Analysis of Think-Aloud Usability Test Videos for Digital Interfaces",
        "session_id": "v-full-full17",
        "abstract": "Reviewing a think-aloud video is both time-consuming and demanding as it requires UX (user experience) professionals to attend to many behavioral signals of the user in the video. Moreover, challenges arise when multiple UX professionals need to collaborate to reduce bias and errors. We propose a collaborative visual analytics tool, CoUX, to facilitate UX evaluators collectively reviewing think-aloud usability test videos of digital interfaces. CoUX seamlessly supports usability problem identification, annotation, and discussion in an integrated environment. To ease the discovery of usability problems, CoUX visualizes a set of problem-indicators based on acoustic, textual, and visual features extracted from the video and audio of a think-aloud session with machine learning.CoUX further enables collaboration amongst UX evaluators for logging, commenting, and consolidating the discovered problems with a chatbox-like user interface. We designed CoUX based on a formative study with two UX experts and insights derived from the literature. We conducted a user study with six pairs of UX practitioners on collaborative think-aloud video analysis tasks. The results indicate that CoUX is useful and effective in facilitating both problem identification and collaborative teamwork. We provide insights into how different features of CoUX were used to support both independent analysis and collaboration. Furthermore, our work highlights opportunities to improve collaborative usability test video analysis.",
        "keywords": [],
        "uid": "v-full-1209",
        "time_stamp": "2021-10-28T13:00:00Z",
        "has_image": true,
        "paper_award": "",
        "image_caption": "CoUX is a collaborative visual analytics tool to support multiple UX evaluators with analyzing think-aloud usability test recordings. From an input video, a video analysis engine extracts various types of features, which are stored on a back-end and presented on a front-end visual interface to facilitate the identification of usability problems among UX evaluators. Moreover, the front-end, consisting of three interactively coordinated panels, communicates with the back-end to support individual problem logging and annotation as well as collaboration amongst a team of UX evaluators.",
        "external_paper_link": "",
        "has_pdf": true,
        "ff_link": "https://youtu.be/36l1zxUS_N4"
    },
    "v-full-1452": {
        "authors": [
            "Kyle Hall",
            "Anthony Kouroupis",
            "Anastasia Bezerianos",
            "Danielle Albers Szafir",
            "Christopher Collins"
        ],
        "title": "Professional Differences: A Comparative Study of Visualization Task Performance and Spatial Ability Across Disciplines",
        "session_id": "v-full-full17",
        "abstract": "Problem-driven visualization work is rooted in deeply understanding the data, actors, processes, and workflows of a target domain. However, an individual's personality traits and cognitive abilities may also influence visualization use. Diverse user needs and abilities raise natural questions for specificity in visualization design: Could individuals from different domains exhibit performance differences when using visualizations? Are any systematic variations related to their cognitive abilities? This study bridges domain-specific perspectives on visualization design with those provided by cognition and perception. We measure variations in visualization task performance across chemistry, computer science, and education, and relate these differences to variations in spatial ability. We conducted an online study with over 60 domain experts consisting of tasks related to pie charts, isocontour plots, and 3D scatterplots, and grounded by a well-documented spatial ability test. Task performance (correctness) varied with profession across more complex visualizations (isocontour plots and scatterplots), but not pie charts, a comparatively common visualization. We found that correctness correlates with spatial ability, and the professions differ in terms of spatial ability. These results indicate that domains differ not only in the specifics of their data and tasks, but also in terms of how effectively their constituent members engage with visualizations and their cognitive traits. Analyzing participants' confidence and strategy comments suggests that focusing on performance neglects important nuances, such as differing approaches to engage with even common visualizations and potential skill transference. Our findings offer a fresh perspective on discipline-specific visualization with specific recommendations to help guide visualization design that celebrates the uniqueness of the disciplines and individuals we seek to serve.",
        "keywords": [],
        "uid": "v-full-1452",
        "time_stamp": "2021-10-28T13:15:00Z",
        "has_image": true,
        "paper_award": "",
        "image_caption": "",
        "external_paper_link": "",
        "has_pdf": true,
        "ff_link": "https://youtu.be/FIkUJnoV6Lg"
    },
    "v-tvcg-9249052": {
        "authors": [
            "Izabela Golebiowska",
            "Arzu \u00c7\u00f6ltekin"
        ],
        "title": "Rainbow Dash: Intuitiveness, Interpretability and Memorability of the Rainbow Color Scheme in Visualization",
        "session_id": "v-full-full17",
        "abstract": "After demonstrating that rainbow colors are still commonly used in scientific publications, we comparatively evaluate the rainbow and sequential color schemes on choropleth and isarithmic maps in an empirical user study with 544 participants to examine if a) people intuitively associate order for the colors in these schemes, b) they can successfully conduct perceptual and semantic map reading and recall tasks with quantitative data where order may have implicit or explicit importance. We find that there is little to no agreement in ordering of rainbow colors while sequential colors are indeed intuitively ordered by the participants with a strong dark is more bias. Sequential colors facilitate most quantitative map reading tasks better than the rainbow colors, whereas rainbow colors competitively facilitate extracting specific values from a map, and may support hue recall better than sequential. We thus contribute to dark- vs. light is more bias debate, and demonstrate why and when rainbow colors may impair performance, and add further nuance to our understanding of this highly popular, yet highly criticized color scheme.",
        "keywords": [
            "Color",
            "visualization",
            "colormap",
            "color perception",
            "visual design"
        ],
        "uid": "v-tvcg-9249052",
        "time_stamp": "2021-10-28T13:30:00Z",
        "has_image": true,
        "paper_award": "",
        "image_caption": "The tested stimuli: Rainbow and sequential color schemes applied on choropleth and isarithmic maps\n",
        "external_paper_link": "https://doi.org/10.1109/TVCG.2020.3035823",
        "has_pdf": false,
        "ff_link": "https://youtu.be/yZ_VvA4mW9g"
    },
    "v-full-1683": {
        "authors": [
            "Paul Parsons"
        ],
        "title": "Understanding Data Visualization Design Practice",
        "session_id": "v-full-full17",
        "abstract": "Professional roles for data visualization designers are growing in popularity, and interest in relationships between the academic research and professional practice communities is gaining traction. However, despite the potential for knowledge sharing between these communities, we have little understanding of the ways in which practitioners design in real-world, professional settings. Inquiry in numerous design disciplines indicates that practitioners approach complex situations in ways that are fundamentally different from those of researchers. In this work, I take a practice-led approach to understanding design practice on its own terms. Twenty data visualization practitioners were interviewed and asked about their design process, including the steps they take, how they make decisions, and the methods they use. Findings suggest that practitioners do not follow highly systematic processes, but instead rely on situated forms of knowing and acting in which they draw from precedent and use methods and principles that are determined appropriate in the moment. These findings have implications for how visualization researchers understand and engage with practitioners, and how educators approach the training of future data visualization designers.",
        "keywords": [],
        "uid": "v-full-1683",
        "time_stamp": "2021-10-28T13:45:00Z",
        "has_image": true,
        "paper_award": "",
        "image_caption": "Professional roles for data visualization designers are growing in popularity, and interest in relationships between the academic research and professional practice communities is gaining traction. However, despite the potential for knowledge sharing between these communities, we have little understanding of the ways in which practitioners design in real-world, professional settings. This work presents a practice-led approach to understanding visualization design practice on its own terms. Twenty data visualization practitioners were interviewed and asked about their design process, including the steps they take, how they make decisions, and the methods they use. Findings have implications for how visualization researchers understand and engage with practitioners, and how educators approach the training of future data visualization designers.\n",
        "external_paper_link": "",
        "has_pdf": true,
        "ff_link": ""
    },
    "v-full-1334": {
        "authors": [
            "Elsie Lee",
            "Shiqing He",
            "Eytan Adar"
        ],
        "title": "Learning Objectives, Insights, and Assessments: How Specification Formats Impact Design",
        "session_id": "v-full-full17",
        "abstract": "Despite the ubiquity of communicative visualizations, specifying communicative intent during design is ad hoc. Whether we are selecting from a set of visualizations, commissioning someone to produce them, or creating them ourselves, an effective way of specifying intent can help guide this process. Ideally, we would have a concise and shared specification language. In previous work, we have argued that communicative intents can be viewed as a learning/assessment problem (i.e., what should the reader learn and what test should they do well on). Learning-based specification formats are linked (e.g., assessments are derived from objectives) but some may more effectively specify communicative intent. Through a large-scale experiment, we studied three specification types: learning objectives, insights, and assessments. Participants, guided by one of these specifications, rated their preferences for a set of visualization designs. Then, we evaluated the set of visualization designs to assess which specification led participants to prefer the most effective visualizations. We find that while all specification types have benefits over no-specification, each format has its own advantages. Our results show that learning objective-based specifications helped participants the most in visualization selection. We also identify situations in which specifications may be insufficient and assessments are vital.",
        "keywords": [],
        "uid": "v-full-1334",
        "time_stamp": "2021-10-28T14:00:00Z",
        "has_image": true,
        "paper_award": "",
        "image_caption": "There are many ways to specify a communicative visualization. We can do it based on learning objectives, insights we want the viewer to have, or assessments we want them to succeed at. In this work, we find that the format of specification impacts the choice of visualization design. In this figure we show examples of the three specifications with the most preferred design identified for each specification. The seven visualization designs are ordered from most effective to least effective.",
        "external_paper_link": "",
        "has_pdf": true,
        "ff_link": ""
    },
    "v-full-1224": {
        "authors": [
            "Lyn Bartram",
            "Michael Correll",
            "Melanie Tory"
        ],
        "title": "Untidy Data: The Unreasonable Effectiveness of Tables",
        "session_id": "v-full-full17",
        "abstract": "Working with data in table form is usually considered a preparatory and tedious step in the sensemaking pipeline; a way of getting the data ready for more sophisticated visualization and analytical tools. But for many people, spreadsheets \u2014 the quintessential table tool \u2014 remain a critical part of their information ecosystem, allowing them to interact with their data in ways that are hidden or abstracted in more complex tools. This is particularly true for data workers [61], people who work with data as part of their job but do not identify as professional analysts or data scientists. We report on a qualitative study of how these workers interact with and reason about their data. Our findings show that data tables serve a broader purpose beyond data cleanup at the initial stage of a linear analytic flow: users want to see and \u201cget their hands on\u201d the underlying data throughout the analytics process, reshaping and augmenting it to support sensemaking. They reorganize, mark up, layer on levels of detail, and spawn alternatives within the context of the base data. These direct interactions and human-readable table representations form a rich and cognitively important part of building understanding of what the data mean and what they can do with it. We argue that interactive tables are an important visualization idiom in their own right; that the direct data interaction they afford offers a fertile design space for visual analytics; and that sense making can be enriched by more flexible human-data interaction than is currently supported in visual analytics tools.",
        "keywords": [],
        "uid": "v-full-1224",
        "time_stamp": "2021-10-28T14:15:00Z",
        "has_image": true,
        "paper_award": "",
        "image_caption": "",
        "external_paper_link": "",
        "has_pdf": true,
        "ff_link": "https://youtu.be/XZl6DopUlDo"
    },
    "v-full-1401": {
        "authors": [
            "Alan Lundgard",
            "Arvind Satyanarayan"
        ],
        "title": "Accessible Visualization via Natural Language Descriptions: A Four-Level Model of Semantic Content",
        "session_id": "v-full-full26",
        "abstract": "Natural language descriptions sometimes accompany visualizations to better communicate and contextualize their insights, and to improve their accessibility for readers with disabilities. However, it is difficult to evaluate the usefulness of these descriptions, and how effectively they improve access to meaningful information, because we have little understanding of the semantic content they convey, and how different readers receive this content. In response, we introduce a conceptual model for the semantic content conveyed by natural language descriptions of visualizations. Developed through a grounded theory analysis of 2,147 sentences, our model spans four levels of semantic content: enumerating visualization construction properties (e.g., marks and encodings); reporting statistical concepts and relations (e.g., extrema and correlations); identifying perceptual and cognitive phenomena (e.g., complex trends and patterns); and elucidating domain-specific insights (e.g., social and political context). To demonstrate how our model can be applied to evaluate the effectiveness of visualization descriptions, we conduct a mixed-methods evaluation with 30 blind and 90 sighted readers, and find that these reader groups differ significantly on which semantic content they rank as most useful. Together, our model and findings suggest that access to meaningful information is strongly reader-specific, and that research in automatic visualization captioning should orient toward descriptions that more richly communicate overall trends and statistics, sensitive to reader preferences. Our work further opens a space of research on natural language as a data interface coequal with visualization.",
        "keywords": [],
        "uid": "v-full-1401",
        "time_stamp": "2021-10-28T13:00:00Z",
        "has_image": true,
        "paper_award": "",
        "image_caption": "Three columns containing various graphics. The first contains the canonical Flatten the Curve coronavirus chart and two textual descriptions of that chart, color-coded according to the four levels of the semantic content model presented in the paper. The second contains a corpus visualization of 2,147 sentences describing charts, also color-coded, and faceted by chart type and difficulty. The third contains two heat maps, corresponding to blind and sighted readers' ranked preferences for the four levels of semantic content, indicating that blind and sighted readers have sharply diverging preferences. An accessible version of the paper is available at vis.mit.edu/pubs/vis-text-model.",
        "external_paper_link": "",
        "has_pdf": true,
        "ff_link": ""
    },
    "v-full-1145": {
        "authors": [
            "Pramod Chundury",
            "Biswaksen Patnaik",
            "Yasmin Reyazuddin",
            "Christine Tang",
            "Jonathan Lazar",
            "Niklas Elmqvist"
        ],
        "title": "Towards Understanding Sensory Substitution for Accessible Visualization: An Interview Study",
        "session_id": "v-full-full26",
        "abstract": "For all its potential in supporting data analysis, particularly in exploratory situations, visualization also creates barriers: accessibility for blind and visually impaired individuals. Regardless of how effective a visualization is, providing equal access for blind users requires a paradigm shift for the visualization research community. To enact such a shift, it is not sufficient to treat visualization accessibility as merely another technical problem to overcome. Instead, supporting the millions of blind and visually impaired users around the world who have equally valid needs for data analysis as sighted individuals requires a respectful, equitable, and holistic approach that includes all users from the onset. In this paper, we draw on accessibility research methodologies to make inroads towards such an approach. We first identify the people who have specific insight into how blind people perceive the world: orientation and mobility (O&M) experts, who are instructors that teach blind individuals how to navigate the physical world using non-visual senses. We interview 10 O&M experts\u2014all of them blind\u2014to understand how best to use sensory substitution other than the visual sense for conveying spatial layouts. Finally, we investigate our qualitative findings using thematic analysis. While blind people in general tend to use both sound and touch to understand their surroundings, we focused on auditory affordances and how they can be used to make data visualizations accessible\u2014using sonification and auralization. However, our experts recommended supporting a combination of senses\u2014sound and touch\u2014to make charts accessible as blind individuals may be more familiar with exploring tactile charts. We report results on both sound and touch affordances, and conclude by discussing implications for accessible visualization for blind individuals.",
        "keywords": [],
        "uid": "v-full-1145",
        "time_stamp": "2021-10-28T13:15:00Z",
        "has_image": true,
        "paper_award": "",
        "image_caption": "This image shows a blind individual with a cane on a sidewalk. The individual is facing a car that is moving in the opposite direction. The image depicts sound waves traveling from the car and traveling from a far away construction site. The individual is using the sound waves and haptic cane feedback to construct a mental map of the layout.",
        "external_paper_link": "",
        "has_pdf": true,
        "ff_link": ""
    },
    "v-full-1500": {
        "authors": [
            "Crescentia Jung",
            "Shubham Mehta",
            "Atharva Kulkarni",
            "Yuhang Zhao",
            "Yea-Seul Kim"
        ],
        "title": "Communicating Visualizations without Visuals: Investigation of Visualization Alternative Text for People with Visual Impairments",
        "session_id": "v-full-full26",
        "abstract": "Alternative text is critical in communicating graphics to people who are blind or have low vision. Especially for graphics that contain rich information, such as visualizations, poorly written or an absence of alternative texts can worsen the information access inequality for people with visual impairments. In this work, we consolidate existing guidelines and survey current practices to inspect to what extent current practices and recommendations are aligned. Then, to gain more insight into what people want in visualization alternative texts, we interviewed 22 people with visual impairments regarding their experience with visualizations and their information needs in alternative texts. The study findings suggest that participants actively try to construct an image of visualizations in their head while listening to alternative texts and wish to carry out visualization tasks (e.g., retrieve specific values) as sighted viewers would. The study also provides ample support for the need to reference the underlying data instead of visual elements to reduce users\u2019 cognitive burden. Informed by the study, we provide a set of recommendations to compose an informative alternative text.",
        "keywords": [],
        "uid": "v-full-1500",
        "time_stamp": "2021-10-28T13:30:00Z",
        "has_image": true,
        "paper_award": "honorable",
        "image_caption": "What information should an alternative text include to describe visualizations, and how does that impact people\u2019s understanding of visualizations? In this work, we consolidate existing guidelines and survey current practices. To gain more insight, we interviewed 22 people with visual impairments regarding their experience with visualizations. We found that participants actively try to construct an image of visualizations in their heads and wish to carry out visualization tasks. Informed by the study, we provide a set of recommendations to compose an informative alternative text.",
        "external_paper_link": "",
        "has_pdf": true,
        "ff_link": "https://youtu.be/JsS1FfH7J8s"
    },
    "v-full-1419": {
        "authors": [
            "Yuyu Luo",
            "Nan Tang",
            "Guoliang Li",
            "Jiawei Tang",
            "Chengliang Chai",
            "Xuedi Qin"
        ],
        "title": "Natural Language to Visualization by Neural Machine Translation",
        "session_id": "v-full-full26",
        "abstract": "Supporting the translation from natural language (NL) to visualization (NL2VIS) can simplify the creation of data visualizations, because if successful, anyone can generate visualizations by their natural language. The state-of-the-art NL2VIS approaches (e.g., NL4DV and FlowSense) are based on semantic parsers and heuristic algorithms, which are not end-to-end and are not designed for supporting (possibly) complex data transformations. Deep neural network powered neural machine translation models have made great strides in many machine translation tasks, which suggests that they might be viable for NL2VIS as well. In this paper, we present ncNet, a Transformer-based sequence-to-sequence model for supporting NL2VIS, with several novel visualization-aware optimizations, including using attention-forcing to optimize the learning process, and visualization-aware rendering to produce better visualization result. To enhance the capability of machine to comprehend natural language queries, ncNet is also designed to take an optional chart template (e.g., a pie chart or a scatter plot) as input, where the chart template will be served as a constraint to limit what could be visualized. We conducted both quantitative evaluation and user study, showing that neural machine translation techniques are easy-to-use and achieve good accuracy that is comparable with the state-of-the-art NL2SQL result.",
        "keywords": [],
        "uid": "v-full-1419",
        "time_stamp": "2021-10-28T13:45:00Z",
        "has_image": true,
        "paper_award": "",
        "image_caption": "",
        "external_paper_link": "",
        "has_pdf": true,
        "ff_link": ""
    },
    "v-full-1490": {
        "authors": [
            "Hendrik Strobelt",
            "Jambay Kinley",
            "Robert Kr\u00fcger",
            "Johanna Beyer",
            "Hanspeter Pfister",
            "Alexander Rush"
        ],
        "title": "GenNI: Human-AI Collaboration for Data-Backed Text Generation",
        "session_id": "v-full-full26",
        "abstract": "Table2Text systems generate textual output based on structured data utilizing machine learning. These systems are essential for fluent natural language interfaces in tools such as virtual assistants; however, left to generate freely these ML systems often produce misleading or unexpected outputs. GenNI (Generation Negotiation Interface) is an interactive visual system for high-level human-AI collaboration in producing descriptive text. The tool utilizes a deep learning model designed with explicit control states. These controls allow users to globally constrain model generations, without sacrificing the representation power of the deep learning models. The visual interface makes it possible for users to interact with AI systems following a Refine-Forecast paradigm to ensure that the generation system acts in a manner human users find suitable. We report multiple use cases on two experiments that improve over uncontrolled generation approaches, while at the same time providing fine-grained control.",
        "keywords": [],
        "uid": "v-full-1490",
        "time_stamp": "2021-10-28T14:00:00Z",
        "has_image": true,
        "paper_award": "",
        "image_caption": "Top: the forecast-refine loop of GenNI to help negotiate constraints between model and human. Bottom: the enabling technology is a modified NLG model to produce control states and text",
        "external_paper_link": "",
        "has_pdf": true,
        "ff_link": ""
    },
    "v-tvcg-9195155": {
        "authors": [
            "Rafael Henkin",
            "Cagatay Turkay"
        ],
        "title": "Words of Estimative Correlation: Studying Verbalizations of Scatterplots",
        "session_id": "v-full-full26",
        "abstract": "Natural language and visualization are being increasingly deployed together for supporting data analysis in different ways, from multimodal interaction to enriched data summaries and insights. Yet, researchers still lack systematic knowledge on how viewers verbalize their interpretations of visualizations, and how they interpret verbalizations of visualizations in such contexts. We describe two studies aimed at identifying characteristics of data and charts that are relevant in such tasks. The first study asks participants to verbalize what they see in scatterplots that depict various levels of orrelations. The second study then asks participants to choose visualizations that match a given verbal description of correlation. We extract key concepts from responses, organize them in a taxonomy and analyze the categorized responses. We observe that participants use a wide range of vocabulary across all scatterplots, but particular concepts are preferred for higher levels of correlation. A comparison between the studies reveals the ambiguity of some of the concepts. We discuss how the results could inform the design of multimodal representations aligned with the data and analytical tasks, and present a research roadmap to deepen the understanding about visualizations and natural language.",
        "keywords": [
            "Information visualization",
            "natural language generation",
            "natural language processing",
            "human-computer interaction"
        ],
        "uid": "v-tvcg-9195155",
        "time_stamp": "2021-10-28T14:15:00Z",
        "has_image": true,
        "paper_award": "",
        "image_caption": "The image show three scatterplots on the left side, a large chart in the middle and eight bar charts on the right side.",
        "external_paper_link": "https://doi.org/10.1109/TVCG.2020.3023537",
        "has_pdf": false,
        "ff_link": "https://youtu.be/2Tx7vYsJbXw"
    },
    "v-tvcg-9405464": {
        "authors": [
            "Hanqi Guo",
            "David Lenz",
            "Jiayi Xu",
            "Xin Liang",
            "Wenbin He",
            "Iulian Grindeanu",
            "Han-Wei Shen",
            "Tom Peterka",
            "Todd Munson",
            "Ian Foster"
        ],
        "title": "FTK: A Simplicial Spacetime Meshing Framework for Robust and Scalable Feature Tracking",
        "session_id": "v-full-full8",
        "abstract": "We present the Feature Tracking Kit (FTK), a framework that simplifies, scales, and delivers various feature-tracking algorithms for scientific data. The key of FTK is our simplicial spacetime meshing scheme that generalizes both regular and unstructured spatial meshes to spacetime while tessellating spacetime mesh elements into simplices. The benefits of using simplicial spacetime meshes include (1) reducing ambiguity cases for feature extraction and tracking, (2) simplifying the handling of degeneracies using symbolic perturbations, and (3) enabling scalable and parallel processing. The use of simplicial spacetime meshing simplifies and improves the implementation of several feature-tracking algorithms for critical points, quantum vortices, and isosurfaces. As a software framework, FTK provides end users with VTK/ParaView filters, Python bindings, a command line interface, and programming interfaces for feature-tracking applications. We demonstrate use cases as well as scalability studies through both synthetic data and scientific applications including tokamak, fluid dynamics, and superconductivity simulations. We also conduct end-to-end performance studies on the Summit supercomputer. FTK is open sourced under the MIT license: https://github.com/hguo/ftk.",
        "keywords": [
            "Feature tracking",
            "spacetime meshing",
            "distributed and parallel processing",
            "critical points",
            "isosurfaces",
            "vortices."
        ],
        "uid": "v-tvcg-9405464",
        "time_stamp": "2021-10-28T13:00:00Z",
        "has_image": false,
        "paper_award": "",
        "image_caption": "",
        "external_paper_link": "https://doi.org/10.1109/TVCG.2021.3073399",
        "has_pdf": false,
        "ff_link": "https://youtu.be/K2evrAG9F4M"
    },
    "v-tvcg-9359504": {
        "authors": [
            "Jules Vidal",
            "Pierre Guillou",
            "Julien Tierny"
        ],
        "title": "A Progressive Approach to Scalar Field Topology",
        "session_id": "v-full-full8",
        "abstract": "This paper introduces progressive algorithms for the topological analysis of scalar data. Our approach is based on a hierarchical representation of the input data and the fast identification of topologically invariant vertices, which are vertices that have no impact on the topological description of the data and for which we show that no computation is required as they are introduced in the hierarchy. This enables the definition of efficient coarse-to-fine topological algorithms, which leverage fast update mechanisms for ordinary vertices and avoid computation for the topologically invariant ones. We demonstrate our approach with two examples of topological algorithms (critical point extraction and persistence diagram computation), which generate interpretable outputs upon interruption requests and which progressively refine them otherwise. Experiments on real-life datasets illustrate that our progressive strategy, in addition to the continuous visual feedback it provides, even improves run time performance with regard to non-progressive algorithms and we describe further accelerations with shared-memory parallelism. We illustrate the utility of our approach in batch-mode and interactive setups, where it respectively enables the control of the execution time of complete topological pipelines as well as previews of the topological features found in a dataset, with progressive updates delivered within interactive times.",
        "keywords": [
            "Topological data analysis",
            "scalar data",
            "progressive visualization."
        ],
        "uid": "v-tvcg-9359504",
        "time_stamp": "2021-10-28T13:15:00Z",
        "has_image": true,
        "paper_award": "",
        "image_caption": "We present fast coarse-to-fine algorithms for the progressive computation of\ntopological representations of a scalar field, namely the critical points and\nthe persistence diagram. Our method enables the fast and interactive exploration of\nthe topological features of a scalar field.\n",
        "external_paper_link": "https://doi.org/10.1109/TVCG.2021.3060500",
        "has_pdf": false,
        "ff_link": "https://youtu.be/waUKWkX38tg"
    },
    "v-tvcg-9420248": {
        "authors": [
            "Kilian Werner",
            "Christoph Garth"
        ],
        "title": "Unordered Task-Parallel Augmented Merge Tree Construction",
        "session_id": "v-full-full8",
        "abstract": "Contemporary scientific data sets require fast and scalable topological analysis to enable visualization, simplification and interaction. Within this field, parallel merge tree construction has seen abundant recent contributions, with a trend of decentralized, task-parallel or SMP-oriented algorithms dominating in terms of total runtime. However, none of these recent approaches computed complete merge trees on distributed systems, leaving this field to traditional divide and conquer approaches. This paper introduces a scalable, parallel and distributed algorithm for merge tree construction outperforming the previously fastest distributed solution by a factor of around three. This is achieved by a task-parallel identification of individual merge tree arcs by growing regions around critical points in the data, without any need for ordered progression or global data structures, based on a novel insight introducing a sufficient local boundary for region growth.",
        "keywords": [
            "I.3.1.d Parallel processing I.3.2.a Distributed/network graphics I.6.9 Visualization Topological Analysis"
        ],
        "uid": "v-tvcg-9420248",
        "time_stamp": "2021-10-28T13:30:00Z",
        "has_image": true,
        "paper_award": "",
        "image_caption": "The Join Tree is constructed by growing regions of \nexclusively monotone reachable vertices from minima\nand identifying inner nodes as the smallest valued\nboundary vertices around these regions. The image\nshows leaf edges embedded in the domain with their\naugmentation colored accordingly. High order nodes\nare currently treated like minima to crawl up the \ntree.\n",
        "external_paper_link": "https://doi.org/10.1109/TVCG.2021.3076875",
        "has_pdf": false,
        "ff_link": "https://youtu.be/SFS-OVu206M"
    },
    "v-full-1163": {
        "authors": [
            "Mathieu Pont",
            "Jules Vidal",
            "Julie Delon",
            "Julien Tierny"
        ],
        "title": "Wasserstein Distances, Geodesics and Barycenters of Merge Trees",
        "session_id": "v-full-full8",
        "abstract": "This paper presents a unified computational framework for the estimation of distances, geodesics and barycenters of merge trees. We extend recent work on the edit distance and introduce a new metric, called the Wasserstein distance between merge trees, which is purposely designed to enable efficient computations of geodesics and barycenters. Specifically, our new distance is strictly equivalent to the L 2 -Wasserstein distance between extremum persistence diagrams, but it is restricted to a smaller solution space, namely, the space of rooted partial isomorphisms between branch decomposition trees. This enables a simple extension of existing optimization frameworks for geodesics and barycenters from persistence diagrams to merge trees. We introduce a task-based algorithm which can be generically applied to distance, geodesic, barycenter or cluster computation. The task-based nature of our approach enables further accelerations with shared-memory parallelism. Extensive experiments on public ensembles and SciVis contest benchmarks demonstrate the efficiency of our approach \u2013 with barycenter computations in the orders of minutes for the largest examples \u2013 as well as its qualitative ability to generate representative barycenter merge trees, visually summarizing the features of interest found in the ensemble. We show the utility of our contributions with dedicated visualization applications: feature tracking, temporal reduction and ensemble clustering. We provide a lightweight C++ implementation that can be used to reproduce our results.",
        "keywords": [],
        "uid": "v-full-1163",
        "time_stamp": "2021-10-28T13:45:00Z",
        "has_image": true,
        "paper_award": "",
        "image_caption": "The merge trees of three members (a-c) of the Isabel ensemble (wind velocity) concisely and visually encode the number, the salience and the connectivity of the features of interest found in the data. The pointwise mean for the three members (d) exhibits 5 salient maxima and its merge tree is not representative of the input trees, in contrast of the Wasserstein barycenter tree (e). Our framework for distances, geodesics and barycenters enables a variety of applications, including (f) feature tracking, (g) temporal reduction and (h) ensemble clustering and summarization of the main trends of features found in the ensemble.\n",
        "external_paper_link": "",
        "has_pdf": true,
        "ff_link": "https://youtu.be/ifVvH0uMW6k"
    },
    "v-tvcg-9372897": {
        "authors": [
            "Hamish Carr",
            "Oliver R\u00fcbel",
            "Gunther Weber",
            "James Ahrens"
        ],
        "title": "Optimization and Augmentation for Data Parallel Contour Trees",
        "session_id": "v-full-full8",
        "abstract": "Contour trees are used for topological data analysis in scientific visualization. While originally computed with serial algorithms, recent work has introduced a vector-parallel algorithm. However, this algorithm is relatively slow for fully augmented contour trees which are needed for many practical data analysis tasks. We therefore introduce a representation called the hyperstructure that enables efficient searches through the contour tree and use it to construct a fully augmented contour tree in data parallel, with performance on average 6 times faster than the state-of-the-art parallel algorithm in the TTK topological toolkit.",
        "keywords": [
            "Computational Topology",
            "Contour Tree",
            "Parallel Algorithms"
        ],
        "uid": "v-tvcg-9372897",
        "time_stamp": "2021-10-28T14:00:00Z",
        "has_image": true,
        "paper_award": "",
        "image_caption": "Parallel acceleration of contour tree augmentation uses a hyperstructure such as this one computed for a small part of the landscape around Vancouver.",
        "external_paper_link": "https://doi.org/10.1109/TVCG.2021.3064385",
        "has_pdf": false,
        "ff_link": "https://youtu.be/DXUTqnzLESE"
    },
    "v-full-1011": {
        "authors": [
            "Yu Qin",
            "Brittany Terese Fasy",
            "Carola Wenk",
            "Brian Summa"
        ],
        "title": "A Domain-Oblivious Approach for Learning Concise Representations of Filtered Topological Spaces for Clustering",
        "session_id": "v-full-full8",
        "abstract": "Persistence diagrams have been widely used to quantify the underlying features of filtered topological spaces in data visualization. In many applications, computing distances between diagrams is essential; however, computing these distances has been challenging due to the computational cost. In this paper, we propose a persistence diagram hashing framework that learns a binary code representation of persistence diagrams, which allows for fast computation of distances. This framework is built upon a generative adversarial network (GAN) with a diagram distance loss function to steer the learning process. Instead of using standard representations, we hash diagrams into binary codes, which have natural advantages in large-scale tasks. The training of this model is domain-oblivious in that it can be computed purely from synthetic, randomly created diagrams. As a consequence, our proposed method is directly applicable to various datasets without the need for retraining the model. These binary codes, when compared using fast Hamming distance, better maintain topological similarity properties between datasets than other vectorized representations. To evaluate this method, we apply our framework to the problem of diagram clustering and we compare the quality and performance of our approach to the state-of-the-art. In addition, we show the scalability of our approach on a dataset with 10k persistence diagrams, which is not possible with current techniques. Moreover, our experimental results demonstrate that our method is significantly faster with the potential of less memory usage, while retaining comparable or better quality comparisons.",
        "keywords": [
            "Topological data analysis",
            "Persistence diagrams",
            "Persistence diagram distances",
            "Learned hashing",
            "Clustering"
        ],
        "uid": "v-full-1011",
        "time_stamp": "2021-10-28T14:15:00Z",
        "has_image": true,
        "paper_award": "",
        "image_caption": "We provided the first approach to generate a learned binary topological representation for clustering that is domain-oblivious in training. This new machine learning approach will reduce comparison times from potentially hours to milliseconds and potentially reducing storage.",
        "external_paper_link": "",
        "has_pdf": true,
        "ff_link": "https://youtu.be/7uGtGV1G7qc"
    },
    "v-tvcg-9174891": {
        "authors": [
            "Jian Zhao",
            "Mingming Fan",
            "Mi Feng"
        ],
        "title": "ChartSeer: Interactive Steering Exploratory Visual Analysis with Machine Intelligence",
        "session_id": "v-full-full4",
        "abstract": "During exploratory visual analysis (EVA), analysts need to continually determine which subsequent activities to perform, such as which data variables to explore or how to present data variables visually. Due to the vast combinations of data variables and visual encodings that are possible, it is often challenging to make such decisions. Further, while performing local explorations, analysts often fail to attend to the holistic picture that is emerging from their analysis, leading them to improperly steer their EVA. These issues become even more impactful in the real world analysis scenarios where EVA occurs in multiple asynchronous sessions that could be completed by one or more analysts. To address these challenges, this work proposes ChartSeer, a system that uses machine intelligence to enable analysts to visually monitor the current state of an EVA and effectively identify future activities to perform. ChartSeer utilizes deep learning techniques to characterize analyst-created data charts to generate visual summaries and recommend appropriate charts for further exploration based on user interactions. A case study was first conducted to demonstrate the usage of ChartSeer in practice, followed by a controlled study to compare ChartSeer\u2019s performance with a baseline during EVA tasks. The results demonstrated that ChartSeer enables analysts to adequately understand current EVA status and advance their analysis by creating charts with increased coverage and visual encoding diversity.",
        "keywords": [
            "Exploratory visual analysis",
            "interactive steering",
            "visualization recommendation",
            "machine learning"
        ],
        "uid": "v-tvcg-9174891",
        "time_stamp": "2021-10-28T15:00:00Z",
        "has_image": true,
        "paper_award": "",
        "image_caption": "This work proposes ChartSeer, a system that uses machine intelligence to enable analysts to visually monitor the current state of an exploratory data analysis and effectively identify future activities to perform. ChartSeer utilizes deep learning techniques to characterize analyst-created data charts to generate visual summaries and recommend appropriate charts for further exploration based on user interactions.",
        "external_paper_link": "https://doi.org/10.1109/TVCG.2020.3018724",
        "has_pdf": false,
        "ff_link": "https://youtu.be/bjV0hNO9T2M"
    },
    "v-tvcg-9350177": {
        "authors": [
            "Alex B\u00e4uerle",
            "Christian van Onzenoodt",
            "Timo Ropinski"
        ],
        "title": "Net2Vis - A Visual Grammar for Automatically Generating Publication-Tailored CNN Architecture Visualizations",
        "session_id": "v-full-full4",
        "abstract": "To convey neural network architectures in publications, appropriate visualizations are of great importance. While most current deep learning papers contain such visualizations, these are usually handcrafted just before publication, which results in a lack of a common visual grammar, significant time investment, errors, and ambiguities. Current automatic network visualization tools focus on debugging the network itself and are not ideal for generating publication visualizations. Therefore, we present an approach to automate this process by translating network architectures specified in Keras into visualizations that can directly be embedded into any publication. To do so, we propose a visual grammar for convolutional neural networks (CNNs), which has been derived from an analysis of such figures extracted from all ICCV and CVPR papers published between 2013 and 2019. The proposed grammar incorporates visual encoding, network layout, layer aggregation, and legend generation. We have further realized our approach in an online system available to the community, which we have evaluated through expert feedback, and a quantitative study. It not only reduces the time needed to generate network visualizations for publications, but also enables a unified and unambiguous visualization design.",
        "keywords": [
            "Neural networks",
            "architecture visualization",
            "graph layouting"
        ],
        "uid": "v-tvcg-9350177",
        "time_stamp": "2021-10-28T15:15:00Z",
        "has_image": true,
        "paper_award": "",
        "image_caption": "Title slide for a presetation called \"Net2Vis - A Visual Grammar for Automatically Generating Publication-Tailored CNN Architecture Visualizations.\nIn the bottom left, one can see an image of the speaker, and two links: viscom.net2vis.uni-ulm.de and a13x.io.\nOn the top right, one can see a logo depicting an eye, and the the affiliation of the author: Visual Computing Group, Ulm University.",
        "external_paper_link": "https://doi.org/10.1109/TVCG.2021.3057483",
        "has_pdf": false,
        "ff_link": "https://youtu.be/j8oUQ8EbyGw"
    },
    "v-full-1274": {
        "authors": [
            "Sehi L'Yi",
            "Qianwen Wang",
            "Fritz Lekschas",
            "Nils Gehlenborg"
        ],
        "title": "Gosling: A Grammar-based Toolkit for Scalable and Interactive Genomics Data Visualization",
        "session_id": "v-full-full4",
        "abstract": "The combination of diverse data types and analysis tasks in genomics has resulted in the development of a wide range of visualization techniques and tools. However, most existing tools are tailored to a specific problem or data type and offer limited customization, making it challenging to optimize visualizations for new analysis tasks or datasets. To address this challenge, we designed Gosling\u2014a grammar for interactive and scalable genomics data visualization. Gosling balances expressiveness for comprehensive multi-scale genomics data visualizations with accessibility for domain scientists. Our accompanying JavaScript toolkit called Gosling.js provides scalable and interactive rendering. Gosling.js is built on top of an existing platform for web-based genomics data visualization to further simplify the visualization of common genomics data formats. We demonstrate the expressiveness of the grammar through a variety of real-world examples. Furthermore, we show how Gosling supports the design of novel genomics visualizations. An online editor and examples of Gosling.js, its source code, and documentation are available at https://gosling.js.org.",
        "keywords": [],
        "uid": "v-full-1274",
        "time_stamp": "2021-10-28T15:30:00Z",
        "has_image": true,
        "paper_award": "",
        "image_caption": "A wide range of genomics visualizations in the wild can be expressed by Gosling and rendered by Gosling.js, a declarative grammar and its JavaScript toolkit for scalable and interactive visualizations for genome-mapped data.\n",
        "external_paper_link": "",
        "has_pdf": true,
        "ff_link": "https://youtu.be/Wvyb9dus9eU"
    },
    "v-full-1504": {
        "authors": [
            "Carla Floricel",
            "Md Nafiul Nipu",
            "Mikayla Biggs",
            "Andrew Wentzel",
            "Guadalupe Canahuate",
            "Lisanne van Dijk",
            "Abdallah Mohamed",
            "Clifton David Fuller",
            "G. Elisabeta Marai"
        ],
        "title": "THALIS: Human-Machine Analysis of Longitudinal Symptoms in Cancer Therapy",
        "session_id": "v-full-full4",
        "abstract": "Although cancer patients survive years after oncologic therapy, they are plagued with long-lasting or permanent residual symptoms, whose severity, rate of development, and resolution after treatment vary largely between survivors. The analysis and interpretation of symptoms is complicated by their partial co-occurrence, variability across populations and across time, and, in the case of cancers that use radiation, by further symptom dependency on the prescribed therapy. We describe an integrated environment for visual analysis and knowledge discovery from cancer therapy symptom data, developed in close collaboration with oncology experts. Our visual environment leverages unsupervised machine learning methodology over cohorts of patients, and, in conjunction with custom visual encodings and interactions, provides context for new patients based on patients with similar diagnostic features and symptom evolution. We evaluate this environment on data collected from a cohort of head and neck cancer patients. Feedback from our clinician collaborators indicates that this environment supports knowledge discovery beyond the limits of machines or humans alone, and that it serves as a valuable tool in both the clinic and symptom research.",
        "keywords": [],
        "uid": "v-full-1504",
        "time_stamp": "2021-10-28T15:45:00Z",
        "has_image": true,
        "paper_award": "",
        "image_caption": "THALIS is a mixed human-machine system for the analysis of longitudinal cancer symptom data. \nAssociation rule diagrams capture symptom clusters in the acute (A.1) and late (A.2) stages of treatment. \nFilament plots show symptom trajectories and emphasize the cohort progress based on 4 therapy plans for mucus (B.1), and for each patient for swallowing (B.2). \nA percentile heatmap provides an abstract summary for the cohort\u2019s symptom data over time (C), with black markers denoting the current patient.",
        "external_paper_link": "",
        "has_pdf": true,
        "ff_link": "https://youtu.be/HXuYC6NimSg"
    },
    "v-full-1035": {
        "authors": [
            "Aoyu Wu",
            "Yun Wang",
            "Mengyu Zhou",
            "Xinyi He",
            "Haidong Zhang",
            "Huamin Qu",
            "Dongmei Zhang"
        ],
        "title": "MultiVision: Designing Analytical Dashboards with Deep Learning Based Recommendation",
        "session_id": "v-full-full4",
        "abstract": "We contribute a deep-learning-based method that assists in designing analytical dashboards for analyzing a data table.\n  Given a data table, data workers usually need to experience a tedious and time-consuming process to select meaningful combinations of data columns for creating charts. This process is further complicated by the needs of creating dashboards composed of multiple views that unveil different perspectives of data. Existing automated approaches for recommending multiple-view visualizations mainly build on manually crafted design rules, producing sub-optimal or irrelevant suggestions. To address this gap, we present a deep learning approach for selecting data columns and recommending multiple charts. More importantly, we integrate the deep learning models into a mixed-initiative system. Our model could make recommendations given optional user-input selections of data columns. The model, in turn, learns from provenance data of authoring logs in an offline manner. We compare our deep learning model with existing methods for visualization recommendation and conduct a user study to evaluate the usefulness of the system.",
        "keywords": [],
        "uid": "v-full-1035",
        "time_stamp": "2021-10-28T16:00:00Z",
        "has_image": true,
        "paper_award": "",
        "image_caption": "The interface: (A) When uploading a tabular dataset, the Table Field shows data columns with the corresponding data type; (B) Clicking the Multiple-View Recommender will generate a dashboard containing a user-specified number of charts. The recommendation is conditioned on, if any, user-locked charts; (C) Users could specify the chart type, encoding channels, and data transformation in the Chart Editor; (D) Chart Ideas recommends charts based on the current dashboard; (E) The Dashboard View displays the charts in an interactive grid layout; and (F) Users could restore the history, change the theme, and save logs in Control Panel.",
        "external_paper_link": "",
        "has_pdf": true,
        "ff_link": ""
    },
    "v-tvcg-9444198": {
        "authors": [
            "Changjian Chen",
            "Zhaowei Wang",
            "Jing Wu",
            "Xiting Wang",
            "Lan-Zhe Guo",
            "Yu-Feng Li",
            "Shixia Liu"
        ],
        "title": "Interactive Graph Construction for Graph-Based Semi-Supervised Learning",
        "session_id": "v-full-full4",
        "abstract": "Semi-supervised learning (SSL) provides a way to improve the performance of prediction models (e.g., classifier) via the usage of unlabeled samples. An effective and widely used method is to construct a graph that describes the relationship between labeled and unlabeled samples. Practical experience indicates that graph quality significantly affects the model performance. In this paper, we present a visual analysis method that interactively constructs a high-quality graph for better model performance. In particular, we propose an interactive graph construction method based on the large margin principle. We have developed a river visualization and a hybrid visualization that combines a scatterplot, a node-link diagram, and a bar chart, to convey the label propagation of graph-based SSL. Based on the understanding of the propagation, a user can select regions of interest to inspect and modify the graph. We conducted two case studies to showcase how our method facilitates the exploitation of labeled and unlabeled samples for improving model performance.",
        "keywords": [
            "Semi-supervised learning",
            "unlabeled samples",
            "graph quality"
        ],
        "uid": "v-tvcg-9444198",
        "time_stamp": "2021-10-28T16:15:00Z",
        "has_image": true,
        "paper_award": "",
        "image_caption": "DataLinker: (a) the Filtering panel helps focus on nodes and edges of interest; (b) the Label Change view shows label changes as an evolving river; (c) the Sample view displays samples as a combination of a scatterplot, a node-link diagram, and a bar chart; (d) the Action Trail records the modification history; (e) the Information view shows the image content of selected samples and their nearest neighbors.",
        "external_paper_link": "https://doi.org/10.1109/TVCG.2021.3084694",
        "has_pdf": false,
        "ff_link": "https://youtu.be/OlVUHKMGpTE"
    },
    "v-full-1524": {
        "authors": [
            "Suphanut Jamonnak",
            "Ye Zhao",
            "Xinyi Huang",
            "Md Amiruzzaman"
        ],
        "title": "Geo-Context Aware Study of Vision-Based Autonomous Driving Models and Spatial Video Data",
        "session_id": "v-full-full25",
        "abstract": "Vision-based deep learning (DL) methods have achieved success in learning autonomous driving models from large scale crowd-sourced video datasets. They are trained to predict instantaneous driving behaviors from video data captured by on-vehicle cameras. In this paper, we develop a geo-context aware visualization system for the study of Autonomous Driving Model (ADM) predictions together with large scale ADM video data. The visual study is seamlessly integrated with the geographical environment by combining DL model performance with geospatial visualization techniques. Model performance measures can be studied together with a set of geo-spatial attributes over map views. Users can also discover and compare prediction behaviors of multiple DL models in both city-wide and street-level analysis, together with road images and video contents. Therefore, the system provides a new visual exploration platform for DL model designers in autonomous driving. Use cases and domain expert evaluation show the utility and effectiveness of the visualization system.",
        "keywords": [],
        "uid": "v-full-1524",
        "time_stamp": "2021-10-28T15:00:00Z",
        "has_image": true,
        "paper_award": "",
        "image_caption": "Vision-based deep learning methods have made great progress in learning autonomous driving models from large-scale crowd-sourced video datasets. They are trained to predict instantaneous driving behaviors from video data captured by on-vehicle cameras. We develop a geo-context aware visualization system for the study of Autonomous Driving Model predictions together with large-scale ADM video data. The visual study is seamlessly integrated with the geographical environment by combining deep learning model performance with geospatial visualization techniques. The system provides a new visual exploration platform for DL model designers in autonomous driving.",
        "external_paper_link": "",
        "has_pdf": true,
        "ff_link": "https://youtu.be/Y8bN-6G3Vko"
    },
    "v-full-1480": {
        "authors": [
            "Yijie Hou",
            "Chengshun Wang",
            "Junhong Wang",
            "Xiangyang Xue",
            "Xiaolong Zhang",
            "Jun Zhu",
            "Dongliang Wang",
            "Siming Chen"
        ],
        "title": "Visual Evaluation for Autonomous Driving",
        "session_id": "v-full-full25",
        "abstract": "Autonomous driving technologies often use state-of-the-art artificial intelligence algorithms to understand the relationship between the vehicle and the external environment, to predict the changes of the environment, and then to plan and control the behaviors of the vehicle accordingly. The complexity of such technologies makes it challenging to evaluate the performance of autonomous driving systems and to find ways to improve them. The current approaches to evaluating such autonomous driving systems largely use a single score to indicate the overall performance of a system, but domain experts have difficulties in understanding how individual components or algorithms in an autonomous driving system may contribute to the score. To address this problem, we collaborate with domain experts on autonomous driving algorithms, and propose a visual evaluation method for autonomous driving. Our method considers the data generated in all components during the whole process of autonomous driving, including perception results, planning routes, prediction of obstacles, various controlling parameters, and evaluation of comfort. We develop a visual analytics workflow to integrate an evaluation mathematical model with adjustable parameters, support the evaluation of the system from the level of the overall performance to the level of detailed measures of individual components, and to show both evaluation scores and their contributing factors. Our implemented visual analytics system provides an overview evaluation score at the beginning and shows the animation of the dynamic change of the scores at each period. Experts can interactively explore the specific component at different time periods and identify related factors. With our method, domain experts not only learn about the performance of an autonomous driving system, but also identify and access the problematic parts of each component. Our visual evaluation system can be applied to the autonomous driving simulation system and used for various evaluation cases. The results of using our system in some simulation cases and the feedback from involved domain experts confirm the usefulness and efficiency of our method in helping people gain in-depth insight into autonomous driving systems.",
        "keywords": [],
        "uid": "v-full-1480",
        "time_stamp": "2021-10-28T15:15:00Z",
        "has_image": true,
        "paper_award": "",
        "image_caption": "System User Interface: (a) Spatial-temporal view to display a simulation scenario of autonomous driving; (b) Radar view to present the evaluation scores of five modules of autonomous driving and an overall evaluation score at a given time; (c) Timeline view to show the scores over time; (d) Parallel coordinates view to display both the score (d-1) and factor value (d-3) of each factor from modules, with customizable ranking settings for different factor priorities (d-2); (e) Visualization of the autonomous driving states on speed, acceleration, wheel turning angle (e-1), as well as the type and priority distributions of obstacles (e-2).",
        "external_paper_link": "",
        "has_pdf": true,
        "ff_link": "https://youtu.be/r2DuL5JMX_0"
    },
    "v-full-1517": {
        "authors": [
            "Wenbin He",
            "Lincan Zou",
            "Shekar Arvind Kumar",
            "Liang Gou",
            "Liu Ren"
        ],
        "title": "Where Can We Help? A Visual Analytics Approach to Diagnosing and Improving Semantic Segmentation of Movable Objects",
        "session_id": "v-full-full25",
        "abstract": "Semantic segmentation is a critical component in autonomous driving and has to be thoroughly evaluated due to safety concerns. Deep neural network (DNN) based semantic segmentation models are widely used in autonomous driving. However, it is challenging to evaluate DNN-based models due to their black-box-like nature, and it is even more difficult to assess model performance for crucial objects, such as lost cargos and pedestrians, in autonomous driving applications. In this work, we propose VASS, a Visual Analytics approach to diagnosing and improving the accuracy and robustness of Semantic Segmentation models, especially for critical objects moving in various driving scenes. The key component of our approach is a context-aware spatial representation learning that extracts important spatial information of objects, such as position, size, and aspect ratio, with respect to given scene contexts. Based on this spatial representation, we first use it to create visual summarization to analyze models\u2019 performance. We then use it to guide the generation of adversarial examples to evaluate models\u2019 spatial robustness and obtain actionable insights. We demonstrate the effectiveness of VASS via two case studies of lost cargo detection and pedestrian detection in autonomous driving. For both cases, we show quantitative evaluation on the improvement of models\u2019 performance with actionable insights obtained from VASS.",
        "keywords": [],
        "uid": "v-full-1517",
        "time_stamp": "2021-10-28T15:30:00Z",
        "has_image": true,
        "paper_award": "",
        "image_caption": "We propose the first visual analytics framework for diagnosing and improving deep semantic segmentation models over critical objects in autonomous driving. Our approach focuses on analyzing models' performance with respect to objects' spatial and contextual information, such as position, size, and interaction with the surrounding context. Moreover, our approach can identify models' potential vulnerabilities regarding objects' spatial information and derive actionable insights to improve models' accuracy and spatial robustness.\n",
        "external_paper_link": "",
        "has_pdf": true,
        "ff_link": "https://youtu.be/8vEHnlzLMes"
    },
    "v-full-1181": {
        "authors": [
            "Zikun Deng",
            "Di Weng",
            "Xiao Xie",
            "Jie Bao",
            "Yu Zheng",
            "Mingliang Xu",
            "Wei Chen",
            "Yingcai Wu"
        ],
        "title": "Compass: Towards Better Causal Analysis of Urban Time Series",
        "session_id": "v-full-full25",
        "abstract": "The spatial time series generated by city sensors allow us to observe urban phenomena like environmental pollution and traffic congestion at an unprecedented scale. However, recovering causal relations from these observations to explain the sources of urban phenomena remains a challenging task because these causal relations tend to be time-varying and demand proper time series partitioning for effective analyses. The prior approaches extract one causal graph given long-time observations, which cannot be directly applied to capturing, interpreting, and validating dynamic urban causality. This paper presents Compass, a novel visual analytics approach for in-depth analyses of the dynamic causality in urban time series. To develop Compass, we identify and address three challenges: detecting urban causality, interpreting dynamic causal relations, and unveiling suspicious causal relations. First, multiple causal graphs over time among urban time series are obtained with a causal detection framework extended from the Granger causality test. Then, a dynamic causal graph visualization is designed to reveal the time-varying causal relations across these causal graphs and facilitate the exploration of the graphs along the time. Finally, a tailored multi-dimensional visualization is developed to support the identification of spurious causal relations, thereby improving the reliability of causal analyses. The effectiveness of Compass is evaluated with two case studies conducted on the real-world urban datasets, including the air pollution and traffic speed datasets, and positive feedback was received from domain experts.",
        "keywords": [],
        "uid": "v-full-1181",
        "time_stamp": "2021-10-28T15:45:00Z",
        "has_image": true,
        "paper_award": "",
        "image_caption": "This study presents Compass, a novel visual analytics approach that assists analysts in detecting and analyzing dynamic causalities in urban domains. We first extend the Granger causality test and propose a causal detection framework for dynamic urban causality. We then develop Compass by coordinating a set of well-designed visualizations. Compass facilitates analysts to interpret dynamic causal relations and improve causal detection results.",
        "external_paper_link": "",
        "has_pdf": true,
        "ff_link": "https://youtu.be/XQpSA3iRs7k"
    },
    "v-tvcg-9397369": {
        "authors": [
            "Zikun Deng",
            "Di Weng",
            "Yuxuan Liang",
            "Jie Bao",
            "Yu Zheng",
            "Tobias Schreck",
            "Mingliang Xu",
            "Yingcai Wu"
        ],
        "title": "Visual Cascade Analytics of Large-scale Spatiotemporal Data",
        "session_id": "v-full-full25",
        "abstract": "Many spatiotemporal events can be viewed as contagions. These events implicitly propagate across space and time by following cascading patterns, expanding their influence, and generating event cascades that involve multiple locations. Analyzing such cascading processes presents valuable implications in various urban applications, such as traffic planning and pollution diagnostics. Motivated by the limited capability of the existing approaches in mining and interpreting cascading patterns, we propose a visual analytics system called VisCas. VisCas combines an inference model with interactive visualizations and empowers analysts to infer and interpret the latent cascading patterns in the spatiotemporal context. To develop VisCas, we address three major challenges, 1) generalized pattern inference, 2) implicit influence visualization, and 3) multifaceted cascade analysis. For the first challenge, we adapt the state-of-the-art cascading network inference technique to general urban scenarios, where cascading patterns can be reliably inferred from large-scale spatiotemporal data. For the second and third challenges, we assemble a set of effective visualizations to support location navigation, influence inspection, and cascading exploration, and facilitate the in-depth cascade analysis. We design a novel influence view based on a three-fold optimization strategy for analyzing the implicit influences of the inferred patterns. We demonstrate the capability and effectiveness of VisCas with two case studies conducted on real-world traffic congestion and air pollution datasets with domain experts.",
        "keywords": [
            "Spatial cascade",
            "pattern mining",
            "spatiotemporal data"
        ],
        "uid": "v-tvcg-9397369",
        "time_stamp": "2021-10-28T16:00:00Z",
        "has_image": true,
        "paper_award": "",
        "image_caption": "This study proposes a visual analytics approach called VisCas for analyzing the cascades of spatiotemporal events. VisCas tightly integrates an automatic mining module and interactive visualizations. The mining module infers the cascading patterns of events for general urban scenarios. The user interface supports an analytical workflow for multi-faceted spatial cascade analyses, including location navigation, influence inspection, and cascading exploration.\n",
        "external_paper_link": "https://doi.org/10.1109/TVCG.2021.3071387",
        "has_pdf": false,
        "ff_link": "https://youtu.be/vlTkBlKQSt8"
    },
    "v-full-1055": {
        "authors": [
            "Chenhui Li",
            "George Baciu",
            "Yunzhe WANG",
            "Junjie Chen",
            "Changbo Wang"
        ],
        "title": "DDLVis: Real-time Visual Query of Spatiotemporal Data Distribution via Density Dictionary Learning",
        "session_id": "v-full-full25",
        "abstract": "Visual query of spatiotemporal data is becoming an increasingly important function in visual analytics applications. Various works have been presented for querying large spatiotemporal data in real time. However, the real-time query of spatiotemporal data distribution is still an open challenge. As spatiotemporal data become larger, methods of aggregation, storage and querying become critical. We propose a new visual query system that creates a low-memory storage component and provides real-time visual interactions of spatiotemporal data. We first present a peak-based kernel density estimation method to produce the data distribution for the spatiotemporal data. Then a novel density dictionary learning approach is proposed to compress temporal density maps and accelerate the query calculation. Moreover, various intuitive query interactions are presented to interactively gain patterns. The experimental results obtained on three datasets demonstrate that the presented system offers an effective query for visual analytics of spatiotemporal data.",
        "keywords": [],
        "uid": "v-full-1055",
        "time_stamp": "2021-10-28T16:15:00Z",
        "has_image": true,
        "paper_award": "",
        "image_caption": "Real-time query of spatiotemporal data distribution is still an open challenge. As spatiotemporal data become larger, methods of aggregation, storage and querying become critical. We propose a new visual query system that creates a low-memory storage component and provides real-time visual interactions of spatiotemporal data.",
        "external_paper_link": "",
        "has_pdf": true,
        "ff_link": "https://youtu.be/mo92ArEk8Nk"
    },
    "v-tvcg-9409710": {
        "authors": [
            "Christoph Neuhauser",
            "Junpeng Wang",
            "R\u00fcdiger Westermann"
        ],
        "title": "Interactive Focus+Context Rendering for Hexahedral Mesh Inspection",
        "session_id": "v-full-full15",
        "abstract": "The visual inspection of a hexahedral mesh with respect to element quality is difficult due to clutter and occlusions that are produced when rendering all element faces or their edges simultaneously. Current approaches overcome this problem by using focus on specific elements that are then rendered opaque, and carving away all elements occluding their view. In this work, we make use of advanced GPU shader functionality to generate a focus+context rendering that highlights the elements in a selected region and simultaneously conveys the global mesh structure and deformation field. To achieve this, we propose a gradual transition from edge-based focus rendering to volumetric context rendering, by combining fragment shader-based edge and face rendering with per-pixel fragment lists. A fragment shader smoothly transitions between wireframe and face-based rendering, including focus-dependent rendering style and depth-dependent edge thickness and halos, and per-pixel fragment lists are used to blend fragments in correct visibility order. To maintain the global mesh structure in the context regions, we propose a new method to construct a sheet-based level-of-detail hierarchy and smoothly blend it with volumetric information. The user guides the exploration process by moving a lens-like hotspot. Since all operations are performed on the GPU, interactive frame rates are achieved even for large meshes.",
        "keywords": [
            "Visualization of Hex-Meshes",
            "Real-Time Rendering",
            "GPUs"
        ],
        "uid": "v-tvcg-9409710",
        "time_stamp": "2021-10-28T15:00:00Z",
        "has_image": true,
        "paper_award": "",
        "image_caption": "In the image, the visualization of a hexahedral mesh using our Focus+Context rendering technique can be seen.\nCoarse mesh edges are blended into a volumetric representation of the mesh. Cell deformations are mapped to color and opacity. Users can interactively explore the hex-mesh by moving a lens-like hotspot. In this lens region, more detailed edge structures or the boundary surface of highly deformed cells can be shown.\n",
        "external_paper_link": "https://doi.org/10.1109/TVCG.2021.3074607",
        "has_pdf": false,
        "ff_link": "https://youtu.be/SjM4kUzl8Xw"
    },
    "v-full-1136": {
        "authors": [
            "Sebastian Weiss",
            "R\u00fcdiger Westermann"
        ],
        "title": "Differentiable Direct Volume Rendering",
        "session_id": "v-full-full15",
        "abstract": "We present a differentiable volume rendering solution that provides differentiability of all continuous parameters of the volume rendering process. This differentiable renderer is used to steer the parameters towards a setting with an optimal solution of a problem-specific objective function. We have tailored the approach to volume rendering by enforcing a constant memory footprint via analytic inversion of the blending functions. This makes it independent of the number of sampling steps through the volume and facilitates the consideration of small-scale changes. The approach forms the basis for automatic optimizations regarding external parameters of the rendering process and the volumetric density field itself. We demonstrate its use for automatic viewpoint selection using differentiable entropy as objective, and for optimizing a transfer function from rendered images of a given volume. Optimization of per-voxel densities is addressed in two different ways: First, we mimic inverse tomography and optimize a 3D density field from images using an absorption model. This simplification enables comparisons with algebraic reconstruction techniques and state-of-the-art differentiable path tracers. Second, we introduce a novel approach for tomographic reconstruction from images using an emission-absorption model with post-shading via an arbitrary transfer function.",
        "keywords": [],
        "uid": "v-full-1136",
        "time_stamp": "2021-10-28T15:15:00Z",
        "has_image": true,
        "paper_award": "",
        "image_caption": "We present a differentiable volume rendering solution that provides differentiability of all continuous parameters of the volume rendering process. \nA fully differentiable direct volume renderer is used for (left) viewpoint optimization, (middle) transfer function optimization, and (right) optimization of voxel properties using an emission-absorption model with rgb-alpha transfer functions",
        "external_paper_link": "",
        "has_pdf": true,
        "ff_link": "https://youtu.be/5E2I8iDKVIc"
    },
    "v-tvcg-9286513": {
        "authors": [
            "Nate Morrical",
            "Ingo Wald",
            "Will Usher",
            "Valerio Pascucci"
        ],
        "title": "Accelerating Unstructured Mesh Point Location with RT Cores",
        "session_id": "v-full-full15",
        "abstract": "We present a technique that leverages ray tracing hardware available in recent Nvidia RTX GPUs to solve a problem other than classical ray tracing. Specifically, we demonstrate how to use these units to accelerate the point location of general unstructured elements consisting of both planar and bilinear faces. This unstructured mesh point location problem has previously been challenging to accelerate on GPU architectures; yet, the performance of these queries is crucial to many unstructured volume rendering and compute applications. Starting with a CUDA reference method, we describe and evaluate three approaches that reformulate these point queries to incrementally map algorithmic complexity to these new hardware ray tracing units. Each variant replaces the simpler problem of point queries with a more complex one of ray queries. Initial variants exploit ray tracing cores for accelerated BVH traversal, and subsequent variants use ray-triangle intersections and per-face metadata to detect point-in-element intersections. Although these later variants are more algorithmically complex, they are significantly faster than the reference method thanks to hardware acceleration. Using our approach, we improve the performance of an unstructured volume renderer by up to 4\u00d7 for tetrahedral meshes and up to 15\u00d7 for general bilinear element meshes, matching, or out-performing state-of-the-art solutions while simultaneously improving on robustness and ease-of-implementation.",
        "keywords": [
            "Scientific Ray Tracing",
            "Unstructured Dcalar Data",
            "GPGPU",
            "Simulation",
            "Volume Rendering"
        ],
        "uid": "v-tvcg-9286513",
        "time_stamp": "2021-10-28T15:30:00Z",
        "has_image": true,
        "paper_award": "",
        "image_caption": "The Agulhas Current dataset, courtesy Niklas R\u00f6ber, DKRZ. This image shows simulated ocean currents off the coast of South Africa, represented using cell-centered wedges. When rendered using our hardware accelerated point queries, we see up to a 14.86\u00d7 performance improvement over a CUDA reference implementation (2.49 FPS vs 37 FPS on an RTX 2080 at 1024\u00d71024).",
        "external_paper_link": "https://doi.org/10.1109/TVCG.2020.3042930",
        "has_pdf": false,
        "ff_link": "https://youtu.be/pVER0z0YV5E"
    },
    "v-full-1586": {
        "authors": [
            "Mohamed Ibrahim",
            "Peter Rautek",
            "Guido Reina",
            "Marco Agus",
            "Markus Hadwiger"
        ],
        "title": "Probabilistic Occlusion Culling using Confidence Maps for High-Quality Rendering of Large Particle Data",
        "session_id": "v-full-full15",
        "abstract": "Achieving high rendering quality in the visualization of large particle data, for example from large-scale molecular dynamics simulations, requires a significant amount of sub-pixel super-sampling, due to very high numbers of particles per pixel. Although it is impossible to super-sample all particles of large-scale data at interactive rates, efficient occlusion culling can decouple the overall data size from a high effective sampling rate of visible particles. However, while the latter is essential for domain scientists to be able to see important data features, performing occlusion culling by sampling or sorting the data is usually slow or error-prone due to visibility estimates of insufficient quality. We present a novel probabilistic culling architecture for super-sampled high-quality rendering of large particle data. Occlusion is dynamically determined at the sub-pixel level, without explicit visibility sorting or data simplification. We introduce confidence maps to probabilistically estimate confidence in the visibility data gathered so far. This enables progressive, confidence-based culling, helping to avoid wrong visibility decisions. In this way, we determine particle visibility with high accuracy, although only a small part of the data set is sampled. This enables extensive super-sampling of (partially) visible particles for high rendering quality, at a fraction of the cost of sampling all particles. For real-time performance with millions of particles, we exploit novel features of recent GPU architectures to group particles into two hierarchy levels, combining fine-grained culling with high frame rates.",
        "keywords": [],
        "uid": "v-full-1586",
        "time_stamp": "2021-10-28T15:45:00Z",
        "has_image": true,
        "paper_award": "",
        "image_caption": "We present a novel probabilistic occlusion culling architecture for large particle data with intricate details, requiring extensive super-sampling for interactive visualization to be able to capture the amount of detail and contained features. We perform progressive culling, reducing the potentially visible set of particles iteratively while refining the rendering quality. To do this, we estimate expected occlusion by accumulating confidence in gathered occlusion information, together with a dynamically-updated coarse particle density volume. Overall, we are able to significantly speed up super-sampling for the visible subset of very large particle data sets. In the example shown here, 75.5% has been culled.\n",
        "external_paper_link": "",
        "has_pdf": true,
        "ff_link": "https://youtu.be/zMbAIqrnRj0"
    },
    "v-full-1420": {
        "authors": [
            "Ingo Wald",
            "Nate Morrical",
            "Stefan Zellmann"
        ],
        "title": "A Memory Efficient Encoding for Ray Tracing Large Unstructured Data",
        "session_id": "v-full-full15",
        "abstract": "In theory, efficient and high-quality rendering of unstructured data should greatly benefit from modern GPUs, but in practice, GPUs are often limited by the large amount of memory that large meshes require for element representation and for sample reconstruction acceleration structures. We describe a memory-optimized encoding for large unstructured meshes that efficiently encodes both the unstructured mesh and corresponding sample reconstruction acceleration structure, while still allowing for fast random-access sampling as required for rendering. We demonstrate that for large data our encoding is more efficient than OSPRay\u2019s and allows for rendering even the 2.9 billion element Mars Lander on a single off-the-shelf GPU--and the largest 6.3 billion version on a pair of such GPUs.",
        "keywords": [],
        "uid": "v-full-1420",
        "time_stamp": "2021-10-28T16:00:00Z",
        "has_image": false,
        "paper_award": "",
        "image_caption": "",
        "external_paper_link": "",
        "has_pdf": true,
        "ff_link": ""
    },
    "v-full-1336": {
        "authors": [
            "Xin Chen",
            "Jian Zhang",
            "Chi-Wing Fu",
            "Jean-Daniel Fekete",
            "Yunhai Wang"
        ],
        "title": "Pyramid-based Scatterplots Sampling for Progressive and Streaming Data Visualization",
        "session_id": "v-full-full15",
        "abstract": "We present a pyramid-based scatterplot sampling technique to avoid overplotting and enable progressive and streaming\n  visualization of large data. Our technique is based on a multiresolution pyramid-based decomposition of the underlying density map\n  and makes use of the density values in the pyramid to guide the sampling at each scale for preserving the relative data densities\n  and outliers. We show that our technique is competitive in quality with state-of-the-art methods and runs faster by about an order of\n  magnitude. Also, we have adapted it to deliver progressive and streaming data visualization by processing the data in chunks and\n  updating the scatterplot areas with visible changes in the density map. A quantitative evaluation shows that our approach generates\n  stable and faithful progressive samples that are comparable to the state-of-the-art method in preserving relative densities and superior\n  to it in keeping outliers and stability when switching frames. We present two case studies that demonstrate the effectiveness of our\n  approach for exploring large data.",
        "keywords": [],
        "uid": "v-full-1336",
        "time_stamp": "2021-10-28T16:15:00Z",
        "has_image": true,
        "paper_award": "",
        "image_caption": "Different sampling methods for presenting the \u201cNew York City TLC Trip Record\u201d data with 2M data points, which are partitioned into chunks, each of 100k data points. (a) The opaque scatterplot is overlaid on the New York map and rendered as (b) a transparent density map, where some major features are highlighted. (c,d,e) The top and bottom rows show the results of different sampling methods in the 9th and 10th frames, respectively, where each result has around 1K points sampled from the original data chunk. It shows our method can preserve relative density and outliers while maintaining temporal coherence. ",
        "external_paper_link": "",
        "has_pdf": true,
        "ff_link": "https://youtu.be/_dj2w4nAqfs"
    },
    "v-short-1025": {
        "authors": [
            "Michael Oppermann",
            "Luce Liu",
            "Tamara Munzner"
        ],
        "title": "TimeElide: Visual Analysis of Non-Contiguous Time Series Slices",
        "session_id": "v-short-short2",
        "abstract": "We introduce the design and implementation of TimeElide, a visual analysis tool for the novel data abstraction of non-contiguous time series slices, namely time intervals that contain a sequence of time-value pairs but are not adjacent to each other. This abstraction is relevant for analysis tasks where time periods of interest are known in advance or inferred from the data, rather than discovered through open-ended visual exploration. We present a visual encoding design space as an underpinning of TimeElide, and the new sparkbox technique for visualizing fine and coarse grained temporal structures within one view. Datasets from different domains and with varying characteristics guided the development and their analysis provides preliminary evidence of TimeElide's utility.",
        "keywords": [
            "Domain Agnostic",
            "Data Abstractions & Types",
            "Software Prototype",
            "Taxonomy, Models, Frameworks, Theory",
            "Temporal Data"
        ],
        "uid": "v-short-1025",
        "time_stamp": "2021-10-28T15:00:00Z",
        "has_image": true,
        "paper_award": "",
        "image_caption": "Screenshot of TimeElide: a domain-agnostic, open source visualization tool for the visual analysis of non-contiguous time series slices. In the left sidebar, users can choose between manual and automatic slicing, and select a visual encoding. The main view shows our proposed visualization technique, sparkboxes, which supports the analysis of within-slice and across-slice patterns. The presented visualization shows the number of per-player events during 22 soccer matches.",
        "external_paper_link": "",
        "has_pdf": true,
        "ff_link": "https://youtu.be/l70SBo4uiks"
    },
    "v-short-1048": {
        "authors": [
            "Vidya Setlur",
            "Haeyong Chung"
        ],
        "title": "Semantic Resizing of Charts Through Generalization: A Case Study with Line Charts",
        "session_id": "v-short-short2",
        "abstract": "Inspired by cartographic generalization principles, we present a generalization technique for rendering line charts at different sizes, preserving the important semantics of the data at that display size.The algorithm automatically determines the generalization operators to be applied at that size based on spatial density, distance, and the semantic importance of the various visualization elements in the line chart. A qualitative evaluation of the prototype that implemented the algorithm indicates that the generalized line charts pre-served the general data shape, while minimizing visual clutter. We identify future opportunities where generalization can be extended and applied to other chart types and visual analysis authoring tools.",
        "keywords": [
            "Multi-Resolution and Level of Detail Techniques",
            "Software Prototype",
            "Visual Representation Design"
        ],
        "uid": "v-short-1048",
        "time_stamp": "2021-10-28T15:10:00Z",
        "has_image": true,
        "paper_award": "honorable",
        "image_caption": "An example of a line chart automatically generalized to different display sizes. The algorithm preserves the various elements of the line chart based on their semantic importances at a given display size.\n",
        "external_paper_link": "",
        "has_pdf": true,
        "ff_link": "https://youtu.be/yPKnVjYf8bM"
    },
    "v-short-1082": {
        "authors": [
            "Anjana Arunkumar",
            "Shashank Ginjpalli",
            "Chris Bryan"
        ],
        "title": "Bayesian Modelling of Alluvial Diagram Complexity",
        "session_id": "v-short-short2",
        "abstract": "Alluvial diagrams are a popular technique for visualizing flow and relational data. However, successfully reading and interpreting the data shown in an alluvial diagram is likely influenced by factors such as data volume, complexity, and chart layout. To understand how alluvial diagram consumption is impacted by its visual features, we conduct two crowdsourced user studies with a set of alluvial diagrams of varying complexity, and examine (i) participant performance on analysis tasks, and (ii) the perceived complexity of the charts. Using the study results, we employ Bayesian modelling to predict participant classification of diagram complexity. We find that, while multiple visual features are important in contributing to alluvial diagram complexity, interestingly the most important feature changes depending on the type of complexity being modeled, i.e. task complexity vs. perceived complexity.",
        "keywords": [
            "Perception & Cognition",
            "Datasets",
            "Human-Subjects Quantitative Studies"
        ],
        "uid": "v-short-1082",
        "time_stamp": "2021-10-28T15:20:00Z",
        "has_image": true,
        "paper_award": "",
        "image_caption": "We study the complexity of alluvial diagrams with the following workflow: (1) We generate a publicly-available testing dataset of alluvial diagrams that vary complexity based on a combination of features. (2) We conduct two crowdsourced user studies and (3) assess the task performance and perceived complexity of the charts. (4) We then model the complexity of alluvial diagrams with a combination of regression and factor analysis and by constructing Bayesian models.",
        "external_paper_link": "",
        "has_pdf": true,
        "ff_link": "https://youtu.be/ugnuGYwf3gE"
    },
    "v-short-1151": {
        "authors": [
            "Jarryullah Ahmad",
            "Elaine Huynh",
            "Fanny Chevalier"
        ],
        "title": "When Red Means Good, Bad, or Canada: Exploring People's Reasoning for Choosing Color Palettes",
        "session_id": "v-short-short2",
        "abstract": "Color palette selection is an essential aspect of visualization creation and design, influencing viewer interpretation of the underlying data and evoking emotions in the viewer. Best practices, grounded in perceptual science and visual arts practice, form the basis of recommendation tools to support palette design and choice, but it is unclear how the general public reconciles the varied facets of color design. Does their decision-making align with established best practices? What factors do they take into consideration? Through a crowdsourced study with 63 participants, we find that the majority of palette choices are perceptually-motivated, but other factors such as semantic associations and bias also play a role. We identify some flaws in participant reasoning, highlight clashes in opinions, and present some implications for future work in recommendation tools.",
        "keywords": [
            "Color",
            "General Public",
            "Human-Subjects Qualitative Studies"
        ],
        "uid": "v-short-1151",
        "time_stamp": "2021-10-28T15:30:00Z",
        "has_image": true,
        "paper_award": "",
        "image_caption": "",
        "external_paper_link": "",
        "has_pdf": true,
        "ff_link": ""
    },
    "v-short-1170": {
        "authors": [
            "Nicolas Grossmann",
            "J\u00fcrgen Bernard",
            "Michael Sedlmair",
            "Manuela Waldner"
        ],
        "title": "Does the Layout Really Matter? A Study on Visual Model Accuracy Estimation",
        "session_id": "v-short-short2",
        "abstract": "In visual interactive labeling, users iteratively assign labels to data items until the machine model reaches an acceptable accuracy. A crucial step of this process is to inspect the model's accuracy and decide whether it is necessary to label additional elements. In scenarios with no or very little labeled data, visual inspection of the predictions is required. Similarity-preserving scatterplots created through a dimensionality reduction algorithm are a common visualization that is used in these cases. Previous studies investigated the effects of layout and image complexity on tasks like labeling. However, model evaluation has not been studied systematically. We present the results of an experiment studying the influence of image complexity and visual grouping of images on model accuracy estimation. We found that users outperform traditional automated approaches when estimating a model's accuracy. Furthermore, while the complexity of images impacts the overall performance, the layout of the items in the plot has little to no effect on estimations.",
        "keywords": [
            "Mixed Initiative Human-Machine Analysis",
            "Perception & Cognition",
            "Human-Subjects Quantitative Studies"
        ],
        "uid": "v-short-1170",
        "time_stamp": "2021-10-28T15:40:00Z",
        "has_image": true,
        "paper_award": "",
        "image_caption": "In a study, we assessed the influence of visual grouping and image complexity on visual model accuracy estimation from similarity-preserving scatterplots. In both scatterplots shown here, the percentage of images with correctly predicted class labels (visualized as border color) is over 90%. We found that users can estimate these accuracies fairly well. Image complexity impacts overall performance, but the layout has very little effect on users\u2019 estimations. ",
        "external_paper_link": "",
        "has_pdf": true,
        "ff_link": ""
    },
    "v-short-1171": {
        "authors": [
            "Raphael Sahann",
            "Torsten M\u00f6ller",
            "Johanna Schmidt"
        ],
        "title": "Histogram binning revisited with a focus on human perception",
        "session_id": "v-short-short2",
        "abstract": "This paper presents a quantitative user study to evaluate how well users can visually perceive the underlying data distribution from a histogram representation. We used histograms with different sample and bin sizes and four different distributions (uniform, normal, bimodal, and gamma). The study results confirm that, in general, more bins correlate with fewer errors by the viewers. However, upon a certain number of bins, the error rate cannot be improved by adding more bins. By comparing our study results with the outcomes of existing mathematical models for histogram binning (e.g., Sturges\u2019 formula, Scott\u2019s normal reference rule, the Rice Rule, or Freedman\u2013Diaconis\u2019 choice), we can see that most of them overestimate the number of bins necessary to make the distribution visible to a human viewer.",
        "keywords": [
            "Perception & Cognition",
            "Guidelines",
            "Human-Subjects Quantitative Studies"
        ],
        "uid": "v-short-1171",
        "time_stamp": "2021-10-28T15:50:00Z",
        "has_image": true,
        "paper_award": "",
        "image_caption": "Revisiting histograms with a focus on human perception. We evaluated how well human viewers can detect the underlying data distribution in a histogram when different sample sizes and bins are used. For this, we created datasets with a different number of samples (first row: few, last row: many) and a different number of bins (left column: 2, right column: 100). In this Figure a bimodal distribution was used to create the datasets in this illustration. We could show that most sample-based algorithms overestimate the number of bins which are necessary for human viewers.",
        "external_paper_link": "",
        "has_pdf": true,
        "ff_link": ""
    },
    "v-short-1205": {
        "authors": [
            "Paul Parsons",
            "Prakash Chandra Shukla",
            "Chorong Park"
        ],
        "title": "Fixation and Creativity in Data Visualization Design: Experiences and Perspectives of Practitioners",
        "session_id": "v-short-short2",
        "abstract": "Data visualization design often requires creativity, and research is needed to understand its nature and means for promoting it. However, the current visualization literature on creativity is not well developed, especially with respect to the experiences of professional data visualization designers. We conducted semi-structured interviews with 15 data visualization practitioners, focusing on a specific aspect of creativity known as \\textit{design fixation}. Fixation occurs when designers adhere blindly or prematurely to a set of ideas that limit creative outcomes. We present practitioners' experiences and perspectives from their own design practice, specifically focusing on their views of (i) the nature of fixation, (ii) factors encouraging fixation, and (iii) factors discouraging fixation in a data visualization context. We identify opportunities for future research related to chart recommendations, inspiration, and perspective shifts in data visualization design.",
        "keywords": [
            "Methodologies",
            "Taxonomy, Models, Frameworks, Theory",
            "Other Contribution",
            "Human-Subjects Qualitative Studies"
        ],
        "uid": "v-short-1205",
        "time_stamp": "2021-10-28T16:00:00Z",
        "has_image": true,
        "paper_award": "",
        "image_caption": "Data visualization design often requires creativity, and research is needed to understand its nature and means for promoting it. The current visualization literature on creativity is not well developed, especially with respect to the experiences of professional data visualization designers. We conducted semi-structured interviews with visualization practitioners, focusing on a specific aspect of creativity known as design fixation. Fixation occurs when designers adhere blindly or prematurely to a set of ideas that limit creative outcomes. We present practitioners' experiences and perspectives from their own design practice, specifically their views of the nature of fixation, factors encouraging fixation, factors discouraging fixation.",
        "external_paper_link": "",
        "has_pdf": true,
        "ff_link": ""
    },
    "v-short-1012": {
        "authors": [
            "Maximilian T. Fischer",
            "Alexander Frings",
            "Daniel Keim",
            "Daniel Seebacher"
        ],
        "title": "Towards a Survey on Static and Dynamic Hypergraph Visualizations",
        "session_id": "v-short-short2",
        "abstract": "Leveraging hypergraph structures to model advanced processes has gained much attention over the last few years in many areas, ranging from protein-interaction in computational biology to image retrieval using machine learning. Hypergraph models can provide a more accurate representation of the underlying processes while reducing the overall number of links compared to regular representations. However, interactive visualization methods for hypergraphs and hypergraph-based models have rarely been explored or systematically analyzed. This paper reviews the existing research landscape for hypergraph and hypergraph model visualizations and assesses the currently employed techniques. We provide an overview and a categorization of proposed approaches, focusing on performance, scalability, interaction support, successful evaluation, and the ability to represent different underlying data structures, including a recent demand for a temporal representation of interaction networks and their improvements beyond graph-based methods. Lastly, we discuss the different strengths and weaknesses of the individual approaches and give an insight into the future challenges arising in this emerging research field.",
        "keywords": [
            "Social Science, Education, Humanities, Journalism, Intelligence Analysis, Knowledge Work",
            "State-of-the-art Survey",
            "Graph/Network and Tree Data"
        ],
        "uid": "v-short-1012",
        "time_stamp": "2021-10-28T16:10:00Z",
        "has_image": true,
        "paper_award": "",
        "image_caption": "Our survey provides a detailed overview and a comparison of the available static and dynamic hypergraph visualization techniques. The individual approaches are described and summarized before the comparison focuses on distinguishing criteria in areas such as application domains, performance aspects, scalability, interaction support, and evaluation criteria, grouped according to their primary visualization technique.",
        "external_paper_link": "",
        "has_pdf": true,
        "ff_link": "https://youtu.be/4jwfa4R1Ndw"
    },
    "v-tvcg-9382844": {
        "authors": [
            "Yifang Wang",
            "Hongye Liang",
            "Xinhuan Shu",
            "Jiachen Wang",
            "Ke Xu",
            "Zikun Deng",
            "Cameron Campbell",
            "Bijia Chen",
            "Yingcai Wu",
            "Huamin Qu"
        ],
        "title": "Interactive Visual Exploration of Longitudinal Historical Career Mobility Data",
        "session_id": "v-full-full22",
        "abstract": "The increased availability of quantitative historical datasets has provided new research opportunities for multiple disciplines in social science. In this paper, we work closely with the constructors of a new dataset, CGED-Q (China Government Employee Database-Qing), that records the career trajectories of over 340,000 government officials in the Qing bureaucracy in China from 1760 to 1912. We use these data to study career mobility from a historical perspective and understand social mobility and inequality. However, existing statistical approaches are inadequate for analyzing career mobility in this historical dataset with its fine-grained attributes and long time span, since they are mostly hypothesis-driven and require substantial effort. We propose CareerLens, an interactive visual analytics system for assisting experts in exploring, understanding, and reasoning from historical career data. With CareerLens, experts examine mobility patterns in three levels-of-detail, namely, the macro-level providing a summary of overall mobility, the meso-level extracting latent group mobility patterns, and the micro-level revealing social relationships of individuals. We demonstrate the effectiveness and usability of CareerLens through two case studies and receive encouraging feedback from follow-up interviews with domain experts",
        "keywords": [
            "Digital Humanities",
            "Quantitative History",
            "Career Mobility",
            "Visual Analytics"
        ],
        "uid": "v-tvcg-9382844",
        "time_stamp": "2021-10-28T17:00:00Z",
        "has_image": true,
        "paper_award": "",
        "image_caption": "We worked with the constructors of a new dataset, CGED-Q (China Government Employee Database-Qing), that records the career trajectories of over 340,000 government officials in the Qing bureaucracy in China from 1760 to 1912. These data are used to study career mobility from a historical perspective and understand social mobility and inequality. We propose CareerLens, an interactive visual analytics system for exploring, understanding, and reasoning from these data. With CareerLens, experts examine mobility patterns in three levels-of-detail: the macro-level providing a summary of overall mobility, the meso-level extracting latent group mobility patterns, and the micro-level revealing social relationships of individuals. ",
        "external_paper_link": "https://doi.org/10.1109/TVCG.2021.3067200",
        "has_pdf": false,
        "ff_link": "https://youtu.be/siMGPd0jL8s"
    },
    "v-full-1433": {
        "authors": [
            "Yue Zhao",
            "Jian Zhang",
            "Chi-Wing Fu",
            "Mingliang Xu",
            "Dominik Moritz",
            "Yunhai Wang"
        ],
        "title": "KD-Box: Line-segment-based KD-tree for Interactive Exploration of Large-scale Time-Series Data",
        "session_id": "v-full-full22",
        "abstract": "Time-series data\u2014usually presented in the form of lines\u2014plays an important role in many domains such as finance, meteorology, health, and urban informatics. Yet, little has been done to support interactive exploration of large-scale time-series data, which requires a clutter-free visual representation with low-latency interactions. In this paper, we contribute a novel line-segment-based KD-tree method to enable interactive analysis of many time series. Our method enables not only fast queries over time series in selected regions of interest but also a line splatting method for efficient computation of the density field and selection of representative lines. Further, we develop KD-Box, an interactive system that provides rich interactions, e.g., timebox, attribute filtering, and coordinated multiple views. We demonstrate the effectiveness of KD-Box in supporting efficient line query and density field computation through a quantitative comparison and show its usefulness for interactive visual analysis on several real-world datasets.",
        "keywords": [],
        "uid": "v-full-1433",
        "time_stamp": "2021-10-28T17:15:00Z",
        "has_image": true,
        "paper_award": "",
        "image_caption": "In KD-Box, we contribute a novel line-segment-based KD-tree method to enable interactive analysis of many time series. Our method enables not only fast queries over time series in selected regions of interest but also a line splatting method for efficient computation of the density field and selection of representative lines. Further, we develop KD-Box, an interactive system that provides rich interactions, e.g.,timebox,attribute filtering, and coordinated multiple views. We demonstrate the effectiveness of KD-Box in supporting efficient line query and density field computation through a quantitative comparison and show its usefulness for interactive visual analysis on several real-world datasets.",
        "external_paper_link": "",
        "has_pdf": true,
        "ff_link": "https://youtu.be/zHiYTZUw5Co"
    },
    "v-full-1116": {
        "authors": [
            "Jessica Magallanes",
            "Tony Stone",
            "Paul Morris",
            "Suzanne Mason",
            "Steven Wood",
            "Maria-Cruz Villa-Uriol"
        ],
        "title": "Sequen-C: A Multilevel Overview of Temporal Event Sequences",
        "session_id": "v-full-full22",
        "abstract": "Building a visual overview of temporal event sequences with an optimal level-of-detail (i.e. simplified but informative) is an ongoing challenge - expecting the user to zoom into every important aspect of the overview can lead to missing insights. We propose a technique to build and explore a multilevel overview of event sequences, from coarse to fine vertical or horizontal level-of-detail, using hierarchical aggregation and a novel cluster data representation Align-Score-Simplify. By default, the overview shows an optimal number of sequence clusters obtained through the average silhouette width metric \u2013 then users are able to explore alternative optimal sequence clusterings. The vertical level-of-detail of the overview changes along with the number of clusters, whilst the horizontal level-of-detail refers to the level of summarization applied to each cluster representation. The proposed technique has been implemented into a visualization system called Sequence Cluster Explorer (Sequen-C) that allows multilevel and detail-on-demand exploration through three coordinated views, and the inspection of data attributes at cluster, unique sequence, and individual sequence level. We present two case studies using real-world datasets in the healthcare domain: CUREd and MIMIC-III, which demonstrate how the technique can aid users in exploring and defining a set of distinct pathways that best summarize the dataset, while also being able of identifying deviating pathways and exploring data attributes for selected patterns.",
        "keywords": [],
        "uid": "v-full-1116",
        "time_stamp": "2021-10-28T17:30:00Z",
        "has_image": true,
        "paper_award": "",
        "image_caption": "Screenshots of Sequen-C for a dataset with 258 event sequences illustrating the methodology. The hierarchical aggregation tree (top left) allows changing the number of clusters shown. The vertical level-of-detail of the multilevel overview can be transformed from coarse (bottom left) to fine (middle and bottom right). Sequence clusters are represented using an Align-Score-Simplify strategy (top right), which allows controlling the horizontal level-of-detail according to an information score.",
        "external_paper_link": "",
        "has_pdf": true,
        "ff_link": "https://youtu.be/cbii6AHG9Oo"
    },
    "v-full-1287": {
        "authors": [
            "Tinghao Feng",
            "Jing Yang",
            "Martha-Cary Eppes",
            "Zhaocong Yang",
            "Faye Moser"
        ],
        "title": "EVis: Visually Analyzing Environmentally Driven Events",
        "session_id": "v-full-full22",
        "abstract": "Earth scientists are increasingly employing time series data with multiple dimensions and high temporal resolution to study the impacts of climate and environmental changes on Earth\u2019s atmosphere, biosphere, hydrosphere, and lithosphere. However, the large number of variables and varying time scales of antecedent conditions contributing to natural phenomena hinder scientists from completing more than the most basic analyses. In this paper, we present EVis (Environmental Visualization), a new visual analytics prototype to help scientists analyze and explore recurring environmental events (e.g. rock fracture, landslides, heat waves, floods) and their relationships with high dimensional time series of continuous numeric environmental variables, such as ambient temperature and precipitation. EVis provides coordinated scatterplots, heatmaps, histograms, and RadViz for foundational analyses. These features allow users to interactively examine relationships between events and one, two, three, or more environmental variables. EVis also provides a novel visual analytics approach to allowing users to discover temporally lagging relationships related to antecedent conditions between events and multiple variables, a critical task in Earth sciences. In particular, this latter approach projects multivariate time series onto trajectories in a 2D space using RadViz, and clusters the trajectories for temporal pattern discovery. Our case studies with rock cracking data and interviews with domain experts from a range of sub-disciplines within Earth sciences illustrate the extensive applicability and usefulness of EVis.",
        "keywords": [],
        "uid": "v-full-1287",
        "time_stamp": "2021-10-28T17:45:00Z",
        "has_image": true,
        "paper_award": "",
        "image_caption": "A screenshot of RadViz-Leash, a RadViz projection and clustering  based visual analytics approach to analyzing temporally lagging relationships between events and multiple environmental conditions. It is one of the visualizations provided by EVis, a prototype developed for Earth scientists to study environmentally driven events.",
        "external_paper_link": "",
        "has_pdf": true,
        "ff_link": ""
    },
    "v-tvcg-9447222": {
        "authors": [
            "Dongyun Han",
            "Gorakh Parsad",
            "Hwiyeon Kim",
            "Jaekyom Shim",
            "Oh-Sang Kwon",
            "Kyung Son",
            "Jooyoung Lee",
            "Isaac Cho",
            "Sungahn Ko"
        ],
        "title": "HisVA: a Visual Analytics System for Learning History",
        "session_id": "v-full-full22",
        "abstract": "Studying history involves many difficult tasks. Examples include searching for proper data in a large event space, understanding stories of historical events by time and space, and finding relationships among events that may not be apparent. Instructors who extensively use well-organized and well-argued materials (e.g., textbooks and online resources) can lead students to a narrow perspective in understanding history and prevent spontaneous investigation of historical events, with the students asking their own questions. In this work, we proposed HisVA, a visual analytics system that allows the efficient exploration of historical events from Wikipedia using three views: event, map, and resource. HisVA provides an effective event exploration space, where users can investigate relationships among historical events by reviewing and linking them in terms of space and time. To evaluate our system, we present two usage scenarios, a user study with a qualitative analysis of user exploration strategies, and in-class deployment results",
        "keywords": [
            "Visualization for Education",
            "Event Visualization",
            "Studying History",
            "Wikipedia"
        ],
        "uid": "v-tvcg-9447222",
        "time_stamp": "2021-10-28T18:00:00Z",
        "has_image": true,
        "paper_award": "",
        "image_caption": "Encouraging students to participate voluntarily in active studying processes is challenging. In conventional history classes, instructors deliver lectures based on textbooks, and such lecture-oriented classes are preferred due to their effectiveness. However, an increasing number of classes in the field of history education have begun to emphasize students\u2019 active participants. In this work, we proposed HisVA, a visual analytics system that allows the efficient exploration of historical events from Wikipedia using three views: A. event, B. map, and C. resource. We present two usage scenarios, a user study with a qualitative analysis of user exploration strategies, and in-class deployment results.",
        "external_paper_link": "https://doi.org/10.1109/TVCG.2021.3086414",
        "has_pdf": false,
        "ff_link": "https://youtu.be/ACWL_4yb5NA"
    },
    "v-tvcg-9229162": {
        "authors": [
            "Jun Han",
            "Chaoli Wang"
        ],
        "title": "SSR-TVD: Spatial Super-Resolution for Time-Varying Data Analysis and Visualization",
        "session_id": "v-full-full22",
        "abstract": "We present SSR-TVD, a novel deep learning framework that produces coherent spatial super-resolution (SSR) of time-varying data (TVD) using adversarial learning. In scientific visualization, SSR-TVD is the first work that applies the generative adversarial network (GAN) to generate high-resolution volumes for three-dimensional time-varying data sets. The design of SSR-TVD includes a generator and two discriminators (spatial and temporal discriminators). The generator takes a low-resolution volume as input and outputs a synthesized high-resolution volume. To capture spatial and temporal coherence in the volume sequence, the two discriminators take the synthesized high-resolution volume(s) as input and produce a score indicating the realness of the volume(s). Our method can work in the in situ visualization setting by downscaling volumetric data from selected time steps as the simulation runs and upscaling downsampled volumes to their original resolution during postprocessing. To demonstrate the effectiveness of SSR-TVD, we show quantitative and qualitative results with several time-varying data sets of different characteristics and compare our method against volume upscaling using bicubic interpolation and a solution solely based on CNN.",
        "keywords": [
            "Time-varying data visualization",
            "deep learning",
            "super-resolution",
            "generative adversarial network"
        ],
        "uid": "v-tvcg-9229162",
        "time_stamp": "2021-10-28T18:15:00Z",
        "has_image": true,
        "paper_award": "",
        "image_caption": "We propose SSR-TVD, a generative model for time-varying data spatial super-resolution task. The generator takes a low-resolution volume as input and outputs a synthesized high-resolution volume. To capture spatial and temporal coherence in the volume sequence, the two discriminators take the synthesized high-resolution volume(s) as input and produce a score indicating the realness of the volume(s).",
        "external_paper_link": "https://doi.org/10.1109/TVCG.2020.3032123",
        "has_pdf": false,
        "ff_link": "https://youtu.be/LajFBGu5Ch4"
    },
    "v-full-1148": {
        "authors": [
            "Emily Wall",
            "Arpit Narechania",
            "Adam Coscia",
            "Jamal Paden",
            "Alex Endert"
        ],
        "title": "Left, Right, and Gender: Exploring Interaction Traces to Mitigate Human Biases",
        "session_id": "v-full-full24",
        "abstract": "Human biases impact the way people analyze data and make decisions. Recent work has shown that some visualization designs can better support cognitive processes and mitigate cognitive biases (i.e., errors that occur due to the use of mental \u201cshortcuts\u201d). In this work, we explore how visualizing a user\u2019s interaction history (i.e., which data points a user has interacted with) can be used to mitigate potential biases that drive decision making by promoting conscious reflection of one\u2019s analysis process. Given an interactive scatterplot-based visualization tool, we showed interaction history in real-time while exploring data (e.g., by coloring points in the scatterplot that the user has interacted with), and in a summative format after a decision has been made (e.g., by comparing the distribution of user interactions to the underlying distribution of the data). We conducted a series of in-lab experiments and a crowdsourced experiment to evaluate the effectiveness of interaction history interventions toward mitigating bias. We contextualized this work in a political scenario in which participants were instructed to choose a committee of 10 fictitious politicians to review a recent bill passed in the U.S. state of Georgia banning abortion after 6 weeks, where e.g. gender bias or political party bias may drive one\u2019s analysis process. We demonstrate the generalizability of this approach by evaluating a second decision making scenario related to movies. Our results are inconclusive for the effectiveness of interaction history (henceforth referred to as interaction traces) toward mitigating biased decision making. However, we find some mixed support that interaction traces, particularly in a summative format, can increase awareness of potential unconscious biases.",
        "keywords": [],
        "uid": "v-full-1148",
        "time_stamp": "2021-10-28T17:00:00Z",
        "has_image": true,
        "paper_award": "",
        "image_caption": "In our paper, Left, Right, and Gender: Exploring Interaction Traces to Mitigate Human Biases, we conduct 3 in-lab and one crowd-sourced experiment in the domains of politics and movies. We find mixed results that interaction traces can increase awareness of bias and impact interactive behavior and decision making.",
        "external_paper_link": "",
        "has_pdf": true,
        "ff_link": "https://youtu.be/KBAkr9ROv5k"
    },
    "v-full-1583": {
        "authors": [
            "Theo Jaunet",
            "Corentin Kervadec",
            "Romain Vuillemot",
            "Grigory Antipov",
            "Moez Baccouche",
            "Christian Wolf"
        ],
        "title": "VisQA: X-raying Vision and Language Reasoning in Transformers",
        "session_id": "v-full-full24",
        "abstract": "Visual Question Answering systems target answering open-ended textual questions given input images. They are a testbed for learning high-level reasoning with a primary use in HCI, for instance assistance for the visually impaired. Recent research has shown that state-of-the-art models tend to produce answers exploiting biases and shortcuts in the training data, and sometimes do not even look at the input image, instead of performing the required reasoning steps. We present VisQA, a visual analytics tool that explores this question of reasoning vs. bias exploitation. It exposes the key element of state-of-the-art neural models --- attention maps in transformers. Our working hypothesis is that reasoning steps leading to model predictions are observable from attention distributions, which are particularly useful for visualization. The design process of VisQA was motivated by well-known bias examples from the fields of deep learning and vision-language reasoning and evaluated in two ways. First, as a result of a collaboration of three fields, machine learning, vision and language reasoning, and data analytics, the work lead to a better understanding of bias exploitation of neural models for VQA, which eventually resulted in an impact on its design and training through the proposition of a method for the transfer of reasoning patterns from an oracle model. Second, we also report on the design of VisQA, and a goal-oriented evaluation of VisQA targeting the analysis of a model decision process from multiple experts, providing evidence that it makes the inner workings of models accessible to users.",
        "keywords": [],
        "uid": "v-full-1583",
        "time_stamp": "2021-10-28T17:15:00Z",
        "has_image": false,
        "paper_award": "",
        "image_caption": "",
        "external_paper_link": "",
        "has_pdf": true,
        "ff_link": ""
    },
    "v-full-1730": {
        "authors": [
            "Matt-Heun Hong",
            "Jessica Witt",
            "Danielle Albers Szafir"
        ],
        "title": "The Weighted Average Illusion: Biases in Perceived Mean Position in Scatterplots",
        "session_id": "v-full-full24",
        "abstract": "Scatterplots can encode a third dimension by using additional channels like size or color (e.g. bubble charts). We explore a potential misinterpretation of trivariate scatterplots, which we call the weighted average illusion, where locations of larger and darker points are given more weight toward x- and y-mean estimates. This systematic bias is sensitive to a designer\u2019s choice of size or lightness ranges mapped onto the data. In this paper, we quantify this bias against varying size/lightness ranges and data correlations. We discuss possible explanations for its cause by measuring attention given to individual data points using a vision science technique called the centroid method. Our work illustrates how ensemble processing mechanisms and mental shortcuts can significantly distort visual summaries of data, and can lead to misjudgments like the demonstrated weighted average illusion.",
        "keywords": [],
        "uid": "v-full-1730",
        "time_stamp": "2021-10-28T17:30:00Z",
        "has_image": true,
        "paper_award": "",
        "image_caption": "We know from prior work that estimation biases can arise in common bar charts and line charts. But what about scatterplots, a seemingly precise technique for presenting multidimensional data? In this presentation we discuss an illusion which arises when presenting more than two data dimensions using scatterplots, which may lead to measurement biases.",
        "external_paper_link": "",
        "has_pdf": true,
        "ff_link": ""
    },
    "v-tvcg-9320596": {
        "authors": [
            "Marianne Procopio",
            "Ab Mosca",
            "Carlos Scheidegger",
            "Eugene Wu",
            "Remco Chang"
        ],
        "title": "Impact of Cognitive Biases on Progressive Visualization",
        "session_id": "v-full-full24",
        "abstract": "Progressive visualization is fast becoming a technique in the visualization community to help users interact with large amounts of data. With progressive visualization, users can examine intermediate results of complex or long running computations, without waiting for the computation to complete. While this has shown to be beneficial to users, recent research has identified potential risks. For example, users may misjudge the uncertainty in the intermediate results and draw incorrect conclusions or see patterns that are not present in the final results. In this paper, we conduct a comprehensive set of studies to quantify the advantages and limitations of progressive visualization. Based on a recent report by Micallef et al., we examine four types of cognitive biases that can occur with progressive visualization: uncertainty bias, illusion bias, control bias, and anchoring bias. The results of the studies suggest a cautious but promising use of progressive visualization \u2014 while there can be significant savings in task completion time, accuracy can be negatively affected in certain conditions. These findings confirm earlier reports of the benefits and drawbacks of progressive visualization and that continued research into mitigating the effects of cognitive biases is necessary.",
        "keywords": [
            "Data visualization",
            "Uncertainty",
            "Bars",
            "Task analysis",
            "Real-time systems",
            "Query processing",
            "Data analysis"
        ],
        "uid": "v-tvcg-9320596",
        "time_stamp": "2021-10-28T17:45:00Z",
        "has_image": true,
        "paper_award": "",
        "image_caption": "Progressive visualization of a bar chart. Data is sampled and a new estimate is calculated at each iteration. The error bars represent the 95% confidence interval of the estimated value. The color indicates the relative error of the estimate. The darker the blue, the more accurate the estimate is. Progressive visualization is a beneficial technique but users need to be cautious of potential cognitive biases.",
        "external_paper_link": "https://doi.org/10.1109/TVCG.2021.3051013",
        "has_pdf": false,
        "ff_link": "https://youtu.be/FonisZGrIe0"
    },
    "v-full-1010": {
        "authors": [
            "Smiti Kaul",
            "David Borland",
            "Nan Cao",
            "David Gotz"
        ],
        "title": "Improving Visualization Interpretation Using Counterfactuals",
        "session_id": "v-full-full24",
        "abstract": "Complex, high-dimensional data is used in a wide range of domains to explore problems and make decisions. Analysis of high-dimensional data, however, is vulnerable to the hidden influence of confounding variables, especially as users apply ad hoc filtering operations to visualize only specific subsets of an entire dataset. Thus, visual data-driven analysis can mislead users and encourage mistaken assumptions about causality or the strength of relationships between features. This work introduces a novel visual approach designed to reveal the presence of confounding variables via counterfactual possibilities during visual data analysis. It is implemented in CoFact, an interactive visualization prototype that determines and visualizes counterfactual subsets to better support user exploration of feature relationships. Using publicly available datasets, we conducted a controlled user study to demonstrate the effectiveness of our approach; the results indicate that users exposed to counterfactual visualizations formed more careful judgments about feature-to-outcome relationships.",
        "keywords": [],
        "uid": "v-full-1010",
        "time_stamp": "2021-10-28T18:00:00Z",
        "has_image": true,
        "paper_award": "",
        "image_caption": "Analysis page of the counterfactual visualization tool. On this page, the user has applied a filter constraint (a) to a multidimensional dataset and is shown the resulting included, counterfactual, and excluded subsets (b), their outcome distributions (c), and other feature information (d--i).",
        "external_paper_link": "",
        "has_pdf": true,
        "ff_link": ""
    },
    "v-full-1017": {
        "authors": [
            "Arpit Narechania",
            "Adam Coscia",
            "Emily Wall",
            "Alex Endert"
        ],
        "title": "Lumos: Increasing Awareness of Analytic Behavior during Visual Data Analysis",
        "session_id": "v-full-full24",
        "abstract": "Visual data analysis tools provide people with the agency and flexibility to explore data using a variety of interactive functionality. However, this flexibility may introduce potential consequences in situations where users unknowingly overemphasize or underemphasize specific subsets of the data or attribute space they are analyzing. For example, users may overemphasize specific attributes and/or their values (e.g., Gender is always encoded on the X axis), underemphasize others (e.g., Religion is never encoded), ignore a subset of the data (e.g., older people are filtered out), etc. In response, we present Lumos, a visual data analysis tool that captures and shows the interaction history with data to increase awareness of such analytic behaviors. Using in-situ (at the place of interaction) and ex-situ (in an external view) visualization techniques, Lumos provides real-time feedback to users for them to reflect on their activities. For example, Lumos highlights datapoints that have been previously examined in the same visualization (in-situ) and also overlays them on the underlying data distribution (i.e., baseline distribution) in a separate visualization (ex-situ). Through a user study with 24 participants, we investigate how Lumos helps users' data exploration and decision-making processes. We found that Lumos increases users' awareness of visual data analysis practices in real-time, promoting reflection upon and acknowledgement of their intentions and potentially influencing subsequent interactions.",
        "keywords": [],
        "uid": "v-full-1017",
        "time_stamp": "2021-10-28T18:15:00Z",
        "has_image": true,
        "paper_award": "",
        "image_caption": "Lumos is a tool that presents ex-situ and in-situ interaction traces to increase awareness of users' biased analytic behaviors (e.g., overemphasis or underemphasis on aspects of data) during visual data analysis. Through a qualitative user study, we found that Lumos increases users' awareness of visual data analysis practices in real-time, promoting reflection upon and acknowledgement of their intentions and potentially influencing subsequent interactions. We open-sourced our system at lumos-vis.github.io.",
        "external_paper_link": "",
        "has_pdf": true,
        "ff_link": "https://youtu.be/KKDiLrsTlLA"
    },
    "v-tvcg-9262073": {
        "authors": [
            "Hsiang-Yun Wu",
            "Martin N\u00f6llenburg",
            "Ivan Viola"
        ],
        "title": "Multi-level Area Balancing of Clustered Graphs",
        "session_id": "v-full-full9",
        "abstract": "We present a multi-level area balancing technique for laying out clustered graphs to facilitate a comprehensive understanding of the complex relationships that exist in various fields, such as life sciences and sociology. Clustered graphs are often used to model relationships that are accompanied by attribute-based grouping information. Such information is essential for robust data analysis, such as for the study of biological taxonomies or educational backgrounds. Hence, the ability to smartly arrange textual labels and packing graphs within a certain screen space is therefore desired to successfully convey the attribute data . Here we propose to hierarchically partition the input screen space using Voronoi tessellations in multiple levels of detail. In our method, the position of textual labels is guided by the blending of constrained forces and the forces derived from centroidal Voronoi cells. The proposed algorithm considers three main factors: (1) area balancing, (2) schematized space partitioning, and (3) hairball management. We primarily focus on area balancing, which aims to allocate a uniform area for each textual label in the diagram. We achieve this by first untangling a general graph to a clustered graph through textual label duplication, and then coupling with spanning-tree-like visual integration. We illustrate the feasibility of our approach with examples and then evaluate our method by comparing it with well-known conventional approaches and collecting feedback from domain experts.",
        "keywords": [
            "Graph drawing",
            "Voronoi tessellation",
            "multi-level",
            "spatially-efficient layout"
        ],
        "uid": "v-tvcg-9262073",
        "time_stamp": "2021-10-28T17:00:00Z",
        "has_image": true,
        "paper_award": "",
        "image_caption": "A collection of pathways in human metabolism, including Alanine and Aspartate Metabolism, Alkaloid Synthesis, and Aminosugar Metabolism. Each of the clusters (i.e., subsystems in metabolism) is highlighted in different colors. White rectangular labels represent reactions, and rounded labels are metabolites involved in the reactions. Pink vertices indicate important vertices, such as the metabolite ATP carrying energy in Alanine and Aspartate Metabolism, and cyan vertices are the duplicated metabolites, such as H2O or H2, which are involved in most of the reactions in human metabolism. The red route here indicates a highlighted metabolite appearing as a duplicate in multiple subsystems.",
        "external_paper_link": "https://doi.org/10.1109/TVCG.2020.3038154",
        "has_pdf": false,
        "ff_link": "https://youtu.be/QL8QOQUN0b4"
    },
    "v-tvcg-9237128": {
        "authors": [
            "Jian Zhao",
            "Maoyuan Sun",
            "Francine Chen",
            "Patrick Chiu"
        ],
        "title": "Understanding Missing Links in Bipartite Networks with MissBiN",
        "session_id": "v-full-full9",
        "abstract": "The analysis of bipartite networks is critical in a variety of application domains, such as exploring entity co-occurrences in intelligence analysis and investigating gene expression in bio-informatics. One important task is missing link prediction, which infers the existence of unseen links based on currently observed ones. In this paper, we propose a visual analysis system, MissBiN, to involve analysts in the loop for making sense of link prediction results. MissBiN equips a novel method for link prediction in a bipartite network by leveraging the information of bi-cliques in the network. It also provides an interactive visualization for understanding the algorithm outputs. The design of MissBiN is based on three high-level analysis questions (what, why, and how) regarding missing links, which are distilled from the literature and expert interviews. We conducted quantitative experiments to assess the performance of the proposed link prediction algorithm, and interviewed two experts from different domains to demonstrate the effectiveness of MissBiN as a whole. We also provide a comprehensive usage scenario to illustrate the usefulness of the tool in an application of intelligence analysis.",
        "keywords": [
            "Missing link prediction",
            "bipartite network",
            "bi-clique",
            "interactive visualization",
            "visual analytics."
        ],
        "uid": "v-tvcg-9237128",
        "time_stamp": "2021-10-28T17:15:00Z",
        "has_image": true,
        "paper_award": "",
        "image_caption": "We propose a visual analysis system, MissBiN, to involve analysts in the loop for making sense of link prediction results. MissBiN equips a novel method for link prediction in a bipartite network by leveraging the information of bi-cliques in the network. It also provides an interactive visualization for understanding the algorithm outputs. ",
        "external_paper_link": "https://doi.org/10.1109/TVCG.2020.3032984",
        "has_pdf": false,
        "ff_link": "https://youtu.be/qFoHPIJTz0M"
    },
    "v-tvcg-9371413": {
        "authors": [
            "Aditeya Pandey",
            "Uzma Syeda",
            "Chaitya Shah",
            "John Gomez",
            "Michelle Borkin"
        ],
        "title": "A State-of-the-Art Survey of Tasks for Tree Design and Evaluation with a Curated Task Dataset",
        "session_id": "v-full-full9",
        "abstract": "In the field of information visualization, the concept of ``tasks'' is an essential component of theories and methodologies for how a visualization researcher or a practitioner understands what tasks a user needs to perform and how to approach the creation of a new design. In this paper, we focus on the collection of tasks for tree visualizations, a common visual encoding in many domains ranging from biology to computer science to geography. In spite of their commonality, no prior efforts exist to collect and abstractly define tree visualization tasks. We present a literature review of tree visualization papers and generate a curated dataset of over 200 tasks. To enable effective task abstraction for trees, we also contribute a novel extension of the Multi-Level Task Typology to include more specificity to support tree-specific tasks as well as a systematic procedure to conduct task abstractions for tree visualizations. All tasks in the dataset were abstracted with the novel typology extension and analyzed to gain a better understanding of the state of tree visualizations. These abstracted tasks can benefit visualization researchers and practitioners as they design evaluation studies or compare their analytical tasks with ones previously studied in the literature to make informed decisions about their design. We also reflect on our novel methodology and advocate more broadly for the creation of task-based knowledge repositories for different types of visualizations. The Supplemental Material will be maintained on OSF: https://osf.io/u5eh",
        "keywords": [
            "STAR",
            "Survey",
            "Tree",
            "Tasks",
            "Task Abstraction",
            "Theory",
            "Dataset"
        ],
        "uid": "v-tvcg-9371413",
        "time_stamp": "2021-10-28T17:30:00Z",
        "has_image": true,
        "paper_award": "",
        "image_caption": "Novel Tree-specific Task Abstraction Framework",
        "external_paper_link": "https://doi.org/10.1109/TVCG.2021.3064037",
        "has_pdf": false,
        "ff_link": "https://youtu.be/lGcF0OjRFJE"
    },
    "v-full-1368": {
        "authors": [
            "Markus Wallinger",
            "Daniel Archambault",
            "David Auber",
            "Martin N\u00f6llenburg",
            "Jaakko Peltonen"
        ],
        "title": "Edge-Path Bundling: A Less Ambiguous Edge Bundling Approach",
        "session_id": "v-full-full9",
        "abstract": "Edge bundling techniques cluster edges with similar attributes (i.e. similarity in direction and proximity) together to reduce the visual clutter. All edge bundling techniques to date implicitly or explicitly cluster groups of individual edges, or parts of them, together based on these attributes. These clusters can result in ambiguous connections that do not exist in the data. Confluent drawings of networks do not have these ambiguities, but require the layout to be computed as part of the bundling process. We devise a new bundling method, edge-path bundling, to simplify edge clutter while greatly reducing ambiguities compared to previous bundling techniques. Edge-path bundling takes a layout as input and clusters each edge along a weighted, shortest path to limit its deviation from a straight line. Edge-path bundling does not incur independent edge ambiguities typically seen in all edge bundling methods, and the level of bundling can be tuned through shortest path distances, Euclidean distances, and combinations of the two. Also, directed edge bundling naturally emerges from the model. Through metric evaluations, we demonstrate the advantages of edge-path bundling over other techniques.",
        "keywords": [],
        "uid": "v-full-1368",
        "time_stamp": "2021-10-28T17:45:00Z",
        "has_image": true,
        "paper_award": "",
        "image_caption": "Edge-Path bundling is a new bundling method to simplify edge clutter while greatly reducing ambiguities compared to previous bundling techniques. It takes a layout as input and clusters each edge along a weighted, shortest path to limit its deviation from a straight line. Edge-Path bundling does not incur independent edge ambiguities typically seen in all edge bundling methods, and the level of bundling can be tuned through shortest path distances, Euclidean distances, and combinations of the two.\n",
        "external_paper_link": "",
        "has_pdf": true,
        "ff_link": "https://youtu.be/3NHfrXpS2IU"
    },
    "v-full-1663": {
        "authors": [
            "Sara Di Bartolomeo",
            "Mirek Riedewald",
            "Wolfgang Gatterbauer",
            "Cody Dunne"
        ],
        "title": "STRATISFIMAL LAYOUT: A Modular Optimization Model for Laying Out Layered Node-link Network Visualizations",
        "session_id": "v-full-full9",
        "abstract": "Node-link visualizations are a familiar and powerful tool for displaying the relationships in a network. The readability of these visualizations highly depends on the spatial layout used for the nodes. In this paper, we focus on computing layered layouts, in which nodes are aligned on a set of parallel axes to better expose hierarchical or sequential relationships. Heuristic-based layouts are widely used as they scale well to larger networks and usually create readable, albeit sub-optimal, visualizations. We instead use a layout optimization model that prioritizes optimality (as compared to scalability) because an optimal solution not only represents the best attainable result, but can also serve as a baseline to evaluate the effectiveness of layout heuristics. We take an important step towards powerful and flexible network visualization by proposing Stratisfimal Layout, a modular integer-linear-programming formulation that can consider several important readability criteria simultaneously: crossing reduction, edge bendiness, and nested and multi-layer groups. The layout can be adapted to diverse use cases through its modularity. Individual features can be enabled and customized depending on the application. We provide open-source and documented implementations of the layout, both for web-based and desktop visualizations. As a proof-of-concept, we apply it to the problem of visualizing complicated SQL queries, which have features that, to the best of our knowledge, cannot be addressed by existing layout optimization models. We also include a benchmark network generator and the results of an empirical evaluation to assess the performance trade-offs of our design choices. A full version of this paper with all appendices, data, and source code is available at https://osf.io/3vqms with live examples at https://visdunneright.github.io/stratisfimal/.",
        "keywords": [],
        "uid": "v-full-1663",
        "time_stamp": "2021-10-28T18:00:00Z",
        "has_image": true,
        "paper_award": "",
        "image_caption": "Stratisfimal Layout is a method to create optimal layouts. The image uses lego blocks to represent how the different features are modular and can be cherry-picked to fit different use cases. \n",
        "external_paper_link": "",
        "has_pdf": true,
        "ff_link": "https://youtu.be/eWkX3zUYuAU"
    },
    "v-full-1494": {
        "authors": [
            "Huan Song",
            "Zeng Dai",
            "Panpan Xu",
            "Liu Ren"
        ],
        "title": "Interactive Visual Pattern Search on Graph Data via Graph Representation Learning",
        "session_id": "v-full-full9",
        "abstract": "Graphs are a ubiquitous data structure to model processes and relations in a wide range of domains. Examples include social networks, knowledge graphs, control-flow graphs in programs, and semantic scene graphs in images. Identifying subgraph patterns (or motifs) in graphs is one important approach to understand their structural properties. We propose a visual analytics system, GraphQ to support human-in-the-loop, example-based, subgraph pattern search in a database containing many individual graphs. Our approach goes beyond a predefined set of motifs and allows users to interactively specify the patterns of interest. To support fast, interactive queries, we use graph neural networks (GNNs) to encode the topological and node attributes in a graph as fixed-length\n  latent vector representations, and perform subgraph matching in the latent space. However, due to the complexity of the problem, it is still difficult to obtain accurate one-to-one node correspondence in the matching results, which is crucial for visualization and\n  interpretation. We, therefore, propose a novel GNN for node-alignment called NeuroAlign, to facilitate easy validation and interpretation of the query results. GraphQ provides a visual query interface with a query editor and a multi-scale visualization of the results, as well as a user feedback mechanism for refining the results with additional constraints. We demonstrate GraphQ through two example usage scenarios in different application domains: analyzing reusable subroutines in program workflows and semantic scene graph search in images. Quantitative experiments show that NeuroAlign achieves 19%\u201329% improvement in node-alignment accuracy compared to baseline GNN and provides up to 100x speedup compared to combinatorial algorithms. Our qualitative study with domain experts confirms the effectiveness of GraphQ for both usage scenarios.",
        "keywords": [],
        "uid": "v-full-1494",
        "time_stamp": "2021-10-28T18:15:00Z",
        "has_image": true,
        "paper_award": "",
        "image_caption": "",
        "external_paper_link": "",
        "has_pdf": true,
        "ff_link": "https://youtu.be/6dP9SR-ZSSc"
    },
    "v-short-1032": {
        "authors": [
            "Yiran Li",
            "Erin Musabandesu",
            "Takanori Fujiwara",
            "Frank J. Loge",
            "Kwan-Liu Ma"
        ],
        "title": "A Visual Analytics System for Water Distribution System Optimization",
        "session_id": "v-short-short4",
        "abstract": "The optimization of water distribution systems (WDSs) is vital to minimize energy costs required for their operations. A principal approach taken by researchers is identifying an optimal scheme for water pump controls through examining computational simulations of WDSs. However, due to a large number of possible control combinations and the complexity of WDS simulations, it remains non-trivial to identify the best pump controls by reviewing the simulation results. To address this problem, we design a visual analytics system that helps understand relationships between simulation inputs and outputs towards better optimization. Our system incorporates interpretable machine learning as well as multiple linked visualizations to capture essential input-output relationships from complex WDS simulations. We demonstrate our system's effectiveness through a practical case study and evaluate its usability through expert reviews. Our results show that our system can lessen the burden of analysis and assist in determining optimal operating schemes.",
        "keywords": [
            "Feature Detection, Extraction, Tracking & Transformation",
            "Other Application Areas",
            "Application Motivated Visualization",
            "Tabular Data"
        ],
        "uid": "v-short-1032",
        "time_stamp": "2021-10-28T17:00:00Z",
        "has_image": true,
        "paper_award": "",
        "image_caption": "The interface of the visual analytics system for WDS optimization. Here, the NET3 simulations are analyzed. The system is composed of (A) the feature distributions view, (B) simulation overview, (C) summary view, (D) decision tree view, and (E) time-series views.\n",
        "external_paper_link": "",
        "has_pdf": true,
        "ff_link": ""
    },
    "v-short-1035": {
        "authors": [
            "Bella Baidak",
            "Yahiya Hussain",
            "Emma Kelminson",
            "Thouis Ray Jones",
            "Loraine Franke",
            "Daniel Haehn"
        ],
        "title": "CellProfiler Analyst Web (CPAW) - Exploration, analysis, and classification of biological images on the web",
        "session_id": "v-short-short4",
        "abstract": "CellProfiler Analyst has enabled the scientific research community to explore image-based data and classify complex biological phenotypes through an interactive user interface since its release in 2008. This paper describes CellProfiler Analyst Web (CPAW), a newly redesigned and web-based version of the software, allowing for greater accessibility, quicker setup, and facilitates a simple overall workflow for users. Installation and managing new versions has been challenging and time-consuming, historically, for CellProfiler Analyst. CPAW is an alternative that ensures installation and future updates are not a hassle to the user. CPAW ports the core iteration loop of CPA to a pure server-less browser environment using modern web-development technologies, allowing computationally heavy activities, like machine learning, to occur without freezing the UI. With a setup as simple as navigating to a website, CPAW presents a clean user interface to the user to refine their classifier and explore phenotypic data with ease. We evaluated both the old and the new version of the software in an extensive domain expert study. We found that users could complete the essential classification tasks in CPAW and CPA 3.0 with the same efficiency. Additionally, users completed the tasks 20 percent faster using CPAW compared to CPA 3.0.\nThe code of CellProfiler Analyst Web is open-source and available at \\url{https://mpsych.github.io/CellProfilerAnalystWeb/",
        "keywords": [
            "Machine Learning Techniques",
            "Life Sciences, Health, Medicine, Biology, Bioinformatics, Genomics",
            "Machine Learning, Statistics, Modelling, and Simulation Applications",
            "Data Analysis, Reasoning, Problem Solving, and Decision Making",
            "Application Motivated Visualization",
            "Task Abstractions & Application Domains",
            "Human-Subjects Qualitative Studies",
            "Human-Subjects Quantitative Studies",
            "Image and Video Data"
        ],
        "uid": "v-short-1035",
        "time_stamp": "2021-10-28T17:10:00Z",
        "has_image": true,
        "paper_award": "",
        "image_caption": "",
        "external_paper_link": "",
        "has_pdf": true,
        "ff_link": ""
    },
    "v-short-1072": {
        "authors": [
            "Qiangqiang Liu",
            "Tangzhi Ye",
            "Zhihua Zhu",
            "Xiaojuan Ma",
            "Quan Li"
        ],
        "title": "Inspecting the Process of Bank Credit Rating via Visual Analytics",
        "session_id": "v-short-short4",
        "abstract": "Bank credit rating refers to classify commercial banks into different levels based on publicly disclosed and internal information, serving as an important input in financial risk management. However, experts still have a vague idea of exploring and comparing different bank credit rating schemes. A loose connection between subjective and quantitative analysis and difficulties for the experts in determining appropriate indicator weights obscure understanding of bank credit ratings. Furthermore, existing models fail to consider bank types by just applying a unified indicator weight set to all banks. We propose RatingVis to assist experts in exploring and comparing different bank credit rating schemes. It supports interactively inferring indicator weights for banks by involving domain knowledge and considers bank types in the analysis loop. A real-world case study verifies the efficacy of RatingVis.",
        "keywords": [
            "Dimensionality Reduction",
            "Social Science, Education, Humanities, Journalism, Intelligence Analysis, Knowledge Work",
            "Tabular Data"
        ],
        "uid": "v-short-1072",
        "time_stamp": "2021-10-28T17:20:00Z",
        "has_image": true,
        "paper_award": "",
        "image_caption": "We propose a visual analytics system termed RatingVis to assist experts in exploring and comparing different bank ranking and rating schemes.",
        "external_paper_link": "",
        "has_pdf": true,
        "ff_link": ""
    },
    "v-short-1105": {
        "authors": [
            "Avi Yaeli",
            "Sergey Zeltyn"
        ],
        "title": "Where and Why is My Bot Failing? A Visual Analytics Approach for Investigating Failures in Chatbot Conversation Flows",
        "session_id": "v-short-short4",
        "abstract": "The ongoing coronavirus pandemic has accelerated the adoption of AI-powered task-oriented chatbots by businesses and healthcare organizations. Despite advancements in chatbot platforms, implementing a successful and effective bot is still challenging and requires a lot of manual work. There is a strong need for tools to help conversation analysts quickly identify problem areas and, consequently, introduce changes to chatbot design. We present a visual analytics approach and tool for conversation analysts to identify and assess common patterns of failure in conversation flows. We focus on two key capabilities: path flow analysis and root cause analysis. Interim evaluation results from applying our tool in real-world customer production projects are presented.",
        "keywords": [
            "Machine Learning Techniques",
            "Other Topics and Techniques",
            "Data Analysis, Reasoning, Problem Solving, and Decision Making",
            "Application Motivated Visualization",
            "Task Abstractions & Application Domains",
            "High-dimensional Data",
            "Text/Document Data"
        ],
        "uid": "v-short-1105",
        "time_stamp": "2021-10-28T17:30:00Z",
        "has_image": true,
        "paper_award": "",
        "image_caption": "Flow analysis UI. Statistical information such as visits, abandonment and trend over time is presented for each dialog node",
        "external_paper_link": "",
        "has_pdf": true,
        "ff_link": "https://youtu.be/G38QCVe5YfU"
    },
    "v-short-1139": {
        "authors": [
            "Noble Saji Mathews",
            "Sridhar Chimalakonda",
            "Suresh Jain"
        ],
        "title": "AiR: An Augmented Reality Application for Visualizing Air Pollution",
        "session_id": "v-short-short4",
        "abstract": "In order to effectively combat Air Pollution, it is necessary for the government and the community to work together. Easily comprehensible visualizations can play a major role in drawing public attention and spreading awareness about seemingly intangible air pollution. Considering the widespread usage of Android-based devices, in this paper, we propose an Augmented Reality based application called AiR, to help users to visualize pollutants in the air and to create an immersive user experience. AiR aims to interactively engage a wide variety of users and create awareness without overwhelming them with data. AiR visualizes 12 pollutants through the use of unique AR generated particles and chemical models. We demonstrate our application on pollution data by CPCB from various weather stations across India collected over the initial lockdown period due to COVID-19 in India.",
        "keywords": [
            "Physical & Environmental Sciences, Engineering, Mathematics",
            "Mobile, AR/VR/Immersive, Specialized Input/Display Hardware",
            "General Public",
            "Software Prototype",
            "Geospatial Data"
        ],
        "uid": "v-short-1139",
        "time_stamp": "2021-10-28T17:40:00Z",
        "has_image": true,
        "paper_award": "",
        "image_caption": "A few Augmented Reality based visualizations generated by AiR depicting pollution levels and AQI info in Indian cities",
        "external_paper_link": "",
        "has_pdf": true,
        "ff_link": ""
    },
    "v-short-1169": {
        "authors": [
            "Raymond Li",
            "Enamul Hoque",
            "Giuseppe Carenini",
            "Richard Lester",
            "Raymond Chau"
        ],
        "title": "ConVIScope: Visual Analytics for Exploring Patient Conversations",
        "session_id": "v-short-short4",
        "abstract": "The proliferation of text messaging for mobile health is generating a large amount of doctor patient conversations that can be extremely valuable to health care professionals.We present ConVIScope, a visual text analytic system that tightly integrates interactive visualization with natural language processing in analyzing doctor-patient conversations. ConVIScope was developed in collaboration with health-care professionals following a user-centered iterative design. Case studies with six domain experts suggest the potential utility of ConVIScope and reveal lessons for further developments.",
        "keywords": [
            "Life Sciences, Health, Medicine, Biology, Bioinformatics, Genomics",
            "Multi-Resolution and Level of Detail Techniques",
            "Task Abstractions & Application Domains",
            "Text/Document Data"
        ],
        "uid": "v-short-1169",
        "time_stamp": "2021-10-28T17:50:00Z",
        "has_image": true,
        "paper_award": "",
        "image_caption": "The ConVIScope interface combines: the Topic View for displaying the discussion topics along with their frequency in a hierarchy (B), and the Analysis View of  conversations that encodes the distributions of sentiments and topics (C). Here, the user filters conversations by different dimensions in the Metadata View (A) followed by selecting a conversation which is shown in details in the Conversation View (D).",
        "external_paper_link": "",
        "has_pdf": true,
        "ff_link": ""
    },
    "v-short-1087": {
        "authors": [
            "Shahid Latif",
            "Shivam Agarwal",
            "Simon Gottschalk",
            "Carina Chrosch",
            "Yanick Christian Tchenko",
            "Felix Feit",
            "Johannes Jahn",
            "Tobias Braun",
            "Elena Demidova",
            "Fabian Beck"
        ],
        "title": "Visually Connecting Historical Figures Through Event Knowledge Graphs",
        "session_id": "v-short-short4",
        "abstract": "To research lives of historical figures and their interactions with other famous people of the same era, users have to usually go through long text documents. Knowledge graphs store information about entities such as historical figures and their relationships---indirectly through shared events---in a structured manner. We develop a visualization system, \\system{}, for analyzing the intertwined lives of historical figures based on the events they participated. The users' query is parsed for identifying named entities and related data is queried from an event knowledge graph. While a short textual answer to the users' query is generated using the GPT-3 language model, various linked visualizations provide context, display additional information related to the query, and allow in-depth exploration.",
        "keywords": [
            "Domain Agnostic",
            "General Public",
            "Graph/Network and Tree Data",
            "Temporal Data",
            "Text/Document Data"
        ],
        "uid": "v-short-1087",
        "time_stamp": "2021-10-28T18:00:00Z",
        "has_image": true,
        "paper_award": "",
        "image_caption": "VisKonnect offers a natural language interface to search the intertwined lives of historical figures using a mix of text and visualizations. (1) It answers a query about the marriage of Marie Curie and Pierre Curie while the event timeline allows for verification of the answer as well as further exploration of their lives. (2) The answer of the query about Albert Einstein and Erwin Schroedinger lists down Solvay conference as a shared event where two scientists met. The relationship graph on the right also leads to the same information disclosing a photograph where the two scientists can be seen together.  ",
        "external_paper_link": "",
        "has_pdf": true,
        "ff_link": ""
    },
    "v-short-1160": {
        "authors": [
            "Hayeong Song",
            "Yu Fu",
            "Bahador Saket",
            "John Stasko"
        ],
        "title": "Understanding the Effects of Visualizing Missing Values on Visual Data Exploration",
        "session_id": "v-short-short4",
        "abstract": "When performing data analysis, people often confront data sets containing missing values. We conducted an empirical study to understand the effect of visualizing those missing values on participants\u2019 decision-making processes while performing a visual data exploration task. More specifically, our study participants purchased a hypothetical portfolio of stocks based on a data set where some stocks had missing values for attributes such as PE ratio, beta, and EPS. The experiment used scatterplots to communicate the stock data. For one group of participants, stocks with missing values simply were not shown, while the second group saw such stocks depicted with estimated values as points with error bars. We measured participants\u2019 cognitive load involved in decision-making with data with missing values. Our results indicate that their decision-making workflow was different across two conditions.",
        "keywords": [
            "Uncertainty Visualization",
            "Data Analysis, Reasoning, Problem Solving, and Decision Making",
            "Guidelines",
            "Human-Subjects Qualitative Studies",
            "Human-Subjects Quantitative Studies",
            "Data Type Agnostic"
        ],
        "uid": "v-short-1160",
        "time_stamp": "2021-10-28T18:10:00Z",
        "has_image": true,
        "paper_award": "",
        "image_caption": "In the error-bars condition, participants were more likely to stick with their original plan. Their decision-making process was consistent and regular because they were able to weigh the evidence for data items with missing values. In the baseline condition, participants faced challenges when attempting to weigh the evidence to select data items for future selection. One of the challenges was it was difficult for participants to compare such a data item with other data items. This tended to lead participants to be entangled in that phase and they had to regenerate a new strategy.\n",
        "external_paper_link": "",
        "has_pdf": true,
        "ff_link": ""
    },
    "a-visap-1020": {
        "authors": [
            "Marilene Oliver",
            "Gary Joynes Joynes",
            "Kumaradevan Punithakumar",
            "Peter Seres"
        ],
        "title": "Deep Connection: Making Virtual Reality  Artworks with Medical Scan Data",
        "session_id": "a-visap-visap2",
        "abstract": null,
        "keywords": [],
        "uid": "a-visap-1020",
        "time_stamp": "2021-10-28T17:05:00Z",
        "has_image": true,
        "paper_award": "",
        "image_caption": "Deep Connection is an installation and virtual reality artwork made using full body 3D and 4D magnetic resonance scan datasets. When the user enters Deep Connection, they see a scanned body lying prone in mid-air. Deep Connection creates a scenario where an embodied human becomes the companion for a virtual body. This paper maps the conceptual and theoretical framework for Deep Connection such as virtual intimacy and digitally mediated companionship. It also reflects on working with scanned bodies more generally in virtual reality by discussing transparency, the cyberbody versus the data body, as well as data privacy and data ethics.",
        "external_paper_link": "",
        "has_pdf": true,
        "ff_link": "https://youtu.be/t0psCKfzEog"
    },
    "a-visap-1042": {
        "authors": [
            "Guangyu Du",
            "Lei Dong",
            "F\u00e1bio Duarte",
            "Carlo Ratti"
        ],
        "title": "Wanderlust: 3D Impressionism in Human Journeys",
        "session_id": "a-visap-visap2",
        "abstract": null,
        "keywords": [],
        "uid": "a-visap-1042",
        "time_stamp": "2021-10-28T17:18:00Z",
        "has_image": true,
        "paper_award": "",
        "image_caption": "This image visualizes the flows of individuals across the Greater Boston area as lines (visiting frequency as color, number of unique visitors as line width)  that form spatial clusters of attractive places, with the height of mountains representing location-specific attractiveness.",
        "external_paper_link": "",
        "has_pdf": true,
        "ff_link": "https://youtu.be/XiTtkAkdBJk"
    },
    "a-visap-1061": {
        "authors": [
            "Rene Cutura",
            "Katrin Angerbauer",
            "Frank Heyen",
            "Natalie Hube",
            "Michael Sedlmair"
        ],
        "title": "DaRt: Generative Art using Dimensionality Reduction Algorithms",
        "session_id": "a-visap-visap2",
        "abstract": null,
        "keywords": [],
        "uid": "a-visap-1061",
        "time_stamp": "2021-10-28T17:26:00Z",
        "has_image": true,
        "paper_award": "",
        "image_caption": "A generated image, using DaRt. For this image, we used the 4 dimensional IRIS dataset, which was input to the iterative UMAP DR algorithm, which projects high-dimensional data to, here, two dimensions. The indices of the points were mapped to the \"Inferno\" colorscale provided by d3, to draw the voronoi cells with the respective color on top of the image with some opacity.",
        "external_paper_link": "",
        "has_pdf": true,
        "ff_link": "https://youtu.be/2jf2VFjLBVU"
    },
    "a-visap-1039": {
        "authors": [
            "Yifang Wang",
            "Yifan Cao",
            "Junxiu Tang",
            "Yang Wang",
            "Huamin Qu",
            "Yingcai Wu"
        ],
        "title": "Explore Mindfulness without Deflection: A Data Art Based on the Books of Songs",
        "session_id": "a-visap-visap2",
        "abstract": null,
        "keywords": [],
        "uid": "a-visap-1039",
        "time_stamp": "2021-10-28T17:34:00Z",
        "has_image": true,
        "paper_award": "",
        "image_caption": "A data art based on The Book of Songs. The Poem Flow shows a summary of all 305 poems in The Book of Songs. Dimensions include Genres, Functions, Themes, Imageries, Emotions, and Rhetorical Devices (from left to right). The six Imagery Glyphs shows the top frequently appeared imageries in The Book of Songs. From left to right, these glyphs represent: Proso Millet, Mulberry, Swallow, Horse, Pest, and Black Bream.",
        "external_paper_link": "",
        "has_pdf": true,
        "ff_link": "https://youtu.be/FRX2Xflzjk4"
    },
    "a-visap-1037": {
        "authors": [
            "Song Anqi",
            "Xintong Song",
            "Yuhao Chen",
            "Guangyu Luo",
            "Qiansheng Li"
        ],
        "title": "Decoding \u00b7 Encoding \u2013 an exploration of data narrative in Tibetan characters",
        "session_id": "a-visap-visap2",
        "abstract": null,
        "keywords": [],
        "uid": "a-visap-1037",
        "time_stamp": "2021-10-28T18:02:00Z",
        "has_image": true,
        "paper_award": "",
        "image_caption": "",
        "external_paper_link": "",
        "has_pdf": true,
        "ff_link": "https://youtu.be/HdRfwpcP30I"
    },
    "a-visap-1044": {
        "authors": [
            "Ignacio P\u00e9rez-Messina",
            "Ilana Levin"
        ],
        "title": "Spectrographies: Decompositions of Music into Light",
        "session_id": "a-visap-visap2",
        "abstract": null,
        "keywords": [],
        "uid": "a-visap-1044",
        "time_stamp": "2021-10-28T18:07:00Z",
        "has_image": true,
        "paper_award": "",
        "image_caption": "",
        "external_paper_link": "",
        "has_pdf": false,
        "ff_link": "https://youtu.be/sEPH0qqa-Jc"
    },
    "a-visap-1035": {
        "authors": [
            "Junlin Zhu",
            "Juanjuan Long",
            "Yingjing Duan",
            "Wenxuan Zhao"
        ],
        "title": "Invisible Pixel\uff1aShort Video Narratives from Machine Perspective",
        "session_id": "a-visap-visap2",
        "abstract": null,
        "keywords": [],
        "uid": "a-visap-1035",
        "time_stamp": "2021-10-28T18:10:00Z",
        "has_image": true,
        "paper_award": "",
        "image_caption": "",
        "external_paper_link": "",
        "has_pdf": true,
        "ff_link": "https://youtu.be/TtaFlWCAmBI"
    },
    "a-visap-1063": {
        "authors": [
            "Emily Fuhrman"
        ],
        "title": "Side-view States ",
        "session_id": "a-visap-visap2",
        "abstract": null,
        "keywords": [],
        "uid": "a-visap-1063",
        "time_stamp": "2021-10-28T18:14:00Z",
        "has_image": true,
        "paper_award": "",
        "image_caption": "",
        "external_paper_link": "",
        "has_pdf": false,
        "ff_link": "https://youtu.be/OfjRqhYSXIc"
    },
    "a-sciviscontest-1005": {
        "authors": [
            "Tim McGraw",
            "Michael Eddy"
        ],
        "title": "Hybrid rendering for interactive visualization of mantle convection",
        "session_id": "a-sciviscontest-contest",
        "abstract": null,
        "keywords": [],
        "uid": "a-sciviscontest-1005",
        "time_stamp": "2021-10-28T17:22:00Z",
        "has_image": false,
        "paper_award": "",
        "image_caption": "",
        "external_paper_link": "",
        "has_pdf": true,
        "ff_link": ""
    },
    "a-sciviscontest-1006": {
        "authors": [
            "Jonathan Fritsch",
            "Simon Schneegans",
            "Markus Flatken",
            "Andreas Gerndt",
            "Ana-Catalina Plesa",
            "Christian H\u00fcttig"
        ],
        "title": "RayPC: Interactive Ray Tracing Meets Parallel Coordinates",
        "session_id": "a-sciviscontest-contest",
        "abstract": null,
        "keywords": [],
        "uid": "a-sciviscontest-1006",
        "time_stamp": "2021-10-28T17:30:00Z",
        "has_image": false,
        "paper_award": "",
        "image_caption": "",
        "external_paper_link": "",
        "has_pdf": true,
        "ff_link": ""
    },
    "a-sciviscontest-1007": {
        "authors": [
            "Lucas Temor",
            "Peter Coppin",
            "David Steinman"
        ],
        "title": "Visualization of simulated convection dynamics in Earth's mantle",
        "session_id": "a-sciviscontest-contest",
        "abstract": null,
        "keywords": [],
        "uid": "a-sciviscontest-1007",
        "time_stamp": "2021-10-28T17:38:00Z",
        "has_image": false,
        "paper_award": "",
        "image_caption": "",
        "external_paper_link": "",
        "has_pdf": true,
        "ff_link": ""
    },
    "a-sciviscontest-1008": {
        "authors": [
            "Sudhanshu Sane",
            "Tushar Athawale",
            "Chris Johnson"
        ],
        "title": "Investigating Multivariate, Vector, and Topological Data Analysis Techniques for Mantle Flow Pattern Visualization",
        "session_id": "a-sciviscontest-contest",
        "abstract": null,
        "keywords": [],
        "uid": "a-sciviscontest-1008",
        "time_stamp": "2021-10-28T17:46:00Z",
        "has_image": false,
        "paper_award": "",
        "image_caption": "",
        "external_paper_link": "",
        "has_pdf": true,
        "ff_link": ""
    },
    "a-sciviscontest-1010": {
        "authors": [
            "Marina Evers",
            "Simon Leistikow",
            "Adrian Derstroff",
            "Tim Gerrits",
            "Karim Huesmann",
            "Jonathan Hollenbeck",
            "Julian Seljami",
            "Lars Linsen"
        ],
        "title": "Visual Analysis of Spatio-temporal Features in Multi-field Earth's Mantle Convection Simulations",
        "session_id": "a-sciviscontest-contest",
        "abstract": null,
        "keywords": [],
        "uid": "a-sciviscontest-1010",
        "time_stamp": "2021-10-28T17:54:00Z",
        "has_image": false,
        "paper_award": "",
        "image_caption": "",
        "external_paper_link": "",
        "has_pdf": true,
        "ff_link": "https://youtu.be/Lc-qLsLePG8"
    },
    "a-sciviscontest-1011": {
        "authors": [
            "Tim von Hahn",
            "Chris Mechefske"
        ],
        "title": "EarthGAN: Can we visualize the Earth\u2019s mantle convection using a surrogate model?",
        "session_id": "a-sciviscontest-contest",
        "abstract": null,
        "keywords": [],
        "uid": "a-sciviscontest-1011",
        "time_stamp": "2021-10-28T18:02:00Z",
        "has_image": false,
        "paper_award": "",
        "image_caption": "",
        "external_paper_link": "",
        "has_pdf": true,
        "ff_link": ""
    },
    "a-sciviscontest-1012": {
        "authors": [
            "Jansen Wong",
            "Vung Pham",
            "Tommy Dang"
        ],
        "title": "Interactive Multidimensional Visual Analytics for Earth's Mantle Convection",
        "session_id": "a-sciviscontest-contest",
        "abstract": null,
        "keywords": [],
        "uid": "a-sciviscontest-1012",
        "time_stamp": "2021-10-28T18:10:00Z",
        "has_image": false,
        "paper_award": "",
        "image_caption": "",
        "external_paper_link": "",
        "has_pdf": true,
        "ff_link": "https://youtu.be/3e_0qIJtSuk"
    },
    "v-full-1301": {
        "authors": [
            "Leni Yang",
            "Xian XU",
            "Xingyu Lan",
            "Ziyan Liu",
            "Shunan Guo",
            "Yang Shi",
            "Huamin Qu",
            "Nan Cao"
        ],
        "title": "A Design Space for Applying the Freytag's Pyramid Structure to Data Stories",
        "session_id": "v-full-full23",
        "abstract": "Data stories integrate compelling visual content to communicate data insights in the form of narratives. The narrative structure of a data story serves as the backbone that determines its expressiveness, and it can largely influence how audiences perceive the insights. Freytag's Pyramid is a classic narrative structure that has been widely used in film and literature. While there are continuous recommendations and discussions about applying Freytag's Pyramid to data stories, there is little systematic and practical guidance for data story creators on how to use Freytag's Pyramid for structured data story creation. To bridge this gap, we examined how existing practices apply Freytag's Pyramid through analyzing stories extracted from 103 data videos. Based on our findings, we propose a design space of narrative patterns, data flows, and visual communications to provide practical guidance on achieving narrative intents, organizing data facts, and selecting visual design techniques through the story creation process. We evaluated the proposed design space through a workshop with 25 participants. The results show that our design space provides a clear framework for rapid storyboarding of data stories with Freytag's Pyramid.",
        "keywords": [],
        "uid": "v-full-1301",
        "time_stamp": "2021-10-29T13:00:00Z",
        "has_image": true,
        "paper_award": "",
        "image_caption": "{\\rtf1\\ansi\\ansicpg936\\cocoartf2580\n\\cocoatextscaling0\\cocoaplatform0{\\fonttbl\\f0\\fswiss\\fcharset0 Helvetica;\\f1\\froman\\fcharset0 Times-Roman;}\n{\\colortbl;\\red255\\green255\\blue255;\\red0\\green0\\blue0;}\n{\\*\\expandedcolortbl;;\\cssrgb\\c0\\c0\\c0;}\n\\paperw11900\\paperh16840\\margl1440\\margr1440\\vieww11520\\viewh8400\\viewkind0\n\\pard\\tx566\\tx1133\\tx1700\\tx2267\\tx2834\\tx3401\\tx3968\\tx4535\\tx5102\\tx5669\\tx6236\\tx6803\\pardirnatural\\partightenfactor0\n\n\\f0\\fs24 \\cf0 The image shows our design space for applying Freytag\\'92s Pyramid to data stories. \n\\f1\\fs32 \\cf2 \\expnd0\\expndtw0\\kerning0\nOur design space considers the setting, rising-climax, and resolution stages in Freytag\\'92s pyramid. Then for each stage of the Freytag\\'92s Pyramid, our design space provides design strategies regarding three dimensions: the narrative pattern, the data flow, and the visual communication. Importantly, the data flow and visual communication dimension center around the narrative pattern dimension and provide strategies for each narrative pattern.\n\\fs24 \\\n\\\n}",
        "external_paper_link": "",
        "has_pdf": true,
        "ff_link": "https://youtu.be/l7mmyofcD8A"
    },
    "v-full-1072": {
        "authors": [
            "Xingyu Lan",
            "Yang Shi",
            "Yanqiu Wu",
            "Xiaohan Jiao",
            "Nan Cao"
        ],
        "title": "Kineticharts: Augmenting Affective Expressiveness of Charts in Data Stories with Animation Design",
        "session_id": "v-full-full23",
        "abstract": "Data stories often seek to elicit affective feelings from viewers. However, how to design affective data stories remains under-explored. In this work, we investigate one specific design factor, animation, and present Kineticharts, an animation design scheme for creating charts that express five positive affects: joy, amusement, surprise, tenderness, and excitement. These five affects were found to be frequently communicated through animation in data stories. Regarding each affect, we designed varied kinetic motions represented by bar charts, line charts, and pie charts, resulting in 60 animated charts for the five affects. We designed Kineticharts by first conducting a need-finding study with professional practitioners from data journalism and then analyzing a corpus of affective motion graphics to identify salient kinetic patterns. We evaluated Kineticharts through two user studies. The results suggest that Kineticharts can accurately convey affects, and improve the expressiveness of data stories, as well as enhance user engagement without hindering data comprehension compared to the animation design from DataClips, an authoring tool for data videos.",
        "keywords": [],
        "uid": "v-full-1072",
        "time_stamp": "2021-10-29T13:15:00Z",
        "has_image": true,
        "paper_award": "",
        "image_caption": "We present Kinecticharts, an animation design scheme for creating affective charts in data stories. Kineticharts consist of two main dimensions: (i) five positive affects, including joy, amusement, surprise, tenderness, and excitement, and (ii) three charts, including bar charts, line charts, and pie charts. We also tagged Kineticharts with five types of editorial layer, including chart marks, chart axes, chart as a whole, embellishment, and camera to describe which objects in a chart have been animated to communicate affective messages. The full version of Kineticharts can be viewed at https://kineticharts.idvxlab.com/.",
        "external_paper_link": "",
        "has_pdf": true,
        "ff_link": "https://youtu.be/e-CXSOpKuyY"
    },
    "v-full-1600": {
        "authors": [
            "Zezhong Wang",
            "Hugo Romat",
            "Fanny Chevalier",
            "Nathalie Henry Riche",
            "Dave Murray-Rust",
            "Benjamin Bach"
        ],
        "title": "Interactive Data Comics",
        "session_id": "v-full-full23",
        "abstract": "This paper investigates how to make data comics interactive. Data comics are an effective and versatile means for visual communication, leveraging the power of sequential narration and combined textual and visual content, while providing an overview of the storyline through panels assembled in expressive layouts. While a powerful static storytelling medium that works well on paper support, adding interactivity to data comics can enable non-linear storytelling, personalization, levels of details, explanations, and potentially enriched user experiences. This paper introduces a set of operations tailored to support data comics narrative goals that go beyond the traditional linear, immutable storyline curated by a story author. The goals and operations include adding and removing panels into pre-defined layouts to support branching, change of perspective, or access to detail-on-demand, as well as providing and modifying data, and interacting with data representation, to support personalization and reader-defined data focus. We propose a lightweight specification language, COMICSCRIPT, for designers to add such interactivity to static comics. To assess the viability of our authoring process, we recruited six professional illustrators, designers and data comics enthusiasts and asked them to craft an interactive comic, allowing us to understand authoring workflow and potential of our approach. We present examples of interactive comics in a gallery. This initial step towards understanding the design space of interactive comics can inform the design of creation tools and experiences for interactive storytelling.",
        "keywords": [],
        "uid": "v-full-1600",
        "time_stamp": "2021-10-29T13:30:00Z",
        "has_image": true,
        "paper_award": "",
        "image_caption": "Data comics are an effective genre for presenting data-driven stories, these are often presented as static. By using ComicScript, creators can add interactions to static data comics through a set of operations, using interactivities like clicking or hovering to change content, layout, structure, and appearance.\n",
        "external_paper_link": "",
        "has_pdf": true,
        "ff_link": "https://youtu.be/w8gLs-jYm04"
    },
    "v-tvcg-9385921": {
        "authors": [
            "Kiran Ajani",
            "Elsie Lee",
            "Cindy Xiong",
            "Cole Knaflic",
            "William Kemper",
            "Steven Franconeri"
        ],
        "title": "Declutter and Focus: Empirically Evaluating Design Guidelines for Effective Data Communication",
        "session_id": "v-full-full23",
        "abstract": "Data visualization design has a powerful effect on which patterns we see as salient and how quickly we see them. The visualization practitioner community prescribes two popular guidelines for creating clear and efficient visualizations: declutter and focus. The declutter guidelines suggest removing non-critical gridlines, excessive labeling of data values, and color variability to improve aesthetics and to maximize the emphasis on the data relative to the design itself. The focus guidelines for explanatory communication recommend including a clear headline that describes the relevant data pattern, highlighting a subset of relevant data values with a unique color, and connecting those values to written annotations that contextualize them in a broader argument. We evaluated how these recommendations impact recall of the depicted information across cluttered, decluttered, and decluttered+focused designs of six graph topics. Undergraduate students were asked to redraw previously seen visualizations, to recall their topics and main conclusions, and to rate the varied designs on aesthetics, clarity, professionalism, and trustworthiness. Decluttering designs led to higher ratings on professionalism, and adding focus to the design led to higher ratings on aesthetics and clarity. They also showed better memory for the highlighted pattern in the data, as reflected across redrawings of the original visualization and typed free-response conclusions, though we do not know whether these results would generalize beyond our memory-based tasks. The results largely empirically validate the intuitions of visualization designers and practitioners. The stimuli, data, analysis code, and Supplementary Materials are available at https://osf.io/wes9u/.",
        "keywords": [
            "H.1.2.a Human factors < H.1.2 User/Machine Systems < H.1 Models and Principles < H Information Technology and Systems",
            "H.5.3.d Evaluation/methodology < H.5.3 Group and Organization Interfaces < H.5 Information Interfaces and Representation (HCI) <",
            "H.5.2.e Evaluation/methodology < H.5.2 User Interfaces < H.5 Information Interfaces and Representation (HCI) < H Information Tec",
            "H.5.1.d Evaluation/methodology < H.5.1 Multimedia Information Systems < H.5 Information Interfaces and Representation (HCI) < H"
        ],
        "uid": "v-tvcg-9385921",
        "time_stamp": "2021-10-29T13:45:00Z",
        "has_image": true,
        "paper_award": "",
        "image_caption": "We empirically evaluated the effects of two visualization design themes frequently prescribed by practitioner guides: declutter and focus. Decluttered designs showed small advantages for subjective ratings, and adding focus to the designs showed additional subjective rating advantages, along with a strong influence on what data pattern was remembered by viewers.",
        "external_paper_link": "https://doi.ieeecomputersociety.org/10.1109/TVCG.2021.3068337",
        "has_pdf": false,
        "ff_link": "https://youtu.be/ByGd1rZ6dJM"
    },
    "v-tvcg-9189859": {
        "authors": [
            "Luiz Morais",
            "Yvonne Jansen",
            "Nazareno Andrade",
            "Pierre Dragicevic"
        ],
        "title": "Showing Data about People: A Design Space of Anthropographics",
        "session_id": "v-full-full23",
        "abstract": "When showing data about people, visualization designers and data journalists often use design strategies that presumably help the audience relate to those people. The term anthropographics has been recently coined to refer to this practice and the resulting visualizations. Anthropographics is a rich and growing area, but the work so far has remained scattered. Despite preliminary empirical work and a few web essays written by practitioners, there is a lack of clear language for thinking about and communicating about anthropographics. We address this gap by introducing a conceptual framework and a design space for anthropographics. Our design space consists of seven elementary design dimensions that can be reasonably hypothesized to have some effect on prosocial feelings or behavior. It extends a previous design space and is informed by an analysis of 105 visualizations collected from newspapers, websites, and research papers. We use our conceptual framework and design space to discuss trade-offs, common design strategies, as well as future opportunities for design and research in the area of anthropographics.",
        "keywords": [
            "Anthropographics",
            "Design Space",
            "Empathy",
            "Compassion",
            "Prosocial Behavior"
        ],
        "uid": "v-tvcg-9189859",
        "time_stamp": "2021-10-29T14:00:00Z",
        "has_image": true,
        "paper_award": "",
        "image_caption": "The image shows the seven dimensions of the anthropographics design space. They are divided into two groups: \u201cwhat is shown\u201d and \u201chow it is shown\u201d. The former group is composed of the dimensions of granularity, specificity, coverage, and authenticity. The latter group contains the dimensions of realism, physicality, and situatedness.",
        "external_paper_link": "https://doi.ieeecomputersociety.org/10.1109/TVCG.2020.3023013",
        "has_pdf": false,
        "ff_link": "https://youtu.be/jxS52dKN8F8"
    },
    "v-full-1376": {
        "authors": [
            "Cindy Xiong",
            "Vidya Setlur",
            "Benjamin Bach",
            "Kylie Lin",
            "Eunyee Koh",
            "Steven Franconeri"
        ],
        "title": "Visual Arrangements of Bar Charts Influence Comparisons in Viewer Takeaways",
        "session_id": "v-full-full23",
        "abstract": "Well-designed data visualizations can lead to more powerful and intuitive processing by a viewer. To help a viewer intuitively compare values to quickly generate key takeaways, visualization designers can manipulate how data values are arranged in a chart to afford particular comparisons. Using simple bar charts as a case study, we empirically tested the comparison affordances of four common arrangements: vertically juxtaposed, horizontally juxtaposed, overlaid, and stacked. We asked participants to type out what patterns they perceived in a chart and we coded their takeaways into types of comparisons. In a second study, we asked data visualization design experts to predict which arrangement they would use to afford each type of comparison and found both alignments and mismatches with our findings. These results provide concrete guidelines for how both human designers and automatic chart recommendation systems can make visualizations that help viewers extract the ``right'' takeaway.",
        "keywords": [],
        "uid": "v-full-1376",
        "time_stamp": "2021-10-29T14:15:00Z",
        "has_image": true,
        "paper_award": "",
        "image_caption": "Visualization designers can manipulate how data values are arranged in a chart to afford particular comparisons, which helps a viewer intuitively compare values and quickly generate key takeaways. In the vertical arrangement, viewers tend to zoom in onto one group and compare values within that group. In the overlaid arrangement, viewers tend to notice interactions between bar pairs. In the adjacent arrangement, viewers tend to group and compare spatially separated bar sets as two units. In the stacked arrangement, viewers tend to identify one stacked bar and compare the two members within it.",
        "external_paper_link": "",
        "has_pdf": true,
        "ff_link": "https://youtu.be/uQWXEvr8Qlc"
    },
    "v-full-1064": {
        "authors": [
            "Matthew Brehmer",
            "Robert Kosara",
            "Carmen Hull"
        ],
        "title": "Generative Design Inspiration for Glyphs with Diatoms",
        "session_id": "v-full-full11",
        "abstract": "We introduce Diatoms, a technique that generates design inspiration for glyphs by sampling from palettes of mark shapes, encoding channels, and glyph scaffold shapes. Diatoms allows for a degree of randomness while respecting constraints imposed by columns in a data table: their data types and domains as well as semantic associations between columns as specified by the designer. We pair this generative design process with two forms of interactive design externalization that enable comparison and critique of the design alternatives. First, we incorporate a familiar small multiples configuration in which every data point is drawn according to a single glyph design, coupled with the ability to page between alternative glyph designs. Second, we propose a small permutables design gallery, in which a single data point is drawn according to each alternative glyph design, coupled with the ability to page between data points. We demonstrate an implementation of our technique as an extension to Tableau featuring three example palettes, and to better understand how Diatoms could fit into existing design workflows, we conducted interviews and chauffeured demos with 12 designers. Finally, we reflect on our process and the designers\u2019 reactions, discussing the potential of our technique in the context of visualization authoring systems. Ultimately, our approach to glyph design and comparison can kickstart and inspire visualization design, allowing for the serendipitous discovery of shape and channel combinations that would have otherwise been overlooked.",
        "keywords": [],
        "uid": "v-full-1064",
        "time_stamp": "2021-10-29T13:00:00Z",
        "has_image": true,
        "paper_award": "",
        "image_caption": "Sampling from palettes of mark shapes, encoding channels, and scaffold shapes (left), Diatoms generated 20 alternative glyph designs for an urban mobility dataset (center), displayed as Small Permutables (a single data point drawn according to each design specification). We highlight two designs as Small Multiples (right):  city\u2019s region, area, and population correspond with the drop\u2019s fill color, size, and rotation in A and with the hexagon\u2019s fill color, rotation, and alpha level in B; a city\u2019s bike, transit, and walk scores correspond with the purple, brown, and pink waves\u2019 amplitudes in A and with the stars\u2019 rotations in B.",
        "external_paper_link": "",
        "has_pdf": true,
        "ff_link": "https://youtu.be/xm2Hn7ykKt4"
    },
    "v-full-1590": {
        "authors": [
            "Lu Ying",
            "Tan Tang",
            "Yuzhe Luo",
            "Lvkesheng Shen",
            "Xiao Xie",
            "Lingyun Yu",
            "Yingcai Wu"
        ],
        "title": "GlyphCreator: Towards Example-Based Automatic Generation of Circular Glyphs",
        "session_id": "v-full-full11",
        "abstract": "Circular glyphs are widely used in different fields because of their effectiveness in representing multidimensional data. However, the creation of circular glyphs remains a difficult task due to the demand for professional design skills and laborious design processes. This paper presents an interactive authoring tool called GlyphCreator to support the example-based generation of circular glyphs. Given an example circular glyph and multidimensional input data, GlyphCreator can promptly generate a list of design candidates and supports interactive editing on the candidates to satisfy different design requirements. To develop GlyphCreator, we first derive a design space of circular glyphs from summarizing the relation between different visual elements. With this design space, we build a circular glyph dataset and develop a deep learning model for glyph parsing. The model is able to deconstruct a circular glyph bitmap into a series of visual elements. Next, we propose an interface with effective interactions to help users bind the input data attributes to visual elements and customize visual styles. We evaluate the parsing model through a quantitative experiment and demonstrate the use of GlyphCreator through a usage scenario. The effectiveness of GlyphCreator is confirmed through user interviews.",
        "keywords": [],
        "uid": "v-full-1590",
        "time_stamp": "2021-10-29T13:15:00Z",
        "has_image": true,
        "paper_award": "",
        "image_caption": "Circular glyphs in (a1) EnsembleLens [68], (b1) SmartAdP [35], (c1) VisMatcher [28], (d1) VAICo [51], (e1) SeqDynamics [65], (f1) Visual IVO Editor [38], (g1) DropoutSeer [8], (h1) MutualRanker [34]. (a2)-(h2) shows the circular glyphs generated by GlyphCreator based on (a1)-(h1), respectively. Dashed arrows are used to associate glyph components with corresponding categories.",
        "external_paper_link": "",
        "has_pdf": true,
        "ff_link": "https://youtu.be/fSRTmJnPaMo"
    },
    "v-tvcg-9328215": {
        "authors": [
            "Ruizhen Hu",
            "Bin Chen",
            "Juzhan Xu",
            "Oliver van Kaick",
            "Oliver Deussen",
            "Hui Huang"
        ],
        "title": "Shape-driven Coordinate Ordering for Star Glyph Sets via Reinforcement Learning",
        "session_id": "v-full-full11",
        "abstract": "We present a neural optimization model trained with reinforcement learning to solve the coordinate ordering problem for sets of star glyphs. Given a set of star glyphs associated to multiple class labels, we propose to use shape context descriptors to measure the perceptual distance between pairs of glyphs, and use the derived silhouette coefficient to measure the perception of class separability within the entire set. To find the optimal coordinate order for the given set, we train a neural network using reinforcement learning to reward orderings with high silhouette coefficients. The network consists of an encoder and a decoder with an attention mechanism. The encoder employs a recurrent neural network (RNN) to encode input shape and class information, while the decoder together with the attention mechanism employs another RNN to output a sequence with the new coordinate order. In addition, we introduce a neural network to efficiently estimate the similarity between shape context descriptors, which allows to speed up the computation of silhouette coefficients and thus the training of the axis ordering network. Two user studies demonstrate that the orders provided by our method are preferred by users for perceiving class separation. We tested our model on different settings to show its robustness and generalization abilities and demonstrate that it allows to order input sets with unseen data size, data dimension, or number of classes. We also demonstrate that our model can be adapted to coordinate ordering of other types of plots such as RadViz by replacing the proposed shape-aware silhouette coefficient with the corresponding quality metric to guide network training.",
        "keywords": [
            "Star glyph set",
            "coordinate ordering",
            "reinforcement learning",
            "shape context"
        ],
        "uid": "v-tvcg-9328215",
        "time_stamp": "2021-10-29T13:30:00Z",
        "has_image": true,
        "paper_award": "",
        "image_caption": "Ordering axes for a set of star glyphs using reinforcement learning. For the same set of high-dimensional data points, we can optimize the axis order of the corresponding star glyphs according to different perceptual criteria: (up) spike and salient shape strategies that place dissimilar axes close-by; (down) maximizing class separability. The class labels of the glyphs are indicated by blue/red color. Star glyphs of the same data point (but with different axis orders) are drawn at the same position inside each plot for comparison.",
        "external_paper_link": "https://doi.org/10.1109/TVCG.2021.3052167",
        "has_pdf": false,
        "ff_link": "https://youtu.be/DID4YjRYYjI"
    },
    "v-full-1435": {
        "authors": [
            "Yunhai Wang",
            "Da Cheng",
            "Zhirui Wang",
            "Jian Zhang",
            "Liang Zhou",
            "Gaoqi He",
            "Oliver Deussen"
        ],
        "title": "F2-Bubbles: Faithful Bubble Set Construction and Flexible Editing",
        "session_id": "v-full-full11",
        "abstract": "In this paper, we propose F2-Bubbles, a set overlay visualization technique that addresses overlapping artifacts and supports interactive editing with intelligent suggestions. The core of our method is a new, ef\ufb01cient set overlay construction algorithm that approximates the optimal set overlay by considering set elements and their non-set neighbors. Thanks to the ef\ufb01ciency of the algorithm, interactive editing is achieved, and with intelligent suggestions, users can easily and \ufb02exibly edit visualizations through direct manipulations with local adaptations. A quantitative comparison with state-of-the-art set visualization techniques and case studies demonstrate the effectiveness of our method and suggests that F2-Bubbles is a helpful technique for set visualization.",
        "keywords": [],
        "uid": "v-full-1435",
        "time_stamp": "2021-10-29T13:45:00Z",
        "has_image": true,
        "paper_award": "",
        "image_caption": "Using F2-Bubbles, a set overlay visualization can be generated automatically that is comparable to manually made bubble sets. With the synergy of the aforementioned interactions, the user is able to conveniently achieve visualizations that are comparable to manually made set overlays within a short time.In comparison, the visualization of Bubble Sets does not retain a similar style.",
        "external_paper_link": "",
        "has_pdf": true,
        "ff_link": "https://youtu.be/AeQuMMXHWl8"
    },
    "v-full-1477": {
        "authors": [
            "Rebecca Kehlbeck",
            "Jochen G\u00f6rtler",
            "Yunhai Wang",
            "Oliver Deussen"
        ],
        "title": "spEuler: Semantics-preserving Euler Diagrams",
        "session_id": "v-full-full11",
        "abstract": "Creating comprehensible visualizations of highly overlapping set-typed data is a challenging task due to its complexity. \n  To facilitate insights into set connectivity and to leverage semantic relations between intersections, we propose a fast two-step layout technique for Euler diagrams that are both well-matched and well-formed. \n  Our method conforms to established form guidelines for Euler diagrams regarding semantics, aesthetics, and readability. \n  First, we establish an initial ordering of the data, which we then use to incrementally create a planar, connected, and monotone dual graph representation. \n  In the next step, the graph is transformed into a circular layout that maintains the semantics and yields simple Euler diagrams with smooth curves. \n  When the data cannot be represented by simple diagrams, our algorithm always falls back to a solution that is not well-formed but still well-matched, whereas previous methods often fail to produce expected results. \n  We show the usefulness of our method for visualizing set-typed data using examples from text analysis and infographics. \n  Furthermore, we discuss the characteristics of our approach and evaluate our method against state-of-the-art methods.",
        "keywords": [],
        "uid": "v-full-1477",
        "time_stamp": "2021-10-29T14:00:00Z",
        "has_image": true,
        "paper_award": "honorable",
        "image_caption": "Venn diagrams are often used to highlight complex interactions of sets. This example fromxkcd.com shows which adjectives can be used in combination. Using our method, we can recreate the original Venn diagram. However, the diagram contains empty intersections. In these cases, Euler diagrams provide a more faithful representation of the data.",
        "external_paper_link": "",
        "has_pdf": true,
        "ff_link": ""
    },
    "v-tvcg-8967163": {
        "authors": [
            "Mohak Patel",
            "David Laidlaw"
        ],
        "title": "Visualization of 3D Stress Tensor Fields Using Superquadric Glyphs on Displacement Streamlines",
        "session_id": "v-full-full11",
        "abstract": "Stress tensor fields play a central role in solid mechanics studies, but their visualization in 3D space remains challenging as the information-dense multi-variate tensor needs to be sampled in 3D space while avoiding clutter. Taking cues from current tensor visualizations, we adapted glyph-based visualization for stress tensors in 3D space. We also developed a testing framework and performed user studies to evaluate the various glyph-based tensor visualizations for objective accuracy measures, and subjective user feedback for each visualization method. To represent the stress tensor, we color encoded the original superquadric glyph, and in the user study, we compared it to superquadric glyphs developed for second-order symmetric tensors. We found that color encoding improved the user accuracy measures, while the users also rated our method the highest. We compared our method of placing stress tensor glyphs on displacement streamlines to the glyph placement on a 3D grid. In the visualization, we modified the glyph to show both the stress tensor and the displacement vector at each sample point. The participants preferred our method of glyph placement on displacement streamlines as it highlighted the underlying continuous structure in the tensor field.",
        "keywords": [
            "ensors",
            "Stress",
            "Visualization",
            "Three-dimensional displays",
            "Data visualization",
            "Clutter",
            "Solids",
            "3D stress tensor field",
            "visualization",
            "glyph",
            "glyph placement",
            "virtual reality",
            "user study"
        ],
        "uid": "v-tvcg-8967163",
        "time_stamp": "2021-10-29T14:15:00Z",
        "has_image": true,
        "paper_award": "",
        "image_caption": "",
        "external_paper_link": "https://doi.org/10.1109/TVCG.2020.2968911",
        "has_pdf": false,
        "ff_link": "https://youtu.be/tnHDF3v2fvQ"
    },
    "v-full-1220": {
        "authors": [
            "Dongping Zhang",
            "Eytan Adar",
            "Jessica Hullman"
        ],
        "title": "Visualizing Uncertainty in Probabilistic Graphs with Network Hypothetical Outcome Plots (NetHOPs)",
        "session_id": "v-full-full12",
        "abstract": "Probabilistic networks are challenging to visualize using the traditional node-link diagram. Encoding edge probability using visual variables like width or fuzziness makes it dif\ufb01cult for users of static network visualizations to estimate network statistics like densities, isolates, path lengths, or clustering under uncertainty. We introduce Network Hypothetical Outcome Plots (NetHOPs), a visualization technique that animates a sequence of network realizations sampled from a network distribution de\ufb01ned by probabilistic edges. NetHOPs employ an aggregation and anchoring algorithm used in dynamic and longitudinal graph drawing to parameterize layout stability to support uncertainty estimation. We present a community matching algorithm we developed to enable visualizing the uncertainty of cluster membership and community occurrence. We describe the results of a study in which 51 network experts used NetHOPs to complete a set of common visual analysis tasks and reported how they perceived network structures and properties subject to uncertainty. Participants\u2019 estimates fell, on average, within 11% of the ground truth statistics, suggesting NetHOPs can be a reasonable approach for enabling network analysts to reason about multiple properties under uncertainty. Participants appeared able to articulate the distribution of network statistics slightly more accurately when they could manipulate the layout anchoring and the animation speed. Based on these \ufb01ndings, we synthesize design recommendations for developing and using animated network visualizations for probabilistic networks.",
        "keywords": [],
        "uid": "v-full-1220",
        "time_stamp": "2021-10-29T13:00:00Z",
        "has_image": true,
        "paper_award": "",
        "image_caption": "Network Hypothetical Outcome Plots (NetHOPs) is a frequency-based uncertainty visualization technique designed to communicate the uncertainty embedded in probabilistic graphs. NetHOPs' visualization function takes a probabilistic graph as input and samples a set of network realizations from a network distribution formed by probabilistically weighted edges. Layouts of all sampled realizations are aggregated to preserve the viewer's mental map. NetHOPs can support common network analysis tasks, such as community detection, for which we developed an instant optimal cluster matching algorithm so that NetHOPs can communicate the uncertainty of cluster distributions.",
        "external_paper_link": "",
        "has_pdf": true,
        "ff_link": ""
    },
    "v-tvcg-9451614": {
        "authors": [
            "Georgia Panagiotidou",
            "Ralf Vandam",
            "Jeroen Poblome",
            "Andrew Vande Moere"
        ],
        "title": "Implicit Error, Uncertainty and Confidence in Visualization: an Archaeological Case Study",
        "session_id": "v-full-full12",
        "abstract": "While we know that the visualization of quantifiable uncertainty impacts the confidence in insights, little is known about whether the same is true for uncertainty that originates from aspects so inherent to the data that they can only be accounted for qualitatively. Being embedded within an archaeological project, we realized how assessing such qualitative uncertainty is crucial in gaining a holistic and accurate understanding of regional spatio-temporal patterns of human settlements over millennia. We therefore investigated the impact of visualizing qualitative implicit errors on the sense-making process via a probe that deliberately represented three distinct implicit errors, i.e. differing collection methods, subjectivity of data interpretations and assumptions on temporal continuity. By analyzing the interactions of 14 archaeologists with different levels of domain expertise, we discovered that novices became more actively aware of typically overlooked data issues and domain experts became more confident of the visualization itself. We observed how participants quoted social factors to alleviate some uncertainty, while in order to minimize it they requested additional contextual breadth or depth of the data. While our visualization did not alleviate all uncertainty, we recognized how it sparked reflective meta-insights regarding methodological directions of the data. We believe our findings inform future visualizations on how to handle the complexity of implicit errors for a range of user typologies and for highly data-critical application domains such as the digital humanities.",
        "keywords": [
            "data uncertainty",
            "data visualization",
            "implicit error",
            "qualitative study",
            "digital humanities",
            "design study",
            "archaeology"
        ],
        "uid": "v-tvcg-9451614",
        "time_stamp": "2021-10-29T13:15:00Z",
        "has_image": true,
        "paper_award": "",
        "image_caption": "SiteVis is a visualization tool for archaeological site data. Given that archaeological site data included implicit errors, SiteVis tried to account for them by using separate data views, by encoding the data collection methods as well as showing the sites in both time and era scales. We used the SiteVis visualization as a probe, and documented both qualitatively and quantitatively the insight-making process of 14 expert and novice archaeologists. We then analysed the origins of uncertainty in their process as well as documented how these archaeologists tried to cope with it.",
        "external_paper_link": "https://doi.org/10.1109/tvcg.2021.3088339",
        "has_pdf": false,
        "ff_link": "https://youtu.be/XWf2SunPFVg"
    },
    "v-full-1696": {
        "authors": [
            "Spencer Castro",
            "Helia Hosseinpour",
            "P. Samuel Quinan",
            "Lace Padilla"
        ],
        "title": "Examining Effort in 1D Uncertainty Communication Using Individual Differences in Working Memory and NASA-TLX",
        "session_id": "v-full-full12",
        "abstract": "As uncertainty visualizations for general audiences become increasingly common, designers must understand the full impact of uncertainty communication techniques on viewers' decision processes. Prior work demonstrates mixed performance outcomes with respect to how individuals make decisions using various visual and textual depictions of uncertainty. Part of the inconsistency across findings may be due to an over-reliance on task accuracy, which cannot, on its own, provide a comprehensive understanding of how uncertainty visualization techniques support reasoning processes. In this work, we advance the debate surrounding the efficacy of modern 1D uncertainty visualizations by conducting converging quantitative and qualitative analyses of both the effort and strategies used by individuals when provided with quantile dotplots, density plots, interval plots, mean plots, and textual descriptions of uncertainty. We utilize two approaches for examining effort across uncertainty communication techniques: a measure of individual differences in working-memory capacity known as an operation span (OSPAN) task and self-reports of perceived workload via the NASA-TLX. The results reveal that both visualization methods and working-memory capacity impact participants' decisions. Specifically, quantile dotplots and density plots (i.e., distributional annotations) result in more accurate judgments than interval plots, textual descriptions of uncertainty, and mean plots (i.e., summary annotations). Additionally, participants' open-ended responses suggest that individuals viewing distributional annotations are more likely to employ a strategy that explicitly incorporates uncertainty into their judgments than those viewing summary annotations. When comparing quantile dotplots to density plots, this work finds that both methods are equally effective for low-working-memory individuals. However, for individuals with high-working-memory capacity, quantile dotplots evoke more accurate responses with less perceived effort. Given these results, we advocate for the inclusion of converging behavioral and subjective workload metrics in addition to accuracy performance to further disambiguate meaningful differences among visualization techniques.",
        "keywords": [],
        "uid": "v-full-1696",
        "time_stamp": "2021-10-29T13:30:00Z",
        "has_image": true,
        "paper_award": "",
        "image_caption": "An empirical study of working memory demand finds that quantile dot plots and density plots (i.e., distributional annotations) require less effort than interval plots, textual descriptions of uncertainty, and mean plots (i.e., summary annotations). ",
        "external_paper_link": "",
        "has_pdf": true,
        "ff_link": ""
    },
    "v-tvcg-9405484": {
        "authors": [
            "Jouni Helske",
            "Satu Helske",
            "Matthew Cooper",
            "Anders Ynnerman",
            "Lonni Besan\u00e7on"
        ],
        "title": "Can Visualization Alleviate Dichotomous Thinking? Effects of Visual Representations on the Cliff Effect",
        "session_id": "v-full-full12",
        "abstract": "Common reporting styles for statistical results in scientific articles, such as p-values and confidence intervals (CI), have been reported to be prone to dichotomous interpretations, especially with respect to the null hypothesis significance testing framework. For example when the p-value is small enough or the CIs of the mean effects of a studied drug and a placebo are not overlapping, scientists tend to claim significant differences while often disregarding the magnitudes and absolute differences in the effect sizes. This type of reasoning has been shown to be potentially harmful to science. Techniques relying on the visual estimation of the strength of evidence have been recommended to reduce such dichotomous interpretations but their effectiveness has also been challenged. We ran two experiments on researchers with expertise in statistical analysis to compare several alternative representations of confidence intervals and used Bayesian multilevel models to estimate the effects of the representation styles on differences in researchers' subjective confidence in the results. We also asked the respondents' opinions and preferences in representation styles. Our results suggest that adding visual information to classic CI representation can decrease the tendency towards dichotomous interpretations - measured as the `cliff effect': the sudden drop in confidence around p-value 0.05 - compared with classic CI visualization and textual representation of the CI with p-values. All data and analyses are publicly available at https://github.com/helske/statvis.",
        "keywords": [
            "Statistical inference, visualization",
            "cliff effect",
            "confidence intervals",
            "hypothesis testing",
            "Bayesian inference"
        ],
        "uid": "v-tvcg-9405484",
        "time_stamp": "2021-10-29T13:45:00Z",
        "has_image": false,
        "paper_award": "",
        "image_caption": "",
        "external_paper_link": "https://doi.org/10.1109/TVCG.2021.3073466",
        "has_pdf": false,
        "ff_link": "https://youtu.be/OS6D8IOLE1Q"
    },
    "v-full-1028": {
        "authors": [
            "Ryan Wesslen",
            "Alireza Karduni",
            "Doug Markant",
            "Wenwen Dou"
        ],
        "title": "Effect of Uncertainty Visualizations on Myopic Loss Aversion and the Equity Premium Puzzle in Retirement Investment Decisions",
        "session_id": "v-full-full12",
        "abstract": "For many households, investing for retirement is one of the most significant decisions and is fraught with uncertainty. In a classic study in behavioral economics, Benartzi and Thaler (1999) found evidence using bar charts that investors exhibit myopic loss aversion in retirement decisions: Investors overly focus on the potential for short-term losses, leading them to invest less in riskier assets and miss out on higher long-term returns. Recently, advances in uncertainty visualizations have shown improvements in decision-making under uncertainty in a variety of tasks. In this paper, we conduct a controlled and incentivized crowdsourced experiment replicating Benartzi and Thaler (1999) and extending it to measure the effect of different uncertainty representations on myopic loss aversion. Consistent with the original study, we find evidence of myopic loss aversion with bar charts and find that participants make better investment decisions with longer evaluation periods. We also find that common uncertainty representations such as interval plots and bar charts achieve the highest mean expected returns while other uncertainty visualizations lead to poorer long-term performance and strong effects on the equity premium. Qualitative feedback further suggests that different uncertainty representations lead to visual reasoning heuristics that can either mitigate or encourage a focus on potential short-term losses. We discuss implications of our results on using uncertainty visualizations for retirement decisions in practice and possible extensions for future work.",
        "keywords": [],
        "uid": "v-full-1028",
        "time_stamp": "2021-10-29T14:00:00Z",
        "has_image": true,
        "paper_award": "",
        "image_caption": "In a classic study in behavioral economics, Benartzi and Thaler (1999) found evidence using bar charts that investors exhibit myopic loss aversion in retirement decisions: Investors overly focus on the potential for short-term losses, leading them to invest less in riskier assets and miss out on higher long-term returns. Recently, advances in uncertainty visualizations have shown improvements in decision-making under uncertainty in a variety of tasks. In this paper, we conduct a controlled and incentivized crowdsourced experiment replicating Benartzi and Thaler (1999) and extending it to measure the effect of different uncertainty representations on myopic loss aversion. ",
        "external_paper_link": "",
        "has_pdf": true,
        "ff_link": "https://youtu.be/kPyd86Io6Ho"
    },
    "v-full-1198": {
        "authors": [
            "Paula Kayongo",
            "Glenn Sun",
            "Jason Hartline",
            "Jessica Hullman"
        ],
        "title": "Visualization Equilibrium",
        "session_id": "v-full-full12",
        "abstract": "In many real-world strategic settings, people use information displays to make decisions. In these settings, an information provider chooses which information to provide to strategic agents and how to present it, and agents formulate a best response based on the information and their anticipation of how others will behave. We contribute the results of a controlled online experiment to examine how the provision and presentation of information impacts people's decisions in a congestion game. Our experiment compares how different visualization approaches for displaying this information, including bar charts and hypothetical outcome plots, and different information conditions, including where the visualized information is private versus public (i.e., available to all agents), affect decision making and welfare. We characterize the effects of visualization anticipation, referring to changes to behavior when an agent goes from alone having access to a visualization to knowing that others also have access to the visualization to guide their decisions. We also empirically identify the visualization equilibrium, i.e., the visualization for which the visualized outcome of agents' decisions matches the realized decisions of the agents who view it. We reflect on the implications of visualization equilibria and visualization anticipation for designing information displays for real-world strategic settings.",
        "keywords": [],
        "uid": "v-full-1198",
        "time_stamp": "2021-10-29T14:15:00Z",
        "has_image": true,
        "paper_award": "",
        "image_caption": "The main challenge to deploying accurate visualizations in strategic environments occurs when there are feedback-induced distributional shifts. To address this challenge we introduce a new solution concept the visualization equilibrium. At the equilibrium, visualizations account for behavioral reactions making them accurate for the distributions they induce.",
        "external_paper_link": "",
        "has_pdf": true,
        "ff_link": ""
    },
    "v-short-1030": {
        "authors": [
            "Anna-Pia Lohfink",
            "Frederike Gartzky",
            "Florian Wetzels",
            "Luisa Vollmer",
            "Christoph Garth"
        ],
        "title": "Time-Varying Fuzzy Contour Trees",
        "session_id": "v-short-short3",
        "abstract": "We present a holistic, topology-based visualization technique for spatial time series data based on an adaptation of Fuzzy Contour Trees. \nCommon analysis approaches for time dependent scalar fields identify and track specific features. To give a more general overview of the data, we extend Fuzzy Contour Trees, from the visualization and simultaneous analysis of the topology of multiple scalar fields, to time dependent scalar fields. The resulting time-varying Fuzzy Contour Trees allow the comparison of multiple time steps that are not required to be consecutive. We provide specific interaction and navigation possibilities that allow the exploration of individual time steps and time windows in addition to the behavior of the contour trees over all time steps. To achieve this, we reduce an existing alignment to multiple sub-alignments and adapt the Fuzzy Contour Tree-layout to continuously reflect changes and similarities in the sub-alignments. We apply time-varying Fuzzy Contour Trees to different real-world data sets and demonstrate their usefulness.",
        "keywords": [
            "Comparison and Similarity",
            "Uncertainty Visualization",
            "Data Analysis, Reasoning, Problem Solving, and Decision Making",
            "Scalar Field Data",
            "Computational Topology-based Techniques"
        ],
        "uid": "v-short-1030",
        "time_stamp": "2021-10-29T13:00:00Z",
        "has_image": true,
        "paper_award": "",
        "image_caption": "We adapted Fuzzy Contour Trees to time-varying scalar fields and provide a holistic view on the data. \n",
        "external_paper_link": "",
        "has_pdf": true,
        "ff_link": ""
    },
    "v-short-1113": {
        "authors": [
            "Alper \u015eah\u0131stan",
            "Serkan Demirci",
            "Nate Morrical",
            "Stefan Zellmann",
            "Aytek Aman",
            "Ingo Wald",
            "Ugur Gudukbay"
        ],
        "title": "Ray-traced Shell Traversal of Tetrahedral Meshes for Direct Volume Visualization",
        "session_id": "v-short-short3",
        "abstract": "A well-known method for rendering unstructured volumetric data is tetrahedral marching tet marching, where rays are marched through a series of tetrahedral elements. However, existing tet marching techniques do not easily generalize to rays with arbitrary origin and direction required for advanced shading effects or non-convex meshes. Additionally, the memory footprint of these methods may exceed GPU memory limits. Interactive performance and high image quality are opposing goals. Our approach significantly lowers the burden to render unstructured datasets with high image fidelity while maintaining real-time and interactive performance even for large datasets. To this end, we leverage hardware-accelerated ray tracing to find entry and exit faces for a given ray into a volume and utilize a compact mesh representation to enable the efficient marching of arbitrary rays, thus allowing for advanced shading effects that ultimately yields more convincing and grounded images. Our approach is also robust, supporting both convex and non-convex unstructured meshes. We show that our method achieves interactive rates even with moderately-sized datasets while secondary effects are applied.",
        "keywords": [
            "Life Sciences, Health, Medicine, Biology, Bioinformatics, Genomics",
            "Physical & Environmental Sciences, Engineering, Mathematics",
            "Computer Graphics Techniques",
            "Specific Computing and Rendering Hardware",
            "Algorithms",
            "Scalar Field Data",
            "Volume Rendering"
        ],
        "uid": "v-short-1113",
        "time_stamp": "2021-10-29T13:10:00Z",
        "has_image": true,
        "paper_award": "",
        "image_caption": "",
        "external_paper_link": "",
        "has_pdf": true,
        "ff_link": ""
    },
    "v-short-1122": {
        "authors": [
            "Mohit Sharma",
            "Talha Bin Masood",
            "Signe Sidwall Thygesen",
            "Mathieu Linares",
            "Ingrid Hotz",
            "Vijay Natarajan"
        ],
        "title": "Segmentation driven Peeling for Visual Analysis of Electronic Transitions",
        "session_id": "v-short-short3",
        "abstract": "Electronic transitions in molecules due to absorption or emission of light is a complex quantum mechanical process. Their study plays an important role in the design of novel materials. A common yet challenging task in the study is to determine the nature of those electronic transitions, i.e. which subgroups of the molecule are involved in the transition by donating or accepting electrons, followed by an investigation of the variation in the donor-acceptor behavior for different transitions or conformations of the molecules. In this paper, we present a novel approach towards the study of electronic transitions based on the visual analysis of a bivariate field, namely the electron density in the hole and particle Natural Transition Orbital (NTO). The visual analysis focuses on the continuous scatter plots (CSPs) of the bivariate field linked to their spatial domain. The method supports selections in the CSP visualized as fiber surfaces in the spatial domain, the grouping of atoms, and segmentation of the density fields to peel the CSP. This peeling operator is central to the visual analysis process and helps identify donors and acceptors. We study different molecular systems, identifying local excitation and charge transfer excitations to demonstrate the utility of the method.",
        "keywords": [
            "Physical & Environmental Sciences, Engineering, Mathematics",
            "Application Motivated Visualization",
            "Scalar Field Data",
            "Isosurface Techniques"
        ],
        "uid": "v-short-1122",
        "time_stamp": "2021-10-29T13:20:00Z",
        "has_image": true,
        "paper_award": "",
        "image_caption": "Studying electronic transitions in a copper complex using domain segmentation driven CSP peeling.",
        "external_paper_link": "",
        "has_pdf": true,
        "ff_link": "https://youtu.be/PEzHUacLJdQ"
    },
    "v-short-1128": {
        "authors": [
            "Hanqi Guo",
            "Tom Peterka"
        ],
        "title": "Exact Analytical Parallel Vectors",
        "session_id": "v-short-short3",
        "abstract": "This paper demonstrates that parallel vector curves are piecewise cubic rational curves in 3D piecewise linear vector fields. Parallel vector curves---loci of points where two vector fields are parallel---have been widely used to extract features including ridges, valleys, and vortex core lines in scientific data. We define the term generalized and underdetermined eigensystem in the form of Ax+a=Bx+b in order to derive the piecewise rational representation of 3D parallel vector curves. We discuss how singularities of the rationals lead to different types of intersections with tetrahedral cells.",
        "keywords": [
            "Mathematical Foundations & Numerical Methods"
        ],
        "uid": "v-short-1128",
        "time_stamp": "2021-10-29T13:30:00Z",
        "has_image": false,
        "paper_award": "honorable",
        "image_caption": "",
        "external_paper_link": "",
        "has_pdf": true,
        "ff_link": ""
    },
    "v-short-1129": {
        "authors": [
            "Tushar M. Athawale",
            "Sudhanshu Sane",
            "Chris R. Johnson"
        ],
        "title": "Uncertainty Visualization of the Marching Squares and Marching Cubes Topology Cases",
        "session_id": "v-short-short3",
        "abstract": "Marching squares (MS) and marching cubes (MC) are widely used algorithms for level-set visualization of scientific data. In this paper, we address the challenge of uncertainty visualization of the topology cases of the MS and MC algorithms for uncertain scalar field data sampled on a uniform grid. The visualization of the MS and MC topology cases for uncertain data is challenging due to their exponential nature and a possibility of multiple topology cases per cell of a grid. We propose the topology case count and entropy-based techniques for quantifying uncertainty in the topology cases of the MS and MC algorithms when noise in data is modeled with probability distributions. We demonstrate the applicability of our techniques for independent and correlated uncertainty assumptions. We visualize the quantified topological uncertainty via color mapping proportional to uncertainty, as well as with interactive probability queries in the MS case and entropy isosurfaces in the MC case. We demonstrate the utility of our uncertainty quantification framework in identifying the isovalues exhibiting relatively high topological uncertainty. We illustrate the effectiveness of our techniques via results on synthetic, simulation, and hixel datasets.",
        "keywords": [
            "Uncertainty Visualization",
            "Scalar Field Data",
            "Computational Topology-based Techniques",
            "Isosurface Techniques"
        ],
        "uid": "v-short-1129",
        "time_stamp": "2021-10-29T13:40:00Z",
        "has_image": false,
        "paper_award": "",
        "image_caption": "",
        "external_paper_link": "",
        "has_pdf": true,
        "ff_link": "https://youtu.be/Z6jXaJ4uG44"
    },
    "v-short-1146": {
        "authors": [
            "Shaolun Ruan",
            "Yong Wang",
            "Qiang Guan"
        ],
        "title": "Intercept Graph: An Interactive Radial Visualization for Comparison of State Changes",
        "session_id": "v-short-short3",
        "abstract": "State change comparison of multiple data items is often necessary in multiple application domains, such as medical science, financial engineering, sociology, etc. Slope graphs and grouped bar charts have been widely used to show a \"before-and-after'' story of different data states and indicate their changes. However, they visualize state changes as either slope or difference of bars, which has been proved less effective for quantitative comparison. Also, both visual designs suffer from visual clutter issues with an increasing number of data items. In this paper, we propose Intercept Grpah, a novel visual design to facilitate effective interactive comparison of state changes. Specifically, a radial design is proposed to visualize the starting and ending states of each data item on two concentric circles and the chord length explicitly encodes the \"state change''. By interactively adjusting the inner circle radius, Intercept Grpah can smoothly filter out the large state changes and magnify the difference between similar state changes, mitigating the visual clutter issues and enhancing the effective comparison of state changes. We conducted case studies through comparing Intercept Grpah with slope graphs and grouped bar charts on real datasets to demonstrate the effectiveness of Intercept Grpah.",
        "keywords": [
            "Mathematical Foundations & Numerical Methods",
            "Other Application Areas",
            "Art & Graphic Design",
            "Data Analysis, Reasoning, Problem Solving, and Decision Making",
            "Perception & Cognition",
            "Interaction Design",
            "Visual Representation Design",
            "Temporal Data"
        ],
        "uid": "v-short-1146",
        "time_stamp": "2021-10-29T13:50:00Z",
        "has_image": true,
        "paper_award": "",
        "image_caption": "",
        "external_paper_link": "",
        "has_pdf": true,
        "ff_link": "https://youtu.be/WbSQHyqAKpY"
    },
    "v-short-1168": {
        "authors": [
            "Jacob Fisher",
            "Remco Chang",
            "Eugene Wu"
        ],
        "title": "Automatic Y-axis Rescaling in Dynamic Visualizations",
        "session_id": "v-short-short3",
        "abstract": "Animated and interactive data visualizations dynamically change the data rendered in a visualization (e.g., bar chart). As the data changes, the y-axis may need to be rescaled as the domain of the data changes. Each axis rescaling potentially improves the readability of the current chart, but may also disorient the user. In contrast to static visualizations, where there is considerable literature to help choose the appropriate y-axis scale, there is a lack of guidance about how and when rescaling should be used in dynamic visualizations. Existing visualization systems and libraries adapt a fixed global y-axis, or rescale every time the data changes. Yet, professional visualizations, such as in data journalism, do not adopt either strategy.They instead carefully and manually choose when to rescale based on the analysis task and data. To this end, we conduct a series of Mechanical Turk experiments to study the potential of dynamic axis rescaling, the factors that affect its effectiveness. We find that the appropriate rescaling policy is both task- and data-dependent, and we do not find one clear policy choice for all situations.",
        "keywords": [
            "Computing: Software, Networks, Security, Performance Engr., Distr. Systems, Databases",
            "Charts, Diagrams, and Plots",
            "Data Analysis, Reasoning, Problem Solving, and Decision Making",
            "Guidelines",
            "Human-Subjects Quantitative Studies"
        ],
        "uid": "v-short-1168",
        "time_stamp": "2021-10-29T14:00:00Z",
        "has_image": false,
        "paper_award": "",
        "image_caption": "",
        "external_paper_link": "",
        "has_pdf": true,
        "ff_link": "https://youtu.be/kUJxYIbcKIk"
    },
    "v-short-1054": {
        "authors": [
            "Fahd Husain",
            "Pascale Proulx",
            "Meng-Wei Chang",
            "Rosa Romero-G\u00f3mez",
            "Holland Marie Vasquez"
        ],
        "title": "A Mixed-Initiative Visual Analytics Approach for Qualitative Causal Modeling",
        "session_id": "v-short-short3",
        "abstract": "Modeling complex systems is a time-consuming, difficult and fragmented task, often requiring the analyst to work with disparate data, a variety of models, and expert knowledge across diverse domains. Applying a user-centered design process, we developed Causemos, an integrated visual analytics approach that allows analysts to rapidly assemble qualitative causal models of complex socio-natural systems. Through a mixed-initiative interaction workflow, this approach facilitates the construction, exploration, and curation of qualitative models bringing together data across diverse domains. Referencing a recent user evaluation, we demonstrate Causemos\u2019 ability to interactively enrich user mental models and accelerate qualitative model building.",
        "keywords": [
            "Large-Scale Data Techniques",
            "Life Sciences, Health, Medicine, Biology, Bioinformatics, Genomics",
            "Physical & Environmental Sciences, Engineering, Mathematics",
            "Social Science, Education, Humanities, Journalism, Intelligence Analysis, Knowledge Work",
            "Other Topics and Techniques",
            "Mixed Initiative Human-Machine Analysis",
            "Application Motivated Visualization",
            "Graph/Network and Tree Data"
        ],
        "uid": "v-short-1054",
        "time_stamp": "2021-10-29T14:10:00Z",
        "has_image": true,
        "paper_award": "",
        "image_caption": "Modeling complex systems is a time-consuming, difficult and fragmented task, often requiring analysts to work with disparate data, a variety of models, and expert knowledge across a diverse set of domains. Applying a user-centered design process, we developed a mixed-initiative visual analytics approach, a subset of the Causemos platform, that allows analysts to rapidly assemble qualitative causal models of complex socio-natural systems. Our approach facilitates the construction, exploration, and curation of qualitative models bringing together data across disparate domains. Referencing a recent user evaluation, we demonstrate our approach\u00d5s ability to interactively enrich user mental models and accelerate qualitative model building.\n",
        "external_paper_link": "",
        "has_pdf": true,
        "ff_link": ""
    },
    "v-tvcg-9408377": {
        "authors": [
            "Quan Li",
            "Xiguang Wei",
            "Huanbin Lin",
            "Yang Liu",
            "Tianjian Chen",
            "Xiaojuan Ma"
        ],
        "title": "Inspecting the Running Process of Horizontal Federated Learning via Visual Analytics",
        "session_id": "v-full-full10",
        "abstract": "As a decentralized training approach, horizontal federated learning (HFL) enables distributed clients to collaboratively learn a machine learning model while keeping personal/private information on local devices. Despite the enhanced performance and efficiency of HFL over local training, clues for inspecting the behaviors of the participating clients and the federated model are usually lacking due to the privacy-preserving nature of HFL. Consequently, the users can only conduct a shallow-level analysis of potential abnormal behaviors and have limited means to assess the contributions of individual clients and implement the necessary intervention. Visualization techniques have been introduced to facilitate the HFL process inspection, usually by providing model metrics and evaluation results as a dashboard representation. Although the existing visualization methods allow a simple examination of the HFL model performance, they cannot support the intensive exploration of the HFL process. In this study, strictly following the HFL privacy-preserving protocol, we design an exploratory visual analytics system for the HFL process termed HFLens, which supports comparative visual interpretation at the overview, communication round, and client instance levels. Specifically, the proposed system facilitates the investigation of the overall process involving all clients, the correlation analysis of clients' information in one or different communication round(s), the identification of potential anomalies, and the contribution assessment of each HFL client. Two case studies confirm the efficacy of our system. Experts' feedback suggests that our approach indeed helps in understanding and diagnosing the HFL process better.",
        "keywords": [
            "Federated learning",
            "anomaly detection",
            "contribution assessment",
            "visualization"
        ],
        "uid": "v-tvcg-9408377",
        "time_stamp": "2021-10-29T15:00:00Z",
        "has_image": true,
        "paper_award": "",
        "image_caption": "HFLens interface: (A) A data loader for selecting cases of interest; (B) The summary and projection view show the statistical information of the dataset and the 2D embedding projection of all pairs of \u201cclient id/server \u2013 round id\u201d; (C) The HFL overview presents the participating status of all the involved HFL clients during the HFL running process; (D) The pairwise comparison view displays the metric details of each HFL client in selected communication rounds; (E) The contribution ranking view shows the ranking distributions of all the involved HFL clients.",
        "external_paper_link": "https://doi.org/10.1109/TVCG.2021.3074010",
        "has_pdf": false,
        "ff_link": "https://youtu.be/tdm7-8jS3ww"
    },
    "v-full-1199": {
        "authors": [
            "Zehua Zeng",
            "Phoebe Moh",
            "Fan Du",
            "Jane Hoffswell",
            "Tak Yeon Lee",
            "Sana Malik",
            "Eunyee Koh",
            "Leilani Battle"
        ],
        "title": "An Evaluation-Focused Framework for Visualization Recommendation Algorithms",
        "session_id": "v-full-full10",
        "abstract": "Although we have seen a proliferation of algorithms for recommending visualizations, these algorithms are rarely compared with one another, making it difficult to ascertain which algorithm is best for a given visual analysis scenario. Though several formal frameworks have been proposed in response, we believe this issue persists because visualization recommendation algorithms are inadequately specified from an evaluation perspective. In this paper, we propose an evaluation-focused framework to contextualize and compare a broad range of visualization recommendation algorithms. We present the structure of our framework, where algorithms are specified using three components: (1) a graph representing the full space of possible visualization designs, (2) the method used to traverse the graph for potential candidates for recommendation, and (3) an oracle used to rank candidate designs. To demonstrate how our framework guides the formal comparison of algorithmic performance, we not only theoretically compare five existing representative recommendation algorithms, but also empirically compare four new algorithms generated based on our findings from the theoretical comparison. Our results show that these algorithms behave similarly in terms of user performance, highlighting the need for more rigorous formal comparisons of recommendation algorithms to further clarify their benefits in various analysis scenarios.",
        "keywords": [],
        "uid": "v-full-1199",
        "time_stamp": "2021-10-29T15:15:00Z",
        "has_image": true,
        "paper_award": "honorable",
        "image_caption": "We propose an evaluation-focused framework for visualization recommendation algorithms. Our framework is defined by three major components: (1) a network representing the space of all possible visualization designs; (2) the method a recommendation algorithm uses to traverse the design space to enumerate candidate visualization designs; (3) an oracle used to approximate and rank the value of candidate visualizations. The image demonstrates the visualization design space and the enumeration step of a recommendation algorithm, using movies data as an example. The user's current visualization is at node n0.",
        "external_paper_link": "",
        "has_pdf": true,
        "ff_link": ""
    },
    "v-full-1113": {
        "authors": [
            "Tom Horak",
            "Norine Coenen",
            "Niklas Metzger",
            "Christopher Hahn",
            "Tamara Flemisch",
            "Juli\u00e1n M\u00e9ndez",
            "Dennis Dimov",
            "Bernd Finkbeiner",
            "Raimund Dachselt"
        ],
        "title": "Visual Analysis of Hyperproperties for Understanding Model Checking Results",
        "session_id": "v-full-full10",
        "abstract": "Model checkers provide algorithms for proving that a mathematical model of a system satisfies a given specification. In case of a violation, a counterexample that shows the erroneous behavior is returned. Understanding these counterexamples is challenging, especially for hyperproperty specifications, i.e., specifications that relate multiple executions of a system to each other. We aim to facilitate the visual analysis of such counterexamples through our HyperVis tool, which provides interactive visualizations of the given model, specification, and counterexample. Within an iterative and interdisciplinary design process, we developed visualization solutions that can effectively communicate the core aspects of the model checking result. Specifically, we introduce graphical representations of binary values for improving pattern recognition, color encoding for better indicating related aspects, visually enhanced textual descriptions, as well as extensive cross-view highlighting mechanisms. Further, through an underlying causal analysis of the counterexample, we are also able to identify values that contributed to the violation and use this knowledge for both improved encoding and highlighting. Finally, the analyst can modify both the specification of the hyperproperty and the system directly within HyperVis and initiate the model checking of the new version. In combination, these features notably support the analyst in understanding the error leading to the counterexample as well as iterating the provided system and specification. We ran multiple case studies with HyperVis and tested it with domain experts in qualitative feedback sessions. The participants' positive feedback confirms the considerable improvement over the manual, text-based status quo and the value of the tool for explaining hyperproperties.",
        "keywords": [],
        "uid": "v-full-1113",
        "time_stamp": "2021-10-29T15:30:00Z",
        "has_image": true,
        "paper_award": "",
        "image_caption": "",
        "external_paper_link": "",
        "has_pdf": true,
        "ff_link": "https://youtu.be/Si3Hw35NWXg"
    },
    "v-full-1361": {
        "authors": [
            "Tiankai Xie",
            "Yuxin Ma",
            "Jian Kang",
            "Hanghang Tong",
            "Ross Maciejewski"
        ],
        "title": "FairRankVis: A Visual Analytics Framework for Exploring Algorithmic Fairness in Graph Mining Models",
        "session_id": "v-full-full10",
        "abstract": "Graph mining is an essential component of recommender systems and search engines. Outputs of graph mining models typically provide a ranked list sorted by each item's relevance or utility. However, recent research has identified issues of algorithmic bias in such models, and new graph mining algorithms have been proposed to correct for bias. \n  As such, algorithm developers need tools that can help them uncover potential biases in their models while also exploring the impacts of correcting for biases when employing fairness-aware algorithms. In this paper, we present FairRankVis, a visual analytics framework designed to enable the exploration of multi-class bias in graph mining algorithms. We support both group and individual fairness levels of comparison. Our framework is designed to enable model developers to compare multi-class fairness between algorithms (for example, comparing PageRank with a debiased PageRank algorithm) to assess the impacts of algorithmic debiasing with respect to group and individual fairness. We demonstrate our framework through two usage scenarios inspecting algorithmic fairness.",
        "keywords": [],
        "uid": "v-full-1361",
        "time_stamp": "2021-10-29T15:45:00Z",
        "has_image": true,
        "paper_award": "",
        "image_caption": "FairRankVis is a visual analytics framework designed to enable the exploration of multi-class bias in graph mining algorithms.  The proposed framework is model agnostic, supports both group and individual fairness levels of comparison, and consists of a suite of interactive visualizations for investigating node attributes and topological features of graph elements to explore algorithmic fairness. The image demonstrates the fairness diagnosis of InFoRM (a debiased ranking model) on Weibo social network data. ",
        "external_paper_link": "",
        "has_pdf": true,
        "ff_link": "https://youtu.be/LAxI6_i3CHo"
    },
    "v-tvcg-9301222": {
        "authors": [
            "Florian Heimerl",
            "Christoph Kralj",
            "Torsten M\u00f6ller",
            "Michael Gleicher"
        ],
        "title": "embComp: Visual Interactive Comparison of Vector Embeddings",
        "session_id": "v-full-full10",
        "abstract": "This paper introduces embComp, a novel approach for comparing two embeddings that capture the similarity between objects, such as word and document embeddings. We survey scenarios where comparing these embedding spaces is useful. From those scenarios, we derive common tasks, introduce visual analysis methods that support these tasks, and combine them into a comprehensive system. One of embComp\u2019s central features are overview visualizations that are based on metrics for measuring differences in the local structure around objects. Summarizing these local metrics over the embeddings provides global overviews of similarities and differences. Detail views allow comparison of the local structure around selected objects and relating this local information to the global views. Integrating and connecting all of these components, embComp supports a range of analysis workflows that help understand similarities and differences between embedding spaces. We assess our approach by applying it in several use cases, including understanding corpora differences via word vector embeddings, and understanding algorithmic differences in generating embeddings.",
        "keywords": [
            "I.6.9.c Information visualization < I.6.9 Visualization < I.6 Simulation",
            "Modeling",
            "and Visualization < I Computing Methodologie",
            "L.3.0.f Human- computer interaction < L.3.0 Integrating touch-based interactions into various domains Assistive technology < L.3",
            "H.5.2 User Interfaces < H.5 Information Interfaces and Representation (HCI) < H Information Technology and Systems"
        ],
        "uid": "v-tvcg-9301222",
        "time_stamp": "2021-10-29T16:00:00Z",
        "has_image": false,
        "paper_award": "",
        "image_caption": "",
        "external_paper_link": "https://doi.org/10.1109/TVCG.2020.3045918",
        "has_pdf": false,
        "ff_link": "https://youtu.be/eU_KgecHTQE"
    },
    "v-full-1284": {
        "authors": [
            "Furui Cheng",
            "Dongyu Liu",
            "Fan Du",
            "Yanna Lin",
            "Alexandra Zytek",
            "Haomin Li",
            "Huamin Qu",
            "Kalyan Veeramachaneni"
        ],
        "title": "VBridge: Connecting the Dots Between Features and Data to Explain Healthcare Models",
        "session_id": "v-full-full10",
        "abstract": "Machine learning (ML) is increasingly applied to Electronic Health Records (EHRs) to solve clinical prediction tasks. Although many ML models perform promisingly, issues with model transparency and interpretability limit their adoption in clinical practice. Directly using existing explainable ML techniques in clinical settings can be challenging. Through literature surveys and collaborations with six clinicians with an average of 17 years of clinical experience, we identified three key challenges, including clinicians' unfamiliarity with ML features, lack of contextual information, and the need for cohort-level evidence. Following an iterative design process, we further designed and developed VBridge, a visual analytics tool that seamlessly incorporates ML explanations into clinicians' decision-making workflow. The system includes a novel hierarchical display of contribution-based feature explanations and enriched interactions that connect the dots between ML features, explanations, and data. We demonstrated the effectiveness of VBridge through two case studies and expert interviews with four clinicians, showing that visually associating model explanations with patients' situational records can help clinicians better interpret and use model predictions when making clinician decisions. We further derived a list of design implications for developing future explainable ML tools to support clinical decision-making.",
        "keywords": [],
        "uid": "v-full-1284",
        "time_stamp": "2021-10-29T16:15:00Z",
        "has_image": true,
        "paper_award": "honorable",
        "image_caption": "The interface of VBridge facilitates clinicians\u2019 understanding and interpretation of ML model predictions. The header menu allows clinicians to view prediction results, and to select a patient group for reference. The profile view and the timeline view show a summary of the target patient\u2019s health records. The feature view shows feature-level explanations in a hierarchical display, linked to the temporal view where healthcare time series are visualized to provide context for feature-level explanations.",
        "external_paper_link": "",
        "has_pdf": true,
        "ff_link": "https://youtu.be/PnAxWRLKgFY"
    },
    "v-full-1310": {
        "authors": [
            "V\u00e1clav Pavlovec",
            "Ladislav \u010cmol\u00edk"
        ],
        "title": "Rapid Labels: Point-Feature Labeling on GPU",
        "session_id": "v-full-full16",
        "abstract": "Labels, short textual annotations are an important component of data visualizations, illustrations, infographics, and geographical maps. In interactive applications, the labeling method responsible for positioning the labels should not take the resources from the application itself. In other words, the labeling method should provide the result as fast as possible. In this work, we propose a greedy point-feature labeling method running on GPU. In contrast to existing methods that position the labels sequentially, the proposed method positions several labels in parallel. Yet, we guarantee that the positioned labels will not overlap, nor will they overlap important visual features. When the proposed method is searching for the label position of a point-feature, the available label candidates are evaluated with respect to overlaps with important visual features, conflicts with label candidates of other point-features, and their ambiguity. The evaluation of each label candidate is done in constant time independently from the number of point-features, the number of important visual features, and the resolution of the created image. Our measurements indicate that the proposed method is able to position more labels than existing greedy methods that do not evaluate conflicts between the label candidates. At the same time, the proposed method achieves a significant increase in performance. The increase in performance is mainly due to the parallelization and the efficient evaluation of label candidates.",
        "keywords": [],
        "uid": "v-full-1310",
        "time_stamp": "2021-10-29T15:00:00Z",
        "has_image": true,
        "paper_award": "",
        "image_caption": "Rapid Labels: Point-Feature Labeling on GPU. Fast greedy method that supports priority-groups of labels and labeling consistent under zoom and pan. All examples were labeled with the proposed method. (left) 3340 airports in US labeled in 72ms. (top right) Line chart labeled without evaluating ambiguity - contains a number of ambiguous labels. (bottom right) Line chart labeled with evaluation of ambiguity - presents a significant improvement in positioning of the labels.",
        "external_paper_link": "",
        "has_pdf": true,
        "ff_link": "https://youtu.be/tVEMCigvWkw"
    },
    "v-tvcg-9354592": {
        "authors": [
            "Michail Schwab",
            "David Saffo",
            "Nicholas Bond",
            "Shash Sinha",
            "Cody Dunne",
            "Jeff Huang",
            "James Tompkin",
            "Michelle Borkin"
        ],
        "title": "Scalable Scalable Vector Graphics: Automatic Translation of Interactive SVGs to a Multithread VDOM for Fast Rendering",
        "session_id": "v-full-full16",
        "abstract": "The dominant markup language for Web visualizations - Scalable Vector Graphics (SVG) - is comparatively easy to learn, and is open, accessible, customizable via CSS, and searchable via the DOM, with easy interaction handling and debugging. Because these attributes allow visualization creators to focus on design on implementation details, tools built on top of SVG, such as D3.js, are essential to the visualization community. However, slow SVG rendering can limit designs by effectively capping the number of on-screen data points, and this can force visualization creators to switch to Canvas or WebGL. These are less flexible (e.g., no search or styling via CSS), and harder to learn. We introduce Scalable Scalable Vector Graphics (SSVG) to reduce these limitations and allow complex and smooth visualizations to be created with SVG. SSVG automatically translates interactive SVG visualizations into a dynamic virtual DOM (VDOM) to bypass the browser's slow `to specification' rendering by intercepting JavaScript function calls. De-coupling the SVG visualization specification from SVG rendering, and obtaining a dynamic VDOM, creates flexibility and opportunity for visualization system research. SSVG uses this flexibility to free up the main thread for more interactivity and renders the visualization with Canvas or WebGL on a web worker. Together, these concepts create a drop-in JavaScript library which can improve rendering performance by 3-9X with only one line of code added. To demonstrate applicability, we describe the use of SSVG on multiple example visualizations including published visualization research. A free copy of this paper, collected data, and source code are available as open science at osf.io/ge8wp.",
        "keywords": [
            "Visualization Systems",
            "SVG",
            "Performance",
            "Virtual DOM",
            "Rendering",
            "D3.js."
        ],
        "uid": "v-tvcg-9354592",
        "time_stamp": "2021-10-29T15:15:00Z",
        "has_image": true,
        "paper_award": "",
        "image_caption": "A SVG-based windmap implementation. It renders slowly with D3 and SVG, but, using SSVG, it renders really smoothly.\n",
        "external_paper_link": "https://doi.org/10.1109/TVCG.2021.3059294",
        "has_pdf": false,
        "ff_link": "https://youtu.be/9Rg4QJm09ng"
    },
    "v-tvcg-9130956": {
        "authors": [
            "Ayan Biswas",
            "Soumya Dutta",
            "Earl Lawrence",
            "John Patchett",
            "Jon Calhoun",
            "James Ahrens"
        ],
        "title": "Probabilistic Data-Driven Sampling via Multi-Criteria Importance Analysis",
        "session_id": "v-full-full16",
        "abstract": "Although supercomputers are becoming increasingly powerful, their components have thus far not scaled proportionately. Compute power is growing enormously and is enabling finely resolved simulations that produce never-before-seen features. However, I/O capabilities lag by orders of magnitude, which means only a fraction of the simulation data can be stored for post hoc analysis. Prespecified plans for saving features and quantities of interest do not work for features that have not been seen before. Data-driven intelligent sampling schemes are needed to detect and save important parts of the simulation while it is running. Here, we propose a novel sampling scheme that reduces the size of the data by orders-of-magnitude while still preserving important regions. The approach we develop selects points with unusual data values and high gradients. We demonstrate that our approach outperforms traditional sampling schemes on a number of tasks.",
        "keywords": [
            "Data visualization",
            "Data models",
            "Computational modeling",
            "Task analysis",
            "Sampling methods",
            "Data analysis",
            "Visualization",
            "Importance sampling",
            "data reduction",
            "error quantification",
            "feature preservation"
        ],
        "uid": "v-tvcg-9130956",
        "time_stamp": "2021-10-29T15:30:00Z",
        "has_image": false,
        "paper_award": "",
        "image_caption": "",
        "external_paper_link": "https://doi.org/10.1109/TVCG.2020.3006426",
        "has_pdf": false,
        "ff_link": "https://youtu.be/NsFQrWT-V14"
    },
    "v-full-1343": {
        "authors": [
            "Mark van de Ruit",
            "Markus Billeter",
            "Elmar Eisemann"
        ],
        "title": "An Efficient Dual-Hierarchy tSNE Minimization",
        "session_id": "v-full-full16",
        "abstract": "t-distributed Stochastic Neighbour Embedding (t-SNE) has become a standard for exploratory data analysis, as it is capable of revealing clusters even in complex data while requiring minimal user input. While its run-time complexity limited it to small datasets in the past, recent efforts improved upon the expensive similarity computations and the previously quadratic minimization. Nevertheless, t-SNE still has high runtime and memory costs when operating on millions of points. We present a novel method for executing the t-SNE minimization. While our method overall retains a linear runtime complexity, we obtain a significant performance increase in the most expensive part of the minimization. We achieve a significant improvement without a noticeable decrease in accuracy even when targeting a 3D embedding. Our method constructs a pair of spatial hierarchies over the embedding, which are simultaneously traversed to approximate many N-body interactions at once. We demonstrate an efficient GPGPU implementation and evaluate its performance against state-of-the-art methods on a variety of datasets.",
        "keywords": [],
        "uid": "v-full-1343",
        "time_stamp": "2021-10-29T15:45:00Z",
        "has_image": true,
        "paper_award": "",
        "image_caption": "We present a novel dual-hierarchy t-SNE method. Our method retains a linear runtime, but obtains significant performance improvements over the state of the art without a noticeable decrease in accuracy even when targeting a 3D embedding. Our method constructs a pair of spatial hierarchies over the embedding, which are simultaneously traversed to approximate many N-body interactions at once. We demonstrate an efficient GPGPU implementation for which source code is publicly available.",
        "external_paper_link": "",
        "has_pdf": true,
        "ff_link": ""
    },
    "v-full-1471": {
        "authors": [
            "Yinqiao Wang",
            "Lu Chen",
            "Jaemin Jo",
            "Yunhai Wang"
        ],
        "title": "Joint t-SNE for Comparable Projections of Multiple High-Dimensional Datasets",
        "session_id": "v-full-full16",
        "abstract": "We present Joint t-Stochastic Neighbor Embedding (Joint t-SNE), a technique to generate comparable projections of multiple high-dimensional datasets. Although t-SNE has been widely employed to visualize high-dimensional datasets from various domains, it is limited to projecting a single dataset. When a series of high-dimensional datasets, such as datasets changing over time, is projected independently using t-SNE, misaligned layouts are obtained. Even items with identical features across datasets are projected to different locations, making the technique unsuitable for comparison tasks. To tackle this problem, we introduce edge similarity, which captures the similarities between two adjacent time frames based on the Graphlet Frequency Distribution (GFD). We then integrate a novel loss term into the t-SNE loss function, which we call vector constraints, to preserve the vectors between projected points across the projections, allowing these points to serve as visual landmarks for direct comparisons between projections. Using synthetic datasets whose ground-truth structures are known, we show that Joint t-SNE outperforms existing techniques, including Dynamic t-SNE, in terms of local coherence error, Kullback-Leibler divergence, and neighborhood preservation. We also showcase a real-world use case to visualize and compare the activation of different layers of a neural network.",
        "keywords": [],
        "uid": "v-full-1471",
        "time_stamp": "2021-10-29T16:00:00Z",
        "has_image": true,
        "paper_award": "",
        "image_caption": "Comparison of the 5-Gaussian dataset projection of four different t-SNE methods. a) t-SNE produced misaligned layouts all across four time frames. b) Equal-initialization t-SNE provides better visual consistency than t-SNE but there are still unnecessary movements of clusters. c) Dynamic t-SNE showed smoothing effect by distorting projections at t = 2 and 3. d) Joint t-SNE generated coherent and reliable projections that re\ufb02ected the ground-truth transformations of clusters.",
        "external_paper_link": "",
        "has_pdf": true,
        "ff_link": "https://youtu.be/6Gxdv-unloM"
    },
    "v-full-1685": {
        "authors": [
            "Botong Qu",
            "Eugene Zhang",
            "Yue Zhang"
        ],
        "title": "Automatic Polygon Layout for Primal-Dual Visualization of Hypergraphs",
        "session_id": "v-full-full16",
        "abstract": "N-ary relationships, which relate N entities where N is not necessarily two, can be visually represented as polygons whose vertices are the entities of the relationships. Manually generating a high-quality layout using this representation is labor-intensive. In this paper, we provide an automatic polygon layout generation algorithm for the visualization of N-ary relationships. At the core of our algorithm is a set of objective functions motivated by a number of design principles that we have identified. These objective functions are then used in an optimization framework that we develop to achieve high-quality layouts. Recognizing the duality between entities and relationships in the data, we provide a second visualization in which the roles of entities and relationships in the original data are reversed. This can lead to additional insight about the data. Furthermore, we enhance our framework for a joint optimization on the primal layout (original data) and the dual layout (where the roles of entities and relationships are reversed). This allows users to inspect their data using two complementary views. We apply our visualization approach to a number of datasets that include co-authorship data and social contact pattern data.",
        "keywords": [],
        "uid": "v-full-1685",
        "time_stamp": "2021-10-29T16:15:00Z",
        "has_image": true,
        "paper_award": "",
        "image_caption": "We provide an automatic optimization framework for high-quality polygon layout of hypergraph data, such as the author-paper data shown in this Figure. The top is a paper-centric view and the bottom is an author-centric view. The primal-dual views were generated using our joint optimization framework.",
        "external_paper_link": "",
        "has_pdf": true,
        "ff_link": "https://youtu.be/M9siftg-vIc"
    },
    "v-full-1205": {
        "authors": [
            "Shreeraj Jadhav",
            "Gaofeng Deng",
            "Marlene Zawin",
            "Arie Kaufman"
        ],
        "title": "COVID-view: Diagnosis of COVID-19 using Chest CT",
        "session_id": "v-full-full6",
        "abstract": "Significant work has been done towards deep learning (DL) models for automatic lung and lesion segmentation and classification of COVID-19 on chest CT data. However, comprehensive visualization systems focused on supporting the dual visual+DL diagnosis of COVID-19 are non-existent. We present COVID-view, a visualization application specially tailored for radiologists to diagnose COVID-19 from chest CT data. The system incorporates a complete pipeline of automatic lungs segmentation, localization/isolation of lung abnormalities, followed by visualization, visual and DL analysis, and measurement/quantification tools. Our system combines the traditional 2D workflow of radiologists with newer 2D and 3D visualization techniques with DL support for a more comprehensive diagnosis. COVID-view incorporates a novel DL model for classifying the patients into positive/negative COVID-19 cases, which acts as a reading aid for the radiologist using COVID-view, and provides the attention heatmap as an explainable DL for the model output. We designed and evaluated COVID-view through suggestions, close feedback and conducting case studies of real-world patient data by expert radiologists who have substantial experience diagnosing chest CT scans for COVID-19, pulmonary embolism, and other forms of lung infections. We present requirements and task analysis for the diagnosis of COVID-19 that motivate our design choices and results in a practical system which is capable of handling real-world patient cases.",
        "keywords": [],
        "uid": "v-full-1205",
        "time_stamp": "2021-10-29T15:00:00Z",
        "has_image": true,
        "paper_award": "",
        "image_caption": "COVID-view is an elaborate visualization application for diagnosis of COVID-19 patients using chest CT. (a) 3D visualization of lungs and a ground glass opacity (GGO) region. (b) Activation heatmap from our classification model trained to classify CT scans into COVID positive and negative. (c) masked MIP view showing GGO and highlighted abnormal regions.",
        "external_paper_link": "",
        "has_pdf": true,
        "ff_link": "https://youtu.be/q9PAQZU8bb0"
    },
    "v-full-1558": {
        "authors": [
            "Qianwen Wang",
            "Tali Mazor",
            "Theresa Harbig",
            "Ethan Cerami",
            "Nils Gehlenborg"
        ],
        "title": "ThreadStates: State-based Visual Analysis of Disease Progression",
        "session_id": "v-full-full6",
        "abstract": "A growing number of longitudinal cohort studies are generating data with extensive patient observations across multiple timepoints. Such data offers promising opportunities to better understand the progression of diseases. However, these observations are usually treated as general events in existing visual analysis tools. As a result, their capabilities in modeling disease progression are not fully utilized. To fill this gap, we designed and implemented ThreadStates, an interactive visual analytics tool for the exploration of longitudinal patient cohort data. The focus of ThreadStatesis to identify the states of disease progression by learning from observation data in a human-in-the-loop manner. We propose a novel Glyph Matrix design and combine it with a scatter plot to enable seamless identification, observation, and refinement of states. The disease progression patterns are then revealed in terms of state transitions using Sankey-based visualizations. We employ sequence clustering techniques to find patient groups with distinctive progression patterns, and to reveal the association between disease progression and patient-level features. The design and development were driven by a requirement analysis and iteratively refined based on feedback from domain experts over the course of a 10-month design study. Case studies and expert interviews demonstrate that ThreadStates can successively summarize disease states, reveal disease progression, and compare patient groups.",
        "keywords": [],
        "uid": "v-full-1558",
        "time_stamp": "2021-10-29T15:15:00Z",
        "has_image": true,
        "paper_award": "",
        "image_caption": "ThreadStates supports an interactive disease progression analysis through two phases: state identification and transition summarization. \nFor state identification, users can interpret and refine states through a novel matrix+glyph visualization and a scatter plot. \nFor transition visualization, ThreadStates provides both an overview and a detail view. \nState transition patterns are summarized and visualized using Sankey-based visualizations to reveal disease progression patterns.",
        "external_paper_link": "",
        "has_pdf": true,
        "ff_link": "https://youtu.be/BuripTHkSKk"
    },
    "v-full-1271": {
        "authors": [
            "Devin Lange",
            "Eddie Polanco",
            "Robert Judson-Torres",
            "Thomas Zangle",
            "Alexander Lex"
        ],
        "title": "Loon: Using Exemplars to Visualize Large Scale Microscopy Data",
        "session_id": "v-full-full6",
        "abstract": "Which drug is most promising for a cancer patient? This is a question a new microscopy-based approach for measuring the mass of individual cancer cells treated with different drugs promises to answer in only a few hours. However, the analysis pipeline for extracting data from these images is still far from complete automation: human intervention is necessary for quality control for preprocessing steps such as segmentation, to adjust filters, and remove noise, and for the analysis of the result. To address this workflow, we developed Loon, a visualization tool for analyzing drug screening data based on quantitative phase microscopy imaging. Loon visualizes both derived data such as growth rates, and imaging data. Since the images are collected automatically at a large scale, manual inspection of images and segmentations is infeasible. However, reviewing representative samples of cells is essential, both for quality control and for data analysis. We introduce a new approach of choosing and visualizing representative exemplar cells that retain a close connection to the low-level data. By tightly integrating the derived data visualization capabilities with the novel exemplar visualization and providing selection and filtering capabilities, Loon is well suited for making decisions about which drugs are suitable for a specific patient.",
        "keywords": [],
        "uid": "v-full-1271",
        "time_stamp": "2021-10-29T15:30:00Z",
        "has_image": true,
        "paper_award": "honorable",
        "image_caption": "The Loon visualization interface contains five views that show different visualizations that interact with each other.",
        "external_paper_link": "",
        "has_pdf": true,
        "ff_link": "https://youtu.be/iRsL3WiZbhI"
    },
    "v-full-1502": {
        "authors": [
            "Jared Jessup",
            "Robert Kr\u00fcger",
            "Simon Warchol",
            "John Hoffer",
            "Jeremy Muhlich",
            "Cecily C. Ritch",
            "Giorgio Gaglia",
            "Shannon Coy",
            "Yu-An Chen",
            "Jia-Ren Lin",
            "Sandro Santagata",
            "Peter Sorger",
            "Hanspeter Pfister"
        ],
        "title": "Scope2Screen: Focus+Context Techniques for Pathology Tumor Assessment in Multivariate Image Data",
        "session_id": "v-full-full6",
        "abstract": "Inspection of tissues using a light microscope is the primary method of diagnosing many diseases, notably cancer. Highly multiplexed tissue imaging builds on this foundation, enabling collection of up to 60 channels of molecular information plus cell and tissue morphology using antibody staining. This provides unique insight into disease biology and promises to help with design of patient-specific therapies. However, a substantial gap remains with respect to visualizing the resulting multivariate image data and effectively supporting pathology workflows in digital environments on screen. We, therefore, developed Scope2Screen, a scalable software system for focus+context exploration and annotation of whole-slide, high-plex, tissue images. Our approach scales to analyzing 100GB images of 10^9 or more pixels per channel, containing millions of individual cells. A multidisciplinary team of visualization experts, microscopists, and pathologists identified key image exploration and annotation tasks involving finding, magnifying, quantifying, and organizing regions of interest (ROIs) in an intuitive and cohesive manner. Building on a scope-to-screen metaphor, we present interactive lensing techniques that operate at single-cell and tissue levels. Lenses are equipped with task-specific functionality and descriptive statistics, making it possible to analyze image features, cell types, and spatial arrangements (neighborhoods) across image channels and scales. A fast sliding-window search guides users to regions similar to those under the lens; these regions can be analyzed and considered either separately or as part of a larger image collection. A novel snapshot method enables linked lens configurations and image statistics to be saved, restored, and shared with these regions. We validate our designs with domain experts and apply Scope2Screen in two case studies involving lung and colorectal cancers to discover cancer-relevant image features.",
        "keywords": [],
        "uid": "v-full-1502",
        "time_stamp": "2021-10-29T15:45:00Z",
        "has_image": true,
        "paper_award": "",
        "image_caption": "Scope2Screen is a scalable system for visualization, focus+context exploration and annotation of whole-slide, high-plex, tissue images, of 100GB in size, with 10^9 or more pixels per channel, and millions of individual cells. As a multidisciplinary team of visualization experts, microscopists, and pathologists we identified key image exploration and annotation tasks involving finding, magnifying, quantifying, and organizing regions of interest in an intuitive and cohesive manner. We validate our designs with domain experts and apply Scope2Screen in two case studies involving lung and colorectal cancer.\n\n",
        "external_paper_link": "",
        "has_pdf": true,
        "ff_link": "https://youtu.be/Y2tGl2r8v6U"
    },
    "v-tvcg-9376675": {
        "authors": [
            "Sarkis Halladjian",
            "David Kouril",
            "Haichao Miao",
            "Eduard Gr\u00f6ller",
            "Ivan Viola",
            "Tobias Isenberg"
        ],
        "title": "Multiscale Unfolding: Illustratively Visualizing the Whole Genome at a Glance",
        "session_id": "v-full-full6",
        "abstract": "We present Multiscale Unfolding, an interactive technique for illustratively visualizing multiple hierarchical scales of DNA in a single view, showing the genome at different scales and demonstrating \n how one scale spatially folds into the next. The DNA's extremely long sequential structure---arranged differently on several distinct scale levels---is often lost in traditional 3D depictions, mainly due to its multiple levels of dense spatial packing and the resulting occlusion. Furthermore, interactive exploration of this complex structure is cumbersome, requiring visibility management like cut-aways. In contrast to existing temporally controlled multiscale data exploration, we allow viewers to always see and interact with any of the involved scales. For this purpose we separate the depiction into constant-scale and scale transition zones. Constant-scale zones maintain a single-scale representation, while still linearly unfolding the DNA. Inspired by illustration, scale transition zones connect adjacent constant-scale zones via level unfolding, scaling, and transparency. We thus represent the spatial structure of the whole DNA macro-molecule, maintain its local organizational characteristics, linearize its higher-level organization, and use spatially controlled, understandable interpolation between neighboring scales. We also contribute interaction techniques that provide viewers with a coarse-to-fine control for navigating within our all-scales-in-one-view representations and visual aids to illustrate the size differences. Overall, Multiscale Unfolding allows viewers to grasp the DNA's structural composition from chromosomes to the atoms, with increasing levels of \"unfoldedness,\" and can be applied in data-driven illustration and communication.",
        "keywords": [
            "Multiscale visualization",
            "spatially-controlled scale \ntransition",
            "visual abstraction",
            "illustrative visualization",
            "genome",
            "DNA."
        ],
        "uid": "v-tvcg-9376675",
        "time_stamp": "2021-10-29T16:00:00Z",
        "has_image": true,
        "paper_award": "",
        "image_caption": "This image shows the Multiscale Unfolding visual representation.",
        "external_paper_link": "https://doi.org/10.1109/TVCG.2021.3065443",
        "has_pdf": false,
        "ff_link": "https://youtu.be/x6NZ0bV9A34"
    },
    "v-siggraph-115": {
        "authors": [
            "Nanxuan Zhao",
            "Quanlong Zheng",
            "Jing Liao",
            "Ying Cao",
            "Hanspeter Pfister",
            "Rynson W. H. Lau"
        ],
        "title": "Selective Region-based Photo Color Adjustment for Graphic Designs",
        "session_id": "v-siggraph-siggraph1",
        "abstract": "When adding a photo onto a graphic design, professional graphic designers often adjust its colors based on some target colors obtained from the brand or product to make the entire design more memorable to audiences and establish a consistent brand identity. However, adjusting the colors of a photo in the context of a graphic design is a difficult task, with two major challenges: (1) Locality: The color is often adjusted locally to preserve the semantics and atmosphere of the original image; and (2) Naturalness: The modified region needs to be carefully chosen and recolored to obtain a semantically valid and visually natural result. To address these challenges, we propose a learning-based approach to photo color adjustment for graphic designs, which maps an input photo along with the target colors to a recolored result. Our method decomposes the color adjustment process into two successive stages: modifiable region selection and target color propagation. The first stage aims to solve the core, challenging problem of which local image region(s) should be adjusted, which requires not only a common sense of colors appearing in our visual world but also understanding of subtle visual design heuristics. To this end, we capitalize on both natural photos and graphic designs to train a region selection network, which detects the most likely regions to be adjusted to the target colors. The second stage trains a recoloring network to naturally propagate the target colors in the detected regions. Through extensive experiments and a user study, we demonstrate the effectiveness of our selective region-based photo recoloring framework.",
        "keywords": [],
        "uid": "v-siggraph-115",
        "time_stamp": "2021-10-29T15:00:00Z",
        "has_image": true,
        "paper_award": "",
        "image_caption": "When inserting a photo into a graphic design, our model can automatically predict appropriate regions for recoloring to the target colors, such that the resulting images still look natural with the original object semantics preserved and the resulting designs look visually more harmonious. Our model is also able to provide multiple suggestions for the user to choose from.",
        "external_paper_link": "https://doi.org/10.1145/3447647",
        "has_pdf": false,
        "ff_link": ""
    },
    "v-siggraph-332": {
        "authors": [
            "Zeyu Wang",
            "Sherry Qiu",
            "Nicole Feng",
            "Holly Rushmeier",
            "Leonard McMillan",
            "Julie Dorsey"
        ],
        "title": "Tracing Versus Freehand for Evaluating Computer-Generated Drawings",
        "session_id": "v-siggraph-siggraph1",
        "abstract": "Non-photorealistic rendering (NPR) and image processing algorithms are widely assumed as a proxy for drawing. However, this assumption is not well assessed due to the difficulty in collecting and registering freehand drawings. Alternatively, tracings are easier to collect and register, but there is no quantitative evaluation of tracing as a proxy for freehand drawing. In this paper, we compare tracing, freehand drawing, and computer-generated drawing approximation (CGDA) to understand their similarities and differences. We collected a dataset of 1,498 tracings and freehand drawings by 110 participants for 100 image prompts. Our drawings are registered to the prompts and include vector-based timestamped strokes collected via stylus input. Comparing tracing and freehand drawing, we found a high degree of similarity in stroke placement and types of strokes used over time. We show that tracing can serve as a viable proxy for freehand drawing because of similar correlations between spatio-temporal stroke features and labeled stroke types. Comparing hand-drawn content and current CGDA output, we found that 60% of drawn pixels corresponded to computer-generated pixels on average. The overlap tended to be commonly drawn content, but people's artistic choices and temporal tendencies remained largely uncaptured. We present an initial analysis to inform new CGDA algorithms and drawing applications, and provide the dataset for use by the community.",
        "keywords": [],
        "uid": "v-siggraph-332",
        "time_stamp": "2021-10-29T15:15:00Z",
        "has_image": true,
        "paper_award": "",
        "image_caption": "",
        "external_paper_link": "https://doi.org/10.1145/3450626.3459819",
        "has_pdf": false,
        "ff_link": "https://youtu.be/xNys91i6Hu8"
    },
    "v-siggraph-322": {
        "authors": [
            "Delio Vicini",
            "Wenzel Jakob",
            "Anton Kaplanyan"
        ],
        "title": "A Non-exponential Transmittance Model for Volumetric Scene Representations",
        "session_id": "v-siggraph-siggraph1",
        "abstract": "We introduce a novel transmittance model to improve the volumetric representation of 3D scenes. The model can represent opaque surfaces in the volumetric light transport framework. Volumetric representations are useful for complex scenes, and become increasingly popular for level of detail and scene reconstruction. The traditional exponential transmittance model found in volumetric light transport cannot capture correlations in visibility across volume elements. When representing opaque surfaces as volumetric density, this leads to both bloating of silhouettes and light leaking artifacts. By introducing a parametric non-exponential transmittance model, we are able to approximate these correlation effects and significantly improve the accuracy of volumetric appearance representation of opaque scenes. Our parametric transmittance model can represent a continuum between the linear transmittance that opaque surfaces exhibit and the traditional exponential transmittance encountered in participating media and unstructured geometries. This covers a large part of the spectrum of geometric structures encountered in complex scenes. In order to handle the spatially varying transmittance correlation effects, we further extend the theory of non-exponential participating media to a heterogeneous transmittance model. Our model is compact in storage and computationally efficient both for evaluation and for reverse-mode gradient computation. Applying our model to optimization algorithms yields significant improvements in volumetric scene appearance quality. We further show improvements for relevant applications, such as scene appearance prefiltering, image-based scene reconstruction using differentiable rendering, neural representations, and compare it to a conventional exponential model.",
        "keywords": [],
        "uid": "v-siggraph-322",
        "time_stamp": "2021-10-29T15:30:00Z",
        "has_image": true,
        "paper_award": "",
        "image_caption": "",
        "external_paper_link": "https://doi.org/10.1145/3450626.3459815",
        "has_pdf": false,
        "ff_link": ""
    },
    "v-siggraph-173": {
        "authors": [
            "Jiakai Zhang",
            "Xinhang Liu",
            "Xinyi Ye",
            "Fuqiang Zhao",
            "Yanshun Zhang",
            "Minye Wu",
            "Yingliang Zhang",
            "Lan Xu",
            "Jingyi Yu"
        ],
        "title": "Editable Free-viewpoint Video using a Layered Neural Representation",
        "session_id": "v-siggraph-siggraph1",
        "abstract": "Generating free-viewpoint videos is critical for immersive VR/AR experience but recent neural advances still lack the editing ability to manipulate the visual perception for large dynamic scenes. To fill this gap, in this paper we propose the first approach for editable photo-realistic free-viewpoint video generation for large-scale dynamic scenes using only sparse 16 cameras. The core of our approach is a new layered neural representation, where each dynamic entity including the environment itself is formulated into a space-time coherent neural layered radiance representation called ST-NeRF. Such layered representation supports fully perception and realistic manipulation of the dynamic scene whilst still supporting a free viewing experience in a wide range. In our ST-NeRF, the dynamic entity/layer is represented as continuous functions, which achieves the disentanglement of location, deformation as well as the appearance of the dynamic entity in a continuous and self-supervised manner. We propose a scene parsing 4D label map tracking to disentangle the spatial information explicitly, and a continuous deform module to disentangle the temporal motion implicitly. An object-aware volume rendering scheme is further introduced for the re-assembling of all the neural layers. We adopt a novel layered loss and motion-aware ray sampling strategy to enable efficient training for a large dynamic scene with multiple performers, Our framework further enables a variety of editing functions, i.e., manipulating the scale and location, duplicating or retiming individual neural layers to create numerous visual effects while preserving high realism. Extensive experiments demonstrate the effectiveness of our approach to achieve high-quality, photo-realistic, and editable free-viewpoint video generation for dynamic scenes.",
        "keywords": [],
        "uid": "v-siggraph-173",
        "time_stamp": "2021-10-29T15:45:00Z",
        "has_image": true,
        "paper_award": "",
        "image_caption": "",
        "external_paper_link": "https://doi.org/10.1145/3450626.3459756",
        "has_pdf": false,
        "ff_link": ""
    },
    "v-siggraph-153": {
        "authors": [
            "Liane Makatura",
            "Minghao Guo",
            "Adriana Schulz",
            "Justin Solomon",
            "Wojciech Matusik"
        ],
        "title": "Pareto Gamuts : exploring optimal designs across varying contexts",
        "session_id": "v-siggraph-siggraph1",
        "abstract": "Manufactured parts are meticulously engineered to perform well with respect to several conflicting metrics, like weight, stress, and cost. The best achievable trade-offs reside on the Pareto front which can be discovered via performance-driven optimization. The objectives that define this Pareto front often incorporate assumptions about the context in which a part will be used, including loading conditions, environmental influences, material properties, or regions that must be preserved to interface with a surrounding assembly. Existing multi-objective optimization tools are only equipped to study one context at a time, so engineers must run independent optimizations for each context of interest. However, engineered parts frequently appear in many contexts: wind turbines must perform well in many wind speeds, and a bracket might be optimized several times with its bolt-holes fixed in different locations on each run. In this paper, we formulate a framework for variable-context multi-objective optimization. We introduce the Pareto gamut, which captures Pareto fronts over a range of contexts. We develop a global/local optimization algorithm to discover the Pareto gamut directly, rather than discovering a single fixed-context \"slice\" at a time. To validate our method, we adapt existing multi-objective optimization benchmarks to contextual scenarios. We also demonstrate the practical utility of Pareto gamut exploration for several engineering design problems.",
        "keywords": [],
        "uid": "v-siggraph-153",
        "time_stamp": "2021-10-29T16:00:00Z",
        "has_image": true,
        "paper_award": "",
        "image_caption": "",
        "external_paper_link": "https://doi.org/10.1145/3450626.3459750",
        "has_pdf": false,
        "ff_link": ""
    },
    "v-siggraph-287": {
        "authors": [
            "Alexandr Kuznetsov",
            "Krishna Mullia",
            "Zexi-ang Xu",
            "Milo\u0161 Ha\u0161an",
            "Ravi Ramamoorthi"
        ],
        "title": "NeuMIP: Multi-Resolution Neural Materials",
        "session_id": "v-siggraph-siggraph1",
        "abstract": "We propose NeuMIP, a neural method for representing and rendering a variety of material appearances at different scales. Classical prefiltering (mipmapping) methods work well on simple material properties such as diffuse color, but fail to generalize to normals, self-shadowing, fibers or more complex microstructures and reflectances. In this work, we generalize traditional mipmap pyramids to pyramids of neural textures, combined with a fully connected network. We also introduce neural offsets, a novel method which allows rendering materials with intricate parallax effects without any tessellation. This generalizes classical parallax mapping, but is trained without supervision by any explicit heightfield. Neural materials within our system support a 7-dimensional query, including position, incoming and outgoing direction, and the desired filter kernel size. The materials have small storage (on the order of standard mipmapping except with more texture channels), and can be integrated within common Monte-Carlo path tracing systems. We demonstrate our method on a variety of materials, resulting in complex appearance across levels of detail, with accurate parallax, self-shadowing, and other effects.",
        "keywords": [],
        "uid": "v-siggraph-287",
        "time_stamp": "2021-10-29T16:15:00Z",
        "has_image": true,
        "paper_award": "",
        "image_caption": "",
        "external_paper_link": "https://doi.org/10.1145/3450626.3459795",
        "has_pdf": false,
        "ff_link": ""
    }
}